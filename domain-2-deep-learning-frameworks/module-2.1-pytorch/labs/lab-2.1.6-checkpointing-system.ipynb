{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.1.6: Checkpointing System - Never Lose Your Progress\n",
    "\n",
    "**Module:** 2.1 - Deep Learning with PyTorch  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Save and load complete training state (model + optimizer + scheduler)\n",
    "- [ ] Implement best model tracking\n",
    "- [ ] Create early stopping logic\n",
    "- [ ] Test interrupt and resume functionality\n",
    "- [ ] Handle optimizer state correctly\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Tasks 6.1-6.5\n",
    "- Knowledge of: Training loops, file I/O\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "Training large models can take days or weeks. You NEED checkpointing to:\n",
    "- **Survive crashes**: Power outage? Network issue? No problem.\n",
    "- **Resume training**: Continue from where you left off\n",
    "- **Track best models**: Save only the models that improved\n",
    "- **Early stopping**: Stop when validation stops improving\n",
    "- **Experiment management**: Compare different runs\n",
    "\n",
    "Every major ML framework (PyTorch Lightning, Hugging Face, etc.) includes robust checkpointing!\n",
    "\n",
    "---\n",
    "\n",
    "## ELI5: What is Checkpointing?\n",
    "\n",
    "> **Imagine you're playing a video game...** üéÆ\n",
    ">\n",
    "> - **Save game**: Store your progress (level, items, score)\n",
    "> - **Load game**: Continue from your last save\n",
    "> - **Autosave**: Game saves automatically at checkpoints\n",
    "> - **Best save**: Keep a save from your highest score\n",
    ">\n",
    "> **Neural network checkpointing is exactly the same!**\n",
    "> - **Model weights** = your character's equipment\n",
    "> - **Optimizer state** = your skill tree progress\n",
    "> - **Epoch number** = current level\n",
    "> - **Best accuracy** = high score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simple model for demonstration\nclass SimpleNet(nn.Module):\n    \"\"\"\n    Simple CNN for CIFAR-10 classification.\n    \n    Architecture:\n        - 2 convolutional blocks with max pooling\n        - 2 fully connected layers\n        - ~350K parameters\n    \n    Args:\n        num_classes: Number of output classes (default: 10 for CIFAR-10)\n    \"\"\"\n    \n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(64 * 8 * 8, 256),\n            nn.ReLU(),\n            nn.Linear(256, num_classes),\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Data\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n])\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n\ntrainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\ntestloader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n\nprint(f\"Model parameters: {sum(p.numel() for p in SimpleNet().parameters()):,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Basic Checkpointing\n",
    "\n",
    "The simplest form of checkpointing: save and load model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint directory\n",
    "checkpoint_dir = Path('./checkpoints')\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create and initialize model\n",
    "model = SimpleNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"=== Before Training ===\")\n",
    "print(f\"First conv weight sum: {model.features[0].weight.sum().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for a few batches\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "\n",
    "for i, (inputs, labels) in enumerate(trainloader):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"=== After Training ===\")\n",
    "print(f\"First conv weight sum: {model.features[0].weight.sum().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic save: Just model weights\n",
    "# This is the SIMPLEST form - only saves model parameters\n",
    "\n",
    "# Save\n",
    "torch.save(model.state_dict(), checkpoint_dir / 'model_weights_only.pth')\n",
    "print(\"Saved model weights\")\n",
    "\n",
    "# Load into a new model\n",
    "model_loaded = SimpleNet().to(device)\n",
    "model_loaded.load_state_dict(torch.load(checkpoint_dir / 'model_weights_only.pth'))\n",
    "\n",
    "print(f\"\\nLoaded model weight sum: {model_loaded.features[0].weight.sum().item():.4f}\")\n",
    "print(f\"Original model weight sum: {model.features[0].weight.sum().item():.4f}\")\n",
    "print(f\"Weights match: {torch.allclose(model.features[0].weight, model_loaded.features[0].weight)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Just Weights Isn't Enough\n",
    "\n",
    "Saving only weights means you lose:\n",
    "- **Optimizer state**: Momentum, adaptive learning rates (Adam's m, v)\n",
    "- **Training progress**: Epoch number, step count\n",
    "- **Scheduler state**: Learning rate schedule position\n",
    "- **Best metrics**: Track of best validation score\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Complete Training State\n",
    "\n",
    "Save EVERYTHING needed to resume training exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    model: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    epoch: int,\n",
    "    loss: float,\n",
    "    accuracy: float,\n",
    "    path: Path,\n",
    "    scheduler: Optional[Any] = None,\n",
    "    extra_info: Optional[Dict] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Save complete training checkpoint.\n",
    "    \n",
    "    This saves everything needed to resume training exactly\n",
    "    where we left off.\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        # Model state\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        \n",
    "        # Optimizer state (CRUCIAL for Adam, SGD with momentum, etc.)\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        \n",
    "        # Training progress\n",
    "        'epoch': epoch,\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "        \n",
    "        # Metadata\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'pytorch_version': torch.__version__,\n",
    "    }\n",
    "    \n",
    "    # Optional: scheduler state\n",
    "    if scheduler is not None:\n",
    "        checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "    \n",
    "    # Optional: extra info\n",
    "    if extra_info:\n",
    "        checkpoint['extra_info'] = extra_info\n",
    "    \n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Checkpoint saved: {path}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(\n",
    "    path: Path,\n",
    "    model: nn.Module,\n",
    "    optimizer: Optional[optim.Optimizer] = None,\n",
    "    scheduler: Optional[Any] = None,\n",
    "    device: torch.device = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load training checkpoint and restore state.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with training info (epoch, loss, accuracy, etc.)\n",
    "    \"\"\"\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    \n",
    "    # Restore model\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Restore optimizer (if provided)\n",
    "    if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # Restore scheduler (if provided)\n",
    "    if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    print(f\"Checkpoint loaded: {path}\")\n",
    "    print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "    print(f\"  Loss: {checkpoint['loss']:.4f}\")\n",
    "    print(f\"  Accuracy: {checkpoint['accuracy']:.2f}%\")\n",
    "    \n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test complete checkpointing\n",
    "model = SimpleNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Simulate some training\n",
    "for epoch in range(3):\n",
    "    for i, (inputs, labels) in enumerate(trainloader):\n",
    "        if i >= 5:\n",
    "            break\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(inputs), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "# Save checkpoint\n",
    "save_checkpoint(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epoch=3,\n",
    "    loss=loss.item(),\n",
    "    accuracy=75.5,  # Simulated\n",
    "    path=checkpoint_dir / 'complete_checkpoint.pth',\n",
    "    scheduler=scheduler,\n",
    "    extra_info={'batch_size': 64, 'learning_rate': 0.001}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint into fresh model\n",
    "new_model = SimpleNet().to(device)\n",
    "new_optimizer = optim.Adam(new_model.parameters(), lr=0.001)\n",
    "new_scheduler = optim.lr_scheduler.StepLR(new_optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "checkpoint_info = load_checkpoint(\n",
    "    checkpoint_dir / 'complete_checkpoint.pth',\n",
    "    new_model,\n",
    "    new_optimizer,\n",
    "    new_scheduler,\n",
    "    device\n",
    ")\n",
    "\n",
    "# Verify optimizer state was restored\n",
    "print(f\"\\nOptimizer state keys: {list(new_optimizer.state_dict()['state'].keys())}\")\n",
    "print(f\"Scheduler last_epoch: {new_scheduler.last_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Best Model Tracking & Early Stopping\n",
    "\n",
    "Save only when performance improves, stop when it doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CheckpointManager:\n",
    "    \"\"\"\n",
    "    Manages checkpoints with best model tracking and early stopping.\n",
    "    \n",
    "    Features:\n",
    "    - Save best model based on validation metric\n",
    "    - Keep last N checkpoints\n",
    "    - Early stopping when metric doesn't improve\n",
    "    \"\"\"\n",
    "    \n",
    "    checkpoint_dir: Path\n",
    "    best_metric: float = float('inf')  # For loss (lower is better)\n",
    "    best_epoch: int = 0\n",
    "    patience: int = 5\n",
    "    patience_counter: int = 0\n",
    "    mode: str = 'min'  # 'min' for loss, 'max' for accuracy\n",
    "    max_checkpoints: int = 3\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.checkpoint_dir = Path(self.checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.checkpoint_history = []\n",
    "        \n",
    "        if self.mode == 'max':\n",
    "            self.best_metric = float('-inf')\n",
    "    \n",
    "    def _is_better(self, metric: float) -> bool:\n",
    "        \"\"\"Check if metric is better than best.\"\"\"\n",
    "        if self.mode == 'min':\n",
    "            return metric < self.best_metric\n",
    "        else:\n",
    "            return metric > self.best_metric\n",
    "    \n",
    "    def save_checkpoint(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        epoch: int,\n",
    "        metric: float,\n",
    "        scheduler: Optional[Any] = None,\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Save checkpoint if metric improved.\n",
    "        \n",
    "        Returns:\n",
    "            True if this was the best model\n",
    "        \"\"\"\n",
    "        is_best = self._is_better(metric)\n",
    "        \n",
    "        # Always save latest\n",
    "        checkpoint = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'metric': metric,\n",
    "            'best_metric': self.best_metric if is_best else self.best_metric,\n",
    "        }\n",
    "        if scheduler:\n",
    "            checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "        \n",
    "        # Save latest checkpoint\n",
    "        latest_path = self.checkpoint_dir / 'checkpoint_latest.pth'\n",
    "        torch.save(checkpoint, latest_path)\n",
    "        \n",
    "        # Save epoch checkpoint\n",
    "        epoch_path = self.checkpoint_dir / f'checkpoint_epoch_{epoch:04d}.pth'\n",
    "        torch.save(checkpoint, epoch_path)\n",
    "        self.checkpoint_history.append(epoch_path)\n",
    "        \n",
    "        # Remove old checkpoints\n",
    "        while len(self.checkpoint_history) > self.max_checkpoints:\n",
    "            old_path = self.checkpoint_history.pop(0)\n",
    "            if old_path.exists():\n",
    "                old_path.unlink()\n",
    "        \n",
    "        # Update best if improved\n",
    "        if is_best:\n",
    "            self.best_metric = metric\n",
    "            self.best_epoch = epoch\n",
    "            self.patience_counter = 0\n",
    "            \n",
    "            best_path = self.checkpoint_dir / 'checkpoint_best.pth'\n",
    "            torch.save(checkpoint, best_path)\n",
    "            print(f\"New best! Epoch {epoch}: {metric:.4f}\")\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "        \n",
    "        return is_best\n",
    "    \n",
    "    def should_stop(self) -> bool:\n",
    "        \"\"\"Check if training should stop (early stopping).\"\"\"\n",
    "        return self.patience_counter >= self.patience\n",
    "    \n",
    "    def load_best(self, model: nn.Module, optimizer: Optional[optim.Optimizer] = None):\n",
    "        \"\"\"Load the best checkpoint.\"\"\"\n",
    "        best_path = self.checkpoint_dir / 'checkpoint_best.pth'\n",
    "        if not best_path.exists():\n",
    "            raise FileNotFoundError(f\"Best checkpoint not found: {best_path}\")\n",
    "        \n",
    "        checkpoint = torch.load(best_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        if optimizer:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        print(f\"Loaded best checkpoint from epoch {checkpoint['epoch']}\")\n",
    "        return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with checkpoint manager\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(dataloader), 100 * correct / total\n",
    "\n",
    "\n",
    "# Setup\n",
    "model = SimpleNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Checkpoint manager tracking validation loss\n",
    "ckpt_manager = CheckpointManager(\n",
    "    checkpoint_dir=checkpoint_dir / 'experiment_1',\n",
    "    patience=5,\n",
    "    mode='min',  # Lower loss is better\n",
    "    max_checkpoints=3,\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training with checkpointing...\\n\")\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for inputs, labels in trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(inputs), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(trainloader)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss, val_acc = evaluate(model, testloader, criterion)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    is_best = ckpt_manager.save_checkpoint(\n",
    "        model, optimizer, epoch, val_loss, scheduler\n",
    "    )\n",
    "    \n",
    "    status = \" (best)\" if is_best else \"\"\n",
    "    print(f\"Epoch {epoch+1:2d} | Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%{status}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if ckpt_manager.should_stop():\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}!\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nBest model: Epoch {ckpt_manager.best_epoch} with loss {ckpt_manager.best_metric:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Resuming Training\n",
    "\n",
    "Test that we can resume training from a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate resuming from checkpoint\n",
    "print(\"=== Resuming Training ===\")\n",
    "\n",
    "# Create fresh model and optimizer\n",
    "resumed_model = SimpleNet().to(device)\n",
    "resumed_optimizer = optim.Adam(resumed_model.parameters(), lr=0.001)\n",
    "\n",
    "# Load latest checkpoint\n",
    "latest_path = checkpoint_dir / 'experiment_1' / 'checkpoint_latest.pth'\n",
    "checkpoint = torch.load(latest_path)\n",
    "\n",
    "resumed_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "resumed_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "print(f\"Resumed from epoch {checkpoint['epoch']}\")\n",
    "print(f\"Continuing from epoch {start_epoch}\")\n",
    "\n",
    "# Verify optimizer state\n",
    "print(f\"\\nOptimizer state restored: {len(resumed_optimizer.state) > 0}\")\n",
    "\n",
    "# Continue training for a few more epochs\n",
    "for epoch in range(start_epoch, start_epoch + 3):\n",
    "    resumed_model.train()\n",
    "    for i, (inputs, labels) in enumerate(trainloader):\n",
    "        if i >= 10:\n",
    "            break\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        resumed_optimizer.zero_grad()\n",
    "        loss = criterion(resumed_model(inputs), labels)\n",
    "        loss.backward()\n",
    "        resumed_optimizer.step()\n",
    "    \n",
    "    val_loss, val_acc = evaluate(resumed_model, testloader, criterion)\n",
    "    print(f\"Epoch {epoch+1} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "print(\"\\nTraining resumed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Production Checkpoint Manager\n",
    "\n",
    "A complete, production-ready checkpoint manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionCheckpointManager:\n",
    "    \"\"\"\n",
    "    Production-ready checkpoint manager.\n",
    "    \n",
    "    Features:\n",
    "    - Best model tracking\n",
    "    - Early stopping\n",
    "    - Checkpoint rotation\n",
    "    - Training log persistence\n",
    "    - Atomic saves (prevents corruption)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        checkpoint_dir: str,\n",
    "        model: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        scheduler: Optional[Any] = None,\n",
    "        mode: str = 'min',\n",
    "        patience: int = 10,\n",
    "        max_checkpoints: int = 5,\n",
    "    ):\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.patience = patience\n",
    "        self.max_checkpoints = max_checkpoints\n",
    "        \n",
    "        # State\n",
    "        self.best_metric = float('inf') if mode == 'min' else float('-inf')\n",
    "        self.best_epoch = 0\n",
    "        self.patience_counter = 0\n",
    "        self.history = []\n",
    "        self.checkpoint_files = []\n",
    "        \n",
    "        # Load existing history if resuming\n",
    "        self._load_history()\n",
    "    \n",
    "    def _load_history(self):\n",
    "        \"\"\"Load training history if exists.\"\"\"\n",
    "        history_path = self.checkpoint_dir / 'training_history.json'\n",
    "        if history_path.exists():\n",
    "            with open(history_path, 'r') as f:\n",
    "                self.history = json.load(f)\n",
    "            print(f\"Loaded {len(self.history)} epochs of history\")\n",
    "    \n",
    "    def _save_history(self):\n",
    "        \"\"\"Save training history.\"\"\"\n",
    "        history_path = self.checkpoint_dir / 'training_history.json'\n",
    "        with open(history_path, 'w') as f:\n",
    "            json.dump(self.history, f, indent=2)\n",
    "    \n",
    "    def _is_better(self, metric: float) -> bool:\n",
    "        if self.mode == 'min':\n",
    "            return metric < self.best_metric\n",
    "        return metric > self.best_metric\n",
    "    \n",
    "    def _save_checkpoint(self, path: Path, checkpoint: dict):\n",
    "        \"\"\"Atomic save to prevent corruption.\"\"\"\n",
    "        # Save to temp file first\n",
    "        temp_path = path.with_suffix('.tmp')\n",
    "        torch.save(checkpoint, temp_path)\n",
    "        # Rename (atomic on most filesystems)\n",
    "        temp_path.rename(path)\n",
    "    \n",
    "    def step(\n",
    "        self,\n",
    "        epoch: int,\n",
    "        train_loss: float,\n",
    "        val_loss: float,\n",
    "        val_metric: float,\n",
    "        extra_metrics: Optional[Dict] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Call at end of each epoch.\n",
    "        \n",
    "        Returns:\n",
    "            Dict with 'is_best', 'should_stop' flags\n",
    "        \"\"\"\n",
    "        is_best = self._is_better(val_metric)\n",
    "        \n",
    "        # Build checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_metric': val_metric,\n",
    "            'best_metric': self.best_metric,\n",
    "            'best_epoch': self.best_epoch,\n",
    "        }\n",
    "        if self.scheduler:\n",
    "            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()\n",
    "        \n",
    "        # Save latest\n",
    "        self._save_checkpoint(\n",
    "            self.checkpoint_dir / 'latest.pth', checkpoint\n",
    "        )\n",
    "        \n",
    "        # Save periodic checkpoint\n",
    "        epoch_path = self.checkpoint_dir / f'epoch_{epoch:04d}.pth'\n",
    "        self._save_checkpoint(epoch_path, checkpoint)\n",
    "        self.checkpoint_files.append(epoch_path)\n",
    "        \n",
    "        # Remove old checkpoints\n",
    "        while len(self.checkpoint_files) > self.max_checkpoints:\n",
    "            old_path = self.checkpoint_files.pop(0)\n",
    "            if old_path.exists():\n",
    "                old_path.unlink()\n",
    "        \n",
    "        # Handle best model\n",
    "        if is_best:\n",
    "            self.best_metric = val_metric\n",
    "            self.best_epoch = epoch\n",
    "            self.patience_counter = 0\n",
    "            self._save_checkpoint(\n",
    "                self.checkpoint_dir / 'best.pth', checkpoint\n",
    "            )\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "        \n",
    "        # Update history\n",
    "        self.history.append({\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_metric': val_metric,\n",
    "            'is_best': is_best,\n",
    "            **(extra_metrics or {})\n",
    "        })\n",
    "        self._save_history()\n",
    "        \n",
    "        return {\n",
    "            'is_best': is_best,\n",
    "            'should_stop': self.patience_counter >= self.patience,\n",
    "            'best_metric': self.best_metric,\n",
    "            'best_epoch': self.best_epoch,\n",
    "            'patience_counter': self.patience_counter,\n",
    "        }\n",
    "    \n",
    "    def load_best(self):\n",
    "        \"\"\"Load the best model.\"\"\"\n",
    "        path = self.checkpoint_dir / 'best.pth'\n",
    "        return self._load_checkpoint(path)\n",
    "    \n",
    "    def load_latest(self):\n",
    "        \"\"\"Load the latest checkpoint for resuming.\"\"\"\n",
    "        path = self.checkpoint_dir / 'latest.pth'\n",
    "        return self._load_checkpoint(path)\n",
    "    \n",
    "    def _load_checkpoint(self, path: Path) -> dict:\n",
    "        \"\"\"Load checkpoint and restore state.\"\"\"\n",
    "        checkpoint = torch.load(path)\n",
    "        \n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        if self.scheduler and 'scheduler_state_dict' in checkpoint:\n",
    "            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "        self.best_metric = checkpoint.get('best_metric', self.best_metric)\n",
    "        self.best_epoch = checkpoint.get('best_epoch', self.best_epoch)\n",
    "        \n",
    "        return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "model = SimpleNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "ckpt = ProductionCheckpointManager(\n",
    "    checkpoint_dir='./checkpoints/production_run',\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    mode='max',  # Track accuracy (higher is better)\n",
    "    patience=5,\n",
    ")\n",
    "\n",
    "print(\"Production checkpoint manager ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ Basic model weight saving/loading\n",
    "- ‚úÖ Complete training state checkpointing\n",
    "- ‚úÖ Best model tracking\n",
    "- ‚úÖ Early stopping implementation\n",
    "- ‚úÖ Production-ready checkpoint management\n",
    "\n",
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Not saving optimizer state\n",
    "```python\n",
    "# ‚ùå Wrong - loses momentum, adaptive LR state\n",
    "torch.save(model.state_dict(), 'checkpoint.pth')\n",
    "\n",
    "# ‚úÖ Right - save complete state\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "}, 'checkpoint.pth')\n",
    "```\n",
    "\n",
    "### Mistake 2: Forgetting map_location when loading\n",
    "```python\n",
    "# ‚ùå Wrong - may fail if saved on GPU, loaded on CPU\n",
    "checkpoint = torch.load('checkpoint.pth')\n",
    "\n",
    "# ‚úÖ Right - specify device\n",
    "checkpoint = torch.load('checkpoint.pth', map_location=device)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [PyTorch Saving and Loading](https://pytorch.org/tutorials/beginner/saving_loading_models.html)\n",
    "- [PyTorch Lightning Checkpointing](https://lightning.ai/docs/pytorch/stable/common/checkpointing.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "# Remove checkpoint directories\n",
    "if checkpoint_dir.exists():\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "    print(\"Removed checkpoint directory\")\n",
    "\n",
    "prod_dir = Path('./checkpoints/production_run')\n",
    "if prod_dir.exists():\n",
    "    shutil.rmtree(prod_dir)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}