{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6.1: Custom Module Lab - Building ResNet from Scratch\n",
    "\n",
    "**Module:** 6 - Deep Learning with PyTorch  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** â­â­ (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand PyTorch's `nn.Module` architecture pattern\n",
    "- [ ] Implement ResNet's `BasicBlock` with skip connections\n",
    "- [ ] Implement the more efficient `Bottleneck` block\n",
    "- [ ] Build a complete ResNet-18 from scratch\n",
    "- [ ] Test your implementation on CIFAR-10\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Phase 1 (Neural Network Fundamentals)\n",
    "- Knowledge of: Convolutional neural networks, backpropagation\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "ResNet (Residual Networks) won the 2015 ImageNet competition and revolutionized deep learning. Before ResNet, training very deep networks (50+ layers) was nearly impossible due to vanishing gradients. \n",
    "\n",
    "Today, ResNet architecture is used everywhere:\n",
    "- **Medical imaging**: Detecting tumors in X-rays\n",
    "- **Autonomous vehicles**: Object detection systems\n",
    "- **Social media**: Photo filtering and face recognition\n",
    "- **Retail**: Product recognition and inventory management\n",
    "\n",
    "Understanding how to build ResNet teaches you the foundational patterns used in almost all modern neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "## ELI5: What is a Residual Network?\n",
    "\n",
    "> **Imagine you're playing telephone...** ðŸŽ®\n",
    ">\n",
    "> In regular telephone, a message passes through many people. By the end, it's often completely different from the original. The more people in the chain, the worse it gets!\n",
    ">\n",
    "> Now imagine if each person could also HEAR the original message directly, and just add small changes to it. Even after 100 people, the message stays mostly intact, with useful additions.\n",
    ">\n",
    "> **That's exactly what skip connections do!**\n",
    ">\n",
    "> - The \"message\" is the data flowing through the network\n",
    "> - Each \"person\" is a layer that transforms the data\n",
    "> - The \"skip connection\" lets the original data bypass layers\n",
    "> - The network only needs to learn what to ADD, not rebuild everything\n",
    ">\n",
    "> **In AI terms:** Skip connections let gradients flow directly backwards, preventing them from vanishing. Instead of learning F(x), the network learns F(x) + x, making training 100+ layer networks possible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment Setup and PyTorch Basics\n",
    "\n",
    "Let's start by setting up our environment and reviewing key PyTorch concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Type, Union, List, Optional\n",
    "import time\n",
    "\n",
    "# Check our environment\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Set device - on DGX Spark, we always want to use the GPU!\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding nn.Module\n",
    "\n",
    "Every neural network in PyTorch inherits from `nn.Module`. Think of it as a **blueprint** that gives your model superpowers:\n",
    "\n",
    "1. **Automatic parameter tracking** - All weights are found automatically\n",
    "2. **Device management** - Move to GPU with `.to(device)`\n",
    "3. **Training/eval modes** - Dropout, BatchNorm behave differently\n",
    "4. **State saving** - Save/load with `state_dict()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple nn.Module example\n",
    "class SimpleConvBlock(nn.Module):\n",
    "    \"\"\"A basic convolutional block: Conv -> BatchNorm -> ReLU\n",
    "    \n",
    "    This is the fundamental building block of most CNNs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3):\n",
    "        # ALWAYS call super().__init__() first!\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define layers as attributes\n",
    "        # PyTorch automatically tracks these as \"submodules\"\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels, \n",
    "            kernel_size=kernel_size, \n",
    "            padding=kernel_size // 2,  # 'same' padding\n",
    "            bias=False  # BatchNorm has its own bias, so we don't need conv bias\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)  # inplace saves memory!\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass - define how data flows through the module.\"\"\"\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "# Let's test it!\n",
    "block = SimpleConvBlock(3, 64)\n",
    "print(f\"Block structure:\\n{block}\")\n",
    "print(f\"\\nNumber of parameters: {sum(p.numel() for p in block.parameters()):,}\")\n",
    "\n",
    "# Test with random input\n",
    "x = torch.randn(1, 3, 32, 32)  # Batch=1, Channels=3, Height=32, Width=32\n",
    "y = block(x)\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "1. We created a module with 3 layers: Conv2d, BatchNorm2d, ReLU\n",
    "2. PyTorch automatically found all the parameters (1,792 = 3Ã—64Ã—3Ã—3 + 64Ã—2 for BatchNorm)\n",
    "3. The output has 64 channels instead of 3, but same spatial dimensions (32Ã—32)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The BasicBlock - ResNet's Foundation\n",
    "\n",
    "Now let's build the actual ResNet blocks. The `BasicBlock` is used in ResNet-18 and ResNet-34.\n",
    "\n",
    "### Architecture:\n",
    "```\n",
    "Input (x)\n",
    "    |\n",
    "    +---> [3x3 Conv -> BN -> ReLU -> 3x3 Conv -> BN] --+\n",
    "    |                                                   |\n",
    "    +------------ (skip connection) -------------------+\n",
    "    |                                                   |\n",
    "    +<-------------------------------------------------+\n",
    "    |\n",
    "    v\n",
    "  [ReLU]\n",
    "    |\n",
    "    v\n",
    "  Output\n",
    "```\n",
    "\n",
    "The key insight: **output = ReLU(F(x) + x)**\n",
    "\n",
    "The network learns the \"residual\" F(x), which is what needs to be ADDED to x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet BasicBlock: Two 3x3 convolutions with a skip connection.\n",
    "    \n",
    "    Used in ResNet-18 and ResNet-34.\n",
    "    \n",
    "    Args:\n",
    "        in_channels: Number of input channels\n",
    "        out_channels: Number of output channels\n",
    "        stride: Stride for the first convolution (2 for downsampling)\n",
    "    \n",
    "    ELI5: This is like a highway exit ramp. The main road (skip connection)\n",
    "    lets you bypass the city (conv layers) if you want. But the city might\n",
    "    add something useful, so we combine both paths!\n",
    "    \"\"\"\n",
    "    \n",
    "    # Expansion factor - BasicBlock doesn't expand channels\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels: int, \n",
    "        out_channels: int, \n",
    "        stride: int = 1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First conv: may downsample if stride=2\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, \n",
    "            kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Second conv: always stride=1\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, out_channels,\n",
    "            kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Skip connection - only needed when dimensions change\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            # Use 1x1 conv to match dimensions\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Save input for skip connection\n",
    "        identity = x\n",
    "        \n",
    "        # Main path: Conv -> BN -> ReLU -> Conv -> BN\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out, inplace=True)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # Add skip connection (possibly transformed)\n",
    "        out += self.shortcut(identity)\n",
    "        \n",
    "        # Final activation\n",
    "        out = F.relu(out, inplace=True)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Test the BasicBlock\n",
    "print(\"=== Testing BasicBlock ===\")\n",
    "\n",
    "# Case 1: Same dimensions (no shortcut needed)\n",
    "block1 = BasicBlock(64, 64, stride=1)\n",
    "x1 = torch.randn(2, 64, 32, 32)\n",
    "y1 = block1(x1)\n",
    "print(f\"Same dimensions: {x1.shape} -> {y1.shape}\")\n",
    "\n",
    "# Case 2: Downsampling (shortcut needed)\n",
    "block2 = BasicBlock(64, 128, stride=2)\n",
    "x2 = torch.randn(2, 64, 32, 32)\n",
    "y2 = block2(x2)\n",
    "print(f\"Downsampling: {x2.shape} -> {y2.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "print(f\"\\nBlock1 parameters: {sum(p.numel() for p in block1.parameters()):,}\")\n",
    "print(f\"Block2 parameters: {sum(p.numel() for p in block2.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "1. **Same dimensions (64â†’64)**: The shortcut is empty (`nn.Sequential()`), so the input passes through directly\n",
    "2. **Downsampling (64â†’128, stride=2)**: The shortcut uses a 1Ã—1 conv to match dimensions\n",
    "\n",
    "Notice:\n",
    "- Spatial dimensions halved: 32Ã—32 â†’ 16Ã—16 (due to stride=2)\n",
    "- Channels doubled: 64 â†’ 128\n",
    "\n",
    "This pattern (halve spatial, double channels) is common in CNNs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ‹ Try It Yourself: Exercise 1\n",
    "\n",
    "Modify the BasicBlock to add a **dropout layer** after the first ReLU for regularization.\n",
    "\n",
    "**Requirements:**\n",
    "1. Add a `dropout` parameter to `__init__` (default 0.0)\n",
    "2. Apply dropout after the first ReLU\n",
    "3. Test with dropout=0.1\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ’¡ Hint</summary>\n",
    "\n",
    "Use `nn.Dropout2d(p=dropout)` for 2D spatial dropout. Remember to add it as a module attribute so it's properly registered!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Implement BasicBlockWithDropout\n",
    "class BasicBlockWithDropout(nn.Module):\n",
    "    \"\"\"BasicBlock with optional dropout regularization.\"\"\"\n",
    "    \n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        # TODO: Copy the BasicBlock implementation and add dropout\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: Implement forward with dropout\n",
    "        pass\n",
    "\n",
    "# Test your implementation\n",
    "# block = BasicBlockWithDropout(64, 64, dropout=0.1)\n",
    "# x = torch.randn(2, 64, 32, 32)\n",
    "# y = block(x)\n",
    "# print(f\"Output shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: The Bottleneck Block - For Deeper Networks\n",
    "\n",
    "For ResNet-50, ResNet-101, and ResNet-152, we use **Bottleneck blocks** instead. These are more efficient for deeper networks.\n",
    "\n",
    "### Architecture:\n",
    "```\n",
    "Input (x, 256 channels)\n",
    "    |\n",
    "    +---> [1x1 Conv -> BN -> ReLU] (squeeze to 64 channels)\n",
    "    |         |\n",
    "    |         v\n",
    "    |     [3x3 Conv -> BN -> ReLU] (process at 64 channels - cheaper!)\n",
    "    |         |\n",
    "    |         v  \n",
    "    |     [1x1 Conv -> BN] (expand back to 256 channels)\n",
    "    |                                                   \n",
    "    +------------ (skip connection) -------------------+\n",
    "                                                        |\n",
    "                                                        v\n",
    "                                                     [Add]\n",
    "                                                        |\n",
    "                                                        v\n",
    "                                                     [ReLU]\n",
    "                                                        |\n",
    "                                                        v\n",
    "                                                     Output (256 channels)\n",
    "```\n",
    "\n",
    "### ELI5: Why \"Bottleneck\"?\n",
    "\n",
    "> Imagine you're moving water through pipes. A wide pipe can move a lot of water but costs more. \n",
    ">\n",
    "> A bottleneck is like: Wide pipe â†’ Narrow pipe â†’ Wide pipe\n",
    ">\n",
    "> The narrow part (\"bottleneck\") is where the expensive 3Ã—3 convolution happens, but with fewer channels, so it's cheaper! The 1Ã—1 convolutions are like adapters that change the pipe width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet Bottleneck Block: 1x1 -> 3x3 -> 1x1 with skip connection.\n",
    "    \n",
    "    Used in ResNet-50, ResNet-101, ResNet-152.\n",
    "    \n",
    "    The expansion factor of 4 means the output has 4x the channels\n",
    "    of the bottleneck layer.\n",
    "    \n",
    "    Args:\n",
    "        in_channels: Number of input channels\n",
    "        bottleneck_channels: Number of channels in the bottleneck (middle) layer\n",
    "        stride: Stride for the 3x3 convolution\n",
    "    \"\"\"\n",
    "    \n",
    "    # Output channels = bottleneck_channels * expansion\n",
    "    expansion = 4\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels: int, \n",
    "        bottleneck_channels: int, \n",
    "        stride: int = 1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        out_channels = bottleneck_channels * self.expansion\n",
    "        \n",
    "        # 1x1 conv: squeeze channels\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, bottleneck_channels, \n",
    "            kernel_size=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(bottleneck_channels)\n",
    "        \n",
    "        # 3x3 conv: spatial processing (possibly with stride)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            bottleneck_channels, bottleneck_channels,\n",
    "            kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n",
    "        \n",
    "        # 1x1 conv: expand channels\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            bottleneck_channels, out_channels,\n",
    "            kernel_size=1, bias=False\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "        \n",
    "        # Squeeze\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out, inplace=True)\n",
    "        \n",
    "        # Process\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = F.relu(out, inplace=True)\n",
    "        \n",
    "        # Expand\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        # Add shortcut\n",
    "        out += self.shortcut(identity)\n",
    "        out = F.relu(out, inplace=True)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Test the Bottleneck\n",
    "print(\"=== Testing Bottleneck ===\")\n",
    "\n",
    "# Case 1: First block (needs shortcut to expand channels)\n",
    "block1 = Bottleneck(64, 64, stride=1)  # 64 -> 256 channels\n",
    "x1 = torch.randn(2, 64, 56, 56)\n",
    "y1 = block1(x1)\n",
    "print(f\"First block: {x1.shape} -> {y1.shape}\")\n",
    "\n",
    "# Case 2: Subsequent block (no shortcut needed)\n",
    "block2 = Bottleneck(256, 64, stride=1)  # 256 -> 256 channels\n",
    "x2 = torch.randn(2, 256, 56, 56)\n",
    "y2 = block2(x2)\n",
    "print(f\"Subsequent block: {x2.shape} -> {y2.shape}\")\n",
    "\n",
    "# Case 3: Downsampling block\n",
    "block3 = Bottleneck(256, 128, stride=2)  # 256 -> 512 channels, size/2\n",
    "x3 = torch.randn(2, 256, 56, 56)\n",
    "y3 = block3(x3)\n",
    "print(f\"Downsampling block: {x3.shape} -> {y3.shape}\")\n",
    "\n",
    "# Compare parameters\n",
    "print(f\"\\nBottleneck parameters: {sum(p.numel() for p in block2.parameters()):,}\")\n",
    "# Compare to equivalent BasicBlock (256 channels throughout)\n",
    "equiv_basic = nn.Sequential(\n",
    "    nn.Conv2d(256, 256, 3, padding=1),\n",
    "    nn.Conv2d(256, 256, 3, padding=1)\n",
    ")\n",
    "print(f\"Equivalent BasicBlock params: {sum(p.numel() for p in equiv_basic.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "The Bottleneck block:\n",
    "1. **Squeezes** channels: 256 â†’ 64 (1Ã—1 conv)\n",
    "2. **Processes** at reduced channels: 64 â†’ 64 (3Ã—3 conv, the expensive part)\n",
    "3. **Expands** back: 64 â†’ 256 (1Ã—1 conv)\n",
    "\n",
    "**Parameter comparison:**\n",
    "- Bottleneck: ~70K parameters\n",
    "- Equivalent BasicBlock: ~1.2M parameters (with 256 channels)\n",
    "\n",
    "That's **17x fewer parameters** for the same layer connectivity!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Building Complete ResNet-18\n",
    "\n",
    "Now let's assemble our blocks into a complete ResNet-18.\n",
    "\n",
    "### ResNet-18 Architecture:\n",
    "\n",
    "```\n",
    "Layer              Output Size   Channels\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Conv 7x7, stride 2    112Ã—112       64\n",
    "MaxPool 3x3, stride 2  56Ã—56        64\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Layer1: 2Ã— BasicBlock   56Ã—56       64\n",
    "Layer2: 2Ã— BasicBlock   28Ã—28      128\n",
    "Layer3: 2Ã— BasicBlock   14Ã—14      256\n",
    "Layer4: 2Ã— BasicBlock    7Ã—7       512\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "AdaptiveAvgPool         1Ã—1        512\n",
    "FC                      1Ã—1       1000 (ImageNet)\n",
    "```\n",
    "\n",
    "For CIFAR-10 (32Ã—32 images), we'll modify the first layer to not downsample so aggressively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet implementation for CIFAR-10.\n",
    "    \n",
    "    Modified from standard ResNet:\n",
    "    - First conv is 3x3 instead of 7x7 (images are 32x32, not 224x224)\n",
    "    - No initial max pooling\n",
    "    \n",
    "    Args:\n",
    "        block: Block type (BasicBlock or Bottleneck)\n",
    "        layers: Number of blocks in each layer [layer1, layer2, layer3, layer4]\n",
    "        num_classes: Number of output classes\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 10\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = 64\n",
    "        \n",
    "        # Initial convolution - smaller for CIFAR-10\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # ResNet layers\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        \n",
    "        # Classifier\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        \n",
    "        # Weight initialization\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _make_layer(\n",
    "        self, \n",
    "        block: Type[Union[BasicBlock, Bottleneck]], \n",
    "        channels: int, \n",
    "        num_blocks: int, \n",
    "        stride: int\n",
    "    ) -> nn.Sequential:\n",
    "        \"\"\"Create a layer with multiple blocks.\"\"\"\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # First block may downsample\n",
    "        layers.append(block(self.in_channels, channels, stride))\n",
    "        self.in_channels = channels * block.expansion\n",
    "        \n",
    "        # Remaining blocks maintain dimensions\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(self.in_channels, channels, stride=1))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using Kaiming initialization.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Initial conv\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        \n",
    "        # ResNet layers\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        # Classifier\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet18(num_classes: int = 10) -> ResNet:\n",
    "    \"\"\"Create ResNet-18 model.\"\"\"\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
    "\n",
    "\n",
    "def resnet34(num_classes: int = 10) -> ResNet:\n",
    "    \"\"\"Create ResNet-34 model.\"\"\"\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "\n",
    "def resnet50(num_classes: int = 10) -> ResNet:\n",
    "    \"\"\"Create ResNet-50 model (with Bottleneck blocks).\"\"\"\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "\n",
    "# Test our ResNet-18\n",
    "model = resnet18(num_classes=10)\n",
    "print(f\"ResNet-18 for CIFAR-10\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(4, 3, 32, 32)  # CIFAR-10 sized input\n",
    "y = model(x)\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "\n",
    "# Show layer progression\n",
    "print(\"\\nLayer dimensions:\")\n",
    "x = torch.randn(1, 3, 32, 32)\n",
    "x = F.relu(model.bn1(model.conv1(x)))\n",
    "print(f\"After conv1: {x.shape}\")\n",
    "x = model.layer1(x)\n",
    "print(f\"After layer1: {x.shape}\")\n",
    "x = model.layer2(x)\n",
    "print(f\"After layer2: {x.shape}\")\n",
    "x = model.layer3(x)\n",
    "print(f\"After layer3: {x.shape}\")\n",
    "x = model.layer4(x)\n",
    "print(f\"After layer4: {x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Training on CIFAR-10\n",
    "\n",
    "Now let's train our ResNet-18 on CIFAR-10 and see it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Data transforms\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\n# Load CIFAR-10\ntrainset = torchvision.datasets.CIFAR10(\n    root='./data', train=True, download=True, transform=transform_train\n)\ntestset = torchvision.datasets.CIFAR10(\n    root='./data', train=False, download=True, transform=transform_test\n)\n\n# DGX Spark optimization: Adaptive batch size based on available GPU memory\nif torch.cuda.is_available():\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    if gpu_mem > 100:  # DGX Spark with 128GB unified memory\n        BATCH_SIZE = 256\n    elif gpu_mem > 16:\n        BATCH_SIZE = 128\n    else:\n        BATCH_SIZE = 64\nelse:\n    BATCH_SIZE = 64\n\ntrainloader = DataLoader(\n    trainset, batch_size=BATCH_SIZE, shuffle=True, \n    num_workers=4, pin_memory=True\n)\ntestloader = DataLoader(\n    testset, batch_size=BATCH_SIZE, shuffle=False, \n    num_workers=4, pin_memory=True\n)\n\n# Class names for visualization\nclasses = ('plane', 'car', 'bird', 'cat', 'deer', \n           'dog', 'frog', 'horse', 'ship', 'truck')\n\nprint(f\"Training samples: {len(trainset):,}\")\nprint(f\"Test samples: {len(testset):,}\")\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"Batches per epoch: {len(trainloader)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_epoch(model, trainloader, criterion, optimizer, device):\n    \"\"\"Train for one epoch.\"\"\"\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for batch_idx, (inputs, targets) in enumerate(trainloader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n    \n    return running_loss / len(trainloader), 100. * correct / total\n\n\ndef evaluate(model, testloader, criterion, device):\n    \"\"\"Evaluate the model.\"\"\"\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, targets in testloader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n    \n    return running_loss / len(testloader), 100. * correct / total\n\n\n# Create model and move to GPU\nmodel = resnet18(num_classes=10).to(device)\n\n# Loss and optimizer\n# Note: weight_decay=5e-4 is L2 regularization, a standard value for CIFAR-10 training\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n\n# Learning rate scheduler - cosine annealing for smooth decay\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n\nprint(f\"Model on device: {next(model.parameters()).device}\")\nprint(f\"GPU Memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop - 10 epochs for demonstration\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_losses, train_accs = [], []\n",
    "test_losses, test_accs = [], []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, trainloader, criterion, optimizer, device)\n",
    "    test_loss, test_acc = evaluate(model, testloader, criterion, device)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}/{NUM_EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}% | \"\n",
    "          f\"Time: {epoch_time:.1f}s | LR: {scheduler.get_last_lr()[0]:.4f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Final Test Accuracy: {test_accs[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(train_losses, label='Train')\n",
    "axes[0].plot(test_losses, label='Test')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Test Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(train_accs, label='Train')\n",
    "axes[1].plot(test_accs, label='Test')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training and Test Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting to call super().__init__()\n",
    "\n",
    "```python\n",
    "# âŒ Wrong - module won't work properly\n",
    "class BadModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.linear = nn.Linear(10, 10)\n",
    "\n",
    "# âœ… Right - always call super first\n",
    "class GoodModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # ESSENTIAL!\n",
    "        self.linear = nn.Linear(10, 10)\n",
    "```\n",
    "\n",
    "**Why:** Without `super().__init__()`, PyTorch can't track your parameters!\n",
    "\n",
    "### Mistake 2: Not matching skip connection dimensions\n",
    "\n",
    "```python\n",
    "# âŒ Wrong - dimensions don't match, will crash\n",
    "class BrokenBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 3, stride, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x) + x  # ERROR: x is wrong size!\n",
    "\n",
    "# âœ… Right - use shortcut to match dimensions\n",
    "class FixedBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 3, stride, 1)\n",
    "        self.shortcut = nn.Conv2d(in_ch, out_ch, 1, stride)  # Match dimensions!\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x) + self.shortcut(x)\n",
    "```\n",
    "\n",
    "### Mistake 3: Using bias with BatchNorm\n",
    "\n",
    "```python\n",
    "# âŒ Wasteful - redundant bias\n",
    "self.conv = nn.Conv2d(64, 64, 3, padding=1, bias=True)\n",
    "self.bn = nn.BatchNorm2d(64)\n",
    "\n",
    "# âœ… Better - BatchNorm has its own bias\n",
    "self.conv = nn.Conv2d(64, 64, 3, padding=1, bias=False)\n",
    "self.bn = nn.BatchNorm2d(64)\n",
    "```\n",
    "\n",
    "**Why:** BatchNorm normalizes and then applies its own learnable bias, making the conv bias redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- âœ… How `nn.Module` tracks parameters and submodules\n",
    "- âœ… The power of skip connections in ResNet\n",
    "- âœ… BasicBlock for shallower networks (ResNet-18/34)\n",
    "- âœ… Bottleneck for deeper networks (ResNet-50+)\n",
    "- âœ… How to train a complete ResNet on CIFAR-10\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "**Implement ResNet with Squeeze-and-Excitation (SE) blocks!**\n",
    "\n",
    "SE blocks add channel attention to ResNet, improving accuracy by ~1%. The idea:\n",
    "\n",
    "1. Global average pool to get channel-wise statistics\n",
    "2. Two FC layers to learn channel importance\n",
    "3. Multiply original features by learned weights\n",
    "\n",
    "```\n",
    "x -> [BasicBlock] -> feature -> GlobalAvgPool -> FC1 -> ReLU -> FC2 -> Sigmoid -> weights\n",
    "                         |                                                          |\n",
    "                         +-------------------------- Ã— ----------------------------+\n",
    "                                                     |\n",
    "                                                   output\n",
    "```\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ’¡ Hint</summary>\n",
    "\n",
    "```python\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.shape\n",
    "        # Global average pool\n",
    "        y = x.view(b, c, -1).mean(-1)\n",
    "        # Channel attention\n",
    "        y = F.relu(self.fc1(y))\n",
    "        y = torch.sigmoid(self.fc2(y)).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "```\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Deep Residual Learning (Original ResNet Paper)](https://arxiv.org/abs/1512.03385)\n",
    "- [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027)\n",
    "- [PyTorch nn.Module Documentation](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)\n",
    "- [Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: Free GPU memory\n",
    "import gc\n",
    "\n",
    "del model, optimizer, scheduler\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"GPU Memory after cleanup: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}