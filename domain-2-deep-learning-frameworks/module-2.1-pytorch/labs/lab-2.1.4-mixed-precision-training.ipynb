{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.1.4: Mixed Precision Training - Speed Up with AMP\n",
    "\n",
    "**Module:** 2.1 - Deep Learning with PyTorch  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê (Intermediate-Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand the difference between FP32, FP16, and BF16 precision\n",
    "- [ ] Implement Automatic Mixed Precision (AMP) training\n",
    "- [ ] Compare memory usage between precision modes\n",
    "- [ ] Measure training speedup from mixed precision\n",
    "- [ ] Handle gradient scaling to prevent underflow\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Tasks 6.1-6.3\n",
    "- Knowledge of: Neural network training, floating-point representation\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "Training large models is expensive! Mixed precision training can:\n",
    "- **2-3x faster training** on modern GPUs\n",
    "- **50% less memory** for activations and gradients\n",
    "- **Same accuracy** when done correctly\n",
    "\n",
    "All major AI labs use mixed precision:\n",
    "- **OpenAI**: GPT models trained with FP16/BF16\n",
    "- **Google**: BERT, T5 trained with BFloat16\n",
    "- **Meta**: LLaMA models use mixed precision throughout\n",
    "\n",
    "Your DGX Spark's Blackwell GPU has specialized Tensor Cores that make mixed precision even faster!\n",
    "\n",
    "---\n",
    "\n",
    "## ELI5: What is Mixed Precision?\n",
    "\n",
    "> **Imagine you're doing math homework...** üìù\n",
    ">\n",
    "> For most problems, you round to 2 decimal places (3.14). That's fast and usually good enough.\n",
    ">\n",
    "> But sometimes you need more precision (3.14159265...) for the final answer to be correct.\n",
    ">\n",
    "> **Mixed precision is exactly this:**\n",
    "> - Do most calculations with \"rough\" numbers (FP16/BF16) - it's faster!\n",
    "> - Keep important things (like weight updates) in \"precise\" numbers (FP32)\n",
    "> - Get the speed of rough math with the accuracy of precise math!\n",
    ">\n",
    "> **In AI terms:**\n",
    "> - FP32 (32-bit): High precision, more memory, slower\n",
    "> - FP16 (16-bit): Lower precision, less memory, faster (but can overflow!)\n",
    "> - BF16 (16-bit): Same range as FP32, less precision, fast and stable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding Floating-Point Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.amp import autocast, GradScaler  # Updated import for PyTorch 2.0+\nimport torchvision\nimport torchvision.transforms as transforms\n\nimport time\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom typing import Tuple, Dict, List\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n    # Check for Tensor Core support\n    major, minor = torch.cuda.get_device_capability()\n    print(f\"Compute Capability: {major}.{minor}\")\n    print(f\"Tensor Cores: {'Yes' if major >= 7 else 'No'}\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare floating-point formats\n",
    "print(\"=== Floating-Point Format Comparison ===\")\n",
    "print()\n",
    "\n",
    "formats = {\n",
    "    'float32': torch.float32,\n",
    "    'float16': torch.float16,\n",
    "    'bfloat16': torch.bfloat16,\n",
    "}\n",
    "\n",
    "for name, dtype in formats.items():\n",
    "    info = torch.finfo(dtype)\n",
    "    print(f\"{name:10s}: bits={info.bits:2d}, \"\n",
    "          f\"range=[{info.min:.2e}, {info.max:.2e}], \"\n",
    "          f\"tiny={info.tiny:.2e}, eps={info.eps:.2e}\")\n",
    "\n",
    "print(\"\\n=== Memory Usage ===\")\n",
    "x = torch.randn(1000, 1000)\n",
    "for name, dtype in formats.items():\n",
    "    tensor = x.to(dtype)\n",
    "    size_mb = tensor.element_size() * tensor.nelement() / 1e6\n",
    "    print(f\"{name:10s}: {size_mb:.2f} MB for 1M elements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Differences:\n",
    "\n",
    "| Format | Bits | Exponent | Mantissa | Range | Use Case |\n",
    "|--------|------|----------|----------|-------|----------|\n",
    "| FP32 | 32 | 8 | 23 | ¬±3.4e38 | Default, master weights |\n",
    "| FP16 | 16 | 5 | 10 | ¬±65504 | Fast math, but overflow risk |\n",
    "| BF16 | 16 | 8 | 7 | ¬±3.4e38 | Same range as FP32, less precision |\n",
    "\n",
    "**BF16 is preferred for training** because it has the same range as FP32, avoiding overflow issues!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate precision differences\n",
    "print(\"=== Precision Demonstration ===\")\n",
    "\n",
    "# A number that's fine in FP32 but problematic in FP16\n",
    "large_value = torch.tensor(70000.0)\n",
    "print(f\"\\nOriginal (FP32): {large_value}\")\n",
    "print(f\"As FP16: {large_value.half()} (overflow to inf!)\")\n",
    "print(f\"As BF16: {large_value.bfloat16()} (works fine)\")\n",
    "\n",
    "# Small gradients that might underflow\n",
    "small_value = torch.tensor(1e-6)\n",
    "print(f\"\\nSmall value (FP32): {small_value}\")\n",
    "print(f\"As FP16: {small_value.half()} (loses precision)\")\n",
    "print(f\"As BF16: {small_value.bfloat16()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Setting Up the Experiment\n",
    "\n",
    "Let's create a model and dataset to compare FP32 vs mixed precision training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple ResNet-like model for benchmarking\n",
    "class SimpleResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified ResNet for CIFAR-10.\n",
    "    \n",
    "    Large enough to benefit from mixed precision,\n",
    "    small enough to train quickly.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int = 10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Layer 1: 64 channels\n",
    "        self.layer1 = self._make_layer(64, 64, 2)\n",
    "        # Layer 2: 128 channels, downsample\n",
    "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
    "        # Layer 3: 256 channels, downsample\n",
    "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
    "        # Layer 4: 512 channels, downsample\n",
    "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def _make_layer(self, in_ch, out_ch, num_blocks, stride=1):\n",
    "        layers = []\n",
    "        # First block may downsample\n",
    "        layers.append(self._block(in_ch, out_ch, stride))\n",
    "        # Remaining blocks\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(self._block(out_ch, out_ch, 1))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _block(self, in_ch, out_ch, stride):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, stride, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Test the model\n",
    "model = SimpleResNet(10)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "x = torch.randn(2, 3, 32, 32)\n",
    "y = model(x)\n",
    "print(f\"Input: {x.shape} -> Output: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load CIFAR-10\ntransform = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntrainset = torchvision.datasets.CIFAR10(\n    root='./data', train=True, download=True, transform=transform\n)\ntestset = torchvision.datasets.CIFAR10(\n    root='./data', train=False, download=True, transform=transform_test\n)\n\n# Adaptive batch size based on available GPU memory (M4 fix)\nif torch.cuda.is_available():\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    if gpu_mem > 100:  # DGX Spark with 128GB\n        BATCH_SIZE = 256\n    elif gpu_mem > 16:\n        BATCH_SIZE = 128\n    else:\n        BATCH_SIZE = 64\nelse:\n    BATCH_SIZE = 64\n\ntrainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, \n                         num_workers=4, pin_memory=True)\ntestloader = DataLoader(testset, batch_size=BATCH_SIZE * 2, shuffle=False,\n                        num_workers=4, pin_memory=True)\n\nprint(f\"Training samples: {len(trainset):,}\")\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"Batches per epoch: {len(trainloader)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Baseline FP32 Training\n",
    "\n",
    "First, let's establish a baseline with standard FP32 training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_fp32(model, trainloader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Standard FP32 training for one epoch.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (avg_loss, accuracy, time_seconds)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for inputs, labels in trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    return running_loss / len(trainloader), 100. * correct / total, epoch_time\n",
    "\n",
    "\n",
    "def evaluate(model, testloader, criterion, device):\n",
    "    \"\"\"Evaluate model accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(testloader), 100. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train FP32 baseline\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "model_fp32 = SimpleResNet(10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_fp32 = optim.SGD(model_fp32.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Track memory\n",
    "torch.cuda.reset_peak_memory_stats() if torch.cuda.is_available() else None\n",
    "\n",
    "print(\"=== FP32 Training ===\")\n",
    "fp32_results = {'loss': [], 'acc': [], 'time': []}\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loss, acc, epoch_time = train_epoch_fp32(\n",
    "        model_fp32, trainloader, criterion, optimizer_fp32, device\n",
    "    )\n",
    "    test_loss, test_acc = evaluate(model_fp32, testloader, criterion, device)\n",
    "    \n",
    "    fp32_results['loss'].append(loss)\n",
    "    fp32_results['acc'].append(test_acc)\n",
    "    fp32_results['time'].append(epoch_time)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "          f\"Loss: {loss:.4f} | Train Acc: {acc:.2f}% | \"\n",
    "          f\"Test Acc: {test_acc:.2f}% | Time: {epoch_time:.1f}s\")\n",
    "\n",
    "fp32_memory = torch.cuda.max_memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "fp32_total_time = sum(fp32_results['time'])\n",
    "\n",
    "print(f\"\\nFP32 Peak Memory: {fp32_memory:.2f} GB\")\n",
    "print(f\"FP32 Total Time: {fp32_total_time:.1f}s\")\n",
    "print(f\"FP32 Final Accuracy: {fp32_results['acc'][-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Mixed Precision Training with AMP\n",
    "\n",
    "PyTorch's Automatic Mixed Precision (AMP) makes this easy!\n",
    "\n",
    "### Key Components:\n",
    "1. **`autocast`**: Automatically chooses precision for each operation\n",
    "2. **`GradScaler`**: Scales gradients to prevent underflow in FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_epoch_amp(model, trainloader, criterion, optimizer, scaler, device, dtype=torch.float16):\n    \"\"\"\n    Mixed precision training for one epoch using AMP.\n    \n    Args:\n        model: Neural network\n        trainloader: DataLoader\n        criterion: Loss function\n        optimizer: Optimizer\n        scaler: GradScaler for gradient scaling\n        device: Device to train on\n        dtype: Precision for autocast (float16 or bfloat16)\n    \n    Returns:\n        Tuple of (avg_loss, accuracy, time_seconds)\n    \"\"\"\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    start_time = time.time()\n    \n    for inputs, labels in trainloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        # Forward pass with autocast (PyTorch 2.0+ API with device_type)\n        with autocast(device_type='cuda', dtype=dtype):\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n        \n        # Backward pass with gradient scaling\n        scaler.scale(loss).backward()\n        \n        # Optimizer step with unscaling\n        scaler.step(optimizer)\n        scaler.update()\n        \n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n    \n    epoch_time = time.time() - start_time\n    \n    return running_loss / len(trainloader), 100. * correct / total, epoch_time"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train with FP16 mixed precision\ntorch.cuda.reset_peak_memory_stats() if torch.cuda.is_available() else None\n\nmodel_fp16 = SimpleResNet(10).to(device)\noptimizer_fp16 = optim.SGD(model_fp16.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n# FP16 requires gradient scaling to prevent underflow\nscaler_fp16 = GradScaler('cuda')\n\nprint(\"=== FP16 Mixed Precision Training ===\")\nfp16_results = {'loss': [], 'acc': [], 'time': []}\n\nfor epoch in range(NUM_EPOCHS):\n    loss, acc, epoch_time = train_epoch_amp(\n        model_fp16, trainloader, criterion, optimizer_fp16, scaler_fp16, device,\n        dtype=torch.float16\n    )\n    test_loss, test_acc = evaluate(model_fp16, testloader, criterion, device)\n    \n    fp16_results['loss'].append(loss)\n    fp16_results['acc'].append(test_acc)\n    fp16_results['time'].append(epoch_time)\n    \n    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n          f\"Loss: {loss:.4f} | Train Acc: {acc:.2f}% | \"\n          f\"Test Acc: {test_acc:.2f}% | Time: {epoch_time:.1f}s\")\n\nfp16_memory = torch.cuda.max_memory_allocated() / 1e9 if torch.cuda.is_available() else 0\nfp16_total_time = sum(fp16_results['time'])\n\nprint(f\"\\nFP16 Peak Memory: {fp16_memory:.2f} GB\")\nprint(f\"FP16 Total Time: {fp16_total_time:.1f}s\")\nprint(f\"FP16 Final Accuracy: {fp16_results['acc'][-1]:.2f}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train with BF16 mixed precision (preferred on modern hardware)\ntorch.cuda.reset_peak_memory_stats() if torch.cuda.is_available() else None\n\nmodel_bf16 = SimpleResNet(10).to(device)\noptimizer_bf16 = optim.SGD(model_bf16.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n# BF16 typically doesn't need gradient scaling due to larger dynamic range\nscaler_bf16 = GradScaler('cuda', enabled=False)  # Disable for BF16\n\nprint(\"=== BF16 Mixed Precision Training ===\")\nbf16_results = {'loss': [], 'acc': [], 'time': []}\n\nfor epoch in range(NUM_EPOCHS):\n    loss, acc, epoch_time = train_epoch_amp(\n        model_bf16, trainloader, criterion, optimizer_bf16, scaler_bf16, device,\n        dtype=torch.bfloat16\n    )\n    test_loss, test_acc = evaluate(model_bf16, testloader, criterion, device)\n    \n    bf16_results['loss'].append(loss)\n    bf16_results['acc'].append(test_acc)\n    bf16_results['time'].append(epoch_time)\n    \n    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n          f\"Loss: {loss:.4f} | Train Acc: {acc:.2f}% | \"\n          f\"Test Acc: {test_acc:.2f}% | Time: {epoch_time:.1f}s\")\n\nbf16_memory = torch.cuda.max_memory_allocated() / 1e9 if torch.cuda.is_available() else 0\nbf16_total_time = sum(bf16_results['time'])\n\nprint(f\"\\nBF16 Peak Memory: {bf16_memory:.2f} GB\")\nprint(f\"BF16 Total Time: {bf16_total_time:.1f}s\")\nprint(f\"BF16 Final Accuracy: {bf16_results['acc'][-1]:.2f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "print(\"=\" * 70)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<25} {'FP32':>12} {'FP16':>12} {'BF16':>12}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Peak Memory (GB)':<25} {fp32_memory:>12.2f} {fp16_memory:>12.2f} {bf16_memory:>12.2f}\")\n",
    "print(f\"{'Training Time (s)':<25} {fp32_total_time:>12.1f} {fp16_total_time:>12.1f} {bf16_total_time:>12.1f}\")\n",
    "print(f\"{'Final Test Accuracy (%)':<25} {fp32_results['acc'][-1]:>12.2f} {fp16_results['acc'][-1]:>12.2f} {bf16_results['acc'][-1]:>12.2f}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Memory Savings vs FP32':<25} {'-':>12} {(1-fp16_memory/fp32_memory)*100:>11.1f}% {(1-bf16_memory/fp32_memory)*100:>11.1f}%\")\n",
    "print(f\"{'Speedup vs FP32':<25} {'-':>12} {fp32_total_time/fp16_total_time:>11.2f}x {fp32_total_time/bf16_total_time:>11.2f}x\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "epochs = range(1, NUM_EPOCHS + 1)\n",
    "\n",
    "# Loss comparison\n",
    "ax1 = axes[0]\n",
    "ax1.plot(epochs, fp32_results['loss'], 'b-o', label='FP32')\n",
    "ax1.plot(epochs, fp16_results['loss'], 'r-s', label='FP16')\n",
    "ax1.plot(epochs, bf16_results['loss'], 'g-^', label='BF16')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Training Loss')\n",
    "ax1.set_title('Training Loss Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy comparison\n",
    "ax2 = axes[1]\n",
    "ax2.plot(epochs, fp32_results['acc'], 'b-o', label='FP32')\n",
    "ax2.plot(epochs, fp16_results['acc'], 'r-s', label='FP16')\n",
    "ax2.plot(epochs, bf16_results['acc'], 'g-^', label='BF16')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Test Accuracy (%)')\n",
    "ax2.set_title('Test Accuracy Comparison')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "# Time per epoch\n",
    "ax3 = axes[2]\n",
    "ax3.bar(['FP32', 'FP16', 'BF16'], \n",
    "        [np.mean(fp32_results['time']), np.mean(fp16_results['time']), np.mean(bf16_results['time'])],\n",
    "        color=['blue', 'red', 'green'])\n",
    "ax3.set_ylabel('Time per Epoch (s)')\n",
    "ax3.set_title('Training Speed Comparison')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Understanding GradScaler\n",
    "\n",
    "The `GradScaler` is crucial for FP16 training. Let's understand why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate gradient underflow problem\n",
    "print(\"=== Gradient Underflow Demonstration ===\")\n",
    "\n",
    "# Simulate small gradients (common in deep networks)\n",
    "small_grad = torch.tensor(1e-5)\n",
    "print(f\"Original gradient (FP32): {small_grad}\")\n",
    "print(f\"In FP16: {small_grad.half()}\")\n",
    "\n",
    "# After several multiplications (backprop through many layers)\n",
    "tiny_grad = small_grad ** 3\n",
    "print(f\"\\nAfter multiplications (FP32): {tiny_grad}\")\n",
    "print(f\"In FP16: {tiny_grad.half()} (underflow to 0!)\")\n",
    "\n",
    "# Solution: Scale up before operations\n",
    "scale = 65536.0  # 2^16\n",
    "scaled_grad = small_grad * scale\n",
    "scaled_result = (scaled_grad.half() ** 3) / (scale ** 3)\n",
    "print(f\"\\nWith scaling: {scaled_result} (preserved!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How GradScaler works internally\n",
    "print(\"=== GradScaler Internal Mechanics ===\")\n",
    "\n",
    "scaler = GradScaler(init_scale=65536.0, growth_interval=2000)\n",
    "\n",
    "print(f\"Initial scale: {scaler.get_scale()}\")\n",
    "print(f\"Growth factor: {scaler._growth_factor}\")\n",
    "print(f\"Backoff factor: {scaler._backoff_factor}\")\n",
    "\n",
    "# The scaler:\n",
    "# 1. Multiplies loss by scale before backward()\n",
    "# 2. Divides gradients by scale before optimizer.step()\n",
    "# 3. Increases scale if no inf/nan for growth_interval steps\n",
    "# 4. Decreases scale if inf/nan detected (and skips that step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself: Exercise\n",
    "\n",
    "Implement a training loop that **automatically falls back to FP32** if too many inf/nan gradients are detected.\n",
    "\n",
    "**Requirements:**\n",
    "1. Track the number of skipped steps (inf/nan)\n",
    "2. If >10% of steps are skipped, switch to FP32\n",
    "3. Log when the fallback occurs\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "Use `scaler.get_scale()` before and after `scaler.update()` - if the scale decreased, a step was skipped due to inf/nan.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Implement adaptive precision training\n",
    "def train_with_fallback(model, trainloader, criterion, optimizer, device, max_skip_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Train with automatic fallback from FP16 to FP32 if unstable.\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "# train_with_fallback(model, trainloader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Common Mistakes\n\n### Mistake 1: Using autocast in the wrong scope\n\n```python\n# ‚ùå Wrong - autocast doesn't cover backward\nwith autocast(device_type='cuda', dtype=torch.float16):\n    output = model(input)\n    loss = criterion(output, target)\n    loss.backward()  # This should be OUTSIDE autocast!\n\n# ‚úÖ Right - backward outside autocast\nwith autocast(device_type='cuda', dtype=torch.float16):\n    output = model(input)\n    loss = criterion(output, target)\nscaler.scale(loss).backward()  # Outside autocast\n```\n\n### Mistake 2: Forgetting to use scaler for FP16\n\n```python\n# ‚ùå Wrong - gradients may underflow\nwith autocast(device_type='cuda', dtype=torch.float16):\n    output = model(input)\n    loss = criterion(output, target)\nloss.backward()\noptimizer.step()\n\n# ‚úÖ Right - use GradScaler\nscaler = GradScaler()\nwith autocast(device_type='cuda', dtype=torch.float16):\n    output = model(input)\n    loss = criterion(output, target)\nscaler.scale(loss).backward()\nscaler.step(optimizer)\nscaler.update()\n```\n\n### Mistake 3: Not disabling GradScaler for BF16\n\n```python\n# ‚ùå Unnecessary - BF16 has same range as FP32\nscaler = GradScaler()  # Enabled by default\n\n# ‚úÖ Better - disable for BF16\nscaler = GradScaler(enabled=False)  # BF16 doesn't need scaling\n```\n\n### Mistake 4: Using deprecated import path (PyTorch 2.0+)\n\n```python\n# ‚ùå Deprecated - will show warning\nfrom torch.cuda.amp import autocast, GradScaler\n\n# ‚úÖ Correct - PyTorch 2.0+ path\nfrom torch.amp import autocast, GradScaler\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Checkpoint\n\nYou've learned:\n- ‚úÖ The differences between FP32, FP16, and BF16\n- ‚úÖ How to use `autocast` and `GradScaler`\n- ‚úÖ Memory and speed benefits of mixed precision\n- ‚úÖ Why gradient scaling is needed for FP16\n- ‚úÖ BF16 is often the best choice (same range as FP32, no scaling needed)\n\n---\n\n## DGX Spark Recommendations\n\nOn your DGX Spark with Blackwell GPU:\n\n1. **Use BF16 by default** - Blackwell has excellent BF16 support\n2. **Try NVFP4 for inference** - Blackwell supports native 4-bit compute!\n3. **Increase batch size** - With 128GB unified memory, you can go much larger\n4. **Monitor memory** - Use the saved memory for larger models\n\n---\n\n## Further Reading\n\n- [PyTorch AMP Documentation](https://pytorch.org/docs/stable/amp.html)\n- [Mixed Precision Training (NVIDIA)](https://developer.nvidia.com/automatic-mixed-precision)\n- [BFloat16 Paper](https://cloud.google.com/tpu/docs/bfloat16)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "\n",
    "del model_fp32, model_fp16, model_bf16\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"GPU Memory after cleanup: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}