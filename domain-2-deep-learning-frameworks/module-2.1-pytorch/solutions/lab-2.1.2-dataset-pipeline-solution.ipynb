{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.1.2: Dataset Pipeline - SOLUTIONS\n",
    "\n",
    "This notebook contains complete solutions for the Dataset Pipeline exercises.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise Solution: MixupDataset\n",
    "\n",
    "Mixup is a data augmentation technique that creates new training samples by linearly interpolating between two random samples. This encourages the model to behave linearly between training examples, improving generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset wrapper that applies Mixup augmentation.\n",
    "    \n",
    "    Mixup creates new training samples by combining two images:\n",
    "        mixed_image = lambda * image1 + (1 - lambda) * image2\n",
    "        \n",
    "    The label is returned as a tuple (label1, label2, lambda) so the loss\n",
    "    function can compute the weighted combination.\n",
    "    \n",
    "    Paper: \"mixup: Beyond Empirical Risk Minimization\" (Zhang et al., 2018)\n",
    "    \n",
    "    Args:\n",
    "        dataset: Base dataset to wrap\n",
    "        alpha: Mixup alpha parameter for Beta distribution (default: 0.2)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset: Dataset, alpha: float = 0.2):\n",
    "        self.dataset = dataset\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        # Get first sample\n",
    "        img1, label1 = self.dataset[idx]\n",
    "        \n",
    "        # Get random second sample\n",
    "        idx2 = np.random.randint(len(self.dataset))\n",
    "        img2, label2 = self.dataset[idx2]\n",
    "        \n",
    "        # Sample lambda from Beta(alpha, alpha) distribution\n",
    "        # When alpha is small (e.g., 0.2), lambda is usually close to 0 or 1\n",
    "        # When alpha is large (e.g., 1.0), lambda is more uniform\n",
    "        if self.alpha > 0:\n",
    "            lam = np.random.beta(self.alpha, self.alpha)\n",
    "        else:\n",
    "            lam = 1.0  # No mixup when alpha = 0\n",
    "        \n",
    "        # Mix images\n",
    "        mixed_img = lam * img1 + (1 - lam) * img2\n",
    "        \n",
    "        return mixed_img, (label1, label2, lam)\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, labels):\n",
    "    \"\"\"\n",
    "    Compute mixed loss for mixup training.\n",
    "    \n",
    "    Args:\n",
    "        criterion: Base loss function (e.g., CrossEntropyLoss)\n",
    "        pred: Model predictions\n",
    "        labels: Tuple of (label1, label2, lambda)\n",
    "        \n",
    "    Returns:\n",
    "        Weighted loss\n",
    "    \"\"\"\n",
    "    label1, label2, lam = labels\n",
    "    return lam * criterion(pred, label1) + (1 - lam) * criterion(pred, label2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple test dataset\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, num_samples=100, num_classes=10):\n",
    "        self.num_samples = num_samples\n",
    "        self.num_classes = num_classes\n",
    "        # Pre-generate random images and labels\n",
    "        self.images = torch.randn(num_samples, 3, 32, 32)\n",
    "        self.labels = torch.randint(0, num_classes, (num_samples,))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx].item()\n",
    "\n",
    "# Test MixupDataset\n",
    "base_dataset = SimpleDataset(num_samples=100)\n",
    "mixup_dataset = MixupDataset(base_dataset, alpha=0.2)\n",
    "\n",
    "print(\"=== Testing MixupDataset ===\")\n",
    "\n",
    "# Get a sample\n",
    "mixed_img, (label1, label2, lam) = mixup_dataset[0]\n",
    "print(f\"Mixed image shape: {mixed_img.shape}\")\n",
    "print(f\"Labels: {label1}, {label2}\")\n",
    "print(f\"Lambda: {lam:.3f}\")\n",
    "\n",
    "# Test with DataLoader\n",
    "loader = DataLoader(mixup_dataset, batch_size=4, shuffle=True)\n",
    "batch_img, (batch_l1, batch_l2, batch_lam) = next(iter(loader))\n",
    "print(f\"\\nBatch images: {batch_img.shape}\")\n",
    "print(f\"Batch labels1: {batch_l1}\")\n",
    "print(f\"Batch labels2: {batch_l2}\")\n",
    "print(f\"Batch lambdas: {batch_lam}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Alternative: Batch Mixup\n",
    "\n",
    "A more efficient approach is to apply mixup at the batch level rather than in the dataset. This is cleaner and allows you to use standard datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"\n",
    "    Apply mixup to a batch of data.\n",
    "    \n",
    "    This is the more common way to implement mixup - at training time\n",
    "    rather than in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        x: Input images (B, C, H, W)\n",
    "        y: Labels (B,)\n",
    "        alpha: Mixup alpha parameter\n",
    "        \n",
    "    Returns:\n",
    "        mixed_x: Mixed images\n",
    "        y_a: First set of labels\n",
    "        y_b: Second set of labels\n",
    "        lam: Mixing coefficient\n",
    "    \"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.0\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "# Example training loop with batch mixup\n",
    "def train_with_mixup(model, train_loader, criterion, optimizer, alpha=0.2):\n",
    "    \"\"\"\n",
    "    Training loop with mixup augmentation.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Apply mixup\n",
    "        mixed_images, labels_a, labels_b, lam = mixup_data(images, labels, alpha)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(mixed_images)\n",
    "        \n",
    "        # Compute mixed loss\n",
    "        loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "print(\"Batch Mixup function defined!\")\n",
    "\n",
    "# Test\n",
    "x = torch.randn(8, 3, 32, 32)\n",
    "y = torch.randint(0, 10, (8,))\n",
    "mixed_x, y_a, y_b, lam = mixup_data(x, y, alpha=0.2)\n",
    "print(f\"Original: {x.shape}, Labels: {y}\")\n",
    "print(f\"Mixed: {mixed_x.shape}, Lambda: {lam:.3f}\")\n",
    "print(f\"Labels A: {y_a}\")\n",
    "print(f\"Labels B: {y_b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Solution: Cutmix\n",
    "\n",
    "Cutmix is similar to Mixup but instead of blending entire images, it cuts a patch from one image and pastes it onto another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    \"\"\"\n",
    "    Generate a random bounding box for Cutmix.\n",
    "    \n",
    "    Args:\n",
    "        size: Image size (B, C, H, W)\n",
    "        lam: Mixing coefficient\n",
    "        \n",
    "    Returns:\n",
    "        Bounding box coordinates (bbx1, bby1, bbx2, bby2)\n",
    "    \"\"\"\n",
    "    W = size[3]\n",
    "    H = size[2]\n",
    "    \n",
    "    # Calculate cut size based on lambda\n",
    "    cut_rat = np.sqrt(1.0 - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "    \n",
    "    # Random center point\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    \n",
    "    # Calculate bounding box with clipping\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    \n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "\n",
    "def cutmix_data(x, y, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Apply Cutmix to a batch of data.\n",
    "    \n",
    "    Args:\n",
    "        x: Input images (B, C, H, W)\n",
    "        y: Labels (B,)\n",
    "        alpha: Cutmix alpha parameter\n",
    "        \n",
    "    Returns:\n",
    "        mixed_x: Mixed images\n",
    "        y_a: First set of labels\n",
    "        y_b: Second set of labels\n",
    "        lam: Actual mixing coefficient (based on box area)\n",
    "    \"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.0\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size)\n",
    "    \n",
    "    y_a = y\n",
    "    y_b = y[index]\n",
    "    \n",
    "    # Get bounding box\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
    "    \n",
    "    # Apply cutmix\n",
    "    mixed_x = x.clone()\n",
    "    mixed_x[:, :, bby1:bby2, bbx1:bbx2] = x[index, :, bby1:bby2, bbx1:bbx2]\n",
    "    \n",
    "    # Adjust lambda based on actual box area\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(2) * x.size(3)))\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "# Test Cutmix\n",
    "print(\"=== Testing Cutmix ===\")\n",
    "x = torch.randn(4, 3, 32, 32)\n",
    "y = torch.randint(0, 10, (4,))\n",
    "mixed_x, y_a, y_b, lam = cutmix_data(x, y, alpha=1.0)\n",
    "print(f\"Mixed images shape: {mixed_x.shape}\")\n",
    "print(f\"Lambda: {lam:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Advanced: Prefetching DataLoader\n",
    "\n",
    "For maximum performance, we can create a DataLoader that prefetches batches in a background thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from queue import Queue\n",
    "\n",
    "class PrefetchLoader:\n",
    "    \"\"\"\n",
    "    DataLoader wrapper that prefetches batches in a background thread.\n",
    "    \n",
    "    This overlaps data loading with GPU computation for better utilization.\n",
    "    \n",
    "    Args:\n",
    "        loader: Base DataLoader to wrap\n",
    "        device: Device to prefetch to\n",
    "        prefetch_count: Number of batches to keep ready\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, loader, device, prefetch_count=2):\n",
    "        self.loader = loader\n",
    "        self.device = device\n",
    "        self.prefetch_count = prefetch_count\n",
    "        self.queue = Queue(maxsize=prefetch_count)\n",
    "        self.stream = torch.cuda.Stream() if device.type == 'cuda' else None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        # Start the prefetching thread\n",
    "        self._prefetch_thread = Thread(\n",
    "            target=self._prefetch_worker,\n",
    "            daemon=True\n",
    "        )\n",
    "        self._prefetch_thread.start()\n",
    "        \n",
    "        for _ in range(len(self.loader)):\n",
    "            batch = self.queue.get()\n",
    "            if batch is None:\n",
    "                break\n",
    "            yield batch\n",
    "    \n",
    "    def _prefetch_worker(self):\n",
    "        \"\"\"Background thread that prefetches batches.\"\"\"\n",
    "        for batch in self.loader:\n",
    "            # Transfer to GPU in stream\n",
    "            if self.stream is not None:\n",
    "                with torch.cuda.stream(self.stream):\n",
    "                    batch = self._to_device(batch)\n",
    "                self.stream.synchronize()\n",
    "            else:\n",
    "                batch = self._to_device(batch)\n",
    "            \n",
    "            self.queue.put(batch)\n",
    "        \n",
    "        self.queue.put(None)  # Signal end\n",
    "    \n",
    "    def _to_device(self, batch):\n",
    "        \"\"\"Move batch to device.\"\"\"\n",
    "        if isinstance(batch, (list, tuple)):\n",
    "            return [self._to_device(x) for x in batch]\n",
    "        elif isinstance(batch, torch.Tensor):\n",
    "            return batch.to(self.device, non_blocking=True)\n",
    "        else:\n",
    "            return batch\n",
    "\n",
    "\n",
    "# Example usage\n",
    "print(\"PrefetchLoader defined!\")\n",
    "print(\"Usage: prefetch_loader = PrefetchLoader(train_loader, device)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Performance Comparison\n",
    "\n",
    "Here's a summary of different augmentation strategies:\n",
    "\n",
    "| Augmentation | Type | Typical Improvement | Overhead |\n",
    "|--------------|------|---------------------|----------|\n",
    "| Standard (flip, crop, jitter) | Spatial/Color | Baseline | Low |\n",
    "| Mixup | Interpolation | +1-2% | Minimal |\n",
    "| Cutmix | Region-based | +1-2% | Minimal |\n",
    "| RandAugment | Automated | +2-3% | Medium |\n",
    "| AutoAugment | Learned | +2-3% | High (training) |\n",
    "\n",
    "**Recommendation for DGX Spark:**\n",
    "- Use standard augmentation + Mixup/Cutmix\n",
    "- Consider RandAugment for larger models\n",
    "- Batch-level augmentation is more efficient than dataset-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}