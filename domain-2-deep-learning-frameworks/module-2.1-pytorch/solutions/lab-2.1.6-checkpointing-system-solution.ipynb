{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.1.6: Checkpointing System - SOLUTIONS\n",
    "\n",
    "This notebook contains complete solutions and extended implementations for the Checkpointing System exercises.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Optional, List\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup: Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    \"\"\"Simple CNN for CIFAR-10.\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 8 * 8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train: {len(trainset)} samples, {len(trainloader)} batches\")\n",
    "print(f\"Test: {len(testset)} samples, {len(testloader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution 1: Complete Checkpoint Functions\n",
    "\n",
    "Production-ready save and load functions with all necessary state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    path: str,\n",
    "    model: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    epoch: int,\n",
    "    train_loss: float,\n",
    "    val_loss: float,\n",
    "    val_accuracy: float,\n",
    "    scheduler: Optional[Any] = None,\n",
    "    best_val_loss: Optional[float] = None,\n",
    "    best_val_accuracy: Optional[float] = None,\n",
    "    config: Optional[Dict] = None,\n",
    "    rng_state: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Save complete training checkpoint with atomic write.\n",
    "    \n",
    "    Args:\n",
    "        path: File path to save checkpoint\n",
    "        model: Model to save\n",
    "        optimizer: Optimizer with state to save\n",
    "        epoch: Current epoch number\n",
    "        train_loss: Training loss this epoch\n",
    "        val_loss: Validation loss this epoch\n",
    "        val_accuracy: Validation accuracy this epoch\n",
    "        scheduler: Learning rate scheduler (optional)\n",
    "        best_val_loss: Best validation loss so far\n",
    "        best_val_accuracy: Best validation accuracy so far\n",
    "        config: Training configuration dict (optional)\n",
    "        rng_state: Whether to save RNG states for exact reproducibility\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        # Model and optimizer state\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        \n",
    "        # Training metrics\n",
    "        'epoch': epoch,\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        \n",
    "        # Best metrics\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'best_val_accuracy': best_val_accuracy,\n",
    "        \n",
    "        # Metadata\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'pytorch_version': torch.__version__,\n",
    "    }\n",
    "    \n",
    "    # Scheduler state\n",
    "    if scheduler is not None:\n",
    "        checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "    \n",
    "    # Configuration\n",
    "    if config is not None:\n",
    "        checkpoint['config'] = config\n",
    "    \n",
    "    # RNG states for exact reproducibility\n",
    "    if rng_state:\n",
    "        checkpoint['rng_state'] = {\n",
    "            'python': None,  # random.getstate() if needed\n",
    "            'numpy': None,   # np.random.get_state() if needed\n",
    "            'torch': torch.get_rng_state(),\n",
    "            'cuda': torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None,\n",
    "        }\n",
    "    \n",
    "    # Atomic save: write to temp file, then rename\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    temp_path = path.with_suffix('.tmp')\n",
    "    \n",
    "    torch.save(checkpoint, temp_path)\n",
    "    temp_path.rename(path)\n",
    "    \n",
    "    return path\n",
    "\n",
    "\n",
    "def load_checkpoint(\n",
    "    path: str,\n",
    "    model: nn.Module,\n",
    "    optimizer: Optional[optim.Optimizer] = None,\n",
    "    scheduler: Optional[Any] = None,\n",
    "    device: Optional[torch.device] = None,\n",
    "    strict: bool = True,\n",
    "    restore_rng: bool = False,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load training checkpoint and restore state.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to checkpoint file\n",
    "        model: Model to load weights into\n",
    "        optimizer: Optimizer to restore state (optional)\n",
    "        scheduler: Scheduler to restore state (optional)\n",
    "        device: Device to map tensors to\n",
    "        strict: Whether to strictly enforce state_dict keys match\n",
    "        restore_rng: Whether to restore RNG states\n",
    "        \n",
    "    Returns:\n",
    "        Checkpoint dictionary with all saved info\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    \n",
    "    # Restore model\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=strict)\n",
    "    \n",
    "    # Restore optimizer\n",
    "    if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        # Move optimizer state to correct device\n",
    "        if device is not None:\n",
    "            for state in optimizer.state.values():\n",
    "                for k, v in state.items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        state[k] = v.to(device)\n",
    "    \n",
    "    # Restore scheduler\n",
    "    if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    # Restore RNG states\n",
    "    if restore_rng and 'rng_state' in checkpoint:\n",
    "        rng = checkpoint['rng_state']\n",
    "        if rng.get('torch') is not None:\n",
    "            torch.set_rng_state(rng['torch'])\n",
    "        if rng.get('cuda') is not None and torch.cuda.is_available():\n",
    "            torch.cuda.set_rng_state_all(rng['cuda'])\n",
    "    \n",
    "    print(f\"Loaded checkpoint from epoch {checkpoint['epoch']}\")\n",
    "    print(f\"  Val Loss: {checkpoint['val_loss']:.4f}\")\n",
    "    print(f\"  Val Accuracy: {checkpoint['val_accuracy']:.2f}%\")\n",
    "    \n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test save/load functions\n",
    "checkpoint_dir = Path('./checkpoints_solution')\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model = SimpleNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Simulate training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for epoch in range(3):\n",
    "    for i, (inputs, labels) in enumerate(trainloader):\n",
    "        if i >= 10:\n",
    "            break\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(inputs), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "# Save\n",
    "save_checkpoint(\n",
    "    path=checkpoint_dir / 'test_checkpoint.pth',\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epoch=3,\n",
    "    train_loss=0.5,\n",
    "    val_loss=0.6,\n",
    "    val_accuracy=75.5,\n",
    "    scheduler=scheduler,\n",
    "    best_val_loss=0.55,\n",
    "    best_val_accuracy=77.0,\n",
    "    config={'lr': 0.001, 'batch_size': 64},\n",
    ")\n",
    "print(\"Checkpoint saved!\")\n",
    "\n",
    "# Load into fresh model\n",
    "new_model = SimpleNet().to(device)\n",
    "new_optimizer = optim.Adam(new_model.parameters(), lr=0.001)\n",
    "new_scheduler = optim.lr_scheduler.StepLR(new_optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "ckpt = load_checkpoint(\n",
    "    checkpoint_dir / 'test_checkpoint.pth',\n",
    "    new_model,\n",
    "    new_optimizer,\n",
    "    new_scheduler,\n",
    "    device,\n",
    ")\n",
    "\n",
    "print(f\"\\nConfig: {ckpt.get('config')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution 2: Enhanced Checkpoint Manager\n",
    "\n",
    "Complete implementation with all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CheckpointConfig:\n",
    "    \"\"\"Configuration for checkpoint manager.\"\"\"\n",
    "    checkpoint_dir: str\n",
    "    mode: str = 'min'  # 'min' for loss, 'max' for accuracy\n",
    "    patience: int = 10\n",
    "    min_delta: float = 0.0  # Minimum improvement to be considered better\n",
    "    max_checkpoints: int = 5\n",
    "    save_every: int = 1  # Save every N epochs\n",
    "    verbose: bool = True\n",
    "\n",
    "\n",
    "class EnhancedCheckpointManager:\n",
    "    \"\"\"\n",
    "    Enhanced checkpoint manager with comprehensive features.\n",
    "    \n",
    "    Features:\n",
    "    - Best model tracking (by loss or accuracy)\n",
    "    - Early stopping with configurable patience and min_delta\n",
    "    - Checkpoint rotation (keep last N checkpoints)\n",
    "    - Training history persistence\n",
    "    - Resume capability\n",
    "    - Atomic saves to prevent corruption\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: CheckpointConfig):\n",
    "        self.config = config\n",
    "        self.checkpoint_dir = Path(config.checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # State\n",
    "        self.best_metric = float('inf') if config.mode == 'min' else float('-inf')\n",
    "        self.best_epoch = 0\n",
    "        self.patience_counter = 0\n",
    "        self.history: List[Dict] = []\n",
    "        self.checkpoint_files: List[Path] = []\n",
    "        \n",
    "        # Load existing state\n",
    "        self._load_state()\n",
    "    \n",
    "    def _load_state(self):\n",
    "        \"\"\"Load existing training state if resuming.\"\"\"\n",
    "        state_path = self.checkpoint_dir / 'manager_state.json'\n",
    "        if state_path.exists():\n",
    "            with open(state_path, 'r') as f:\n",
    "                state = json.load(f)\n",
    "            self.best_metric = state['best_metric']\n",
    "            self.best_epoch = state['best_epoch']\n",
    "            self.patience_counter = state['patience_counter']\n",
    "            self.history = state.get('history', [])\n",
    "            self.checkpoint_files = [Path(p) for p in state.get('checkpoint_files', [])]\n",
    "            if self.config.verbose:\n",
    "                print(f\"Resumed checkpoint manager: best={self.best_metric:.4f} at epoch {self.best_epoch}\")\n",
    "    \n",
    "    def _save_state(self):\n",
    "        \"\"\"Save manager state.\"\"\"\n",
    "        state = {\n",
    "            'best_metric': self.best_metric,\n",
    "            'best_epoch': self.best_epoch,\n",
    "            'patience_counter': self.patience_counter,\n",
    "            'history': self.history,\n",
    "            'checkpoint_files': [str(p) for p in self.checkpoint_files],\n",
    "        }\n",
    "        state_path = self.checkpoint_dir / 'manager_state.json'\n",
    "        with open(state_path, 'w') as f:\n",
    "            json.dump(state, f, indent=2)\n",
    "    \n",
    "    def _is_better(self, metric: float) -> bool:\n",
    "        \"\"\"Check if metric is better than best (with min_delta threshold).\"\"\"\n",
    "        if self.config.mode == 'min':\n",
    "            return metric < (self.best_metric - self.config.min_delta)\n",
    "        return metric > (self.best_metric + self.config.min_delta)\n",
    "    \n",
    "    def _atomic_save(self, path: Path, data: dict):\n",
    "        \"\"\"Save with atomic write to prevent corruption.\"\"\"\n",
    "        temp_path = path.with_suffix('.tmp')\n",
    "        torch.save(data, temp_path)\n",
    "        temp_path.rename(path)\n",
    "    \n",
    "    def step(\n",
    "        self,\n",
    "        epoch: int,\n",
    "        model: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        train_loss: float,\n",
    "        val_loss: float,\n",
    "        val_accuracy: float,\n",
    "        scheduler: Optional[Any] = None,\n",
    "        extra_metrics: Optional[Dict] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process end of epoch: save checkpoints and check early stopping.\n",
    "        \n",
    "        Args:\n",
    "            epoch: Current epoch number\n",
    "            model: Model to checkpoint\n",
    "            optimizer: Optimizer to checkpoint\n",
    "            train_loss: Training loss this epoch\n",
    "            val_loss: Validation loss this epoch\n",
    "            val_accuracy: Validation accuracy this epoch\n",
    "            scheduler: Learning rate scheduler (optional)\n",
    "            extra_metrics: Additional metrics to log (optional)\n",
    "            \n",
    "        Returns:\n",
    "            Dict with status: is_best, should_stop, etc.\n",
    "        \"\"\"\n",
    "        # Determine tracked metric\n",
    "        tracked_metric = val_loss if self.config.mode == 'min' else val_accuracy\n",
    "        is_best = self._is_better(tracked_metric)\n",
    "        \n",
    "        # Build checkpoint data\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'best_metric': self.best_metric,\n",
    "            'best_epoch': self.best_epoch,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "        }\n",
    "        if scheduler is not None:\n",
    "            checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "        if extra_metrics:\n",
    "            checkpoint['extra_metrics'] = extra_metrics\n",
    "        \n",
    "        # Save latest checkpoint (always)\n",
    "        latest_path = self.checkpoint_dir / 'latest.pth'\n",
    "        self._atomic_save(latest_path, checkpoint)\n",
    "        \n",
    "        # Save periodic checkpoint\n",
    "        if epoch % self.config.save_every == 0:\n",
    "            epoch_path = self.checkpoint_dir / f'epoch_{epoch:04d}.pth'\n",
    "            self._atomic_save(epoch_path, checkpoint)\n",
    "            self.checkpoint_files.append(epoch_path)\n",
    "            \n",
    "            # Remove old checkpoints\n",
    "            while len(self.checkpoint_files) > self.config.max_checkpoints:\n",
    "                old_path = self.checkpoint_files.pop(0)\n",
    "                if old_path.exists():\n",
    "                    old_path.unlink()\n",
    "        \n",
    "        # Handle best model\n",
    "        if is_best:\n",
    "            self.best_metric = tracked_metric\n",
    "            self.best_epoch = epoch\n",
    "            self.patience_counter = 0\n",
    "            \n",
    "            best_path = self.checkpoint_dir / 'best.pth'\n",
    "            self._atomic_save(best_path, checkpoint)\n",
    "            \n",
    "            if self.config.verbose:\n",
    "                print(f\"  -> New best model! {self.config.mode}={tracked_metric:.4f}\")\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "        \n",
    "        # Update history\n",
    "        history_entry = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'is_best': is_best,\n",
    "            'lr': optimizer.param_groups[0]['lr'],\n",
    "        }\n",
    "        if extra_metrics:\n",
    "            history_entry.update(extra_metrics)\n",
    "        self.history.append(history_entry)\n",
    "        \n",
    "        # Persist state\n",
    "        self._save_state()\n",
    "        \n",
    "        return {\n",
    "            'is_best': is_best,\n",
    "            'should_stop': self.patience_counter >= self.config.patience,\n",
    "            'best_metric': self.best_metric,\n",
    "            'best_epoch': self.best_epoch,\n",
    "            'patience_counter': self.patience_counter,\n",
    "            'patience_remaining': self.config.patience - self.patience_counter,\n",
    "        }\n",
    "    \n",
    "    def load_best(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        optimizer: Optional[optim.Optimizer] = None,\n",
    "        scheduler: Optional[Any] = None,\n",
    "        device: Optional[torch.device] = None,\n",
    "    ) -> Dict:\n",
    "        \"\"\"Load the best checkpoint.\"\"\"\n",
    "        return self._load(self.checkpoint_dir / 'best.pth', model, optimizer, scheduler, device)\n",
    "    \n",
    "    def load_latest(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        optimizer: Optional[optim.Optimizer] = None,\n",
    "        scheduler: Optional[Any] = None,\n",
    "        device: Optional[torch.device] = None,\n",
    "    ) -> Dict:\n",
    "        \"\"\"Load the latest checkpoint for resuming training.\"\"\"\n",
    "        return self._load(self.checkpoint_dir / 'latest.pth', model, optimizer, scheduler, device)\n",
    "    \n",
    "    def _load(\n",
    "        self,\n",
    "        path: Path,\n",
    "        model: nn.Module,\n",
    "        optimizer: Optional[optim.Optimizer],\n",
    "        scheduler: Optional[Any],\n",
    "        device: Optional[torch.device],\n",
    "    ) -> Dict:\n",
    "        \"\"\"Load a checkpoint file.\"\"\"\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"Checkpoint not found: {path}\")\n",
    "        \n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        if optimizer and 'optimizer_state_dict' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        if scheduler and 'scheduler_state_dict' in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "        if self.config.verbose:\n",
    "            print(f\"Loaded checkpoint: epoch {checkpoint['epoch']}, \"\n",
    "                  f\"val_loss={checkpoint['val_loss']:.4f}\")\n",
    "        \n",
    "        return checkpoint\n",
    "    \n",
    "    def get_history_df(self):\n",
    "        \"\"\"Get training history as pandas DataFrame (if pandas available).\"\"\"\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            return pd.DataFrame(self.history)\n",
    "        except ImportError:\n",
    "            return self.history\n",
    "    \n",
    "    def plot_history(self):\n",
    "        \"\"\"Plot training history.\"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "            \n",
    "            epochs = [h['epoch'] for h in self.history]\n",
    "            \n",
    "            # Loss\n",
    "            axes[0].plot(epochs, [h['train_loss'] for h in self.history], label='Train')\n",
    "            axes[0].plot(epochs, [h['val_loss'] for h in self.history], label='Val')\n",
    "            axes[0].axvline(self.best_epoch, color='g', linestyle='--', label='Best')\n",
    "            axes[0].set_xlabel('Epoch')\n",
    "            axes[0].set_ylabel('Loss')\n",
    "            axes[0].legend()\n",
    "            axes[0].set_title('Training Loss')\n",
    "            \n",
    "            # Accuracy\n",
    "            axes[1].plot(epochs, [h['val_accuracy'] for h in self.history])\n",
    "            axes[1].axvline(self.best_epoch, color='g', linestyle='--', label='Best')\n",
    "            axes[1].set_xlabel('Epoch')\n",
    "            axes[1].set_ylabel('Accuracy (%)')\n",
    "            axes[1].set_title('Validation Accuracy')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except ImportError:\n",
    "            print(\"matplotlib not available for plotting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the enhanced checkpoint manager\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    \"\"\"Evaluate model on dataloader.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(dataloader), 100 * correct / total\n",
    "\n",
    "\n",
    "# Setup\n",
    "model = SimpleNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create checkpoint manager\n",
    "ckpt_config = CheckpointConfig(\n",
    "    checkpoint_dir='./checkpoints_solution/enhanced_run',\n",
    "    mode='min',  # Track validation loss\n",
    "    patience=5,\n",
    "    min_delta=0.001,  # Require at least 0.001 improvement\n",
    "    max_checkpoints=3,\n",
    ")\n",
    "ckpt_manager = EnhancedCheckpointManager(ckpt_config)\n",
    "\n",
    "# Training loop\n",
    "print(\"=== Training with Enhanced Checkpoint Manager ===\")\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for inputs, labels in trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(inputs), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(trainloader)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss, val_acc = evaluate(model, testloader, criterion)\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Checkpoint\n",
    "    status = ckpt_manager.step(\n",
    "        epoch=epoch,\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        train_loss=train_loss,\n",
    "        val_loss=val_loss,\n",
    "        val_accuracy=val_acc,\n",
    "        scheduler=scheduler,\n",
    "    )\n",
    "    \n",
    "    best_marker = \" *\" if status['is_best'] else \"\"\n",
    "    print(f\"Epoch {epoch+1:2d} | Train: {train_loss:.4f} | \"\n",
    "          f\"Val: {val_loss:.4f} | Acc: {val_acc:.2f}% | \"\n",
    "          f\"Patience: {status['patience_remaining']}{best_marker}\")\n",
    "    \n",
    "    if status['should_stop']:\n",
    "        print(f\"\\nEarly stopping triggered at epoch {epoch+1}!\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nBest model: epoch {ckpt_manager.best_epoch} with val_loss={ckpt_manager.best_metric:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution 3: Training Resume Demo\n",
    "\n",
    "Demonstrate complete training resume capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resumable_training(\n",
    "    model: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    trainloader: DataLoader,\n",
    "    testloader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    num_epochs: int,\n",
    "    checkpoint_dir: str,\n",
    "    scheduler: Optional[Any] = None,\n",
    "    resume: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Training loop with full resume capability.\n",
    "    \n",
    "    Can be interrupted and resumed exactly where it left off.\n",
    "    \"\"\"\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    history = []\n",
    "    \n",
    "    # Try to resume\n",
    "    latest_path = checkpoint_dir / 'latest.pth'\n",
    "    if resume and latest_path.exists():\n",
    "        print(\"Resuming from checkpoint...\")\n",
    "        checkpoint = torch.load(latest_path, map_location=device)\n",
    "        \n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if scheduler and 'scheduler_state_dict' in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "        history = checkpoint.get('history', [])\n",
    "        \n",
    "        print(f\"Resumed from epoch {checkpoint['epoch']}, \"\n",
    "              f\"best_val_loss={best_val_loss:.4f}\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(inputs), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(trainloader)\n",
    "        \n",
    "        # Evaluate\n",
    "        val_loss, val_acc = evaluate(model, testloader, criterion)\n",
    "        \n",
    "        if scheduler:\n",
    "            if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        \n",
    "        # Track best\n",
    "        is_best = val_loss < best_val_loss\n",
    "        if is_best:\n",
    "            best_val_loss = val_loss\n",
    "        \n",
    "        # Save history\n",
    "        history.append({\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_acc,\n",
    "        })\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_acc,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'history': history,\n",
    "        }\n",
    "        if scheduler:\n",
    "            checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "        \n",
    "        torch.save(checkpoint, latest_path)\n",
    "        \n",
    "        if is_best:\n",
    "            torch.save(checkpoint, checkpoint_dir / 'best.pth')\n",
    "        \n",
    "        status = \" (best)\" if is_best else \"\"\n",
    "        print(f\"Epoch {epoch+1:2d}/{num_epochs} | \"\n",
    "              f\"Train: {train_loss:.4f} | Val: {val_loss:.4f} | \"\n",
    "              f\"Acc: {val_acc:.2f}%{status}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Train for 3 epochs, then \"interrupt\" and resume\n",
    "print(\"=== First Training Run (3 epochs) ===\")\n",
    "model = SimpleNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Clear previous checkpoints\n",
    "resume_dir = Path('./checkpoints_solution/resume_demo')\n",
    "if resume_dir.exists():\n",
    "    shutil.rmtree(resume_dir)\n",
    "\n",
    "history1 = resumable_training(\n",
    "    model, optimizer, trainloader, testloader, criterion,\n",
    "    num_epochs=3,\n",
    "    checkpoint_dir=resume_dir,\n",
    "    resume=False,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Simulating interrupt... ===\")\n",
    "print(\"Creating new model and optimizer (simulating restart)...\")\n",
    "\n",
    "# Create fresh model/optimizer (simulating program restart)\n",
    "model = SimpleNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\n=== Resuming Training (3 more epochs) ===\")\n",
    "history2 = resumable_training(\n",
    "    model, optimizer, trainloader, testloader, criterion,\n",
    "    num_epochs=6,  # Total epochs we want\n",
    "    checkpoint_dir=resume_dir,\n",
    "    resume=True,  # This will load the checkpoint\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal epochs trained: {len(history2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution 4: Multi-Model Checkpoint\n",
    "\n",
    "Save multiple models (e.g., for GANs, ensemble, or distillation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_multi_model_checkpoint(\n",
    "    path: str,\n",
    "    models: Dict[str, nn.Module],\n",
    "    optimizers: Dict[str, optim.Optimizer],\n",
    "    epoch: int,\n",
    "    metrics: Dict[str, float],\n",
    "    schedulers: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Save checkpoint with multiple models (for GANs, ensembles, etc.).\n",
    "    \n",
    "    Args:\n",
    "        path: Checkpoint file path\n",
    "        models: Dict of model_name -> model\n",
    "        optimizers: Dict of model_name -> optimizer\n",
    "        epoch: Current epoch\n",
    "        metrics: Training metrics\n",
    "        schedulers: Dict of model_name -> scheduler (optional)\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'metrics': metrics,\n",
    "        'models': {},\n",
    "        'optimizers': {},\n",
    "    }\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        checkpoint['models'][name] = model.state_dict()\n",
    "    \n",
    "    for name, opt in optimizers.items():\n",
    "        checkpoint['optimizers'][name] = opt.state_dict()\n",
    "    \n",
    "    if schedulers:\n",
    "        checkpoint['schedulers'] = {}\n",
    "        for name, sched in schedulers.items():\n",
    "            checkpoint['schedulers'][name] = sched.state_dict()\n",
    "    \n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Saved multi-model checkpoint: {list(models.keys())}\")\n",
    "\n",
    "\n",
    "def load_multi_model_checkpoint(\n",
    "    path: str,\n",
    "    models: Dict[str, nn.Module],\n",
    "    optimizers: Optional[Dict[str, optim.Optimizer]] = None,\n",
    "    schedulers: Optional[Dict[str, Any]] = None,\n",
    "    device: Optional[torch.device] = None,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Load multi-model checkpoint.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        if name in checkpoint['models']:\n",
    "            model.load_state_dict(checkpoint['models'][name])\n",
    "    \n",
    "    if optimizers:\n",
    "        for name, opt in optimizers.items():\n",
    "            if name in checkpoint.get('optimizers', {}):\n",
    "                opt.load_state_dict(checkpoint['optimizers'][name])\n",
    "    \n",
    "    if schedulers and 'schedulers' in checkpoint:\n",
    "        for name, sched in schedulers.items():\n",
    "            if name in checkpoint['schedulers']:\n",
    "                sched.load_state_dict(checkpoint['schedulers'][name])\n",
    "    \n",
    "    print(f\"Loaded multi-model checkpoint from epoch {checkpoint['epoch']}\")\n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "# Example: Save/load two models\n",
    "model_a = SimpleNet().to(device)\n",
    "model_b = SimpleNet().to(device)\n",
    "\n",
    "opt_a = optim.Adam(model_a.parameters(), lr=0.001)\n",
    "opt_b = optim.Adam(model_b.parameters(), lr=0.001)\n",
    "\n",
    "# Save\n",
    "save_multi_model_checkpoint(\n",
    "    checkpoint_dir / 'multi_model.pth',\n",
    "    models={'model_a': model_a, 'model_b': model_b},\n",
    "    optimizers={'model_a': opt_a, 'model_b': opt_b},\n",
    "    epoch=5,\n",
    "    metrics={'loss_a': 0.5, 'loss_b': 0.6},\n",
    ")\n",
    "\n",
    "# Load into fresh models\n",
    "new_a = SimpleNet().to(device)\n",
    "new_b = SimpleNet().to(device)\n",
    "\n",
    "load_multi_model_checkpoint(\n",
    "    checkpoint_dir / 'multi_model.pth',\n",
    "    models={'model_a': new_a, 'model_b': new_b},\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Checkpointing Best Practices\n",
    "\n",
    "| Practice | Why It Matters |\n",
    "|----------|----------------|\n",
    "| Save optimizer state | Preserves momentum, Adam's m/v buffers |\n",
    "| Save scheduler state | Maintains LR schedule position |\n",
    "| Use map_location | Handles GPU/CPU portability |\n",
    "| Atomic saves | Prevents corruption from crashes |\n",
    "| Track best model | Always have best performing checkpoint |\n",
    "| Save training history | Enables analysis and visualization |\n",
    "| Checkpoint rotation | Manages disk space |\n",
    "| Early stopping | Prevents overfitting, saves time |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "\n",
    "if checkpoint_dir.exists():\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "if resume_dir.exists():\n",
    "    shutil.rmtree(resume_dir)\n",
    "\n",
    "# Clean up production run if exists\n",
    "prod_run = Path('./checkpoints_solution/enhanced_run')\n",
    "if prod_run.exists():\n",
    "    shutil.rmtree(prod_run)\n",
    "\n",
    "# Remove main solution dir\n",
    "sol_dir = Path('./checkpoints_solution')\n",
    "if sol_dir.exists():\n",
    "    shutil.rmtree(sol_dir)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}