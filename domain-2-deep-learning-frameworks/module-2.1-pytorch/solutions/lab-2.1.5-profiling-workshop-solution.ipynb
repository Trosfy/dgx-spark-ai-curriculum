{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6.5: Profiling Workshop - SOLUTIONS\n",
    "\n",
    "Complete solutions for the Profiling Workshop exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Solution: Finding and Fixing Bottlenecks\n",
    "\n",
    "The inefficient training loop has several issues. Let's identify and fix them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 8 * 8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL INEFFICIENT VERSION\n",
    "def inefficient_training(model, trainset, epochs=1, max_batches=100):\n",
    "    \"\"\"\n",
    "    This function has FOUR performance issues:\n",
    "    \n",
    "    Issue 1: num_workers=0 - Data loading is single-threaded\n",
    "    Issue 2: No pin_memory - Slower CPU-GPU transfer\n",
    "    Issue 3: Using set_to_none=False - Slower gradient zeroing\n",
    "    Issue 4: loss.item() every iteration - Forces CPU-GPU sync\n",
    "    \"\"\"\n",
    "    # Issue 1: num_workers=0 means all data loading happens on main thread\n",
    "    loader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        for i, (inputs, labels) in enumerate(loader):\n",
    "            # Issue 2: Not using non_blocking transfer\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Issue 3: zero_grad() without set_to_none=True is slower\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Issue 4: .item() forces CPU-GPU synchronization EVERY iteration\n",
    "            if i % 1 == 0:  # Every iteration!\n",
    "                print(f\"Loss: {loss.item():.4f}\", end='\\r')\n",
    "            \n",
    "            if i >= max_batches:\n",
    "                break\n",
    "    \n",
    "    return time.time() - start\n",
    "\n",
    "# Benchmark inefficient version\n",
    "model = SimpleModel().to(device)\n",
    "inefficient_time = inefficient_training(model, trainset)\n",
    "print(f\"\\nInefficient time: {inefficient_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZED VERSION\n",
    "def efficient_training(model, trainset, epochs=1, max_batches=100):\n",
    "    \"\"\"\n",
    "    Fixed version with all issues resolved:\n",
    "    \n",
    "    Fix 1: num_workers=4 - Parallel data loading\n",
    "    Fix 2: pin_memory=True + non_blocking transfer\n",
    "    Fix 3: set_to_none=True - Faster gradient zeroing\n",
    "    Fix 4: Reduced logging frequency, accumulate loss\n",
    "    \"\"\"\n",
    "    # Fix 1 & 2: Multi-worker loading with pinned memory\n",
    "    loader = DataLoader(\n",
    "        trainset, \n",
    "        batch_size=32, \n",
    "        shuffle=True, \n",
    "        num_workers=4,           # Fix 1: Parallel data loading\n",
    "        pin_memory=True,         # Fix 2: Pinned memory for faster transfer\n",
    "        persistent_workers=True  # Bonus: Keep workers alive\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0  # Accumulate without sync\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(loader):\n",
    "            # Fix 2: Non-blocking transfer\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Fix 3: set_to_none is faster than zeroing\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Fix 4: Accumulate loss as tensor, only sync occasionally\n",
    "            running_loss += loss.detach()  # detach() keeps it on GPU\n",
    "            \n",
    "            if (i + 1) % 20 == 0:  # Log every 20 batches, not every 1\n",
    "                avg_loss = running_loss.item() / 20  # Single sync point\n",
    "                print(f\"Batch {i+1}: Loss: {avg_loss:.4f}\", end='\\r')\n",
    "                running_loss = 0.0\n",
    "            \n",
    "            if i >= max_batches:\n",
    "                break\n",
    "    \n",
    "    return time.time() - start\n",
    "\n",
    "# Benchmark efficient version\n",
    "model = SimpleModel().to(device)\n",
    "efficient_time = efficient_training(model, trainset)\n",
    "print(f\"\\nEfficient time: {efficient_time:.2f}s\")\n",
    "print(f\"Speedup: {inefficient_time/efficient_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Fixes\n",
    "\n",
    "| Issue | Problem | Fix | Impact |\n",
    "|-------|---------|-----|--------|\n",
    "| 1 | num_workers=0 | num_workers=4 | Data loading parallelized |\n",
    "| 2 | No pin_memory | pin_memory=True + non_blocking | Faster CPU-GPU transfer |\n",
    "| 3 | zero_grad() default | set_to_none=True | ~5-10% speedup |\n",
    "| 4 | .item() every iteration | Accumulate, sync occasionally | Reduced CPU-GPU sync |\n",
    "\n",
    "### Additional Optimizations\n",
    "\n",
    "- `persistent_workers=True`: Workers stay alive between epochs\n",
    "- `prefetch_factor`: Increase to prefetch more batches\n",
    "- Larger batch size: Better GPU utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
