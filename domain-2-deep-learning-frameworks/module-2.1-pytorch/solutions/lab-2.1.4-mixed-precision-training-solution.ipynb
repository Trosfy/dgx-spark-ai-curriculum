{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.1.4: Mixed Precision Training - SOLUTIONS\n",
    "\n",
    "This notebook contains complete solutions for the Mixed Precision Training exercises.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nfrom torch.amp import autocast, GradScaler  # Updated for PyTorch 2.0+\nfrom torch.utils.data import DataLoader\nimport time\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise Solution: Adaptive Precision Training with Fallback\n",
    "\n",
    "This implementation automatically falls back from FP16 to FP32 if too many gradient overflows are detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class AdaptivePrecisionTrainer:\n    \"\"\"\n    Trainer that automatically falls back from FP16 to FP32 if unstable.\n    \n    This is useful when:\n    - Training with very small learning rates\n    - Using architectures prone to gradient issues\n    - Working with unusual loss functions\n    \n    Args:\n        model: Neural network model\n        criterion: Loss function\n        max_skip_ratio: Maximum ratio of skipped steps before fallback (default: 0.1)\n        window_size: Number of steps to consider for skip ratio (default: 100)\n    \"\"\"\n    \n    def __init__(\n        self, \n        model: nn.Module, \n        criterion: nn.Module,\n        max_skip_ratio: float = 0.1,\n        window_size: int = 100\n    ):\n        self.model = model\n        self.criterion = criterion\n        self.max_skip_ratio = max_skip_ratio\n        self.window_size = window_size\n        \n        # Start with FP16\n        self.use_fp16 = True\n        self.scaler = GradScaler()\n        \n        # Tracking\n        self.skip_history = []\n        self.fallback_triggered = False\n        self.total_steps = 0\n        self.total_skips = 0\n    \n    def _check_and_update_precision(self):\n        \"\"\"\n        Check if we should fall back to FP32.\n        \"\"\"\n        if not self.use_fp16 or self.fallback_triggered:\n            return\n        \n        # Only check after enough history\n        if len(self.skip_history) < self.window_size:\n            return\n        \n        # Calculate skip ratio over recent window\n        recent_skips = sum(self.skip_history[-self.window_size:])\n        skip_ratio = recent_skips / self.window_size\n        \n        if skip_ratio > self.max_skip_ratio:\n            print(f\"\\n⚠️  Fallback triggered! Skip ratio: {skip_ratio:.2%} > {self.max_skip_ratio:.2%}\")\n            print(f\"    Switching from FP16 to FP32...\")\n            self.use_fp16 = False\n            self.fallback_triggered = True\n            # Disable scaler\n            self.scaler = GradScaler(enabled=False)\n    \n    def train_step(self, inputs, targets, optimizer):\n        \"\"\"\n        Perform a single training step with adaptive precision.\n        \n        Args:\n            inputs: Input batch\n            targets: Target labels\n            optimizer: Optimizer instance\n            \n        Returns:\n            Tuple of (loss_value, was_skipped)\n        \"\"\"\n        self.model.train()\n        optimizer.zero_grad()\n        \n        # Record scale before step\n        old_scale = self.scaler.get_scale() if self.use_fp16 else 1.0\n        \n        # Forward pass with PyTorch 2.0+ API\n        if self.use_fp16:\n            with autocast(device_type='cuda', dtype=torch.float16):\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, targets)\n            \n            # Backward with scaling\n            self.scaler.scale(loss).backward()\n            self.scaler.step(optimizer)\n            self.scaler.update()\n        else:\n            # FP32 fallback\n            outputs = self.model(inputs)\n            loss = self.criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n        \n        # Check if step was skipped (scale decreased)\n        new_scale = self.scaler.get_scale() if self.use_fp16 else 1.0\n        was_skipped = new_scale < old_scale\n        \n        # Update tracking\n        self.skip_history.append(1 if was_skipped else 0)\n        self.total_steps += 1\n        if was_skipped:\n            self.total_skips += 1\n        \n        # Check if we should fall back\n        self._check_and_update_precision()\n        \n        return loss.item(), was_skipped\n    \n    def train_epoch(self, dataloader, optimizer):\n        \"\"\"\n        Train for one epoch with adaptive precision.\n        \n        Returns:\n            Dict with training metrics\n        \"\"\"\n        self.model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        epoch_skips = 0\n        \n        start_time = time.time()\n        \n        for inputs, targets in dataloader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            loss, skipped = self.train_step(inputs, targets, optimizer)\n            \n            running_loss += loss\n            if skipped:\n                epoch_skips += 1\n            \n            # Get predictions for accuracy\n            with torch.no_grad():\n                if self.use_fp16:\n                    with autocast(device_type='cuda', dtype=torch.float16):\n                        outputs = self.model(inputs)\n                else:\n                    outputs = self.model(inputs)\n                _, predicted = outputs.max(1)\n                total += targets.size(0)\n                correct += predicted.eq(targets).sum().item()\n        \n        return {\n            'loss': running_loss / len(dataloader),\n            'accuracy': 100. * correct / total,\n            'time': time.time() - start_time,\n            'skipped_steps': epoch_skips,\n            'precision': 'fp16' if self.use_fp16 else 'fp32',\n            'fallback_triggered': self.fallback_triggered,\n        }\n    \n    def get_stats(self):\n        \"\"\"Get training statistics.\"\"\"\n        return {\n            'total_steps': self.total_steps,\n            'total_skips': self.total_skips,\n            'skip_ratio': self.total_skips / max(1, self.total_steps),\n            'current_precision': 'fp16' if self.use_fp16 else 'fp32',\n            'fallback_triggered': self.fallback_triggered,\n        }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the adaptive trainer\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Create a simple model\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(32 * 32 * 3, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 10),\n",
    ").to(device)\n",
    "\n",
    "# Load data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "# Create trainer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "trainer = AdaptivePrecisionTrainer(\n",
    "    model, criterion,\n",
    "    max_skip_ratio=0.1,\n",
    "    window_size=50\n",
    ")\n",
    "\n",
    "# Train for a few epochs\n",
    "print(\"=== Adaptive Precision Training ===\")\n",
    "for epoch in range(3):\n",
    "    metrics = trainer.train_epoch(trainloader, optimizer)\n",
    "    print(f\"Epoch {epoch+1}: Loss={metrics['loss']:.4f}, \"\n",
    "          f\"Acc={metrics['accuracy']:.2f}%, \"\n",
    "          f\"Precision={metrics['precision']}, \"\n",
    "          f\"Skipped={metrics['skipped_steps']}\")\n",
    "\n",
    "# Print final stats\n",
    "stats = trainer.get_stats()\n",
    "print(f\"\\nFinal Stats:\")\n",
    "print(f\"  Total steps: {stats['total_steps']}\")\n",
    "print(f\"  Total skips: {stats['total_skips']}\")\n",
    "print(f\"  Skip ratio: {stats['skip_ratio']:.2%}\")\n",
    "print(f\"  Final precision: {stats['current_precision']}\")\n",
    "print(f\"  Fallback triggered: {stats['fallback_triggered']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Alternative: Dynamic Loss Scaling\n",
    "\n",
    "Instead of falling back completely, we can be smarter about scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartGradScaler:\n",
    "    \"\"\"\n",
    "    Enhanced gradient scaler with better overflow handling.\n",
    "    \n",
    "    Features:\n",
    "    - More aggressive backoff on repeated overflows\n",
    "    - Slower growth to avoid oscillation\n",
    "    - Statistics tracking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        init_scale: float = 65536.0,\n",
    "        growth_factor: float = 1.5,  # Slower growth than default 2.0\n",
    "        backoff_factor: float = 0.25,  # More aggressive backoff than 0.5\n",
    "        growth_interval: int = 1000,  # Longer interval\n",
    "        max_scale: float = 2**24,\n",
    "        min_scale: float = 1.0,\n",
    "    ):\n",
    "        self.scale = init_scale\n",
    "        self.growth_factor = growth_factor\n",
    "        self.backoff_factor = backoff_factor\n",
    "        self.growth_interval = growth_interval\n",
    "        self.max_scale = max_scale\n",
    "        self.min_scale = min_scale\n",
    "        \n",
    "        self.steps_since_growth = 0\n",
    "        self.consecutive_overflows = 0\n",
    "        self.total_overflows = 0\n",
    "        self.total_steps = 0\n",
    "    \n",
    "    def scale_loss(self, loss):\n",
    "        \"\"\"Scale the loss for backward pass.\"\"\"\n",
    "        return loss * self.scale\n",
    "    \n",
    "    def unscale_grads(self, optimizer):\n",
    "        \"\"\"Unscale gradients and check for overflow.\"\"\"\n",
    "        found_inf = False\n",
    "        \n",
    "        for group in optimizer.param_groups:\n",
    "            for param in group['params']:\n",
    "                if param.grad is not None:\n",
    "                    # Unscale\n",
    "                    param.grad.div_(self.scale)\n",
    "                    \n",
    "                    # Check for inf/nan\n",
    "                    if torch.isinf(param.grad).any() or torch.isnan(param.grad).any():\n",
    "                        found_inf = True\n",
    "                        # Zero out to prevent optimizer step from using bad grads\n",
    "                        param.grad.zero_()\n",
    "        \n",
    "        return found_inf\n",
    "    \n",
    "    def step(self, optimizer, found_inf: bool):\n",
    "        \"\"\"\n",
    "        Update optimizer and adjust scale.\n",
    "        \n",
    "        Args:\n",
    "            optimizer: The optimizer\n",
    "            found_inf: Whether overflow was detected\n",
    "            \n",
    "        Returns:\n",
    "            True if step was taken, False if skipped\n",
    "        \"\"\"\n",
    "        self.total_steps += 1\n",
    "        \n",
    "        if found_inf:\n",
    "            self.total_overflows += 1\n",
    "            self.consecutive_overflows += 1\n",
    "            \n",
    "            # More aggressive backoff with consecutive overflows\n",
    "            backoff = self.backoff_factor ** min(self.consecutive_overflows, 3)\n",
    "            self.scale = max(self.scale * backoff, self.min_scale)\n",
    "            self.steps_since_growth = 0\n",
    "            \n",
    "            return False\n",
    "        else:\n",
    "            self.consecutive_overflows = 0\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Consider growing scale\n",
    "            self.steps_since_growth += 1\n",
    "            if self.steps_since_growth >= self.growth_interval:\n",
    "                self.scale = min(self.scale * self.growth_factor, self.max_scale)\n",
    "                self.steps_since_growth = 0\n",
    "            \n",
    "            return True\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get scaler statistics.\"\"\"\n",
    "        return {\n",
    "            'current_scale': self.scale,\n",
    "            'total_steps': self.total_steps,\n",
    "            'total_overflows': self.total_overflows,\n",
    "            'overflow_ratio': self.total_overflows / max(1, self.total_steps),\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "print(\"SmartGradScaler example:\")\n",
    "scaler = SmartGradScaler()\n",
    "print(f\"Initial scale: {scaler.scale}\")\n",
    "\n",
    "# Simulate some steps\n",
    "for i in range(10):\n",
    "    # Simulate overflow on step 3\n",
    "    found_inf = (i == 3)\n",
    "    scaler.step(None if found_inf else torch.optim.SGD([torch.zeros(1, requires_grad=True)], lr=0.01), found_inf)\n",
    "\n",
    "stats = scaler.get_stats()\n",
    "print(f\"After 10 steps: scale={stats['current_scale']:.2f}, overflows={stats['total_overflows']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Best Practices Summary\n",
    "\n",
    "1. **Start with BF16** - Same range as FP32, no scaling needed\n",
    "2. **Use FP16 with GradScaler** - Required for FP16 stability\n",
    "3. **Monitor skip ratio** - If >5%, consider adjustments\n",
    "4. **Gradient clipping helps** - Use with mixed precision\n",
    "5. **Test accuracy** - Verify no degradation from FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}