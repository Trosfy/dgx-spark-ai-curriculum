{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.1.3: Autograd Deep Dive - SOLUTIONS\n",
    "\n",
    "This notebook contains complete solutions for the Autograd Deep Dive exercises.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function, gradcheck\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise Solution: Hard Swish\n",
    "\n",
    "Hard Swish is a computationally efficient approximation of Swish used in MobileNetV3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardSwishFunction(Function):\n",
    "    \"\"\"\n",
    "    Custom autograd function for Hard Swish activation.\n",
    "    \n",
    "    HardSwish(x) = x * ReLU6(x + 3) / 6\n",
    "    \n",
    "    Where ReLU6(x) = min(max(0, x), 6) = clamp(x, 0, 6)\n",
    "    \n",
    "    This can be rewritten as:\n",
    "    - x < -3: 0\n",
    "    - -3 <= x <= 3: x * (x + 3) / 6\n",
    "    - x > 3: x\n",
    "    \n",
    "    Derivative:\n",
    "    - x < -3: 0\n",
    "    - -3 <= x <= 3: (2x + 3) / 6\n",
    "    - x > 3: 1\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: compute HardSwish(x).\n",
    "        \"\"\"\n",
    "        # Save input for backward\n",
    "        ctx.save_for_backward(x)\n",
    "        \n",
    "        # Compute: x * ReLU6(x + 3) / 6\n",
    "        # ReLU6(x) = clamp(x, 0, 6)\n",
    "        return x * torch.clamp(x + 3, min=0, max=6) / 6\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradient of HardSwish.\n",
    "        \n",
    "        The gradient is piecewise:\n",
    "        - x < -3: 0\n",
    "        - -3 <= x <= 3: (2x + 3) / 6\n",
    "        - x > 3: 1\n",
    "        \"\"\"\n",
    "        x, = ctx.saved_tensors\n",
    "        \n",
    "        # Initialize gradient to zeros\n",
    "        grad_input = torch.zeros_like(x)\n",
    "        \n",
    "        # Region 1: x < -3 -> gradient = 0 (already set)\n",
    "        \n",
    "        # Region 2: -3 <= x <= 3 -> gradient = (2x + 3) / 6\n",
    "        mask_middle = (x >= -3) & (x <= 3)\n",
    "        grad_input[mask_middle] = (2 * x[mask_middle] + 3) / 6\n",
    "        \n",
    "        # Region 3: x > 3 -> gradient = 1\n",
    "        mask_upper = x > 3\n",
    "        grad_input[mask_upper] = 1.0\n",
    "        \n",
    "        # Apply chain rule\n",
    "        return grad_output * grad_input\n",
    "\n",
    "\n",
    "def hard_swish_custom(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Apply custom Hard Swish activation.\"\"\"\n",
    "    return HardSwishFunction.apply(x)\n",
    "\n",
    "\n",
    "# Test the implementation\n",
    "print(\"=== Testing HardSwish ===\")\n",
    "\n",
    "# Test values in each region\n",
    "x_test = torch.tensor([-5.0, -3.0, 0.0, 3.0, 5.0], requires_grad=True)\n",
    "y = hard_swish_custom(x_test)\n",
    "y.sum().backward()\n",
    "\n",
    "print(f\"Input: {x_test.data}\")\n",
    "print(f\"Output: {y.data}\")\n",
    "print(f\"Gradient: {x_test.grad}\")\n",
    "\n",
    "# Expected outputs:\n",
    "# x=-5: 0 (x < -3)\n",
    "# x=-3: 0 (boundary, -3 * 0 / 6 = 0)\n",
    "# x=0: 0 (0 * 3 / 6 = 0)\n",
    "# x=3: 3 (3 * 6 / 6 = 3)\n",
    "# x=5: 5 (x > 3, output = x)\n",
    "print(f\"\\nExpected output: [0, 0, 0, 3, 5]\")\n",
    "\n",
    "# Expected gradients:\n",
    "# x=-5: 0\n",
    "# x=-3: (-6+3)/6 = -0.5 (actually at boundary, should be 0)\n",
    "# x=0: 3/6 = 0.5\n",
    "# x=3: 9/6 = 1.5 (but we clamp, so actually should be at boundary)\n",
    "# x=5: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify gradients with gradcheck\n",
    "x_test = torch.randn(3, 4, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "test_passed = gradcheck(\n",
    "    HardSwishFunction.apply, \n",
    "    (x_test,), \n",
    "    eps=1e-6, \n",
    "    atol=1e-4, \n",
    "    rtol=1e-3\n",
    ")\n",
    "\n",
    "print(f\"HardSwish gradient check passed: {test_passed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with PyTorch's built-in hardswish\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = torch.linspace(-6, 6, 200)\n",
    "\n",
    "# Our custom implementation\n",
    "x_custom = x.clone().requires_grad_(True)\n",
    "y_custom = hard_swish_custom(x_custom)\n",
    "\n",
    "# PyTorch built-in\n",
    "y_builtin = F.hardswish(x)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(x.numpy(), y_custom.detach().numpy(), label='Custom', linewidth=2)\n",
    "axes[0].plot(x.numpy(), y_builtin.numpy(), '--', label='Built-in', linewidth=2)\n",
    "axes[0].set_title('Hard Swish: Custom vs Built-in')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot difference\n",
    "diff = (y_custom.detach() - y_builtin).abs()\n",
    "axes[1].plot(x.numpy(), diff.numpy(), linewidth=2)\n",
    "axes[1].set_title('Absolute Difference')\n",
    "axes[1].set_ylabel('|Custom - Built-in|')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Max difference: {diff.max():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Solution: Gradient Checkpointing\n",
    "\n",
    "Gradient checkpointing is a memory optimization technique that trades compute for memory. Instead of storing all intermediate activations, we only store \"checkpoints\" and recompute activations during the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointFunction(Function):\n",
    "    \"\"\"\n",
    "    Gradient checkpointing function.\n",
    "    \n",
    "    This function wraps another function and recomputes the forward pass\n",
    "    during backward instead of storing intermediate activations.\n",
    "    \n",
    "    Memory savings: O(sqrt(n)) instead of O(n) for n layers\n",
    "    Compute overhead: One additional forward pass\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, run_function, preserve_rng_state, *args):\n",
    "        \"\"\"\n",
    "        Forward pass: run the function and save inputs for recomputation.\n",
    "        \n",
    "        Args:\n",
    "            run_function: The function to checkpoint\n",
    "            preserve_rng_state: Whether to preserve RNG state\n",
    "            *args: Arguments to pass to run_function\n",
    "        \"\"\"\n",
    "        ctx.run_function = run_function\n",
    "        ctx.preserve_rng_state = preserve_rng_state\n",
    "        \n",
    "        # Save RNG state if requested\n",
    "        if preserve_rng_state:\n",
    "            ctx.cpu_rng_state = torch.get_rng_state()\n",
    "            if torch.cuda.is_available():\n",
    "                ctx.cuda_rng_state = torch.cuda.get_rng_state()\n",
    "        \n",
    "        # Save inputs (detached to not track computation)\n",
    "        ctx.save_for_backward(*args)\n",
    "        \n",
    "        # Run forward without tracking gradients for intermediate ops\n",
    "        with torch.no_grad():\n",
    "            outputs = run_function(*args)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, *grad_outputs):\n",
    "        \"\"\"\n",
    "        Backward pass: recompute forward and then backpropagate.\n",
    "        \"\"\"\n",
    "        # Restore inputs\n",
    "        inputs = ctx.saved_tensors\n",
    "        \n",
    "        # Restore RNG state if needed\n",
    "        if ctx.preserve_rng_state:\n",
    "            torch.set_rng_state(ctx.cpu_rng_state)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.set_rng_state(ctx.cuda_rng_state)\n",
    "        \n",
    "        # Recompute forward pass with gradient tracking\n",
    "        detached_inputs = [x.detach().requires_grad_(x.requires_grad) for x in inputs]\n",
    "        \n",
    "        with torch.enable_grad():\n",
    "            outputs = ctx.run_function(*detached_inputs)\n",
    "        \n",
    "        # Handle single output\n",
    "        if not isinstance(outputs, tuple):\n",
    "            outputs = (outputs,)\n",
    "        \n",
    "        # Compute gradients\n",
    "        torch.autograd.backward(outputs, grad_outputs)\n",
    "        \n",
    "        # Return gradients for inputs (None for run_function and preserve_rng_state)\n",
    "        grads = tuple(inp.grad if inp.grad is not None else None for inp in detached_inputs)\n",
    "        return (None, None) + grads\n",
    "\n",
    "\n",
    "def checkpoint(function, *args, preserve_rng_state=True):\n",
    "    \"\"\"\n",
    "    Checkpoint a function to save memory during backward pass.\n",
    "    \n",
    "    Args:\n",
    "        function: Function to checkpoint\n",
    "        *args: Arguments to pass to function\n",
    "        preserve_rng_state: Whether to preserve random state\n",
    "    \n",
    "    Returns:\n",
    "        Output of function(*args)\n",
    "    \n",
    "    Example:\n",
    "        >>> def my_func(x):\n",
    "        ...     return x.relu().sin().cos()\n",
    "        >>> y = checkpoint(my_func, x)\n",
    "    \"\"\"\n",
    "    return CheckpointFunction.apply(function, preserve_rng_state, *args)\n",
    "\n",
    "\n",
    "# Test checkpointing\n",
    "print(\"=== Testing Gradient Checkpointing ===\")\n",
    "\n",
    "def expensive_computation(x):\n",
    "    \"\"\"A function with many intermediate activations.\"\"\"\n",
    "    for _ in range(5):\n",
    "        x = x.relu()\n",
    "        x = x.sin()\n",
    "        x = x.cos()\n",
    "    return x\n",
    "\n",
    "# Without checkpointing\n",
    "x1 = torch.randn(100, 100, requires_grad=True)\n",
    "y1 = expensive_computation(x1)\n",
    "y1.sum().backward()\n",
    "grad1 = x1.grad.clone()\n",
    "\n",
    "# With checkpointing\n",
    "x2 = x1.detach().clone().requires_grad_(True)\n",
    "y2 = checkpoint(expensive_computation, x2)\n",
    "y2.sum().backward()\n",
    "grad2 = x2.grad.clone()\n",
    "\n",
    "# Compare\n",
    "print(f\"Outputs match: {torch.allclose(y1.detach(), y2.detach())}\")\n",
    "print(f\"Gradients match: {torch.allclose(grad1, grad2)}\")\n",
    "print(f\"Max gradient difference: {(grad1 - grad2).abs().max():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Comparison\n",
    "\n",
    "In practice, checkpointing is most useful for deep networks. PyTorch includes a built-in version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.checkpoint import checkpoint as torch_checkpoint\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a deep model\n",
    "class DeepModel(nn.Module):\n",
    "    def __init__(self, num_layers=20):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(256, 256) for _ in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, use_checkpoint=False):\n",
    "        for layer in self.layers:\n",
    "            if use_checkpoint:\n",
    "                x = torch_checkpoint(lambda y: F.relu(layer(y)), x, use_reentrant=False)\n",
    "            else:\n",
    "                x = F.relu(layer(x))\n",
    "        return x\n",
    "\n",
    "model = DeepModel(num_layers=20).to(device)\n",
    "\n",
    "# Compare memory usage\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    # Without checkpointing\n",
    "    x = torch.randn(64, 256, device=device, requires_grad=True)\n",
    "    y = model(x, use_checkpoint=False)\n",
    "    y.sum().backward()\n",
    "    mem_no_ckpt = torch.cuda.max_memory_allocated() / 1e6\n",
    "    \n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    # With checkpointing\n",
    "    x = torch.randn(64, 256, device=device, requires_grad=True)\n",
    "    y = model(x, use_checkpoint=True)\n",
    "    y.sum().backward()\n",
    "    mem_ckpt = torch.cuda.max_memory_allocated() / 1e6\n",
    "    \n",
    "    print(f\"Memory without checkpointing: {mem_no_ckpt:.2f} MB\")\n",
    "    print(f\"Memory with checkpointing: {mem_ckpt:.2f} MB\")\n",
    "    print(f\"Memory saved: {(1 - mem_ckpt/mem_no_ckpt)*100:.1f}%\")\n",
    "else:\n",
    "    print(\"GPU not available, skipping memory comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Alternative Solution: Using torch.autograd.function with Setup Context\n",
    "\n",
    "PyTorch 2.0+ supports a new `setup_context` method for cleaner code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModernSwishFunction(Function):\n",
    "    \"\"\"\n",
    "    Modern-style autograd function using setup_context.\n",
    "    \n",
    "    This style separates context setup from forward computation,\n",
    "    making the code cleaner and more maintainable.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Pure forward computation.\"\"\"\n",
    "        sigmoid_x = torch.sigmoid(x)\n",
    "        return x * sigmoid_x\n",
    "    \n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        \"\"\"Setup context after forward.\"\"\"\n",
    "        x, = inputs\n",
    "        # Save sigmoid for backward (computing it from x is equivalent)\n",
    "        sigmoid_x = torch.sigmoid(x)\n",
    "        ctx.save_for_backward(x, sigmoid_x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        \"\"\"Backward computation.\"\"\"\n",
    "        x, sigmoid_x = ctx.saved_tensors\n",
    "        grad_input = sigmoid_x * (1 + x * (1 - sigmoid_x))\n",
    "        return grad_output * grad_input\n",
    "\n",
    "# This style may not work in all PyTorch versions\n",
    "# The classic style with forward(ctx, x) is more compatible\n",
    "print(\"Modern autograd function style demonstrated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}