{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.6.1 Solution: Diffusion Theory\n",
    "\n",
    "This solution notebook contains completed exercises from Lab 2.6.1.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Experiment with Different Images\n",
    "\n",
    "The exercise asked to visualize forward diffusion on different MNIST digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Visualize different digits\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Get different digits\n",
    "digit_indices = {}\n",
    "for i, (_, label) in enumerate(train_dataset):\n",
    "    if label not in digit_indices:\n",
    "        digit_indices[label] = i\n",
    "    if len(digit_indices) == 10:\n",
    "        break\n",
    "\n",
    "# Visualize each digit at different noise levels\n",
    "timesteps = [0, 250, 500, 750, 999]\n",
    "\n",
    "fig, axes = plt.subplots(10, len(timesteps), figsize=(12, 24))\n",
    "\n",
    "for digit in range(10):\n",
    "    image, _ = train_dataset[digit_indices[digit]]\n",
    "    image = image.unsqueeze(0)\n",
    "    noise = torch.randn_like(image)\n",
    "    \n",
    "    for j, t in enumerate(timesteps):\n",
    "        # Add noise (simplified - use scheduler in actual code)\n",
    "        alpha = 1.0 - (t / 1000)\n",
    "        noisy = alpha**0.5 * image + (1 - alpha)**0.5 * noise\n",
    "        \n",
    "        img = noisy.squeeze().numpy()\n",
    "        img = (img + 1) / 2\n",
    "        \n",
    "        axes[digit, j].imshow(img, cmap='gray', vmin=0, vmax=1)\n",
    "        axes[digit, j].axis('off')\n",
    "        if digit == 0:\n",
    "            axes[digit, j].set_title(f't={t}')\n",
    "\n",
    "plt.suptitle('Forward Diffusion on All MNIST Digits', fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1: Class-Conditional Generation\n",
    "\n",
    "Add class conditioning to generate specific digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Class-Conditional U-Net\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "def get_timestep_embedding(timesteps, embedding_dim):\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = math.log(10000) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, device=timesteps.device) * -emb)\n",
    "    emb = timesteps[:, None].float() * emb[None, :]\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "    return emb\n",
    "\n",
    "class ConditionalResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        self.norm1 = nn.GroupNorm(8, out_channels)\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
    "        self.emb_proj = nn.Linear(emb_dim, out_channels)\n",
    "        \n",
    "        if in_channels != out_channels:\n",
    "            self.skip = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        else:\n",
    "            self.skip = nn.Identity()\n",
    "    \n",
    "    def forward(self, x, emb):\n",
    "        h = self.conv1(x)\n",
    "        h = self.norm1(h)\n",
    "        h = F.silu(h)\n",
    "        \n",
    "        # Add time+class embedding\n",
    "        h = h + self.emb_proj(emb)[:, :, None, None]\n",
    "        \n",
    "        h = self.conv2(h)\n",
    "        h = self.norm2(h)\n",
    "        h = F.silu(h)\n",
    "        \n",
    "        return h + self.skip(x)\n",
    "\n",
    "class ConditionalUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Class-conditional U-Net.\n",
    "    \n",
    "    Takes digit class as additional input to guide generation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64, time_emb=128, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.time_emb_dim = time_emb\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(time_emb, time_emb * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb * 4, time_emb),\n",
    "        )\n",
    "        \n",
    "        # Class embedding (learnable embedding for each digit)\n",
    "        self.class_emb = nn.Embedding(num_classes, time_emb)\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = ConditionalResidualBlock(in_ch, base_ch, time_emb)\n",
    "        self.enc2 = ConditionalResidualBlock(base_ch, base_ch * 2, time_emb)\n",
    "        self.enc3 = ConditionalResidualBlock(base_ch * 2, base_ch * 4, time_emb)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = ConditionalResidualBlock(base_ch * 4, base_ch * 4, time_emb)\n",
    "        \n",
    "        # Decoder\n",
    "        self.up3 = nn.ConvTranspose2d(base_ch * 4, base_ch * 4, 2, stride=2)\n",
    "        self.dec3 = ConditionalResidualBlock(base_ch * 8, base_ch * 2, time_emb)\n",
    "        self.up2 = nn.ConvTranspose2d(base_ch * 2, base_ch * 2, 2, stride=2)\n",
    "        self.dec2 = ConditionalResidualBlock(base_ch * 4, base_ch, time_emb)\n",
    "        self.up1 = nn.ConvTranspose2d(base_ch, base_ch, 2, stride=2)\n",
    "        self.dec1 = ConditionalResidualBlock(base_ch * 2, base_ch, time_emb)\n",
    "        \n",
    "        self.out = nn.Conv2d(base_ch, out_ch, 1)\n",
    "    \n",
    "    def forward(self, x, t, c):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Noisy image (B, 1, 28, 28)\n",
    "            t: Timesteps (B,)\n",
    "            c: Class labels (B,)\n",
    "        \"\"\"\n",
    "        # Compute combined embedding\n",
    "        t_emb = get_timestep_embedding(t, self.time_emb_dim)\n",
    "        t_emb = self.time_mlp(t_emb)\n",
    "        c_emb = self.class_emb(c)\n",
    "        emb = t_emb + c_emb  # Combine time and class!\n",
    "        \n",
    "        # Encoder\n",
    "        e1 = self.enc1(x, emb)\n",
    "        e2 = self.enc2(self.pool(e1), emb)\n",
    "        e3 = self.enc3(self.pool(e2), emb)\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(self.pool(e3), emb)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        d3 = self.up3(b)\n",
    "        d3 = F.interpolate(d3, size=e3.shape[2:])\n",
    "        d3 = self.dec3(torch.cat([d3, e3], dim=1), emb)\n",
    "        \n",
    "        d2 = self.up2(d3)\n",
    "        d2 = F.interpolate(d2, size=e2.shape[2:])\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1), emb)\n",
    "        \n",
    "        d1 = self.up1(d2)\n",
    "        d1 = F.interpolate(d1, size=e1.shape[2:])\n",
    "        d1 = self.dec1(torch.cat([d1, e1], dim=1), emb)\n",
    "        \n",
    "        return self.out(d1)\n",
    "\n",
    "# Test\n",
    "model = ConditionalUNet()\n",
    "x = torch.randn(4, 1, 28, 28)\n",
    "t = torch.randint(0, 1000, (4,))\n",
    "c = torch.randint(0, 10, (4,))  # Class labels\n",
    "out = model(x, t, c)\n",
    "print(f\"Input: {x.shape}\")\n",
    "print(f\"Output: {out.shape}\")\n",
    "print(f\"Class labels: {c.tolist()}\")\n",
    "print(\"\\nâœ… Class-conditional U-Net works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2: Fashion-MNIST\n",
    "\n",
    "Adapt the model for Fashion-MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Load Fashion-MNIST\n",
    "from torchvision import datasets\n",
    "\n",
    "fashion_dataset = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "# Class names\n",
    "fashion_classes = [\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "]\n",
    "\n",
    "# Show samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    img, label = fashion_dataset[i * 1000]\n",
    "    ax.imshow(img.squeeze().numpy() * 0.5 + 0.5, cmap='gray')\n",
    "    ax.set_title(fashion_classes[label])\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Fashion-MNIST Samples')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTo train on Fashion-MNIST:\")\n",
    "print(\"1. Replace train_dataset with fashion_dataset\")\n",
    "print(\"2. Train with same hyperparameters\")\n",
    "print(\"3. May need more epochs (20-50) for good quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Forward diffusion** gradually adds Gaussian noise to images\n",
    "2. **Cosine schedule** works better than linear by preserving signal longer\n",
    "3. **U-Net** architecture uses skip connections for denoising\n",
    "4. **Timestep embeddings** help the model know the noise level\n",
    "5. **Class conditioning** enables controllable generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
