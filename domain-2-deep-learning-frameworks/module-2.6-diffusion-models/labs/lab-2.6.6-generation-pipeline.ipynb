{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.6.6: Image Generation Pipeline - End-to-End System\n",
    "\n",
    "**Module:** 2.6 - Diffusion Models  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** â­â­â­ (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Build a complete image generation pipeline\n",
    "- [ ] Implement seed management for reproducibility\n",
    "- [ ] Add batch generation with variations\n",
    "- [ ] Create a prompt templating system\n",
    "- [ ] Add automatic captioning of outputs\n",
    "- [ ] (Optional) Build a Gradio interface\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Prerequisites\n",
    "\n",
    "- Completed: Labs 2.6.1-2.6.5\n",
    "- Knowledge of: All previous diffusion model concepts\n",
    "- Installed: `diffusers`, `transformers`, `gradio` (optional)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ Real-World Context\n",
    "\n",
    "**Production image generation systems need:**\n",
    "\n",
    "- **Reproducibility**: Same seed = same image\n",
    "- **Batch processing**: Generate many images efficiently\n",
    "- **Variations**: Explore different versions of a concept\n",
    "- **Metadata**: Track what was generated and how\n",
    "- **UI**: Let non-technical users interact with the system\n",
    "\n",
    "This lab combines everything you've learned into a reusable pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setting Up the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "# Diffusers\n",
    "from diffusers import (\n",
    "    StableDiffusionXLPipeline,\n",
    "    DPMSolverMultistepScheduler,\n",
    ")\n",
    "\n",
    "# Image handling\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GenerationConfig:\n",
    "    \"\"\"\n",
    "    Configuration for image generation.\n",
    "    \n",
    "    All parameters needed to reproduce a generation.\n",
    "    \"\"\"\n",
    "    prompt: str\n",
    "    negative_prompt: str = \"blurry, low quality, distorted, ugly, deformed\"\n",
    "    width: int = 1024\n",
    "    height: int = 1024\n",
    "    num_inference_steps: int = 30\n",
    "    guidance_scale: float = 7.5\n",
    "    seed: Optional[int] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.seed is None:\n",
    "            self.seed = random.randint(0, 2**32 - 1)\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return asdict(self)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data: Dict) -> 'GenerationConfig':\n",
    "        return cls(**data)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GenerationResult:\n",
    "    \"\"\"\n",
    "    Result of an image generation.\n",
    "    \n",
    "    Contains the image and metadata for reproducibility.\n",
    "    \"\"\"\n",
    "    image: Image.Image\n",
    "    config: GenerationConfig\n",
    "    generation_time: float\n",
    "    timestamp: str = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.timestamp is None:\n",
    "            self.timestamp = datetime.now().isoformat()\n",
    "    \n",
    "    def save(self, output_dir: str, name: str = None):\n",
    "        \"\"\"Save image and metadata.\"\"\"\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        if name is None:\n",
    "            name = f\"gen_{self.config.seed}\"\n",
    "        \n",
    "        # Save image\n",
    "        image_path = output_dir / f\"{name}.png\"\n",
    "        self.image.save(image_path)\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'config': self.config.to_dict(),\n",
    "            'generation_time': self.generation_time,\n",
    "            'timestamp': self.timestamp,\n",
    "        }\n",
    "        metadata_path = output_dir / f\"{name}.json\"\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        return image_path, metadata_path\n",
    "\n",
    "\n",
    "print(\"Data classes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageGenerationPipeline:\n",
    "    \"\"\"\n",
    "    A complete image generation pipeline.\n",
    "    \n",
    "    Features:\n",
    "    - Reproducible generation with seeds\n",
    "    - Batch generation\n",
    "    - Variation generation\n",
    "    - Automatic metadata saving\n",
    "    - Prompt templates\n",
    "    \n",
    "    Example:\n",
    "        pipeline = ImageGenerationPipeline()\n",
    "        \n",
    "        # Single image\n",
    "        result = pipeline.generate(\"a beautiful sunset\")\n",
    "        \n",
    "        # Batch with variations\n",
    "        results = pipeline.generate_batch(\n",
    "            \"a cat\",\n",
    "            num_images=4,\n",
    "            variation_mode=\"seed\",\n",
    "        )\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str = \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "        dtype: torch.dtype = torch.bfloat16,\n",
    "    ):\n",
    "        self.model_id = model_id\n",
    "        self.dtype = dtype\n",
    "        self.pipe = None\n",
    "        self.is_loaded = False\n",
    "        \n",
    "        # Prompt templates\n",
    "        self.templates = {\n",
    "            'photorealistic': \"{subject}, professional photography, 8K resolution, sharp focus, natural lighting\",\n",
    "            'artistic': \"{subject}, digital art, trending on artstation, highly detailed, vibrant colors\",\n",
    "            'anime': \"{subject}, anime style, Studio Ghibli inspired, beautiful, detailed\",\n",
    "            'oil_painting': \"{subject}, oil painting, classical style, rich colors, masterpiece\",\n",
    "            'watercolor': \"{subject}, watercolor painting, soft colors, artistic, delicate brushstrokes\",\n",
    "            'fantasy': \"{subject}, fantasy art, magical, ethereal lighting, epic, detailed\",\n",
    "            'cyberpunk': \"{subject}, cyberpunk style, neon lights, futuristic, dark, atmospheric\",\n",
    "            'minimalist': \"{subject}, minimalist style, simple, clean, modern, elegant\",\n",
    "        }\n",
    "        \n",
    "        # Default negative prompts by style\n",
    "        self.negative_templates = {\n",
    "            'photorealistic': \"cartoon, anime, illustration, painting, drawing, art, 3D render\",\n",
    "            'artistic': \"photo, photograph, realistic, 3D, render\",\n",
    "            'default': \"blurry, low quality, distorted, ugly, deformed, watermark, text\",\n",
    "        }\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the diffusion model.\"\"\"\n",
    "        if self.is_loaded:\n",
    "            return\n",
    "        \n",
    "        print(f\"Loading {self.model_id}...\")\n",
    "        start = time.time()\n",
    "        \n",
    "        self.pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "            self.model_id,\n",
    "            torch_dtype=self.dtype,\n",
    "            variant=\"fp16\",\n",
    "            use_safetensors=True,\n",
    "        )\n",
    "        self.pipe = self.pipe.to(device)\n",
    "        self.pipe.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "            self.pipe.scheduler.config\n",
    "        )\n",
    "        \n",
    "        self.is_loaded = True\n",
    "        print(f\"âœ… Loaded in {time.time() - start:.1f}s\")\n",
    "    \n",
    "    def unload_model(self):\n",
    "        \"\"\"Unload model to free memory.\"\"\"\n",
    "        if self.pipe:\n",
    "            del self.pipe\n",
    "            self.pipe = None\n",
    "        self.is_loaded = False\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Model unloaded\")\n",
    "    \n",
    "    def apply_template(self, subject: str, style: str = None) -> str:\n",
    "        \"\"\"Apply a prompt template to a subject.\"\"\"\n",
    "        if style is None or style not in self.templates:\n",
    "            return subject\n",
    "        return self.templates[style].format(subject=subject)\n",
    "    \n",
    "    def get_negative_prompt(self, style: str = None) -> str:\n",
    "        \"\"\"Get appropriate negative prompt for style.\"\"\"\n",
    "        if style in self.negative_templates:\n",
    "            return self.negative_templates[style] + \", \" + self.negative_templates['default']\n",
    "        return self.negative_templates['default']\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        style: str = None,\n",
    "        config: GenerationConfig = None,\n",
    "        **kwargs,\n",
    "    ) -> GenerationResult:\n",
    "        \"\"\"\n",
    "        Generate a single image.\n",
    "        \n",
    "        Args:\n",
    "            prompt: Text prompt or subject (if using template)\n",
    "            style: Optional style template to apply\n",
    "            config: Optional full configuration\n",
    "            **kwargs: Override config parameters\n",
    "        \n",
    "        Returns:\n",
    "            GenerationResult with image and metadata\n",
    "        \"\"\"\n",
    "        self.load_model()\n",
    "        \n",
    "        # Apply template if style specified\n",
    "        full_prompt = self.apply_template(prompt, style)\n",
    "        negative = self.get_negative_prompt(style)\n",
    "        \n",
    "        # Create or update config\n",
    "        if config is None:\n",
    "            config = GenerationConfig(\n",
    "                prompt=full_prompt,\n",
    "                negative_prompt=negative,\n",
    "                **kwargs,\n",
    "            )\n",
    "        \n",
    "        # Generate\n",
    "        generator = torch.Generator(device=device).manual_seed(config.seed)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        \n",
    "        image = self.pipe(\n",
    "            prompt=config.prompt,\n",
    "            negative_prompt=config.negative_prompt,\n",
    "            width=config.width,\n",
    "            height=config.height,\n",
    "            num_inference_steps=config.num_inference_steps,\n",
    "            guidance_scale=config.guidance_scale,\n",
    "            generator=generator,\n",
    "        ).images[0]\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        gen_time = time.time() - start\n",
    "        \n",
    "        return GenerationResult(\n",
    "            image=image,\n",
    "            config=config,\n",
    "            generation_time=gen_time,\n",
    "        )\n",
    "    \n",
    "    def generate_batch(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        num_images: int = 4,\n",
    "        style: str = None,\n",
    "        variation_mode: str = \"seed\",\n",
    "        base_seed: int = None,\n",
    "        **kwargs,\n",
    "    ) -> List[GenerationResult]:\n",
    "        \"\"\"\n",
    "        Generate multiple images.\n",
    "        \n",
    "        Args:\n",
    "            prompt: Text prompt or subject\n",
    "            num_images: Number of images to generate\n",
    "            style: Optional style template\n",
    "            variation_mode: \"seed\" (different seeds) or \"guidance\" (different guidance)\n",
    "            base_seed: Starting seed for variations\n",
    "            **kwargs: Additional config parameters\n",
    "        \n",
    "        Returns:\n",
    "            List of GenerationResults\n",
    "        \"\"\"\n",
    "        if base_seed is None:\n",
    "            base_seed = random.randint(0, 2**32 - 1)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i in range(num_images):\n",
    "            print(f\"Generating {i+1}/{num_images}...\")\n",
    "            \n",
    "            if variation_mode == \"seed\":\n",
    "                seed = base_seed + i\n",
    "                guidance = kwargs.get('guidance_scale', 7.5)\n",
    "            elif variation_mode == \"guidance\":\n",
    "                seed = base_seed\n",
    "                guidances = [5.0, 7.0, 9.0, 12.0]\n",
    "                guidance = guidances[i % len(guidances)]\n",
    "            else:\n",
    "                seed = base_seed + i\n",
    "                guidance = kwargs.get('guidance_scale', 7.5)\n",
    "            \n",
    "            result = self.generate(\n",
    "                prompt=prompt,\n",
    "                style=style,\n",
    "                seed=seed,\n",
    "                guidance_scale=guidance,\n",
    "                **kwargs,\n",
    "            )\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_style_comparison(\n",
    "        self,\n",
    "        subject: str,\n",
    "        styles: List[str] = None,\n",
    "        seed: int = None,\n",
    "    ) -> Dict[str, GenerationResult]:\n",
    "        \"\"\"\n",
    "        Generate the same subject in different styles.\n",
    "        \n",
    "        Args:\n",
    "            subject: Subject description (without style)\n",
    "            styles: List of styles to try (uses all if None)\n",
    "            seed: Seed for consistency\n",
    "        \n",
    "        Returns:\n",
    "            Dict mapping style name to GenerationResult\n",
    "        \"\"\"\n",
    "        if styles is None:\n",
    "            styles = list(self.templates.keys())\n",
    "        \n",
    "        if seed is None:\n",
    "            seed = random.randint(0, 2**32 - 1)\n",
    "        \n",
    "        results = {}\n",
    "        for style in styles:\n",
    "            print(f\"Generating {style} style...\")\n",
    "            result = self.generate(subject, style=style, seed=seed)\n",
    "            results[style] = result\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "print(\"ImageGenerationPipeline defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Using the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pipeline\n",
    "pipeline = ImageGenerationPipeline()\n",
    "\n",
    "print(\"\\nAvailable style templates:\")\n",
    "for name, template in pipeline.templates.items():\n",
    "    print(f\"  {name}: {template[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single generation with template\n",
    "result = pipeline.generate(\n",
    "    \"a majestic lion\",\n",
    "    style=\"photorealistic\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"\\nGeneration complete!\")\n",
    "print(f\"  Prompt: {result.config.prompt}\")\n",
    "print(f\"  Seed: {result.config.seed}\")\n",
    "print(f\"  Time: {result.generation_time:.2f}s\")\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(result.image)\n",
    "plt.title(f\"Seed: {result.config.seed}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch generation with seed variations\n",
    "results = pipeline.generate_batch(\n",
    "    \"a cozy cabin in the woods\",\n",
    "    num_images=4,\n",
    "    style=\"artistic\",\n",
    "    variation_mode=\"seed\",\n",
    "    base_seed=100,\n",
    ")\n",
    "\n",
    "# Display all results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 14))\n",
    "for ax, result in zip(axes.flatten(), results):\n",
    "    ax.imshow(result.image)\n",
    "    ax.set_title(f\"Seed: {result.config.seed}\")\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Seed Variations\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style comparison\n",
    "style_results = pipeline.generate_style_comparison(\n",
    "    \"a serene Japanese garden\",\n",
    "    styles=[\"photorealistic\", \"watercolor\", \"anime\", \"cyberpunk\"],\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Display\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 14))\n",
    "for ax, (style, result) in zip(axes.flatten(), style_results.items()):\n",
    "    ax.imshow(result.image)\n",
    "    ax.set_title(f\"{style} ({result.generation_time:.1f}s)\")\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Style Comparison: Same Subject, Different Styles\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Saving and Reproducing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results with metadata\n",
    "output_dir = Path(\"./pipeline_output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Generate and save\n",
    "result = pipeline.generate(\n",
    "    \"a mystical dragon\",\n",
    "    style=\"fantasy\",\n",
    "    seed=12345,\n",
    ")\n",
    "\n",
    "image_path, metadata_path = result.save(output_dir, \"dragon_fantasy\")\n",
    "\n",
    "print(f\"Saved to:\")\n",
    "print(f\"  Image: {image_path}\")\n",
    "print(f\"  Metadata: {metadata_path}\")\n",
    "\n",
    "# Show saved metadata\n",
    "print(f\"\\nMetadata contents:\")\n",
    "with open(metadata_path) as f:\n",
    "    print(json.dumps(json.load(f), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce from metadata\n",
    "def reproduce_from_metadata(metadata_path: str, pipeline: ImageGenerationPipeline) -> GenerationResult:\n",
    "    \"\"\"Reproduce an image from saved metadata.\"\"\"\n",
    "    with open(metadata_path) as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    config = GenerationConfig.from_dict(metadata['config'])\n",
    "    return pipeline.generate(config=config, prompt=config.prompt)\n",
    "\n",
    "# Reproduce the dragon\n",
    "reproduced = reproduce_from_metadata(metadata_path, pipeline)\n",
    "\n",
    "# Compare original and reproduced\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "axes[0].imshow(result.image)\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(reproduced.image)\n",
    "axes[1].set_title(\"Reproduced from Metadata\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.suptitle(\"Reproducibility Test\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Same seed = identical image!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Custom Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptBuilder:\n",
    "    \"\"\"\n",
    "    Advanced prompt builder with modular components.\n",
    "    \n",
    "    Builds prompts from:\n",
    "    - Subject\n",
    "    - Style\n",
    "    - Lighting\n",
    "    - Camera angle\n",
    "    - Quality modifiers\n",
    "    \"\"\"\n",
    "    \n",
    "    STYLES = {\n",
    "        'photo': 'professional photography',\n",
    "        'oil': 'oil painting',\n",
    "        'watercolor': 'watercolor painting',\n",
    "        'digital': 'digital art',\n",
    "        'anime': 'anime style',\n",
    "        '3d': '3D render',\n",
    "        'sketch': 'pencil sketch',\n",
    "    }\n",
    "    \n",
    "    LIGHTING = {\n",
    "        'golden': 'golden hour lighting',\n",
    "        'studio': 'studio lighting',\n",
    "        'dramatic': 'dramatic lighting',\n",
    "        'soft': 'soft natural lighting',\n",
    "        'neon': 'neon lights',\n",
    "        'moonlight': 'moonlit scene',\n",
    "    }\n",
    "    \n",
    "    CAMERA = {\n",
    "        'closeup': 'close-up shot',\n",
    "        'wide': 'wide angle shot',\n",
    "        'portrait': 'portrait shot',\n",
    "        'aerial': 'aerial view',\n",
    "        'macro': 'macro photography',\n",
    "    }\n",
    "    \n",
    "    QUALITY = {\n",
    "        'basic': 'detailed',\n",
    "        'good': 'highly detailed, sharp focus',\n",
    "        'best': 'highly detailed, 8K resolution, sharp focus, professional',\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def build(\n",
    "        cls,\n",
    "        subject: str,\n",
    "        style: str = 'photo',\n",
    "        lighting: str = None,\n",
    "        camera: str = None,\n",
    "        quality: str = 'good',\n",
    "        extras: List[str] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Build a complete prompt from components.\n",
    "        \n",
    "        Example:\n",
    "            prompt = PromptBuilder.build(\n",
    "                subject=\"a red fox in a forest\",\n",
    "                style=\"photo\",\n",
    "                lighting=\"golden\",\n",
    "                camera=\"closeup\",\n",
    "                quality=\"best\",\n",
    "            )\n",
    "        \"\"\"\n",
    "        parts = [subject]\n",
    "        \n",
    "        if style in cls.STYLES:\n",
    "            parts.append(cls.STYLES[style])\n",
    "        \n",
    "        if lighting in cls.LIGHTING:\n",
    "            parts.append(cls.LIGHTING[lighting])\n",
    "        \n",
    "        if camera in cls.CAMERA:\n",
    "            parts.append(cls.CAMERA[camera])\n",
    "        \n",
    "        if quality in cls.QUALITY:\n",
    "            parts.append(cls.QUALITY[quality])\n",
    "        \n",
    "        if extras:\n",
    "            parts.extend(extras)\n",
    "        \n",
    "        return \", \".join(parts)\n",
    "\n",
    "\n",
    "# Test the prompt builder\n",
    "prompt = PromptBuilder.build(\n",
    "    subject=\"a red fox in a forest\",\n",
    "    style=\"photo\",\n",
    "    lighting=\"golden\",\n",
    "    camera=\"closeup\",\n",
    "    quality=\"best\",\n",
    ")\n",
    "\n",
    "print(\"Built prompt:\")\n",
    "print(f\"  {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate using the prompt builder\n",
    "built_prompt = PromptBuilder.build(\n",
    "    subject=\"a wise old owl perched on a branch\",\n",
    "    style=\"digital\",\n",
    "    lighting=\"moonlight\",\n",
    "    camera=\"portrait\",\n",
    "    quality=\"best\",\n",
    "    extras=[\"mystical atmosphere\", \"stars in background\"],\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {built_prompt}\\n\")\n",
    "\n",
    "result = pipeline.generate(\n",
    "    built_prompt,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(result.image)\n",
    "plt.title(\"Generated with PromptBuilder\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Automatic Captioning (Optional)\n",
    "\n",
    "Let's add automatic description generation for our outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load BLIP for captioning (optional)\n",
    "try:\n",
    "    from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "    \n",
    "    class ImageCaptioner:\n",
    "        \"\"\"Generate captions for images using BLIP.\"\"\"\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.processor = BlipProcessor.from_pretrained(\n",
    "                \"Salesforce/blip-image-captioning-base\"\n",
    "            )\n",
    "            self.model = BlipForConditionalGeneration.from_pretrained(\n",
    "                \"Salesforce/blip-image-captioning-base\",\n",
    "                torch_dtype=torch.bfloat16,\n",
    "            ).to(device)\n",
    "        \n",
    "        def caption(self, image: Image.Image) -> str:\n",
    "            \"\"\"Generate a caption for an image.\"\"\"\n",
    "            inputs = self.processor(image, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = self.model.generate(**inputs, max_new_tokens=50)\n",
    "            \n",
    "            return self.processor.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Initialize captioner\n",
    "    captioner = ImageCaptioner()\n",
    "    print(\"âœ… Image captioner loaded!\")\n",
    "    \n",
    "    # Test captioning on our generated image\n",
    "    caption = captioner.caption(result.image)\n",
    "    print(f\"\\nGenerated caption: {caption}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"BLIP not available. Install with: pip install transformers\")\n",
    "    captioner = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Gradio Interface (Optional)\n",
    "\n",
    "Let's create a simple web interface for our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import gradio as gr\n",
    "    \n",
    "    def generate_interface(\n",
    "        prompt: str,\n",
    "        style: str,\n",
    "        guidance: float,\n",
    "        steps: int,\n",
    "        seed: int,\n",
    "    ):\n",
    "        \"\"\"Gradio interface function.\"\"\"\n",
    "        if seed == -1:\n",
    "            seed = random.randint(0, 2**32 - 1)\n",
    "        \n",
    "        result = pipeline.generate(\n",
    "            prompt=prompt,\n",
    "            style=style if style != \"none\" else None,\n",
    "            guidance_scale=guidance,\n",
    "            num_inference_steps=steps,\n",
    "            seed=seed,\n",
    "        )\n",
    "        \n",
    "        info = f\"Seed: {result.config.seed}\\nTime: {result.generation_time:.1f}s\"\n",
    "        return result.image, info\n",
    "    \n",
    "    # Create Gradio interface\n",
    "    demo = gr.Interface(\n",
    "        fn=generate_interface,\n",
    "        inputs=[\n",
    "            gr.Textbox(label=\"Prompt\", placeholder=\"Enter your prompt...\"),\n",
    "            gr.Dropdown(\n",
    "                choices=[\"none\"] + list(pipeline.templates.keys()),\n",
    "                value=\"none\",\n",
    "                label=\"Style Template\",\n",
    "            ),\n",
    "            gr.Slider(1, 15, value=7.5, label=\"Guidance Scale\"),\n",
    "            gr.Slider(10, 50, value=25, step=5, label=\"Steps\"),\n",
    "            gr.Number(value=-1, label=\"Seed (-1 for random)\"),\n",
    "        ],\n",
    "        outputs=[\n",
    "            gr.Image(label=\"Generated Image\"),\n",
    "            gr.Textbox(label=\"Generation Info\"),\n",
    "        ],\n",
    "        title=\"Image Generation Pipeline\",\n",
    "        description=\"Generate images with SDXL on DGX Spark\",\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Gradio interface created!\")\n",
    "    print(\"Run demo.launch() to start the web interface\")\n",
    "    \n",
    "    # Uncomment to launch:\n",
    "    # demo.launch(share=True)\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Gradio not available. Install with: pip install gradio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Complete Pipeline Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline demonstration\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPLETE PIPELINE DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Build prompt with PromptBuilder\n",
    "print(\"\\n1. Building prompt...\")\n",
    "prompt = PromptBuilder.build(\n",
    "    subject=\"a futuristic cityscape\",\n",
    "    style=\"digital\",\n",
    "    lighting=\"neon\",\n",
    "    quality=\"best\",\n",
    "    extras=[\"flying cars\", \"holographic billboards\"],\n",
    ")\n",
    "print(f\"   Prompt: {prompt[:70]}...\")\n",
    "\n",
    "# 2. Generate image\n",
    "print(\"\\n2. Generating image...\")\n",
    "result = pipeline.generate(prompt, seed=42)\n",
    "print(f\"   Time: {result.generation_time:.2f}s\")\n",
    "\n",
    "# 3. Save with metadata\n",
    "print(\"\\n3. Saving with metadata...\")\n",
    "image_path, metadata_path = result.save(output_dir, \"futuristic_city\")\n",
    "print(f\"   Saved to: {image_path}\")\n",
    "\n",
    "# 4. Caption (if available)\n",
    "if captioner:\n",
    "    print(\"\\n4. Auto-captioning...\")\n",
    "    caption = captioner.caption(result.image)\n",
    "    print(f\"   Caption: {caption}\")\n",
    "\n",
    "# 5. Display\n",
    "print(\"\\n5. Displaying result...\")\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(result.image)\n",
    "plt.title(f\"Seed: {result.config.seed} | Time: {result.generation_time:.1f}s\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Checkpoint - Module Complete!\n",
    "\n",
    "You've now completed the entire Diffusion Models module:\n",
    "\n",
    "### Labs Completed:\n",
    "- âœ… **Lab 2.6.1**: Diffusion theory and DDPM from scratch\n",
    "- âœ… **Lab 2.6.2**: SDXL generation and prompt engineering\n",
    "- âœ… **Lab 2.6.3**: ControlNet for guided generation\n",
    "- âœ… **Lab 2.6.4**: Flux architecture exploration\n",
    "- âœ… **Lab 2.6.5**: LoRA training for custom styles\n",
    "- âœ… **Lab 2.6.6**: Complete generation pipeline\n",
    "\n",
    "### Skills Acquired:\n",
    "- Understanding diffusion model theory\n",
    "- Text-to-image generation with SDXL and Flux\n",
    "- Structural control with ControlNet\n",
    "- Custom style training with LoRA\n",
    "- Building production-ready pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up all resources\n",
    "pipeline.unload_model()\n",
    "\n",
    "if captioner:\n",
    "    del captioner\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"All resources cleaned up!\")\n",
    "print(\"\\nðŸŽ‰ Congratulations on completing Module 2.6: Diffusion Models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you've mastered diffusion models, you're ready for:\n",
    "\n",
    "**Domain 3: LLM Systems** - Module 3.1: LLM Fine-Tuning\n",
    "\n",
    "You'll learn to fine-tune large language models using the same principles you learned here with LoRA!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
