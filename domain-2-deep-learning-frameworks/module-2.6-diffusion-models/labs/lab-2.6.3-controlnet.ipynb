{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 2.6.3: ControlNet Workshop - Guided Image Generation\n\n**Module:** 2.6 - Diffusion Models  \n**Time:** 2 hours  \n**Difficulty:** ‚≠ê‚≠ê‚≠ê (Intermediate)\n\n---\n\n## üéØ Learning Objectives\n\nBy the end of this notebook, you will:\n- [ ] Understand how ControlNet enables structural control\n- [ ] Use Canny edge detection for outline-guided generation\n- [ ] Apply depth maps for spatial composition\n- [ ] Control character poses using OpenPose\n- [ ] Combine multiple control methods\n- [ ] Create consistent characters across images\n\n---\n\n## üìö Prerequisites\n\n- Completed: Lab 2.6.2 (Stable Diffusion Generation)\n- Knowledge of: Basic SDXL usage, prompt engineering\n- **Required packages:**\n  - `diffusers>=0.27.0`\n  - `opencv-python>=4.8.0` (for edge detection)\n  - `controlnet_aux` (optional, for advanced preprocessors)\n\n**Install if needed:**\n```bash\npip install opencv-python>=4.8.0\npip install controlnet_aux  # Optional: for pose, depth, etc.\n```\n\n---\n\n## üåç Real-World Context\n\n**ControlNet solves the \"I know what I want but can't describe it\" problem:**\n\n- **Architects** sketch a building outline ‚Üí get photorealistic renders\n- **Game developers** use character poses ‚Üí generate concept art\n- **Interior designers** provide room layouts ‚Üí visualize different styles\n- **Fashion designers** sketch silhouettes ‚Üí see clothing designs\n\nIt transforms AI from \"creative assistant\" to \"precise tool\"!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What is ControlNet?\n",
    "\n",
    "> **Imagine you're teaching someone to color in a coloring book:**\n",
    ">\n",
    "> Without ControlNet:\n",
    "> - \"Draw a cat\" ‚Üí They draw whatever cat they imagine\n",
    ">\n",
    "> With ControlNet:\n",
    "> - You give them an outline of a specific cat\n",
    "> - \"Color this cat\" ‚Üí They follow YOUR lines!\n",
    ">\n",
    "> **ControlNet adds \"training wheels\" to diffusion models:**\n",
    "> - **Canny edges**: \"Follow these outlines\"\n",
    "> - **Depth map**: \"Near things here, far things there\"\n",
    "> - **Pose skeleton**: \"The person should stand like this\"\n",
    "> - **Segmentation**: \"Sky here, ground there, tree here\"\n",
    "\n",
    "### How It Works\n",
    "\n",
    "```\n",
    "Control Image (edges/depth/pose)    Text Prompt\n",
    "         ‚îÇ                              ‚îÇ\n",
    "         ‚ñº                              ‚ñº\n",
    "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "  ‚îÇ  ControlNet ‚îÇ               ‚îÇ    CLIP     ‚îÇ\n",
    "  ‚îÇ  (Encoder)  ‚îÇ               ‚îÇ  (Encoder)  ‚îÇ\n",
    "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ                              ‚îÇ\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                    ‚ñº\n",
    "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "              ‚îÇ   U-Net  ‚îÇ  ‚óÑ‚îÄ‚îÄ Noise\n",
    "              ‚îÇ(Denoiser)‚îÇ\n",
    "              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                   ‚îÇ\n",
    "                   ‚ñº\n",
    "            Generated Image\n",
    "       (follows structure + prompt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Core imports\nimport torch\nimport gc\nimport time\nimport numpy as np\nfrom pathlib import Path\n\n# Diffusers\nfrom diffusers import (\n    StableDiffusionXLControlNetPipeline,\n    ControlNetModel,\n    AutoencoderKL,\n)\nfrom diffusers.utils import load_image\n\n# Image processing\nfrom PIL import Image\nimport cv2\n\n# Visualization\nimport matplotlib.pyplot as plt\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name()}\")\n    total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"Total Memory: {total_mem:.1f} GB\")\n    print(f\"\\nDGX Spark's 128GB unified memory = ControlNet at full precision! üöÄ\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Helper functions\ndef show_images_side_by_side(images, titles, figsize=(15, 5)):\n    \"\"\"Display images side by side for comparison.\"\"\"\n    n = len(images)\n    fig, axes = plt.subplots(1, n, figsize=figsize)\n    if n == 1:\n        axes = [axes]\n    \n    for ax, img, title in zip(axes, images, titles):\n        if isinstance(img, np.ndarray):\n            if len(img.shape) == 2:  # Grayscale\n                ax.imshow(img, cmap='gray')\n            else:\n                ax.imshow(img)\n        else:\n            ax.imshow(img)\n        ax.set_title(title, fontsize=11)\n        ax.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ndef get_memory_usage():\n    \"\"\"Get current GPU memory usage.\"\"\"\n    if torch.cuda.is_available():\n        allocated = torch.cuda.memory_allocated() / 1e9\n        reserved = torch.cuda.memory_reserved() / 1e9\n        return f\"Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\"\n    return \"No GPU\"\n\ndef create_sample_image(size=(1024, 1024)):\n    \"\"\"Create a simple sample image for testing.\"\"\"\n    # Create a simple geometric composition\n    img = np.zeros((*size, 3), dtype=np.uint8)\n    img[:] = (200, 200, 200)  # Light gray background\n    \n    # Add some shapes\n    center = (size[0] // 2, size[1] // 2)\n    cv2.circle(img, center, size[0] // 4, (100, 100, 100), -1)\n    cv2.rectangle(img, (size[0]//4, size[1]//4), (3*size[0]//4, 3*size[1]//4), (50, 50, 50), 3)\n    \n    return Image.fromarray(img)\n\nprint(\"Helper functions ready!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Canny Edge ControlNet\n",
    "\n",
    "### üßí ELI5: Canny Edges\n",
    "\n",
    "> **Canny edge detection is like tracing the outlines in a picture:**\n",
    ">\n",
    "> Take a photo of your room ‚Üí Canny finds all the edges of furniture, walls, windows\n",
    "> ‚Üí Now you can \"color in\" those outlines in any style!\n",
    ">\n",
    "> Perfect for:\n",
    "> - Keeping the structure of a scene while changing the style\n",
    "> - Using sketches as input\n",
    "> - Maintaining architectural details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ControlNet for Canny edges\n",
    "print(\"Loading Canny ControlNet...\")\n",
    "print(f\"Memory before: {get_memory_usage()}\")\n",
    "\n",
    "controlnet_canny = ControlNetModel.from_pretrained(\n",
    "    \"diffusers/controlnet-canny-sdxl-1.0\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "\n",
    "# Load the pipeline with ControlNet\n",
    "pipe_canny = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    controlnet=controlnet_canny,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "pipe_canny = pipe_canny.to(device)\n",
    "\n",
    "print(f\"\\n‚úÖ Canny ControlNet loaded!\")\n",
    "print(f\"Memory after: {get_memory_usage()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_canny_edges(image, low_threshold=100, high_threshold=200):\n",
    "    \"\"\"\n",
    "    Extract Canny edges from an image.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image or numpy array\n",
    "        low_threshold: Lower threshold for edge detection\n",
    "        high_threshold: Upper threshold for edge detection\n",
    "    \n",
    "    Returns:\n",
    "        PIL Image with edges (white on black)\n",
    "    \"\"\"\n",
    "    if isinstance(image, Image.Image):\n",
    "        image = np.array(image)\n",
    "    \n",
    "    # Convert to grayscale if needed\n",
    "    if len(image.shape) == 3:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray = image\n",
    "    \n",
    "    # Apply Canny edge detection\n",
    "    edges = cv2.Canny(gray, low_threshold, high_threshold)\n",
    "    \n",
    "    # Convert to 3-channel for ControlNet\n",
    "    edges_3ch = np.stack([edges, edges, edges], axis=-1)\n",
    "    \n",
    "    return Image.fromarray(edges_3ch)\n",
    "\n",
    "# Test with a sample image\n",
    "# You can replace this with any image URL\n",
    "sample_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a7/Camponotus_flavomarginatus_ant.jpg/640px-Camponotus_flavomarginatus_ant.jpg\"\n",
    "\n",
    "try:\n",
    "    original_image = load_image(sample_url)\n",
    "    original_image = original_image.resize((1024, 1024))\n",
    "except:\n",
    "    print(\"Could not load sample image, creating a synthetic one...\")\n",
    "    original_image = create_sample_image()\n",
    "\n",
    "# Get edges\n",
    "canny_edges = get_canny_edges(original_image, low_threshold=100, high_threshold=200)\n",
    "\n",
    "# Display\n",
    "show_images_side_by_side(\n",
    "    [original_image, canny_edges],\n",
    "    [\"Original Image\", \"Canny Edges\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images with different styles using the same edges\n",
    "prompts = [\n",
    "    \"A detailed pencil sketch, artistic drawing, fine lines, on paper\",\n",
    "    \"A neon cyberpunk scene, glowing edges, futuristic, dark background\",\n",
    "    \"An oil painting, impressionist style, vibrant colors, artistic\",\n",
    "    \"A watercolor painting, soft colors, artistic, delicate brushstrokes\",\n",
    "]\n",
    "\n",
    "negative_prompt = \"blurry, low quality, distorted, ugly\"\n",
    "\n",
    "images = []\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"Generating {i+1}/{len(prompts)}: {prompt[:40]}...\")\n",
    "    \n",
    "    generator = torch.Generator(device=device).manual_seed(42)\n",
    "    \n",
    "    image = pipe_canny(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        image=canny_edges,\n",
    "        num_inference_steps=25,\n",
    "        guidance_scale=7.5,\n",
    "        controlnet_conditioning_scale=0.5,  # How strongly to follow edges\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    \n",
    "    images.append(image)\n",
    "\n",
    "# Display all results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Show original and edges first\n",
    "axes[0].imshow(original_image)\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(canny_edges)\n",
    "axes[1].set_title(\"Canny Edges (Control)\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Show generated images\n",
    "for i, (img, prompt) in enumerate(zip(images, prompts)):\n",
    "    axes[i+2].imshow(img)\n",
    "    axes[i+2].set_title(prompt[:30] + \"...\", fontsize=9)\n",
    "    axes[i+2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Notice how all images follow the same edge structure!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling the Strength\n",
    "\n",
    "The `controlnet_conditioning_scale` parameter controls how strictly the model follows the control image:\n",
    "\n",
    "- **0.0**: Ignore control completely (just text prompt)\n",
    "- **0.3-0.5**: Loose guidance (inspired by edges, not strict)\n",
    "- **0.7-0.8**: Moderate guidance (balanced)\n",
    "- **1.0**: Strong guidance (strict edge following)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different conditioning scales\n",
    "prompt = \"A fantasy castle with magical towers, digital art, highly detailed\"\n",
    "scales = [0.0, 0.3, 0.5, 0.7, 1.0]\n",
    "\n",
    "images = []\n",
    "for scale in scales:\n",
    "    print(f\"Generating with scale={scale}...\")\n",
    "    generator = torch.Generator(device=device).manual_seed(42)\n",
    "    \n",
    "    image = pipe_canny(\n",
    "        prompt=prompt,\n",
    "        image=canny_edges,\n",
    "        num_inference_steps=20,\n",
    "        guidance_scale=7.5,\n",
    "        controlnet_conditioning_scale=scale,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    \n",
    "    images.append(image)\n",
    "\n",
    "# Display\n",
    "fig, axes = plt.subplots(1, len(scales), figsize=(20, 4))\n",
    "for ax, img, scale in zip(axes, images, scales):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Scale: {scale}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"ControlNet Conditioning Scale Comparison\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observations:\")\n",
    "print(\"  - 0.0: Pure text-to-image (no edge guidance)\")\n",
    "print(\"  - 0.3: Loose inspiration from edges\")\n",
    "print(\"  - 0.5: Balanced (recommended for most cases)\")\n",
    "print(\"  - 1.0: Strict edge following\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Depth Map ControlNet\n",
    "\n",
    "### üßí ELI5: Depth Maps\n",
    "\n",
    "> **A depth map shows how far away things are:**\n",
    ">\n",
    "> - White = Close to camera\n",
    "> - Black = Far from camera\n",
    "> - Shades of gray = In between\n",
    ">\n",
    "> This lets you control the 3D composition:\n",
    "> - \"Put the subject in front, background far away\"\n",
    "> - \"Make a scene with proper perspective\"\n",
    "> - \"Control spatial relationships between objects\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up Canny pipeline to free memory\n",
    "del pipe_canny, controlnet_canny\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"Memory after cleanup: {get_memory_usage()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Depth ControlNet\n",
    "print(\"Loading Depth ControlNet...\")\n",
    "\n",
    "controlnet_depth = ControlNetModel.from_pretrained(\n",
    "    \"diffusers/controlnet-depth-sdxl-1.0\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "\n",
    "pipe_depth = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    controlnet=controlnet_depth,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "pipe_depth = pipe_depth.to(device)\n",
    "\n",
    "print(f\"\\n‚úÖ Depth ControlNet loaded!\")\n",
    "print(f\"Memory: {get_memory_usage()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Understanding `torch.hub.load()` for Pre-trained Models\n\nBefore we create our depth estimation function, let's understand `torch.hub.load()`:\n\n```python\n# torch.hub.load() downloads and loads pre-trained models from GitHub repos\n# Syntax: torch.hub.load('repo_owner/repo_name', 'model_name', trust_repo=True)\n\n# Example: Load MiDaS depth estimation model\nmidas = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS_small\", trust_repo=True)\n```\n\n**Key parameters:**\n- `'intel-isl/MiDaS'`: GitHub repository (owner/repo)\n- `'MiDaS_small'`: Specific model or function to load\n- `trust_repo=True`: Required to acknowledge you trust the code\n\n**Why use it?**\n- Access pre-trained models without manual download\n- Models are cached locally after first download\n- Many popular models available (MiDaS, YOLO, etc.)\n\n**Note:** This is OPTIONAL for this lab - we provide `create_synthetic_depth()` as a faster alternative.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_depth(image, model_type=\"MiDaS_small\"):\n",
    "    \"\"\"\n",
    "    Estimate depth from an image using MiDaS.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image\n",
    "        model_type: \"MiDaS_small\", \"DPT_Hybrid\", or \"DPT_Large\"\n",
    "    \n",
    "    Returns:\n",
    "        PIL Image with depth map\n",
    "    \"\"\"\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    # Load MiDaS model\n",
    "    midas = torch.hub.load(\"intel-isl/MiDaS\", model_type, trust_repo=True)\n",
    "    midas = midas.to(device).eval()\n",
    "    \n",
    "    # Load transforms\n",
    "    midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\", trust_repo=True)\n",
    "    if model_type in [\"DPT_Large\", \"DPT_Hybrid\"]:\n",
    "        transform = midas_transforms.dpt_transform\n",
    "    else:\n",
    "        transform = midas_transforms.small_transform\n",
    "    \n",
    "    # Prepare input\n",
    "    np_image = np.array(image)\n",
    "    input_batch = transform(np_image).to(device)\n",
    "    \n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        prediction = midas(input_batch)\n",
    "        prediction = F.interpolate(\n",
    "            prediction.unsqueeze(1),\n",
    "            size=np_image.shape[:2],\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        ).squeeze()\n",
    "    \n",
    "    # Normalize to 0-255\n",
    "    depth = prediction.cpu().numpy()\n",
    "    depth = (depth - depth.min()) / (depth.max() - depth.min()) * 255\n",
    "    depth = depth.astype(np.uint8)\n",
    "    \n",
    "    # Convert to 3-channel\n",
    "    depth_3ch = np.stack([depth, depth, depth], axis=-1)\n",
    "    \n",
    "    # Cleanup\n",
    "    del midas\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return Image.fromarray(depth_3ch)\n",
    "\n",
    "# Or create a synthetic depth map for testing\n",
    "def create_synthetic_depth(size=(1024, 1024)):\n",
    "    \"\"\"Create a synthetic depth map with gradient.\"\"\"\n",
    "    h, w = size\n",
    "    # Create radial gradient (center bright, edges dark)\n",
    "    y, x = np.ogrid[:h, :w]\n",
    "    center = (h // 2, w // 2)\n",
    "    dist = np.sqrt((x - center[1])**2 + (y - center[0])**2)\n",
    "    max_dist = np.sqrt(center[0]**2 + center[1]**2)\n",
    "    depth = 255 - (dist / max_dist * 255).astype(np.uint8)\n",
    "    depth_3ch = np.stack([depth, depth, depth], axis=-1)\n",
    "    return Image.fromarray(depth_3ch)\n",
    "\n",
    "print(\"Depth estimation functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or estimate depth map\n",
    "# Using synthetic for speed - you can use estimate_depth() for real images\n",
    "\n",
    "# Option 1: Synthetic depth (fast)\n",
    "depth_map = create_synthetic_depth((1024, 1024))\n",
    "\n",
    "# Option 2: From real image (slower but more realistic)\n",
    "# depth_map = estimate_depth(original_image)\n",
    "\n",
    "# Generate images with different prompts using depth control\n",
    "prompts = [\n",
    "    \"A mystical forest with ancient trees, foggy atmosphere, fantasy art\",\n",
    "    \"An underwater scene with coral reef, tropical fish, sunlight rays\",\n",
    "    \"A space scene with nebula and stars, cosmic, ethereal\",\n",
    "]\n",
    "\n",
    "images = []\n",
    "for prompt in prompts:\n",
    "    print(f\"Generating: {prompt[:40]}...\")\n",
    "    generator = torch.Generator(device=device).manual_seed(42)\n",
    "    \n",
    "    image = pipe_depth(\n",
    "        prompt=prompt,\n",
    "        image=depth_map,\n",
    "        num_inference_steps=25,\n",
    "        guidance_scale=7.5,\n",
    "        controlnet_conditioning_scale=0.5,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    \n",
    "    images.append(image)\n",
    "\n",
    "# Display\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "axes[0].imshow(depth_map)\n",
    "axes[0].set_title(\"Depth Map (Control)\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "for i, (img, prompt) in enumerate(zip(images, prompts)):\n",
    "    axes[i+1].imshow(img)\n",
    "    axes[i+1].set_title(prompt[:25] + \"...\", fontsize=10)\n",
    "    axes[i+1].axis('off')\n",
    "\n",
    "plt.suptitle(\"Depth-Controlled Generation\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Notice how the depth composition is preserved across different styles!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Creating Custom Control Images\n",
    "\n",
    "You can draw your own control images! This is powerful for creative control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_simple_scene():\n",
    "    \"\"\"\n",
    "    Draw a simple scene with lines for edge control.\n",
    "    This simulates a hand-drawn sketch.\n",
    "    \"\"\"\n",
    "    # Create blank canvas\n",
    "    img = np.zeros((1024, 1024, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Draw horizon line\n",
    "    cv2.line(img, (0, 512), (1024, 512), (255, 255, 255), 2)\n",
    "    \n",
    "    # Draw mountains\n",
    "    pts = np.array([[0, 512], [200, 300], [400, 450], [600, 250], [800, 400], [1024, 350], [1024, 512]], np.int32)\n",
    "    cv2.polylines(img, [pts], False, (255, 255, 255), 2)\n",
    "    \n",
    "    # Draw sun circle\n",
    "    cv2.circle(img, (800, 150), 80, (255, 255, 255), 2)\n",
    "    \n",
    "    # Draw tree\n",
    "    cv2.line(img, (150, 512), (150, 400), (255, 255, 255), 3)  # Trunk\n",
    "    cv2.ellipse(img, (150, 350), (60, 80), 0, 0, 360, (255, 255, 255), 2)  # Foliage\n",
    "    \n",
    "    # Draw house\n",
    "    cv2.rectangle(img, (400, 450), (550, 512), (255, 255, 255), 2)  # Body\n",
    "    pts_roof = np.array([[380, 450], [475, 380], [570, 450]], np.int32)\n",
    "    cv2.polylines(img, [pts_roof], True, (255, 255, 255), 2)  # Roof\n",
    "    cv2.rectangle(img, (450, 480), (500, 512), (255, 255, 255), 2)  # Door\n",
    "    \n",
    "    return Image.fromarray(img)\n",
    "\n",
    "# Create custom sketch\n",
    "custom_sketch = draw_simple_scene()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(custom_sketch)\n",
    "plt.title(\"Custom Sketch for ControlNet\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up depth pipeline\n",
    "del pipe_depth, controlnet_depth\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reload Canny for sketch-to-image\n",
    "print(\"Reloading Canny ControlNet for sketch-to-image...\")\n",
    "\n",
    "controlnet_canny = ControlNetModel.from_pretrained(\n",
    "    \"diffusers/controlnet-canny-sdxl-1.0\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "pipe_canny = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    controlnet=controlnet_canny,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "pipe_canny = pipe_canny.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate from custom sketch\n",
    "prompts = [\n",
    "    \"A peaceful countryside landscape at sunset, oil painting, warm colors\",\n",
    "    \"A fantasy village in a magical land, digital art, vibrant\",\n",
    "    \"A winter scene with snow-covered mountains, photorealistic\",\n",
    "]\n",
    "\n",
    "images = []\n",
    "for prompt in prompts:\n",
    "    print(f\"Generating: {prompt[:40]}...\")\n",
    "    generator = torch.Generator(device=device).manual_seed(42)\n",
    "    \n",
    "    image = pipe_canny(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=\"blurry, low quality, ugly\",\n",
    "        image=custom_sketch,\n",
    "        num_inference_steps=25,\n",
    "        guidance_scale=7.5,\n",
    "        controlnet_conditioning_scale=0.7,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    \n",
    "    images.append(image)\n",
    "\n",
    "# Display\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "axes[0].imshow(custom_sketch)\n",
    "axes[0].set_title(\"Your Sketch\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "for i, (img, prompt) in enumerate(zip(images, prompts)):\n",
    "    axes[i+1].imshow(img)\n",
    "    axes[i+1].set_title(prompt[:35] + \"...\", fontsize=10)\n",
    "    axes[i+1].axis('off')\n",
    "\n",
    "plt.suptitle(\"From Sketch to Art with ControlNet\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüé® Your simple sketch became beautiful artwork!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Creating Consistent Characters\n",
    "\n",
    "One powerful use of ControlNet is maintaining character consistency across multiple images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a base character\n",
    "character_prompt = \"A portrait of a young woman with red hair and green eyes, fantasy style, detailed face, looking at camera\"\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(12345)\n",
    "\n",
    "base_character = pipe_canny.text_encoder.device  # Just to use the base pipe\n",
    "\n",
    "# Use the underlying pipe without controlnet for first generation\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "\n",
    "# Generate base\n",
    "pipe_base = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "pipe_base = pipe_base.to(device)\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(12345)\n",
    "base_character = pipe_base(\n",
    "    prompt=character_prompt,\n",
    "    negative_prompt=\"blurry, low quality, deformed\",\n",
    "    num_inference_steps=25,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(base_character)\n",
    "plt.title(\"Base Character\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract edges from the character\n",
    "character_edges = get_canny_edges(base_character, low_threshold=50, high_threshold=150)\n",
    "\n",
    "# Generate variations with the same structure but different contexts\n",
    "variation_prompts = [\n",
    "    \"The same red-haired woman in a medieval castle, fantasy portrait\",\n",
    "    \"The same red-haired woman in a cyberpunk city, neon lights, futuristic\",\n",
    "    \"The same red-haired woman in a enchanted forest, magical atmosphere\",\n",
    "    \"The same red-haired woman in an art studio, painting, warm light\",\n",
    "]\n",
    "\n",
    "variations = []\n",
    "for prompt in variation_prompts:\n",
    "    print(f\"Generating: {prompt[:40]}...\")\n",
    "    generator = torch.Generator(device=device).manual_seed(42)\n",
    "    \n",
    "    image = pipe_canny(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=\"blurry, low quality, different person, different face\",\n",
    "        image=character_edges,\n",
    "        num_inference_steps=25,\n",
    "        guidance_scale=7.5,\n",
    "        controlnet_conditioning_scale=0.6,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    \n",
    "    variations.append(image)\n",
    "\n",
    "# Display\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "axes[0].imshow(base_character)\n",
    "axes[0].set_title(\"Base Character\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(character_edges)\n",
    "axes[1].set_title(\"Edge Control\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "for i, (img, prompt) in enumerate(zip(variations, variation_prompts)):\n",
    "    axes[i+2].imshow(img)\n",
    "    axes[i+2].set_title(prompt[20:50] + \"...\", fontsize=9)\n",
    "    axes[i+2].axis('off')\n",
    "\n",
    "plt.suptitle(\"Consistent Character Across Scenes\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° The character maintains consistency while the scene changes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Control Image Wrong Size\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Control image different size than output\n",
    "control = Image.open(\"control.png\")  # 512x512\n",
    "image = pipe(control=control, width=1024, height=1024)  # Mismatch!\n",
    "\n",
    "# ‚úÖ Right: Match sizes\n",
    "control = Image.open(\"control.png\").resize((1024, 1024))\n",
    "image = pipe(image=control, width=1024, height=1024)\n",
    "```\n",
    "\n",
    "### Mistake 2: Conditioning Scale Too High\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Too strict, loses creativity\n",
    "controlnet_conditioning_scale = 1.5  \n",
    "\n",
    "# ‚úÖ Right: Balanced control\n",
    "controlnet_conditioning_scale = 0.5  # Good starting point\n",
    "```\n",
    "\n",
    "### Mistake 3: Wrong Control Type\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Giving edges to depth ControlNet\n",
    "depth_controlnet = ControlNetModel.from_pretrained(\"depth-model\")\n",
    "pipe(image=canny_edges)  # Wrong input type!\n",
    "\n",
    "# ‚úÖ Right: Match control image to model type\n",
    "depth_controlnet = ControlNetModel.from_pretrained(\"depth-model\")\n",
    "pipe(image=depth_map)  # Correct!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How ControlNet adds structural control to diffusion\n",
    "- ‚úÖ Using Canny edge detection for outline-guided generation\n",
    "- ‚úÖ Controlling conditioning strength\n",
    "- ‚úÖ Creating custom control images from sketches\n",
    "- ‚úÖ Maintaining character consistency across scenes\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "1. **Architecture Control**: Draw a simple building outline and generate it in 5 different architectural styles\n",
    "2. **Pose Control**: Find an OpenPose model and control character poses\n",
    "3. **Multi-ControlNet**: Combine edges + depth for even more control\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up all pipelines\n",
    "del pipe_canny, controlnet_canny, pipe_base\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"GPU memory cleared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Proceed to **Lab 2.6.4: Flux Exploration** to explore the Flux architecture and compare it with SDXL!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}