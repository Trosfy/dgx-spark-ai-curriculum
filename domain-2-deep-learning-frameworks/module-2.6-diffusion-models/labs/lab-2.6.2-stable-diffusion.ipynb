{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.6.2: Stable Diffusion XL - Master Image Generation\n",
    "\n",
    "**Module:** 2.6 - Diffusion Models  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** â­â­ (Beginner-Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Load and run Stable Diffusion XL on DGX Spark\n",
    "- [ ] Master prompt engineering for better images\n",
    "- [ ] Understand and use guidance scale effectively\n",
    "- [ ] Leverage negative prompts to avoid artifacts\n",
    "- [ ] Generate images in various styles and resolutions\n",
    "- [ ] Benchmark generation performance\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Prerequisites\n",
    "\n",
    "- Completed: Lab 2.6.1 (Diffusion Theory)\n",
    "- Knowledge of: Basic diffusion model concepts\n",
    "- Installed: `diffusers`, `transformers`, `accelerate`\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ Real-World Context\n",
    "\n",
    "**Stable Diffusion XL (SDXL) is the workhorse of AI image generation:**\n",
    "\n",
    "- **Midjourney** and **Leonardo.AI** are built on similar architectures\n",
    "- **Adobe Firefly** uses diffusion for creative tools\n",
    "- **Game studios** generate concept art and textures\n",
    "- **Marketing teams** create ad visuals instantly\n",
    "\n",
    "DGX Spark's 128GB unified memory lets you run SDXL at full precision - something that would normally require an expensive cloud GPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§’ ELI5: How Does Text-to-Image Work?\n",
    "\n",
    "> **Imagine you're playing Pictionary with a really talented artist friend.**\n",
    ">\n",
    "> 1. You say: \"Draw a cat wearing a top hat\"\n",
    "> 2. Your friend starts with a blank, noisy canvas\n",
    "> 3. They think: \"What would a top-hat cat look like?\"\n",
    "> 4. They gradually clean up the noise, guided by your description\n",
    "> 5. After many small improvements, a cat with a top hat appears!\n",
    ">\n",
    "> **The key components:**\n",
    "> - **CLIP Text Encoder**: Translates your words into numbers the model understands\n",
    "> - **U-Net**: The \"artist\" that removes noise guided by text\n",
    "> - **VAE**: Compresses images to save memory (works in \"latent space\")\n",
    "> - **Scheduler**: Controls how quickly/carefully to denoise\n",
    "\n",
    "### The SDXL Architecture\n",
    "\n",
    "```\n",
    "\"A cat wearing a top hat\"     Random Noise\n",
    "         â”‚                         â”‚\n",
    "         â–¼                         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  CLIP Text      â”‚         â”‚    VAE      â”‚\n",
    "â”‚  Encoder        â”‚         â”‚  (Latent)   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚                         â”‚\n",
    "         â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚\n",
    "         â””â”€â”€â”€â–ºâ”‚      U-Net        â”‚â—„â”˜\n",
    "              â”‚  (Denoises with   â”‚\n",
    "              â”‚   text guidance)  â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â”‚ x30 steps\n",
    "                        â–¼\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚   VAE Decoder   â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                       â”‚\n",
    "                       â–¼\n",
    "                ğŸ–¼ï¸ Final Image\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setting Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Diffusers - the main library for diffusion models\n",
    "from diffusers import (\n",
    "    StableDiffusionXLPipeline,\n",
    "    DPMSolverMultistepScheduler,\n",
    "    EulerDiscreteScheduler,\n",
    "    EulerAncestralDiscreteScheduler,\n",
    ")\n",
    "\n",
    "# Image display\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"Total Memory: {total_mem:.1f} GB\")\n",
    "    print(f\"\\nDGX Spark's 128GB unified memory = SDXL at full precision! ğŸš€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to display images nicely\n",
    "def show_image(image, title=None, figsize=(10, 10)):\n",
    "    \"\"\"Display a single image with optional title.\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(image)\n",
    "    if title:\n",
    "        plt.title(title, fontsize=12, wrap=True)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_images_grid(images, titles=None, ncols=2, figsize=(12, 12)):\n",
    "    \"\"\"Display multiple images in a grid.\"\"\"\n",
    "    n = len(images)\n",
    "    nrows = (n + ncols - 1) // ncols\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "    if nrows == 1:\n",
    "        axes = [axes] if ncols == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, (ax, img) in enumerate(zip(axes, images)):\n",
    "        ax.imshow(img)\n",
    "        if titles and i < len(titles):\n",
    "            ax.set_title(titles[i], fontsize=10, wrap=True)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        return f\"Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\"\n",
    "    return \"No GPU\"\n",
    "\n",
    "print(\"Helper functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Loading Stable Diffusion XL\n",
    "\n",
    "SDXL is a ~7GB model. On DGX Spark, we can load it at full bfloat16 precision!\n",
    "\n",
    "### DGX Spark Advantage\n",
    "\n",
    "| Hardware | SDXL Loading | Quality |\n",
    "|----------|--------------|----------|\n",
    "| 8GB GPU | Must quantize to 4-bit | Lower quality |\n",
    "| 16GB GPU | FP16 with memory optimization | Good |\n",
    "| 24GB GPU | FP16 comfortable | Great |\n",
    "| **DGX Spark (128GB)** | **BF16 + room for batches** | **Best** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Stable Diffusion XL...\")\n",
    "print(f\"Memory before loading: {get_memory_usage()}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load SDXL base model\n",
    "# Using bfloat16 - native support on DGX Spark's Blackwell architecture!\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.bfloat16,  # Best for Blackwell\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\",  # Downloads the FP16 weights (smaller)\n",
    ")\n",
    "\n",
    "# Move to GPU\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "# Enable memory efficient attention (optional on DGX Spark, but still good practice)\n",
    "# pipe.enable_xformers_memory_efficient_attention()  # Uncomment if xformers installed\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"\\nâœ… SDXL loaded in {load_time:.1f} seconds\")\n",
    "print(f\"Memory after loading: {get_memory_usage()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the pipeline components\n",
    "print(\"SDXL Pipeline Components:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n1. Text Encoder 1 (CLIP ViT-L):\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in pipe.text_encoder.parameters()):,}\")\n",
    "\n",
    "print(f\"\\n2. Text Encoder 2 (OpenCLIP ViT-bigG):\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in pipe.text_encoder_2.parameters()):,}\")\n",
    "\n",
    "print(f\"\\n3. U-Net (Denoising Network):\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in pipe.unet.parameters()):,}\")\n",
    "\n",
    "print(f\"\\n4. VAE (Image Encoder/Decoder):\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in pipe.vae.parameters()):,}\")\n",
    "\n",
    "total_params = sum(\n",
    "    sum(p.numel() for p in component.parameters())\n",
    "    for component in [pipe.text_encoder, pipe.text_encoder_2, pipe.unet, pipe.vae]\n",
    ")\n",
    "print(f\"\\nğŸ“Š Total Parameters: {total_params:,} ({total_params/1e9:.2f}B)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Your First Generation\n",
    "\n",
    "Let's generate an image with a simple prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple first generation\n",
    "prompt = \"A majestic lion resting on a rock in the African savanna, golden hour lighting, wildlife photography\"\n",
    "\n",
    "print(f\"Generating image for:\\n'{prompt}'\\n\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "\n",
    "# Generate!\n",
    "start_time = time.time()\n",
    "\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=30,  # Number of denoising steps\n",
    "    guidance_scale=7.5,      # How strongly to follow the prompt\n",
    "    generator=generator,\n",
    "    width=1024,              # SDXL native resolution\n",
    "    height=1024,\n",
    ").images[0]\n",
    "\n",
    "gen_time = time.time() - start_time\n",
    "\n",
    "print(f\"â±ï¸ Generation time: {gen_time:.1f} seconds\")\n",
    "print(f\"ğŸ“ Image size: {image.size}\")\n",
    "\n",
    "# Display the result\n",
    "show_image(image, f\"Prompt: {prompt[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” What Just Happened?\n",
    "\n",
    "1. **Text Encoding**: The prompt was converted to numerical embeddings by CLIP\n",
    "2. **Random Noise**: Started with pure random noise in latent space (64Ã—64Ã—4)\n",
    "3. **Denoising Loop**: 30 steps of guided noise removal\n",
    "4. **VAE Decoding**: Latent â†’ full resolution 1024Ã—1024 image\n",
    "\n",
    "On DGX Spark, you should see generation times around **5-8 seconds** at 1024Ã—1024!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Prompt Engineering\n",
    "\n",
    "### ğŸ§’ ELI5: Good Prompts vs Bad Prompts\n",
    "\n",
    "> Imagine giving directions to an artist:\n",
    ">\n",
    "> âŒ **Bad**: \"Draw a cat\"  \n",
    "> The artist doesn't know what kind, what style, what setting...\n",
    ">\n",
    "> âœ… **Good**: \"A fluffy orange tabby cat curled up on a velvet cushion, soft afternoon sunlight from a window, oil painting style, warm colors\"  \n",
    "> Now the artist knows exactly what you want!\n",
    "\n",
    "### Prompt Structure Formula\n",
    "\n",
    "```\n",
    "[Subject], [Setting/Background], [Style], [Lighting/Mood], [Quality Modifiers]\n",
    "```\n",
    "\n",
    "**Examples:**\n",
    "- Subject: \"A wise old wizard\"\n",
    "- Setting: \"in a tower filled with ancient books\"\n",
    "- Style: \"fantasy art, digital painting\"\n",
    "- Lighting: \"candlelit, mysterious atmosphere\"\n",
    "- Quality: \"highly detailed, 8K resolution\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different prompt styles\n",
    "prompts = [\n",
    "    # Basic prompt\n",
    "    \"a castle\",\n",
    "    \n",
    "    # Detailed prompt\n",
    "    \"A medieval castle perched on a cliff overlooking the sea, dramatic sunset, fantasy art style, highly detailed\",\n",
    "    \n",
    "    # Artistic style\n",
    "    \"A medieval castle, oil painting by Thomas Cole, Hudson River School style, romantic landscape, golden light\",\n",
    "    \n",
    "    # Photorealistic\n",
    "    \"A medieval castle in Scotland, professional travel photography, golden hour, 8K resolution, National Geographic\",\n",
    "]\n",
    "\n",
    "images = []\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"Generating {i+1}/{len(prompts)}: {prompt[:50]}...\")\n",
    "    generator = torch.Generator(device=device).manual_seed(42)  # Same seed for comparison\n",
    "    \n",
    "    img = pipe(\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=25,\n",
    "        guidance_scale=7.5,\n",
    "        generator=generator,\n",
    "        width=1024,\n",
    "        height=1024,\n",
    "    ).images[0]\n",
    "    images.append(img)\n",
    "\n",
    "# Display comparison\n",
    "titles = [\"Basic\", \"Detailed\", \"Artistic\", \"Photorealistic\"]\n",
    "show_images_grid(images, titles, ncols=2, figsize=(14, 14))\n",
    "\n",
    "print(\"\\nğŸ’¡ Notice how more detailed prompts give you more control over the output!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Weighting\n",
    "\n",
    "You can emphasize or de-emphasize parts of your prompt using `(term:weight)` syntax.\n",
    "\n",
    "- `(golden:1.3)` - 30% more attention to \"golden\"\n",
    "- `(blur:0.5)` - 50% less attention to \"blur\"\n",
    "- `((very important))` - Double emphasis (same as 1.1 Ã— 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate prompt weighting\n",
    "# Note: Diffusers handles weighting through compel or manual attention\n",
    "# For simplicity, we'll show the concept with careful prompt construction\n",
    "\n",
    "base_prompt = \"A cat and a dog playing in a garden\"\n",
    "\n",
    "weighted_prompts = [\n",
    "    \"A cat and a dog playing in a garden\",\n",
    "    \"A cat and a dog playing in a beautiful magical garden with flowers\",\n",
    "    \"A fluffy persian cat and a small dog playing in a garden\",\n",
    "    \"A cat and a golden retriever dog playing in a Japanese zen garden\",\n",
    "]\n",
    "\n",
    "images = []\n",
    "for prompt in weighted_prompts:\n",
    "    generator = torch.Generator(device=device).manual_seed(123)\n",
    "    img = pipe(\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=25,\n",
    "        guidance_scale=7.5,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    images.append(img)\n",
    "\n",
    "titles = [\"Basic\", \"Emphasize Garden\", \"Emphasize Cat\", \"Emphasize Dog & Setting\"]\n",
    "show_images_grid(images, titles, ncols=2, figsize=(14, 14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Guidance Scale Experiments\n",
    "\n",
    "### ğŸ§’ ELI5: What is Guidance Scale?\n",
    "\n",
    "> Imagine the AI is an artist who has their own creative ideas:\n",
    ">\n",
    "> - **Low guidance (1-4)**: \"Here's what I think you meant, with my own artistic flair\"\n",
    ">   - More creative, may deviate from prompt\n",
    ">   - Softer, more painterly results\n",
    ">\n",
    "> - **Medium guidance (7-9)**: \"I'll follow your description closely, but add some creativity\"\n",
    ">   - Best balance for most use cases\n",
    ">   - Default is typically 7.5\n",
    ">\n",
    "> - **High guidance (12+)**: \"I'll do EXACTLY what you said, nothing more\"\n",
    ">   - Very literal interpretation\n",
    ">   - Can become oversaturated or distorted\n",
    "\n",
    "### The Math (Classifier-Free Guidance)\n",
    "\n",
    "```\n",
    "output = unconditional_output + guidance_scale Ã— (conditional_output - unconditional_output)\n",
    "```\n",
    "\n",
    "Higher scale = more \"push\" toward the text-conditioned output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different guidance scales\n",
    "prompt = \"A cyberpunk city at night, neon lights reflecting on wet streets, futuristic architecture\"\n",
    "\n",
    "guidance_scales = [1.5, 5.0, 7.5, 12.0, 20.0]\n",
    "images = []\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "for gs in guidance_scales:\n",
    "    print(f\"Generating with guidance_scale={gs}...\")\n",
    "    generator = torch.Generator(device=device).manual_seed(42)\n",
    "    \n",
    "    img = pipe(\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=25,\n",
    "        guidance_scale=gs,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    images.append(img)\n",
    "\n",
    "# Display comparison\n",
    "titles = [f\"Scale: {gs}\" for gs in guidance_scales]\n",
    "show_images_grid(images, titles, ncols=3, figsize=(15, 10))\n",
    "\n",
    "print(\"\\nğŸ“Š Observations:\")\n",
    "print(\"  - 1.5: Very soft, dreamy, may not follow prompt well\")\n",
    "print(\"  - 5.0: Creative interpretation with good prompt following\")\n",
    "print(\"  - 7.5: Balanced (recommended default)\")\n",
    "print(\"  - 12.0: Strong prompt adherence, may oversaturate\")\n",
    "print(\"  - 20.0: Often too extreme, artifacts may appear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Negative Prompts\n",
    "\n",
    "### ğŸ§’ ELI5: What are Negative Prompts?\n",
    "\n",
    "> Telling an artist what NOT to do:\n",
    ">\n",
    "> \"Draw me a portrait, but:\n",
    "> - Don't make it blurry\n",
    "> - Don't add any watermarks\n",
    "> - Don't make the hands weird\"\n",
    ">\n",
    "> This helps avoid common issues!\n",
    "\n",
    "### Common Negative Prompt Patterns\n",
    "\n",
    "**For photorealistic images:**\n",
    "```\n",
    "blurry, low quality, distorted, deformed, ugly, bad anatomy, watermark, text, signature\n",
    "```\n",
    "\n",
    "**For portraits:**\n",
    "```\n",
    "bad hands, extra fingers, missing fingers, bad proportions, cross-eyed, mutated\n",
    "```\n",
    "\n",
    "**For artistic styles:**\n",
    "```\n",
    "photo, photograph, realistic, 3D render, CGI\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with and without negative prompts\n",
    "prompt = \"A portrait of a wise old wizard with a long beard, fantasy art\"\n",
    "\n",
    "negative_prompts = [\n",
    "    None,  # No negative prompt\n",
    "    \"blurry, low quality\",\n",
    "    \"blurry, low quality, bad anatomy, deformed, ugly, mutated\",\n",
    "    \"blurry, low quality, bad anatomy, deformed, ugly, mutated, bad hands, extra fingers, watermark, text, signature, amateur\",\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    \"No Negative\",\n",
    "    \"Basic Negative\",\n",
    "    \"Standard Negative\",\n",
    "    \"Comprehensive Negative\",\n",
    "]\n",
    "\n",
    "images = []\n",
    "for neg in negative_prompts:\n",
    "    print(f\"Generating with negative: {str(neg)[:50]}...\")\n",
    "    generator = torch.Generator(device=device).manual_seed(88)\n",
    "    \n",
    "    img = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=neg,\n",
    "        num_inference_steps=25,\n",
    "        guidance_scale=7.5,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    images.append(img)\n",
    "\n",
    "show_images_grid(images, labels, ncols=2, figsize=(14, 14))\n",
    "\n",
    "print(\"\\nğŸ’¡ Tip: Build your own 'negative prompt library' for different use cases!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Different Art Styles\n",
    "\n",
    "SDXL excels at generating images in various artistic styles. Let's explore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same subject, different styles\n",
    "subject = \"A serene mountain lake at sunset\"\n",
    "\n",
    "styles = {\n",
    "    \"Photorealistic\": f\"{subject}, professional landscape photography, 8K, National Geographic\",\n",
    "    \"Oil Painting\": f\"{subject}, oil painting, impressionist style, thick brushstrokes, like Monet\",\n",
    "    \"Watercolor\": f\"{subject}, delicate watercolor painting, soft washes, artistic\",\n",
    "    \"Anime\": f\"{subject}, anime style, Studio Ghibli inspired, vibrant colors, detailed\",\n",
    "    \"Digital Art\": f\"{subject}, digital art, concept art, artstation trending, fantasy\",\n",
    "    \"Pixel Art\": f\"{subject}, pixel art style, 16-bit, retro game aesthetic, nostalgic\",\n",
    "}\n",
    "\n",
    "images = []\n",
    "for style_name, prompt in styles.items():\n",
    "    print(f\"Generating: {style_name}...\")\n",
    "    generator = torch.Generator(device=device).manual_seed(42)\n",
    "    \n",
    "    img = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=\"blurry, low quality, distorted\",\n",
    "        num_inference_steps=25,\n",
    "        guidance_scale=7.5,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    images.append(img)\n",
    "\n",
    "show_images_grid(images, list(styles.keys()), ncols=3, figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ‹ Try It Yourself: Create Your Own Style Gallery\n",
    "\n",
    "Choose a subject and generate it in 4 different styles!\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Style Ideas</summary>\n",
    "\n",
    "- \"Art Deco poster style, geometric patterns\"\n",
    "- \"Ukiyo-e Japanese woodblock print\"\n",
    "- \"Steampunk illustration, copper and brass tones\"\n",
    "- \"Pop art, Andy Warhol inspired, bold colors\"\n",
    "- \"Pencil sketch, detailed linework, sketchbook style\"\n",
    "- \"3D render, Pixar style, cute and colorful\"\n",
    "- \"Gothic dark fantasy, detailed, dramatic lighting\"\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Create your own style gallery\n",
    "# Choose a subject and experiment with different styles!\n",
    "\n",
    "my_subject = \"\"  # Your subject here\n",
    "my_styles = {\n",
    "    # \"Style Name\": \"full prompt\",\n",
    "}\n",
    "\n",
    "# Generate and display...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Different Schedulers\n",
    "\n",
    "The **scheduler** (also called sampler) controls the denoising process.\n",
    "Different schedulers can give different results and speeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different schedulers\n",
    "from diffusers import (\n",
    "    DDPMScheduler,\n",
    "    DDIMScheduler,\n",
    "    PNDMScheduler,\n",
    "    LMSDiscreteScheduler,\n",
    "    EulerDiscreteScheduler,\n",
    "    DPMSolverMultistepScheduler,\n",
    ")\n",
    "\n",
    "prompt = \"A magical forest with glowing mushrooms, fantasy art, ethereal lighting\"\n",
    "\n",
    "schedulers = {\n",
    "    \"Euler\": EulerDiscreteScheduler.from_config(pipe.scheduler.config),\n",
    "    \"Euler Ancestral\": EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config),\n",
    "    \"DPM++ 2M\": DPMSolverMultistepScheduler.from_config(pipe.scheduler.config),\n",
    "    \"DDIM\": DDIMScheduler.from_config(pipe.scheduler.config),\n",
    "}\n",
    "\n",
    "images = []\n",
    "times = []\n",
    "\n",
    "for name, scheduler in schedulers.items():\n",
    "    print(f\"Testing {name} scheduler...\")\n",
    "    pipe.scheduler = scheduler\n",
    "    generator = torch.Generator(device=device).manual_seed(42)\n",
    "    \n",
    "    start = time.time()\n",
    "    img = pipe(\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=25,\n",
    "        guidance_scale=7.5,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    images.append(img)\n",
    "    times.append(elapsed)\n",
    "\n",
    "# Display with timing info\n",
    "titles = [f\"{name}\\n({t:.1f}s)\" for name, t in zip(schedulers.keys(), times)]\n",
    "show_images_grid(images, titles, ncols=2, figsize=(14, 14))\n",
    "\n",
    "# Reset to default scheduler\n",
    "pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "print(\"\\nğŸ“Š Scheduler Comparison:\")\n",
    "print(\"  - Euler: Fast, good quality, deterministic\")\n",
    "print(\"  - Euler Ancestral: Adds randomness, more creative\")\n",
    "print(\"  - DPM++ 2M: Very fast, high quality, recommended!\")\n",
    "print(\"  - DDIM: Classic, consistent, slightly slower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Performance Benchmarking\n",
    "\n",
    "Let's measure DGX Spark's performance across different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark different configurations\n",
    "def benchmark(pipe, prompt, steps, width, height, num_runs=3):\n",
    "    \"\"\"Run benchmark and return average time.\"\"\"\n",
    "    times = []\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        generator = torch.Generator(device=device).manual_seed(42)\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        \n",
    "        _ = pipe(\n",
    "            prompt=prompt,\n",
    "            num_inference_steps=steps,\n",
    "            guidance_scale=7.5,\n",
    "            generator=generator,\n",
    "            width=width,\n",
    "            height=height,\n",
    "        )\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    return sum(times) / len(times)\n",
    "\n",
    "# Use DPM++ for fastest results\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "prompt = \"A beautiful sunset over the ocean, photorealistic\"\n",
    "\n",
    "print(\"ğŸš€ DGX Spark SDXL Benchmark\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Benchmark different configurations\n",
    "configs = [\n",
    "    (\"512Ã—512, 20 steps\", 20, 512, 512),\n",
    "    (\"768Ã—768, 25 steps\", 25, 768, 768),\n",
    "    (\"1024Ã—1024, 25 steps\", 25, 1024, 1024),\n",
    "    (\"1024Ã—1024, 30 steps\", 30, 1024, 1024),\n",
    "    (\"1024Ã—1024, 50 steps\", 50, 1024, 1024),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for name, steps, w, h in configs:\n",
    "    avg_time = benchmark(pipe, prompt, steps, w, h)\n",
    "    results.append((name, avg_time))\n",
    "    print(f\"  {name}: {avg_time:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"Memory Usage: {get_memory_usage()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš ï¸ Common Mistakes\n",
    "\n",
    "### Mistake 1: Prompts Too Short\n",
    "\n",
    "```python\n",
    "# âŒ Wrong: Too vague\n",
    "prompt = \"a dog\"\n",
    "\n",
    "# âœ… Right: Detailed and specific\n",
    "prompt = \"A golden retriever puppy playing in autumn leaves, soft afternoon light, shallow depth of field, professional pet photography\"\n",
    "```\n",
    "\n",
    "### Mistake 2: Extreme Guidance Scale\n",
    "\n",
    "```python\n",
    "# âŒ Wrong: Too high, causes artifacts\n",
    "guidance_scale = 30.0\n",
    "\n",
    "# âœ… Right: Stay in reasonable range\n",
    "guidance_scale = 7.5  # Good default\n",
    "# Use 5-9 for most cases, up to 12 for strong prompt adherence\n",
    "```\n",
    "\n",
    "### Mistake 3: Too Few Steps\n",
    "\n",
    "```python\n",
    "# âŒ Wrong: Too few, blurry results\n",
    "num_inference_steps = 5\n",
    "\n",
    "# âœ… Right: Enough steps for quality\n",
    "num_inference_steps = 25  # Good balance\n",
    "# Use 20-30 for most cases, 50 for maximum quality\n",
    "```\n",
    "\n",
    "### Mistake 4: Not Using Negative Prompts\n",
    "\n",
    "```python\n",
    "# âŒ Wrong: No negative prompt\n",
    "image = pipe(prompt=prompt).images[0]\n",
    "\n",
    "# âœ… Right: Always use negative prompts\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=\"blurry, low quality, distorted, watermark\",\n",
    ").images[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- âœ… How to load and run SDXL on DGX Spark\n",
    "- âœ… Prompt engineering techniques for better images\n",
    "- âœ… How guidance scale affects generation\n",
    "- âœ… Using negative prompts to avoid artifacts\n",
    "- âœ… Generating images in various artistic styles\n",
    "- âœ… Comparing different schedulers\n",
    "- âœ… Benchmarking performance\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Challenge (Optional)\n",
    "\n",
    "### Challenge 1: Prompt Battle\n",
    "Create the best prompt for \"a cozy reading nook\". Compare 5 variations and pick the winner!\n",
    "\n",
    "### Challenge 2: Style Transfer\n",
    "Generate the same scene (e.g., \"a coffee shop\") in 10 different famous artists' styles.\n",
    "\n",
    "### Challenge 3: Optimal Settings\n",
    "Find the optimal guidance scale and step count for photorealistic portraits vs. anime art."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“– Further Reading\n",
    "\n",
    "- [SDXL Paper](https://arxiv.org/abs/2307.01952) - \"SDXL: Improving Latent Diffusion Models\"\n",
    "- [Diffusers Documentation](https://huggingface.co/docs/diffusers) - Official docs\n",
    "- [Prompt Engineering Guide](https://stable-diffusion-art.com/prompt-guide/) - Comprehensive tips\n",
    "- [CivitAI](https://civitai.com/) - Community models and prompts\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "del pipe\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"GPU memory cleared!\")\n",
    "print(f\"Memory after cleanup: {get_memory_usage()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you've mastered basic SDXL generation, proceed to:\n",
    "\n",
    "**Lab 2.6.3: ControlNet Workshop** - Learn to guide image generation with edge maps, poses, and depth!\n",
    "\n",
    "You'll learn:\n",
    "- Using Canny edges for structural control\n",
    "- Pose-controlled character generation\n",
    "- Depth-based composition\n",
    "- Creating consistent characters across images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
