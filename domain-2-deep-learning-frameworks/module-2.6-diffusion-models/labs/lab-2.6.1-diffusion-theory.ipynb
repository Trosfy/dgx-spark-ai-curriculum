{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 2.6.1: Diffusion Model Theory - From Noise to Art\n\n**Module:** 2.6 - Diffusion Models  \n**Time:** 2 hours  \n**Difficulty:** ‚≠ê‚≠ê‚≠ê (Intermediate)\n\n---\n\n## üéØ Learning Objectives\n\nBy the end of this notebook, you will:\n- [ ] Understand the mathematical foundation of diffusion models\n- [ ] Implement forward diffusion (adding noise) from scratch\n- [ ] Visualize noise schedules and their effects\n- [ ] Build a simple U-Net for denoising\n- [ ] Implement reverse diffusion to generate new images\n- [ ] Train a DDPM on MNIST and generate digits!\n\n---\n\n## üìö Prerequisites\n\n- Completed: Module 2.5 (Hugging Face Ecosystem)\n- Knowledge of: PyTorch basics, neural network fundamentals\n- Math comfort: Basic probability, normal distributions\n- **Required packages:**\n  - `torch>=2.0.0`\n  - `torchvision>=0.15.0`\n  - `matplotlib>=3.7.0`\n  - `tqdm`\n\n---\n\n## üåç Real-World Context\n\n**Diffusion models power the AI art revolution!**\n\n- **Midjourney** uses diffusion to create stunning artwork\n- **DALL-E 3** generates images from text descriptions\n- **Stable Diffusion** is the open-source king of image generation\n- **Sora** uses diffusion principles for video generation\n\nUnderstanding how diffusion works gives you the foundation to use, customize, and even build these systems."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What is Diffusion?\n",
    "\n",
    "> **Imagine you have a beautiful photo of a cat.** üê±\n",
    ">\n",
    "> Now imagine slowly adding TV static to it - a tiny bit at first, then more and more.\n",
    "> After 1000 steps of adding static, your cat photo is completely unrecognizable -\n",
    "> it's just pure noise, like a TV with no signal.\n",
    ">\n",
    "> **That's the \"forward\" process - destroying the image with noise.**\n",
    ">\n",
    "> Now here's the magic: What if we could train a neural network to **reverse this process**?\n",
    "> \n",
    "> If we show the AI many examples of \"noisy image at step 500\" ‚Üí \"slightly less noisy image at step 499\",\n",
    "> it learns to remove noise one step at a time.\n",
    ">\n",
    "> **The \"reverse\" process - starting from pure noise and gradually revealing an image!**\n",
    ">\n",
    "> The amazing part? Once trained, we can start with *random* noise and the model will\n",
    "> denoise it into a *new* image that looks like the training data - a brand new cat! üê±‚ú®\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "```\n",
    "Forward:  Real Image  ‚îÄ‚îÄ‚îÄ[add noise]‚îÄ‚îÄ‚îÄ>  Pure Noise    (easy, just math)\n",
    "                                              ‚îÇ\n",
    "Reverse:  New Image   <‚îÄ‚îÄ‚îÄ[remove noise]‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ          (learned by neural network)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setting Up Our Environment\n",
    "\n",
    "Let's import everything we need and check our DGX Spark GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Core imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport math\n\n# Set up device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name()}\")\n    total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"Total Memory: {total_mem:.1f} GB\")\n    print(f\"\\nDGX Spark's 128GB unified memory = room for all experiments! üöÄ\")\n    \n    # Use bfloat16 for DGX Spark's Blackwell architecture\n    dtype = torch.bfloat16\nelse:\n    dtype = torch.float32\n\nprint(f\"\\nUsing dtype: {dtype}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding the Forward Process\n",
    "\n",
    "### The Math Behind Adding Noise\n",
    "\n",
    "In diffusion models, we add Gaussian noise to images according to a **schedule**.\n",
    "\n",
    "At each timestep $t$, we have:\n",
    "\n",
    "$$x_t = \\sqrt{\\bar{\\alpha}_t} \\cdot x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\epsilon$$\n",
    "\n",
    "Where:\n",
    "- $x_0$ = Original clean image\n",
    "- $x_t$ = Noisy image at timestep $t$  \n",
    "- $\\epsilon$ = Random Gaussian noise (same shape as image)\n",
    "- $\\bar{\\alpha}_t$ = Cumulative product of $(1 - \\beta_i)$ from $i=1$ to $t$\n",
    "- $\\beta_t$ = Noise schedule (how much noise to add at step $t$)\n",
    "\n",
    "### üßí ELI5: The Noise Schedule\n",
    "\n",
    "> Think of $\\beta$ as a \"noise dial\" that goes from 0 to 1:\n",
    "> - $\\beta = 0$: No noise added (pure signal)\n",
    "> - $\\beta = 1$: All noise (no signal)\n",
    ">\n",
    "> The schedule controls how fast we turn this dial. A **linear schedule** turns it\n",
    "> steadily. A **cosine schedule** turns it slowly at first, then faster in the middle,\n",
    "> then slowly again at the end - like an S-curve.\n",
    ">\n",
    "> Cosine usually works better because it preserves image structure longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseScheduler:\n",
    "    \"\"\"\n",
    "    Manages the noise schedule for diffusion models.\n",
    "    \n",
    "    This class handles:\n",
    "    - Computing beta (noise variance) at each timestep\n",
    "    - Computing alpha (signal preservation) at each timestep\n",
    "    - Adding noise to images (forward process)\n",
    "    - Computing loss weights for training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        num_timesteps: int = 1000,\n",
    "        beta_start: float = 0.0001,\n",
    "        beta_end: float = 0.02,\n",
    "        schedule_type: str = \"cosine\"\n",
    "    ):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        \n",
    "        # Compute beta schedule\n",
    "        if schedule_type == \"linear\":\n",
    "            self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        elif schedule_type == \"cosine\":\n",
    "            # Cosine schedule from \"Improved DDPM\" paper\n",
    "            self.betas = self._cosine_schedule(num_timesteps)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown schedule: {schedule_type}\")\n",
    "        \n",
    "        # Compute derived values\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        \n",
    "        # For sampling (reverse process)\n",
    "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
    "        \n",
    "        # Posterior variance for sampling\n",
    "        self.posterior_variance = (\n",
    "            self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "    \n",
    "    def _cosine_schedule(self, num_timesteps: int, s: float = 0.008):\n",
    "        \"\"\"Cosine schedule as in 'Improved DDPM' paper.\"\"\"\n",
    "        steps = num_timesteps + 1\n",
    "        x = torch.linspace(0, num_timesteps, steps)\n",
    "        alphas_cumprod = torch.cos(((x / num_timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "        return torch.clip(betas, 0.0001, 0.9999)\n",
    "    \n",
    "    def add_noise(self, x_0: torch.Tensor, t: torch.Tensor, noise: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Add noise to images according to the forward process.\n",
    "        \n",
    "        x_t = sqrt(alpha_cumprod_t) * x_0 + sqrt(1 - alpha_cumprod_t) * noise\n",
    "        \n",
    "        Args:\n",
    "            x_0: Clean images, shape (B, C, H, W)\n",
    "            t: Timesteps, shape (B,)\n",
    "            noise: Optional pre-generated noise\n",
    "            \n",
    "        Returns:\n",
    "            Noisy images x_t and the noise that was added\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "        \n",
    "        # Get coefficients for each sample in batch\n",
    "        sqrt_alpha = self.sqrt_alphas_cumprod[t].view(-1, 1, 1, 1).to(x_0.device)\n",
    "        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1).to(x_0.device)\n",
    "        \n",
    "        # Forward diffusion\n",
    "        x_t = sqrt_alpha * x_0 + sqrt_one_minus_alpha * noise\n",
    "        \n",
    "        return x_t, noise\n",
    "    \n",
    "    def to(self, device):\n",
    "        \"\"\"Move all tensors to specified device.\"\"\"\n",
    "        self.betas = self.betas.to(device)\n",
    "        self.alphas = self.alphas.to(device)\n",
    "        self.alphas_cumprod = self.alphas_cumprod.to(device)\n",
    "        self.alphas_cumprod_prev = self.alphas_cumprod_prev.to(device)\n",
    "        self.sqrt_alphas_cumprod = self.sqrt_alphas_cumprod.to(device)\n",
    "        self.sqrt_one_minus_alphas_cumprod = self.sqrt_one_minus_alphas_cumprod.to(device)\n",
    "        self.posterior_variance = self.posterior_variance.to(device)\n",
    "        return self\n",
    "\n",
    "\n",
    "# Create scheduler\n",
    "scheduler = NoiseScheduler(num_timesteps=1000, schedule_type=\"cosine\")\n",
    "print(f\"Created noise scheduler with {scheduler.num_timesteps} timesteps\")\n",
    "print(f\"Beta range: {scheduler.betas[0]:.6f} to {scheduler.betas[-1]:.6f}\")\n",
    "print(f\"Alpha cumprod range: {scheduler.alphas_cumprod[-1]:.6f} to {scheduler.alphas_cumprod[0]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Noise Schedules\n",
    "\n",
    "Let's compare linear vs cosine schedules to understand why cosine is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create both schedulers for comparison\n",
    "linear_scheduler = NoiseScheduler(num_timesteps=1000, schedule_type=\"linear\")\n",
    "cosine_scheduler = NoiseScheduler(num_timesteps=1000, schedule_type=\"cosine\")\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "timesteps = np.arange(1000)\n",
    "\n",
    "# Plot 1: Beta values (noise added at each step)\n",
    "axes[0].plot(timesteps, linear_scheduler.betas.numpy(), label='Linear', alpha=0.8)\n",
    "axes[0].plot(timesteps, cosine_scheduler.betas.numpy(), label='Cosine', alpha=0.8)\n",
    "axes[0].set_xlabel('Timestep')\n",
    "axes[0].set_ylabel('Beta (noise variance)')\n",
    "axes[0].set_title('Noise Added Per Step')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Alpha cumprod (signal remaining)\n",
    "axes[1].plot(timesteps, linear_scheduler.alphas_cumprod.numpy(), label='Linear', alpha=0.8)\n",
    "axes[1].plot(timesteps, cosine_scheduler.alphas_cumprod.numpy(), label='Cosine', alpha=0.8)\n",
    "axes[1].set_xlabel('Timestep')\n",
    "axes[1].set_ylabel('Alpha Cumprod (signal remaining)')\n",
    "axes[1].set_title('Signal Preservation Over Time')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: SNR (Signal-to-Noise Ratio)\n",
    "linear_snr = linear_scheduler.alphas_cumprod / (1 - linear_scheduler.alphas_cumprod + 1e-8)\n",
    "cosine_snr = cosine_scheduler.alphas_cumprod / (1 - cosine_scheduler.alphas_cumprod + 1e-8)\n",
    "axes[2].semilogy(timesteps, linear_snr.numpy(), label='Linear', alpha=0.8)\n",
    "axes[2].semilogy(timesteps, cosine_snr.numpy(), label='Cosine', alpha=0.8)\n",
    "axes[2].set_xlabel('Timestep')\n",
    "axes[2].set_ylabel('SNR (log scale)')\n",
    "axes[2].set_title('Signal-to-Noise Ratio')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Key Observations:\")\n",
    "print(\"- Cosine schedule preserves signal longer at the start (better for learning)\")\n",
    "print(\"- Linear schedule destroys information too quickly in early steps\")\n",
    "print(\"- Both converge to pure noise by t=1000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Visualizing the Forward Process\n",
    "\n",
    "Let's load MNIST and watch an image get progressively noisier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Get a sample image\n",
    "sample_image, label = train_dataset[0]\n",
    "sample_image = sample_image.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "print(f\"Dataset size: {len(train_dataset)}\")\n",
    "print(f\"Image shape: {sample_image.shape}\")\n",
    "print(f\"Label: {label}\")\n",
    "print(f\"Pixel range: [{sample_image.min():.2f}, {sample_image.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_forward_diffusion(image, scheduler, timesteps_to_show):\n",
    "    \"\"\"\n",
    "    Visualize the forward diffusion process at multiple timesteps.\n",
    "    \n",
    "    This shows how an image progressively becomes noise.\n",
    "    \"\"\"\n",
    "    n_steps = len(timesteps_to_show)\n",
    "    fig, axes = plt.subplots(1, n_steps, figsize=(2.5 * n_steps, 3))\n",
    "    \n",
    "    # Use the same noise for all timesteps (to see progression clearly)\n",
    "    noise = torch.randn_like(image)\n",
    "    \n",
    "    for idx, t in enumerate(timesteps_to_show):\n",
    "        t_tensor = torch.tensor([t])\n",
    "        noisy_image, _ = scheduler.add_noise(image, t_tensor, noise)\n",
    "        \n",
    "        # Convert for display\n",
    "        img_display = noisy_image.squeeze().numpy()\n",
    "        img_display = (img_display + 1) / 2  # [-1, 1] -> [0, 1]\n",
    "        \n",
    "        axes[idx].imshow(img_display, cmap='gray', vmin=0, vmax=1)\n",
    "        axes[idx].set_title(f't = {t}')\n",
    "        axes[idx].axis('off')\n",
    "        \n",
    "        # Calculate and show signal/noise ratio\n",
    "        alpha = scheduler.alphas_cumprod[t].item()\n",
    "        axes[idx].set_xlabel(f'Signal: {alpha*100:.1f}%', fontsize=9)\n",
    "    \n",
    "    plt.suptitle('Forward Diffusion: Adding Noise Over Time', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize at different timesteps\n",
    "timesteps_to_show = [0, 50, 100, 250, 500, 750, 900, 999]\n",
    "visualize_forward_diffusion(sample_image, cosine_scheduler, timesteps_to_show)\n",
    "\n",
    "print(\"\\nüîç Notice how:\")\n",
    "print(\"  - t=0: Original image (100% signal)\")\n",
    "print(\"  - t=250: Still recognizable, but fuzzy\")\n",
    "print(\"  - t=500: Barely recognizable\")\n",
    "print(\"  - t=999: Pure noise (almost 0% signal)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself: Experiment with Different Images\n",
    "\n",
    "Try visualizing the forward process on different MNIST digits.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "```python\n",
    "# Get a different digit\n",
    "different_image, different_label = train_dataset[42]  # or any index\n",
    "different_image = different_image.unsqueeze(0)\n",
    "visualize_forward_diffusion(different_image, cosine_scheduler, timesteps_to_show)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Try different images from the dataset\n",
    "# Experiment with different indices to see different digits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Building the U-Net Denoiser\n",
    "\n",
    "### üßí ELI5: Why U-Net?\n",
    "\n",
    "> The U-Net is like a smart photo editor:\n",
    "> \n",
    "> 1. **Encoder** (going down): \"Zoom out\" to understand the big picture\n",
    ">    - What kind of digit is this? Where are the main strokes?\n",
    "> \n",
    "> 2. **Bottleneck** (the bottom): Process the high-level understanding\n",
    "> \n",
    "> 3. **Decoder** (going up): \"Zoom in\" to fill in details\n",
    ">    - Now that I know it's a \"7\", let me sharpen the edges!\n",
    "> \n",
    "> 4. **Skip connections**: Let the decoder see the original noisy pixels\n",
    ">    - \"Here's exactly what you're working with at each scale\"\n",
    "> \n",
    "> The model also gets told \"what timestep is this?\" so it knows how much\n",
    "> noise it's dealing with. Removing 1% noise is different from removing 50%!\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "```\n",
    "Input (noisy image) ‚îÄ‚îê\n",
    "                     ‚îÇ\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ   [Conv Block] ‚îÇ + Timestep     ‚îÇ\n",
    "    ‚îÇ        ‚Üì       ‚îÇ   Embedding    ‚îÇ\n",
    "    ‚îÇ   [Conv Block]‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ Skip\n",
    "    ‚îÇ        ‚Üì       ‚îÇ                ‚îÇ\n",
    "    ‚îÇ   [Downsample] ‚îÇ                ‚îÇ\n",
    "    ‚îÇ        ‚Üì       ‚îÇ                ‚îÇ\n",
    "    ‚îÇ   [Conv Block]‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ Skip\n",
    "    ‚îÇ        ‚Üì       ‚îÇ                ‚îÇ\n",
    "    ‚îÇ   [Bottleneck] ‚îÇ                ‚îÇ\n",
    "    ‚îÇ        ‚Üì       ‚îÇ                ‚îÇ\n",
    "    ‚îÇ   [Upsample]   ‚îÇ                ‚îÇ\n",
    "    ‚îÇ        ‚Üì       ‚îÇ                ‚îÇ\n",
    "    ‚îÇ   [Conv Block]‚Üê‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Concat\n",
    "    ‚îÇ        ‚Üì       ‚îÇ\n",
    "    ‚îÇ   [Upsample]   ‚îÇ\n",
    "    ‚îÇ        ‚Üì       ‚îÇ\n",
    "    ‚îÇ   [Conv Block]‚Üê‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Concat\n",
    "    ‚îÇ        ‚Üì       ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ>[Output]‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "            ‚îÇ\n",
    "            ‚Üì\n",
    "    Predicted Noise\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestep_embedding(timesteps, embedding_dim):\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings.\n",
    "    \n",
    "    This gives the model a unique \"fingerprint\" for each timestep,\n",
    "    allowing it to know how noisy the input is.\n",
    "    \n",
    "    Similar to positional embeddings in Transformers!\n",
    "    \"\"\"\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = math.log(10000) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, device=timesteps.device) * -emb)\n",
    "    emb = timesteps[:, None].float() * emb[None, :]\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "    if embedding_dim % 2 == 1:\n",
    "        emb = F.pad(emb, (0, 1))\n",
    "    return emb\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block with timestep conditioning.\n",
    "    \n",
    "    The timestep embedding is added to allow the network to behave\n",
    "    differently depending on the noise level.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, time_emb_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        self.norm1 = nn.GroupNorm(8, out_channels)\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
    "        \n",
    "        # Project timestep embedding to channel dimension\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_channels)\n",
    "        \n",
    "        # Skip connection (if channels change)\n",
    "        if in_channels != out_channels:\n",
    "            self.skip = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        else:\n",
    "            self.skip = nn.Identity()\n",
    "    \n",
    "    def forward(self, x, t_emb):\n",
    "        # First conv\n",
    "        h = self.conv1(x)\n",
    "        h = self.norm1(h)\n",
    "        h = F.silu(h)  # SiLU activation (smoother than ReLU)\n",
    "        \n",
    "        # Add timestep embedding\n",
    "        t = self.time_mlp(t_emb)[:, :, None, None]  # (B, C) -> (B, C, 1, 1)\n",
    "        h = h + t\n",
    "        \n",
    "        # Second conv\n",
    "        h = self.conv2(h)\n",
    "        h = self.norm2(h)\n",
    "        h = F.silu(h)\n",
    "        \n",
    "        # Skip connection\n",
    "        return h + self.skip(x)\n",
    "\n",
    "\n",
    "class SimpleUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified U-Net for MNIST diffusion.\n",
    "    \n",
    "    This is a minimal implementation for educational purposes.\n",
    "    Production models (like Stable Diffusion) are much larger!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels: int = 1,\n",
    "        out_channels: int = 1,\n",
    "        base_channels: int = 64,\n",
    "        time_emb_dim: int = 128\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.time_emb_dim = time_emb_dim\n",
    "        \n",
    "        # Time embedding MLP\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(time_emb_dim, time_emb_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim * 4, time_emb_dim),\n",
    "        )\n",
    "        \n",
    "        # Encoder (downsampling path)\n",
    "        self.enc1 = ResidualBlock(in_channels, base_channels, time_emb_dim)\n",
    "        self.enc2 = ResidualBlock(base_channels, base_channels * 2, time_emb_dim)\n",
    "        self.enc3 = ResidualBlock(base_channels * 2, base_channels * 4, time_emb_dim)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = ResidualBlock(base_channels * 4, base_channels * 4, time_emb_dim)\n",
    "        \n",
    "        # Decoder (upsampling path)\n",
    "        self.up3 = nn.ConvTranspose2d(base_channels * 4, base_channels * 4, 2, stride=2)\n",
    "        self.dec3 = ResidualBlock(base_channels * 8, base_channels * 2, time_emb_dim)  # *8 due to concat\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(base_channels * 2, base_channels * 2, 2, stride=2)\n",
    "        self.dec2 = ResidualBlock(base_channels * 4, base_channels, time_emb_dim)\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(base_channels, base_channels, 2, stride=2)\n",
    "        self.dec1 = ResidualBlock(base_channels * 2, base_channels, time_emb_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        self.out_conv = nn.Conv2d(base_channels, out_channels, 1)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Noisy image, shape (B, C, H, W)\n",
    "            t: Timesteps, shape (B,)\n",
    "            \n",
    "        Returns:\n",
    "            Predicted noise, shape (B, C, H, W)\n",
    "        \"\"\"\n",
    "        # Get timestep embedding\n",
    "        t_emb = get_timestep_embedding(t, self.time_emb_dim)\n",
    "        t_emb = self.time_mlp(t_emb)\n",
    "        \n",
    "        # Encoder\n",
    "        e1 = self.enc1(x, t_emb)      # (B, 64, 28, 28)\n",
    "        e2 = self.enc2(self.pool(e1), t_emb)  # (B, 128, 14, 14)\n",
    "        e3 = self.enc3(self.pool(e2), t_emb)  # (B, 256, 7, 7)\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(self.pool(e3), t_emb)  # (B, 256, 3, 3)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        # Pad if needed to match encoder dimensions\n",
    "        d3 = self.up3(b)  # (B, 256, 6, 6) -> need (B, 256, 7, 7)\n",
    "        d3 = F.interpolate(d3, size=e3.shape[2:])  # Match encoder size\n",
    "        d3 = self.dec3(torch.cat([d3, e3], dim=1), t_emb)  # (B, 128, 7, 7)\n",
    "        \n",
    "        d2 = self.up2(d3)\n",
    "        d2 = F.interpolate(d2, size=e2.shape[2:])\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1), t_emb)  # (B, 64, 14, 14)\n",
    "        \n",
    "        d1 = self.up1(d2)\n",
    "        d1 = F.interpolate(d1, size=e1.shape[2:])\n",
    "        d1 = self.dec1(torch.cat([d1, e1], dim=1), t_emb)  # (B, 64, 28, 28)\n",
    "        \n",
    "        # Output\n",
    "        return self.out_conv(d1)  # (B, 1, 28, 28)\n",
    "\n",
    "\n",
    "# Test the model\n",
    "model = SimpleUNet(in_channels=1, out_channels=1, base_channels=64).to(device)\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {n_params:,} ({n_params/1e6:.2f}M)\")\n",
    "\n",
    "# Test forward pass\n",
    "test_x = torch.randn(4, 1, 28, 28).to(device)\n",
    "test_t = torch.randint(0, 1000, (4,)).to(device)\n",
    "test_out = model(test_x, test_t)\n",
    "print(f\"Input shape: {test_x.shape}\")\n",
    "print(f\"Output shape: {test_out.shape}\")\n",
    "print(\"Model forward pass successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Training the Diffusion Model\n",
    "\n",
    "### The Training Objective\n",
    "\n",
    "We train the model to **predict the noise** that was added to an image:\n",
    "\n",
    "$$\\mathcal{L} = \\mathbb{E}_{x_0, \\epsilon, t} \\left[ \\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2 \\right]$$\n",
    "\n",
    "Where:\n",
    "- $\\epsilon$ = The actual noise we added\n",
    "- $\\epsilon_\\theta(x_t, t)$ = The noise our model predicts\n",
    "\n",
    "### üßí ELI5: Why Predict Noise?\n",
    "\n",
    "> It's like a game of \"spot the difference\":\n",
    "> \n",
    "> 1. We take a clean picture\n",
    "> 2. We add some static (noise) to it - we know exactly what static we added\n",
    "> 3. We show the noisy picture to the model and ask: \"What static do you see?\"\n",
    "> 4. The model guesses the static\n",
    "> 5. We compare its guess to the real static and train it to be more accurate\n",
    ">\n",
    "> After training, the model becomes really good at seeing \"what doesn't belong\" in an image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \n",
    "    For each batch:\n",
    "    1. Sample random timesteps\n",
    "    2. Add noise to images\n",
    "    3. Predict the noise\n",
    "    4. Compute MSE loss\n",
    "    5. Backpropagate\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Training\")\n",
    "    for images, _ in pbar:\n",
    "        images = images.to(device)\n",
    "        batch_size = images.shape[0]\n",
    "        \n",
    "        # Sample random timesteps for each image\n",
    "        t = torch.randint(0, scheduler.num_timesteps, (batch_size,), device=device)\n",
    "        \n",
    "        # Add noise\n",
    "        noisy_images, noise = scheduler.add_noise(images, t)\n",
    "        \n",
    "        # Predict the noise\n",
    "        noise_pred = model(noisy_images, t)\n",
    "        \n",
    "        # Compute loss (simple MSE)\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        \n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "# Setup training\n",
    "batch_size = 128\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10  # Increase for better results!\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "model = SimpleUNet(in_channels=1, out_channels=1, base_channels=64).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = NoiseScheduler(num_timesteps=1000, schedule_type=\"cosine\").to(device)\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  Epochs: {num_epochs}\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model!\n",
    "print(\"Starting training...\")\n",
    "print(\"(This will take a few minutes on DGX Spark)\\n\")\n",
    "\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = train_one_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Plot training curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n Training complete! Final loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Sampling (The Reverse Process)\n",
    "\n",
    "Now for the magic - generating NEW digits from pure noise!\n",
    "\n",
    "### The Sampling Algorithm (DDPM)\n",
    "\n",
    "Starting from pure noise $x_T \\sim \\mathcal{N}(0, I)$:\n",
    "\n",
    "For $t = T, T-1, ..., 1$:\n",
    "1. Predict the noise: $\\hat{\\epsilon} = \\epsilon_\\theta(x_t, t)$\n",
    "2. Estimate the clean image: $\\hat{x}_0 = \\frac{x_t - \\sqrt{1-\\bar{\\alpha}_t} \\cdot \\hat{\\epsilon}}{\\sqrt{\\bar{\\alpha}_t}}$\n",
    "3. Compute the previous step: $x_{t-1} = \\mu_\\theta(x_t, \\hat{x}_0, t) + \\sigma_t \\cdot z$ where $z \\sim \\mathcal{N}(0, I)$\n",
    "\n",
    "### üßí ELI5: The Reverse Process\n",
    "\n",
    "> Imagine you're an artist who's really good at \"cleaning up\" blurry photos.\n",
    "> \n",
    "> 1. Someone gives you a picture of pure static (random noise)\n",
    "> 2. You squint at it and think \"hmm, this COULD be a '7' if I clean it up\"\n",
    "> 3. You remove a tiny bit of static - now it's slightly less noisy\n",
    "> 4. You look again: \"yes, definitely looking more like a '7'!\"\n",
    "> 5. You repeat 999 more times, each time removing a little noise\n",
    "> 6. At the end: a crisp, clear digit '7'!\n",
    ">\n",
    "> The model learned to \"imagine\" what's under the static."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, scheduler, num_samples=16, image_size=28, num_channels=1, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generate samples using the trained model.\n",
    "    \n",
    "    This implements the DDPM sampling algorithm.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Start from pure noise\n",
    "    x = torch.randn(num_samples, num_channels, image_size, image_size, device=device)\n",
    "    \n",
    "    # Store intermediate steps for visualization\n",
    "    intermediates = [x.cpu().clone()]\n",
    "    \n",
    "    # Reverse diffusion\n",
    "    for t in tqdm(reversed(range(scheduler.num_timesteps)), desc=\"Sampling\", total=scheduler.num_timesteps):\n",
    "        t_batch = torch.full((num_samples,), t, device=device, dtype=torch.long)\n",
    "        \n",
    "        # Predict noise\n",
    "        noise_pred = model(x, t_batch)\n",
    "        \n",
    "        # Get coefficients\n",
    "        alpha = scheduler.alphas[t]\n",
    "        alpha_cumprod = scheduler.alphas_cumprod[t]\n",
    "        alpha_cumprod_prev = scheduler.alphas_cumprod_prev[t]\n",
    "        beta = scheduler.betas[t]\n",
    "        \n",
    "        # Predict x_0\n",
    "        x0_pred = (x - torch.sqrt(1 - alpha_cumprod) * noise_pred) / torch.sqrt(alpha_cumprod)\n",
    "        x0_pred = torch.clamp(x0_pred, -1, 1)  # Clip for stability\n",
    "        \n",
    "        # Compute posterior mean\n",
    "        posterior_mean = (\n",
    "            beta * torch.sqrt(alpha_cumprod_prev) / (1 - alpha_cumprod) * x0_pred +\n",
    "            (1 - alpha_cumprod_prev) * torch.sqrt(alpha) / (1 - alpha_cumprod) * x\n",
    "        )\n",
    "        \n",
    "        # Add noise (except for t=0)\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "            posterior_variance = scheduler.posterior_variance[t]\n",
    "            x = posterior_mean + torch.sqrt(posterior_variance) * noise\n",
    "        else:\n",
    "            x = posterior_mean\n",
    "        \n",
    "        # Store intermediate (every 100 steps)\n",
    "        if t % 100 == 0:\n",
    "            intermediates.append(x.cpu().clone())\n",
    "    \n",
    "    return x, intermediates\n",
    "\n",
    "\n",
    "# Generate samples!\n",
    "print(\"Generating new digits from noise...\")\n",
    "samples, intermediates = sample(model, scheduler, num_samples=16, device=device)\n",
    "print(f\"Generated {samples.shape[0]} samples!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display generated samples\n",
    "def show_samples(samples, title=\"Generated Samples\"):\n",
    "    \"\"\"Display a grid of samples.\"\"\"\n",
    "    samples = samples.cpu()\n",
    "    # Denormalize from [-1, 1] to [0, 1]\n",
    "    samples = (samples + 1) / 2\n",
    "    samples = torch.clamp(samples, 0, 1)\n",
    "    \n",
    "    grid = make_grid(samples, nrow=4, padding=2, normalize=False)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(grid.permute(1, 2, 0).numpy(), cmap='gray')\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_samples(samples, \"Generated MNIST Digits\")\n",
    "\n",
    "print(\"\\nüéâ Congratulations! You just generated images using a diffusion model you trained!\")\n",
    "print(\"\\nTips for better results:\")\n",
    "print(\"  - Train for more epochs (20-50)\")\n",
    "print(\"  - Use a larger model (increase base_channels)\")\n",
    "print(\"  - Try different learning rates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the reverse process step by step\n",
    "def visualize_reverse_process(intermediates):\n",
    "    \"\"\"Show how an image emerges from noise.\"\"\"\n",
    "    n_steps = len(intermediates)\n",
    "    fig, axes = plt.subplots(1, n_steps, figsize=(2 * n_steps, 2.5))\n",
    "    \n",
    "    # Take the first sample from each intermediate\n",
    "    for i, inter in enumerate(intermediates):\n",
    "        img = inter[0].squeeze().numpy()\n",
    "        img = (img + 1) / 2  # Denormalize\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        t = 1000 - (i * 100)\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(f't={max(t, 0)}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Reverse Diffusion: From Noise to Image', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_reverse_process(intermediates)\n",
    "\n",
    "print(\"\\nüîç Watch how the digit emerges from pure noise!\")\n",
    "print(\"   This is the reverse of the forward process we saw earlier.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Wrong Normalization Range\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Images in [0, 1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Gives [0, 1]\n",
    "])\n",
    "\n",
    "# ‚úÖ Right: Images in [-1, 1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Centers at 0\n",
    "])\n",
    "```\n",
    "**Why:** Noise is centered at 0, so images should be too!\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 2: Forgetting to Clip During Sampling\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: No clipping\n",
    "x0_pred = (x - sqrt_1m_alpha * noise_pred) / sqrt_alpha\n",
    "\n",
    "# ‚úÖ Right: Clip to valid range\n",
    "x0_pred = (x - sqrt_1m_alpha * noise_pred) / sqrt_alpha\n",
    "x0_pred = torch.clamp(x0_pred, -1, 1)\n",
    "```\n",
    "**Why:** Without clipping, predictions can explode and ruin generation!\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 3: Training with Very High Loss\n",
    "\n",
    "If your loss stays above 0.5:\n",
    "- Check your timestep embedding (is it being used?)\n",
    "- Verify the noise schedule (alphas_cumprod should go from 1‚Üí0)\n",
    "- Make sure images and noise have the same shape\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 4: Generating Blurry Images\n",
    "\n",
    "- Train for more epochs!\n",
    "- Increase model capacity (more channels)\n",
    "- Check if using the correct number of sampling steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How forward diffusion adds noise to images\n",
    "- ‚úÖ Different noise schedules (linear vs cosine)\n",
    "- ‚úÖ The U-Net architecture for denoising\n",
    "- ‚úÖ How to train a model to predict noise\n",
    "- ‚úÖ The reverse diffusion sampling process\n",
    "- ‚úÖ How to generate new images from random noise!\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "### Challenge 1: Class-Conditional Generation\n",
    "Modify the model to generate specific digits by adding class conditioning!\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "Add an embedding layer for class labels, similar to the timestep embedding:\n",
    "```python\n",
    "self.class_emb = nn.Embedding(10, time_emb_dim)  # 10 digits\n",
    "\n",
    "def forward(self, x, t, class_labels):\n",
    "    t_emb = get_timestep_embedding(t, self.time_emb_dim)\n",
    "    t_emb = self.time_mlp(t_emb)\n",
    "    c_emb = self.class_emb(class_labels)\n",
    "    emb = t_emb + c_emb  # Combine!\n",
    "    ...\n",
    "```\n",
    "</details>\n",
    "\n",
    "### Challenge 2: Faster Sampling with DDIM\n",
    "Implement DDIM sampling to generate images in 50 steps instead of 1000!\n",
    "\n",
    "### Challenge 3: Try Fashion-MNIST\n",
    "Adapt your model to generate clothing items from Fashion-MNIST!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [DDPM Paper](https://arxiv.org/abs/2006.11239) - \"Denoising Diffusion Probabilistic Models\"\n",
    "- [Improved DDPM](https://arxiv.org/abs/2102.09672) - Cosine schedule and other improvements\n",
    "- [DDIM Paper](https://arxiv.org/abs/2010.02502) - Faster sampling\n",
    "- [Lilian Weng's Blog](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) - Excellent explanation\n",
    "- [The Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion) - Code walkthrough\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save the trained model (optional)\nfrom pathlib import Path\n\n# Create output directory if it doesn't exist\noutput_dir = Path(\"./model_checkpoints\")\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nmodel_path = output_dir / \"mnist_diffusion_model.pt\"\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'losses': losses,\n}, model_path)\nprint(f\"Model saved to {model_path}\")\n\n# Clear GPU memory\nimport gc\ntorch.cuda.empty_cache()\ngc.collect()\nprint(\"GPU memory cleared\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you understand diffusion theory, proceed to:\n",
    "\n",
    "**Lab 2.6.2: Stable Diffusion Generation** - Learn to use production-grade diffusion models for text-to-image generation!\n",
    "\n",
    "You'll learn:\n",
    "- Loading SDXL on DGX Spark\n",
    "- Prompt engineering techniques\n",
    "- Guidance scale and its effects\n",
    "- Generating stunning images!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}