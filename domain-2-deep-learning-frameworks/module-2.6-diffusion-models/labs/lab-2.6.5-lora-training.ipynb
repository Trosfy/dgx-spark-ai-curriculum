{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 2.6.5: LoRA Style Training - Create Your Own Style\n\n**Module:** 2.6 - Diffusion Models  \n**Time:** 2 hours  \n**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê (Intermediate-Advanced)\n\n---\n\n## üéØ Learning Objectives\n\nBy the end of this notebook, you will:\n- [ ] Understand how LoRA adapters work for diffusion models\n- [ ] Prepare a dataset for style training\n- [ ] Train an SDXL LoRA on custom style images\n- [ ] Apply and combine multiple LoRAs\n- [ ] Adjust LoRA strength for style control\n\n---\n\n## üìö Prerequisites\n\n- Completed: Labs 2.6.1-2.6.4\n- Knowledge of: SDXL generation, basic training concepts\n- **Required packages:**\n  - `diffusers>=0.27.0`\n  - `peft>=0.10.0` (for LoRA adapter support)\n  - `transformers>=4.38.0`\n  - `datasets`\n\n**Version check:**\n```python\n# Run this to verify your versions\nimport diffusers, peft, transformers\nprint(f\"diffusers: {diffusers.__version__}\")  # Need >=0.27.0\nprint(f\"peft: {peft.__version__}\")            # Need >=0.10.0\nprint(f\"transformers: {transformers.__version__}\")  # Need >=4.38.0\n```\n\n---\n\n## üåç Real-World Context\n\n**LoRA lets you create custom AI art styles:**\n\n- **Artists** create their signature style as a LoRA\n- **Game studios** train LoRAs for consistent game art\n- **Brands** develop on-brand image generation\n- **Researchers** adapt models for specific domains\n\nWith DGX Spark's 128GB memory, you can train LoRAs comfortably at full precision!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What is LoRA?\n",
    "\n",
    "> **Imagine you have a master artist (SDXL) who can paint anything.**\n",
    ">\n",
    "> Instead of retraining them completely (expensive!), you give them a small\n",
    "> \"style guide\" notebook (LoRA) that shows examples of a specific style.\n",
    ">\n",
    "> Now when they paint, they reference the notebook to add that style!\n",
    ">\n",
    "> **Benefits:**\n",
    "> - The notebook is tiny (10-100MB vs 7GB for the full model)\n",
    "> - You can swap notebooks (styles) instantly\n",
    "> - You can combine multiple notebooks\n",
    "> - The original skills aren't forgotten\n",
    "\n",
    "### How LoRA Works (Technical)\n",
    "\n",
    "```\n",
    "Original Model:      With LoRA:\n",
    "                     \n",
    "   W                 W + ŒîW\n",
    "   ‚îÇ                  ‚îÇ\n",
    "   ‚îÇ                  ‚îÇ  where ŒîW = A √ó B\n",
    "   ‚ñº                  ‚ñº  (low-rank matrices)\n",
    "[Input] ‚îÄ‚îÄ‚ñ∫ [Output]  [Input] ‚îÄ‚îÄ‚ñ∫ [Output + Style Shift]\n",
    "\n",
    "- W: Original weights (frozen, not trained)\n",
    "- A, B: Small trainable matrices (rank 4-32)\n",
    "- ŒîW = A √ó B: The \"style adjustment\"\n",
    "```\n",
    "\n",
    "LoRA trains only A and B (~0.1% of total parameters)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Diffusers and PEFT\n",
    "from diffusers import StableDiffusionXLPipeline, DPMSolverMultistepScheduler\n",
    "from diffusers.utils import load_image\n",
    "from peft import LoraConfig\n",
    "\n",
    "# Data handling\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"Memory: {mem:.1f} GB\")\n",
    "    \n",
    "# Set random seed\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        return f\"{allocated:.2f}GB\"\n",
    "    return \"N/A\"\n",
    "\n",
    "def show_images_grid(images, titles=None, ncols=4, figsize=(16, 4)):\n",
    "    \"\"\"Display images in a grid.\"\"\"\n",
    "    n = len(images)\n",
    "    nrows = (n + ncols - 1) // ncols\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "    axes = axes.flatten() if nrows > 1 else [axes] if ncols == 1 else axes\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < n:\n",
    "            ax.imshow(images[i])\n",
    "            if titles and i < len(titles):\n",
    "                ax.set_title(titles[i], fontsize=10)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Helper functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding LoRA Configuration\n",
    "\n",
    "Before training, let's understand the key LoRA parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration explanation\n",
    "print(\"LoRA Configuration Parameters:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\"\"\n",
    "r (rank): How much capacity the LoRA has\n",
    "  - 4:   Minimal capacity, subtle changes\n",
    "  - 16:  Good balance (recommended)\n",
    "  - 32:  More capacity, stronger styles\n",
    "  - 64+: Maximum capacity, can overfit\n",
    "\n",
    "lora_alpha: Scaling factor\n",
    "  - Usually set to 2√ó rank (e.g., r=16, alpha=32)\n",
    "  - Higher = stronger effect at inference\n",
    "\n",
    "target_modules: Which layers to adapt\n",
    "  - For SDXL U-Net attention:\n",
    "    [\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"]\n",
    "  - Can also include cross-attention:\n",
    "    [\"add_k_proj\", \"add_v_proj\"]\n",
    "\n",
    "lora_dropout: Regularization\n",
    "  - 0.0: No dropout (can overfit)\n",
    "  - 0.05-0.1: Light regularization (recommended)\n",
    "\"\"\")\n",
    "\n",
    "# Example configuration\n",
    "example_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "print(\"\\nExample LoRA Config:\")\n",
    "print(f\"  Rank: {example_config.r}\")\n",
    "print(f\"  Alpha: {example_config.lora_alpha}\")\n",
    "print(f\"  Target modules: {example_config.target_modules}\")\n",
    "print(f\"  Dropout: {example_config.lora_dropout}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Preparing Training Data\n",
    "\n",
    "For this example, we'll create a synthetic \"art style\" dataset.\n",
    "In practice, you would use 10-50 images in your desired style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple dataset class\n",
    "class StyleDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for LoRA training.\n",
    "    \n",
    "    Expected structure:\n",
    "    data_dir/\n",
    "        image1.jpg\n",
    "        image1.txt  (caption)\n",
    "        image2.jpg\n",
    "        image2.txt\n",
    "        ...\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, resolution=1024, center_crop=True):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.resolution = resolution\n",
    "        self.center_crop = center_crop\n",
    "        \n",
    "        # Find all images\n",
    "        self.image_paths = []\n",
    "        self.captions = []\n",
    "        \n",
    "        for ext in ['*.jpg', '*.jpeg', '*.png', '*.webp']:\n",
    "            for img_path in self.data_dir.glob(ext):\n",
    "                caption_path = img_path.with_suffix('.txt')\n",
    "                if caption_path.exists():\n",
    "                    with open(caption_path) as f:\n",
    "                        caption = f.read().strip()\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.captions.append(caption)\n",
    "        \n",
    "        print(f\"Found {len(self.image_paths)} images with captions\")\n",
    "        \n",
    "        # Transforms\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(resolution, interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "            transforms.CenterCrop(resolution) if center_crop else transforms.RandomCrop(resolution),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),  # [-1, 1] range\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        caption = self.captions[idx]\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': image,\n",
    "            'caption': caption,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample training data for demonstration\n",
    "# In practice, you would use your own style images\n",
    "\n",
    "sample_data_dir = Path(\"./sample_lora_data\")\n",
    "sample_data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create synthetic \"style\" images using base SDXL\n",
    "# This is just for demonstration - normally you'd have real images\n",
    "\n",
    "print(\"Creating sample training data...\")\n",
    "\n",
    "# Load base model for generating sample data\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "pipe = pipe.to(device)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "# Generate sample images in a specific \"style\" (watercolor for demo)\n",
    "style_prefix = \"watercolor painting of\"\n",
    "subjects = [\n",
    "    \"a serene mountain landscape\",\n",
    "    \"a beautiful flower garden\",\n",
    "    \"a cozy cottage in the woods\",\n",
    "    \"a peaceful lake at sunset\",\n",
    "    \"rolling hills with wildflowers\",\n",
    "    \"a charming village street\",\n",
    "    \"autumn trees with golden leaves\",\n",
    "    \"a rustic bridge over a stream\",\n",
    "]\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    prompt = f\"{style_prefix} {subject}, soft colors, artistic, delicate brushstrokes\"\n",
    "    generator = torch.Generator(device=device).manual_seed(42 + i)\n",
    "    \n",
    "    image = pipe(\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=25,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    \n",
    "    # Save image and caption\n",
    "    image_path = sample_data_dir / f\"sample_{i:02d}.jpg\"\n",
    "    caption_path = sample_data_dir / f\"sample_{i:02d}.txt\"\n",
    "    \n",
    "    image.save(image_path)\n",
    "    with open(caption_path, 'w') as f:\n",
    "        f.write(f\"a watercolor painting of {subject}\")\n",
    "    \n",
    "    print(f\"  Created: {image_path.name}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(subjects)} sample training images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the sample training images\n",
    "sample_images = [Image.open(f) for f in sorted(sample_data_dir.glob(\"*.jpg\"))]\n",
    "show_images_grid(sample_images[:8], ncols=4, figsize=(16, 8))\n",
    "print(\"These are the sample images we'll train our LoRA on.\")\n",
    "print(\"The goal: Learn the 'watercolor' style to apply to any prompt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: LoRA Training Loop\n",
    "\n",
    "Now let's set up and run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Training configuration\n",
    "config = {\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_epochs': 50,  # Reduce for faster demo, increase for quality\n",
    "    'batch_size': 1,   # Increase if memory allows\n",
    "    'gradient_accumulation_steps': 4,\n",
    "    'lora_rank': 16,\n",
    "    'lora_alpha': 32,\n",
    "    'output_dir': './lora_output',\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "dataset = StyleDataset(sample_data_dir, resolution=1024)\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=config['batch_size'], \n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Set to 0 for notebook compatibility\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Batches per epoch: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA for the U-Net\n",
    "lora_config = LoraConfig(\n",
    "    r=config['lora_rank'],\n",
    "    lora_alpha=config['lora_alpha'],\n",
    "    target_modules=[\n",
    "        \"to_q\", \"to_k\", \"to_v\", \"to_out.0\",  # Self-attention\n",
    "        \"add_k_proj\", \"add_v_proj\",           # Cross-attention\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# Add LoRA adapters to U-Net\n",
    "pipe.unet.add_adapter(lora_config)\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable = sum(p.numel() for p in pipe.unet.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in pipe.unet.parameters())\n",
    "print(f\"\\nLoRA adapter added!\")\n",
    "print(f\"Trainable parameters: {trainable:,} ({100*trainable/total:.2f}%)\")\n",
    "print(f\"Total U-Net parameters: {total:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for training\n",
    "from diffusers import DDPMScheduler\n",
    "\n",
    "# Use DDPM scheduler for training noise schedule\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    subfolder=\"scheduler\"\n",
    ")\n",
    "\n",
    "# Freeze everything except LoRA\n",
    "pipe.vae.requires_grad_(False)\n",
    "pipe.text_encoder.requires_grad_(False)\n",
    "pipe.text_encoder_2.requires_grad_(False)\n",
    "\n",
    "# Move VAE to float32 for stability\n",
    "pipe.vae.to(dtype=torch.float32)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in pipe.unet.parameters() if p.requires_grad],\n",
    "    lr=config['learning_rate'],\n",
    ")\n",
    "\n",
    "print(\"Training setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(f\"\\nüöÄ Starting LoRA training...\")\n",
    "print(f\"   Epochs: {config['num_epochs']}\")\n",
    "print(f\"   Memory: {get_memory_usage()}\")\n",
    "print()\n",
    "\n",
    "losses = []\n",
    "pipe.unet.train()\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(dataloader):\n",
    "        # Move to device\n",
    "        pixel_values = batch['pixel_values'].to(device, dtype=torch.float32)\n",
    "        captions = batch['caption']\n",
    "        \n",
    "        # Encode images to latent space\n",
    "        with torch.no_grad():\n",
    "            latents = pipe.vae.encode(pixel_values).latent_dist.sample()\n",
    "            latents = latents * pipe.vae.config.scaling_factor\n",
    "            latents = latents.to(dtype=torch.bfloat16)\n",
    "        \n",
    "        # Sample noise and timesteps\n",
    "        noise = torch.randn_like(latents)\n",
    "        timesteps = torch.randint(\n",
    "            0, noise_scheduler.config.num_train_timesteps, \n",
    "            (latents.shape[0],), device=device\n",
    "        ).long()\n",
    "        \n",
    "        # Add noise\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "        \n",
    "        # Get text embeddings\n",
    "        with torch.no_grad():\n",
    "            prompt_embeds, pooled_embeds = pipe.encode_prompt(\n",
    "                captions,\n",
    "                device=device,\n",
    "                num_images_per_prompt=1,\n",
    "                do_classifier_free_guidance=False,\n",
    "            )\n",
    "        \n",
    "        # Add time embeddings for SDXL\n",
    "        add_time_ids = pipe._get_add_time_ids(\n",
    "            (1024, 1024),  # original_size\n",
    "            (0, 0),        # crops_coords_top_left\n",
    "            (1024, 1024),  # target_size\n",
    "            dtype=prompt_embeds.dtype,\n",
    "            text_encoder_projection_dim=pipe.text_encoder_2.config.projection_dim,\n",
    "        ).to(device)\n",
    "        add_time_ids = add_time_ids.repeat(latents.shape[0], 1)\n",
    "        \n",
    "        # Predict noise\n",
    "        added_cond_kwargs = {\n",
    "            \"text_embeds\": pooled_embeds,\n",
    "            \"time_ids\": add_time_ids,\n",
    "        }\n",
    "        \n",
    "        noise_pred = pipe.unet(\n",
    "            noisy_latents,\n",
    "            timesteps,\n",
    "            encoder_hidden_states=prompt_embeds,\n",
    "            added_cond_kwargs=added_cond_kwargs,\n",
    "        ).sample\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(noise_pred, noise, reduction=\"mean\")\n",
    "        loss = loss / config['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Gradient accumulation\n",
    "        if (step + 1) % config['gradient_accumulation_steps'] == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(pipe.unet.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{config['num_epochs']} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete!\")\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('LoRA Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained LoRA\n",
    "output_dir = Path(config['output_dir'])\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save LoRA weights\n",
    "pipe.unet.save_attn_procs(output_dir / \"watercolor_lora\")\n",
    "print(f\"\\nüíæ LoRA saved to {output_dir / 'watercolor_lora'}\")\n",
    "\n",
    "# Check file sizes\n",
    "for f in (output_dir / \"watercolor_lora\").iterdir():\n",
    "    size = f.stat().st_size / 1e6\n",
    "    print(f\"   {f.name}: {size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Testing the Trained LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to eval mode\n",
    "pipe.unet.eval()\n",
    "\n",
    "# Restore inference scheduler\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "# Test prompts - subjects NOT in training data\n",
    "test_prompts = [\n",
    "    \"a watercolor painting of a cat sleeping on a windowsill\",\n",
    "    \"a watercolor painting of a bustling city street\",\n",
    "    \"a watercolor painting of a spaceship in deep space\",\n",
    "    \"a watercolor painting of a dragon flying over mountains\",\n",
    "]\n",
    "\n",
    "print(\"Testing trained LoRA with new subjects...\\n\")\n",
    "\n",
    "test_images = []\n",
    "for prompt in test_prompts:\n",
    "    print(f\"Generating: {prompt[:50]}...\")\n",
    "    generator = torch.Generator(device=device).manual_seed(42)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image = pipe(\n",
    "            prompt=prompt,\n",
    "            num_inference_steps=25,\n",
    "            generator=generator,\n",
    "        ).images[0]\n",
    "    \n",
    "    test_images.append(image)\n",
    "\n",
    "# Display results\n",
    "titles = [p[23:50] + \"...\" for p in test_prompts]\n",
    "show_images_grid(test_images, titles, ncols=2, figsize=(14, 14))\n",
    "\n",
    "print(\"\\nüé® The watercolor style is now applied to new subjects!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Adjusting LoRA Strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different LoRA strengths\n",
    "prompt = \"a watercolor painting of a majestic eagle soaring through clouds\"\n",
    "strengths = [0.0, 0.3, 0.5, 0.7, 1.0]\n",
    "\n",
    "images = []\n",
    "for strength in strengths:\n",
    "    print(f\"Generating with LoRA scale={strength}...\")\n",
    "    \n",
    "    # Set LoRA scale\n",
    "    pipe.unet.set_adapters([\"default\"], adapter_weights=[strength])\n",
    "    \n",
    "    generator = torch.Generator(device=device).manual_seed(42)\n",
    "    with torch.no_grad():\n",
    "        image = pipe(\n",
    "            prompt=prompt,\n",
    "            num_inference_steps=25,\n",
    "            generator=generator,\n",
    "        ).images[0]\n",
    "    \n",
    "    images.append(image)\n",
    "\n",
    "# Reset to full strength\n",
    "pipe.unet.set_adapters([\"default\"], adapter_weights=[1.0])\n",
    "\n",
    "# Display\n",
    "titles = [f\"Scale: {s}\" for s in strengths]\n",
    "show_images_grid(images, titles, ncols=5, figsize=(20, 4))\n",
    "\n",
    "print(\"\\nüìä LoRA Scale Guide:\")\n",
    "print(\"  0.0: No LoRA effect (base model)\")\n",
    "print(\"  0.3-0.5: Subtle style influence\")\n",
    "print(\"  0.7-1.0: Strong style application\")\n",
    "print(\"  >1.0: Over-stylized (use carefully)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Loading Pre-trained LoRAs from CivitAI\n",
    "\n",
    "You can also use community-created LoRAs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading a LoRA from file\n",
    "print(\"Loading a LoRA from file:\")\n",
    "print(\"\"\"\n",
    "# Download a LoRA from CivitAI or Hugging Face\n",
    "# Then load it like this:\n",
    "\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load LoRA weights\n",
    "pipe.load_lora_weights(\"./my_lora.safetensors\")\n",
    "\n",
    "# Generate with LoRA\n",
    "image = pipe(\n",
    "    prompt=\"...\",\n",
    "    cross_attention_kwargs={\"scale\": 0.8},  # LoRA strength\n",
    ").images[0]\n",
    "\n",
    "# Unload LoRA when done\n",
    "pipe.unload_lora_weights()\n",
    "\"\"\")\n",
    "\n",
    "print(\"üí° Popular LoRA sources:\")\n",
    "print(\"  - https://civitai.com (largest community)\")\n",
    "print(\"  - https://huggingface.co/models (official)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Too High Learning Rate\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Will cause training instability\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# ‚úÖ Right: Start lower\n",
    "learning_rate = 1e-4  # Good starting point\n",
    "```\n",
    "\n",
    "### Mistake 2: Too Few Training Images\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Will overfit to specific images\n",
    "dataset = 3 images\n",
    "\n",
    "# ‚úÖ Right: Need variety\n",
    "dataset = 10-50 images with varied subjects\n",
    "```\n",
    "\n",
    "### Mistake 3: Poor Captions\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Generic captions\n",
    "caption = \"a picture\"\n",
    "\n",
    "# ‚úÖ Right: Descriptive with trigger word\n",
    "caption = \"a watercolor painting of a mountain landscape, soft colors\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How LoRA adapters work (low-rank updates)\n",
    "- ‚úÖ Preparing datasets for style training\n",
    "- ‚úÖ Training a custom SDXL LoRA\n",
    "- ‚úÖ Adjusting LoRA strength at inference\n",
    "- ‚úÖ Loading pre-trained LoRAs\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del pipe\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"GPU memory cleared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Proceed to **Lab 2.6.6: Image Generation Pipeline** to build a complete end-to-end generation system!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}