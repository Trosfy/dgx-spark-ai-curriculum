{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.4.2: Mamba Architecture Study\n",
    "\n",
    "**Module:** 2.4 - Efficient Architectures  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê (Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- [ ] Understand the mathematical foundation of State Space Models\n",
    "- [ ] Implement a simplified selective scan algorithm in PyTorch\n",
    "- [ ] Visualize how Mamba's state evolves across a sequence\n",
    "- [ ] Compare Mamba's \"attention\" with transformer attention patterns\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Lab 2.4.1 (Mamba Inference)\n",
    "- Knowledge of: Linear algebra basics, RNNs\n",
    "- Helpful: Understanding of differential equations (but not required)\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "Understanding Mamba's internals helps you:\n",
    "- **Debug** why a model behaves unexpectedly on certain inputs\n",
    "- **Optimize** inference by understanding bottlenecks\n",
    "- **Choose** the right architecture for your use case\n",
    "- **Research** improvements and hybrid architectures (like Jamba)\n",
    "\n",
    "Major companies are investing in SSM research: Google (S4), Microsoft, NVIDIA, and AI21 Labs (Jamba)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: State Space Models\n",
    "\n",
    "> **Imagine you're a weather forecaster...**\n",
    ">\n",
    "> You have a \"state\" that represents your understanding of the weather:\n",
    "> - Temperature trends\n",
    "> - Humidity patterns  \n",
    "> - Pressure systems\n",
    ">\n",
    "> Each day, you update this state based on new observations:\n",
    "> ```\n",
    "> new_state = A √ó old_state + B √ó today's_observation\n",
    "> prediction = C √ó new_state\n",
    "> ```\n",
    ">\n",
    "> Where A, B, C are learned \"rules\" for how weather evolves.\n",
    ">\n",
    "> **The \"Selective\" part in Mamba:**\n",
    "> Normal forecasters use the SAME rules every day. But imagine if the rules CHANGED based on what you observe:\n",
    "> - Sunny day ‚Üí update temperature state strongly\n",
    "> - Rainy day ‚Üí update humidity state strongly\n",
    ">\n",
    "> This is what makes Mamba special: A, B, C change based on input!\n",
    "\n",
    "### The Mathematics (Simplified)\n",
    "\n",
    "**Classical State Space Model:**\n",
    "```\n",
    "h(t) = A¬∑h(t-1) + B¬∑x(t)    # State update\n",
    "y(t) = C¬∑h(t) + D¬∑x(t)       # Output\n",
    "```\n",
    "\n",
    "**Selective State Space (Mamba):**\n",
    "```\n",
    "A, B, C = f(x(t))            # Parameters depend on input!\n",
    "h(t) = A¬∑h(t-1) + B¬∑x(t)     # State update (with dynamic A, B)\n",
    "y(t) = C¬∑h(t)                # Output (with dynamic C)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup and Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Optional\n",
    "import math\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Classical State Space Model\n",
    "\n",
    "Before understanding Mamba's selective scan, let's implement a simple state space model.\n",
    "\n",
    "### The Continuous-Time State Space\n",
    "\n",
    "The classical formulation (from control theory):\n",
    "```\n",
    "dx/dt = Ax + Bu    (state evolves continuously)\n",
    "y = Cx + Du        (output)\n",
    "```\n",
    "\n",
    "For sequence modeling, we discretize this:\n",
    "```\n",
    "h[k] = ƒÄ¬∑h[k-1] + BÃÑ¬∑x[k]\n",
    "y[k] = C¬∑h[k]\n",
    "```\n",
    "\n",
    "Where ƒÄ and BÃÑ are discretized versions of A and B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSSM(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple (non-selective) State Space Model.\n",
    "    \n",
    "    This is the building block that Mamba extends with selectivity.\n",
    "    \n",
    "    Parameters:\n",
    "        d_model: Input/output dimension\n",
    "        d_state: Hidden state dimension (compression factor)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int = 64, d_state: int = 16):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        \n",
    "        # State space parameters\n",
    "        # A: State transition matrix (d_state x d_state)\n",
    "        # We use a diagonal initialization for stability\n",
    "        self.A = nn.Parameter(torch.randn(d_state) * 0.1)\n",
    "        \n",
    "        # B: Input projection (d_state x d_model)\n",
    "        self.B = nn.Parameter(torch.randn(d_state, d_model) * 0.1)\n",
    "        \n",
    "        # C: Output projection (d_model x d_state)\n",
    "        self.C = nn.Parameter(torch.randn(d_model, d_state) * 0.1)\n",
    "        \n",
    "        # Delta: Discretization step (learnable)\n",
    "        self.log_delta = nn.Parameter(torch.zeros(d_model))\n",
    "        \n",
    "    def discretize(self):\n",
    "        \"\"\"\n",
    "        Discretize continuous parameters using Zero-Order Hold (ZOH).\n",
    "        \n",
    "        ƒÄ = exp(Œî¬∑A)\n",
    "        BÃÑ = (Œî¬∑A)^(-1) ¬∑ (ƒÄ - I) ¬∑ Œî¬∑B ‚âà Œî¬∑B for small Œî\n",
    "        \"\"\"\n",
    "        delta = F.softplus(self.log_delta)  # Ensure positive\n",
    "        \n",
    "        # Simplified discretization\n",
    "        A_discrete = torch.exp(delta.unsqueeze(-1) * self.A)  # [d_model, d_state]\n",
    "        B_discrete = delta.unsqueeze(-1) * self.B.T  # [d_model, d_state]\n",
    "        \n",
    "        return A_discrete, B_discrete\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Process a sequence through the SSM.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape [batch, seq_len, d_model]\n",
    "            \n",
    "        Returns:\n",
    "            output: Output tensor of shape [batch, seq_len, d_model]\n",
    "            states: Hidden states of shape [batch, seq_len, d_state]\n",
    "        \"\"\"\n",
    "        batch, seq_len, _ = x.shape\n",
    "        \n",
    "        # Discretize parameters\n",
    "        A_bar, B_bar = self.discretize()\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        h = torch.zeros(batch, self.d_state, device=x.device)\n",
    "        \n",
    "        outputs = []\n",
    "        states = []\n",
    "        \n",
    "        # Sequential scan (this is what Mamba parallelizes!)\n",
    "        for t in range(seq_len):\n",
    "            # Input at time t\n",
    "            x_t = x[:, t, :]  # [batch, d_model]\n",
    "            \n",
    "            # State update: h = A¬∑h + B¬∑x\n",
    "            # Using diagonal A for efficiency\n",
    "            h = torch.einsum('bd,ds->bs', A_bar * h.unsqueeze(1).expand(-1, self.d_model, -1).mean(1), \n",
    "                           torch.ones(self.d_state, self.d_state, device=x.device))\n",
    "            h = h[:, :self.d_state] + torch.einsum('bd,ds->bs', x_t, self.B)\n",
    "            \n",
    "            # Output: y = C¬∑h\n",
    "            y_t = torch.einsum('bs,ds->bd', h, self.C.T)\n",
    "            \n",
    "            outputs.append(y_t)\n",
    "            states.append(h.clone())\n",
    "        \n",
    "        output = torch.stack(outputs, dim=1)\n",
    "        all_states = torch.stack(states, dim=1)\n",
    "        \n",
    "        return output, all_states\n",
    "\n",
    "# Test the simple SSM\n",
    "simple_ssm = SimpleSSM(d_model=64, d_state=16).to(device)\n",
    "test_input = torch.randn(2, 100, 64, device=device)  # [batch=2, seq=100, dim=64]\n",
    "\n",
    "with torch.no_grad():\n",
    "    output, states = simple_ssm(test_input)\n",
    "\n",
    "print(f\"Input shape:  {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"States shape: {states.shape}\")\n",
    "print(f\"\\n‚úÖ Simple SSM working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Key Insight: The Sequential Bottleneck\n",
    "\n",
    "Notice the `for t in range(seq_len)` loop? This is the **sequential bottleneck** of RNNs!\n",
    "\n",
    "- We can't parallelize because h[t] depends on h[t-1]\n",
    "- This makes training slow on GPUs (which love parallelism)\n",
    "\n",
    "**Mamba's trick**: The parallel scan algorithm that computes all states in O(log n) parallel steps!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: The Selective Scan\n",
    "\n",
    "Now let's implement Mamba's key innovation: **input-dependent parameters**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectiveSSM(nn.Module):\n",
    "    \"\"\"\n",
    "    Selective State Space Model (simplified Mamba block).\n",
    "    \n",
    "    The key difference from SimpleSSM:\n",
    "    - A, B, C, delta are computed FROM THE INPUT\n",
    "    - This makes the model \"selective\" about what to remember\n",
    "    \n",
    "    Parameters:\n",
    "        d_model: Input/output dimension\n",
    "        d_state: Hidden state dimension\n",
    "        d_conv: Local convolution width (for local context)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int = 64, d_state: int = 16, d_conv: int = 4):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        \n",
    "        # Expand dimension for internal processing\n",
    "        self.d_inner = d_model * 2\n",
    "        \n",
    "        # Input projection\n",
    "        self.in_proj = nn.Linear(d_model, self.d_inner * 2, bias=False)\n",
    "        \n",
    "        # Local convolution (like in Mamba)\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            self.d_inner, self.d_inner,\n",
    "            kernel_size=d_conv,\n",
    "            padding=d_conv - 1,\n",
    "            groups=self.d_inner,  # Depthwise\n",
    "        )\n",
    "        \n",
    "        # Selective parameters - computed from input!\n",
    "        self.x_proj = nn.Linear(self.d_inner, d_state * 2 + 1, bias=False)  # B, C, delta\n",
    "        \n",
    "        # A is still a base parameter (log for stability)\n",
    "        self.A_log = nn.Parameter(torch.log(torch.arange(1, d_state + 1, dtype=torch.float32)))\n",
    "        \n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, return_states: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Selective scan forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input [batch, seq_len, d_model]\n",
    "            return_states: If True, also return hidden states\n",
    "            \n",
    "        Returns:\n",
    "            output: [batch, seq_len, d_model]\n",
    "            states (optional): [batch, seq_len, d_state]\n",
    "        \"\"\"\n",
    "        batch, seq_len, _ = x.shape\n",
    "        \n",
    "        # Input projection and split\n",
    "        xz = self.in_proj(x)  # [batch, seq_len, d_inner * 2]\n",
    "        x_branch, z = xz.chunk(2, dim=-1)  # Each [batch, seq_len, d_inner]\n",
    "        \n",
    "        # Local convolution (for position awareness)\n",
    "        x_conv = self.conv1d(x_branch.transpose(1, 2))[:, :, :seq_len].transpose(1, 2)\n",
    "        x_conv = F.silu(x_conv)  # Activation\n",
    "        \n",
    "        # Generate selective parameters FROM INPUT\n",
    "        x_params = self.x_proj(x_conv)  # [batch, seq_len, d_state*2 + 1]\n",
    "        \n",
    "        # Split into B, C, delta\n",
    "        B = x_params[:, :, :self.d_state]  # [batch, seq_len, d_state]\n",
    "        C = x_params[:, :, self.d_state:2*self.d_state]  # [batch, seq_len, d_state]\n",
    "        delta = F.softplus(x_params[:, :, -1])  # [batch, seq_len] - discretization step\n",
    "        \n",
    "        # Get A (negative for stability)\n",
    "        A = -torch.exp(self.A_log)  # [d_state]\n",
    "        \n",
    "        # Discretize: A_bar = exp(delta * A)\n",
    "        A_bar = torch.exp(delta.unsqueeze(-1) * A)  # [batch, seq_len, d_state]\n",
    "        \n",
    "        # Run selective scan\n",
    "        h = torch.zeros(batch, self.d_state, device=x.device)\n",
    "        outputs = []\n",
    "        states = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # Input-dependent state update\n",
    "            h = A_bar[:, t] * h + B[:, t] * x_conv[:, t, 0:1]  # Selective!\n",
    "            \n",
    "            # Input-dependent output\n",
    "            y_t = (C[:, t] * h).sum(dim=-1, keepdim=True)  # Selective!\n",
    "            \n",
    "            outputs.append(y_t)\n",
    "            states.append(h.clone())\n",
    "        \n",
    "        y = torch.cat(outputs, dim=-1)  # [batch, d_inner]\n",
    "        y = y.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "        \n",
    "        # Gated output (like in Mamba)\n",
    "        y = y * F.silu(z)\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.out_proj(y)\n",
    "        \n",
    "        if return_states:\n",
    "            all_states = torch.stack(states, dim=1)\n",
    "            return output, all_states\n",
    "        return output\n",
    "\n",
    "# Test selective SSM\n",
    "selective_ssm = SelectiveSSM(d_model=64, d_state=16).to(device)\n",
    "test_input = torch.randn(2, 50, 64, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output, states = selective_ssm(test_input, return_states=True)\n",
    "\n",
    "print(f\"Input shape:  {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"States shape: {states.shape}\")\n",
    "print(f\"\\n‚úÖ Selective SSM working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç The Key Difference: Selectivity\n",
    "\n",
    "Notice these lines in the code:\n",
    "\n",
    "```python\n",
    "# Parameters computed FROM input!\n",
    "x_params = self.x_proj(x_conv)  \n",
    "B = x_params[:, :, :self.d_state]\n",
    "C = x_params[:, :, self.d_state:2*self.d_state]\n",
    "delta = F.softplus(x_params[:, :, -1])\n",
    "```\n",
    "\n",
    "**This is the magic of Mamba!**\n",
    "- When processing \"important\" tokens, B and delta can be large (update state more)\n",
    "- When processing \"unimportant\" tokens, B can be small (ignore them)\n",
    "- C controls what parts of state to output\n",
    "\n",
    "The model LEARNS what's important!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Visualizing State Evolution\n",
    "\n",
    "Let's see how the hidden state evolves as it processes a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_state_evolution(model, tokenizer, text: str, max_tokens: int = 50):\n",
    "    \"\"\"\n",
    "    Visualize how the model's hidden state changes across tokens.\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)[:max_tokens]\n",
    "    token_strs = [tokenizer.decode([t]) for t in tokens]\n",
    "    \n",
    "    # Create input\n",
    "    input_ids = torch.tensor([tokens], device=device)\n",
    "    \n",
    "    # Get hidden states from first layer\n",
    "    model.config.output_hidden_states = True\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, output_hidden_states=True)\n",
    "    \n",
    "    # Extract hidden states\n",
    "    # For Mamba, hidden_states[0] is embeddings, hidden_states[1] is after first layer\n",
    "    hidden_states = outputs.hidden_states[1][0].cpu().numpy()  # [seq_len, hidden_dim]\n",
    "    \n",
    "    # Take first 32 dimensions for visualization\n",
    "    states_viz = hidden_states[:, :32]\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "    \n",
    "    # Heatmap of state evolution\n",
    "    im = axes[0].imshow(states_viz.T, aspect='auto', cmap='RdBu_r', \n",
    "                        interpolation='nearest')\n",
    "    axes[0].set_xlabel('Token Position')\n",
    "    axes[0].set_ylabel('State Dimension')\n",
    "    axes[0].set_title('Hidden State Evolution (First 32 Dimensions)', fontweight='bold')\n",
    "    plt.colorbar(im, ax=axes[0], label='Activation')\n",
    "    \n",
    "    # Add token labels if not too many\n",
    "    if len(token_strs) <= 30:\n",
    "        axes[0].set_xticks(range(len(token_strs)))\n",
    "        axes[0].set_xticklabels(token_strs, rotation=45, ha='right', fontsize=8)\n",
    "    \n",
    "    # State magnitude over time\n",
    "    state_norms = np.linalg.norm(hidden_states, axis=1)\n",
    "    axes[1].plot(state_norms, 'b-', linewidth=2)\n",
    "    axes[1].fill_between(range(len(state_norms)), state_norms, alpha=0.3)\n",
    "    axes[1].set_xlabel('Token Position')\n",
    "    axes[1].set_ylabel('State Magnitude (L2 norm)')\n",
    "    axes[1].set_title('State Magnitude Over Sequence', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return hidden_states, token_strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a Mamba model for visualization\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Use smaller model for faster loading\n",
    "MODEL_NAME = \"state-spaces/mamba-130m-hf\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} for state visualization...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize state evolution on a sample text\n",
    "sample_text = \"The quick brown fox jumps over the lazy dog. This is a test.\"\n",
    "\n",
    "print(f\"Analyzing: '{sample_text}'\\n\")\n",
    "states, tokens = visualize_state_evolution(model, tokenizer, sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare state evolution for different content types\n",
    "\n",
    "texts = {\n",
    "    \"Code\": \"def fibonacci(n):\\n    if n <= 1:\\n        return n\\n    return fibonacci(n-1) + fibonacci(n-2)\",\n",
    "    \"Math\": \"The derivative of x squared plus three x equals two x plus three\",\n",
    "    \"Story\": \"Once upon a time in a faraway kingdom there lived a brave knight\",\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(len(texts), 1, figsize=(14, 4*len(texts)))\n",
    "\n",
    "for idx, (label, text) in enumerate(texts.items()):\n",
    "    # Get hidden states\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)[:40]\n",
    "    input_ids = torch.tensor([tokens], device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, output_hidden_states=True)\n",
    "    \n",
    "    hidden = outputs.hidden_states[1][0].cpu().numpy()[:, :32]\n",
    "    \n",
    "    # Plot\n",
    "    im = axes[idx].imshow(hidden.T, aspect='auto', cmap='RdBu_r')\n",
    "    axes[idx].set_title(f'{label}: \"{text[:50]}...\"', fontweight='bold')\n",
    "    axes[idx].set_ylabel('State Dim')\n",
    "    plt.colorbar(im, ax=axes[idx])\n",
    "\n",
    "axes[-1].set_xlabel('Token Position')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Notice how different content types create different state patterns!\")\n",
    "print(\"   This is the 'selectivity' - the model adapts to what it's processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Comparing with Attention Patterns\n",
    "\n",
    "Transformers have explicit attention patterns we can visualize. What does Mamba's \"implicit attention\" look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_effective_attention(states: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute an \"effective attention\" matrix from state evolution.\n",
    "    \n",
    "    This shows how much each position's output depends on each previous position.\n",
    "    Computed as correlation between state changes.\n",
    "    \"\"\"\n",
    "    seq_len, hidden_dim = states.shape\n",
    "    \n",
    "    # Compute state differences (how much state changed)\n",
    "    state_diffs = np.diff(states, axis=0)  # [seq_len-1, hidden_dim]\n",
    "    \n",
    "    # Compute correlation matrix\n",
    "    # This approximates \"how much does position j influence position i\"\n",
    "    attention_like = np.zeros((seq_len, seq_len))\n",
    "    \n",
    "    for i in range(seq_len):\n",
    "        for j in range(i + 1):  # Causal: can only attend to past\n",
    "            # Measure similarity between current state and state at j\n",
    "            similarity = np.dot(states[i], states[j]) / (\n",
    "                np.linalg.norm(states[i]) * np.linalg.norm(states[j]) + 1e-8\n",
    "            )\n",
    "            # Weight by recency (newer = more influence)\n",
    "            decay = np.exp(-0.1 * (i - j))\n",
    "            attention_like[i, j] = similarity * decay\n",
    "    \n",
    "    # Normalize rows to sum to 1\n",
    "    row_sums = attention_like.sum(axis=1, keepdims=True) + 1e-8\n",
    "    attention_like = attention_like / row_sums\n",
    "    \n",
    "    return attention_like\n",
    "\n",
    "# Compute and visualize effective attention\n",
    "text = \"The cat sat on the mat because it was tired.\"\n",
    "tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "token_strs = [tokenizer.decode([t]) for t in tokens]\n",
    "\n",
    "input_ids = torch.tensor([tokens], device=device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, output_hidden_states=True)\n",
    "states = outputs.hidden_states[1][0].cpu().numpy()\n",
    "\n",
    "# Compute effective attention\n",
    "eff_attention = compute_effective_attention(states)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(eff_attention, cmap='Blues')\n",
    "ax.set_xticks(range(len(token_strs)))\n",
    "ax.set_yticks(range(len(token_strs)))\n",
    "ax.set_xticklabels(token_strs, rotation=45, ha='right')\n",
    "ax.set_yticklabels(token_strs)\n",
    "ax.set_xlabel('Source Token (attending from)')\n",
    "ax.set_ylabel('Target Token (attending to)')\n",
    "ax.set_title('Mamba \"Effective Attention\" Pattern\\n(derived from state evolution)', \n",
    "             fontweight='bold')\n",
    "plt.colorbar(im, ax=ax, label='Attention Weight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Compare this to transformer attention:\")\n",
    "print(\"   - Mamba's pattern is smoother (compressed state)\")\n",
    "print(\"   - Strong diagonal = recency bias (recent tokens matter more)\")\n",
    "print(\"   - But notice: 'it' still attends to 'cat' (pronoun resolution!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: The Parallel Scan Algorithm (Conceptual)\n",
    "\n",
    "How does Mamba avoid the sequential bottleneck? The **parallel scan** algorithm!\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "The recurrence `h[t] = A¬∑h[t-1] + B¬∑x[t]` can be rewritten as an associative operation:\n",
    "\n",
    "```\n",
    "‚äï: (a‚ÇÅ, b‚ÇÅ) ‚äï (a‚ÇÇ, b‚ÇÇ) = (a‚ÇÅ¬∑a‚ÇÇ, a‚ÇÇ¬∑b‚ÇÅ + b‚ÇÇ)\n",
    "```\n",
    "\n",
    "This associativity allows us to compute all states in O(log n) parallel steps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_scan(A: torch.Tensor, B: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Sequential scan: h[t] = A[t]¬∑h[t-1] + B[t]¬∑x[t]\n",
    "    \n",
    "    Time: O(n) sequential\n",
    "    \"\"\"\n",
    "    seq_len = x.shape[0]\n",
    "    h = torch.zeros_like(x[0])\n",
    "    outputs = []\n",
    "    \n",
    "    for t in range(seq_len):\n",
    "        h = A[t] * h + B[t] * x[t]\n",
    "        outputs.append(h)\n",
    "    \n",
    "    return torch.stack(outputs)\n",
    "\n",
    "def parallel_scan(A: torch.Tensor, B: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Parallel scan using associative operation.\n",
    "    \n",
    "    Time: O(log n) parallel steps\n",
    "    \n",
    "    This is a simplified illustration - real implementation uses GPU primitives.\n",
    "    \"\"\"\n",
    "    seq_len = x.shape[0]\n",
    "    \n",
    "    # Pack into tuples: (A_cumulative, B¬∑x_cumulative)\n",
    "    # Start: [(A[0], B[0]¬∑x[0]), (A[1], B[1]¬∑x[1]), ...]\n",
    "    coeffs = A.clone()\n",
    "    values = B * x\n",
    "    \n",
    "    # Parallel prefix sum\n",
    "    offset = 1\n",
    "    while offset < seq_len:\n",
    "        # In parallel: combine pairs at distance 'offset'\n",
    "        for i in range(offset, seq_len):\n",
    "            # Associative operation: (a1, b1) ‚äï (a2, b2) = (a1¬∑a2, a2¬∑b1 + b2)\n",
    "            values[i] = coeffs[i] * values[i - offset] + values[i]\n",
    "            coeffs[i] = coeffs[i] * coeffs[i - offset]\n",
    "        offset *= 2\n",
    "    \n",
    "    return values\n",
    "\n",
    "# Compare sequential vs parallel\n",
    "seq_len = 8\n",
    "A = torch.rand(seq_len) * 0.9  # Decay factor < 1 for stability\n",
    "B = torch.rand(seq_len)\n",
    "x = torch.rand(seq_len)\n",
    "\n",
    "h_seq = sequential_scan(A, B, x)\n",
    "h_par = parallel_scan(A.clone(), B.clone(), x.clone())\n",
    "\n",
    "print(\"Sequential scan result:\")\n",
    "print(h_seq.numpy())\n",
    "print(\"\\nParallel scan result:\")\n",
    "print(h_par.numpy())\n",
    "print(f\"\\nResults match: {torch.allclose(h_seq, h_par, atol=1e-5)}\")\n",
    "print(\"\\n‚úÖ Parallel scan produces identical results!\")\n",
    "print(\"\\nüìä Complexity comparison:\")\n",
    "print(f\"   Sequential: {seq_len} steps\")\n",
    "print(f\"   Parallel:   {int(np.ceil(np.log2(seq_len)))} parallel steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Parallel Scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how parallel scan works\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sequential scan visualization\n",
    "ax = axes[0]\n",
    "seq_len = 8\n",
    "for i in range(seq_len):\n",
    "    # Draw node\n",
    "    ax.scatter(i, 0, s=300, c='#3498DB', zorder=5)\n",
    "    ax.text(i, 0, str(i), ha='center', va='center', fontweight='bold', color='white')\n",
    "    \n",
    "    # Draw arrow from previous\n",
    "    if i > 0:\n",
    "        ax.annotate('', xy=(i-0.15, 0), xytext=(i-0.85, 0),\n",
    "                   arrowprops=dict(arrowstyle='->', color='#E74C3C', lw=2))\n",
    "\n",
    "ax.set_xlim(-0.5, seq_len - 0.5)\n",
    "ax.set_ylim(-1, 1)\n",
    "ax.set_title('Sequential Scan\\n(O(n) sequential steps)', fontweight='bold', fontsize=12)\n",
    "ax.axis('off')\n",
    "ax.text(seq_len/2, -0.7, f'Total: {seq_len} sequential operations', \n",
    "        ha='center', fontsize=11, color='#E74C3C')\n",
    "\n",
    "# Parallel scan visualization\n",
    "ax = axes[1]\n",
    "levels = int(np.ceil(np.log2(seq_len)))\n",
    "\n",
    "# Draw nodes\n",
    "for i in range(seq_len):\n",
    "    ax.scatter(i, 0, s=300, c='#3498DB', zorder=5)\n",
    "    ax.text(i, 0, str(i), ha='center', va='center', fontweight='bold', color='white')\n",
    "\n",
    "# Draw parallel operations\n",
    "colors = ['#27AE60', '#E74C3C', '#9B59B6']\n",
    "for level in range(levels):\n",
    "    offset = 2 ** level\n",
    "    y_pos = -(level + 1) * 0.5\n",
    "    \n",
    "    for i in range(offset, seq_len):\n",
    "        # Draw arc showing combination\n",
    "        ax.annotate('', xy=(i, y_pos + 0.1), xytext=(i - offset, y_pos + 0.1),\n",
    "                   arrowprops=dict(arrowstyle='->', color=colors[level % len(colors)], \n",
    "                                  lw=2, connectionstyle='arc3,rad=-0.2'))\n",
    "    \n",
    "    ax.text(-0.7, y_pos, f'Step {level+1}', fontsize=10, va='center')\n",
    "\n",
    "ax.set_xlim(-1.5, seq_len - 0.5)\n",
    "ax.set_ylim(-levels * 0.5 - 0.5, 0.5)\n",
    "ax.set_title('Parallel Scan\\n(O(log n) parallel steps)', fontweight='bold', fontsize=12)\n",
    "ax.axis('off')\n",
    "ax.text(seq_len/2, -levels * 0.5 - 0.3, f'Total: {levels} parallel steps', \n",
    "        ha='center', fontsize=11, color='#27AE60')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç Key insight: With enough parallel processors, the scan completes in O(log n) time!\")\n",
    "print(f\"   For seq_len={seq_len}: {seq_len} sequential ops ‚Üí {levels} parallel ops\")\n",
    "print(f\"   For seq_len=65536: 65536 sequential ops ‚Üí {int(np.log2(65536))} parallel ops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Thinking Mamba is \"Just an RNN\"\n",
    "```python\n",
    "# ‚ùå Wrong mental model\n",
    "# Mamba is NOT like traditional RNNs (LSTM, GRU)\n",
    "\n",
    "# ‚úÖ Correct understanding\n",
    "# Mamba = Selective State Space + Parallel Scan + Hardware-aware design\n",
    "# - Selectivity makes it content-aware (like attention)\n",
    "# - Parallel scan makes it trainable at scale\n",
    "# - Hardware design makes it fast on GPUs\n",
    "```\n",
    "\n",
    "### Mistake 2: Expecting Identical Behavior to Transformers\n",
    "```python\n",
    "# ‚ùå Mamba won't perfectly copy what transformers do\n",
    "# - Different inductive bias\n",
    "# - Different handling of long-range dependencies\n",
    "\n",
    "# ‚úÖ Understand the tradeoffs\n",
    "# - Mamba: Better memory, may miss precise long-range patterns\n",
    "# - Transformer: Precise attention, but O(n¬≤) memory\n",
    "# - Hybrid (Jamba): Best of both!\n",
    "```\n",
    "\n",
    "### Mistake 3: Ignoring State Initialization\n",
    "```python\n",
    "# ‚ùå Starting with random state\n",
    "h = torch.randn(batch, d_state)  # Bad!\n",
    "\n",
    "# ‚úÖ Start with zeros for deterministic behavior\n",
    "h = torch.zeros(batch, d_state)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ The mathematical foundation of State Space Models\n",
    "- ‚úÖ How selectivity makes Mamba content-aware\n",
    "- ‚úÖ The parallel scan algorithm for efficient training\n",
    "- ‚úÖ How to visualize Mamba's \"implicit attention\"\n",
    "- ‚úÖ The tradeoffs vs transformer attention\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise: State Evolution Analysis\n",
    "\n",
    "Analyze how Mamba's state evolves differently for:\n",
    "1. Repetitive text (\"the the the the the\")\n",
    "2. Structured text (code with consistent patterns)\n",
    "3. Diverse text (random words)\n",
    "\n",
    "Visualize and compare the state evolution patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [Mamba Paper](https://arxiv.org/abs/2312.00752) - Original paper with full mathematical details\n",
    "- [Annotated S4](https://srush.github.io/annotated-s4/) - Excellent walkthrough of the predecessor\n",
    "- [Parallel Scan Tutorial](https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda) - GPU Gems chapter on parallel scan\n",
    "- [Jamba Paper](https://arxiv.org/abs/2403.19887) - Hybrid Mamba-Attention architecture\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "\n",
    "if 'model' in dir():\n",
    "    del model\n",
    "if 'simple_ssm' in dir():\n",
    "    del simple_ssm\n",
    "if 'selective_ssm' in dir():\n",
    "    del selective_ssm\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
