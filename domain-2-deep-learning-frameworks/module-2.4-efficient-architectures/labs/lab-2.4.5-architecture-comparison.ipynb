{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.4.5: Architecture Comparison\n",
    "\n",
    "**Module:** 2.4 - Efficient Architectures  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** â­â­â­ (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- [ ] Systematically compare Mamba, Transformer, and MoE architectures\n",
    "- [ ] Benchmark perplexity, speed, and memory usage\n",
    "- [ ] Understand when to use each architecture\n",
    "- [ ] Create a decision framework for architecture selection\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Prerequisites\n",
    "\n",
    "- Completed: Labs 2.4.1-2.4.4\n",
    "- Knowledge of: Perplexity, benchmarking methodology\n",
    "- Hardware: DGX Spark (128GB) recommended for full comparison\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ Real-World Context\n",
    "\n",
    "**Choosing the Right Architecture**\n",
    "\n",
    "In production, you'll face decisions like:\n",
    "- \"We need to process 100K-token documents\" â†’ Mamba?\n",
    "- \"We want the best quality regardless of cost\" â†’ Dense Transformer?\n",
    "- \"We need 70B-level quality at 7B compute cost\" â†’ MoE?\n",
    "- \"We need both quality AND long context\" â†’ Hybrid (Jamba)?\n",
    "\n",
    "This lab gives you the data to make these decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name()\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Memory: {total_memory:.1f} GB\")\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU memory between models.\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current GPU memory usage in GB.\"\"\"\n",
    "    return torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass BenchmarkResult:\n    \"\"\"Container for benchmark results.\"\"\"\n    model_name: str\n    architecture: str\n    total_params_b: float\n    active_params_b: float\n    memory_gb: float\n    perplexity: Optional[float]\n    tokens_per_second: Dict[int, float]  # context_length -> speed\n    peak_memory: Dict[int, float]  # context_length -> memory\n\n# Define models to compare\n# Adjust based on your available memory\nMODELS_TO_COMPARE = {\n    \"mamba-2.8b\": {\n        \"name\": \"state-spaces/mamba-2.8b-hf\",\n        \"architecture\": \"Mamba\",  # State Space Models\n        \"active_ratio\": 1.0,  # All params active\n    },\n    # Smaller transformer for fair comparison\n    \"phi-2\": {\n        \"name\": \"microsoft/phi-2\",\n        \"architecture\": \"Transformer\",\n        \"active_ratio\": 1.0,\n    },\n    # MoE model (if memory allows)\n    # \"deepseek-moe-16b\": {\n    #     \"name\": \"deepseek-ai/deepseek-moe-16b-base\",\n    #     \"architecture\": \"MoE\",\n    #     \"active_ratio\": 0.156,  # 2.5B / 16B\n    # },\n}\n\nprint(f\"Will compare {len(MODELS_TO_COMPARE)} models:\")\nfor name, config in MODELS_TO_COMPARE.items():\n    print(f\"  - {name} ({config['architecture']})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Benchmarking Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArchitectureBenchmark:\n",
    "    \"\"\"\n",
    "    Comprehensive benchmarking suite for architecture comparison.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, context_lengths: List[int] = None):\n",
    "        self.context_lengths = context_lengths or [512, 1024, 2048, 4096, 8192]\n",
    "        self.results: List[BenchmarkResult] = []\n",
    "    \n",
    "    def load_model(self, model_name: str) -> Tuple:\n",
    "        \"\"\"Load a model and tokenizer.\"\"\"\n",
    "        clear_memory()\n",
    "        \n",
    "        print(f\"Loading {model_name}...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        \n",
    "        return model, tokenizer\n",
    "    \n",
    "    def benchmark_speed(self, model, tokenizer, context_lengths: List[int],\n",
    "                       generation_length: int = 50,\n",
    "                       warmup: int = 2, runs: int = 3) -> Dict[int, float]:\n",
    "        \"\"\"\n",
    "        Benchmark generation speed at various context lengths.\n",
    "        \"\"\"\n",
    "        speeds = {}\n",
    "        peak_memories = {}\n",
    "        \n",
    "        for ctx_len in context_lengths:\n",
    "            print(f\"  Testing context: {ctx_len}...\")\n",
    "            \n",
    "            try:\n",
    "                # Create input\n",
    "                input_ids = torch.randint(\n",
    "                    100, tokenizer.vocab_size - 100,\n",
    "                    (1, ctx_len), device=device\n",
    "                )\n",
    "                \n",
    "                # Warmup\n",
    "                for _ in range(warmup):\n",
    "                    with torch.no_grad():\n",
    "                        _ = model.generate(\n",
    "                            input_ids[:, :min(512, ctx_len)],\n",
    "                            max_new_tokens=5,\n",
    "                            do_sample=False,\n",
    "                            pad_token_id=tokenizer.pad_token_id,\n",
    "                        )\n",
    "                \n",
    "                # Clear and benchmark\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "                \n",
    "                times = []\n",
    "                for _ in range(runs):\n",
    "                    torch.cuda.synchronize()\n",
    "                    start = time.perf_counter()\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        _ = model.generate(\n",
    "                            input_ids,\n",
    "                            max_new_tokens=generation_length,\n",
    "                            do_sample=False,\n",
    "                            pad_token_id=tokenizer.pad_token_id,\n",
    "                        )\n",
    "                    \n",
    "                    torch.cuda.synchronize()\n",
    "                    times.append(time.perf_counter() - start)\n",
    "                \n",
    "                avg_time = np.mean(times)\n",
    "                speeds[ctx_len] = generation_length / avg_time\n",
    "                peak_memories[ctx_len] = torch.cuda.max_memory_allocated() / 1e9\n",
    "                \n",
    "                print(f\"    {speeds[ctx_len]:.1f} tok/s, {peak_memories[ctx_len]:.1f} GB peak\")\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    print(f\"    OOM at {ctx_len}\")\n",
    "                    speeds[ctx_len] = 0\n",
    "                    peak_memories[ctx_len] = float('inf')\n",
    "                    torch.cuda.empty_cache()\n",
    "                else:\n",
    "                    raise\n",
    "        \n",
    "        return speeds, peak_memories\n",
    "    \n",
    "    def benchmark_perplexity(self, model, tokenizer, \n",
    "                            text: str = None, max_length: int = 2048) -> float:\n",
    "        \"\"\"\n",
    "        Calculate perplexity on sample text.\n",
    "        \"\"\"\n",
    "        if text is None:\n",
    "            # Use a standard test text\n",
    "            text = \"\"\"The Transformer architecture has revolutionized natural language processing. \n",
    "            Introduced in the paper \"Attention is All You Need\", it relies on self-attention \n",
    "            mechanisms to process sequences in parallel. However, the quadratic complexity of \n",
    "            attention poses challenges for long sequences. Recent innovations like Mamba and \n",
    "            Mixture of Experts offer alternative approaches with different trade-offs.\"\"\"\n",
    "            text = text * 20  # Repeat for longer sequence\n",
    "        \n",
    "        encodings = tokenizer(text, return_tensors=\"pt\", truncation=True, \n",
    "                             max_length=max_length)\n",
    "        input_ids = encodings.input_ids.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        perplexity = torch.exp(loss).item()\n",
    "        return perplexity\n",
    "    \n",
    "    def run_benchmark(self, model_configs: Dict) -> List[BenchmarkResult]:\n",
    "        \"\"\"\n",
    "        Run complete benchmark on all models.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for model_key, config in model_configs.items():\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Benchmarking: {model_key}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            try:\n",
    "                model, tokenizer = self.load_model(config['name'])\n",
    "                \n",
    "                # Get model info\n",
    "                total_params = sum(p.numel() for p in model.parameters())\n",
    "                active_params = total_params * config['active_ratio']\n",
    "                memory = get_memory_usage()\n",
    "                \n",
    "                print(f\"Loaded: {total_params/1e9:.2f}B params, {memory:.1f} GB\")\n",
    "                \n",
    "                # Speed benchmark\n",
    "                print(\"\\nSpeed benchmark:\")\n",
    "                speeds, peak_memories = self.benchmark_speed(\n",
    "                    model, tokenizer, self.context_lengths\n",
    "                )\n",
    "                \n",
    "                # Perplexity benchmark\n",
    "                print(\"\\nPerplexity benchmark:\")\n",
    "                try:\n",
    "                    ppl = self.benchmark_perplexity(model, tokenizer)\n",
    "                    print(f\"  Perplexity: {ppl:.2f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Perplexity failed: {e}\")\n",
    "                    ppl = None\n",
    "                \n",
    "                result = BenchmarkResult(\n",
    "                    model_name=model_key,\n",
    "                    architecture=config['architecture'],\n",
    "                    total_params_b=total_params / 1e9,\n",
    "                    active_params_b=active_params / 1e9,\n",
    "                    memory_gb=memory,\n",
    "                    perplexity=ppl,\n",
    "                    tokens_per_second=speeds,\n",
    "                    peak_memory=peak_memories,\n",
    "                )\n",
    "                results.append(result)\n",
    "                \n",
    "                # Cleanup\n",
    "                del model, tokenizer\n",
    "                clear_memory()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to benchmark {model_key}: {e}\")\n",
    "                clear_memory()\n",
    "        \n",
    "        self.results = results\n",
    "        return results\n",
    "\n",
    "print(\"Benchmark framework ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmarks\n",
    "benchmark = ArchitectureBenchmark(\n",
    "    context_lengths=[512, 1024, 2048, 4096]  # Adjust based on memory\n",
    ")\n",
    "\n",
    "results = benchmark.run_benchmark(MODELS_TO_COMPARE)\n",
    "print(f\"\\nâœ… Completed {len(results)} benchmarks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Visualizing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def visualize_benchmark_results(results: List[BenchmarkResult]):\n    \"\"\"\n    Create comprehensive visualizations of benchmark results.\n    \"\"\"\n    if not results:\n        print(\"No results to visualize\")\n        return\n    \n    # Get common context lengths\n    all_contexts = set()\n    for r in results:\n        all_contexts.update(r.tokens_per_second.keys())\n    contexts = sorted(all_contexts)\n    \n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    \n    # Architecture colors: Mamba (State Space Models), Transformer, MoE\n    colors = {'Mamba': '#27AE60', 'Transformer': '#E74C3C', 'MoE': '#3498DB'}\n    \n    # 1. Speed comparison\n    ax = axes[0, 0]\n    for result in results:\n        ctx = sorted(result.tokens_per_second.keys())\n        speeds = [result.tokens_per_second[c] for c in ctx]\n        color = colors.get(result.architecture, '#9B59B6')\n        ax.plot(ctx, speeds, 'o-', label=result.model_name, \n               linewidth=2, markersize=8, color=color)\n    \n    ax.set_xlabel('Context Length (tokens)')\n    ax.set_ylabel('Tokens per Second')\n    ax.set_title('Generation Speed vs Context Length', fontweight='bold')\n    ax.legend()\n    ax.set_xscale('log', base=2)\n    ax.grid(True, alpha=0.3)\n    \n    # 2. Memory comparison\n    ax = axes[0, 1]\n    for result in results:\n        ctx = sorted(result.peak_memory.keys())\n        mems = [result.peak_memory[c] if result.peak_memory[c] < float('inf') else np.nan \n                for c in ctx]\n        color = colors.get(result.architecture, '#9B59B6')\n        ax.plot(ctx, mems, 's-', label=result.model_name,\n               linewidth=2, markersize=8, color=color)\n    \n    ax.axhline(y=128, color='gray', linestyle='--', label='DGX Spark (128GB)')\n    ax.set_xlabel('Context Length (tokens)')\n    ax.set_ylabel('Peak Memory (GB)')\n    ax.set_title('Memory Usage vs Context Length', fontweight='bold')\n    ax.legend()\n    ax.set_xscale('log', base=2)\n    ax.grid(True, alpha=0.3)\n    \n    # 3. Model size comparison\n    ax = axes[1, 0]\n    model_names = [r.model_name for r in results]\n    total_params = [r.total_params_b for r in results]\n    active_params = [r.active_params_b for r in results]\n    \n    x = np.arange(len(model_names))\n    width = 0.35\n    \n    bars1 = ax.bar(x - width/2, total_params, width, label='Total Params', \n                  color='#3498DB')\n    bars2 = ax.bar(x + width/2, active_params, width, label='Active Params',\n                  color='#27AE60')\n    \n    ax.set_xlabel('Model')\n    ax.set_ylabel('Parameters (Billions)')\n    ax.set_title('Total vs Active Parameters', fontweight='bold')\n    ax.set_xticks(x)\n    ax.set_xticklabels(model_names, rotation=15)\n    ax.legend()\n    \n    # 4. Perplexity comparison\n    ax = axes[1, 1]\n    ppls = [r.perplexity if r.perplexity else 0 for r in results]\n    bars_colors = [colors.get(r.architecture, '#9B59B6') for r in results]\n    \n    bars = ax.bar(model_names, ppls, color=bars_colors)\n    ax.set_xlabel('Model')\n    ax.set_ylabel('Perplexity (lower = better)')\n    ax.set_title('Model Perplexity Comparison', fontweight='bold')\n    ax.set_xticklabels(model_names, rotation=15)\n    \n    # Add legend for architecture colors\n    from matplotlib.patches import Patch\n    legend_elements = [Patch(facecolor=c, label=a) for a, c in colors.items()]\n    ax.legend(handles=legend_elements, loc='upper right')\n    \n    plt.tight_layout()\n    plt.show()\n\nvisualize_benchmark_results(results)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "def create_summary_table(results: List[BenchmarkResult]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a summary comparison table.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for r in results:\n",
    "        # Get speed at common context length (1024)\n",
    "        speed_1k = r.tokens_per_second.get(1024, r.tokens_per_second.get(512, 0))\n",
    "        speed_4k = r.tokens_per_second.get(4096, 0)\n",
    "        \n",
    "        data.append({\n",
    "            'Model': r.model_name,\n",
    "            'Architecture': r.architecture,\n",
    "            'Total Params (B)': f\"{r.total_params_b:.2f}\",\n",
    "            'Active Params (B)': f\"{r.active_params_b:.2f}\",\n",
    "            'Model Memory (GB)': f\"{r.memory_gb:.1f}\",\n",
    "            'Speed @1K (tok/s)': f\"{speed_1k:.1f}\" if speed_1k else \"N/A\",\n",
    "            'Speed @4K (tok/s)': f\"{speed_4k:.1f}\" if speed_4k else \"N/A\",\n",
    "            'Perplexity': f\"{r.perplexity:.2f}\" if r.perplexity else \"N/A\",\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "if results:\n",
    "    summary_df = create_summary_table(results)\n",
    "    print(\"\\nğŸ“Š ARCHITECTURE COMPARISON SUMMARY\")\n",
    "    print(\"=\" * 100)\n",
    "    print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Decision Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_architecture(requirements: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Recommend an architecture based on requirements.\n",
    "    \n",
    "    Args:\n",
    "        requirements: Dict with keys like:\n",
    "            - max_context_length: int\n",
    "            - quality_priority: 'high', 'medium', 'low'\n",
    "            - memory_limit_gb: float\n",
    "            - speed_priority: 'high', 'medium', 'low'\n",
    "            - use_case: 'chat', 'document', 'code', 'general'\n",
    "    \n",
    "    Returns:\n",
    "        Recommendation string with reasoning\n",
    "    \"\"\"\n",
    "    ctx_len = requirements.get('max_context_length', 4096)\n",
    "    quality = requirements.get('quality_priority', 'medium')\n",
    "    memory = requirements.get('memory_limit_gb', 24)\n",
    "    speed = requirements.get('speed_priority', 'medium')\n",
    "    use_case = requirements.get('use_case', 'general')\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # Long context requirement\n",
    "    if ctx_len > 32000:\n",
    "        recommendations.append({\n",
    "            'arch': 'Mamba',\n",
    "            'score': 10,\n",
    "            'reason': 'Linear memory scaling essential for 32K+ context'\n",
    "        })\n",
    "        recommendations.append({\n",
    "            'arch': 'Jamba (Hybrid)',\n",
    "            'score': 9,\n",
    "            'reason': 'Combines Mamba efficiency with Attention quality'\n",
    "        })\n",
    "    \n",
    "    # Quality priority\n",
    "    if quality == 'high':\n",
    "        recommendations.append({\n",
    "            'arch': 'Dense Transformer',\n",
    "            'score': 8 if ctx_len < 16000 else 5,\n",
    "            'reason': 'Best quality for most tasks (if context fits)'\n",
    "        })\n",
    "    \n",
    "    # Memory constraint with large model need\n",
    "    if memory < 50 and requirements.get('model_size', 'large') == 'large':\n",
    "        recommendations.append({\n",
    "            'arch': 'MoE',\n",
    "            'score': 9,\n",
    "            'reason': 'More parameters with less compute/memory'\n",
    "        })\n",
    "    \n",
    "    # Speed priority\n",
    "    if speed == 'high':\n",
    "        recommendations.append({\n",
    "            'arch': 'Mamba',\n",
    "            'score': 8,\n",
    "            'reason': 'No KV cache overhead, consistent speed'\n",
    "        })\n",
    "    \n",
    "    # Use case specific\n",
    "    if use_case == 'document':\n",
    "        recommendations.append({\n",
    "            'arch': 'Mamba',\n",
    "            'score': 9,\n",
    "            'reason': 'Document processing benefits from linear scaling'\n",
    "        })\n",
    "    elif use_case == 'code':\n",
    "        recommendations.append({\n",
    "            'arch': 'Dense Transformer',\n",
    "            'score': 8,\n",
    "            'reason': 'Code completion benefits from precise attention'\n",
    "        })\n",
    "    \n",
    "    # Sort by score\n",
    "    recommendations.sort(key=lambda x: -x['score'])\n",
    "    \n",
    "    # Build recommendation text\n",
    "    text = \"\\nğŸ¯ ARCHITECTURE RECOMMENDATION\\n\" + \"=\" * 50 + \"\\n\"\n",
    "    text += f\"\\nRequirements:\\n\"\n",
    "    for k, v in requirements.items():\n",
    "        text += f\"  - {k}: {v}\\n\"\n",
    "    \n",
    "    text += f\"\\nTop Recommendations:\\n\"\n",
    "    for i, rec in enumerate(recommendations[:3], 1):\n",
    "        text += f\"\\n  {i}. {rec['arch']} (Score: {rec['score']}/10)\\n\"\n",
    "        text += f\"     Reason: {rec['reason']}\\n\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example recommendations\n",
    "print(recommend_architecture({\n",
    "    'max_context_length': 100000,\n",
    "    'quality_priority': 'high',\n",
    "    'memory_limit_gb': 128,\n",
    "    'use_case': 'document'\n",
    "}))\n",
    "\n",
    "print(recommend_architecture({\n",
    "    'max_context_length': 4096,\n",
    "    'quality_priority': 'high',\n",
    "    'memory_limit_gb': 24,\n",
    "    'use_case': 'code'\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Architecture Selection Flowchart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual decision tree\n",
    "decision_tree = \"\"\"\n",
    "ğŸ”„ ARCHITECTURE SELECTION FLOWCHART\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚ Need context > 32K? â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                               â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚ YES                              â”‚ NO\n",
    "              â–¼                                  â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Need best qualityâ”‚              â”‚ Memory limited?  â”‚\n",
    "    â”‚ at long context? â”‚              â”‚ (<50GB for 70B+) â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚                                  â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ YES        â”‚ NO â”‚              â”‚ YES          â”‚ NO â”‚\n",
    "    â–¼            â–¼    â”‚              â–¼              â–¼    â”‚\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•— â•”â•â•â•â•â•â•â•â•—         â•”â•â•â•â•â•â•â•â•â•â•â•â•â•— â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘  JAMBA    â•‘ â•‘ MAMBA â•‘         â•‘    MoE     â•‘ â•‘ Dense          â•‘\n",
    "â•‘ (Hybrid)  â•‘ â•‘       â•‘         â•‘ (Sparse)   â•‘ â•‘ Transformer    â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â• â•šâ•â•â•â•â•â•â•â•         â•šâ•â•â•â•â•â•â•â•â•â•â•â•â• â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    " Best of both   Fastest          More params,   Best quality\n",
    " worlds         for long ctx     less compute   (if fits!)\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ğŸ“‹ QUICK REFERENCE:\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Use Case        â”‚ Recommended  â”‚ Context Limitâ”‚ Notes           â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Chatbot         â”‚ Transformer  â”‚ ~8K tokens   â”‚ Quality matters â”‚\n",
    "â”‚ Document QA     â”‚ Mamba/Jamba  â”‚ 100K+ tokens â”‚ Long context    â”‚\n",
    "â”‚ Code Generation â”‚ Transformer  â”‚ ~16K tokens  â”‚ Precision neededâ”‚\n",
    "â”‚ Audio/Video     â”‚ Mamba        â”‚ 1M+ samples  â”‚ Streaming       â”‚\n",
    "â”‚ Cost-Efficient  â”‚ MoE          â”‚ ~8K tokens   â”‚ 10x efficiency  â”‚\n",
    "â”‚ Edge Deployment â”‚ Small Trans. â”‚ ~4K tokens   â”‚ Memory limited  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\"\n",
    "\n",
    "print(decision_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- âœ… How to systematically benchmark different architectures\n",
    "- âœ… Speed, memory, and quality tradeoffs\n",
    "- âœ… When to choose each architecture type\n",
    "- âœ… A decision framework for real-world selection\n",
    "\n",
    "---\n",
    "\n",
    "## âœ‹ Try It Yourself\n",
    "\n",
    "Add another model to the comparison (e.g., Qwen-MoE or a different transformer size) and re-run the benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_memory()\n",
    "print(\"âœ… Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}