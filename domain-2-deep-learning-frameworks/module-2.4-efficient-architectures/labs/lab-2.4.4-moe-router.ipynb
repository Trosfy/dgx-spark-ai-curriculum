{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.4.4: MoE Router Analysis\n",
    "\n",
    "**Module:** 2.4 - Efficient Architectures  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê (Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- [ ] Understand how the router/gating network works\n",
    "- [ ] Extract and analyze router weights\n",
    "- [ ] Visualize expert selection distribution\n",
    "- [ ] Understand load balancing and auxiliary losses\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Lab 2.4.3 (MoE Exploration)\n",
    "- Knowledge of: Softmax, top-k selection, loss functions\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The Router Problem**\n",
    "\n",
    "The router is the \"brain\" of MoE‚Äîit decides which experts process each token. A bad router:\n",
    "- Uses only a few experts (wasting capacity)\n",
    "- Creates training instability\n",
    "- Fails to learn specialization\n",
    "\n",
    "Understanding routers helps you:\n",
    "- Debug underperforming MoE models\n",
    "- Design better routing strategies\n",
    "- Optimize inference for your use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: The Router\n",
    "\n",
    "> **Remember our hospital analogy?**\n",
    ">\n",
    "> The router is like the receptionist who decides which doctor you see.\n",
    ">\n",
    "> **How does the receptionist work?**\n",
    "> 1. Look at your symptoms (input features)\n",
    "> 2. Assign a \"relevance score\" for each doctor\n",
    "> 3. Send you to the top 2 doctors (top-k routing)\n",
    "> 4. Each doctor spends time proportional to their score (weighted combination)\n",
    ">\n",
    "> **The challenge:**\n",
    "> - Don't send ALL patients to the same doctor (overload!)\n",
    "> - Don't let any doctor sit idle (waste!)\n",
    "> - This is the \"load balancing\" problem\n",
    "\n",
    "### The Math\n",
    "\n",
    "```\n",
    "Router Input:  x ‚àà ‚Ñù^d           (hidden state for a token)\n",
    "Router Weights: W ‚àà ‚Ñù^(n_experts √ó d)\n",
    "Router Logits: logits = W @ x    (score for each expert)\n",
    "Expert Weights: weights = softmax(topk(logits, k))\n",
    "Output: Œ£ weights[i] √ó Expert[i](x)  (weighted expert outputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import gc\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MoE model (same as Lab 2.4.3)\n",
    "MODEL_NAME = \"deepseek-ai/deepseek-moe-16b-base\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Extracting Router Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_router_layers(model) -> List[Tuple[str, torch.nn.Module]]:\n",
    "    \"\"\"\n",
    "    Find all router/gate modules in an MoE model.\n",
    "    \"\"\"\n",
    "    routers = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        # Check for common router names\n",
    "        if any(keyword in name.lower() for keyword in ['gate', 'router']):\n",
    "            if hasattr(module, 'weight'):  # Linear layer\n",
    "                routers.append((name, module))\n",
    "    \n",
    "    return routers\n",
    "\n",
    "# Find routers\n",
    "routers = find_router_layers(model)\n",
    "print(f\"Found {len(routers)} router layers\")\n",
    "\n",
    "if routers:\n",
    "    for i, (name, module) in enumerate(routers[:5]):\n",
    "        print(f\"  Layer {i}: {name}\")\n",
    "        if hasattr(module, 'weight'):\n",
    "            print(f\"           Weight shape: {module.weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_router_weights(model, layer_idx: int = 0) -> Optional[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Extract router weight matrix from a specific layer.\n",
    "    \n",
    "    Returns:\n",
    "        Weight tensor of shape [num_experts, hidden_dim]\n",
    "    \"\"\"\n",
    "    routers = find_router_layers(model)\n",
    "    \n",
    "    if layer_idx < len(routers):\n",
    "        name, module = routers[layer_idx]\n",
    "        return module.weight.data.clone().float()\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Extract router weights from first layer\n",
    "router_weights = extract_router_weights(model, layer_idx=0)\n",
    "\n",
    "if router_weights is not None:\n",
    "    print(f\"Router weight shape: {router_weights.shape}\")\n",
    "    print(f\"  Interpretation: {router_weights.shape[0]} experts, {router_weights.shape[1]} hidden dim\")\n",
    "    \n",
    "    num_experts = router_weights.shape[0]\n",
    "    hidden_dim = router_weights.shape[1]\n",
    "else:\n",
    "    print(\"Could not extract router weights\")\n",
    "    num_experts = 64  # Default\n",
    "    hidden_dim = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Analyzing Router Weight Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if router_weights is not None:\n",
    "    # Analyze weight statistics\n",
    "    weight_norms = torch.norm(router_weights, dim=1).cpu().numpy()\n",
    "    weight_means = router_weights.mean(dim=1).cpu().numpy()\n",
    "    weight_stds = router_weights.std(dim=1).cpu().numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Weight norms per expert\n",
    "    axes[0, 0].bar(range(num_experts), weight_norms, color='#3498DB')\n",
    "    axes[0, 0].set_xlabel('Expert Index')\n",
    "    axes[0, 0].set_ylabel('L2 Norm')\n",
    "    axes[0, 0].set_title('Router Weight Norms by Expert', fontweight='bold')\n",
    "    axes[0, 0].axhline(y=weight_norms.mean(), color='red', linestyle='--', \n",
    "                       label=f'Mean: {weight_norms.mean():.2f}')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Weight distribution heatmap\n",
    "    sample_weights = router_weights[:, :100].cpu().numpy()  # First 100 dims\n",
    "    im = axes[0, 1].imshow(sample_weights, aspect='auto', cmap='RdBu_r')\n",
    "    axes[0, 1].set_xlabel('Hidden Dimension (first 100)')\n",
    "    axes[0, 1].set_ylabel('Expert Index')\n",
    "    axes[0, 1].set_title('Router Weight Heatmap', fontweight='bold')\n",
    "    plt.colorbar(im, ax=axes[0, 1])\n",
    "    \n",
    "    # Expert similarity matrix\n",
    "    weights_norm = F.normalize(router_weights, dim=1)\n",
    "    similarity = (weights_norm @ weights_norm.T).cpu().numpy()\n",
    "    \n",
    "    im = axes[1, 0].imshow(similarity, cmap='viridis')\n",
    "    axes[1, 0].set_xlabel('Expert Index')\n",
    "    axes[1, 0].set_ylabel('Expert Index')\n",
    "    axes[1, 0].set_title('Expert Similarity (Cosine)', fontweight='bold')\n",
    "    plt.colorbar(im, ax=axes[1, 0])\n",
    "    \n",
    "    # Weight variance per expert\n",
    "    axes[1, 1].bar(range(num_experts), weight_stds, color='#E74C3C')\n",
    "    axes[1, 1].set_xlabel('Expert Index')\n",
    "    axes[1, 1].set_ylabel('Standard Deviation')\n",
    "    axes[1, 1].set_title('Router Weight Variance by Expert', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find similar experts\n",
    "    print(\"\\nüîç Expert Similarity Analysis:\")\n",
    "    np.fill_diagonal(similarity, 0)  # Ignore self-similarity\n",
    "    \n",
    "    for i in range(min(5, num_experts)):\n",
    "        most_similar = np.argmax(similarity[i])\n",
    "        sim_score = similarity[i, most_similar]\n",
    "        print(f\"  Expert {i} most similar to Expert {most_similar} (cosine: {sim_score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Router Weights Tell Us\n",
    "\n",
    "1. **Weight Norms**: Experts with higher norms may be \"more confident\" in their routing\n",
    "2. **Heatmap**: Shows which input features each expert is sensitive to\n",
    "3. **Similarity**: Low similarity means experts are diverse (good!); high means redundancy\n",
    "4. **Variance**: Higher variance = more selective; lower = more general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Load Balancing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_load_distribution(model, tokenizer, texts: List[str], \n",
    "                             top_k: int = 2) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze how tokens are distributed across experts.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with load statistics\n",
    "    \"\"\"\n",
    "    # Hook to capture router outputs\n",
    "    router_outputs = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            logits = output[0]\n",
    "        else:\n",
    "            logits = output\n",
    "        router_outputs.append(logits.detach().cpu())\n",
    "    \n",
    "    # Attach hook to first router\n",
    "    routers = find_router_layers(model)\n",
    "    if not routers:\n",
    "        return {}\n",
    "    \n",
    "    _, first_router = routers[0]\n",
    "    hook = first_router.register_forward_hook(hook_fn)\n",
    "    \n",
    "    # Process texts\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "    \n",
    "    hook.remove()\n",
    "    \n",
    "    # Analyze distribution\n",
    "    expert_counts = defaultdict(int)\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for logits in router_outputs:\n",
    "        if len(logits.shape) == 3:  # [batch, seq, experts]\n",
    "            logits = logits.view(-1, logits.shape[-1])  # [tokens, experts]\n",
    "        \n",
    "        for token_logits in logits:\n",
    "            top_experts = torch.topk(token_logits, k=min(top_k, len(token_logits))).indices\n",
    "            for exp in top_experts.tolist():\n",
    "                expert_counts[exp] += 1\n",
    "            total_tokens += 1\n",
    "    \n",
    "    # Calculate statistics\n",
    "    num_experts = max(expert_counts.keys()) + 1 if expert_counts else 64\n",
    "    \n",
    "    # Ensure all experts represented\n",
    "    for i in range(num_experts):\n",
    "        if i not in expert_counts:\n",
    "            expert_counts[i] = 0\n",
    "    \n",
    "    counts = [expert_counts[i] for i in range(num_experts)]\n",
    "    expected_per_expert = total_tokens * top_k / num_experts\n",
    "    \n",
    "    return {\n",
    "        'expert_counts': dict(expert_counts),\n",
    "        'total_tokens': total_tokens,\n",
    "        'num_experts': num_experts,\n",
    "        'top_k': top_k,\n",
    "        'expected_per_expert': expected_per_expert,\n",
    "        'actual_mean': np.mean(counts),\n",
    "        'actual_std': np.std(counts),\n",
    "        'max_load': max(counts),\n",
    "        'min_load': min(counts),\n",
    "        'load_balance_score': 1 - (np.std(counts) / (np.mean(counts) + 1e-8)),\n",
    "    }\n",
    "\n",
    "# Analyze load distribution\n",
    "test_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2)\",\n",
    "    \"In the realm of mathematics, calculus provides tools for understanding change.\",\n",
    "    \"SELECT * FROM users WHERE created_at > '2024-01-01' ORDER BY name;\",\n",
    "    \"Once upon a time in a land far away, there lived a brave princess.\",\n",
    "] * 10  # Repeat for more data\n",
    "\n",
    "print(\"Analyzing load distribution...\")\n",
    "load_stats = analyze_load_distribution(model, tokenizer, test_texts)\n",
    "\n",
    "if load_stats:\n",
    "    print(f\"\\nüìä Load Balancing Statistics:\")\n",
    "    print(f\"   Total tokens processed: {load_stats['total_tokens']}\")\n",
    "    print(f\"   Number of experts: {load_stats['num_experts']}\")\n",
    "    print(f\"   Top-k routing: {load_stats['top_k']}\")\n",
    "    print(f\"\\n   Expected load per expert: {load_stats['expected_per_expert']:.1f}\")\n",
    "    print(f\"   Actual mean load: {load_stats['actual_mean']:.1f}\")\n",
    "    print(f\"   Actual std load: {load_stats['actual_std']:.1f}\")\n",
    "    print(f\"   Max load: {load_stats['max_load']}\")\n",
    "    print(f\"   Min load: {load_stats['min_load']}\")\n",
    "    print(f\"\\n   Load balance score: {load_stats['load_balance_score']:.3f} (1.0 = perfect)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize load distribution\n",
    "if load_stats and load_stats['expert_counts']:\n",
    "    counts = load_stats['expert_counts']\n",
    "    experts = sorted(counts.keys())\n",
    "    loads = [counts[e] for e in experts]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar chart of expert loads\n",
    "    colors = ['#E74C3C' if l > load_stats['expected_per_expert'] * 1.5 \n",
    "              else '#27AE60' if l < load_stats['expected_per_expert'] * 0.5\n",
    "              else '#3498DB' for l in loads]\n",
    "    \n",
    "    axes[0].bar(experts, loads, color=colors)\n",
    "    axes[0].axhline(y=load_stats['expected_per_expert'], color='red', \n",
    "                   linestyle='--', label='Expected', linewidth=2)\n",
    "    axes[0].set_xlabel('Expert Index')\n",
    "    axes[0].set_ylabel('Token Count')\n",
    "    axes[0].set_title('Expert Load Distribution\\n(Red = Overloaded, Green = Underutilized)', \n",
    "                     fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Histogram of loads\n",
    "    axes[1].hist(loads, bins=20, color='#3498DB', edgecolor='white')\n",
    "    axes[1].axvline(x=load_stats['expected_per_expert'], color='red', \n",
    "                   linestyle='--', label='Expected', linewidth=2)\n",
    "    axes[1].set_xlabel('Token Count per Expert')\n",
    "    axes[1].set_ylabel('Number of Experts')\n",
    "    axes[1].set_title('Distribution of Expert Loads', fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify problematic experts\n",
    "    print(\"\\n‚ö†Ô∏è Load Balance Issues:\")\n",
    "    overloaded = [(e, c) for e, c in counts.items() \n",
    "                  if c > load_stats['expected_per_expert'] * 1.5]\n",
    "    underused = [(e, c) for e, c in counts.items() \n",
    "                 if c < load_stats['expected_per_expert'] * 0.5]\n",
    "    \n",
    "    if overloaded:\n",
    "        print(f\"   Overloaded experts: {sorted(overloaded, key=lambda x: -x[1])[:5]}\")\n",
    "    if underused:\n",
    "        print(f\"   Underutilized experts: {sorted(underused, key=lambda x: x[1])[:5]}\")\n",
    "    \n",
    "    if not overloaded and not underused:\n",
    "        print(\"   ‚úÖ Load is well balanced!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Understanding Auxiliary Loss\n",
    "\n",
    "MoE models use an **auxiliary loss** to encourage load balancing during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_load_balancing_loss(router_logits: torch.Tensor, \n",
    "                                top_k: int = 2) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the auxiliary load balancing loss.\n",
    "    \n",
    "    This encourages the router to use all experts equally.\n",
    "    \n",
    "    Args:\n",
    "        router_logits: [num_tokens, num_experts]\n",
    "        top_k: Number of experts selected per token\n",
    "    \n",
    "    Returns:\n",
    "        Scalar loss value\n",
    "    \"\"\"\n",
    "    num_tokens, num_experts = router_logits.shape\n",
    "    \n",
    "    # Compute routing probabilities (softmax)\n",
    "    routing_probs = F.softmax(router_logits, dim=-1)  # [tokens, experts]\n",
    "    \n",
    "    # Fraction of tokens routed to each expert (average)\n",
    "    expert_usage = routing_probs.mean(dim=0)  # [experts]\n",
    "    \n",
    "    # Ideal: each expert gets 1/num_experts of tokens\n",
    "    ideal_usage = 1.0 / num_experts\n",
    "    \n",
    "    # Load balancing loss: variance from ideal\n",
    "    # We want to minimize how much expert usage differs from uniform\n",
    "    load_balance_loss = num_experts * (expert_usage ** 2).sum()\n",
    "    \n",
    "    return load_balance_loss\n",
    "\n",
    "# Demonstrate with synthetic data\n",
    "print(\"üìä Auxiliary Loss Demonstration:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "num_tokens = 100\n",
    "num_experts_demo = 8\n",
    "\n",
    "# Case 1: Uniform routing (ideal)\n",
    "uniform_logits = torch.zeros(num_tokens, num_experts_demo)\n",
    "uniform_loss = compute_load_balancing_loss(uniform_logits)\n",
    "print(f\"\\nUniform routing logits (ideal):\")\n",
    "print(f\"  Loss: {uniform_loss:.4f}\")\n",
    "\n",
    "# Case 2: Imbalanced routing (bad)\n",
    "imbalanced_logits = torch.zeros(num_tokens, num_experts_demo)\n",
    "imbalanced_logits[:, 0] = 5.0  # First expert heavily preferred\n",
    "imbalanced_loss = compute_load_balancing_loss(imbalanced_logits)\n",
    "print(f\"\\nImbalanced routing (expert 0 preferred):\")\n",
    "print(f\"  Loss: {imbalanced_loss:.4f}\")\n",
    "\n",
    "# Case 3: Slight imbalance\n",
    "slight_imbalance = torch.randn(num_tokens, num_experts_demo) * 0.5\n",
    "slight_loss = compute_load_balancing_loss(slight_imbalance)\n",
    "print(f\"\\nSlight imbalance (random):\")\n",
    "print(f\"  Loss: {slight_loss:.4f}\")\n",
    "\n",
    "print(f\"\\nüí° Lower loss = better load balance\")\n",
    "print(f\"   The auxiliary loss is added to the main training loss\")\n",
    "print(f\"   to prevent expert collapse during training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how auxiliary loss affects routing\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "scenarios = [\n",
    "    (\"Uniform (Low Loss)\", torch.zeros(100, 8)),\n",
    "    (\"Random (Medium Loss)\", torch.randn(100, 8) * 0.5),\n",
    "    (\"Collapsed (High Loss)\", torch.zeros(100, 8)),\n",
    "]\n",
    "scenarios[2][1][:, 0] = 5.0  # Collapse to expert 0\n",
    "\n",
    "for idx, (title, logits) in enumerate(scenarios):\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    expert_usage = probs.mean(dim=0).numpy()\n",
    "    loss = compute_load_balancing_loss(logits)\n",
    "    \n",
    "    axes[idx].bar(range(8), expert_usage, color='#3498DB')\n",
    "    axes[idx].axhline(y=1/8, color='red', linestyle='--', label='Ideal')\n",
    "    axes[idx].set_xlabel('Expert Index')\n",
    "    axes[idx].set_ylabel('Usage Fraction')\n",
    "    axes[idx].set_title(f'{title}\\nLoss: {loss:.2f}', fontweight='bold')\n",
    "    axes[idx].set_ylim(0, 1)\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Top-K Routing Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_topk_routing(logits: torch.Tensor, top_k: int = 2) -> Dict:\n",
    "    \"\"\"\n",
    "    Simulate top-k routing and analyze the results.\n",
    "    \n",
    "    Args:\n",
    "        logits: [num_tokens, num_experts]\n",
    "        top_k: Number of experts per token\n",
    "    \n",
    "    Returns:\n",
    "        Dict with routing statistics\n",
    "    \"\"\"\n",
    "    num_tokens, num_experts = logits.shape\n",
    "    \n",
    "    # Get top-k experts and their weights\n",
    "    top_values, top_indices = torch.topk(logits, k=top_k, dim=-1)\n",
    "    \n",
    "    # Normalize weights (softmax over selected experts only)\n",
    "    routing_weights = F.softmax(top_values, dim=-1)\n",
    "    \n",
    "    # Analyze\n",
    "    expert_counts = defaultdict(int)\n",
    "    expert_weight_sums = defaultdict(float)\n",
    "    \n",
    "    for token_idx in range(num_tokens):\n",
    "        for k_idx in range(top_k):\n",
    "            expert = top_indices[token_idx, k_idx].item()\n",
    "            weight = routing_weights[token_idx, k_idx].item()\n",
    "            expert_counts[expert] += 1\n",
    "            expert_weight_sums[expert] += weight\n",
    "    \n",
    "    # First expert weight statistics\n",
    "    first_expert_weights = routing_weights[:, 0]\n",
    "    \n",
    "    return {\n",
    "        'expert_counts': dict(expert_counts),\n",
    "        'expert_weight_sums': dict(expert_weight_sums),\n",
    "        'mean_first_expert_weight': first_expert_weights.mean().item(),\n",
    "        'std_first_expert_weight': first_expert_weights.std().item(),\n",
    "        'routing_weights': routing_weights,\n",
    "        'top_indices': top_indices,\n",
    "    }\n",
    "\n",
    "# Simulate routing with different scenarios\n",
    "print(\"üìä Top-K Routing Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate realistic-ish router logits\n",
    "num_tokens = 500\n",
    "num_experts_sim = 64\n",
    "\n",
    "# Simulate: some experts naturally preferred\n",
    "base_logits = torch.randn(num_tokens, num_experts_sim)\n",
    "# Add bias to a few \"popular\" experts\n",
    "base_logits[:, :5] += 1.0\n",
    "\n",
    "for top_k in [1, 2, 4]:\n",
    "    results = simulate_topk_routing(base_logits, top_k=top_k)\n",
    "    \n",
    "    print(f\"\\nTop-{top_k} Routing:\")\n",
    "    print(f\"  Mean first expert weight: {results['mean_first_expert_weight']:.3f}\")\n",
    "    print(f\"  Std first expert weight: {results['std_first_expert_weight']:.3f}\")\n",
    "    \n",
    "    # How many experts actually used?\n",
    "    used_experts = len([c for c in results['expert_counts'].values() if c > 0])\n",
    "    print(f\"  Experts actually used: {used_experts}/{num_experts_sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top-k routing behavior\n",
    "\n",
    "results = simulate_topk_routing(base_logits, top_k=2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Expert selection frequency\n",
    "counts = results['expert_counts']\n",
    "experts = sorted(counts.keys())\n",
    "freq = [counts.get(e, 0) for e in experts]\n",
    "\n",
    "axes[0].bar(experts, freq, color='#3498DB')\n",
    "axes[0].axhline(y=num_tokens * 2 / num_experts_sim, color='red', \n",
    "               linestyle='--', label='Expected')\n",
    "axes[0].set_xlabel('Expert Index')\n",
    "axes[0].set_ylabel('Selection Count')\n",
    "axes[0].set_title('Expert Selection Frequency', fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# First vs second expert weight distribution\n",
    "weights = results['routing_weights']\n",
    "axes[1].hist(weights[:, 0].numpy(), bins=30, alpha=0.7, label='1st Expert', color='#27AE60')\n",
    "axes[1].hist(weights[:, 1].numpy(), bins=30, alpha=0.7, label='2nd Expert', color='#E74C3C')\n",
    "axes[1].set_xlabel('Routing Weight')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Expert Weights', fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "# Weight ratio (1st / 2nd)\n",
    "weight_ratio = weights[:, 0] / (weights[:, 1] + 1e-8)\n",
    "axes[2].hist(weight_ratio.numpy(), bins=30, color='#9B59B6')\n",
    "axes[2].axvline(x=1.0, color='red', linestyle='--', label='Equal weights')\n",
    "axes[2].set_xlabel('Weight Ratio (1st / 2nd)')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].set_title('1st vs 2nd Expert Weight Ratio', fontweight='bold')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Observations:\")\n",
    "print(f\"   - Mean weight ratio: {weight_ratio.mean():.2f}\")\n",
    "print(f\"   - The 1st expert typically gets {weights[:, 0].mean():.1%} of the weight\")\n",
    "print(f\"   - This means the 2nd expert provides {weights[:, 1].mean():.1%} refinement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Ignoring Load Balance During Fine-Tuning\n",
    "```python\n",
    "# ‚ùå Forgetting auxiliary loss\n",
    "loss = cross_entropy_loss\n",
    "\n",
    "# ‚úÖ Include load balancing\n",
    "loss = cross_entropy_loss + alpha * load_balance_loss\n",
    "# alpha typically 0.01-0.1\n",
    "```\n",
    "\n",
    "### Mistake 2: Wrong Top-K for Your Use Case\n",
    "```python\n",
    "# ‚ùå Always using top-2\n",
    "# ‚úÖ Consider:\n",
    "#    - top-1: Fastest, but less expert diversity\n",
    "#    - top-2: Good balance (most common)\n",
    "#    - top-4+: More capacity, slower inference\n",
    "```\n",
    "\n",
    "### Mistake 3: Not Monitoring Expert Usage\n",
    "```python\n",
    "# ‚ùå Training without monitoring\n",
    "# ‚úÖ Log expert usage during training\n",
    "wandb.log({\"expert_usage\": expert_counts})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How the router selects experts for each token\n",
    "- ‚úÖ Extracting and analyzing router weights\n",
    "- ‚úÖ Load balancing and its importance\n",
    "- ‚úÖ The auxiliary loss mechanism\n",
    "- ‚úÖ Top-k routing behavior and tradeoffs\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'model' in dir():\n",
    "    del model\n",
    "if 'tokenizer' in dir():\n",
    "    del tokenizer\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
