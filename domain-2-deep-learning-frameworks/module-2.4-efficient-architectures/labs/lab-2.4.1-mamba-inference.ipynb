{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.4.1: Mamba Inference\n",
    "\n",
    "**Module:** 2.4 - Efficient Architectures  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- [ ] Understand why Mamba's O(n) complexity matters for long sequences\n",
    "- [ ] Load and run Mamba models using HuggingFace\n",
    "- [ ] Benchmark Mamba against transformers on speed and memory\n",
    "- [ ] See Mamba's advantage scale with context length\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Module 2.3 (NLP & Transformers)\n",
    "- Knowledge of: Basic transformer architecture, attention mechanism\n",
    "- Hardware: DGX Spark with 128GB unified memory (or GPU with 16GB+)\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The Long Context Problem**\n",
    "\n",
    "Imagine you're building an AI assistant that needs to:\n",
    "- Analyze entire codebases (100K+ tokens)\n",
    "- Read and summarize legal documents (50+ pages)\n",
    "- Process hour-long meeting transcripts\n",
    "\n",
    "Traditional transformers struggle here because their attention mechanism is O(n¬≤)‚Äîdoubling the context length quadruples the computation! A 32K context requires **1 billion** attention computations per layer.\n",
    "\n",
    "**Enter Mamba**: A new architecture with O(n) complexity. Process twice as much text with only twice the compute. On DGX Spark's 128GB, this means processing 100K+ token contexts that would crash consumer GPUs.\n",
    "\n",
    "**Companies using long-context models:**\n",
    "- Google (Gemini 1M context)\n",
    "- Anthropic (Claude 200K context)\n",
    "- AI21 Labs (Jamba hybrid architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: Understanding Mamba\n",
    "\n",
    "> **Imagine you're reading a very long book...**\n",
    ">\n",
    "> **Transformer approach**: At each word, you flip back through ALL previous pages to understand context. Page 1, page 2, page 3... For a 500-page book, you'd flip through 500 pages at EVERY word. That's exhausting!\n",
    ">\n",
    "> **Mamba approach**: You read like a human‚Äîone word at a time, keeping a mental \"summary\" of what came before. You don't flip back; you just update your summary as you go. Reading page 500 is just as easy as reading page 5!\n",
    ">\n",
    "> **In AI terms**: \n",
    "> - Transformers use \"attention\" which looks at all previous tokens (O(n¬≤) memory for KV cache)\n",
    "> - Mamba uses a \"state space\" that compresses history into a fixed-size state (O(1) memory!)\n",
    "> - This means Mamba can read a 100,000-word document using the same memory as a 1,000-word document\n",
    "\n",
    "### The Key Insight: Selective State Spaces\n",
    "\n",
    "Mamba doesn't just blindly compress‚Äîit **selects** what's important:\n",
    "\n",
    "```\n",
    "Traditional RNN: state = fixed_function(state, input)\n",
    "Mamba:          state = learned_function(state, input, context)\n",
    "                        ^^^^^^^^^^^^^^^^\n",
    "                        The \"selective\" part!\n",
    "```\n",
    "\n",
    "Think of it like taking notes: Mamba learns WHAT to write down based on what it's reading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment Setup\n",
    "\n",
    "Let's verify our environment and understand our hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check environment\nimport sys\nprint(f\"Python: {sys.version}\")\n\n# Check transformers version (need >= 4.46.0 for Mamba)\nimport transformers\nprint(f\"Transformers: {transformers.__version__}\")\n\n# Verify minimum version\nmin_version = (4, 46, 0)\ncurrent = tuple(map(int, transformers.__version__.split('.')[:3]))\nif current < min_version:\n    print(f\"‚ö†Ô∏è  Mamba requires transformers >= 4.46.0\")\n    print(f\"   Run: pip install --upgrade transformers\")\nelse:\n    print(f\"‚úÖ Transformers version OK for Mamba\")\n\nimport torch\nprint(f\"\\nPyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name()}\")\n    total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"GPU Memory: {total_mem:.1f} GB\")\n    \n    # DGX Spark detection\n    if total_mem > 100:\n        print(f\"\\nüöÄ DGX Spark detected! 128GB unified memory available.\")\n        print(f\"   You can run very long context experiments!\")\n    else:\n        print(f\"\\nüí° Tip: Reduce context lengths if you run out of memory.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Upgrade transformers if needed (uncomment to run)\n# !pip install --upgrade transformers>=4.46.0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Set default device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Helper function to clear GPU memory\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU cache and run garbage collection.\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print(f\"GPU memory cleared. Allocated: {torch.cuda.memory_allocated()/1e9:.2f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Loading Mamba Models\n",
    "\n",
    "### Available Mamba Models\n",
    "\n",
    "| Model | Parameters | Memory (BF16) | DGX Spark Fit? |\n",
    "|-------|------------|---------------|----------------|\n",
    "| mamba-130m | 130M | ~260 MB | ‚úÖ Easily |\n",
    "| mamba-370m | 370M | ~740 MB | ‚úÖ Easily |\n",
    "| mamba-790m | 790M | ~1.6 GB | ‚úÖ Easily |\n",
    "| mamba-1.4b | 1.4B | ~2.8 GB | ‚úÖ Easily |\n",
    "| mamba-2.8b | 2.8B | ~5.6 GB | ‚úÖ Easily |\n",
    "\n",
    "Let's load Mamba-2.8B‚Äîthe largest publicly available Mamba model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Mamba model\n",
    "# Using the HuggingFace version (state-spaces/mamba-2.8b-hf)\n",
    "\n",
    "MODEL_NAME = \"state-spaces/mamba-2.8b-hf\"\n",
    "# For faster testing, try: \"state-spaces/mamba-130m-hf\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "print(\"This may take a minute on first run (downloading weights)...\\n\")\n",
    "\n",
    "# Clear memory first\n",
    "clear_gpu_memory()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f\"‚úÖ Tokenizer loaded. Vocab size: {tokenizer.vocab_size:,}\")\n",
    "\n",
    "# Load model with bfloat16 (native to DGX Spark's Blackwell architecture)\n",
    "mamba_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,  # Blackwell-optimized\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Report stats\n",
    "num_params = sum(p.numel() for p in mamba_model.parameters())\n",
    "memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "print(f\"\\n‚úÖ Model loaded!\")\n",
    "print(f\"   Parameters: {num_params/1e9:.2f}B\")\n",
    "print(f\"   GPU Memory: {memory_used:.2f} GB\")\n",
    "print(f\"   DGX Spark headroom: {128 - memory_used:.1f} GB remaining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "We loaded a 2.8 billion parameter Mamba model in ~5.6 GB (BF16). Notice:\n",
    "- **No attention layers** = No quadratic memory scaling\n",
    "- **Fixed state size** = Memory doesn't grow with context\n",
    "- **Massive headroom** = 128GB - 5.6GB = 122+ GB free for inference!\n",
    "\n",
    "This headroom is crucial for long-context inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test: Generate some text\n",
    "prompt = \"The key insight of Mamba over transformers is\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Input tokens: {inputs['input_ids'].shape[1]}\")\n",
    "print(\"\\nGenerating...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = mamba_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"\\nü§ñ Generated text:\\n{generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Understanding Mamba's Memory Advantage\n",
    "\n",
    "Let's visualize why Mamba's constant memory matters.\n",
    "\n",
    "### Theoretical Memory Comparison\n",
    "\n",
    "```\n",
    "Transformer Memory = Model + KV Cache\n",
    "                   = Model + 2 √ó layers √ó heads √ó context √ó head_dim √ó precision\n",
    "                   \n",
    "Mamba Memory      = Model + State\n",
    "                   = Model + layers √ó state_dim √ó precision\n",
    "                   \n",
    "Key difference: Transformer scales with context, Mamba doesn't!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize memory scaling (theoretical)\n",
    "\n",
    "def calculate_transformer_memory(context_length: int, \n",
    "                                  model_memory_gb: float = 6.0,\n",
    "                                  num_layers: int = 32,\n",
    "                                  num_heads: int = 32,\n",
    "                                  head_dim: int = 128,\n",
    "                                  precision_bytes: int = 2) -> float:\n",
    "    \"\"\"\n",
    "    Calculate approximate transformer memory including KV cache.\n",
    "    \n",
    "    KV cache = 2 (K and V) √ó layers √ó heads √ó context √ó head_dim √ó precision\n",
    "    \"\"\"\n",
    "    kv_cache_bytes = 2 * num_layers * num_heads * context_length * head_dim * precision_bytes\n",
    "    kv_cache_gb = kv_cache_bytes / 1e9\n",
    "    return model_memory_gb + kv_cache_gb\n",
    "\n",
    "def calculate_mamba_memory(context_length: int,\n",
    "                           model_memory_gb: float = 5.6,\n",
    "                           num_layers: int = 64,\n",
    "                           state_dim: int = 16,\n",
    "                           d_model: int = 2560,\n",
    "                           precision_bytes: int = 2) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Mamba memory (constant regardless of context!).\n",
    "    \n",
    "    State memory = layers √ó state_dim √ó d_model √ó precision\n",
    "    (This is independent of context length)\n",
    "    \"\"\"\n",
    "    state_bytes = num_layers * state_dim * d_model * precision_bytes\n",
    "    state_gb = state_bytes / 1e9\n",
    "    return model_memory_gb + state_gb  # State is tiny!\n",
    "\n",
    "# Generate comparison data\n",
    "context_lengths = [1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072]\n",
    "\n",
    "transformer_memory = [calculate_transformer_memory(ctx) for ctx in context_lengths]\n",
    "mamba_memory = [calculate_mamba_memory(ctx) for ctx in context_lengths]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(context_lengths, transformer_memory, 'o-', label='Transformer (3B)', \n",
    "        linewidth=2, markersize=8, color='#E74C3C')\n",
    "ax.plot(context_lengths, mamba_memory, 's-', label='Mamba (2.8B)', \n",
    "        linewidth=2, markersize=8, color='#27AE60')\n",
    "\n",
    "# DGX Spark limit\n",
    "ax.axhline(y=128, color='#3498DB', linestyle='--', linewidth=2, label='DGX Spark (128GB)')\n",
    "ax.axhline(y=24, color='#9B59B6', linestyle=':', linewidth=2, label='Consumer GPU (24GB)')\n",
    "\n",
    "ax.set_xscale('log', base=2)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Context Length (tokens)', fontsize=12)\n",
    "ax.set_ylabel('GPU Memory (GB)', fontsize=12)\n",
    "ax.set_title('Memory Scaling: Transformer vs Mamba', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper left', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotations\n",
    "ax.annotate('Transformer OOM\\non consumer GPU!', \n",
    "            xy=(16384, 30), fontsize=10, color='#E74C3C',\n",
    "            arrowprops=dict(arrowstyle='->', color='#E74C3C'),\n",
    "            xytext=(8192, 60))\n",
    "\n",
    "ax.annotate('Mamba: constant memory!', \n",
    "            xy=(65536, 5.7), fontsize=10, color='#27AE60',\n",
    "            arrowprops=dict(arrowstyle='->', color='#27AE60'),\n",
    "            xytext=(32768, 12))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Memory at 64K context:\")\n",
    "print(f\"   Transformer: {calculate_transformer_memory(65536):.1f} GB\")\n",
    "print(f\"   Mamba:       {calculate_mamba_memory(65536):.1f} GB\")\n",
    "print(f\"   Savings:     {calculate_transformer_memory(65536) - calculate_mamba_memory(65536):.1f} GB!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What This Means\n",
    "\n",
    "The graph shows Mamba's killer advantage:\n",
    "- **At 8K tokens**: Transformer uses ~14GB, Mamba uses ~5.6GB\n",
    "- **At 32K tokens**: Transformer uses ~38GB, Mamba STILL uses ~5.6GB\n",
    "- **At 128K tokens**: Transformer needs 140GB+, Mamba STILL uses ~5.6GB!\n",
    "\n",
    "On DGX Spark:\n",
    "- Transformer can process ~60K tokens before hitting 128GB limit\n",
    "- Mamba can process 200K+ tokens easily (limited by compute, not memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Benchmarking Mamba Inference\n",
    "\n",
    "Let's measure actual performance across different context lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_generation(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context_length: int,\n",
    "    generation_length: int = 50,\n",
    "    warmup_runs: int = 2,\n",
    "    benchmark_runs: int = 3,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Benchmark model generation at a specific context length.\n",
    "    \n",
    "    Returns dict with timing and memory stats.\n",
    "    \"\"\"\n",
    "    # Create input of specified length\n",
    "    # Use a repeating sentence pattern for realistic tokens\n",
    "    base_text = \"The quick brown fox jumps over the lazy dog. \" * 50\n",
    "    tokens = tokenizer.encode(base_text, add_special_tokens=False)\n",
    "    while len(tokens) < context_length:\n",
    "        tokens = tokens + tokens\n",
    "    tokens = tokens[:context_length]\n",
    "    \n",
    "    input_ids = torch.tensor([tokens], device=device)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(warmup_runs):\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=5,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "    \n",
    "    # Reset memory tracking\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    # Benchmark runs\n",
    "    times = []\n",
    "    for _ in range(benchmark_runs):\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=generation_length,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        times.append(time.perf_counter() - start)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "    \n",
    "    return {\n",
    "        \"context_length\": context_length,\n",
    "        \"generation_length\": generation_length,\n",
    "        \"avg_time_seconds\": avg_time,\n",
    "        \"tokens_per_second\": generation_length / avg_time,\n",
    "        \"peak_memory_gb\": peak_memory,\n",
    "        \"times\": times,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmarks across context lengths\n",
    "# Adjust these based on your available memory\n",
    "\n",
    "# For DGX Spark (128GB): can go up to 100K+\n",
    "# For 24GB GPU: limit to 16K\n",
    "# For 8GB GPU: limit to 4K\n",
    "\n",
    "total_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "\n",
    "if total_memory_gb > 100:  # DGX Spark\n",
    "    test_contexts = [1024, 4096, 8192, 16384, 32768, 65536]\n",
    "elif total_memory_gb > 20:  # RTX 3090/4090\n",
    "    test_contexts = [1024, 4096, 8192, 16384]\n",
    "else:  # Smaller GPU\n",
    "    test_contexts = [1024, 2048, 4096]\n",
    "\n",
    "print(f\"Benchmarking Mamba at context lengths: {test_contexts}\")\n",
    "print(f\"GPU Memory: {total_memory_gb:.1f} GB\\n\")\n",
    "\n",
    "mamba_results = []\n",
    "\n",
    "for ctx_len in test_contexts:\n",
    "    print(f\"Testing context length: {ctx_len:,} tokens...\")\n",
    "    try:\n",
    "        result = benchmark_generation(mamba_model, tokenizer, ctx_len)\n",
    "        mamba_results.append(result)\n",
    "        print(f\"  ‚úÖ {result['tokens_per_second']:.1f} tokens/sec, {result['peak_memory_gb']:.2f} GB peak\")\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            print(f\"  ‚ùå Out of memory at {ctx_len:,} tokens\")\n",
    "            break\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "print(\"\\n‚úÖ Benchmarks complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Mamba benchmark results\n",
    "\n",
    "if mamba_results:\n",
    "    contexts = [r['context_length'] for r in mamba_results]\n",
    "    speeds = [r['tokens_per_second'] for r in mamba_results]\n",
    "    memories = [r['peak_memory_gb'] for r in mamba_results]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Speed plot\n",
    "    ax1.bar(range(len(contexts)), speeds, color='#27AE60', alpha=0.8)\n",
    "    ax1.set_xticks(range(len(contexts)))\n",
    "    ax1.set_xticklabels([f'{c//1024}K' for c in contexts], fontsize=10)\n",
    "    ax1.set_xlabel('Context Length', fontsize=12)\n",
    "    ax1.set_ylabel('Tokens per Second', fontsize=12)\n",
    "    ax1.set_title('Mamba Generation Speed', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(speeds):\n",
    "        ax1.text(i, v + 1, f'{v:.1f}', ha='center', fontsize=9)\n",
    "    \n",
    "    # Memory plot\n",
    "    ax2.bar(range(len(contexts)), memories, color='#3498DB', alpha=0.8)\n",
    "    ax2.set_xticks(range(len(contexts)))\n",
    "    ax2.set_xticklabels([f'{c//1024}K' for c in contexts], fontsize=10)\n",
    "    ax2.set_xlabel('Context Length', fontsize=12)\n",
    "    ax2.set_ylabel('Peak Memory (GB)', fontsize=12)\n",
    "    ax2.set_title('Mamba Memory Usage', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, axis='y', alpha=0.3)\n",
    "    \n",
    "    # Highlight the key insight: memory barely changes!\n",
    "    if len(memories) > 1:\n",
    "        memory_increase = memories[-1] - memories[0]\n",
    "        ax2.annotate(f'Only +{memory_increase:.2f}GB\\nfrom {contexts[0]//1024}K to {contexts[-1]//1024}K!',\n",
    "                    xy=(len(contexts)-1, memories[-1]),\n",
    "                    xytext=(len(contexts)-2, memories[-1] + 2),\n",
    "                    fontsize=10, color='#E74C3C',\n",
    "                    arrowprops=dict(arrowstyle='->', color='#E74C3C'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nüìä Mamba Benchmark Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Context':<12} {'Speed (tok/s)':<15} {'Memory (GB)':<12}\")\n",
    "    print(\"-\" * 50)\n",
    "    for r in mamba_results:\n",
    "        print(f\"{r['context_length']:,} tokens  {r['tokens_per_second']:<15.1f} {r['peak_memory_gb']:<12.2f}\")\n",
    "else:\n",
    "    print(\"No benchmark results to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Key Observations\n",
    "\n",
    "Notice in the benchmarks:\n",
    "\n",
    "1. **Memory stays nearly constant** - Whether processing 1K or 64K tokens, Mamba uses almost the same memory. This is the O(n) vs O(n¬≤) advantage in action!\n",
    "\n",
    "2. **Speed is consistent** - Generation speed doesn't drop dramatically with longer contexts (unlike transformers where longer KV cache = slower generation)\n",
    "\n",
    "3. **DGX Spark advantage** - With 128GB, you can process contexts that would crash consumer GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Loading a Transformer for Comparison\n",
    "\n",
    "To truly appreciate Mamba, let's compare with a similar-sized transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a comparable transformer model\n",
    "# Using a smaller model for fair comparison on limited memory\n",
    "\n",
    "TRANSFORMER_MODEL = \"microsoft/phi-2\"  # 2.7B parameters\n",
    "# Alternatives: \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"facebook/opt-2.7b\"\n",
    "\n",
    "print(f\"Loading transformer: {TRANSFORMER_MODEL}...\")\n",
    "print(\"(This is for comparison purposes)\\n\")\n",
    "\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL, trust_remote_code=True)\n",
    "if transformer_tokenizer.pad_token is None:\n",
    "    transformer_tokenizer.pad_token = transformer_tokenizer.eos_token\n",
    "\n",
    "transformer_model = AutoModelForCausalLM.from_pretrained(\n",
    "    TRANSFORMER_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "t_params = sum(p.numel() for p in transformer_model.parameters())\n",
    "t_memory = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "print(f\"‚úÖ Transformer loaded!\")\n",
    "print(f\"   Parameters: {t_params/1e9:.2f}B\")\n",
    "print(f\"   GPU Memory (total): {t_memory:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark transformer at same context lengths\n",
    "\n",
    "# Transformers may OOM at high context lengths\n",
    "# Limit based on available memory\n",
    "if total_memory_gb > 100:  # DGX Spark\n",
    "    transformer_contexts = [1024, 4096, 8192, 16384, 32768]\n",
    "elif total_memory_gb > 20:\n",
    "    transformer_contexts = [1024, 4096, 8192]\n",
    "else:\n",
    "    transformer_contexts = [1024, 2048]\n",
    "\n",
    "print(f\"Benchmarking Transformer at context lengths: {transformer_contexts}\\n\")\n",
    "\n",
    "transformer_results = []\n",
    "\n",
    "for ctx_len in transformer_contexts:\n",
    "    print(f\"Testing context length: {ctx_len:,} tokens...\")\n",
    "    try:\n",
    "        # Clear before each test\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        result = benchmark_generation(\n",
    "            transformer_model, \n",
    "            transformer_tokenizer, \n",
    "            ctx_len,\n",
    "            warmup_runs=1,  # Fewer warmups for transformer (slower)\n",
    "            benchmark_runs=2,\n",
    "        )\n",
    "        transformer_results.append(result)\n",
    "        print(f\"  ‚úÖ {result['tokens_per_second']:.1f} tokens/sec, {result['peak_memory_gb']:.2f} GB peak\")\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            print(f\"  ‚ùå Out of memory at {ctx_len:,} tokens\")\n",
    "            print(f\"     (This is expected - transformer KV cache grows with context!)\")\n",
    "            break\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "print(\"\\n‚úÖ Transformer benchmarks complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison visualization\n",
    "\n",
    "if mamba_results and transformer_results:\n",
    "    # Find common context lengths\n",
    "    mamba_contexts = {r['context_length']: r for r in mamba_results}\n",
    "    transformer_contexts = {r['context_length']: r for r in transformer_results}\n",
    "    common_contexts = sorted(set(mamba_contexts.keys()) & set(transformer_contexts.keys()))\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Speed comparison\n",
    "    x = np.arange(len(common_contexts))\n",
    "    width = 0.35\n",
    "    \n",
    "    mamba_speeds = [mamba_contexts[c]['tokens_per_second'] for c in common_contexts]\n",
    "    transformer_speeds = [transformer_contexts[c]['tokens_per_second'] for c in common_contexts]\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, mamba_speeds, width, label='Mamba', color='#27AE60')\n",
    "    bars2 = ax1.bar(x + width/2, transformer_speeds, width, label='Transformer', color='#E74C3C')\n",
    "    \n",
    "    ax1.set_xlabel('Context Length', fontsize=12)\n",
    "    ax1.set_ylabel('Tokens per Second', fontsize=12)\n",
    "    ax1.set_title('Generation Speed Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([f'{c//1024}K' for c in common_contexts])\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, axis='y', alpha=0.3)\n",
    "    \n",
    "    # Memory comparison\n",
    "    mamba_memories = [mamba_contexts[c]['peak_memory_gb'] for c in common_contexts]\n",
    "    transformer_memories = [transformer_contexts[c]['peak_memory_gb'] for c in common_contexts]\n",
    "    \n",
    "    ax2.plot(common_contexts, mamba_memories, 'o-', label='Mamba', \n",
    "             linewidth=2, markersize=10, color='#27AE60')\n",
    "    ax2.plot(common_contexts, transformer_memories, 's-', label='Transformer', \n",
    "             linewidth=2, markersize=10, color='#E74C3C')\n",
    "    \n",
    "    ax2.set_xscale('log', base=2)\n",
    "    ax2.set_xlabel('Context Length (tokens)', fontsize=12)\n",
    "    ax2.set_ylabel('Peak Memory (GB)', fontsize=12)\n",
    "    ax2.set_title('Memory Usage Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print comparison table\n",
    "    print(\"\\nüìä Architecture Comparison:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Context':<12} {'Mamba Speed':<15} {'Trans. Speed':<15} {'Mamba Mem':<12} {'Trans. Mem':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    for c in common_contexts:\n",
    "        print(f\"{c//1024}K tokens    \"\n",
    "              f\"{mamba_contexts[c]['tokens_per_second']:<15.1f} \"\n",
    "              f\"{transformer_contexts[c]['tokens_per_second']:<15.1f} \"\n",
    "              f\"{mamba_contexts[c]['peak_memory_gb']:<12.2f} \"\n",
    "              f\"{transformer_contexts[c]['peak_memory_gb']:<12.2f}\")\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"Need both Mamba and Transformer results for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ‚ö†Ô∏è Common Mistakes\n\n### Mistake 1: Wrong Transformers Version\n```python\n# ‚ùå Error: Unknown model type 'mamba'\nmodel = AutoModelForCausalLM.from_pretrained(\"state-spaces/mamba-2.8b-hf\")\n\n# ‚úÖ Fix: Upgrade transformers\n# pip install --upgrade transformers>=4.46.0\n```\n**Why:** Mamba support requires transformers 4.46.0 or higher\n\n### Mistake 2: Using Float32 Instead of BFloat16\n```python\n# ‚ùå Wastes memory (2x more than needed)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"state-spaces/mamba-2.8b-hf\",\n    torch_dtype=torch.float32,  # 4 bytes per param\n)\n\n# ‚úÖ Use bfloat16 (native Blackwell support)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"state-spaces/mamba-2.8b-hf\",\n    torch_dtype=torch.bfloat16,  # 2 bytes per param\n)\n```\n**Why:** DGX Spark's Blackwell architecture has native BF16 support\n\n### Mistake 3: Expecting Mamba to Match Transformer Quality Everywhere\n```python\n# ‚ö†Ô∏è Mamba may not match transformers on all tasks\n# Mamba excels at:\n#   - Long document processing\n#   - Audio/time-series\n#   - Streaming inference\n# Transformers still win on:\n#   - Complex reasoning requiring precise attention\n#   - Tasks with well-established transformer benchmarks\n```\n**Why:** Different architectures have different strengths"
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## üìö Working with Long Documents: The Datasets Library\n\nBefore we practice with long documents, let's learn how to load them efficiently using the HuggingFace `datasets` library.\n\n### The `datasets` Library\n\nThe `datasets` library provides easy access to thousands of text datasets, perfect for testing long-context models:\n\n```python\nfrom datasets import load_dataset\n\n# Load a dataset with optional split specification\ndataset = load_dataset(\"dataset_name\", split=\"train[:10]\")  # First 10 examples\n\n# Access data like a list\ntext = dataset[0][\"text\"]  # First document's text field\n```\n\n**Key Functions:**\n- `load_dataset(name, split)` - Load a dataset by name\n- `split=\"train[:N]\"` - Load first N examples from train split\n- `split=\"test[:1]\"` - Load first example from test split\n\n**Useful Long-Document Datasets:**\n- `pg19` - Project Gutenberg books (very long texts)\n- `scientific_papers` - arXiv/PubMed papers\n- `bookcorpus` - Book excerpts",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Install datasets if needed (uncomment to run)\n# !pip install datasets\n\n# Import and demonstrate the datasets library\nfrom datasets import load_dataset\n\n# Load a sample from Project Gutenberg (PG19) - contains full books\nprint(\"Loading a sample from PG19 (Project Gutenberg books)...\")\npg19_sample = load_dataset(\"pg19\", split=\"test[:1]\", trust_remote_code=True)\n\n# Access the text\nsample_text = pg19_sample[0][\"text\"]\n\nprint(f\"‚úÖ Loaded 1 book from PG19\")\nprint(f\"   Total characters: {len(sample_text):,}\")\nprint(f\"   First 200 chars: {sample_text[:200]}...\")\n\n# Tokenize to see token count\nsample_tokens = tokenizer.encode(sample_text[:50000])  # First 50K chars\nprint(f\"\\n   Tokens in first 50K chars: {len(sample_tokens):,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ Why transformers have O(n¬≤) complexity (attention looks at all pairs)\n",
    "- ‚úÖ How Mamba achieves O(n) with selective state spaces\n",
    "- ‚úÖ Loading Mamba models using HuggingFace transformers\n",
    "- ‚úÖ Benchmarking and comparing architectures\n",
    "- ‚úÖ Mamba's constant memory advantage for long contexts\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Long Document Processing\n",
    "Load a long text document (e.g., a book chapter from Project Gutenberg) and:\n",
    "1. Tokenize it and measure the token count\n",
    "2. Run Mamba inference on the full document\n",
    "3. Compare memory usage with what a transformer would theoretically need\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "```python\n",
    "# Load a long document\n",
    "from datasets import load_dataset\n",
    "pg19 = load_dataset(\"pg19\", split=\"test[:1]\")\n",
    "long_text = pg19[0][\"text\"][:50000]  # First 50K characters\n",
    "\n",
    "# Tokenize and check length\n",
    "tokens = tokenizer.encode(long_text)\n",
    "print(f\"Token count: {len(tokens)}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 1 here\n",
    "# Try loading a long document and processing it with Mamba\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Context Scaling Analysis\n",
    "Create a more detailed benchmark that measures:\n",
    "1. Time-to-first-token (TTFT) at different context lengths\n",
    "2. How throughput (tokens/sec) changes with context\n",
    "3. Memory usage at each context length\n",
    "\n",
    "Plot all three metrics.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "For TTFT measurement, you can use streaming generation or measure time to generate just 1 token:\n",
    "```python\n",
    "# Measure time to first token\n",
    "start = time.perf_counter()\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(inputs, max_new_tokens=1, do_sample=False)\n",
    "ttft = time.perf_counter() - start\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 2 here\n",
    "# Create a detailed scaling analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "### Advanced Challenge: Build a Long-Context Summarizer\n",
    "\n",
    "Build a function that:\n",
    "1. Takes a very long document (100K+ tokens if on DGX Spark)\n",
    "2. Uses Mamba to generate a summary\n",
    "3. Compares performance with a chunked-transformer approach\n",
    "\n",
    "For the chunked approach:\n",
    "- Split document into 4K chunks\n",
    "- Summarize each chunk\n",
    "- Combine summaries and summarize again\n",
    "\n",
    "Compare:\n",
    "- Total processing time\n",
    "- Memory usage\n",
    "- Quality of final summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your advanced challenge code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [Mamba Paper](https://arxiv.org/abs/2312.00752) - \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\"\n",
    "- [Mamba-2 Paper](https://arxiv.org/abs/2405.21060) - 8√ó faster training with structured state space duality\n",
    "- [HuggingFace Mamba Guide](https://huggingface.co/docs/transformers/model_doc/mamba)\n",
    "- [State Spaces Explained](https://srush.github.io/annotated-s4/) - Annotated S4 (Mamba's predecessor)\n",
    "- [The Mamba Repository](https://github.com/state-spaces/mamba) - Official implementation\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "print(\"Cleaning up...\")\n",
    "\n",
    "# Delete models\n",
    "if 'mamba_model' in dir():\n",
    "    del mamba_model\n",
    "if 'transformer_model' in dir():\n",
    "    del transformer_model\n",
    "if 'tokenizer' in dir():\n",
    "    del tokenizer\n",
    "if 'transformer_tokenizer' in dir():\n",
    "    del transformer_tokenizer\n",
    "\n",
    "# Clear GPU cache\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"GPU memory after cleanup: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "print(\"\\n‚úÖ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèÅ Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "| Concept | Key Takeaway |\n",
    "|---------|-------------|\n",
    "| Mamba Architecture | Selective State Space = O(n) complexity |\n",
    "| Memory Advantage | Constant memory regardless of context length |\n",
    "| Speed | Consistent generation speed at all context lengths |\n",
    "| DGX Spark | 128GB enables 100K+ token contexts |\n",
    "| Use Cases | Long documents, streaming, audio/time-series |\n",
    "\n",
    "**Next:** In Lab 2.4.2, we'll dive deeper into Mamba's architecture and visualize the selective scan mechanism!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}