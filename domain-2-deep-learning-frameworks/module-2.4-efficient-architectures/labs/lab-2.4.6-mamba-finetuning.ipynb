{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.4.6: Mamba Fine-tuning\n",
    "\n",
    "**Module:** 2.4 - Efficient Architectures  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê (Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- [ ] Understand that LoRA works with Mamba (not just transformers!)\n",
    "- [ ] Fine-tune a Mamba model on a custom instruction dataset\n",
    "- [ ] Compare fine-tuning memory requirements vs transformers\n",
    "- [ ] Evaluate fine-tuned model performance\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Labs 2.4.1-2.4.5\n",
    "- Knowledge of: LoRA, fine-tuning basics, PEFT library\n",
    "- Hardware: DGX Spark recommended (fine-tuning requires more memory)\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**Why Fine-tune Mamba?**\n",
    "\n",
    "Mamba excels at long-context tasks, but base models need customization for:\n",
    "- Domain-specific vocabulary (legal, medical, code)\n",
    "- Following your organization's style guidelines\n",
    "- Specific output formats\n",
    "- Custom knowledge injection\n",
    "\n",
    "**Good news**: LoRA works with Mamba! The linear projections in Mamba can be adapted just like transformer attention layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: LoRA on Mamba\n",
    "\n",
    "> **Remember our LoRA analogy?**\n",
    ">\n",
    "> LoRA is like adding thin wallpaper instead of repainting the entire house.\n",
    ">\n",
    "> **For transformers**: We add wallpaper to attention layers (Q, K, V, O projections)\n",
    ">\n",
    "> **For Mamba**: We add wallpaper to:\n",
    "> - Input projections (entering the SSM)\n",
    "> - Output projections (leaving the SSM)\n",
    "> - The selective parameter generators (B, C, delta)\n",
    ">\n",
    "> The principle is the same: low-rank updates to existing linear layers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install peft>=0.8.0 datasets trl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Optional\n",
    "import gc\n",
    "import time\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "try:\n",
    "    from peft import (\n",
    "        LoraConfig,\n",
    "        get_peft_model,\n",
    "        TaskType,\n",
    "        prepare_model_for_kbit_training,\n",
    "    )\n",
    "    HAS_PEFT = True\n",
    "except ImportError:\n",
    "    HAS_PEFT = False\n",
    "    print(\"‚ö†Ô∏è PEFT not installed. Run: pip install peft>=0.8.0\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Load Mamba Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Mamba model - smaller for faster fine-tuning\n",
    "MODEL_NAME = \"state-spaces/mamba-1.4b-hf\"  # Good balance of size and capability\n",
    "# Alternatives:\n",
    "# MODEL_NAME = \"state-spaces/mamba-130m-hf\"  # Fastest for testing\n",
    "# MODEL_NAME = \"state-spaces/mamba-2.8b-hf\"  # Best quality\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Report\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "memory = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "print(f\"\\n‚úÖ Model loaded!\")\n",
    "print(f\"   Total params: {total_params/1e9:.2f}B\")\n",
    "print(f\"   Trainable: {trainable_params/1e6:.1f}M (all for now)\")\n",
    "print(f\"   Memory: {memory:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Explore Model Structure for LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find linear layers suitable for LoRA\n",
    "def find_lora_targets(model) -> List[str]:\n",
    "    \"\"\"\n",
    "    Find linear layers that can be targeted with LoRA.\n",
    "    \"\"\"\n",
    "    linear_layers = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Extract the layer name without indices\n",
    "            layer_type = name.split('.')[-1]\n",
    "            if layer_type not in linear_layers:\n",
    "                linear_layers.append(layer_type)\n",
    "    \n",
    "    return linear_layers\n",
    "\n",
    "lora_targets = find_lora_targets(model)\n",
    "print(\"Available LoRA targets:\")\n",
    "for target in lora_targets:\n",
    "    print(f\"  - {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at specific layer structure\n",
    "print(\"\\nMamba layer structure (first layer):\")\n",
    "for name, module in model.named_modules():\n",
    "    if '.0.' in name and isinstance(module, nn.Linear):\n",
    "        print(f\"  {name}: {module.in_features} -> {module.out_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Apply LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if HAS_PEFT:\n    # Auto-detect available LoRA target modules\n    # Different Mamba versions may have different layer names\n    available_targets = find_lora_targets(model)\n    \n    # Preferred Mamba target modules (in order of preference)\n    preferred_targets = [\"in_proj\", \"out_proj\", \"x_proj\", \"dt_proj\", \"embed_tokens\", \"lm_head\"]\n    \n    # Find which preferred targets actually exist in the model\n    valid_targets = [t for t in preferred_targets if t in available_targets]\n    \n    if not valid_targets:\n        # Fallback: use first few available linear layers\n        print(\"‚ö†Ô∏è Standard Mamba targets not found, using available layers...\")\n        valid_targets = available_targets[:3] if len(available_targets) >= 3 else available_targets\n    \n    if not valid_targets:\n        print(\"‚ùå No linear layers found for LoRA! Check model architecture.\")\n    else:\n        print(f\"Detected valid LoRA targets: {valid_targets}\")\n        \n        # Configure LoRA for Mamba\n        lora_config = LoraConfig(\n            r=16,  # LoRA rank\n            lora_alpha=32,  # Scaling factor\n            target_modules=valid_targets,  # Auto-detected layers\n            lora_dropout=0.05,\n            bias=\"none\",\n            task_type=TaskType.CAUSAL_LM,\n        )\n        \n        print(\"\\nLoRA Configuration:\")\n        print(f\"  Rank: {lora_config.r}\")\n        print(f\"  Alpha: {lora_config.lora_alpha}\")\n        print(f\"  Targets: {lora_config.target_modules}\")\n        \n        # Apply LoRA with error handling\n        print(\"\\nApplying LoRA...\")\n        try:\n            model = get_peft_model(model, lora_config)\n            \n            # Check trainable parameters\n            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n            total_params = sum(p.numel() for p in model.parameters())\n            \n            print(f\"\\n‚úÖ LoRA applied!\")\n            print(f\"   Trainable params: {trainable_params/1e6:.2f}M\")\n            print(f\"   Total params: {total_params/1e9:.2f}B\")\n            print(f\"   Trainable %: {100 * trainable_params / total_params:.2f}%\")\n            \n            # Print trainable modules\n            model.print_trainable_parameters()\n        except ValueError as e:\n            print(f\"‚ùå LoRA application failed: {e}\")\n            print(f\"\\nAvailable targets in this model: {available_targets}\")\n            print(\"Try modifying target_modules to match available layers.\")\nelse:\n    print(\"PEFT not available. Skipping LoRA application.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple instruction dataset\n",
    "# For demonstration, we'll create a small synthetic dataset\n",
    "\n",
    "instruction_examples = [\n",
    "    {\n",
    "        \"instruction\": \"Summarize the following text in one sentence.\",\n",
    "        \"input\": \"Machine learning is a subset of artificial intelligence that enables systems to learn from data.\",\n",
    "        \"output\": \"Machine learning is an AI subset that allows systems to learn from data.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Translate to French.\",\n",
    "        \"input\": \"Hello, how are you?\",\n",
    "        \"output\": \"Bonjour, comment allez-vous?\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Write a Python function to calculate factorial.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"def factorial(n):\\n    if n <= 1:\\n        return 1\\n    return n * factorial(n-1)\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is the capital of France?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"The capital of France is Paris.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain photosynthesis simply.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Photosynthesis is how plants make food using sunlight, water, and carbon dioxide.\"\n",
    "    },\n",
    "] * 100  # Repeat for more training data\n",
    "\n",
    "def format_instruction(example):\n",
    "    \"\"\"Format an instruction example for training.\"\"\"\n",
    "    if example[\"input\"]:\n",
    "        text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\\n{example['output']}\"\n",
    "    else:\n",
    "        text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}\"\n",
    "    return text\n",
    "\n",
    "# Create dataset\n",
    "formatted_data = [format_instruction(ex) for ex in instruction_examples]\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": formatted_data})\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(tokenized_dataset)}\")\n",
    "print(f\"\\nExample formatted instruction:\")\n",
    "print(\"-\" * 50)\n",
    "print(formatted_data[0][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Eval size: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mamba-lora-output\",\n",
    "    num_train_epochs=1,  # Quick demo - increase for better results\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,  # Use bfloat16 for DGX Spark\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",  # Disable wandb for demo\n",
    "    gradient_checkpointing=True,  # Save memory\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM, not masked\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer configured!\")\n",
    "print(f\"Training steps: {len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"Starting training...\")\n",
    "print(f\"Initial GPU memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ Training complete!\")\n",
    "    print(f\"   Time: {training_time/60:.1f} minutes\")\n",
    "    print(f\"   Final loss: {train_result.training_loss:.4f}\")\n",
    "    print(f\"   Peak GPU memory: {torch.cuda.max_memory_allocated()/1e9:.2f} GB\")\n",
    "    \n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\"‚ùå Out of memory! Try:\")\n",
    "        print(\"   1. Reduce batch size\")\n",
    "        print(\"   2. Reduce max_length\")\n",
    "        print(\"   3. Use a smaller model\")\n",
    "        print(\"   4. Enable gradient checkpointing\")\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Evaluate Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model\n",
    "def generate_response(model, tokenizer, instruction: str, input_text: str = \"\") -> str:\n",
    "    \"\"\"Generate a response from the fine-tuned model.\"\"\"\n",
    "    if input_text:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract just the response part\n",
    "    if \"### Response:\" in response:\n",
    "        response = response.split(\"### Response:\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test with various prompts\n",
    "test_prompts = [\n",
    "    (\"What is machine learning?\", \"\"),\n",
    "    (\"Write a haiku about programming.\", \"\"),\n",
    "    (\"Summarize this text:\", \"The sun rose over the mountains, casting golden light across the valley.\"),\n",
    "]\n",
    "\n",
    "print(\"\\nü§ñ Testing Fine-tuned Model:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model.eval()\n",
    "for instruction, input_text in test_prompts:\n",
    "    print(f\"\\nüìù Instruction: {instruction}\")\n",
    "    if input_text:\n",
    "        print(f\"   Input: {input_text}\")\n",
    "    \n",
    "    response = generate_response(model, tokenizer, instruction, input_text)\n",
    "    print(f\"   Response: {response}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Compare Fine-tuning Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory comparison: Mamba LoRA vs Full Fine-tuning\n",
    "\n",
    "print(\"üìä Memory Comparison: Mamba Fine-tuning\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Theoretical calculations for comparison\n",
    "models_comparison = {\n",
    "    \"Mamba-1.4B\": {\n",
    "        \"params_b\": 1.4,\n",
    "        \"lora_params_m\": 5,  # Approximate\n",
    "    },\n",
    "    \"Mamba-2.8B\": {\n",
    "        \"params_b\": 2.8,\n",
    "        \"lora_params_m\": 10,\n",
    "    },\n",
    "    \"Llama-3B (Transformer)\": {\n",
    "        \"params_b\": 3.0,\n",
    "        \"lora_params_m\": 15,  # More target layers\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Model':<25} {'Full FT (GB)':<15} {'LoRA (GB)':<15} {'Savings':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for name, specs in models_comparison.items():\n",
    "    # Full fine-tuning: model + gradients + optimizer states\n",
    "    full_ft_memory = specs[\"params_b\"] * 2 * 6  # BF16 * (model + grad + adam states)\n",
    "    \n",
    "    # LoRA: model frozen + small trainable + gradients + optimizer\n",
    "    lora_memory = (specs[\"params_b\"] * 2) + (specs[\"lora_params_m\"] / 1000 * 6)\n",
    "    \n",
    "    savings = (full_ft_memory - lora_memory) / full_ft_memory * 100\n",
    "    \n",
    "    print(f\"{name:<25} {full_ft_memory:<15.1f} {lora_memory:<15.1f} {savings:.0f}%\")\n",
    "\n",
    "print(\"\\nüí° LoRA reduces memory by ~70-80% compared to full fine-tuning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Save and Load LoRA Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_PEFT:\n",
    "    # Save LoRA weights (much smaller than full model!)\n",
    "    output_dir = \"./mamba-lora-adapter\"\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    # Check size\n",
    "    import os\n",
    "    total_size = sum(\n",
    "        os.path.getsize(os.path.join(output_dir, f))\n",
    "        for f in os.listdir(output_dir)\n",
    "        if os.path.isfile(os.path.join(output_dir, f))\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ LoRA adapter saved to {output_dir}\")\n",
    "    print(f\"   Size: {total_size / 1e6:.1f} MB\")\n",
    "    print(f\"   (vs ~{1.4 * 2 * 1000:.0f} MB for full model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the LoRA adapter later:\n",
    "# from peft import PeftModel\n",
    "# \n",
    "# base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "# model = PeftModel.from_pretrained(base_model, \"./mamba-lora-adapter\")\n",
    "\n",
    "print(\"\\nüìã To load the adapter later:\")\n",
    "print(\"\"\"\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"state-spaces/mamba-1.4b-hf\")\n",
    "model = PeftModel.from_pretrained(base_model, \"./mamba-lora-adapter\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ‚ö†Ô∏è Common Mistakes\n\n### Mistake 1: Wrong Target Modules\n```python\n# ‚ùå Using transformer target names for Mamba\ntarget_modules=[\"q_proj\", \"v_proj\"]  # These don't exist in Mamba!\n\n# ‚ùå Hardcoding without checking\ntarget_modules=[\"in_proj\", \"out_proj\"]  # May not exist in all Mamba variants\n\n# ‚úÖ Auto-detect available targets\navailable = find_lora_targets(model)\ntarget_modules = [t for t in [\"in_proj\", \"out_proj\", \"x_proj\"] if t in available]\n```\n\n### Mistake 2: Too High Learning Rate\n```python\n# ‚ùå Standard fine-tuning LR\nlearning_rate=3e-5  # Too low for LoRA\n\n# ‚úÖ Higher LR works for LoRA\nlearning_rate=2e-4  # LoRA can handle higher LR\n```\n\n### Mistake 3: Not Enabling Gradient Checkpointing\n```python\n# ‚ùå Memory pressure on long sequences\ngradient_checkpointing=False\n\n# ‚úÖ Save memory with gradient checkpointing\ngradient_checkpointing=True\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ LoRA works on Mamba (not just transformers!)\n",
    "- ‚úÖ Which Mamba layers to target for LoRA\n",
    "- ‚úÖ How to prepare instruction data for fine-tuning\n",
    "- ‚úÖ Memory savings from LoRA vs full fine-tuning\n",
    "- ‚úÖ How to save and load LoRA adapters\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise: Custom Domain Fine-tuning\n",
    "1. Create a dataset specific to your domain (e.g., customer service, medical, legal)\n",
    "2. Fine-tune Mamba on this dataset\n",
    "3. Evaluate on domain-specific test prompts\n",
    "4. Compare with base model responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [PEFT Documentation](https://huggingface.co/docs/peft)\n",
    "- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n",
    "- [QLoRA Paper](https://arxiv.org/abs/2305.14314)\n",
    "- [Mamba Fine-tuning Guide](https://github.com/state-spaces/mamba)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "if 'model' in dir():\n",
    "    del model\n",
    "if 'trainer' in dir():\n",
    "    del trainer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}