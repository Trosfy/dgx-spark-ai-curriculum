{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.4.3: MoE Exploration\n",
    "\n",
    "**Module:** 2.4 - Efficient Architectures  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** â­â­â­ (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- [ ] Understand the Mixture of Experts (MoE) architecture\n",
    "- [ ] Load and run MoE models on DGX Spark\n",
    "- [ ] Analyze expert activation patterns across different prompts\n",
    "- [ ] Compare total vs active parameters\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Prerequisites\n",
    "\n",
    "- Completed: Lab 2.4.1-2.4.2 (Mamba labs)\n",
    "- Knowledge of: Transformer architecture, feed-forward layers\n",
    "- Hardware: DGX Spark (128GB) for DeepSeekMoE-16B, or 24GB+ GPU for smaller models\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ Real-World Context\n",
    "\n",
    "**The Scaling Challenge**\n",
    "\n",
    "Everyone wants larger models (they perform better!), but:\n",
    "- GPT-4: ~1.7 trillion parameters (rumored)\n",
    "- Cost: Millions of dollars to train and run\n",
    "- Inference: Slow and expensive\n",
    "\n",
    "**MoE's solution**: Have 1 trillion parameters, but only USE 100 billion per token!\n",
    "\n",
    "**Real-world MoE models:**\n",
    "- **Mixtral 8x7B**: 45B total, 12.9B active (3.5x efficiency)\n",
    "- **DeepSeekMoE-16B**: 16B total, 2.5B active (6.4x efficiency!)\n",
    "- **GPT-4**: Rumored to be MoE (8 experts, ~200B each)\n",
    "- **DeepSeek-V3**: 671B total, 37B active (18x efficiency)\n",
    "\n",
    "On DGX Spark, you can run MoE models that would require a data center with dense architectures!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ§’ ELI5: Mixture of Experts\n",
    "\n",
    "> **Imagine a hospital with specialist doctors...**\n",
    ">\n",
    "> **Dense model approach**: One super-doctor who knows everything about every medical specialty. They're amazing but:\n",
    "> - Very expensive to train (medical school for 50 years!)\n",
    "> - Takes forever for each consultation (thinking about everything)\n",
    ">\n",
    "> **MoE approach**: A hospital with many specialists:\n",
    "> - Cardiologist (heart expert)\n",
    "> - Neurologist (brain expert)  \n",
    "> - Dermatologist (skin expert)\n",
    "> - ... and a smart receptionist (router)\n",
    ">\n",
    "> When you arrive, the receptionist looks at your symptoms and sends you to the RIGHT specialists (usually 2-3). You don't see ALL doctorsâ€”just the relevant ones!\n",
    ">\n",
    "> **In AI terms:**\n",
    "> - Each \"expert\" is a feed-forward network (the expensive part of transformers)\n",
    "> - The \"router\" is a small network that picks which experts to use\n",
    "> - For each token, only top-k experts are activated (k=2 typically)\n",
    "> - Result: 10x parameters, but only 1x compute!\n",
    "\n",
    "### The Key Insight: Sparse Activation\n",
    "\n",
    "```\n",
    "Dense model:  All parameters used for every token\n",
    "              Total params = Active params = 70B\n",
    "              \n",
    "MoE model:    Many parameters, few used per token\n",
    "              Total params = 200B\n",
    "              Active params = 20B (only 10%!)\n",
    "              \n",
    "Same compute cost, but 10x more knowledge stored!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Check environment\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name()\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Memory: {total_memory:.1f} GB\")\n",
    "    \n",
    "    if total_memory > 100:\n",
    "        print(\"\\nðŸš€ DGX Spark detected! You can run large MoE models.\")\n",
    "        RECOMMENDED_MODEL = \"deepseek-ai/deepseek-moe-16b-base\"\n",
    "    elif total_memory > 20:\n",
    "        print(\"\\nðŸ’¡ Large GPU detected. Some MoE models will fit.\")\n",
    "        RECOMMENDED_MODEL = \"Qwen/Qwen1.5-MoE-A2.7B\"\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ Limited GPU memory. Using 8-bit quantization.\")\n",
    "        RECOMMENDED_MODEL = \"Qwen/Qwen1.5-MoE-A2.7B\"\n",
    "else:\n",
    "    print(\"âš ï¸ No CUDA available. This lab requires GPU.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def clear_gpu():\n",
    "    \"\"\"Clear GPU memory.\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU memory: {torch.cuda.memory_allocated()/1e9:.2f} GB allocated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Loading an MoE Model\n",
    "\n",
    "### Available MoE Models\n",
    "\n",
    "| Model | Total Params | Active Params | Memory (FP16) | DGX Spark? |\n",
    "|-------|-------------|---------------|---------------|------------|\n",
    "| Qwen1.5-MoE-A2.7B | 14.3B | 2.7B | ~29GB | âœ… Easy |\n",
    "| DeepSeekMoE-16B | 16B | 2.5B | ~32GB | âœ… Easy |\n",
    "| Mixtral-8x7B | 45B | 12.9B | ~90GB | âœ… Fits |\n",
    "| DeepSeek-V3 | 671B | 37B | ~1.3TB | âŒ Too large |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model based on available memory\n",
    "# Uncomment the model you want to use\n",
    "\n",
    "# MODEL_NAME = \"Qwen/Qwen1.5-MoE-A2.7B\"  # Smaller, fits on 24GB\n",
    "MODEL_NAME = \"deepseek-ai/deepseek-moe-16b-base\"  # Recommended for DGX Spark\n",
    "# MODEL_NAME = \"mistralai/Mixtral-8x7B-v0.1\"  # Larger, needs ~90GB\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "print(\"(This may take a few minutes for large models)\\n\")\n",
    "\n",
    "clear_gpu()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "# For limited memory, add: load_in_8bit=True or load_in_4bit=True\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Report stats\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "print(f\"\\nâœ… Model loaded!\")\n",
    "print(f\"   Total parameters: {total_params/1e9:.2f}B\")\n",
    "print(f\"   GPU Memory used: {memory_used:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract MoE configuration\n",
    "config = model.config\n",
    "\n",
    "# Different models store this differently\n",
    "try:\n",
    "    num_experts = getattr(config, 'num_experts', \n",
    "                         getattr(config, 'n_routed_experts',\n",
    "                                getattr(config, 'num_local_experts', 8)))\n",
    "    experts_per_token = getattr(config, 'num_experts_per_tok',\n",
    "                               getattr(config, 'top_k', 2))\n",
    "except:\n",
    "    num_experts = 8\n",
    "    experts_per_token = 2\n",
    "\n",
    "print(f\"\\nðŸ“Š MoE Architecture:\")\n",
    "print(f\"   Number of experts: {num_experts}\")\n",
    "print(f\"   Experts per token: {experts_per_token}\")\n",
    "print(f\"   Sparsity ratio: {num_experts / experts_per_token:.1f}x\")\n",
    "print(f\"   Active params (approx): {total_params / (num_experts / experts_per_token) / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” What's Special About MoE Memory?\n",
    "\n",
    "Notice that we loaded ALL parameters (~32GB for DeepSeekMoE-16B), but:\n",
    "- During inference, only ~2.5B parameters are actually **computed**\n",
    "- This means 6x+ compute savings compared to a dense 16B model\n",
    "- On DGX Spark's 128GB, we have tons of room for batch processing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Testing MoE Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt: str, max_tokens: int = 50) -> str:\n",
    "    \"\"\"Generate text with the MoE model.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test with different types of prompts\n",
    "test_prompts = {\n",
    "    \"Code\": \"def fibonacci(n):\",\n",
    "    \"Math\": \"The derivative of x^2 is\",\n",
    "    \"Story\": \"Once upon a time,\",\n",
    "    \"Science\": \"Photosynthesis is the process\",\n",
    "}\n",
    "\n",
    "print(\"Testing MoE generation on different domains:\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for domain, prompt in test_prompts.items():\n",
    "    print(f\"\\nðŸ“ {domain}:\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    result = generate_text(prompt, max_tokens=30)\n",
    "    print(f\"Output: {result}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 4: Analyzing Expert Activation Patterns\n\nThe key question: **Do different experts specialize in different topics?**\n\nLet's hook into the router to see which experts activate for different inputs.\n\n### PyTorch Hooks: Inspecting Model Internals\n\nPyTorch provides **hooks** to intercept data flowing through a model. We'll use `register_forward_hook` to capture router outputs:\n\n```python\ndef hook_function(module, input, output):\n    \"\"\"Called every time the module processes data.\"\"\"\n    # module: The layer being hooked\n    # input: Tuple of inputs to the layer\n    # output: The layer's output\n    captured_outputs.append(output)\n\n# Attach hook to a module\nhook_handle = layer.register_forward_hook(hook_function)\n\n# Run inference - hook captures outputs\nmodel(input_data)\n\n# Remove hook when done\nhook_handle.remove()\n```\n\nThis is powerful for:\n- Visualizing intermediate activations\n- Debugging model behavior\n- Analyzing which experts activate in MoE models"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpertActivationTracker:\n",
    "    \"\"\"\n",
    "    Track which experts are activated during inference.\n",
    "    \n",
    "    Uses hooks to capture router outputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, num_experts: int = 64):\n",
    "        self.model = model\n",
    "        self.num_experts = num_experts\n",
    "        self.activations = []  # Store (layer, token_idx, expert_indices, weights)\n",
    "        self.hooks = []\n",
    "        \n",
    "    def _find_router_modules(self):\n",
    "        \"\"\"Find router/gate modules in the model.\"\"\"\n",
    "        routers = []\n",
    "        \n",
    "        for name, module in self.model.named_modules():\n",
    "            # Different models use different names\n",
    "            if 'gate' in name.lower() or 'router' in name.lower():\n",
    "                if hasattr(module, 'weight'):  # Linear layer\n",
    "                    routers.append((name, module))\n",
    "        \n",
    "        return routers\n",
    "    \n",
    "    def _hook_fn(self, layer_idx):\n",
    "        \"\"\"Create a hook function for a specific layer.\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            # Output is typically router logits [batch, seq, num_experts]\n",
    "            if isinstance(output, tuple):\n",
    "                router_logits = output[0]\n",
    "            else:\n",
    "                router_logits = output\n",
    "            \n",
    "            if router_logits is not None and len(router_logits.shape) >= 2:\n",
    "                self.activations.append({\n",
    "                    'layer': layer_idx,\n",
    "                    'logits': router_logits.detach().cpu(),\n",
    "                })\n",
    "        return hook\n",
    "    \n",
    "    def attach_hooks(self):\n",
    "        \"\"\"Attach hooks to router modules.\"\"\"\n",
    "        routers = self._find_router_modules()\n",
    "        print(f\"Found {len(routers)} router modules\")\n",
    "        \n",
    "        for idx, (name, module) in enumerate(routers[:8]):  # First 8 layers\n",
    "            hook = module.register_forward_hook(self._hook_fn(idx))\n",
    "            self.hooks.append(hook)\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Remove all hooks.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear stored activations.\"\"\"\n",
    "        self.activations = []\n",
    "    \n",
    "    def get_expert_counts(self, top_k: int = 2) -> Dict[int, int]:\n",
    "        \"\"\"Count how often each expert was in top-k.\"\"\"\n",
    "        counts = defaultdict(int)\n",
    "        \n",
    "        for act in self.activations:\n",
    "            logits = act['logits']\n",
    "            if len(logits.shape) == 3:  # [batch, seq, experts]\n",
    "                for batch in range(logits.shape[0]):\n",
    "                    for seq in range(logits.shape[1]):\n",
    "                        top_experts = torch.topk(logits[batch, seq], k=top_k).indices\n",
    "                        for exp in top_experts.tolist():\n",
    "                            counts[exp] += 1\n",
    "            elif len(logits.shape) == 2:  # [tokens, experts]\n",
    "                for seq in range(logits.shape[0]):\n",
    "                    top_experts = torch.topk(logits[seq], k=top_k).indices\n",
    "                    for exp in top_experts.tolist():\n",
    "                        counts[exp] += 1\n",
    "        \n",
    "        return dict(counts)\n",
    "\n",
    "# Create tracker\n",
    "tracker = ExpertActivationTracker(model, num_experts=num_experts)\n",
    "print(\"Created ExpertActivationTracker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach hooks and run inference\n",
    "tracker.attach_hooks()\n",
    "\n",
    "# Define test prompts for different domains\n",
    "domain_prompts = {\n",
    "    \"Code\": [\n",
    "        \"def quicksort(arr):\",\n",
    "        \"import numpy as np\\nimport pandas as pd\",\n",
    "        \"class DatabaseConnection:\",\n",
    "        \"SELECT * FROM users WHERE\",\n",
    "    ],\n",
    "    \"Math\": [\n",
    "        \"The integral of sin(x) is\",\n",
    "        \"Solve the equation: 2x + 5 = 15\",\n",
    "        \"The Pythagorean theorem states\",\n",
    "        \"Calculate the derivative of\",\n",
    "    ],\n",
    "    \"Creative\": [\n",
    "        \"Once upon a time in a magical kingdom\",\n",
    "        \"The sunset painted the sky with\",\n",
    "        \"She walked through the ancient forest\",\n",
    "        \"The old lighthouse keeper watched\",\n",
    "    ],\n",
    "    \"Science\": [\n",
    "        \"The theory of relativity explains\",\n",
    "        \"DNA replication begins when\",\n",
    "        \"The laws of thermodynamics state\",\n",
    "        \"Quantum entanglement is a phenomenon\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Analyze expert activation for each domain\n",
    "domain_expert_counts = {}\n",
    "\n",
    "for domain, prompts in domain_prompts.items():\n",
    "    print(f\"\\nAnalyzing {domain} prompts...\")\n",
    "    tracker.clear()\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "    \n",
    "    counts = tracker.get_expert_counts(top_k=experts_per_token)\n",
    "    domain_expert_counts[domain] = counts\n",
    "    \n",
    "    # Show top 5 experts for this domain\n",
    "    sorted_experts = sorted(counts.items(), key=lambda x: -x[1])[:5]\n",
    "    print(f\"  Top experts: {sorted_experts}\")\n",
    "\n",
    "tracker.remove_hooks()\n",
    "print(\"\\nâœ… Expert analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize expert specialization\n",
    "\n",
    "# Normalize counts for each domain\n",
    "all_experts = set()\n",
    "for counts in domain_expert_counts.values():\n",
    "    all_experts.update(counts.keys())\n",
    "all_experts = sorted(all_experts)\n",
    "\n",
    "# Create matrix: domains x experts\n",
    "matrix = np.zeros((len(domain_prompts), len(all_experts)))\n",
    "domain_names = list(domain_prompts.keys())\n",
    "\n",
    "for i, domain in enumerate(domain_names):\n",
    "    counts = domain_expert_counts[domain]\n",
    "    total = sum(counts.values()) + 1e-8\n",
    "    for j, exp in enumerate(all_experts):\n",
    "        matrix[i, j] = counts.get(exp, 0) / total\n",
    "\n",
    "# Plot heatmap\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "im = ax.imshow(matrix, aspect='auto', cmap='YlOrRd')\n",
    "\n",
    "ax.set_yticks(range(len(domain_names)))\n",
    "ax.set_yticklabels(domain_names, fontsize=12)\n",
    "ax.set_xlabel('Expert Index', fontsize=12)\n",
    "ax.set_ylabel('Domain', fontsize=12)\n",
    "ax.set_title('Expert Activation Patterns by Domain\\n(Darker = More Active)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Show expert indices for top-activated ones\n",
    "if len(all_experts) <= 30:\n",
    "    ax.set_xticks(range(len(all_experts)))\n",
    "    ax.set_xticklabels(all_experts, rotation=45, fontsize=8)\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='Activation Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find domain-specific experts\n",
    "print(\"\\nðŸ” Domain-Specific Expert Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, domain in enumerate(domain_names):\n",
    "    # Find experts that are most activated for this domain vs others\n",
    "    domain_activations = matrix[i]\n",
    "    other_activations = np.mean(matrix[[j for j in range(len(domain_names)) if j != i]], axis=0)\n",
    "    \n",
    "    specialization = domain_activations - other_activations\n",
    "    top_specialized = np.argsort(specialization)[-3:][::-1]\n",
    "    \n",
    "    print(f\"\\n{domain}:\")\n",
    "    for exp_idx in top_specialized:\n",
    "        exp_id = all_experts[exp_idx]\n",
    "        score = specialization[exp_idx]\n",
    "        if score > 0:\n",
    "            print(f\"  Expert {exp_id}: +{score:.3f} specialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” What Do Expert Patterns Tell Us?\n",
    "\n",
    "The visualization shows:\n",
    "1. **Some experts are \"generalists\"** - activated frequently across all domains\n",
    "2. **Some experts are \"specialists\"** - activated more for specific domains\n",
    "3. **The model learned this naturally** - no explicit domain labels during training!\n",
    "\n",
    "This emergent specialization is what makes MoE powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Memory vs Compute Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze memory vs compute efficiency\n",
    "\n",
    "def analyze_moe_efficiency(total_params: int, active_params: int, \n",
    "                           dense_equivalent_params: int) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze MoE efficiency compared to dense models.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"total_params_B\": total_params / 1e9,\n",
    "        \"active_params_B\": active_params / 1e9,\n",
    "        \"sparsity_ratio\": total_params / active_params,\n",
    "        \"compute_savings\": (1 - active_params / total_params) * 100,\n",
    "        \"memory_fp16_GB\": total_params * 2 / 1e9,\n",
    "        \"memory_fp8_GB\": total_params * 1 / 1e9,\n",
    "        \"memory_fp4_GB\": total_params * 0.5 / 1e9,\n",
    "        \"equivalent_dense_memory_GB\": dense_equivalent_params * 2 / 1e9,\n",
    "    }\n",
    "\n",
    "# Compare different MoE models\n",
    "moe_models = {\n",
    "    \"DeepSeekMoE-16B\": (16e9, 2.5e9, 2.5e9),  # (total, active, dense_equiv)\n",
    "    \"Mixtral-8x7B\": (45e9, 12.9e9, 12.9e9),\n",
    "    \"Qwen-MoE-A2.7B\": (14.3e9, 2.7e9, 2.7e9),\n",
    "    \"GPT-4 (rumored)\": (1700e9, 200e9, 200e9),\n",
    "}\n",
    "\n",
    "print(\"ðŸ“Š MoE Model Efficiency Analysis\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Model':<20} {'Total':<10} {'Active':<10} {'Sparsity':<10} {'Memory (FP16)':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, (total, active, dense) in moe_models.items():\n",
    "    analysis = analyze_moe_efficiency(total, active, dense)\n",
    "    print(f\"{name:<20} {analysis['total_params_B']:.1f}B      \"\n",
    "          f\"{analysis['active_params_B']:.1f}B      \"\n",
    "          f\"{analysis['sparsity_ratio']:.1f}x       \"\n",
    "          f\"{analysis['memory_fp16_GB']:.1f} GB\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insight: MoE lets you have much more 'knowledge' (total params)\")\n",
    "print(\"   while keeping inference cost manageable (active params)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the efficiency tradeoff\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Data for visualization\n",
    "models = list(moe_models.keys())\n",
    "totals = [moe_models[m][0]/1e9 for m in models]\n",
    "actives = [moe_models[m][1]/1e9 for m in models]\n",
    "\n",
    "# Left: Bar chart of total vs active\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[0].bar(x - width/2, totals, width, label='Total Params', color='#3498DB')\n",
    "bars2 = axes[0].bar(x + width/2, actives, width, label='Active Params', color='#27AE60')\n",
    "\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('Parameters (Billions)')\n",
    "axes[0].set_title('Total vs Active Parameters', fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(models, rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Right: Efficiency ratio\n",
    "ratios = [t/a for t, a in zip(totals, actives)]\n",
    "colors = plt.cm.RdYlGn([r/max(ratios) for r in ratios])\n",
    "\n",
    "bars = axes[1].bar(models, ratios, color=colors)\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('Efficiency Ratio (Total/Active)')\n",
    "axes[1].set_title('MoE Efficiency Ratio\\n(Higher = More Efficient)', fontweight='bold')\n",
    "axes[1].set_xticklabels(models, rotation=45, ha='right')\n",
    "\n",
    "# Add value labels\n",
    "for bar, ratio in zip(bars, ratios):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2,\n",
    "                f'{ratio:.1f}x', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸš€ On DGX Spark (128GB):\")\n",
    "for name, (total, active, _) in moe_models.items():\n",
    "    memory_needed = total * 2 / 1e9  # FP16\n",
    "    if memory_needed < 128:\n",
    "        print(f\"   âœ… {name}: {memory_needed:.0f}GB fits!\")\n",
    "    else:\n",
    "        print(f\"   âŒ {name}: {memory_needed:.0f}GB too large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš ï¸ Common Mistakes\n",
    "\n",
    "### Mistake 1: Thinking All Experts Are Always Loaded\n",
    "```python\n",
    "# âœ… All experts ARE in memory (needed for routing)\n",
    "# âŒ But NOT all are computed per token!\n",
    "\n",
    "# Memory: Total params Ã— precision\n",
    "# Compute: Active params Ã— precision\n",
    "```\n",
    "\n",
    "### Mistake 2: Ignoring Load Balancing\n",
    "```python\n",
    "# âŒ Some experts may be overused, others ignored\n",
    "# This wastes capacity!\n",
    "\n",
    "# âœ… MoE models include auxiliary loss for load balancing\n",
    "# Check if your model has balanced expert usage\n",
    "```\n",
    "\n",
    "### Mistake 3: Wrong Batch Size Expectations\n",
    "```python\n",
    "# âŒ \"MoE should be fast with any batch size\"\n",
    "# âœ… MoE benefits from batching - experts can process multiple tokens\n",
    "\n",
    "# For efficiency, use batch_size >= num_experts_per_token\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- âœ… How MoE achieves efficiency through sparse activation\n",
    "- âœ… Loading and running MoE models on DGX Spark\n",
    "- âœ… Expert specialization emerges from training\n",
    "- âœ… Total vs active parameter tradeoffs\n",
    "- âœ… Visualizing expert activation patterns\n",
    "\n",
    "---\n",
    "\n",
    "## âœ‹ Try It Yourself\n",
    "\n",
    "### Exercise: Custom Domain Analysis\n",
    "Add new domain categories and analyze expert activation:\n",
    "1. Legal text\n",
    "2. Medical text\n",
    "3. Financial text\n",
    "\n",
    "Which experts specialize in these domains?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“– Further Reading\n",
    "\n",
    "- [Mixtral Paper](https://arxiv.org/abs/2401.04088) - Mixture of Experts Made Easy\n",
    "- [DeepSeekMoE Paper](https://arxiv.org/abs/2401.06066) - Efficient MoE Architecture\n",
    "- [Switch Transformer](https://arxiv.org/abs/2101.03961) - Simplified MoE at Scale\n",
    "- [GShard Paper](https://arxiv.org/abs/2006.16668) - Scaling Giant Models with MoE\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "if 'model' in dir():\n",
    "    del model\n",
    "if 'tokenizer' in dir():\n",
    "    del tokenizer\n",
    "if 'tracker' in dir():\n",
    "    tracker.remove_hooks()\n",
    "    del tracker\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}