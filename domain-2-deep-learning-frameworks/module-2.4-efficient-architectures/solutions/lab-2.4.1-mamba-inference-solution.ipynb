{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.4.1: Mamba Inference - SOLUTIONS\n",
    "\n",
    "This notebook contains complete solutions to the exercises from Lab 2.4.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup (Same as Lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load Mamba model\n",
    "MODEL_NAME = \"state-spaces/mamba-2.8b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1 Solution: Long Document Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Load and process a long document\n",
    "\n",
    "# Load a book from PG19 dataset\n",
    "print(\"Loading PG19 dataset...\")\n",
    "pg19 = load_dataset(\"pg19\", split=\"test[:1]\")\n",
    "\n",
    "# Get the text and truncate to manageable size\n",
    "long_text = pg19[0][\"text\"][:100000]  # First 100K characters\n",
    "print(f\"\\nDocument length: {len(long_text):,} characters\")\n",
    "\n",
    "# Tokenize and measure\n",
    "tokens = tokenizer.encode(long_text, add_special_tokens=False)\n",
    "print(f\"Token count: {len(tokens):,} tokens\")\n",
    "\n",
    "# Calculate theoretical memory for transformer\n",
    "def transformer_memory(context_length, hidden_size=4096, num_layers=32, \n",
    "                       num_heads=32, bytes_per_element=2):\n",
    "    \"\"\"Calculate KV cache memory for a transformer.\"\"\"\n",
    "    head_dim = hidden_size // num_heads\n",
    "    # KV cache: 2 (K and V) * num_layers * context * num_heads * head_dim\n",
    "    kv_cache_bytes = 2 * num_layers * context_length * num_heads * head_dim * bytes_per_element\n",
    "    return kv_cache_bytes / 1e9  # GB\n",
    "\n",
    "# Run Mamba inference\n",
    "print(f\"\\nRunning Mamba inference on {len(tokens):,} tokens...\")\n",
    "\n",
    "# Truncate if needed for memory\n",
    "max_tokens = min(len(tokens), 32768)  # 32K limit for demo\n",
    "input_ids = torch.tensor([tokens[:max_tokens]], device=device)\n",
    "\n",
    "# Clear memory stats\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# Measure memory during forward pass\n",
    "with torch.no_grad():\n",
    "    _ = model(input_ids)\n",
    "\n",
    "mamba_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "transformer_theoretical = transformer_memory(max_tokens)\n",
    "\n",
    "print(f\"\\nüìä Memory Comparison at {max_tokens:,} tokens:\")\n",
    "print(f\"   Mamba actual: {mamba_memory:.2f} GB\")\n",
    "print(f\"   Transformer KV cache (theoretical): {transformer_theoretical:.2f} GB\")\n",
    "print(f\"   Memory savings: {(transformer_theoretical - mamba_memory):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2 Solution: Context Scaling Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Detailed benchmark with TTFT, throughput, and memory\n",
    "\n",
    "def comprehensive_benchmark(model, tokenizer, context_lengths, generation_length=50):\n",
    "    \"\"\"\n",
    "    Comprehensive benchmark measuring:\n",
    "    - Time-to-first-token (TTFT)\n",
    "    - Throughput (tokens/second)\n",
    "    - Peak memory usage\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for ctx_len in context_lengths:\n",
    "        print(f\"\\nBenchmarking {ctx_len:,} tokens...\")\n",
    "        \n",
    "        try:\n",
    "            # Create input\n",
    "            input_ids = torch.randint(\n",
    "                100, tokenizer.vocab_size - 100,\n",
    "                (1, ctx_len), device=device\n",
    "            )\n",
    "            \n",
    "            # Clear memory\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            # Measure TTFT (time to generate first token)\n",
    "            torch.cuda.synchronize()\n",
    "            ttft_start = time.perf_counter()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                first_output = model.generate(\n",
    "                    input_ids,\n",
    "                    max_new_tokens=1,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                )\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            ttft = time.perf_counter() - ttft_start\n",
    "            \n",
    "            # Measure throughput (full generation)\n",
    "            torch.cuda.synchronize()\n",
    "            gen_start = time.perf_counter()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                full_output = model.generate(\n",
    "                    input_ids,\n",
    "                    max_new_tokens=generation_length,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                )\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            gen_time = time.perf_counter() - gen_start\n",
    "            \n",
    "            # Get memory\n",
    "            peak_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "            \n",
    "            results.append({\n",
    "                'context_length': ctx_len,\n",
    "                'ttft_seconds': ttft,\n",
    "                'throughput': generation_length / gen_time,\n",
    "                'peak_memory_gb': peak_memory,\n",
    "            })\n",
    "            \n",
    "            print(f\"  TTFT: {ttft*1000:.1f}ms | Throughput: {generation_length/gen_time:.1f} tok/s | Memory: {peak_memory:.2f}GB\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                print(f\"  OOM at {ctx_len:,} tokens\")\n",
    "                torch.cuda.empty_cache()\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comprehensive benchmark\n",
    "test_contexts = [1024, 2048, 4096, 8192, 16384]\n",
    "benchmark_results = comprehensive_benchmark(model, tokenizer, test_contexts)\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "contexts = [r['context_length'] for r in benchmark_results]\n",
    "ttfts = [r['ttft_seconds'] * 1000 for r in benchmark_results]  # Convert to ms\n",
    "throughputs = [r['throughput'] for r in benchmark_results]\n",
    "memories = [r['peak_memory_gb'] for r in benchmark_results]\n",
    "\n",
    "# TTFT plot\n",
    "axes[0].plot(contexts, ttfts, 'o-', linewidth=2, markersize=8, color='#E74C3C')\n",
    "axes[0].set_xlabel('Context Length (tokens)')\n",
    "axes[0].set_ylabel('Time to First Token (ms)')\n",
    "axes[0].set_title('TTFT vs Context Length', fontweight='bold')\n",
    "axes[0].set_xscale('log', base=2)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Throughput plot\n",
    "axes[1].plot(contexts, throughputs, 's-', linewidth=2, markersize=8, color='#27AE60')\n",
    "axes[1].set_xlabel('Context Length (tokens)')\n",
    "axes[1].set_ylabel('Tokens per Second')\n",
    "axes[1].set_title('Throughput vs Context Length', fontweight='bold')\n",
    "axes[1].set_xscale('log', base=2)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Memory plot\n",
    "axes[2].plot(contexts, memories, '^-', linewidth=2, markersize=8, color='#3498DB')\n",
    "axes[2].set_xlabel('Context Length (tokens)')\n",
    "axes[2].set_ylabel('Peak Memory (GB)')\n",
    "axes[2].set_title('Memory vs Context Length', fontweight='bold')\n",
    "axes[2].set_xscale('log', base=2)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Key insight\n",
    "print(\"\\nüîç Key Observations:\")\n",
    "print(f\"   TTFT scales approximately linearly with context (O(n))\")\n",
    "print(f\"   Throughput remains relatively stable\")\n",
    "print(f\"   Memory increases minimally - this is Mamba's advantage!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Solution: Long-Context Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Long-context summarizer comparison\n",
    "\n",
    "def mamba_summarize(model, tokenizer, text, max_summary_tokens=200):\n",
    "    \"\"\"\n",
    "    Summarize a long document using Mamba (processes full context).\n",
    "    \"\"\"\n",
    "    prompt = f\"Summarize the following text:\\n\\n{text}\\n\\nSummary:\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, \n",
    "                      max_length=32768).to(device)\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_summary_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    elapsed = time.perf_counter() - start\n",
    "    \n",
    "    summary = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], \n",
    "                               skip_special_tokens=True)\n",
    "    \n",
    "    return summary, elapsed, inputs['input_ids'].shape[1]\n",
    "\n",
    "def chunked_summarize(model, tokenizer, text, chunk_size=2048, max_summary_tokens=100):\n",
    "    \"\"\"\n",
    "    Summarize using chunking approach (for comparison with transformer-style).\n",
    "    \"\"\"\n",
    "    # Tokenize full text\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    # Split into chunks\n",
    "    chunks = [tokens[i:i+chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    chunk_summaries = []\n",
    "    \n",
    "    for chunk_tokens in chunks[:5]:  # Limit chunks for demo\n",
    "        chunk_text = tokenizer.decode(chunk_tokens)\n",
    "        prompt = f\"Summarize briefly:\\n{chunk_text}\\n\\nSummary:\"\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True,\n",
    "                          max_length=chunk_size + 100).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_summary_tokens // len(chunks[:5]),\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "        \n",
    "        chunk_summary = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:],\n",
    "                                         skip_special_tokens=True)\n",
    "        chunk_summaries.append(chunk_summary)\n",
    "    \n",
    "    # Combine chunk summaries\n",
    "    combined = \" \".join(chunk_summaries)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    \n",
    "    return combined, elapsed, len(chunks)\n",
    "\n",
    "# Test on a long document\n",
    "if 'pg19' in dir():\n",
    "    test_text = pg19[0][\"text\"][:20000]  # 20K characters\n",
    "else:\n",
    "    test_text = \"This is a test document. \" * 500\n",
    "\n",
    "print(f\"Document length: {len(test_text):,} characters\")\n",
    "print(f\"Document tokens: {len(tokenizer.encode(test_text)):,}\")\n",
    "\n",
    "# Mamba full-context approach\n",
    "print(\"\\nü¶é Mamba Full-Context Summarization:\")\n",
    "mamba_summary, mamba_time, mamba_tokens = mamba_summarize(model, tokenizer, test_text)\n",
    "print(f\"   Time: {mamba_time:.2f}s\")\n",
    "print(f\"   Input tokens: {mamba_tokens:,}\")\n",
    "print(f\"   Summary: {mamba_summary[:300]}...\")\n",
    "\n",
    "# Chunked approach\n",
    "print(\"\\nüìë Chunked Summarization:\")\n",
    "chunked_summary, chunked_time, num_chunks = chunked_summarize(model, tokenizer, test_text)\n",
    "print(f\"   Time: {chunked_time:.2f}s\")\n",
    "print(f\"   Chunks processed: {num_chunks}\")\n",
    "print(f\"   Summary: {chunked_summary[:300]}...\")\n",
    "\n",
    "print(\"\\nüìä Comparison:\")\n",
    "print(f\"   Mamba full-context: {mamba_time:.2f}s (sees entire document at once)\")\n",
    "print(f\"   Chunked approach: {chunked_time:.2f}s (loses cross-chunk context)\")\n",
    "print(f\"   Speedup: {chunked_time/mamba_time:.2f}x (but Mamba has better context!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"‚úÖ Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
