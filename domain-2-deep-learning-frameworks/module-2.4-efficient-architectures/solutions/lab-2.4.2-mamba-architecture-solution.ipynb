{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.4.2: Mamba Architecture Study - SOLUTIONS\n",
    "\n",
    "Complete solutions for the Mamba architecture exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Solution: State Evolution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\nMODEL_NAME = 'state-spaces/mamba-130m-hf'\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16, device_map='auto')\n\n# Test texts\ntexts = {\n    'Repetitive': 'the the the the the the the the the the',\n    'Structured (Code)': 'def foo(x):\\n    return x * 2\\n\\ndef bar(y):\\n    return y + 1',\n    'Diverse': 'Apple banana computer dance elephant future galaxy horizon ice jazz',\n}\n\nfig, axes = plt.subplots(len(texts), 2, figsize=(14, 4*len(texts)))\n\nfor idx, (label, text) in enumerate(texts.items()):\n    tokens = tokenizer.encode(text, add_special_tokens=False)[:30]\n    input_ids = torch.tensor([tokens], device=device)\n    \n    with torch.no_grad():\n        outputs = model(input_ids, output_hidden_states=True)\n    \n    hidden = outputs.hidden_states[1][0].cpu().numpy()[:, :32]\n    \n    # Heatmap\n    axes[idx, 0].imshow(hidden.T, aspect='auto', cmap='RdBu_r')\n    axes[idx, 0].set_title(f'{label}: State Evolution')\n    axes[idx, 0].set_ylabel('State Dim')\n    \n    # State magnitude\n    magnitudes = np.linalg.norm(outputs.hidden_states[1][0].cpu().numpy(), axis=1)\n    axes[idx, 1].plot(magnitudes, 'b-', linewidth=2)\n    axes[idx, 1].set_title(f'{label}: State Magnitude')\n    axes[idx, 1].set_ylabel('L2 Norm')\n\naxes[-1, 0].set_xlabel('Token Position')\naxes[-1, 1].set_xlabel('Token Position')\nplt.tight_layout()\nplt.show()\n\nprint('\\n Key Observations:')\nprint('1. Repetitive text: State converges to stable pattern')\nprint('2. Structured code: Regular patterns matching code structure')\nprint('3. Diverse text: High variance as model processes new concepts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\ndel model, tokenizer\nimport gc\ntorch.cuda.empty_cache()\ngc.collect()\nprint('Cleanup complete!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
