{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 2.4.6: Uploading Models to Hugging Face Hub\n\n**Module:** 2.4 - Hugging Face Ecosystem  \n**Time:** 2 hours  \n**Difficulty:** â­â­â­ (Intermediate)\n\n---\n\n## Learning Objectives\n\nBy the end of this notebook, you will:\n- [ ] Create a Hugging Face account and authentication token\n- [ ] Write a professional model card\n- [ ] Upload a fine-tuned model to the Hub\n- [ ] Organize model repositories properly\n- [ ] Share models with the community\n- [ ] Update and version your models\n\n---\n\n## Prerequisites\n\n- Completed: Labs 2.4.4 and 2.4.5 (Fine-tuning with Trainer and LoRA)\n- Have: A **Hugging Face account** (free at [huggingface.co/join](https://huggingface.co/join)) - **CREATE BEFORE STARTING**\n- Have: A **Write access token** from [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n- Have: A fine-tuned model to upload (or we'll create one in this notebook)\n\n---\n\n## Real-World Context\n\nYou've trained an amazing model. Now what?\n\n**Sharing your models enables:**\n- Reproducibility: Others can verify and build on your work\n- Collaboration: Team members can access the same model\n- Deployment: Use the Hub's inference API\n- Recognition: Build your ML portfolio and reputation\n\n**Success stories:**\n- Researchers share models alongside papers for reproducibility\n- Companies share specialized models for domain problems\n- Individuals build portfolios that lead to job opportunities\n- Open source models drive the entire AI community forward!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: What is the Hugging Face Hub?\n",
    "\n",
    "> **Imagine the Hub is like YouTube, but for AI models.**\n",
    ">\n",
    "> Just like YouTube:\n",
    "> - You create an account\n",
    "> - You upload your \"content\" (models instead of videos)\n",
    "> - You write a description (model card instead of video description)\n",
    "> - Others can view, download, and use your work\n",
    "> - Popular models get more attention and usage\n",
    ">\n",
    "> **Why share?**\n",
    "> - Your model might help someone solve their problem\n",
    "> - The community can improve your work\n",
    "> - You build a portfolio of your ML skills\n",
    "> - Companies look at your Hub profile when hiring!\n",
    ">\n",
    "> **The Model Card** is like the \"About\" section of your video:\n",
    "> - What does the model do?\n",
    "> - How was it trained?\n",
    "> - What are its limitations?\n",
    "> - How should people use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Authentication Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n# Note: These packages are pre-installed in the NGC PyTorch container.\n# Running pip install ensures you have compatible versions.\n# If NOT using NGC container, ensure you have ARM64-compatible packages for DGX Spark.\n\n!pip install -q \"transformers>=4.35.0\" \"huggingface_hub>=0.19.0\" \"datasets>=2.14.0\" \"peft>=0.6.0\" \"evaluate>=0.4.0\"\n\nimport torch\nimport numpy as np\nimport os\nfrom huggingface_hub import HfApi, login, whoami, create_repo\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport evaluate\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"Packages loaded!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Your Access Token\n",
    "\n",
    "To upload to the Hub, you need an access token:\n",
    "\n",
    "1. Go to [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "2. Click \"New token\"\n",
    "3. Give it a name (e.g., \"dgx-spark-training\")\n",
    "4. Select \"Write\" access (needed for uploads)\n",
    "5. Copy the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Interactive login (recommended for first time)\n",
    "# This will prompt for your token and save it\n",
    "# login()\n",
    "\n",
    "# Method 2: Login from terminal (run this in a cell if needed)\n",
    "# !huggingface-cli login\n",
    "\n",
    "# Method 3: Environment variable (for scripts/automation)\n",
    "# os.environ['HF_TOKEN'] = 'your_token_here'\n",
    "\n",
    "print(\"Authentication options:\")\n",
    "print(\"1. Run: login() - Interactive login\")\n",
    "print(\"2. Run: !huggingface-cli login - Terminal login\")\n",
    "print(\"3. Set HF_TOKEN environment variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if logged in\n",
    "try:\n",
    "    user_info = whoami()\n",
    "    print(f\"Logged in as: {user_info['name']}\")\n",
    "    print(f\"Organizations: {user_info.get('orgs', [])}\")\n",
    "    logged_in = True\n",
    "except Exception as e:\n",
    "    print(f\"Not logged in: {e}\")\n",
    "    print(\"\\nPlease run login() or !huggingface-cli login first\")\n",
    "    logged_in = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Preparing Your Model\n",
    "\n",
    "Let's create a simple fine-tuned model to upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Load and prepare data\n",
    "print(\"Loading IMDB dataset...\")\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "# Use small subset for quick training\n",
    "small_train = imdb['train'].shuffle(seed=42).select(range(2000))\n",
    "small_val = imdb['train'].shuffle(seed=42).select(range(2000, 2500))\n",
    "small_test = imdb['test'].shuffle(seed=42).select(range(500))\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': small_train,\n",
    "    'validation': small_val,\n",
    "    'test': small_test\n",
    "})\n",
    "\n",
    "print(f\"Train: {len(dataset['train'])}\")\n",
    "print(f\"Validation: {len(dataset['validation'])}\")\n",
    "print(f\"Test: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    ")\n",
    "\n",
    "print(f\"Model: {model.config.model_type}\")\n",
    "print(f\"Parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "tokenized = dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "tokenized = tokenized.rename_column('label', 'labels')\n",
    "\n",
    "print(f\"Tokenized columns: {tokenized['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model_for_upload\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    bf16=True,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized['train'],\n",
    "    eval_dataset=tokenized['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_results = trainer.evaluate(tokenized['test'])\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "print(f\"  Loss: {test_results['eval_loss']:.4f}\")\n",
    "\n",
    "# Store for model card\n",
    "model_accuracy = test_results['eval_accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Creating a Model Card\n",
    "\n",
    "A model card is ESSENTIAL. It tells users:\n",
    "- What the model does\n",
    "- How to use it\n",
    "- What data it was trained on\n",
    "- Known limitations and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a comprehensive model card\n# NOTE: Replace YOUR_USERNAME with your actual Hugging Face username before uploading!\n\nmodel_card_content = f\"\"\"\n---\nlanguage:\n- en\nlicense: mit\ntags:\n- sentiment-analysis\n- text-classification\n- distilbert\n- imdb\ndatasets:\n- imdb\nmetrics:\n- accuracy\nmodel-index:\n- name: distilbert-imdb-sentiment\n  results:\n  - task:\n      type: text-classification\n      name: Sentiment Analysis\n    dataset:\n      type: imdb\n      name: IMDB Movie Reviews\n    metrics:\n    - type: accuracy\n      value: {model_accuracy:.4f}\n---\n\n# DistilBERT fine-tuned for IMDB Sentiment Analysis\n\nThis model is a fine-tuned version of [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) \non the IMDB movie reviews dataset for sentiment analysis.\n\n## Model Description\n\n- **Base Model:** distilbert-base-uncased\n- **Task:** Binary sentiment classification (positive/negative)\n- **Language:** English\n- **Training Data:** IMDB Movie Reviews (subset)\n\n## Training Details\n\n### Hardware\n- **Platform:** NVIDIA DGX Spark\n- **GPU:** NVIDIA Blackwell GB10 Superchip\n- **Memory:** 128GB Unified LPDDR5X\n\n### Training Hyperparameters\n- Learning rate: 2e-5\n- Batch size: 16\n- Epochs: 2\n- Warmup ratio: 0.1\n- Max sequence length: 256\n- Precision: bfloat16\n\n### Training Data\n- Training samples: 2,000\n- Validation samples: 500\n- Test samples: 500\n\n## Evaluation Results\n\n| Metric | Value |\n|--------|-------|\n| Accuracy | {model_accuracy:.2%} |\n\n## Usage\n\n### Using the Pipeline API (Recommended)\n\n```python\nfrom transformers import pipeline\n\n# Replace YOUR_USERNAME with your actual username!\nclassifier = pipeline(\"sentiment-analysis\", model=\"YOUR_USERNAME/distilbert-imdb-sentiment\")\n\nresult = classifier(\"This movie was amazing!\")\n# Output: [{{'label': 'POSITIVE', 'score': 0.99}}]\n```\n\n### Using AutoModel\n\n```python\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\n\n# Replace YOUR_USERNAME with your actual username!\ntokenizer = AutoTokenizer.from_pretrained(\"YOUR_USERNAME/distilbert-imdb-sentiment\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"YOUR_USERNAME/distilbert-imdb-sentiment\")\n\ninputs = tokenizer(\"This movie was terrible!\", return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model(**inputs)\n\npredictions = torch.softmax(outputs.logits, dim=1)\n# predictions[0][0] = negative probability\n# predictions[0][1] = positive probability\n```\n\n## Limitations\n\n- **Domain:** Trained on movie reviews; may not generalize well to other domains\n- **Length:** Best with reviews under 256 tokens\n- **Language:** English only\n- **Bias:** May reflect biases present in IMDB reviews\n\n## Intended Use\n\nThis model is intended for:\n- Classifying sentiment of movie/entertainment reviews\n- Educational purposes (learning about fine-tuning)\n- Baseline for sentiment analysis tasks\n\nNot intended for:\n- High-stakes decision making\n- Medical or legal sentiment analysis\n- Languages other than English\n\n## Citation\n\nIf you use this model, please cite:\n\n```bibtex\n@misc{{distilbert-imdb-sentiment,\n  author = {{DGX Spark Student}},\n  title = {{DistilBERT fine-tuned for IMDB Sentiment Analysis}},\n  year = {{2025}},\n  publisher = {{Hugging Face}},\n  howpublished = {{\\\\url{{https://huggingface.co/YOUR_USERNAME/distilbert-imdb-sentiment}}}}\n}}\n```\n\n## Training Curriculum\n\nThis model was trained as part of the **DGX Spark AI Curriculum**, \nModule 9: Hugging Face Ecosystem.\n\n---\n\n*Last updated: 2025*\n\"\"\"\n\n# Save to file\nwith open(\"./model_for_upload/README.md\", \"w\") as f:\n    f.write(model_card_content)\n\nprint(\"Model card created!\")\nprint(\"\\nPreview (first 50 lines):\")\nprint(\"=\"*50)\nfor line in model_card_content.split(\"\\n\")[:50]:\n    print(line)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Card Best Practices\n",
    "\n",
    "| Section | Why It Matters |\n",
    "|---------|----------------|\n",
    "| **Description** | Quick understanding of what the model does |\n",
    "| **Training Details** | Reproducibility |\n",
    "| **Evaluation** | Trust and comparison |\n",
    "| **Usage Examples** | Easy adoption |\n",
    "| **Limitations** | Responsible AI - prevents misuse |\n",
    "| **Citation** | Credit for your work |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Uploading to the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer locally first\n",
    "save_path = \"./model_for_upload\"\n",
    "\n",
    "print(\"Saving model and tokenizer...\")\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "# List saved files\n",
    "print(\"\\nSaved files:\")\n",
    "for f in os.listdir(save_path):\n",
    "    size_mb = os.path.getsize(os.path.join(save_path, f)) / 1e6\n",
    "    print(f\"  {f}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Using push_to_hub() (simplest)\n",
    "# This uploads everything including the model card!\n",
    "\n",
    "# Define your repository name\n",
    "# Replace 'your-username' with your actual username!\n",
    "repo_name = \"distilbert-imdb-sentiment-demo\"\n",
    "\n",
    "print(\"To upload your model, run:\")\n",
    "print(f\"\"\"\\n\n",
    "# Option 1: Using trainer (includes training info)\n",
    "trainer.push_to_hub(\"{repo_name}\")\n",
    "\n",
    "# Option 2: Using model and tokenizer directly\n",
    "model.push_to_hub(\"{repo_name}\")\n",
    "tokenizer.push_to_hub(\"{repo_name}\")\n",
    "\n",
    "# Option 3: Upload entire directory\n",
    "from huggingface_hub import upload_folder\n",
    "upload_folder(\n",
    "    folder_path=\"{save_path}\",\n",
    "    repo_id=\"your-username/{repo_name}\",\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to actually upload (requires login)\n",
    "# trainer.push_to_hub(repo_name)\n",
    "# print(f\"\\nModel uploaded to: https://huggingface.co/YOUR_USERNAME/{repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Using the Hub API (More Control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, upload_file, upload_folder\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Create repository first (optional, push_to_hub creates it)\n",
    "# api.create_repo(repo_id=repo_name, repo_type=\"model\", private=False)\n",
    "\n",
    "print(\"Hub API options:\")\n",
    "print(\"\"\"\n",
    "# Create a new repository\n",
    "api.create_repo(repo_id=\"model-name\", repo_type=\"model\", private=False)\n",
    "\n",
    "# Upload a single file\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"./path/to/file\",\n",
    "    path_in_repo=\"filename\",\n",
    "    repo_id=\"username/repo-name\",\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "# Upload entire folder\n",
    "api.upload_folder(\n",
    "    folder_path=\"./local/folder\",\n",
    "    repo_id=\"username/repo-name\",\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "# Delete repository\n",
    "api.delete_repo(repo_id=\"username/repo-name\", repo_type=\"model\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Uploading LoRA Adapters\n",
    "\n",
    "LoRA adapters can also be uploaded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Create a LoRA model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],\n",
    "    lora_dropout=0.1,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    modules_to_save=[\"classifier\"]\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(base_model, lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save LoRA adapter\nlora_save_path = \"./lora_adapter_for_upload\"\nlora_model.save_pretrained(lora_save_path)\n\n# Add model card for adapter\n# NOTE: Replace YOUR_USERNAME with your actual Hugging Face username!\nadapter_card = \"\"\"\n---\nlibrary_name: peft\nbase_model: distilbert-base-uncased\n---\n\n# LoRA Adapter for DistilBERT Sentiment Analysis\n\nThis is a LoRA adapter for sentiment analysis.\n\n## Usage\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom peft import PeftModel\n\n# Load base model\nbase = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n\n# Load adapter (replace YOUR_USERNAME with your actual username!)\nmodel = PeftModel.from_pretrained(base, \"YOUR_USERNAME/lora-adapter-name\")\n```\n\n## Configuration\n\n- Rank (r): 8\n- Alpha: 16\n- Target modules: q_lin, v_lin\n\"\"\"\n\nwith open(os.path.join(lora_save_path, \"README.md\"), \"w\") as f:\n    f.write(adapter_card)\n\nprint(\"LoRA adapter saved!\")\nprint(f\"\\nTo upload: lora_model.push_to_hub('your-lora-adapter-name')\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Managing Your Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repository management examples\n",
    "print(\"Repository Management Commands:\")\n",
    "print(\"\"\"\n",
    "# List your models\n",
    "from huggingface_hub import list_models\n",
    "my_models = list(list_models(author=\"your-username\"))\n",
    "\n",
    "# Get model info\n",
    "info = api.model_info(\"username/model-name\")\n",
    "print(f\"Downloads: {info.downloads}\")\n",
    "print(f\"Likes: {info.likes}\")\n",
    "\n",
    "# Update model card\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"./new_README.md\",\n",
    "    path_in_repo=\"README.md\",\n",
    "    repo_id=\"username/model-name\"\n",
    ")\n",
    "\n",
    "# Make private/public\n",
    "api.update_repo_visibility(\"username/model-name\", private=True)\n",
    "\n",
    "# Add collaborators (for organizations)\n",
    "# Manage through web UI or API\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Git for version control\n",
    "print(\"Using Git with Hugging Face Hub:\")\n",
    "print(\"\"\"\n",
    "# Clone your model repository\n",
    "git clone https://huggingface.co/username/model-name\n",
    "cd model-name\n",
    "\n",
    "# Make changes\n",
    "# ... edit files ...\n",
    "\n",
    "# Commit and push\n",
    "git add .\n",
    "git commit -m \"Update model card\"\n",
    "git push\n",
    "\n",
    "# Use Git LFS for large files (automatic for model files)\n",
    "git lfs install\n",
    "git lfs track \"*.bin\"\n",
    "git lfs track \"*.safetensors\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Best Practices for Model Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "MODEL SHARING BEST PRACTICES\n",
    "============================\n",
    "\n",
    "1. NAMING CONVENTIONS\n",
    "   - Use descriptive names: 'bert-base-sentiment' not 'my_model_1'\n",
    "   - Include base model: 'distilbert-imdb-classifier'\n",
    "   - Include task/domain: 'roberta-medical-ner'\n",
    "\n",
    "2. MODEL CARD ESSENTIALS\n",
    "   - Clear description of what the model does\n",
    "   - Training data and methodology\n",
    "   - Evaluation metrics and results\n",
    "   - Code examples for usage\n",
    "   - Known limitations and biases\n",
    "   - License information\n",
    "\n",
    "3. FILE ORGANIZATION\n",
    "   - README.md (model card)\n",
    "   - config.json (model configuration)\n",
    "   - model.safetensors (weights - prefer safetensors over pytorch_model.bin)\n",
    "   - tokenizer files\n",
    "   - training_args.json (optional)\n",
    "\n",
    "4. VERSIONING\n",
    "   - Use Git tags for major versions\n",
    "   - Document changes in model card\n",
    "   - Keep backward compatibility when possible\n",
    "\n",
    "5. LICENSING\n",
    "   - Choose appropriate license (MIT, Apache 2.0, etc.)\n",
    "   - Check base model license compatibility\n",
    "   - Consider data licensing implications\n",
    "\n",
    "6. REPRODUCIBILITY\n",
    "   - Include training script or link to it\n",
    "   - Document random seeds used\n",
    "   - List package versions\n",
    "   - Provide training logs if possible\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Upload Your Model\n",
    "\n",
    "Complete the following steps to upload your own model:\n",
    "\n",
    "1. Login to Hugging Face\n",
    "2. Fine-tune a model (or use one from previous tasks)\n",
    "3. Create a comprehensive model card\n",
    "4. Upload to the Hub\n",
    "5. Test loading your model from the Hub\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "```python\n",
    "# Step 1: Login\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "\n",
    "# Step 2: Have your model ready (from Task 9.4 or 9.5)\n",
    "\n",
    "# Step 3: Create model card (copy template above)\n",
    "\n",
    "# Step 4: Upload\n",
    "trainer.push_to_hub(\"your-model-name\")\n",
    "\n",
    "# Step 5: Test\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-classification\", model=\"your-username/your-model-name\")\n",
    "print(pipe(\"Test sentence\"))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Upload your model to the Hub!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting the Model Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Upload without model card\n",
    "# model.push_to_hub(\"my-model\")  # No README.md!\n",
    "\n",
    "# CORRECT: Create model card first\n",
    "# 1. Save model card to output directory\n",
    "# 2. Then push\n",
    "\n",
    "print(\"Always include a model card!\")\n",
    "print(\"Users need to know how to use your model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Wrong Token Permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Using read-only token for upload\n",
    "# Error: \"You don't have permission to push to this repo\"\n",
    "\n",
    "# CORRECT: Use a token with WRITE access\n",
    "# Create at: huggingface.co/settings/tokens\n",
    "# Select \"Write\" when creating the token\n",
    "\n",
    "print(\"Token types:\")\n",
    "print(\"  Read: Can only download\")\n",
    "print(\"  Write: Can upload and modify\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Not Including All Required Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Only uploading model weights\n",
    "# model.save_pretrained(\"./model\")\n",
    "# api.upload_file(\"./model/pytorch_model.bin\", ...)\n",
    "# Missing: config.json, tokenizer files!\n",
    "\n",
    "# CORRECT: Upload entire directory or use push_to_hub\n",
    "# model.push_to_hub(\"model-name\")\n",
    "# tokenizer.push_to_hub(\"model-name\")\n",
    "\n",
    "print(\"Required files for loading a model:\")\n",
    "print(\"  - config.json\")\n",
    "print(\"  - model.safetensors or pytorch_model.bin\")\n",
    "print(\"  - tokenizer_config.json\")\n",
    "print(\"  - vocab.txt or tokenizer.json\")\n",
    "print(\"  - special_tokens_map.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- âœ… How to authenticate with the Hugging Face Hub\n",
    "- âœ… How to create professional model cards\n",
    "- âœ… How to upload models and adapters\n",
    "- âœ… How to manage your model repositories\n",
    "- âœ… Best practices for sharing models\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge: Complete Model Sharing Workflow\n",
    "\n",
    "Complete end-to-end workflow:\n",
    "1. Fine-tune a model for a unique task\n",
    "2. Create a comprehensive model card\n",
    "3. Upload to Hub\n",
    "4. Test inference API\n",
    "5. Share the link!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Model Cards Guide](https://huggingface.co/docs/hub/model-cards)\n",
    "- [Hub Python Library](https://huggingface.co/docs/huggingface_hub)\n",
    "- [Sharing Best Practices](https://huggingface.co/docs/hub/repositories-getting-started)\n",
    "- [Model Card Template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import gc\n",
    "\n",
    "# Clean up local files\n",
    "for path in [\"./model_for_upload\", \"./lora_adapter_for_upload\"]:\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Removed {path}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nCleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Module 2.4 Complete!\n\nCongratulations! You've completed the entire Hugging Face Ecosystem module!\n\n**What you've learned:**\n1. âœ… Lab 2.4.1: Hub Exploration - Finding and evaluating models\n2. âœ… Lab 2.4.2: Pipeline Showcase - Quick inference with pipelines\n3. âœ… Lab 2.4.3: Dataset Processing - Efficient data handling\n4. âœ… Lab 2.4.4: Trainer Fine-tuning - Professional training workflows\n5. âœ… Lab 2.4.5: LoRA Introduction - Parameter-efficient fine-tuning\n6. âœ… Lab 2.4.6: Model Upload - Sharing with the community\n\n**You can now:**\n- Find and use any model from the 500K+ on the Hub\n- Process datasets of any size efficiently\n- Fine-tune models with professional best practices\n- Use LoRA to train large models on limited hardware\n- Share your work with the global ML community\n\n**Next:** Continue your journey with Domain 3: LLM Systems!\n\nGreat job! ðŸŽ‰"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}