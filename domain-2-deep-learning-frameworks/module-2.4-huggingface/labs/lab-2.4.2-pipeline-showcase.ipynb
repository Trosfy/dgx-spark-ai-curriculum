{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.4.2: Pipeline Showcase\n",
    "\n",
    "**Module:** 2.4 - Hugging Face Ecosystem  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐ (Beginner-Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Use the Pipeline API for quick inference without boilerplate code\n",
    "- [ ] Demonstrate 5 different pipeline types: text generation, sentiment, NER, QA, summarization\n",
    "- [ ] Understand when to use pipelines vs. manual model loading\n",
    "- [ ] Customize pipeline behavior with advanced parameters\n",
    "- [ ] Build a multi-task inference demo\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Lab 2.4.1 (Hub Exploration)\n",
    "- Knowledge of: Basic Python, model loading concepts\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "Imagine you're building a customer support chatbot that needs to:\n",
    "1. Understand if the customer is happy or angry (sentiment)\n",
    "2. Extract key entities like product names and dates (NER)\n",
    "3. Answer questions about policies (QA)\n",
    "4. Summarize long complaint emails (summarization)\n",
    "5. Generate helpful responses (text generation)\n",
    "\n",
    "Without pipelines, you'd write hundreds of lines of tokenization, model loading, and post-processing code. With pipelines, each task is literally **one line of code**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: What is a Pipeline?\n",
    "\n",
    "> **Imagine you want a smoothie.** You could:\n",
    "> - Option A: Buy fruits, wash them, peel them, cut them, put them in a blender, blend, pour into glass\n",
    "> - Option B: Walk up to a smoothie bar and say \"One strawberry smoothie please!\"\n",
    ">\n",
    "> **Pipelines are like the smoothie bar.** They handle all the messy preparation work:\n",
    "> - Loading the right model\n",
    "> - Preparing your text (tokenization)\n",
    "> - Running the model\n",
    "> - Converting the output to something useful\n",
    ">\n",
    "> You just say: `pipeline('sentiment-analysis')('I love this!')` → `POSITIVE`\n",
    ">\n",
    "> **In AI terms:** A pipeline is a high-level abstraction that chains together tokenization → model inference → post-processing into a single, easy-to-use function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Pipeline Basics"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Install required packages\n# Note: These packages are pre-installed in the NGC PyTorch container.\n# Running pip install ensures you have compatible versions.\n\n!pip install -q \"transformers>=4.35.0\" \"huggingface_hub>=0.19.0\" \"datasets>=2.14.0\"\n\nprint(\"Packages ready!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import the pipeline function\nfrom transformers import pipeline\nimport torch\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Check hardware\n# IMPORTANT: For pipelines, use integer device index (0=GPU, -1=CPU)\n# This is DIFFERENT from torch.device(\"cuda\") used elsewhere in the curriculum!\n# The pipeline() function expects: device=0 (GPU) or device=-1 (CPU)\ndevice = 0 if torch.cuda.is_available() else -1\nprint(f\"Using device: {'GPU (cuda:0)' if device == 0 else 'CPU'}\")\n\n# Helper to show memory\ndef show_memory():\n    if torch.cuda.is_available():\n        used = torch.cuda.memory_allocated() / 1e9\n        print(f\"GPU Memory: {used:.2f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Simplest Pipeline\n",
    "\n",
    "Creating a pipeline is as simple as specifying the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sentiment analysis pipeline\n",
    "# This automatically downloads the default model for sentiment analysis!\n",
    "sentiment_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    device=device,\n",
    "    torch_dtype=torch.bfloat16  # Use bfloat16 on DGX Spark\n",
    ")\n",
    "\n",
    "print(\"Pipeline created!\")\n",
    "print(f\"Model: {sentiment_pipe.model.config._name_or_path}\")\n",
    "show_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pipeline - it's that simple!\n",
    "result = sentiment_pipe(\"I absolutely love learning about AI on my DGX Spark!\")\n",
    "print(result)\n",
    "\n",
    "# Process multiple texts at once (batched inference)\n",
    "texts = [\n",
    "    \"This is the best course ever!\",\n",
    "    \"I'm struggling with this concept.\",\n",
    "    \"The weather is nice today.\",\n",
    "    \"I can't believe how terrible this experience was.\"\n",
    "]\n",
    "\n",
    "results = sentiment_pipe(texts)\n",
    "print(\"\\nBatch results:\")\n",
    "for text, result in zip(texts, results):\n",
    "    print(f\"  '{text[:40]}...' → {result['label']} ({result['score']:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "With just `pipeline(\"sentiment-analysis\")`, Hugging Face:\n",
    "1. Downloaded the default sentiment model (`distilbert-base-uncased-finetuned-sst-2-english`)\n",
    "2. Created a tokenizer for that model\n",
    "3. Set up the model for inference\n",
    "4. Created a post-processor that converts logits → readable labels\n",
    "\n",
    "All in one line! And when you call it, it handles tokenization, inference, and output formatting automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The Five Essential Pipelines\n",
    "\n",
    "Let's showcase the five pipelines required for this task's deliverable.\n",
    "\n",
    "### Pipeline 1: Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up previous pipeline\n",
    "del sentiment_pipe\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Creating text generation pipeline...\")\n",
    "\n",
    "# Text Generation Pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2\",  # Specify model explicitly\n",
    "    device=device,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "show_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with different parameters\n",
    "prompt = \"Artificial intelligence will transform\"\n",
    "\n",
    "print(\"Text Generation Demo\\n\")\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "\n",
    "# Default generation\n",
    "result = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=50,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "print(\"Default (greedy):\")\n",
    "print(f\"  {result[0]['generated_text']}\\n\")\n",
    "\n",
    "# Creative generation with sampling\n",
    "result = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=50,\n",
    "    num_return_sequences=2,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    top_p=0.95\n",
    ")\n",
    "print(\"Creative (sampling, temp=0.9):\")\n",
    "for i, r in enumerate(result):\n",
    "    print(f\"  {i+1}. {r['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation Parameters Explained\n",
    "\n",
    "| Parameter | Effect | When to Use |\n",
    "|-----------|--------|-------------|\n",
    "| `max_new_tokens` | Limit output length | Always set this! |\n",
    "| `do_sample=True` | Enable random sampling | Creative tasks |\n",
    "| `temperature` | Higher = more random | 0.7-1.0 for creativity |\n",
    "| `top_p` | Nucleus sampling threshold | 0.9-0.95 typical |\n",
    "| `top_k` | Limit vocabulary per step | 50-100 typical |\n",
    "| `num_return_sequences` | Multiple outputs | Compare options |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 2: Sentiment Analysis (already shown above)\n",
    "\n",
    "Let's do a more comprehensive demo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del generator\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Create sentiment pipeline with a more powerful model\n",
    "sentiment = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    device=device,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(\"Loaded Twitter-RoBERTa sentiment model\")\n",
    "show_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on realistic examples\n",
    "customer_feedback = [\n",
    "    \"Your product changed my life! Best purchase ever!\",\n",
    "    \"Waited 3 weeks for delivery. Unacceptable.\",\n",
    "    \"It's okay I guess. Does what it says.\",\n",
    "    \"The customer service was incredibly helpful and patient.\",\n",
    "    \"Don't waste your money on this garbage.\",\n",
    "    \"Shipped fast but packaging was damaged.\",\n",
    "    \"I've recommended this to all my friends!\",\n",
    "    \"Why is this so complicated to set up?\"\n",
    "]\n",
    "\n",
    "print(\"Customer Feedback Sentiment Analysis\\n\")\n",
    "print(f\"{'Feedback':<55} {'Sentiment':<10} {'Score'}\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "results = sentiment(customer_feedback)\n",
    "for text, result in zip(customer_feedback, results):\n",
    "    display_text = text[:52] + \"...\" if len(text) > 55 else text\n",
    "    print(f\"{display_text:<55} {result['label']:<10} {result['score']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 3: Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del sentiment\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Create NER pipeline\n",
    "ner = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"dslim/bert-base-NER\",\n",
    "    device=device,\n",
    "    aggregation_strategy=\"simple\"  # Group tokens into words\n",
    ")\n",
    "\n",
    "print(\"Loaded BERT-NER model\")\n",
    "show_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test NER on various texts\n",
    "test_texts = [\n",
    "    \"Apple CEO Tim Cook announced new products in Cupertino, California.\",\n",
    "    \"The DGX Spark was developed by NVIDIA and announced at CES 2025.\",\n",
    "    \"Dr. Sarah Johnson from MIT published research in Nature last Tuesday.\",\n",
    "    \"Amazon and Microsoft are competing with Google in cloud services.\"\n",
    "]\n",
    "\n",
    "print(\"Named Entity Recognition Demo\\n\")\n",
    "\n",
    "for text in test_texts:\n",
    "    print(f\"Text: {text}\")\n",
    "    entities = ner(text)\n",
    "    \n",
    "    if entities:\n",
    "        print(\"  Entities found:\")\n",
    "        for entity in entities:\n",
    "            print(f\"    - '{entity['word']}' → {entity['entity_group']} (confidence: {entity['score']:.2%})\")\n",
    "    else:\n",
    "        print(\"  No entities found.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER Entity Types\n",
    "\n",
    "| Entity | Meaning | Examples |\n",
    "|--------|---------|----------|\n",
    "| PER | Person | Tim Cook, Sarah Johnson |\n",
    "| ORG | Organization | Apple, NVIDIA, MIT |\n",
    "| LOC | Location | California, Cupertino |\n",
    "| MISC | Miscellaneous | CES 2025, Nature |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 4: Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del ner\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Create QA pipeline\n",
    "qa = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"deepset/roberta-base-squad2\",\n",
    "    device=device,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(\"Loaded RoBERTa QA model (trained on SQuAD 2.0)\")\n",
    "show_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context about DGX Spark\n",
    "context = \"\"\"\n",
    "The NVIDIA DGX Spark is a desktop AI supercomputer designed for researchers, \n",
    "developers, and data scientists. Powered by the NVIDIA Blackwell GB10 Superchip, \n",
    "it features 128GB of unified LPDDR5X memory shared between CPU and GPU. \n",
    "\n",
    "The system delivers up to 1 PFLOP of FP4 compute performance and approximately \n",
    "209 TFLOPS at FP8 precision. It includes 6,144 CUDA cores and 192 fifth-generation \n",
    "Tensor Cores optimized for AI workloads.\n",
    "\n",
    "Unlike cloud-based solutions, the DGX Spark runs locally on your desk, \n",
    "eliminating cloud costs and data privacy concerns. It can run models with \n",
    "up to 70 billion parameters using the unified memory architecture.\n",
    "\n",
    "The device was announced at CES 2025 and runs on the NVIDIADesktop OS, \n",
    "a Linux-based operating system. It supports popular frameworks like \n",
    "PyTorch and TensorFlow through NGC containers.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"How much memory does the DGX Spark have?\",\n",
    "    \"What chip powers the DGX Spark?\",\n",
    "    \"How many CUDA cores are in the system?\",\n",
    "    \"When was the DGX Spark announced?\",\n",
    "    \"What operating system does it run?\",\n",
    "    \"What is the maximum model size it can run?\"\n",
    "]\n",
    "\n",
    "print(\"Question Answering Demo\\n\")\n",
    "print(\"Context: Information about DGX Spark\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for question in questions:\n",
    "    result = qa(question=question, context=context)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {result['answer']} (confidence: {result['score']:.2%})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QA Pipeline Tips\n",
    "\n",
    "1. **Context matters**: The answer must be in the context (extractive QA)\n",
    "2. **Confidence scores**: Low scores often mean the answer isn't in the context\n",
    "3. **Chunk long documents**: Split into smaller contexts for better results\n",
    "4. **SQuAD 2.0 models**: Can indicate \"no answer\" if context doesn't contain it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 5: Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del qa\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Create summarization pipeline\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"facebook/bart-large-cnn\",\n",
    "    device=device,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(\"Loaded BART-CNN summarization model\")\n",
    "show_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long article to summarize\n",
    "article = \"\"\"\n",
    "Artificial intelligence has made remarkable strides in recent years, transforming \n",
    "industries from healthcare to finance. The development of large language models \n",
    "has been particularly significant, with systems like GPT-4 and Claude demonstrating \n",
    "unprecedented capabilities in understanding and generating human-like text.\n",
    "\n",
    "One of the most important developments has been the democratization of AI technology. \n",
    "Platforms like Hugging Face have made it possible for developers worldwide to access \n",
    "and deploy state-of-the-art models without needing massive computational resources \n",
    "or specialized expertise. This has led to an explosion of AI applications across \n",
    "every sector of the economy.\n",
    "\n",
    "However, challenges remain. The environmental impact of training large models is \n",
    "substantial, with some estimates suggesting a single training run can produce as \n",
    "much carbon as five cars over their lifetime. Additionally, concerns about bias, \n",
    "misinformation, and job displacement continue to fuel debates about how AI should \n",
    "be developed and regulated.\n",
    "\n",
    "Looking ahead, experts predict that AI will become increasingly integrated into \n",
    "daily life. From personal assistants that truly understand context to medical \n",
    "diagnostic systems that can detect diseases earlier than human doctors, the \n",
    "potential applications are vast. The key challenge will be ensuring these \n",
    "technologies are developed responsibly and benefit all of humanity.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Summarization Demo\\n\")\n",
    "print(f\"Original article length: {len(article.split())} words\\n\")\n",
    "\n",
    "# Generate different length summaries\n",
    "short_summary = summarizer(\n",
    "    article, \n",
    "    max_length=50, \n",
    "    min_length=20,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "medium_summary = summarizer(\n",
    "    article,\n",
    "    max_length=100,\n",
    "    min_length=50,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "print(\"Short Summary (20-50 tokens):\")\n",
    "print(f\"  {short_summary[0]['summary_text']}\")\n",
    "print(f\"  ({len(short_summary[0]['summary_text'].split())} words)\\n\")\n",
    "\n",
    "print(\"Medium Summary (50-100 tokens):\")\n",
    "print(f\"  {medium_summary[0]['summary_text']}\")\n",
    "print(f\"  ({len(medium_summary[0]['summary_text'].split())} words)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Advanced Pipeline Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying Custom Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use ANY compatible model from the Hub\n",
    "# Clean up first\n",
    "del summarizer\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Use a specific model for sentiment\n",
    "financial_sentiment = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"ProsusAI/finbert\",  # Specialized for financial text!\n",
    "    device=device\n",
    ")\n",
    "\n",
    "financial_texts = [\n",
    "    \"The stock price surged 15% after the earnings report.\",\n",
    "    \"The company filed for bankruptcy following poor quarterly results.\",\n",
    "    \"Revenue remained stable compared to last year.\",\n",
    "    \"Investors are optimistic about the merger announcement.\"\n",
    "]\n",
    "\n",
    "print(\"Financial Sentiment Analysis (using FinBERT)\\n\")\n",
    "results = financial_sentiment(financial_texts)\n",
    "for text, result in zip(financial_texts, results):\n",
    "    print(f\"'{text[:50]}...'\")\n",
    "    print(f\"  → {result['label']} ({result['score']:.2%})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Processing for Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create 100 sample texts\n",
    "sample_texts = [f\"Sample text number {i} for batch processing test.\" for i in range(100)]\n",
    "\n",
    "# Process one at a time (slow)\n",
    "start = time.time()\n",
    "for text in sample_texts[:20]:  # Just first 20 for demo\n",
    "    _ = financial_sentiment(text)\n",
    "sequential_time = time.time() - start\n",
    "\n",
    "# Process in batches (fast!)\n",
    "start = time.time()\n",
    "_ = financial_sentiment(sample_texts[:20], batch_size=8)\n",
    "batch_time = time.time() - start\n",
    "\n",
    "print(\"Batch Processing Performance Comparison\\n\")\n",
    "print(f\"Sequential (one at a time): {sequential_time:.2f}s\")\n",
    "print(f\"Batched (batch_size=8):      {batch_time:.2f}s\")\n",
    "print(f\"Speedup: {sequential_time/batch_time:.1f}x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline on GPU with Specific Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can specify GPU device in several ways\n",
    "print(\"Device specification options:\\n\")\n",
    "\n",
    "# Method 1: device index\n",
    "# pipe = pipeline(\"task\", device=0)  # First GPU\n",
    "\n",
    "# Method 2: device string  \n",
    "# pipe = pipeline(\"task\", device=\"cuda:0\")  # Explicit\n",
    "\n",
    "# Method 3: device_map for large models\n",
    "# pipe = pipeline(\"task\", device_map=\"auto\")  # Auto-distribute\n",
    "\n",
    "# Method 4: Stay on CPU\n",
    "# pipe = pipeline(\"task\", device=-1)  # Force CPU\n",
    "\n",
    "print(\"device=0          → First GPU\")\n",
    "print(\"device='cuda:0'   → Explicit GPU selection\")\n",
    "print(\"device_map='auto' → Auto-distribute (for huge models)\")\n",
    "print(\"device=-1         → Force CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Building a Multi-Task Demo\n",
    "\n",
    "Let's build a unified demo that showcases all five pipelines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del financial_sentiment\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class MultiTaskNLP:\n",
    "    \"\"\"\n",
    "    A multi-task NLP system using Hugging Face pipelines.\n",
    "    Demonstrates all five required pipeline types.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, device=0):\n",
    "        self.device = device\n",
    "        self.pipelines = {}\n",
    "        print(\"Initializing Multi-Task NLP System...\")\n",
    "    \n",
    "    def load_pipeline(self, task, model=None):\n",
    "        \"\"\"Load a specific pipeline on demand.\"\"\"\n",
    "        if task in self.pipelines:\n",
    "            return self.pipelines[task]\n",
    "        \n",
    "        print(f\"  Loading {task} pipeline...\")\n",
    "        \n",
    "        task_configs = {\n",
    "            \"sentiment\": (\"sentiment-analysis\", \"distilbert-base-uncased-finetuned-sst-2-english\"),\n",
    "            \"ner\": (\"ner\", \"dslim/bert-base-NER\"),\n",
    "            \"qa\": (\"question-answering\", \"distilbert-base-cased-distilled-squad\"),\n",
    "            \"summarization\": (\"summarization\", \"sshleifer/distilbart-cnn-12-6\"),\n",
    "            \"generation\": (\"text-generation\", \"distilgpt2\")\n",
    "        }\n",
    "        \n",
    "        task_name, default_model = task_configs.get(task, (task, None))\n",
    "        model = model or default_model\n",
    "        \n",
    "        kwargs = {\"device\": self.device}\n",
    "        if task == \"ner\":\n",
    "            kwargs[\"aggregation_strategy\"] = \"simple\"\n",
    "        \n",
    "        self.pipelines[task] = pipeline(task_name, model=model, **kwargs)\n",
    "        return self.pipelines[task]\n",
    "    \n",
    "    def analyze_text(self, text, context=None):\n",
    "        \"\"\"Run all analyses on a piece of text.\"\"\"\n",
    "        results = {\"original_text\": text}\n",
    "        \n",
    "        # Sentiment\n",
    "        pipe = self.load_pipeline(\"sentiment\")\n",
    "        sentiment_result = pipe(text)[0]\n",
    "        results[\"sentiment\"] = {\n",
    "            \"label\": sentiment_result[\"label\"],\n",
    "            \"confidence\": sentiment_result[\"score\"]\n",
    "        }\n",
    "        \n",
    "        # NER\n",
    "        pipe = self.load_pipeline(\"ner\")\n",
    "        ner_result = pipe(text)\n",
    "        results[\"entities\"] = [\n",
    "            {\"text\": e[\"word\"], \"type\": e[\"entity_group\"], \"confidence\": e[\"score\"]}\n",
    "            for e in ner_result\n",
    "        ]\n",
    "        \n",
    "        # QA (if context provided)\n",
    "        if context:\n",
    "            pipe = self.load_pipeline(\"qa\")\n",
    "            qa_result = pipe(question=text, context=context)\n",
    "            results[\"qa_answer\"] = {\n",
    "                \"answer\": qa_result[\"answer\"],\n",
    "                \"confidence\": qa_result[\"score\"]\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def summarize(self, text, max_length=100):\n",
    "        \"\"\"Summarize a piece of text.\"\"\"\n",
    "        pipe = self.load_pipeline(\"summarization\")\n",
    "        result = pipe(text, max_length=max_length, min_length=30)\n",
    "        return result[0][\"summary_text\"]\n",
    "    \n",
    "    def generate(self, prompt, max_tokens=50):\n",
    "        \"\"\"Generate text continuation.\"\"\"\n",
    "        pipe = self.load_pipeline(\"generation\")\n",
    "        result = pipe(\n",
    "            prompt, \n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return result[0][\"generated_text\"]\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Free GPU memory.\"\"\"\n",
    "        self.pipelines.clear()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Cleaned up pipelines.\")\n",
    "\n",
    "\n",
    "# Create instance\n",
    "nlp = MultiTaskNLP(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo the multi-task system\n",
    "print(\"=\" * 70)\n",
    "print(\"MULTI-TASK NLP DEMO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test text\n",
    "test_text = \"Apple CEO Tim Cook announced that the company will invest $1 billion in AI research.\"\n",
    "\n",
    "print(f\"\\nInput: '{test_text}'\\n\")\n",
    "\n",
    "# Analyze\n",
    "analysis = nlp.analyze_text(test_text)\n",
    "\n",
    "print(\"--- Sentiment Analysis ---\")\n",
    "print(f\"  {analysis['sentiment']['label']} (confidence: {analysis['sentiment']['confidence']:.2%})\")\n",
    "\n",
    "print(\"\\n--- Named Entities ---\")\n",
    "for entity in analysis['entities']:\n",
    "    print(f\"  {entity['text']}: {entity['type']} ({entity['confidence']:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test summarization\n",
    "long_text = \"\"\"\n",
    "The field of natural language processing has seen tremendous advances in recent years. \n",
    "Large language models, trained on vast amounts of text data, can now perform a wide \n",
    "range of tasks from translation to creative writing. Companies like OpenAI, Google, \n",
    "and Anthropic have developed increasingly capable systems. However, these advances \n",
    "come with challenges including computational costs, potential biases, and concerns \n",
    "about misuse. Researchers are working on making these models more efficient and safer.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- Summarization ---\")\n",
    "print(f\"Original ({len(long_text.split())} words):\")\n",
    "print(f\"  {long_text[:150]}...\\n\")\n",
    "\n",
    "summary = nlp.summarize(long_text)\n",
    "print(f\"Summary ({len(summary.split())} words):\")\n",
    "print(f\"  {summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation\n",
    "print(\"\\n--- Text Generation ---\")\n",
    "prompt = \"The future of AI is\"\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "\n",
    "generated = nlp.generate(prompt, max_tokens=40)\n",
    "print(f\"Generated: {generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test QA\n",
    "print(\"\\n--- Question Answering ---\")\n",
    "context = \"\"\"The DGX Spark is NVIDIA's desktop AI supercomputer. It has 128GB of \n",
    "unified memory and is powered by the Blackwell GB10 chip. It was announced at CES 2025.\"\"\"\n",
    "\n",
    "question = \"How much memory does the DGX Spark have?\"\n",
    "print(f\"Context: {context[:80]}...\")\n",
    "print(f\"Question: {question}\")\n",
    "\n",
    "analysis = nlp.analyze_text(question, context=context)\n",
    "print(f\"Answer: {analysis['qa_answer']['answer']} (confidence: {analysis['qa_answer']['confidence']:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "nlp.cleanup()\n",
    "show_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Customer Support Bot\n",
    "\n",
    "Build a simple customer support analyzer that:\n",
    "1. Detects if the customer is angry (sentiment)\n",
    "2. Extracts product names mentioned (NER)\n",
    "3. Generates a helpful response (generation)\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "```python\n",
    "def analyze_support_ticket(ticket_text):\n",
    "    # 1. Check sentiment\n",
    "    sentiment_pipe = pipeline(\"sentiment-analysis\", device=device)\n",
    "    sentiment = sentiment_pipe(ticket_text)[0]\n",
    "    \n",
    "    # 2. Extract entities\n",
    "    ner_pipe = pipeline(\"ner\", aggregation_strategy=\"simple\", device=device)\n",
    "    entities = ner_pipe(ticket_text)\n",
    "    \n",
    "    # 3. Generate response based on sentiment\n",
    "    gen_pipe = pipeline(\"text-generation\", device=device)\n",
    "    if sentiment['label'] == 'NEGATIVE':\n",
    "        prompt = \"Dear valued customer, we apologize for\"\n",
    "    else:\n",
    "        prompt = \"Dear valued customer, thank you for\"\n",
    "    \n",
    "    response = gen_pipe(prompt, max_new_tokens=50)[0]['generated_text']\n",
    "    \n",
    "    return sentiment, entities, response\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Build your customer support analyzer!\n",
    "\n",
    "def analyze_support_ticket(ticket_text):\n",
    "    \"\"\"Analyze a customer support ticket.\"\"\"\n",
    "    # TODO: Implement the analyzer\n",
    "    pass\n",
    "\n",
    "# Test with sample tickets\n",
    "sample_tickets = [\n",
    "    \"I've been waiting 3 weeks for my MacBook to arrive! This is unacceptable!\",\n",
    "    \"Just wanted to say your customer service team was amazing. Thanks!\",\n",
    "    \"The iPhone screen is cracked and it's only been a week since I bought it.\"\n",
    "]\n",
    "\n",
    "# for ticket in sample_tickets:\n",
    "#     result = analyze_support_ticket(ticket)\n",
    "#     print(f\"Ticket: {ticket}\")\n",
    "#     print(f\"Analysis: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Specifying Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Let pipeline decide (might use CPU on some systems)\n",
    "# pipe = pipeline(\"sentiment-analysis\")  # May run on CPU!\n",
    "\n",
    "# CORRECT: Explicitly specify device\n",
    "# pipe = pipeline(\"sentiment-analysis\", device=0)  # Force GPU\n",
    "\n",
    "print(\"Always specify device for predictable performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Wrong Pipeline for Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Using text-classification pipeline for NER\n",
    "# The model might load but give wrong outputs!\n",
    "\n",
    "# CORRECT: Match pipeline type to model type\n",
    "print(\"Pipeline-Task Matching:\\n\")\n",
    "print(\"sentiment-analysis  → Text classification models\")\n",
    "print(\"ner                 → Token classification models\")\n",
    "print(\"question-answering  → QA models (extractive)\")\n",
    "print(\"summarization       → Seq2Seq models (BART, T5)\")\n",
    "print(\"text-generation     → Causal LM models (GPT-2, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Not Cleaning Up Between Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Creating many pipelines without cleanup\n",
    "# p1 = pipeline(\"sentiment-analysis\")\n",
    "# p2 = pipeline(\"ner\")\n",
    "# p3 = pipeline(\"summarization\")\n",
    "# All three are in memory!\n",
    "\n",
    "# CORRECT: Delete and clear cache between pipelines\n",
    "# p1 = pipeline(\"sentiment-analysis\")\n",
    "# ... use p1 ...\n",
    "# del p1\n",
    "# torch.cuda.empty_cache()\n",
    "# p2 = pipeline(\"ner\")\n",
    "\n",
    "print(\"Memory tip: delete pipelines and call torch.cuda.empty_cache()!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ✅ How to create pipelines for instant NLP capabilities\n",
    "- ✅ How to use 5 different pipeline types (generation, sentiment, NER, QA, summarization)\n",
    "- ✅ How to customize pipeline behavior with parameters\n",
    "- ✅ How to build multi-task NLP systems\n",
    "- ✅ Best practices for device management and memory\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge: Build a Content Moderation System\n",
    "\n",
    "Create a content moderation pipeline that:\n",
    "1. Detects toxic language (sentiment or specialized toxicity model)\n",
    "2. Extracts mentioned entities (NER)\n",
    "3. Summarizes the content for human review\n",
    "4. Generates a moderation decision explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE\n",
    "# Hint: Look for toxicity detection models on the Hub!\n",
    "# e.g., \"unitary/toxic-bert\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Pipeline Documentation](https://huggingface.co/docs/transformers/main_classes/pipelines)\n",
    "- [Available Pipeline Tasks](https://huggingface.co/docs/transformers/task_summary)\n",
    "- [Custom Pipelines](https://huggingface.co/docs/transformers/add_new_pipeline)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup\n",
    "import gc\n",
    "\n",
    "# Clear any remaining objects\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"Cleanup complete!\")\n",
    "show_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, **03-dataset-processing.ipynb**, we'll learn how to load and process datasets efficiently using the Hugging Face `datasets` library - essential for fine-tuning models!\n",
    "\n",
    "Great job completing Lab 2.4.2! You now have a powerful toolkit of NLP pipelines at your fingertips!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}