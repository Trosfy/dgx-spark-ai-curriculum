{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.4.1: Hugging Face Hub Exploration\n",
    "\n",
    "**Module:** 2.4 - Hugging Face Ecosystem  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐ (Beginner-Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Navigate the Hugging Face Hub to discover models and datasets\n",
    "- [ ] Understand model cards and how to evaluate model suitability\n",
    "- [ ] Load pre-trained models using Auto classes\n",
    "- [ ] Test models locally on your DGX Spark\n",
    "- [ ] Document and compare different models for a task\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Module 8 (NLP & Transformers basics)\n",
    "- Knowledge of: Basic Python, PyTorch tensors\n",
    "- Setup: NGC container running, Hugging Face account created\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "Imagine you're a chef who just walked into the world's largest ingredient warehouse. You could spend months exploring every aisle, or you could learn to quickly find exactly what you need for your recipe.\n",
    "\n",
    "The Hugging Face Hub is exactly that warehouse for AI - over 500,000 models and 100,000 datasets! Companies like Google, Meta, Microsoft, and thousands of researchers share their work there. Knowing how to navigate it efficiently is a superpower.\n",
    "\n",
    "**Real examples:**\n",
    "- A startup needs a sentiment analysis model → finds `cardiffnlp/twitter-roberta-base-sentiment`\n",
    "- A researcher needs multilingual embeddings → discovers `sentence-transformers/paraphrase-multilingual-mpnet-base-v2`\n",
    "- A developer needs fast text generation → chooses between `gpt2`, `distilgpt2`, and `microsoft/phi-2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: What is the Hugging Face Hub?\n",
    "\n",
    "> **Imagine you collect trading cards**, but instead of sports players, each card is a trained AI brain.\n",
    ">\n",
    "> Each card tells you:\n",
    "> - What the AI is good at (playing chess, translating languages, detecting spam)\n",
    "> - How big it is (small cards fit in your pocket, big ones need a backpack)\n",
    "> - Who made it (Google's cards are usually really good!)\n",
    "> - How to use it (simple instructions on the back)\n",
    ">\n",
    "> The Hugging Face Hub is like a giant trading card shop where:\n",
    "> - Most cards are **FREE** to take home\n",
    "> - You can try cards before taking them (online demos)\n",
    "> - You can share your own cards\n",
    "> - There are reviews telling you which cards work best\n",
    ">\n",
    "> **In AI terms:** The Hub hosts pre-trained models (the \"brains\") that anyone can download and use. Instead of training models from scratch (which costs millions), you grab a pre-trained one and fine-tune it for your specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setting Up Your Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n# Note: These packages are pre-installed in the NGC PyTorch container.\n# Running pip install ensures you have compatible versions.\n# If NOT using NGC container, ensure you have ARM64-compatible packages for DGX Spark.\n\n!pip install -q \"transformers>=4.35.0\" \"huggingface_hub>=0.19.0\" \"datasets>=2.14.0\" \"accelerate>=0.24.0\"\n\nprint(\"Packages ready!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import the libraries we'll use throughout this notebook\nimport torch\nfrom transformers import (\n    AutoModel, \n    AutoTokenizer, \n    AutoModelForSequenceClassification,\n    AutoModelForCausalLM,\n    AutoModelForQuestionAnswering\n)\nfrom huggingface_hub import HfApi\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Check our hardware\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### What Just Happened?\n\nWe imported the core building blocks:\n- **`AutoModel`** and friends: Smart loaders that automatically pick the right model class\n- **`HfApi`**: Python interface to search and interact with the Hub (use `api.list_models()` to search for models programmatically)\n\nOn your DGX Spark, you should see the Blackwell GPU with ~128GB of unified memory. This is your secret weapon!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Exploring the Hub Programmatically\n",
    "\n",
    "While you can browse [huggingface.co](https://huggingface.co) in a web browser, real power users search programmatically. Let's learn how!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Hugging Face API\n",
    "api = HfApi()\n",
    "\n",
    "# Search for sentiment analysis models\n",
    "# This is like searching \"sentiment analysis\" on the website\n",
    "models = list(api.list_models(\n",
    "    filter=\"text-classification\",\n",
    "    sort=\"downloads\",\n",
    "    direction=-1,  # Descending (most downloads first)\n",
    "    limit=10\n",
    "))\n",
    "\n",
    "print(\"Top 10 Text Classification Models by Downloads:\\n\")\n",
    "print(f\"{'Rank':<5} {'Model Name':<50} {'Downloads':<15}\")\n",
    "print(\"=\" * 70)\n",
    "for i, model in enumerate(models, 1):\n",
    "    downloads = model.downloads if hasattr(model, 'downloads') else 'N/A'\n",
    "    print(f\"{i:<5} {model.id:<50} {downloads:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Results\n",
    "\n",
    "Notice how the most downloaded models often come from:\n",
    "- **Research labs**: `cardiffnlp/`, `facebook/`, `google/`\n",
    "- **Companies**: `microsoft/`, `distilbert-base-uncased`\n",
    "- **Community**: Individual researchers sharing their work\n",
    "\n",
    "Downloads indicate popularity, but not necessarily quality for YOUR task. Always test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's search for different types of models\n",
    "# This shows the variety available on the Hub\n",
    "\n",
    "task_types = [\n",
    "    \"text-generation\",\n",
    "    \"text-classification\",\n",
    "    \"question-answering\",\n",
    "    \"summarization\",\n",
    "    \"translation\",\n",
    "    \"fill-mask\",\n",
    "    \"token-classification\",  # NER\n",
    "    \"image-classification\",\n",
    "    \"object-detection\",\n",
    "    \"automatic-speech-recognition\"\n",
    "]\n",
    "\n",
    "print(\"Model counts by task type:\\n\")\n",
    "print(f\"{'Task':<35} {'Top Model':<40}\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "for task in task_types:\n",
    "    try:\n",
    "        top_models = list(api.list_models(filter=task, sort=\"downloads\", direction=-1, limit=1))\n",
    "        top_model = top_models[0].id if top_models else \"None found\"\n",
    "        print(f\"{task:<35} {top_model:<40}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{task:<35} Error: {str(e)[:30]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Reading Model Cards\n",
    "\n",
    "Model cards are like nutrition labels for AI. They tell you:\n",
    "- What the model does\n",
    "- How it was trained\n",
    "- Known limitations and biases\n",
    "- How to use it\n",
    "\n",
    "**Good model cards are a sign of a responsible model creator!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine a popular model's metadata\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# Get detailed model information\n",
    "info = api.model_info(model_name)\n",
    "\n",
    "print(f\"Model: {info.id}\")\n",
    "print(f\"\\nAuthor: {info.author}\")\n",
    "print(f\"Downloads (last month): {info.downloads:,}\")\n",
    "print(f\"Likes: {info.likes}\")\n",
    "print(f\"\\nTags: {info.tags}\")\n",
    "print(f\"\\nPipeline tag: {info.pipeline_tag}\")\n",
    "print(f\"\\nLibrary: {info.library_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model card content (README)\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # Download just the README\n",
    "    readme_path = hf_hub_download(\n",
    "        repo_id=model_name,\n",
    "        filename=\"README.md\"\n",
    "    )\n",
    "    \n",
    "    with open(readme_path, 'r') as f:\n",
    "        readme_content = f.read()\n",
    "    \n",
    "    # Show first 2000 characters\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MODEL CARD PREVIEW\")\n",
    "    print(\"=\" * 60)\n",
    "    print(readme_content[:2000])\n",
    "    print(\"\\n... [truncated] ...\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not fetch README: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to Look for in a Model Card\n",
    "\n",
    "**Essential sections:**\n",
    "1. **Model Description**: What does it do?\n",
    "2. **Intended Use**: What's it designed for?\n",
    "3. **Training Data**: What was it trained on? (affects biases!)\n",
    "4. **Limitations**: What does it NOT do well?\n",
    "5. **How to Use**: Code examples\n",
    "6. **Evaluation Results**: Benchmark scores\n",
    "\n",
    "**Red flags:**\n",
    "- No model card at all\n",
    "- No information about training data\n",
    "- No mention of limitations\n",
    "- Very few downloads + no documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Loading and Testing Models\n",
    "\n",
    "Now for the fun part - let's actually load and run some models!\n",
    "\n",
    "### The Auto Classes Magic\n",
    "\n",
    "Hugging Face provides \"Auto\" classes that automatically figure out:\n",
    "- Which architecture to use (BERT, GPT, T5, etc.)\n",
    "- Which weights to load\n",
    "- How to configure the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to monitor memory\n",
    "def print_memory_usage(label=\"\"):\n",
    "    \"\"\"Print current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        print(f\"[{label}] GPU Memory - Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "\n",
    "print_memory_usage(\"Before loading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Sentiment Analysis (Text Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sentiment analysis model\n",
    "sentiment_model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "print(f\"Loading {sentiment_model_name}...\")\n",
    "\n",
    "# AutoTokenizer knows how to load the right tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name)\n",
    "\n",
    "# AutoModelForSequenceClassification knows this is a classification model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    sentiment_model_name,\n",
    "    torch_dtype=torch.bfloat16  # Use bfloat16 on DGX Spark!\n",
    ")\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"Model loaded on {device}\")\n",
    "print_memory_usage(\"After loading sentiment model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the sentiment model\n",
    "test_texts = [\n",
    "    \"I absolutely loved this movie! The acting was superb.\",\n",
    "    \"This was a complete waste of time. Terrible.\",\n",
    "    \"It was okay, nothing special but not bad either.\",\n",
    "    \"The DGX Spark is an amazing piece of hardware!\",\n",
    "    \"I'm not sure how I feel about this product.\"\n",
    "]\n",
    "\n",
    "print(\"Sentiment Analysis Results:\\n\")\n",
    "print(f\"{'Text':<55} {'Sentiment':<10} {'Confidence':<10}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for text in test_texts:\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get prediction\n",
    "    probs = torch.softmax(outputs.logits, dim=1)\n",
    "    prediction = torch.argmax(probs, dim=1).item()\n",
    "    confidence = probs[0][prediction].item()\n",
    "    \n",
    "    sentiment = \"POSITIVE\" if prediction == 1 else \"NEGATIVE\"\n",
    "    \n",
    "    # Truncate text for display\n",
    "    display_text = text[:52] + \"...\" if len(text) > 55 else text\n",
    "    print(f\"{display_text:<55} {sentiment:<10} {confidence:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "1. **Tokenization**: Text → numbers (tokens) that the model understands\n",
    "2. **Forward Pass**: Model processes tokens through layers\n",
    "3. **Logits**: Raw scores for each class (positive/negative)\n",
    "4. **Softmax**: Convert logits to probabilities (sum to 1)\n",
    "5. **Argmax**: Pick the class with highest probability\n",
    "\n",
    "Notice how the model is confident about clear sentiments but less certain about neutral or ambiguous text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear previous model to free memory\n",
    "del model, tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print_memory_usage(\"After cleanup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a question-answering model\n",
    "qa_model_name = \"distilbert-base-cased-distilled-squad\"\n",
    "\n",
    "print(f\"Loading {qa_model_name}...\")\n",
    "\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    qa_model_name,\n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device)\n",
    "qa_model.eval()\n",
    "\n",
    "print(f\"Model loaded!\")\n",
    "print_memory_usage(\"After loading QA model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question answering\n",
    "context = \"\"\"\n",
    "The NVIDIA DGX Spark is a revolutionary desktop AI computer powered by the \n",
    "Blackwell GB10 Superchip. It features 128GB of unified LPDDR5X memory shared \n",
    "between the CPU and GPU, eliminating the need for data transfers. The system \n",
    "delivers up to 1 PFLOP of FP4 compute and approximately 209 TFLOPS at FP8 \n",
    "precision. With 6,144 CUDA cores and 192 fifth-generation Tensor Cores, \n",
    "the DGX Spark can run 70B parameter models locally without cloud dependencies.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"How much memory does the DGX Spark have?\",\n",
    "    \"What chip powers the DGX Spark?\",\n",
    "    \"How many CUDA cores does it have?\",\n",
    "    \"What size models can it run?\"\n",
    "]\n",
    "\n",
    "print(\"Question Answering Results:\\n\")\n",
    "print(\"Context:\", context[:100], \"...\\n\")\n",
    "\n",
    "for question in questions:\n",
    "    # Tokenize question and context together\n",
    "    inputs = qa_tokenizer(\n",
    "        question, \n",
    "        context, \n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get answer\n",
    "    with torch.no_grad():\n",
    "        outputs = qa_model(**inputs)\n",
    "    \n",
    "    # Find start and end positions of answer\n",
    "    answer_start = torch.argmax(outputs.start_logits)\n",
    "    answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "    \n",
    "    # Decode the answer\n",
    "    answer_tokens = inputs[\"input_ids\"][0][answer_start:answer_end]\n",
    "    answer = qa_tokenizer.decode(answer_tokens)\n",
    "    \n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Text Generation (The Fun One!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear previous model\n",
    "del qa_model, qa_tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print_memory_usage(\"After cleanup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a text generation model\n",
    "# GPT-2 is a classic, reliable choice\n",
    "gen_model_name = \"gpt2\"\n",
    "\n",
    "print(f\"Loading {gen_model_name}...\")\n",
    "\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_name)\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(\n",
    "    gen_model_name,\n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device)\n",
    "gen_model.eval()\n",
    "\n",
    "# GPT-2 doesn't have a padding token by default\n",
    "gen_tokenizer.pad_token = gen_tokenizer.eos_token\n",
    "\n",
    "print(f\"Model loaded!\")\n",
    "print_memory_usage(\"After loading GPT-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some text!\n",
    "prompts = [\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"Once upon a time in a land of GPUs,\",\n",
    "    \"The best way to learn programming is\"\n",
    "]\n",
    "\n",
    "print(\"Text Generation Results:\\n\")\n",
    "\n",
    "for prompt in prompts:\n",
    "    inputs = gen_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = gen_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=gen_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = gen_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {generated_text}\")\n",
    "    print(\"-\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Parameters Explained\n",
    "\n",
    "| Parameter | What it does | Analogy |\n",
    "|-----------|--------------|----------|\n",
    "| `max_new_tokens` | Maximum words to generate | \"Write at most 50 words\" |\n",
    "| `temperature` | Randomness (0=deterministic, 1+=creative) | \"How wild can you get?\" |\n",
    "| `do_sample` | Whether to sample or pick best | \"Roll dice vs. pick favorite\" |\n",
    "| `top_k` | Only consider top K options | \"Choose from top 50 words\" |\n",
    "| `top_p` | Nucleus sampling threshold | \"Consider words until 90% probability\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Documenting Models\n",
    "\n",
    "### Exercise: Create Your Own Model Documentation\n",
    "\n",
    "For this task, you need to document **10 models** from the Hub. Here's a template and example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Documentation Template\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def document_model(model_id: str) -> dict:\n",
    "    \"\"\"Create documentation for a Hugging Face model.\"\"\"\n",
    "    try:\n",
    "        info = api.model_info(model_id)\n",
    "        \n",
    "        doc = {\n",
    "            \"model_id\": model_id,\n",
    "            \"author\": info.author,\n",
    "            \"task\": info.pipeline_tag,\n",
    "            \"downloads\": info.downloads,\n",
    "            \"likes\": info.likes,\n",
    "            \"library\": info.library_name,\n",
    "            \"tags\": info.tags[:10] if info.tags else [],  # First 10 tags\n",
    "            \"documented_at\": datetime.now().isoformat(),\n",
    "            \"notes\": \"\",  # Add your own notes!\n",
    "            \"tested_locally\": False,\n",
    "            \"local_test_results\": \"\"\n",
    "        }\n",
    "        return doc\n",
    "    except Exception as e:\n",
    "        return {\"model_id\": model_id, \"error\": str(e)}\n",
    "\n",
    "# Example: Document a model\n",
    "example_doc = document_model(\"bert-base-uncased\")\n",
    "print(json.dumps(example_doc, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try It Yourself: Document 10 Models\n",
    "\n",
    "Find and document 10 models across different tasks:\n",
    "- 2 text classification models\n",
    "- 2 text generation models  \n",
    "- 2 question answering models\n",
    "- 2 named entity recognition models\n",
    "- 2 of your choice!\n",
    "\n",
    "<details>\n",
    "<summary>Hint: How to find models</summary>\n",
    "\n",
    "```python\n",
    "# Search for specific tasks\n",
    "text_gen_models = list(api.list_models(filter=\"text-generation\", limit=5))\n",
    "ner_models = list(api.list_models(filter=\"token-classification\", limit=5))\n",
    "\n",
    "# Or search by keyword\n",
    "sentiment_models = list(api.list_models(search=\"sentiment\", limit=5))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Document 10 models\n",
    "\n",
    "my_model_docs = []\n",
    "\n",
    "# Example models to get you started (replace with your own discoveries!)\n",
    "models_to_document = [\n",
    "    # Text Classification\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    # TODO: Add 1 more text classification model\n",
    "    \n",
    "    # Text Generation  \n",
    "    \"gpt2\",\n",
    "    # TODO: Add 1 more text generation model\n",
    "    \n",
    "    # Question Answering\n",
    "    \"distilbert-base-cased-distilled-squad\",\n",
    "    # TODO: Add 1 more QA model\n",
    "    \n",
    "    # Named Entity Recognition\n",
    "    # TODO: Add 2 NER models\n",
    "    \n",
    "    # Your choice!\n",
    "    # TODO: Add 2 models of any type\n",
    "]\n",
    "\n",
    "for model_id in models_to_document:\n",
    "    doc = document_model(model_id)\n",
    "    my_model_docs.append(doc)\n",
    "    print(f\"Documented: {model_id}\")\n",
    "\n",
    "print(f\"\\nTotal models documented: {len(my_model_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Testing Models Locally\n",
    "\n",
    "For the deliverable, you need to **test 3 models locally**. Let's create a testing framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def test_model_locally(model_id: str, task: str, test_input: str) -> dict:\n",
    "    \"\"\"\n",
    "    Test a model locally and return results.\n",
    "    \n",
    "    Args:\n",
    "        model_id: HuggingFace model identifier\n",
    "        task: Type of task (classification, generation, qa)\n",
    "        test_input: Sample input to test with\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with test results\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"model_id\": model_id,\n",
    "        \"task\": task,\n",
    "        \"test_input\": test_input,\n",
    "        \"success\": False,\n",
    "        \"output\": None,\n",
    "        \"load_time_seconds\": 0,\n",
    "        \"inference_time_ms\": 0,\n",
    "        \"memory_used_gb\": 0,\n",
    "        \"error\": None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Clear memory first\n",
    "        torch.cuda.empty_cache()\n",
    "        initial_memory = torch.cuda.memory_allocated() / 1e9\n",
    "        \n",
    "        # Time model loading\n",
    "        start_load = time.time()\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        \n",
    "        # Load appropriate model class based on task\n",
    "        if task == \"classification\":\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                model_id, torch_dtype=torch.bfloat16\n",
    "            ).to(device).eval()\n",
    "        elif task == \"generation\":\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id, torch_dtype=torch.bfloat16\n",
    "            ).to(device).eval()\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "        elif task == \"qa\":\n",
    "            model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "                model_id, torch_dtype=torch.bfloat16\n",
    "            ).to(device).eval()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown task: {task}\")\n",
    "        \n",
    "        results[\"load_time_seconds\"] = time.time() - start_load\n",
    "        results[\"memory_used_gb\"] = (torch.cuda.memory_allocated() / 1e9) - initial_memory\n",
    "        \n",
    "        # Time inference\n",
    "        start_inference = time.time()\n",
    "        \n",
    "        inputs = tokenizer(test_input, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if task == \"classification\":\n",
    "                outputs = model(**inputs)\n",
    "                probs = torch.softmax(outputs.logits, dim=1)\n",
    "                pred = torch.argmax(probs, dim=1).item()\n",
    "                conf = probs[0][pred].item()\n",
    "                results[\"output\"] = f\"Class {pred} (confidence: {conf:.2%})\"\n",
    "            elif task == \"generation\":\n",
    "                outputs = model.generate(\n",
    "                    **inputs, max_new_tokens=30, \n",
    "                    do_sample=True, temperature=0.7,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "                results[\"output\"] = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            elif task == \"qa\":\n",
    "                # For QA, test_input should be \"question ||| context\"\n",
    "                if \"|||\" in test_input:\n",
    "                    q, c = test_input.split(\"|||\")\n",
    "                    inputs = tokenizer(q.strip(), c.strip(), return_tensors=\"pt\", truncation=True)\n",
    "                    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                outputs = model(**inputs)\n",
    "                start_idx = torch.argmax(outputs.start_logits)\n",
    "                end_idx = torch.argmax(outputs.end_logits) + 1\n",
    "                answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
    "                results[\"output\"] = answer\n",
    "        \n",
    "        results[\"inference_time_ms\"] = (time.time() - start_inference) * 1000\n",
    "        results[\"success\"] = True\n",
    "        \n",
    "        # Cleanup\n",
    "        del model, tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        results[\"error\"] = str(e)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3 models locally\n",
    "tests = [\n",
    "    {\n",
    "        \"model_id\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        \"task\": \"classification\",\n",
    "        \"test_input\": \"This product exceeded all my expectations!\"\n",
    "    },\n",
    "    {\n",
    "        \"model_id\": \"gpt2\",\n",
    "        \"task\": \"generation\",\n",
    "        \"test_input\": \"Machine learning is\"\n",
    "    },\n",
    "    {\n",
    "        \"model_id\": \"distilbert-base-cased-distilled-squad\",\n",
    "        \"task\": \"qa\",\n",
    "        \"test_input\": \"What is the capital of France? ||| France is a country in Europe. Its capital is Paris, a beautiful city known for the Eiffel Tower.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Local Model Testing Results\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_test_results = []\n",
    "for test in tests:\n",
    "    print(f\"\\nTesting: {test['model_id']}\")\n",
    "    print(f\"Task: {test['task']}\")\n",
    "    \n",
    "    result = test_model_locally(**test)\n",
    "    all_test_results.append(result)\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        print(f\"Status: SUCCESS\")\n",
    "        print(f\"Load time: {result['load_time_seconds']:.2f}s\")\n",
    "        print(f\"Inference time: {result['inference_time_ms']:.2f}ms\")\n",
    "        print(f\"Memory used: {result['memory_used_gb']:.2f} GB\")\n",
    "        print(f\"Output: {result['output'][:100]}...\" if len(str(result['output'])) > 100 else f\"Output: {result['output']}\")\n",
    "    else:\n",
    "        print(f\"Status: FAILED\")\n",
    "        print(f\"Error: {result['error']}\")\n",
    "    \n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Using the Right Auto Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Using AutoModel for sequence classification\n",
    "# This loads the base model without the classification head!\n",
    "\n",
    "# model = AutoModel.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "# This won't have the classification layers!\n",
    "\n",
    "# CORRECT: Use task-specific Auto class\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "# )\n",
    "\n",
    "print(\"Auto class cheat sheet:\")\n",
    "print(\"- AutoModelForSequenceClassification → sentiment, topic classification\")\n",
    "print(\"- AutoModelForTokenClassification → NER, POS tagging\")\n",
    "print(\"- AutoModelForQuestionAnswering → extractive QA\")\n",
    "print(\"- AutoModelForCausalLM → text generation (GPT-style)\")\n",
    "print(\"- AutoModelForSeq2SeqLM → translation, summarization (T5-style)\")\n",
    "print(\"- AutoModelForMaskedLM → fill-in-the-blank (BERT-style)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Forgetting to Set Evaluation Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Forgetting model.eval()\n",
    "# Dropout and batch norm behave differently in training vs eval!\n",
    "\n",
    "# model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "# output = model(**inputs)  # Still in training mode!\n",
    "\n",
    "# CORRECT:\n",
    "# model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "# model.eval()  # Set to evaluation mode\n",
    "# with torch.no_grad():  # Also disable gradient computation\n",
    "#     output = model(**inputs)\n",
    "\n",
    "print(\"Always use model.eval() and torch.no_grad() for inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Not Moving Inputs to the Same Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Model on GPU, inputs on CPU\n",
    "# model = model.to(\"cuda\")\n",
    "# inputs = tokenizer(\"Hello\", return_tensors=\"pt\")  # On CPU!\n",
    "# output = model(**inputs)  # ERROR: tensors on different devices\n",
    "\n",
    "# CORRECT:\n",
    "# model = model.to(\"cuda\")\n",
    "# inputs = tokenizer(\"Hello\", return_tensors=\"pt\")\n",
    "# inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}  # Move to GPU!\n",
    "# output = model(**inputs)\n",
    "\n",
    "print(\"Remember: model.to(device) AND inputs.to(device)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 4: Using Wrong dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On DGX Spark, bfloat16 is optimal\n",
    "# float16 can cause issues with some operations\n",
    "# float32 uses 2x memory unnecessarily\n",
    "\n",
    "print(\"dtype recommendations for DGX Spark:\")\n",
    "print(\"- bfloat16 (preferred): Best balance of speed and stability\")\n",
    "print(\"- float16: Faster but may have numerical issues\")\n",
    "print(\"- float32: Most stable but uses 2x memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ✅ How to search the Hugging Face Hub programmatically\n",
    "- ✅ How to read and interpret model cards\n",
    "- ✅ How to load models using Auto classes\n",
    "- ✅ How to test models for classification, QA, and generation\n",
    "- ✅ How to document models for your own reference\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge: Find a Hidden Gem\n",
    "\n",
    "Find a model that:\n",
    "1. Has fewer than 10,000 downloads\n",
    "2. Performs a task you're interested in\n",
    "3. Has a good model card\n",
    "4. Actually works when you test it!\n",
    "\n",
    "Document why you think this is an underrated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE\n",
    "# Find your hidden gem!\n",
    "\n",
    "# Hint: Try searching for specific domains or languages\n",
    "# less_popular = list(api.list_models(\n",
    "#     filter=\"your-task\",\n",
    "#     sort=\"downloads\",\n",
    "#     direction=1,  # Ascending - fewer downloads first\n",
    "#     limit=50\n",
    "# ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Hugging Face Hub Documentation](https://huggingface.co/docs/hub)\n",
    "- [Model Cards Paper](https://arxiv.org/abs/1810.03993)\n",
    "- [Transformers Documentation](https://huggingface.co/docs/transformers)\n",
    "- [Best Practices for Model Selection](https://huggingface.co/blog/model-selection)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Optional: Using the Utility Scripts\n\nThis module includes utility scripts that provide reusable functions for common tasks.\nYou can import and use them in your own projects:\n\n```python\n# From the module directory, you can import the utility scripts:\nfrom scripts.hub_utils import search_models, document_model, test_model_locally\nfrom scripts.training_utils import create_training_args, compute_metrics_factory\nfrom scripts.peft_utils import create_lora_config, apply_lora\n\n# Example: Search for models\nmodels = search_models(task=\"text-classification\", limit=5)\n\n# Example: Document a model with full metadata\ndoc = document_model(\"distilbert-base-uncased-finetuned-sst-2-english\")\nprint(doc.to_json())\n```\n\nSee the `scripts/` directory for full documentation of available utilities.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clear GPU memory\nimport gc\n\n# Clear any loaded models\ntry:\n    del gen_model, gen_tokenizer\nexcept NameError:\n    pass\n\ntorch.cuda.empty_cache()\ngc.collect()\n\nprint(\"Memory cleaned up!\")\nprint_memory_usage(\"Final\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, **02-pipeline-showcase.ipynb**, we'll explore the Pipeline API - an even easier way to use models without writing tokenization and post-processing code!\n",
    "\n",
    "Great job completing Lab 2.4.1!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}