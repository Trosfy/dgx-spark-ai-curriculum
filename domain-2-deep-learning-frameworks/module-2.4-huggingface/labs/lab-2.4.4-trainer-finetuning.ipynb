{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.4.4: Fine-tuning with the Trainer API\n",
    "\n",
    "**Module:** 2.4 - Hugging Face Ecosystem  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐⭐ (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Configure TrainingArguments for optimal training\n",
    "- [ ] Use the Trainer API for fine-tuning\n",
    "- [ ] Implement custom metrics with compute_metrics\n",
    "- [ ] Use callbacks for custom behavior\n",
    "- [ ] Leverage DGX Spark's capabilities (bf16, large batches)\n",
    "- [ ] Evaluate and compare models\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Lab 2.4.3 (Dataset Processing)\n",
    "- Knowledge of: Tokenization, basic neural network training concepts\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "You've been hired by a movie streaming company to build a review sentiment classifier. They have thousands of movie reviews and want to automatically categorize them as positive or negative.\n",
    "\n",
    "You could train a model from scratch, but that would take weeks and millions of examples. Instead, you'll **fine-tune** a pre-trained model in just minutes!\n",
    "\n",
    "**Fine-tuning success stories:**\n",
    "- Twitter trained a toxicity classifier in hours (vs. months from scratch)\n",
    "- Healthcare companies fine-tune for medical text understanding\n",
    "- Banks fine-tune for fraud detection in transaction descriptions\n",
    "- E-commerce uses fine-tuned models for product categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: What is Fine-tuning?\n",
    "\n",
    "> **Imagine you're learning to play piano.** You could:\n",
    "> - Option A: Start from scratch, learn music theory, practice scales for years\n",
    "> - Option B: Find someone who already plays guitar (similar skill!), teach them piano differences\n",
    ">\n",
    "> **Fine-tuning is Option B for AI.** You take a model that already knows language (like BERT) and teach it YOUR specific task.\n",
    ">\n",
    "> **What the pre-trained model already knows:**\n",
    "> - Grammar and sentence structure\n",
    "> - Word meanings and relationships\n",
    "> - Common sense about the world\n",
    ">\n",
    "> **What fine-tuning teaches it:**\n",
    "> - \"This specific task: positive vs negative reviews\"\n",
    "> - \"Your specific data patterns\"\n",
    ">\n",
    "> **The Trainer API** is like a personal coach that handles all the training logistics:\n",
    "> - Schedules practice sessions (epochs)\n",
    "> - Tracks progress (metrics)\n",
    "> - Adjusts difficulty (learning rate)\n",
    "> - Saves best performances (checkpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n# Note: These packages are pre-installed in the NGC PyTorch container.\n# Running pip install ensures you have compatible versions.\n# If NOT using NGC container, ensure you have ARM64-compatible packages for DGX Spark.\n\n!pip install -q \"transformers>=4.35.0\" \"datasets>=2.14.0\" \"evaluate>=0.4.0\" \"accelerate>=0.24.0\" scikit-learn\n\nimport torch\nimport numpy as np\nfrom typing import List, Dict, Any, Union\nfrom datasets import load_dataset, DatasetDict\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    EarlyStoppingCallback,\n    PreTrainedModel,\n    PreTrainedTokenizer\n)\nimport evaluate\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Check hardware\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the IMDB dataset\n",
    "print(\"Loading IMDB dataset...\")\n",
    "raw_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Create train/validation split\n",
    "train_val = raw_dataset['train'].train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=42,\n",
    "    stratify_by_column='label'\n",
    ")\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_val['train'],\n",
    "    'validation': train_val['test'],\n",
    "    'test': raw_dataset['test']\n",
    "})\n",
    "\n",
    "print(f\"\\nDataset splits:\")\n",
    "print(f\"  Train: {len(dataset['train']):,}\")\n",
    "print(f\"  Validation: {len(dataset['validation']):,}\")\n",
    "print(f\"  Test: {len(dataset['test']):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,  # Binary classification\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    "    remove_columns=['text'],\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "# Rename label to labels (expected by Trainer)\n",
    "tokenized_dataset = tokenized_dataset.rename_column('label', 'labels')\n",
    "\n",
    "print(f\"\\nTokenized columns: {tokenized_dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding TrainingArguments\n",
    "\n",
    "TrainingArguments controls EVERYTHING about training. Let's understand the key parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TrainingArguments with detailed comments\n",
    "training_args = TrainingArguments(\n",
    "    # === Output Settings ===\n",
    "    output_dir=\"./results/imdb-sentiment\",  # Where to save checkpoints\n",
    "    overwrite_output_dir=True,              # Overwrite existing dir\n",
    "    \n",
    "    # === Training Duration ===\n",
    "    num_train_epochs=3,                     # Total passes through data\n",
    "    # max_steps=-1,                         # Alternative: stop after N steps\n",
    "    \n",
    "    # === Batch Size (DGX Spark can handle larger!) ===\n",
    "    per_device_train_batch_size=16,         # Batch size per GPU\n",
    "    per_device_eval_batch_size=32,          # Larger for eval (no gradients)\n",
    "    # gradient_accumulation_steps=2,        # Effective batch = 16*2 = 32\n",
    "    \n",
    "    # === Learning Rate ===\n",
    "    learning_rate=2e-5,                     # Standard for fine-tuning\n",
    "    weight_decay=0.01,                      # L2 regularization\n",
    "    warmup_ratio=0.1,                       # 10% of training for warmup\n",
    "    # warmup_steps=500,                     # Alternative: fixed warmup steps\n",
    "    lr_scheduler_type=\"linear\",             # Linear decay after warmup\n",
    "    \n",
    "    # === Evaluation ===\n",
    "    eval_strategy=\"epoch\",                  # Evaluate after each epoch\n",
    "    # eval_steps=500,                       # Alternative: every N steps\n",
    "    \n",
    "    # === Checkpointing ===\n",
    "    save_strategy=\"epoch\",                  # Save after each epoch\n",
    "    save_total_limit=2,                     # Keep only 2 best checkpoints\n",
    "    load_best_model_at_end=True,            # Load best model after training\n",
    "    metric_for_best_model=\"accuracy\",       # Which metric to track\n",
    "    greater_is_better=True,                 # Higher accuracy = better\n",
    "    \n",
    "    # === Precision (DGX Spark optimization!) ===\n",
    "    bf16=True,                              # Use bfloat16 (Blackwell native!)\n",
    "    # fp16=False,                           # Don't use fp16 on Blackwell\n",
    "    \n",
    "    # === Logging ===\n",
    "    logging_dir=\"./logs\",                   # TensorBoard logs\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,                      # Log every 100 steps\n",
    "    report_to=\"none\",                       # Disable W&B/etc reporting\n",
    "    \n",
    "    # === Performance ===\n",
    "    dataloader_num_workers=4,               # Parallel data loading\n",
    "    dataloader_pin_memory=True,             # Pin memory for faster GPU transfer\n",
    "    \n",
    "    # === Reproducibility ===\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"TrainingArguments configured!\")\n",
    "print(f\"\\nKey settings:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  BF16: {training_args.bf16}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Key Parameters\n",
    "\n",
    "| Parameter | What it does | DGX Spark recommendation |\n",
    "|-----------|--------------|---------------------------|\n",
    "| `per_device_train_batch_size` | Samples per forward pass | 16-64 (you have memory!) |\n",
    "| `learning_rate` | Step size for updates | 1e-5 to 5e-5 for fine-tuning |\n",
    "| `warmup_ratio` | Gradual LR increase period | 0.1 (10% of training) |\n",
    "| `bf16` | Use bfloat16 precision | **True** (native Blackwell support) |\n",
    "| `gradient_accumulation_steps` | Simulate larger batches | Use if batch doesn't fit |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Custom Metrics with compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute multiple metrics for evaluation.\n",
    "    \n",
    "    Args:\n",
    "        eval_pred: EvalPrediction with predictions and label_ids\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of metric names and values\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Convert logits to predictions\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy['accuracy'],\n",
    "        'f1': f1['f1'],\n",
    "        'precision': precision['precision'],\n",
    "        'recall': recall['recall']\n",
    "    }\n",
    "\n",
    "print(\"Metrics function ready!\")\n",
    "print(\"Will compute: accuracy, f1, precision, recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Each Metric Means\n",
    "\n",
    "| Metric | Meaning | When it matters |\n",
    "|--------|---------|------------------|\n",
    "| **Accuracy** | % correct predictions | Balanced datasets |\n",
    "| **Precision** | Of predicted positives, % correct | Cost of false positives high |\n",
    "| **Recall** | Of actual positives, % found | Cost of missing positives high |\n",
    "| **F1** | Harmonic mean of precision & recall | Imbalanced datasets |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Creating and Running the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Stop if no improvement\n",
    ")\n",
    "\n",
    "print(\"Trainer created!\")\n",
    "print(f\"\\nTraining samples: {len(trainer.train_dataset):,}\")\n",
    "print(f\"Validation samples: {len(trainer.eval_dataset):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate before training (baseline)\n",
    "print(\"Evaluating baseline (before training)...\")\n",
    "baseline_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nBaseline Results:\")\n",
    "for key, value in baseline_results.items():\n",
    "    if 'loss' in key or 'accuracy' in key or 'f1' in key:\n",
    "        print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model!\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# This is where the magic happens!\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training statistics\n",
    "print(\"\\nTraining Statistics:\")\n",
    "print(f\"  Total steps: {train_result.global_step}\")\n",
    "print(f\"  Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"  Training time: {train_result.metrics['train_runtime']:.1f}s\")\n",
    "print(f\"  Samples/second: {train_result.metrics['train_samples_per_second']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "test_results = trainer.evaluate(tokenized_dataset['test'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for key, value in test_results.items():\n",
    "    if not key.startswith('eval_runtime'):\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for detailed analysis\n",
    "predictions = trainer.predict(tokenized_dataset['test'])\n",
    "\n",
    "# Convert to class predictions\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(f\"               Predicted\")\n",
    "print(f\"              NEG    POS\")\n",
    "print(f\"Actual NEG  {cm[0,0]:5d}  {cm[0,1]:5d}\")\n",
    "print(f\"       POS  {cm[1,0]:5d}  {cm[1,1]:5d}\")\n",
    "\n",
    "print(\"\\n\" + classification_report(\n",
    "    true_labels, \n",
    "    pred_labels, \n",
    "    target_names=['NEGATIVE', 'POSITIVE']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Function to predict sentiment for new texts\ndef predict_sentiment(\n    texts: Union[str, List[str]], \n    model: PreTrainedModel, \n    tokenizer: PreTrainedTokenizer\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    Predict sentiment for a list of texts.\n    \n    Args:\n        texts: List of strings or single string to classify\n        model: Trained sequence classification model\n        tokenizer: Tokenizer corresponding to the model\n        \n    Returns:\n        List of predictions with 'label' and 'confidence' keys\n    \"\"\"\n    if isinstance(texts, str):\n        texts = [texts]\n    \n    # Tokenize\n    inputs = tokenizer(\n        texts,\n        truncation=True,\n        padding=True,\n        max_length=256,\n        return_tensors=\"pt\"\n    )\n    \n    # Move to same device as model\n    device = next(model.parameters()).device\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    # Predict\n    model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    # Process outputs\n    probs = torch.softmax(outputs.logits, dim=1)\n    predictions = torch.argmax(probs, dim=1)\n    confidences = probs.max(dim=1).values\n    \n    results: List[Dict[str, Any]] = []\n    for pred, conf in zip(predictions, confidences):\n        label = model.config.id2label[pred.item()]\n        results.append({\n            'label': label,\n            'confidence': conf.item()\n        })\n    \n    return results\n\nprint(\"Prediction function ready!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on new reviews\n",
    "test_reviews = [\n",
    "    \"This movie was absolutely fantastic! The acting was superb and the plot kept me engaged throughout.\",\n",
    "    \"Waste of time. Terrible acting, boring plot, would not recommend to anyone.\",\n",
    "    \"It was okay. Not great, not terrible. Just average.\",\n",
    "    \"One of the best films I've seen this year! Definitely Oscar-worthy.\",\n",
    "    \"I walked out after 30 minutes. Couldn't stand it.\",\n",
    "    \"The special effects were amazing but the story was lacking.\"\n",
    "]\n",
    "\n",
    "print(\"Testing on new reviews:\\n\")\n",
    "print(f\"{'Review':<70} {'Prediction':<12} {'Confidence'}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "predictions = predict_sentiment(test_reviews, trainer.model, tokenizer)\n",
    "\n",
    "for review, pred in zip(test_reviews, predictions):\n",
    "    display_review = review[:67] + \"...\" if len(review) > 70 else review\n",
    "    print(f\"{display_review:<70} {pred['label']:<12} {pred['confidence']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Saving and Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "save_path = \"./saved_models/imdb-sentiment\"\n",
    "\n",
    "print(f\"Saving model to {save_path}...\")\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "# Check what was saved\n",
    "import os\n",
    "print(\"\\nSaved files:\")\n",
    "for f in os.listdir(save_path):\n",
    "    size = os.path.getsize(os.path.join(save_path, f)) / 1e6\n",
    "    print(f\"  {f}: {size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "print(\"\\nLoading saved model...\")\n",
    "\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(save_path)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loaded_model = loaded_model.to(device)\n",
    "\n",
    "# Test loaded model\n",
    "test_text = \"This movie was incredible!\"\n",
    "result = predict_sentiment(test_text, loaded_model, loaded_tokenizer)\n",
    "print(f\"\\nLoaded model prediction: '{test_text}' → {result[0]['label']} ({result[0]['confidence']:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Comparing Training Configurations\n",
    "\n",
    "Let's compare different training setups to see their impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train with different configs and compare\n",
    "def quick_train(config_name, learning_rate, batch_size, epochs=1):\n",
    "    \"\"\"Train with specific config and return results.\"\"\"\n",
    "    print(f\"\\nTraining: {config_name}\")\n",
    "    print(f\"  LR: {learning_rate}, Batch: {batch_size}, Epochs: {epochs}\")\n",
    "    \n",
    "    # Fresh model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=2\n",
    "    )\n",
    "    \n",
    "    # Use smaller subset for speed\n",
    "    small_train = tokenized_dataset['train'].select(range(2000))\n",
    "    small_val = tokenized_dataset['validation'].select(range(500))\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./results/{config_name}\",\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size * 2,\n",
    "        learning_rate=learning_rate,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        bf16=True,\n",
    "        logging_strategy=\"no\",\n",
    "        report_to=\"none\",\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=small_train,\n",
    "        eval_dataset=small_val,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    import time\n",
    "    start = time.time()\n",
    "    trainer.train()\n",
    "    train_time = time.time() - start\n",
    "    \n",
    "    results = trainer.evaluate()\n",
    "    \n",
    "    return {\n",
    "        'config': config_name,\n",
    "        'accuracy': results['eval_accuracy'],\n",
    "        'f1': results['eval_f1'],\n",
    "        'time': train_time\n",
    "    }\n",
    "\n",
    "print(\"Comparison function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different configurations\n",
    "configs = [\n",
    "    (\"small_lr\", 1e-5, 16),\n",
    "    (\"medium_lr\", 2e-5, 16),\n",
    "    (\"large_lr\", 5e-5, 16),\n",
    "    (\"large_batch\", 2e-5, 32),\n",
    "]\n",
    "\n",
    "comparison_results = []\n",
    "for name, lr, bs in configs:\n",
    "    result = quick_train(name, lr, bs)\n",
    "    comparison_results.append(result)\n",
    "    \n",
    "# Display comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONFIGURATION COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Config':<15} {'Accuracy':<12} {'F1':<12} {'Time (s)':<10}\")\n",
    "print(\"-\"*50)\n",
    "for r in comparison_results:\n",
    "    print(f\"{r['config']:<15} {r['accuracy']:.4f}       {r['f1']:.4f}       {r['time']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Train on AG News\n",
    "\n",
    "Fine-tune a model on the AG News dataset (news category classification):\n",
    "1. Load the `ag_news` dataset\n",
    "2. Create appropriate splits\n",
    "3. Tokenize with a model of your choice\n",
    "4. Configure TrainingArguments\n",
    "5. Train and evaluate\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "```python\n",
    "# AG News has 4 classes: World, Sports, Business, Sci/Tech\n",
    "ag_news = load_dataset(\"ag_news\")\n",
    "\n",
    "# Configure model for 4 classes\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=4,\n",
    "    id2label={0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"},\n",
    "    label2id={\"World\": 0, \"Sports\": 1, \"Business\": 2, \"Sci/Tech\": 3}\n",
    ")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Train a news classifier on AG News\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Wrong Label Column Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Trainer expects 'labels', not 'label'\n",
    "# dataset.column_names = ['input_ids', 'attention_mask', 'label']\n",
    "# trainer.train()  # Error or poor results!\n",
    "\n",
    "# CORRECT: Rename label to labels\n",
    "# dataset = dataset.rename_column('label', 'labels')\n",
    "\n",
    "print(\"Trainer expects 'labels' (plural) as the target column!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Learning Rate Too High"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: LR too high for fine-tuning\n",
    "# learning_rate=1e-3  # Will destroy pre-trained knowledge!\n",
    "\n",
    "# CORRECT: Use small LR for fine-tuning\n",
    "# learning_rate=2e-5  # Gentle updates to pre-trained weights\n",
    "\n",
    "print(\"Fine-tuning LR guide:\")\n",
    "print(\"  1e-5 to 5e-5: Safe range for most tasks\")\n",
    "print(\"  1e-4: Can work with warmup\")\n",
    "print(\"  1e-3+: Too high, will forget pre-training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Not Using eval_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: No evaluation during training\n",
    "# TrainingArguments(eval_strategy=\"no\")  # Can't track progress!\n",
    "\n",
    "# CORRECT: Evaluate regularly\n",
    "# TrainingArguments(\n",
    "#     eval_strategy=\"epoch\",  # or \"steps\" with eval_steps=500\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"accuracy\"\n",
    "# )\n",
    "\n",
    "print(\"Always enable evaluation to track training progress!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ✅ How to configure TrainingArguments for optimal training\n",
    "- ✅ How to use the Trainer API for fine-tuning\n",
    "- ✅ How to implement custom metrics\n",
    "- ✅ How to use callbacks for early stopping\n",
    "- ✅ How to evaluate and compare models\n",
    "- ✅ How to save and load fine-tuned models\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge: Multi-class Emotion Detection\n",
    "\n",
    "Train an emotion classifier using the `emotion` dataset:\n",
    "1. Load and explore the dataset (6 emotions)\n",
    "2. Fine-tune with custom metrics including per-class F1\n",
    "3. Implement a confusion matrix callback\n",
    "4. Achieve >90% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE\n",
    "# emotion dataset has: sadness, joy, love, anger, fear, surprise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Trainer Documentation](https://huggingface.co/docs/transformers/main_classes/trainer)\n",
    "- [TrainingArguments Reference](https://huggingface.co/docs/transformers/main_classes/trainer#trainingarguments)\n",
    "- [Fine-tuning Guide](https://huggingface.co/docs/transformers/training)\n",
    "- [Evaluate Library](https://huggingface.co/docs/evaluate)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "# Remove saved models and results\n",
    "for path in [\"./results\", \"./logs\", \"./saved_models\"]:\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Removed {path}\")\n",
    "\n",
    "# Clear memory\n",
    "del model, trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nCleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, **05-lora-introduction.ipynb**, we'll learn about Parameter-Efficient Fine-Tuning (PEFT) with LoRA - how to fine-tune large models using just a fraction of the parameters!\n",
    "\n",
    "Great job completing Lab 2.4.4! You now know how to fine-tune transformer models like a pro!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}