{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.4.1 Solutions: Hub Exploration\n",
    "\n",
    "This notebook contains solutions to the exercises in the Hub Exploration notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from huggingface_hub import HfApi, list_models\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "api = HfApi()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Document 10 Models\n",
    "\n",
    "Complete solution for documenting 10 models from different task categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_model(model_id: str) -> dict:\n",
    "    \"\"\"Create documentation for a Hugging Face model.\"\"\"\n",
    "    try:\n",
    "        info = api.model_info(model_id)\n",
    "        return {\n",
    "            \"model_id\": model_id,\n",
    "            \"author\": info.author,\n",
    "            \"task\": info.pipeline_tag,\n",
    "            \"downloads\": info.downloads,\n",
    "            \"likes\": info.likes,\n",
    "            \"library\": info.library_name,\n",
    "            \"tags\": info.tags[:10] if info.tags else [],\n",
    "            \"documented_at\": datetime.now().isoformat(),\n",
    "            \"notes\": \"\",\n",
    "            \"tested_locally\": False\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"model_id\": model_id, \"error\": str(e)}\n",
    "\n",
    "# List of 10 models across different tasks\n",
    "models_to_document = [\n",
    "    # Text Classification (2 models)\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    \n",
    "    # Text Generation (2 models)\n",
    "    \"gpt2\",\n",
    "    \"distilgpt2\",\n",
    "    \n",
    "    # Question Answering (2 models)\n",
    "    \"distilbert-base-cased-distilled-squad\",\n",
    "    \"deepset/roberta-base-squad2\",\n",
    "    \n",
    "    # Named Entity Recognition (2 models)\n",
    "    \"dslim/bert-base-NER\",\n",
    "    \"Jean-Baptiste/camembert-ner\",\n",
    "    \n",
    "    # Other (2 models - summarization and translation)\n",
    "    \"facebook/bart-large-cnn\",\n",
    "    \"Helsinki-NLP/opus-mt-en-de\"\n",
    "]\n",
    "\n",
    "# Document all models\n",
    "my_model_docs = []\n",
    "print(\"Documenting models...\\n\")\n",
    "\n",
    "for model_id in models_to_document:\n",
    "    doc = document_model(model_id)\n",
    "    my_model_docs.append(doc)\n",
    "    if \"error\" not in doc:\n",
    "        print(f\"[OK] {model_id}\")\n",
    "        print(f\"     Task: {doc['task']}, Downloads: {doc['downloads']:,}\")\n",
    "    else:\n",
    "        print(f\"[ERR] {model_id}: {doc['error']}\")\n",
    "\n",
    "print(f\"\\nTotal models documented: {len(my_model_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display documentation summary\n",
    "print(\"\\nModel Documentation Summary:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Model ID':<50} {'Task':<20} {'Downloads':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for doc in my_model_docs:\n",
    "    if \"error\" not in doc:\n",
    "        print(f\"{doc['model_id']:<50} {doc['task'] or 'N/A':<20} {doc['downloads']:>10,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Find a Hidden Gem\n",
    "\n",
    "Finding an underrated model with fewer than 10,000 downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for less popular but potentially useful models\n",
    "# Let's look for specialized sentiment models\n",
    "\n",
    "# Search for financial sentiment models (specialized domain)\n",
    "financial_models = list(api.list_models(\n",
    "    search=\"financial sentiment\",\n",
    "    sort=\"downloads\",\n",
    "    direction=1,  # Ascending - fewer downloads first\n",
    "    limit=20\n",
    "))\n",
    "\n",
    "print(\"Potential Hidden Gems (Financial Sentiment):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for m in financial_models:\n",
    "    downloads = m.downloads if hasattr(m, 'downloads') else 0\n",
    "    if 100 < downloads < 10000:  # Has some usage but not too popular\n",
    "        print(f\"\\n{m.id}\")\n",
    "        print(f\"  Downloads: {downloads:,}\")\n",
    "        print(f\"  Task: {m.pipeline_tag if hasattr(m, 'pipeline_tag') else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My Hidden Gem Selection and Analysis\n",
    "hidden_gem = \"nickmuchi/finbert-tone-finetuned-finance-topic-classification\"\n",
    "\n",
    "try:\n",
    "    gem_doc = document_model(hidden_gem)\n",
    "    \n",
    "    print(\"\\nMY HIDDEN GEM ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Model: {hidden_gem}\")\n",
    "    print(f\"Author: {gem_doc.get('author', 'Unknown')}\")\n",
    "    print(f\"Task: {gem_doc.get('task', 'Unknown')}\")\n",
    "    print(f\"Downloads: {gem_doc.get('downloads', 0):,}\")\n",
    "    print(f\"\\nWhy I think this is underrated:\")\n",
    "    print(\"  - Specialized for financial text classification\")\n",
    "    print(\"  - Built on FinBERT which is domain-adapted\")\n",
    "    print(\"  - Useful for fintech applications\")\n",
    "    print(\"  - Has clear documentation\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not analyze hidden gem: {e}\")\n",
    "    print(\"\\nAlternative: Try 'yiyanghkust/finbert-tone' for financial sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Test 3 Models Locally\n",
    "\n",
    "Complete solution for testing models locally with detailed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "\n",
    "def test_model_locally(model_id: str, task: str, test_input: str, context: str = None):\n",
    "    \"\"\"Test a model locally and return results.\"\"\"\n",
    "    results = {\n",
    "        \"model_id\": model_id,\n",
    "        \"task\": task,\n",
    "        \"success\": False,\n",
    "        \"output\": None,\n",
    "        \"load_time\": 0,\n",
    "        \"inference_time_ms\": 0,\n",
    "        \"memory_gb\": 0,\n",
    "        \"error\": None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "        initial_mem = torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "        \n",
    "        # Load\n",
    "        start = time.time()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        \n",
    "        if task == \"classification\":\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                model_id, torch_dtype=torch.bfloat16\n",
    "            ).to(device).eval()\n",
    "        elif task == \"generation\":\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id, torch_dtype=torch.bfloat16\n",
    "            ).to(device).eval()\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        elif task == \"qa\":\n",
    "            model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "                model_id, torch_dtype=torch.bfloat16\n",
    "            ).to(device).eval()\n",
    "        \n",
    "        results[\"load_time\"] = time.time() - start\n",
    "        results[\"memory_gb\"] = (torch.cuda.memory_allocated() / 1e9) - initial_mem\n",
    "        \n",
    "        # Inference\n",
    "        start = time.time()\n",
    "        \n",
    "        if task == \"qa\" and context:\n",
    "            inputs = tokenizer(test_input, context, return_tensors=\"pt\", truncation=True)\n",
    "        else:\n",
    "            inputs = tokenizer(test_input, return_tensors=\"pt\", truncation=True)\n",
    "        \n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if task == \"classification\":\n",
    "                outputs = model(**inputs)\n",
    "                probs = torch.softmax(outputs.logits, dim=1)\n",
    "                pred = torch.argmax(probs, dim=1).item()\n",
    "                conf = probs[0][pred].item()\n",
    "                label = model.config.id2label.get(pred, f\"Class {pred}\")\n",
    "                results[\"output\"] = f\"{label} ({conf:.2%})\"\n",
    "            elif task == \"generation\":\n",
    "                outputs = model.generate(**inputs, max_new_tokens=30, do_sample=True, temperature=0.7)\n",
    "                results[\"output\"] = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            elif task == \"qa\":\n",
    "                outputs = model(**inputs)\n",
    "                start_idx = torch.argmax(outputs.start_logits)\n",
    "                end_idx = torch.argmax(outputs.end_logits) + 1\n",
    "                results[\"output\"] = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
    "        \n",
    "        results[\"inference_time_ms\"] = (time.time() - start) * 1000\n",
    "        results[\"success\"] = True\n",
    "        \n",
    "        del model, tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        results[\"error\"] = str(e)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3 models\n",
    "tests = [\n",
    "    {\n",
    "        \"model_id\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        \"task\": \"classification\",\n",
    "        \"test_input\": \"This product exceeded all my expectations! Best purchase ever.\"\n",
    "    },\n",
    "    {\n",
    "        \"model_id\": \"gpt2\",\n",
    "        \"task\": \"generation\",\n",
    "        \"test_input\": \"The future of artificial intelligence is\"\n",
    "    },\n",
    "    {\n",
    "        \"model_id\": \"distilbert-base-cased-distilled-squad\",\n",
    "        \"task\": \"qa\",\n",
    "        \"test_input\": \"What is the capital of France?\",\n",
    "        \"context\": \"France is a country in Western Europe. Its capital is Paris, a beautiful city known for the Eiffel Tower.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"LOCAL MODEL TESTING RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for test in tests:\n",
    "    print(f\"\\nTesting: {test['model_id']}\")\n",
    "    print(f\"Task: {test['task']}\")\n",
    "    print(f\"Input: {test['test_input'][:50]}...\")\n",
    "    \n",
    "    result = test_model_locally(\n",
    "        test['model_id'],\n",
    "        test['task'],\n",
    "        test['test_input'],\n",
    "        test.get('context')\n",
    "    )\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        print(f\"\\nStatus: SUCCESS\")\n",
    "        print(f\"Output: {result['output']}\")\n",
    "        print(f\"Load time: {result['load_time']:.2f}s\")\n",
    "        print(f\"Inference: {result['inference_time_ms']:.2f}ms\")\n",
    "        print(f\"Memory: {result['memory_gb']:.2f} GB\")\n",
    "    else:\n",
    "        print(f\"\\nStatus: FAILED\")\n",
    "        print(f\"Error: {result['error']}\")\n",
    "    \n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Expected Output (Sample)\n\nWhen you run the tests above, you should see output similar to:\n\n```\nLOCAL MODEL TESTING RESULTS\n================================================================================\n\nTesting: distilbert-base-uncased-finetuned-sst-2-english\nTask: classification\nInput: This product exceeded all my expectations! Best pu...\n\nStatus: SUCCESS\nOutput: POSITIVE (99.87%)\nLoad time: 1.23s\nInference: 12.45ms\nMemory: 0.26 GB\n------------------------------------------------------------\n\nTesting: gpt2\nTask: generation\nInput: The future of artificial intelligence is...\n\nStatus: SUCCESS\nOutput: The future of artificial intelligence is uncertain, but one thing is certain: it will change the world in ways we cannot yet imagine.\nLoad time: 2.15s\nInference: 245.32ms\nMemory: 0.48 GB\n------------------------------------------------------------\n\nTesting: distilbert-base-cased-distilled-squad\nTask: qa\nInput: What is the capital of France?...\n\nStatus: SUCCESS\nOutput: Paris\nLoad time: 1.18s\nInference: 8.21ms\nMemory: 0.26 GB\n------------------------------------------------------------\n```\n\n**Note:** Actual values will vary slightly based on your hardware and model versions.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this solution notebook, we:\n",
    "\n",
    "1. **Documented 10 models** across different task categories:\n",
    "   - Text Classification (2)\n",
    "   - Text Generation (2)\n",
    "   - Question Answering (2)\n",
    "   - Named Entity Recognition (2)\n",
    "   - Other tasks (2)\n",
    "\n",
    "2. **Found a hidden gem**: Specialized financial sentiment model with good documentation but fewer downloads\n",
    "\n",
    "3. **Tested 3 models locally**:\n",
    "   - Sentiment classification\n",
    "   - Text generation\n",
    "   - Question answering\n",
    "\n",
    "Key learnings:\n",
    "- Use `HfApi` for programmatic Hub access\n",
    "- Model cards are essential for understanding model capabilities\n",
    "- Different Auto classes for different tasks\n",
    "- Memory management is important when testing multiple models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}