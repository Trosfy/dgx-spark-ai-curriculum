{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 9.6 Solutions: Model Upload\n",
    "\n",
    "This notebook contains solutions to the exercises in the Model Upload notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from huggingface_hub import HfApi, whoami, login\n",
    "import evaluate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Solution: Upload Your Model\n",
    "\n",
    "Complete the following steps to upload your own model:\n",
    "1. Login to Hugging Face\n",
    "2. Fine-tune a model\n",
    "3. Create a comprehensive model card\n",
    "4. Upload to the Hub\n",
    "5. Test loading your model from the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Authentication\n",
    "# Uncomment and run to login:\n",
    "# login()\n",
    "\n",
    "# Check if logged in\n",
    "try:\n",
    "    user_info = whoami()\n",
    "    print(f\"Logged in as: {user_info['name']}\")\n",
    "    USERNAME = user_info['name']\n",
    "    logged_in = True\n",
    "except Exception as e:\n",
    "    print(f\"Not logged in: {e}\")\n",
    "    print(\"\\nTo login, run: login()\")\n",
    "    USERNAME = \"your-username\"  # Placeholder\n",
    "    logged_in = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Fine-tune a model\n",
    "print(\"Loading and preparing data...\")\n",
    "\n",
    "# Load IMDB\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "# Use subset for demo\n",
    "train_data = imdb['train'].shuffle(seed=42).select(range(3000))\n",
    "val_data = imdb['train'].shuffle(seed=42).select(range(3000, 3500))\n",
    "test_data = imdb['test'].shuffle(seed=42).select(range(500))\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_data,\n",
    "    'validation': val_data,\n",
    "    'test': test_data\n",
    "})\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    ")\n",
    "\n",
    "# Tokenize\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "tokenized = dataset.map(tokenize, batched=True, remove_columns=['text'])\n",
    "tokenized = tokenized.rename_column('label', 'labels')\n",
    "\n",
    "print(f\"Data prepared: {len(tokenized['train'])} train, {len(tokenized['validation'])} val, {len(tokenized['test'])} test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_metric.compute(predictions=predictions, references=labels)['accuracy'],\n",
    "        'f1': f1_metric.compute(predictions=predictions, references=labels)['f1']\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model_to_upload\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    bf16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized['train'],\n",
    "    eval_dataset=tokenized['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "test_results = trainer.evaluate(tokenized['test'])\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "print(f\"  F1: {test_results['eval_f1']:.4f}\")\n",
    "\n",
    "# Store for model card\n",
    "TEST_ACCURACY = test_results['eval_accuracy']\n",
    "TEST_F1 = test_results['eval_f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create comprehensive model card\n",
    "from datetime import datetime\n",
    "\n",
    "model_card_content = f'''---\nlanguage:\n- en\nlicense: mit\ntags:\n- sentiment-analysis\n- text-classification\n- distilbert\n- imdb\n- movie-reviews\n- dgx-spark\ndatasets:\n- imdb\nmetrics:\n- accuracy\n- f1\npipeline_tag: text-classification\nmodel-index:\n- name: distilbert-imdb-sentiment-dgx\n  results:\n  - task:\n      type: text-classification\n      name: Sentiment Analysis\n    dataset:\n      type: imdb\n      name: IMDB Movie Reviews\n      config: plain_text\n      split: test\n    metrics:\n    - type: accuracy\n      value: {TEST_ACCURACY:.4f}\n    - type: f1\n      value: {TEST_F1:.4f}\n---\n\n# DistilBERT Fine-tuned for IMDB Sentiment Analysis\n\nThis model is a fine-tuned version of [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) \non the IMDB movie reviews dataset for binary sentiment classification.\n\n## Model Description\n\n- **Model type:** DistilBERT for Sequence Classification\n- **Language:** English\n- **Task:** Binary Sentiment Classification (Positive/Negative)\n- **Training data:** IMDB Movie Reviews (3,000 samples)\n- **Fine-tuned from:** distilbert-base-uncased\n\n## Intended Uses & Limitations\n\n### Intended Uses\n\n- Classifying sentiment of movie reviews\n- Classifying sentiment of similar entertainment-related text\n- Educational purposes (learning about fine-tuning)\n- Baseline for sentiment analysis experiments\n\n### Limitations\n\n- **Domain specificity:** Trained on movie reviews, may not generalize to other domains\n- **Language:** English only\n- **Length:** Best with text under 256 tokens\n- **Bias:** May reflect biases present in IMDB reviews\n\n## Training Details\n\n### Hardware\n\n- **Platform:** NVIDIA DGX Spark\n- **GPU:** NVIDIA Blackwell GB10 Superchip\n- **Memory:** 128GB Unified LPDDR5X\n- **Precision:** bfloat16\n\n### Hyperparameters\n\n- **Learning rate:** 2e-5\n- **Batch size:** 16\n- **Epochs:** 2\n- **Warmup ratio:** 0.1\n- **Max sequence length:** 256\n- **Weight decay:** 0.01\n\n### Training Data\n\n- **Training samples:** 3,000\n- **Validation samples:** 500\n- **Test samples:** 500\n\n## Evaluation Results\n\n| Metric | Value |\n|--------|-------|\n| Accuracy | {TEST_ACCURACY:.2%} |\n| F1 Score | {TEST_F1:.2%} |\n\n## Usage\n\n### Using the Pipeline API (Recommended)\n\n```python\nfrom transformers import pipeline\n\nclassifier = pipeline(\"sentiment-analysis\", model=\"{USERNAME}/distilbert-imdb-sentiment-dgx\")\n\nresult = classifier(\"This movie was absolutely fantastic!\")\nprint(result)\n# Output: [{{\\'label\\': \\'POSITIVE\\', \\'score\\': 0.99}}]\n```\n\n### Using AutoModel\n\n```python\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"{USERNAME}/distilbert-imdb-sentiment-dgx\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"{USERNAME}/distilbert-imdb-sentiment-dgx\")\n\ninputs = tokenizer(\"This movie was terrible!\", return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model(**inputs)\n\npredictions = torch.softmax(outputs.logits, dim=1)\nprint(f\"Negative: {{predictions[0][0]:.2%}}, Positive: {{predictions[0][1]:.2%}}\")\n```\n\n## Example Predictions\n\n| Input | Prediction | Confidence |\n|-------|------------|------------|\n| \"Best movie I\\'ve ever seen!\" | POSITIVE | 99% |\n| \"Complete waste of time.\" | NEGATIVE | 98% |\n| \"It was okay, nothing special.\" | NEGATIVE | 65% |\n\n## Training Procedure\n\nThis model was fine-tuned as part of the **DGX Spark AI Curriculum**, Module 9: Hugging Face Ecosystem.\n\nThe training procedure followed these steps:\n1. Load pre-trained DistilBERT\n2. Add classification head (2 classes)\n3. Fine-tune on IMDB with AdamW optimizer\n4. Select best checkpoint based on validation accuracy\n\n## Citation\n\nIf you use this model, please cite:\n\n```bibtex\n@misc{{distilbert-imdb-sentiment-dgx,\n  author = {{DGX Spark Student}},\n  title = {{DistilBERT fine-tuned for IMDB Sentiment Analysis}},\n  year = {{2025}},\n  publisher = {{Hugging Face}},\n  howpublished = {{\\\\url{{https://huggingface.co/{USERNAME}/distilbert-imdb-sentiment-dgx}}}}\n}}\n```\n\n## Model Card Contact\n\nFor questions or issues, please open a discussion on the model page.\n\n---\n\n*Last updated: {datetime.now().strftime(\"%Y-%m-%d\")}*\n'''\n\n# Save model card\n",
    "save_path = \"./model_to_upload\"\n",
    "with open(os.path.join(save_path, \"README.md\"), \"w\") as f:\n",
    "    f.write(model_card_content)\n",
    "\n",
    "print(\"Model card created!\")\n",
    "print(\"\\nPreview (first 50 lines):\")\n",
    "print(\"=\"*50)\n",
    "for line in model_card_content.split(\"\\n\")[:50]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer\n",
    "print(\"Saving model and tokenizer...\")\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(\"\\nSaved files:\")\n",
    "for f in os.listdir(save_path):\n",
    "    size_mb = os.path.getsize(os.path.join(save_path, f)) / 1e6\n",
    "    print(f\"  {f}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Upload to Hub\n",
    "repo_name = \"distilbert-imdb-sentiment-dgx\"\n",
    "\n",
    "if logged_in:\n",
    "    print(f\"Uploading to: {USERNAME}/{repo_name}\")\n",
    "    \n",
    "    # Method 1: Using trainer.push_to_hub\n",
    "    # trainer.push_to_hub(repo_name)\n",
    "    \n",
    "    # Method 2: Using model.push_to_hub (more control)\n",
    "    # model.push_to_hub(repo_name)\n",
    "    # tokenizer.push_to_hub(repo_name)\n",
    "    \n",
    "    print(\"\\nTo actually upload, uncomment one of the methods above.\")\n",
    "    print(f\"Your model will be at: https://huggingface.co/{USERNAME}/{repo_name}\")\n",
    "else:\n",
    "    print(\"Not logged in. To upload:\")\n",
    "    print(\"1. Run: login()\")\n",
    "    print(\"2. Then run: trainer.push_to_hub('distilbert-imdb-sentiment-dgx')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Test loading from Hub (after upload)\n",
    "# This will work after you've uploaded the model\n",
    "\n",
    "print(\"Testing model loading...\")\n",
    "\n",
    "# For now, test loading from local save\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(save_path)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "\n",
    "# Test inference\n",
    "test_texts = [\n",
    "    \"This movie was incredible! Best film of the year.\",\n",
    "    \"Terrible waste of time. Would not recommend.\",\n",
    "    \"It was okay, pretty average really.\"\n",
    "]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loaded_model = loaded_model.to(device).eval()\n",
    "\n",
    "print(\"\\nTest Predictions:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for text in test_texts:\n",
    "    inputs = loaded_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**inputs)\n",
    "    \n",
    "    probs = torch.softmax(outputs.logits, dim=1)\n",
    "    pred = torch.argmax(probs, dim=1).item()\n",
    "    conf = probs[0][pred].item()\n",
    "    label = loaded_model.config.id2label[pred]\n",
    "    \n",
    "    print(f\"\\nText: {text[:50]}...\")\n",
    "    print(f\"Prediction: {label} ({conf:.2%})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Model loading test successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Solution: Complete Model Sharing Workflow\n",
    "\n",
    "End-to-end workflow for a unique task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete workflow: Train an emotion classifier and prepare for upload\n",
    "print(\"\\n\" + \"#\"*60)\n",
    "print(\"CHALLENGE: Complete Model Sharing Workflow\")\n",
    "print(\"#\"*60)\n",
    "\n",
    "# 1. Load emotion dataset\n",
    "print(\"\\n1. Loading emotion dataset...\")\n",
    "emotion = load_dataset(\"emotion\")\n",
    "emotion_labels = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
    "\n",
    "# Use subset\n",
    "emotion_train = emotion['train'].shuffle(seed=42).select(range(4000))\n",
    "emotion_val = emotion['validation'].select(range(500))\n",
    "emotion_test = emotion['test'].select(range(500))\n",
    "\n",
    "emotion_dataset = DatasetDict({\n",
    "    'train': emotion_train,\n",
    "    'validation': emotion_val,\n",
    "    'test': emotion_test\n",
    "})\n",
    "\n",
    "print(f\"   Train: {len(emotion_dataset['train'])}, Val: {len(emotion_dataset['validation'])}, Test: {len(emotion_dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Prepare model\n",
    "print(\"\\n2. Preparing model...\")\n",
    "\n",
    "emotion_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=6,\n",
    "    id2label={i: label for i, label in enumerate(emotion_labels)},\n",
    "    label2id={label: i for i, label in enumerate(emotion_labels)}\n",
    ")\n",
    "\n",
    "# Tokenize\n",
    "emotion_tokenized = emotion_dataset.map(\n",
    "    lambda x: tokenizer(x['text'], truncation=True, padding='max_length', max_length=128),\n",
    "    batched=True,\n",
    "    remove_columns=['text']\n",
    ")\n",
    "emotion_tokenized = emotion_tokenized.rename_column('label', 'labels')\n",
    "\n",
    "print(\"   Model and data ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Train\n",
    "print(\"\\n3. Training emotion classifier...\")\n",
    "\n",
    "emotion_args = TrainingArguments(\n",
    "    output_dir=\"./emotion_model_to_upload\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=2e-5,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    bf16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "emotion_trainer = Trainer(\n",
    "    model=emotion_model,\n",
    "    args=emotion_args,\n",
    "    train_dataset=emotion_tokenized['train'],\n",
    "    eval_dataset=emotion_tokenized['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "emotion_trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "emotion_results = emotion_trainer.evaluate(emotion_tokenized['test'])\n",
    "print(f\"\\n   Test Accuracy: {emotion_results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create model card\n",
    "print(\"\\n4. Creating model card...\")\n",
    "\n",
    "emotion_model_card = f'''---\nlanguage:\n- en\nlicense: mit\ntags:\n- emotion-detection\n- text-classification\n- distilbert\n- emotions\ndatasets:\n- emotion\nmetrics:\n- accuracy\n- f1\n---\n\n# DistilBERT Emotion Classifier\n\nClassifies text into 6 emotions: sadness, joy, love, anger, fear, surprise.\n\n## Usage\n\n```python\nfrom transformers import pipeline\n\nclassifier = pipeline(\"text-classification\", model=\"{USERNAME}/distilbert-emotion\")\nresult = classifier(\"I am so happy today!\")\n# Output: [{{\\'label\\': \\'joy\\', \\'score\\': 0.98}}]\n```\n\n## Results\n\n- Accuracy: {emotion_results[\"eval_accuracy\"]:.2%}\n\n## Labels\n\n| Label | Description |\n|-------|-------------|\n| sadness | Sad, depressed, unhappy |\n| joy | Happy, joyful, delighted |\n| love | Loving, affectionate |\n| anger | Angry, frustrated |\n| fear | Scared, anxious |\n| surprise | Surprised, amazed |\n\n---\n*Trained as part of DGX Spark AI Curriculum*\n'''\n\nemotion_save_path = \"./emotion_model_to_upload\"\n",
    "with open(os.path.join(emotion_save_path, \"README.md\"), \"w\") as f:\n",
    "    f.write(emotion_model_card)\n",
    "\n",
    "emotion_trainer.save_model(emotion_save_path)\n",
    "tokenizer.save_pretrained(emotion_save_path)\n",
    "\n",
    "print(\"   Model and card saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Test before upload\n",
    "print(\"\\n5. Testing model...\")\n",
    "\n",
    "test_emotions = [\n",
    "    \"I am so happy and excited about this!\",\n",
    "    \"This makes me really sad and depressed.\",\n",
    "    \"I love you so much!\",\n",
    "    \"This is absolutely infuriating!\",\n",
    "    \"I'm scared of what might happen.\",\n",
    "    \"Wow, I didn't expect that at all!\"\n",
    "]\n",
    "\n",
    "emotion_model = emotion_model.to(device).eval()\n",
    "\n",
    "print(\"\\nEmotion Predictions:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for text in test_emotions:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = emotion_model(**inputs)\n",
    "    \n",
    "    probs = torch.softmax(outputs.logits, dim=1)\n",
    "    pred = torch.argmax(probs, dim=1).item()\n",
    "    conf = probs[0][pred].item()\n",
    "    \n",
    "    print(f\"{text[:40]:<40} -> {emotion_labels[pred]:<10} ({conf:.0%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload instructions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"UPLOAD INSTRUCTIONS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\"\"\"\\nTo upload your emotion model:\n",
    "\n",
    "1. Login:\n",
    "   login()\n",
    "\n",
    "2. Upload:\n",
    "   emotion_trainer.push_to_hub(\"distilbert-emotion\")\n",
    "\n",
    "3. Access at:\n",
    "   https://huggingface.co/YOUR_USERNAME/distilbert-emotion\n",
    "\n",
    "4. Test from Hub:\n",
    "   from transformers import pipeline\n",
    "   classifier = pipeline(\"text-classification\", model=\"YOUR_USERNAME/distilbert-emotion\")\n",
    "   print(classifier(\"I am so happy!\"))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "for path in [\"./model_to_upload\", \"./emotion_model_to_upload\"]:\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "\n",
    "del model, emotion_model, trainer, emotion_trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "print(\"Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this solution notebook, we demonstrated the complete model upload workflow:\n",
    "\n",
    "1. **Authentication** with Hugging Face Hub\n",
    "2. **Fine-tuning** a sentiment classifier\n",
    "3. **Creating a comprehensive model card** with:\n",
    "   - YAML metadata for discoverability\n",
    "   - Model description and intended uses\n",
    "   - Training details and hyperparameters\n",
    "   - Evaluation results\n",
    "   - Usage examples\n",
    "   - Limitations and biases\n",
    "4. **Uploading** to the Hub\n",
    "5. **Testing** the uploaded model\n",
    "\n",
    "We also completed the **challenge** by creating an emotion classifier with the same workflow.\n",
    "\n",
    "Key learnings:\n",
    "- Model cards are essential for discoverability and trust\n",
    "- Include usage examples in your model card\n",
    "- Test your model before uploading\n",
    "- Use appropriate tags and metadata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
