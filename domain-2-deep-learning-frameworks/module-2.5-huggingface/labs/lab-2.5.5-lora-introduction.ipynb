{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.5.5: Introduction to LoRA (Low-Rank Adaptation)\n",
    "\n",
    "**Module:** 2.5 - Hugging Face Ecosystem  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐⭐ (Intermediate-Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- [ ] Understand how LoRA reduces trainable parameters\n",
    "- [ ] Configure LoRA with the PEFT library\n",
    "- [ ] Compare memory usage: LoRA vs full fine-tuning\n",
    "- [ ] Train a model with LoRA adapters\n",
    "- [ ] Merge and save LoRA weights\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Labs 2.5.1 through 2.5.4\n",
    "- Knowledge of: Matrix operations, fine-tuning concepts\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "**The Fine-Tuning Dilemma**: You want to customize a 7B parameter model for your use case. Full fine-tuning requires:\n",
    "- ~28 GB for model weights (FP32)\n",
    "- ~56 GB for gradients and optimizer states\n",
    "- Total: ~84 GB minimum!\n",
    "\n",
    "**LoRA's Solution**: Only train ~0.1% of parameters. Same 7B model, but:\n",
    "- ~14 GB for model weights (frozen, BF16)\n",
    "- ~500 MB for LoRA adapters + gradients\n",
    "- Total: ~15 GB!\n",
    "\n",
    "This is why LoRA has become the go-to method for fine-tuning large models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: How Does LoRA Work?\n",
    "\n",
    "> **Imagine you're customizing a car...**\n",
    ">\n",
    "> **Full fine-tuning**: Replace every single part of the engine, transmission, interior - basically build a new car.\n",
    ">\n",
    "> **LoRA**: Keep the original car, just add a small turbo booster and a custom air filter. The car still works the same way, but now it's tuned for YOUR driving style.\n",
    ">\n",
    "> **The Math (simplified)**:\n",
    "> - Original weight matrix W: 1000 x 1000 = 1,000,000 parameters\n",
    "> - LoRA: Add A (1000 x 8) + B (8 x 1000) = 16,000 parameters\n",
    "> - Reduction: 98.4% fewer trainable parameters!\n",
    ">\n",
    "> **Key Insight**: Most of the \"knowledge\" is in the pretrained weights. We only need to add a small \"adjustment\" for our specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding LoRA Mathematically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# Check environment\n",
    "print(\"Environment Check\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize how LoRA works\n",
    "print(\"LoRA: Low-Rank Adaptation Explained\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Original weight matrix (pretrained)\n",
    "d_in = 768   # Input dimension (like BERT hidden size)\n",
    "d_out = 768  # Output dimension\n",
    "rank = 8     # LoRA rank (much smaller than d_in, d_out)\n",
    "\n",
    "print(f\"\\nOriginal linear layer: {d_in} x {d_out} = {d_in * d_out:,} parameters\")\n",
    "\n",
    "# Original weight matrix W (frozen during LoRA training)\n",
    "W = torch.randn(d_out, d_in)\n",
    "\n",
    "# LoRA decomposition: Instead of updating W directly,\n",
    "# we add a low-rank update: W' = W + BA\n",
    "# where B is d_out x rank and A is rank x d_in\n",
    "\n",
    "A = torch.randn(rank, d_in)  # \"Down projection\" \n",
    "B = torch.randn(d_out, rank)  # \"Up projection\"\n",
    "\n",
    "print(f\"\\nLoRA matrices:\")\n",
    "print(f\"  A (down): {rank} x {d_in} = {rank * d_in:,} parameters\")\n",
    "print(f\"  B (up):   {d_out} x {rank} = {d_out * rank:,} parameters\")\n",
    "print(f\"  Total LoRA: {rank * d_in + d_out * rank:,} parameters\")\n",
    "\n",
    "reduction = 1 - (rank * d_in + d_out * rank) / (d_in * d_out)\n",
    "print(f\"\\nParameter reduction: {reduction:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate LoRA forward pass\n",
    "print(\"\\nLoRA Forward Pass Demo\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Input vector\n",
    "x = torch.randn(1, d_in)  # Batch of 1, hidden_size input\n",
    "\n",
    "# Original output (without LoRA)\n",
    "original_output = x @ W.T\n",
    "\n",
    "# LoRA adds a low-rank update\n",
    "# h = x @ W.T + x @ A.T @ B.T\n",
    "# The BA product forms a low-rank matrix that \"adjusts\" the original weights\n",
    "\n",
    "lora_adjustment = x @ A.T @ B.T  # This is the \"delta\" from LoRA\n",
    "lora_output = original_output + lora_adjustment\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Original output shape: {original_output.shape}\")\n",
    "print(f\"LoRA adjustment shape: {lora_adjustment.shape}\")\n",
    "print(f\"Final output shape: {lora_output.shape}\")\n",
    "\n",
    "# The key insight: BA forms a rank-r matrix\n",
    "print(f\"\\nRank of BA matrix: {rank} (by construction)\")\n",
    "print(f\"Rank of original W: up to {min(d_in, d_out)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key LoRA Parameters\n",
    "\n",
    "| Parameter | Typical Value | Description |\n",
    "|-----------|---------------|-------------|\n",
    "| **rank (r)** | 8-64 | Rank of the low-rank matrices. Higher = more capacity, more params |\n",
    "| **alpha** | 16-32 | Scaling factor. Often set to 2*rank. Final scaling = alpha/rank |\n",
    "| **dropout** | 0.05-0.1 | Dropout applied to LoRA layers |\n",
    "| **target_modules** | [\"q_proj\", \"v_proj\"] | Which layers to apply LoRA to |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: LoRA with PEFT Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Load base model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device)\n",
    "\n",
    "# Count parameters before LoRA\n",
    "total_params = sum(p.numel() for p in base_model.parameters())\n",
    "trainable_before = sum(p.numel() for p in base_model.parameters() if p.requires_grad)\n",
    "print(f\"\\nBase model parameters: {total_params:,}\")\n",
    "print(f\"Trainable before LoRA: {trainable_before:,} (100%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore model structure to find target modules\n",
    "print(\"\\nModel layer names (looking for linear layers):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, module in base_model.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        print(f\"{name}: {module.in_features} -> {module.out_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Sequence classification\n",
    "    r=8,                         # Rank\n",
    "    lora_alpha=16,               # Scaling factor\n",
    "    lora_dropout=0.1,            # Dropout\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],  # DistilBERT uses these names\n",
    "    bias=\"none\",                 # Don't train biases\n",
    "    modules_to_save=[\"classifier\", \"pre_classifier\"]  # Train these normally\n",
    ")\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Rank: {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Scaling: {lora_config.lora_alpha / lora_config.r}\")\n",
    "print(f\"  Target modules: {lora_config.target_modules}\")\n",
    "print(f\"  Modules to save: {lora_config.modules_to_save}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to the model\n",
    "print(\"\\nApplying LoRA...\")\n",
    "peft_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Check trainable parameters\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "# Manual calculation\n",
    "trainable_after = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "total_after = sum(p.numel() for p in peft_model.parameters())\n",
    "\n",
    "print(f\"\\nDetailed breakdown:\")\n",
    "print(f\"  Total parameters: {total_after:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_after:,}\")\n",
    "print(f\"  Trainable %: {100 * trainable_after / total_after:.2f}%\")\n",
    "print(f\"  Reduction: {100 * (1 - trainable_after / trainable_before):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Memory Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def measure_training_memory(model, sample_input, sample_labels, optimizer_class=torch.optim.AdamW):\n",
    "    \"\"\"\n",
    "    Measure memory usage during a training step.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    # Move to GPU\n",
    "    model = model.to(device).train()\n",
    "    sample_input = {k: v.to(device) for k, v in sample_input.items()}\n",
    "    sample_labels = sample_labels.to(device)\n",
    "    \n",
    "    # Memory after model load\n",
    "    model_memory = torch.cuda.memory_allocated() / 1e9\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = optimizer_class(model.parameters(), lr=2e-5)\n",
    "    optimizer_memory = torch.cuda.memory_allocated() / 1e9 - model_memory\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(**sample_input, labels=sample_labels)\n",
    "    loss = outputs.loss\n",
    "    forward_memory = torch.cuda.memory_allocated() / 1e9\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    backward_memory = torch.cuda.memory_allocated() / 1e9\n",
    "    \n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "    \n",
    "    # Cleanup\n",
    "    del optimizer\n",
    "    \n",
    "    return {\n",
    "        \"model_memory_gb\": model_memory,\n",
    "        \"optimizer_memory_gb\": optimizer_memory,\n",
    "        \"forward_memory_gb\": forward_memory,\n",
    "        \"backward_memory_gb\": backward_memory,\n",
    "        \"peak_memory_gb\": peak_memory\n",
    "    }\n",
    "\n",
    "# Create sample input\n",
    "sample_text = \"This is a test sentence for memory measurement.\"\n",
    "sample_input = tokenizer(sample_text, return_tensors=\"pt\", padding=\"max_length\", max_length=128)\n",
    "sample_labels = torch.tensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure LoRA memory\n",
    "print(\"Measuring LoRA training memory...\")\n",
    "lora_memory = measure_training_memory(peft_model, sample_input, sample_labels)\n",
    "\n",
    "# Clean up for full fine-tuning test\n",
    "del peft_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fresh model for full fine-tuning comparison\n",
    "print(\"\\nMeasuring full fine-tuning memory...\")\n",
    "full_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "full_memory = measure_training_memory(full_model, sample_input, sample_labels)\n",
    "\n",
    "# Clean up\n",
    "del full_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare memory usage\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MEMORY COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n{'Metric':<25} {'Full Fine-tune':<15} {'LoRA':<15} {'Savings':>10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for key in lora_memory:\n",
    "    full_val = full_memory[key]\n",
    "    lora_val = lora_memory[key]\n",
    "    savings = (1 - lora_val / full_val) * 100 if full_val > 0 else 0\n",
    "    \n",
    "    metric_name = key.replace(\"_\", \" \").replace(\" gb\", \"\").title()\n",
    "    print(f\"{metric_name:<25} {full_val:<15.2f} {lora_val:<15.2f} {savings:>9.1f}%\")\n",
    "\n",
    "print(\"\\nKey Insight: LoRA significantly reduces optimizer memory!\")\n",
    "print(\"(Adam stores m & v for each trainable parameter)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Training with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import evaluate\n",
    "\n",
    "# Load dataset (smaller subset for demo)\n",
    "print(\"Loading IMDB dataset...\")\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Use smaller subsets for faster demo\n",
    "train_dataset = dataset['train'].shuffle(seed=42).select(range(5000))\n",
    "eval_dataset = dataset['test'].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "print(f\"Train: {len(train_dataset):,} examples\")\n",
    "print(f\"Eval: {len(eval_dataset):,} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing...\")\n",
    "tokenized_train = train_dataset.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "tokenized_eval = eval_dataset.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "\n",
    "tokenized_train = tokenized_train.rename_column(\"label\", \"labels\")\n",
    "tokenized_eval = tokenized_eval.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fresh LoRA model\n",
    "print(\"\\nCreating LoRA model...\")\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=16,              # Slightly higher rank for better performance\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],\n",
    "    modules_to_save=[\"classifier\", \"pre_classifier\"]\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(base_model, lora_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/lora_imdb\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=3e-4,  # Higher LR for LoRA is common\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    bf16=True,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"\\nStarting LoRA training...\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(\"-\" * 40)\n",
    "eval_results = trainer.evaluate()\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Saving and Loading LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save only the LoRA adapters (very small!)\n",
    "adapter_path = \"./results/lora_adapter\"\n",
    "peft_model.save_pretrained(adapter_path)\n",
    "\n",
    "print(f\"Adapter saved to {adapter_path}\")\n",
    "\n",
    "# Check size\n",
    "import os\n",
    "total_size = 0\n",
    "print(\"\\nSaved files:\")\n",
    "for f in os.listdir(adapter_path):\n",
    "    size = os.path.getsize(os.path.join(adapter_path, f)) / 1e6\n",
    "    total_size += size\n",
    "    print(f\"  {f}: {size:.2f} MB\")\n",
    "print(f\"\\nTotal adapter size: {total_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load adapter onto a fresh base model\n",
    "from peft import PeftModel\n",
    "\n",
    "print(\"\\nLoading adapter onto fresh base model...\")\n",
    "\n",
    "# Load base model\n",
    "fresh_base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "loaded_model = PeftModel.from_pretrained(fresh_base, adapter_path)\n",
    "loaded_model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\nAdapter loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the loaded model\n",
    "loaded_model = loaded_model.to(device).eval()\n",
    "\n",
    "test_texts = [\n",
    "    \"This movie was absolutely fantastic! A masterpiece!\",\n",
    "    \"Terrible film. Waste of time and money.\",\n",
    "    \"It was okay, nothing special.\"\n",
    "]\n",
    "\n",
    "print(\"\\nTesting loaded model:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for text in test_texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1).item()\n",
    "        conf = probs[0][pred].item()\n",
    "    \n",
    "    sentiment = \"POSITIVE\" if pred == 1 else \"NEGATIVE\"\n",
    "    print(f\"{sentiment} ({conf:.1%}): {text[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Merging LoRA Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA weights into base model for deployment\n",
    "# This creates a standard model (no PEFT overhead)\n",
    "\n",
    "print(\"Merging LoRA weights into base model...\")\n",
    "merged_model = loaded_model.merge_and_unload()\n",
    "\n",
    "print(f\"\\nMerged model type: {type(merged_model).__name__}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in merged_model.parameters()):,}\")\n",
    "\n",
    "# All parameters are now regular (not LoRA)\n",
    "trainable = sum(p.numel() for p in merged_model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable: {trainable:,} (100% - it's a regular model now!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged model\n",
    "merged_path = \"./results/merged_model\"\n",
    "merged_model.save_pretrained(merged_path)\n",
    "tokenizer.save_pretrained(merged_path)\n",
    "\n",
    "print(f\"\\nMerged model saved to {merged_path}\")\n",
    "\n",
    "# Check size\n",
    "total_size = 0\n",
    "print(\"\\nSaved files:\")\n",
    "for f in os.listdir(merged_path):\n",
    "    size = os.path.getsize(os.path.join(merged_path, f)) / 1e6\n",
    "    total_size += size\n",
    "    if size > 1:\n",
    "        print(f\"  {f}: {size:.1f} MB\")\n",
    "print(f\"\\nTotal merged model size: {total_size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: LoRA Rank Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different LoRA ranks\n",
    "print(\"LoRA Rank Comparison (Theoretical)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ranks = [4, 8, 16, 32, 64]\n",
    "d = 768  # DistilBERT hidden size\n",
    "num_layers = 6  # DistilBERT has 6 layers\n",
    "num_targets = 2  # q_lin and v_lin\n",
    "\n",
    "base_params = 66_955_010  # DistilBERT-base\n",
    "\n",
    "print(f\"\\n{'Rank':<8} {'LoRA Params':<15} {'% of Base':<12} {'Estimated Acc':>15}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for r in ranks:\n",
    "    # Each LoRA layer adds: r*d_in + r*d_out parameters\n",
    "    lora_per_layer = 2 * r * d  # A and B matrices\n",
    "    total_lora = lora_per_layer * num_targets * num_layers\n",
    "    \n",
    "    pct = 100 * total_lora / base_params\n",
    "    \n",
    "    # Rough accuracy estimate (higher rank generally = better)\n",
    "    est_acc = 0.88 + 0.02 * np.log2(r / 4)\n",
    "    est_acc = min(est_acc, 0.93)  # Cap at base model performance\n",
    "    \n",
    "    print(f\"{r:<8} {total_lora:<15,} {pct:<12.3f} {est_acc:>14.1%}\")\n",
    "\n",
    "print(\"\\nNote: Actual accuracy depends heavily on task and data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Wrong Target Modules\n",
    "\n",
    "```python\n",
    "# Wrong: Using BERT module names for DistilBERT\n",
    "lora_config = LoraConfig(\n",
    "    target_modules=[\"query\", \"value\"]  # BERT uses these\n",
    ")\n",
    "\n",
    "# Right: Check model's actual layer names\n",
    "lora_config = LoraConfig(\n",
    "    target_modules=[\"q_lin\", \"v_lin\"]  # DistilBERT uses these\n",
    ")\n",
    "```\n",
    "\n",
    "### Mistake 2: Too Low Learning Rate\n",
    "\n",
    "```python\n",
    "# Wrong: Using full fine-tuning LR\n",
    "args = TrainingArguments(learning_rate=2e-5)\n",
    "\n",
    "# Right: LoRA often needs higher LR\n",
    "args = TrainingArguments(learning_rate=3e-4)  # 10-15x higher\n",
    "```\n",
    "\n",
    "### Mistake 3: Forgetting modules_to_save\n",
    "\n",
    "```python\n",
    "# Wrong: Only LoRA layers are trained, classifier stays random!\n",
    "lora_config = LoraConfig(\n",
    "    target_modules=[\"q_lin\", \"v_lin\"]\n",
    ")\n",
    "\n",
    "# Right: Also train the classifier head\n",
    "lora_config = LoraConfig(\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],\n",
    "    modules_to_save=[\"classifier\"]  # Train this normally\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- The mathematical intuition behind LoRA\n",
    "- How to configure LoRA with PEFT\n",
    "- Memory benefits of LoRA vs full fine-tuning\n",
    "- How to train, save, and load LoRA adapters\n",
    "- How to merge LoRA weights for deployment\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n",
    "- [PEFT Documentation](https://huggingface.co/docs/peft)\n",
    "- [QLoRA Paper](https://arxiv.org/abs/2305.14314) (LoRA + Quantization)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del peft_model, loaded_model, merged_model, trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"GPU memory after cleanup: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "print(\"\\nLab 2.5.5 complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
