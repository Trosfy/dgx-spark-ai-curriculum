{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 2.5.2: Pipeline Showcase\n\n**Module:** 2.5 - Hugging Face Ecosystem  \n**Time:** 2 hours  \n**Difficulty:** â­â­ (Intermediate)\n\n---\n\n## Learning Objectives\n\nBy the end of this lab, you will:\n- [ ] Master the `pipeline()` API for rapid prototyping\n- [ ] Use 5 different pipeline types (sentiment, NER, QA, summarization, text-generation)\n- [ ] Understand when to use pipelines vs. manual model loading\n- [ ] Measure and compare pipeline performance\n\n---\n\n## Prerequisites\n\n- Completed: Lab 2.5.1 (Hub Exploration)\n- Knowledge of: Basic NLP concepts (tokens, embeddings)\n\n---\n\n## DGX Spark Optimization Notes\n\nWhen using pipelines on DGX Spark (128GB unified memory, Blackwell GB10):\n- Always use `torch_dtype=torch.bfloat16` for GPU inference (native Blackwell support)\n- Use `device=0` for GPU (not `device=\"cuda\"`)\n- Clean up between pipelines: `del pipeline; gc.collect(); torch.cuda.empty_cache()`\n- For production inference, consider Module 3.3 (SGLang, vLLM) for higher throughput\n\n---\n\n## Real-World Context\n\n**The Swiss Army Knife of AI**: Imagine you're building a customer support dashboard. You need to:\n- Analyze sentiment of incoming messages\n- Extract key entities (product names, dates, prices)\n- Answer common questions automatically\n- Summarize long customer complaints\n- Generate response drafts\n\nWith **Hugging Face Pipelines**, you can prototype ALL of these in under 10 minutes!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: What is a Pipeline?\n",
    "\n",
    "> **Imagine you want to make a smoothie...**\n",
    ">\n",
    "> The hard way: Get fruits, peel them, chop them, put in blender, blend at right speed, pour into glass.\n",
    ">\n",
    "> The pipeline way: Put everything in a smoothie maker with one button - it does ALL the steps automatically!\n",
    ">\n",
    "> **In AI terms:** A pipeline wraps tokenization + model loading + inference + post-processing into a single function call:\n",
    "> ```python\n",
    "> # Instead of 50 lines of code...\n",
    "> result = pipeline(\"sentiment-analysis\")(\"I love this!\")\n",
    "> # Output: [{'label': 'POSITIVE', 'score': 0.9998}]\n",
    "> ```\n",
    ">\n",
    "> It's the fastest way to go from \"I have text\" to \"I have AI predictions\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup and Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check environment\n",
    "print(\"Environment Check\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Set device for pipelines\n",
    "DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"\\nUsing device: {'GPU' if DEVICE >= 0 else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Your First Pipeline\n",
    "\n",
    "Let's start with the simplest possible example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simplest AI application - ONE line of code!\n",
    "classifier = pipeline(\"sentiment-analysis\", device=DEVICE)\n",
    "\n",
    "# Classify a single text\n",
    "result = classifier(\"I love learning about AI! This is so exciting!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "In that ONE line, the pipeline:\n",
    "1. Downloaded a pre-trained model (distilbert-base-uncased-finetuned-sst-2-english)\n",
    "2. Downloaded the matching tokenizer\n",
    "3. Tokenized your input text\n",
    "4. Ran inference on the model\n",
    "5. Converted logits to probabilities\n",
    "6. Returned a human-readable result\n",
    "\n",
    "That's what we mean by \"Swiss Army Knife\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The Five Essential Pipelines\n",
    "\n",
    "### Pipeline 1: Sentiment Analysis (Text Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PIPELINE 1: SENTIMENT ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nUse case: Understanding if customer feedback is positive or negative\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Create sentiment pipeline (already loaded above, but let's be explicit)\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    device=DEVICE,\n",
    "    torch_dtype=torch.bfloat16 if DEVICE >= 0 else torch.float32\n",
    ")\n",
    "\n",
    "# Test examples\n",
    "test_reviews = [\n",
    "    \"This product exceeded my expectations! Absolutely wonderful!\",\n",
    "    \"Terrible experience. The product arrived broken and customer service was unhelpful.\",\n",
    "    \"It's okay. Does what it's supposed to do, nothing more.\",\n",
    "    \"Best purchase I've made this year! Highly recommend!\",\n",
    "    \"Waste of money. Don't buy this.\"\n",
    "]\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(\"-\"*70)\n",
    "for review in test_reviews:\n",
    "    result = sentiment_analyzer(review)[0]\n",
    "    emoji = \"\" if result['label'] == 'POSITIVE' else \"\"\n",
    "    print(f\"{emoji} [{result['label']:8}] ({result['score']:.1%}) | {review[:50]}...\")\n",
    "\n",
    "# Batch processing (more efficient)\n",
    "print(\"\\n\\nBatch Processing (more efficient):\")\n",
    "start = time.time()\n",
    "batch_results = sentiment_analyzer(test_reviews)\n",
    "print(f\"Processed {len(test_reviews)} reviews in {(time.time()-start)*1000:.1f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 2: Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PIPELINE 2: NAMED ENTITY RECOGNITION (NER)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nUse case: Extract names, organizations, locations from text\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Clean up previous model\n",
    "del sentiment_analyzer\n",
    "torch.cuda.empty_cache() if DEVICE >= 0 else None\n",
    "\n",
    "# Create NER pipeline\n",
    "ner = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"dslim/bert-base-NER\",\n",
    "    aggregation_strategy=\"simple\",  # Groups tokens into entities\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# Test examples\n",
    "test_texts = [\n",
    "    \"Apple CEO Tim Cook announced the new iPhone 15 at the Cupertino headquarters.\",\n",
    "    \"Elon Musk's company SpaceX launched a Falcon 9 rocket from Cape Canaveral, Florida.\",\n",
    "    \"The European Union and United States signed a trade agreement in Brussels.\"\n",
    "]\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "for text in test_texts:\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(\"Entities:\")\n",
    "    entities = ner(text)\n",
    "    for ent in entities:\n",
    "        print(f\"  - {ent['entity_group']:12} : '{ent['word']}' (confidence: {ent['score']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 3: Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\nprint(\"PIPELINE 3: QUESTION ANSWERING\")\nprint(\"=\"*70)\nprint(\"\\nUse case: Extract answers from documents (extractive QA)\")\nprint(\"-\"*70)\n\n# Clean up\ndel ner\ntorch.cuda.empty_cache() if DEVICE >= 0 else None\n\n# Create QA pipeline\nqa = pipeline(\n    \"question-answering\",\n    model=\"deepset/roberta-base-squad2\",\n    device=DEVICE\n)\n\n# Context about DGX Spark (imagine this is a product document)\ncontext = \"\"\"\nThe NVIDIA DGX Spark is a personal AI supercomputer designed for developers \nand researchers. It features the Blackwell GB10 Superchip with 128GB of unified \nLPDDR5X memory shared between CPU and GPU. The system delivers 1 PFLOP of NVFP4 \nAI performance and can run models up to 200 billion parameters. It was announced \nat CES 2025 and is expected to ship in May 2025 with a starting price around $3,000.\n\"\"\"\n\nquestions = [\n    \"How much memory does the DGX Spark have?\",\n    \"What is the price of DGX Spark?\",\n    \"When will DGX Spark ship?\",\n    \"How many parameters can it handle?\"\n]\n\nprint(\"\\nContext: (DGX Spark product info)\")\nprint(\"-\"*70)\nprint(context.strip())\nprint(\"\\nQuestions & Answers:\")\nprint(\"-\"*70)\n\nfor question in questions:\n    result = qa(question=question, context=context)\n    print(f\"Q: {question}\")\n    print(f\"A: {result['answer']} (confidence: {result['score']:.1%})\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 4: Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PIPELINE 4: TEXT SUMMARIZATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nUse case: Condense long documents into key points\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Clean up\n",
    "del qa\n",
    "torch.cuda.empty_cache() if DEVICE >= 0 else None\n",
    "\n",
    "# Create summarization pipeline\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"facebook/bart-large-cnn\",\n",
    "    device=DEVICE,\n",
    "    torch_dtype=torch.bfloat16 if DEVICE >= 0 else torch.float32\n",
    ")\n",
    "\n",
    "# Long article to summarize\n",
    "article = \"\"\"\n",
    "Artificial intelligence has transformed the technology landscape in unprecedented ways \n",
    "over the past decade. Large language models, which can understand and generate human-like \n",
    "text, have become increasingly powerful and accessible. Companies like OpenAI, Google, \n",
    "and Anthropic have released models capable of writing code, answering complex questions, \n",
    "and even passing professional exams.\n",
    "\n",
    "The implications of these advances extend far beyond the tech industry. Healthcare \n",
    "professionals are using AI to diagnose diseases and develop new treatments. Financial \n",
    "institutions employ machine learning algorithms to detect fraud and make investment \n",
    "decisions. Even creative fields like art and music are being influenced by generative \n",
    "AI tools that can produce original works.\n",
    "\n",
    "However, these developments have also raised significant concerns. Issues around job \n",
    "displacement, privacy, bias in AI systems, and the potential misuse of the technology \n",
    "have prompted calls for regulation and ethical guidelines. Many experts argue that \n",
    "while AI offers tremendous benefits, society must carefully consider how to deploy \n",
    "these powerful tools responsibly.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Original article ({len(article.split())} words):\")\n",
    "print(\"-\"*70)\n",
    "print(article.strip())\n",
    "\n",
    "print(\"\\n\\nSummary:\")\n",
    "print(\"-\"*70)\n",
    "summary = summarizer(\n",
    "    article,\n",
    "    max_length=100,\n",
    "    min_length=30,\n",
    "    do_sample=False\n",
    ")\n",
    "print(summary[0]['summary_text'])\n",
    "print(f\"\\n(Reduced to {len(summary[0]['summary_text'].split())} words)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 5: Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PIPELINE 5: TEXT GENERATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nUse case: Generate creative text, code, or continue a prompt\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Clean up\n",
    "del summarizer\n",
    "torch.cuda.empty_cache() if DEVICE >= 0 else None\n",
    "\n",
    "# Create text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2-medium\",  # Larger than gpt2-small, still fits easily on DGX Spark\n",
    "    device=DEVICE,\n",
    "    torch_dtype=torch.bfloat16 if DEVICE >= 0 else torch.float32\n",
    ")\n",
    "\n",
    "# Generation prompts\n",
    "prompts = [\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"In a world where robots and humans coexist,\",\n",
    "    \"The most important skill for the 21st century is\"\n",
    "]\n",
    "\n",
    "print(\"\\nGenerations:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(\"Generated:\")\n",
    "    \n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=50,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=generator.tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Show continuation (removing the prompt)\n",
    "    full_text = result[0]['generated_text']\n",
    "    continuation = full_text[len(prompt):]\n",
    "    print(f\"  {continuation.strip()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Pipeline Performance Comparison\n",
    "\n",
    "Let's measure and compare the performance of our pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up previous pipeline\n",
    "del generator\n",
    "torch.cuda.empty_cache() if DEVICE >= 0 else None\n",
    "\n",
    "def benchmark_pipeline(\n",
    "    task: str,\n",
    "    model: str,\n",
    "    test_input: Any,\n",
    "    warmup_runs: int = 3,\n",
    "    test_runs: int = 10\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Benchmark a pipeline's performance.\n",
    "    \n",
    "    Returns dict with load_time, inference_time, memory_gb\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache() if DEVICE >= 0 else None\n",
    "    initial_memory = torch.cuda.memory_allocated() / 1e9 if DEVICE >= 0 else 0\n",
    "    \n",
    "    # Time loading\n",
    "    start = time.time()\n",
    "    pipe = pipeline(\n",
    "        task,\n",
    "        model=model,\n",
    "        device=DEVICE,\n",
    "        torch_dtype=torch.bfloat16 if DEVICE >= 0 else torch.float32\n",
    "    )\n",
    "    load_time = time.time() - start\n",
    "    \n",
    "    memory_used = (torch.cuda.memory_allocated() / 1e9 - initial_memory) if DEVICE >= 0 else 0\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(warmup_runs):\n",
    "        _ = pipe(test_input)\n",
    "    \n",
    "    # Benchmark inference\n",
    "    if DEVICE >= 0:\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    times = []\n",
    "    for _ in range(test_runs):\n",
    "        start = time.time()\n",
    "        _ = pipe(test_input)\n",
    "        if DEVICE >= 0:\n",
    "            torch.cuda.synchronize()\n",
    "        times.append((time.time() - start) * 1000)\n",
    "    \n",
    "    # Clean up\n",
    "    del pipe\n",
    "    torch.cuda.empty_cache() if DEVICE >= 0 else None\n",
    "    \n",
    "    return {\n",
    "        \"load_time_s\": load_time,\n",
    "        \"inference_mean_ms\": sum(times) / len(times),\n",
    "        \"inference_std_ms\": (sum((t - sum(times)/len(times))**2 for t in times) / len(times)) ** 0.5,\n",
    "        \"memory_gb\": memory_used,\n",
    "        \"throughput_per_sec\": 1000 / (sum(times) / len(times))\n",
    "    }\n",
    "\n",
    "# Benchmark configurations\n",
    "benchmarks = [\n",
    "    (\"sentiment-analysis\", \"distilbert-base-uncased-finetuned-sst-2-english\", \"This is a test sentence.\"),\n",
    "    (\"ner\", \"dslim/bert-base-NER\", \"Apple CEO Tim Cook announced new products.\"),\n",
    "    (\"question-answering\", \"deepset/roberta-base-squad2\", {\"question\": \"What is AI?\", \"context\": \"AI is artificial intelligence.\"}),\n",
    "]\n",
    "\n",
    "print(\"\\nPIPELINE BENCHMARKS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Task':<25} {'Load(s)':<10} {'Infer(ms)':<12} {'Memory(GB)':<12} {'Throughput':>12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "results = []\n",
    "for task, model, test_input in benchmarks:\n",
    "    print(f\"Benchmarking {task}...\", end=\" \", flush=True)\n",
    "    metrics = benchmark_pipeline(task, model, test_input)\n",
    "    results.append((task, metrics))\n",
    "    print(f\"\\r{task:<25} {metrics['load_time_s']:<10.2f} {metrics['inference_mean_ms']:<12.2f} {metrics['memory_gb']:<12.2f} {metrics['throughput_per_sec']:>10.1f}/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Advanced Pipeline Usage\n",
    "\n",
    "### 4.1 Zero-Shot Classification\n",
    "\n",
    "Classify text into categories the model has never seen during training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BONUS: ZERO-SHOT CLASSIFICATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nClassify text into ANY categories - no training needed!\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Create zero-shot classifier\n",
    "zero_shot = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\",\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# Custom categories\n",
    "categories = [\"technology\", \"sports\", \"politics\", \"entertainment\", \"business\"]\n",
    "\n",
    "test_headlines = [\n",
    "    \"NVIDIA announces new AI chip with breakthrough performance\",\n",
    "    \"Lakers defeat Celtics in overtime thriller\",\n",
    "    \"Senate passes new infrastructure bill with bipartisan support\",\n",
    "    \"Marvel announces new superhero movie for 2025\",\n",
    "    \"Tech startup raises $500 million in Series C funding\"\n",
    "]\n",
    "\n",
    "print(f\"\\nCategories: {categories}\")\n",
    "print(\"\\nResults:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for headline in test_headlines:\n",
    "    result = zero_shot(headline, candidate_labels=categories)\n",
    "    top_label = result['labels'][0]\n",
    "    top_score = result['scores'][0]\n",
    "    print(f\"[{top_label:12}] ({top_score:.1%}) | {headline}\")\n",
    "\n",
    "# Clean up\n",
    "del zero_shot\n",
    "torch.cuda.empty_cache() if DEVICE >= 0 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Fill-Mask (Understanding Language Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BONUS: FILL-MASK\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nSee how language models understand context!\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Create fill-mask pipeline\n",
    "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\", device=DEVICE)\n",
    "\n",
    "# Test sentences with [MASK]\n",
    "masked_sentences = [\n",
    "    \"The capital of France is [MASK].\",\n",
    "    \"I went to the [MASK] to buy groceries.\",\n",
    "    \"The doctor prescribed [MASK] for my headache.\",\n",
    "    \"Python is a popular [MASK] language.\"\n",
    "]\n",
    "\n",
    "for sentence in masked_sentences:\n",
    "    print(f\"\\nInput: {sentence}\")\n",
    "    results = fill_mask(sentence)\n",
    "    print(\"Top predictions:\")\n",
    "    for r in results[:3]:\n",
    "        print(f\"  - '{r['token_str']}' ({r['score']:.1%})\")\n",
    "\n",
    "# Clean up\n",
    "del fill_mask\n",
    "torch.cuda.empty_cache() if DEVICE >= 0 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: When NOT to Use Pipelines\n",
    "\n",
    "Pipelines are great for prototyping, but sometimes you need more control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPIPELINE vs MANUAL LOADING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison = \"\"\"\n",
    "USE PIPELINE WHEN:\n",
    "  Quick prototyping and experimentation\n",
    "  Standard tasks with default settings\n",
    "  Single-model inference\n",
    "  Batch processing with simple inputs\n",
    "\n",
    "USE MANUAL LOADING WHEN:\n",
    "  Need custom preprocessing/postprocessing\n",
    "  Fine-tuning or training\n",
    "  Need access to hidden states or attention weights\n",
    "  Complex multi-model pipelines\n",
    "  Maximum performance optimization\n",
    "  Need to modify model architecture\n",
    "\"\"\"\n",
    "print(comparison)\n",
    "\n",
    "# Example: Manual loading gives you more control\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "print(\"\\nExample: Manual Loading for More Control\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Manual loading\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(\"cuda\" if DEVICE >= 0 else \"cpu\")\n",
    "\n",
    "# Now you can access everything!\n",
    "text = \"This is an amazing product!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True, output_attentions=True)\n",
    "\n",
    "print(f\"Logits shape: {outputs.logits.shape}\")\n",
    "print(f\"Number of hidden states: {len(outputs.hidden_states)}\")\n",
    "print(f\"Number of attention layers: {len(outputs.attentions)}\")\n",
    "print(f\"Attention shape (per layer): {outputs.attentions[0].shape}\")\n",
    "\n",
    "# You can analyze attention patterns, extract embeddings, etc.\n",
    "print(\"\\nWith manual loading, you can:\")\n",
    "print(\"  - Access all hidden states for each layer\")\n",
    "print(\"  - Visualize attention patterns\")\n",
    "print(\"  - Extract embeddings for downstream tasks\")\n",
    "print(\"  - Modify inference behavior\")\n",
    "\n",
    "# Clean up\n",
    "del model, tokenizer\n",
    "torch.cuda.empty_cache() if DEVICE >= 0 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Try It Yourself: Build a Multi-Pipeline App\n\nCombine multiple pipelines to analyze customer feedback:\n\n**Your Task:** Complete the `analyze_customer_feedback()` function to:\n1. **Sentiment Analysis** - Determine if the feedback is positive or negative\n2. **Named Entity Recognition** - Extract product names, people, organizations, locations\n3. **Summarization** - If text is long (> 100 words), provide a summary\n\n<details>\n<summary>ðŸ’¡ Hint: Pipeline Creation</summary>\n\n```python\n# Create each pipeline (remember to clean up between them!)\nsentiment_pipe = pipeline(\"sentiment-analysis\", device=DEVICE)\nner_pipe = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\", device=DEVICE)\nsummarizer_pipe = pipeline(\"summarization\", device=DEVICE)\n```\n</details>\n\n<details>\n<summary>ðŸ’¡ Hint: Checking Text Length</summary>\n\n```python\n# Count words in text\nword_count = len(text.split())\nif word_count > 100:\n    # Summarize long text\n    summary = summarizer_pipe(text, max_length=50, min_length=20)[0]['summary_text']\n```\n</details>\n\n<details>\n<summary>ðŸ’¡ Hint: Complete Solution Structure</summary>\n\n```python\ndef analyze_customer_feedback(text: str) -> Dict[str, Any]:\n    result = {\"original_text\": text, \"sentiment\": None, \"entities\": [], \"summary\": None}\n    \n    # 1. Sentiment\n    sentiment_pipe = pipeline(\"sentiment-analysis\", device=DEVICE)\n    sentiment_result = sentiment_pipe(text)[0]\n    result[\"sentiment\"] = {\"label\": sentiment_result[\"label\"], \"score\": sentiment_result[\"score\"]}\n    del sentiment_pipe; torch.cuda.empty_cache()\n    \n    # 2. NER\n    ner_pipe = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\", device=DEVICE)\n    entities = ner_pipe(text)\n    result[\"entities\"] = [{\"entity\": e[\"entity_group\"], \"word\": e[\"word\"]} for e in entities]\n    del ner_pipe; torch.cuda.empty_cache()\n    \n    # 3. Summary (if long)\n    if len(text.split()) > 100:\n        summarizer = pipeline(\"summarization\", device=DEVICE)\n        result[\"summary\"] = summarizer(text, max_length=50, min_length=20)[0]['summary_text']\n        del summarizer; torch.cuda.empty_cache()\n    \n    return result\n```\n</details>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create a function that takes customer feedback and returns:\n",
    "# 1. Sentiment (positive/negative)\n",
    "# 2. Key entities mentioned\n",
    "# 3. A summary if the text is long\n",
    "\n",
    "def analyze_customer_feedback(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Comprehensive customer feedback analysis.\n",
    "    \n",
    "    Args:\n",
    "        text: Customer feedback text\n",
    "        \n",
    "    Returns:\n",
    "        Dict with sentiment, entities, and summary\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"original_text\": text,\n",
    "        \"sentiment\": None,\n",
    "        \"entities\": [],\n",
    "        \"summary\": None\n",
    "    }\n",
    "    \n",
    "    # TODO: Add sentiment analysis\n",
    "    # TODO: Add NER\n",
    "    # TODO: Add summarization if text is long (> 100 words)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test with a sample review\n",
    "sample_feedback = \"\"\"\n",
    "I bought the iPhone 15 Pro from Apple Store in San Francisco last week. \n",
    "The sales representative John was incredibly helpful and knowledgeable. \n",
    "However, I'm disappointed with the battery life - it barely lasts a full day \n",
    "with normal usage. The camera quality is excellent though, especially for \n",
    "low-light photography. I've contacted Apple Support and they suggested a \n",
    "software update might help. Overall, it's a mixed experience but I'm hopeful \n",
    "the issues will be resolved.\n",
    "\"\"\"\n",
    "\n",
    "# Uncomment after implementing:\n",
    "# analysis = analyze_customer_feedback(sample_feedback)\n",
    "# print(json.dumps(analysis, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Common Mistakes\n\n### Mistake 1: Not Cleaning Up Between Pipelines\n\n```python\n# Wrong: Memory keeps accumulating\npipe1 = pipeline(\"sentiment-analysis\", device=0)\npipe2 = pipeline(\"ner\", device=0)  # OOM on smaller GPUs!\n\n# Right: Standard cleanup pattern\npipe1 = pipeline(\"sentiment-analysis\", device=0)\n# ... use pipe1 ...\ndel pipe1\ngc.collect()  # Python garbage collection first\ntorch.cuda.empty_cache()  # Then release CUDA memory\npipe2 = pipeline(\"ner\", device=0)  # Works!\n```\n\n### Mistake 2: Forgetting batch_size for Large Datasets\n\n```python\n# Wrong: Processes one by one (slow)\nfor text in large_list:\n    result = pipe(text)\n\n# Right: Batch processing (much faster!)\nresults = pipe(large_list, batch_size=32)\n```\n\n### Mistake 3: Using Wrong Device Index\n\n```python\n# Wrong: device=\"cuda\" (string)\npipe = pipeline(\"sentiment-analysis\", device=\"cuda\")\n\n# Right: device=0 (integer for GPU index, -1 for CPU)\npipe = pipeline(\"sentiment-analysis\", device=0)\n```\n\n### Mistake 4: Not Using BFloat16 on DGX Spark\n\n```python\n# Wrong: Uses default FP32, wastes memory\npipe = pipeline(\"text-generation\", device=0)\n\n# Right: Use BF16 for Blackwell GPU (native support)\npipe = pipeline(\n    \"text-generation\",\n    device=0,\n    torch_dtype=torch.bfloat16\n)\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- How to use 5 essential pipeline types\n",
    "- How to benchmark pipeline performance\n",
    "- Advanced pipelines: zero-shot classification, fill-mask\n",
    "- When to use pipelines vs. manual model loading\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Pipeline Documentation](https://huggingface.co/docs/transformers/main_classes/pipelines)\n",
    "- [Available Tasks](https://huggingface.co/tasks)\n",
    "- [Inference Optimization](https://huggingface.co/docs/transformers/perf_infer_gpu_one)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "print(\"\\nLab 2.5.2 complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}