{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 2.5.3: Dataset Processing with Hugging Face Datasets\n\n**Module:** 2.5 - Hugging Face Ecosystem  \n**Time:** 2 hours  \n**Difficulty:** ⭐⭐ (Intermediate)\n\n---\n\n## Learning Objectives\n\nBy the end of this lab, you will:\n- [ ] Load datasets from the Hugging Face Hub\n- [ ] Apply transformations with `map()` and `filter()`\n- [ ] Create train/validation/test splits\n- [ ] Handle large datasets with streaming\n- [ ] Prepare datasets for training with tokenization\n\n---\n\n## Prerequisites\n\n- Completed: Labs 2.5.1 and 2.5.2\n- Knowledge of: Tokenization concepts, PyTorch DataLoader basics\n\n---\n\n## DGX Spark Advantage\n\nWith DGX Spark's 128GB unified memory, you can:\n- Load larger datasets directly into memory (no streaming needed for most datasets)\n- Use more parallel workers (`num_proc=4-8`) for faster preprocessing\n- Process entire datasets without batching constraints\n- Cache tokenized datasets for faster iteration\n\n---\n\n## Real-World Context\n\n**The Data Pipeline Challenge**: You're building a sentiment classifier for product reviews. You have:\n- 1 million reviews from multiple sources\n- Mixed languages, varying lengths\n- Imbalanced classes (80% positive, 20% negative)\n\nThe Hugging Face **Datasets** library handles all of this efficiently, with:\n- Memory-mapped data (process datasets larger than RAM)\n- Parallel processing with `num_proc`\n- Built-in streaming for huge datasets\n- One-line integration with the Trainer API"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: Datasets Library\n",
    "\n",
    "> **Imagine you're organizing a massive library...**\n",
    ">\n",
    "> The old way: Load every book into your arms, then sort them. Your arms get tired fast!\n",
    ">\n",
    "> The Datasets way: Use a magical catalog that lets you:\n",
    "> - **Point** to books without carrying them (memory-mapping)\n",
    "> - **Clone yourself** to sort faster (parallel processing)\n",
    "> - **Read page by page** without loading the whole book (streaming)\n",
    ">\n",
    "> **In AI terms:** It's a library that handles millions of examples efficiently:\n",
    "> ```python\n",
    "> # 1 million examples? No problem!\n",
    "> dataset = load_dataset(\"imdb\")  # Downloads once, cached forever\n",
    "> dataset = dataset.map(tokenize, num_proc=8)  # 8x parallel processing\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from datasets import concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "print(\"Environment Check\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loading from the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB dataset - a classic for sentiment analysis\n",
    "print(\"Loading IMDB dataset...\")\n",
    "start = time.time()\n",
    "imdb = load_dataset(\"imdb\")\n",
    "print(f\"Loaded in {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# Explore the structure\n",
    "print(\"\\nDataset Structure:\")\n",
    "print(imdb)\n",
    "\n",
    "print(\"\\nSplits available:\")\n",
    "for split in imdb:\n",
    "    print(f\"  {split}: {len(imdb[split]):,} examples\")\n",
    "\n",
    "print(\"\\nFeatures (columns):\")\n",
    "print(imdb['train'].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a single example\n",
    "print(\"\\nSample from training set:\")\n",
    "print(\"-\" * 60)\n",
    "sample = imdb['train'][0]\n",
    "print(f\"Label: {sample['label']} ({'positive' if sample['label'] == 1 else 'negative'})\")\n",
    "print(f\"Text preview: {sample['text'][:300]}...\")\n",
    "print(f\"Text length: {len(sample['text'])} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Loading Dataset Subsets and Specific Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load only specific split\n",
    "train_only = load_dataset(\"imdb\", split=\"train\")\n",
    "print(f\"Train only: {len(train_only):,} examples\")\n",
    "\n",
    "# Load a slice (for quick testing)\n",
    "small_train = load_dataset(\"imdb\", split=\"train[:1000]\")\n",
    "print(f\"Small train (first 1000): {len(small_train):,} examples\")\n",
    "\n",
    "# Load percentage\n",
    "train_10pct = load_dataset(\"imdb\", split=\"train[:10%]\")\n",
    "print(f\"10% of train: {len(train_10pct):,} examples\")\n",
    "\n",
    "# Load with train/test merged\n",
    "all_data = load_dataset(\"imdb\", split=\"train+test\")\n",
    "print(f\"All data combined: {len(all_data):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Loading Datasets with Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many datasets have configurations (subsets)\n",
    "# GLUE is a benchmark with multiple tasks\n",
    "\n",
    "print(\"Loading GLUE SST-2 (Stanford Sentiment Treebank):\")\n",
    "sst2 = load_dataset(\"glue\", \"sst2\")\n",
    "print(sst2)\n",
    "\n",
    "print(\"\\nSample:\")\n",
    "print(sst2['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(dataset, text_column='text', label_column='label', name='Dataset'):\n",
    "    \"\"\"\n",
    "    Comprehensive dataset analysis.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DATASET ANALYSIS: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\nSize: {len(dataset):,} examples\")\n",
    "    print(f\"Columns: {dataset.column_names}\")\n",
    "    print(f\"Features: {dataset.features}\")\n",
    "    \n",
    "    # Label distribution\n",
    "    if label_column in dataset.column_names:\n",
    "        labels = dataset[label_column]\n",
    "        label_counts = Counter(labels)\n",
    "        print(f\"\\nLabel Distribution:\")\n",
    "        total = len(labels)\n",
    "        for label, count in sorted(label_counts.items()):\n",
    "            pct = 100 * count / total\n",
    "            print(f\"  Label {label}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Text length statistics\n",
    "    if text_column in dataset.column_names:\n",
    "        # Sample for efficiency\n",
    "        sample_size = min(1000, len(dataset))\n",
    "        lengths = [len(dataset[i][text_column]) for i in range(sample_size)]\n",
    "        \n",
    "        print(f\"\\nText Length (characters) - sampled {sample_size}:\")\n",
    "        print(f\"  Mean: {np.mean(lengths):.0f}\")\n",
    "        print(f\"  Std: {np.std(lengths):.0f}\")\n",
    "        print(f\"  Min: {min(lengths)}\")\n",
    "        print(f\"  Max: {max(lengths)}\")\n",
    "        print(f\"  Median: {np.median(lengths):.0f}\")\n",
    "    \n",
    "    # Memory estimate\n",
    "    if hasattr(dataset, '_indices'):\n",
    "        print(f\"\\nMemory: Using indices (memory-mapped)\")\n",
    "    else:\n",
    "        try:\n",
    "            size_mb = dataset.data.nbytes / 1e6\n",
    "            print(f\"\\nEstimated memory: {size_mb:.1f} MB\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Analyze IMDB\n",
    "analyze_dataset(imdb['train'], name='IMDB Train')\n",
    "analyze_dataset(imdb['test'], name='IMDB Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: The `map()` Function - Your Swiss Army Knife\n",
    "\n",
    "The `map()` function applies a transformation to every example in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple transformation: add text length\n",
    "def add_length(example):\n",
    "    example['text_length'] = len(example['text'])\n",
    "    return example\n",
    "\n",
    "# Apply to dataset\n",
    "print(\"Adding text length column...\")\n",
    "imdb_with_length = imdb['train'].map(add_length)\n",
    "\n",
    "print(\"\\nNew columns:\", imdb_with_length.column_names)\n",
    "print(\"Sample:\", imdb_with_length[0]['text_length'], \"characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Batched Processing (Much Faster!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batched transformation - processes multiple examples at once\n",
    "def add_length_batched(examples):\n",
    "    examples['text_length'] = [len(t) for t in examples['text']]\n",
    "    return examples\n",
    "\n",
    "# Compare speeds\n",
    "print(\"Speed Comparison:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Non-batched (slow)\n",
    "start = time.time()\n",
    "_ = imdb['train'].map(add_length)\n",
    "print(f\"Non-batched: {time.time() - start:.2f}s\")\n",
    "\n",
    "# Batched (fast!)\n",
    "start = time.time()\n",
    "_ = imdb['train'].map(add_length_batched, batched=True)\n",
    "print(f\"Batched: {time.time() - start:.2f}s\")\n",
    "\n",
    "# Batched + parallel (fastest!)\n",
    "start = time.time()\n",
    "_ = imdb['train'].map(add_length_batched, batched=True, num_proc=4)\n",
    "print(f\"Batched + 4 workers: {time.time() - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Tokenization with `map()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "# Tokenize the dataset\n",
    "print(\"Tokenizing IMDB dataset...\")\n",
    "start = time.time()\n",
    "\n",
    "tokenized_imdb = imdb.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=['text'],  # Remove original text to save memory\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "print(f\"\\nTokenization completed in {time.time() - start:.2f}s\")\n",
    "print(\"\\nNew columns:\", tokenized_imdb['train'].column_names)\n",
    "print(\"\\nSample tokenized example:\")\n",
    "print(f\"  input_ids shape: {len(tokenized_imdb['train'][0]['input_ids'])}\")\n",
    "print(f\"  attention_mask shape: {len(tokenized_imdb['train'][0]['attention_mask'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter examples based on criteria\n",
    "\n",
    "# Keep only reviews with at least 500 characters\n",
    "long_reviews = imdb['train'].filter(\n",
    "    lambda x: len(x['text']) >= 500,\n",
    "    num_proc=4\n",
    ")\n",
    "print(f\"Reviews >= 500 chars: {len(long_reviews):,} / {len(imdb['train']):,}\")\n",
    "\n",
    "# Keep only positive reviews\n",
    "positive_only = imdb['train'].filter(lambda x: x['label'] == 1)\n",
    "print(f\"Positive reviews: {len(positive_only):,}\")\n",
    "\n",
    "# Keep reviews between 200-1000 characters\n",
    "medium_reviews = imdb['train'].filter(\n",
    "    lambda x: 200 <= len(x['text']) <= 1000\n",
    ")\n",
    "print(f\"Medium reviews (200-1000 chars): {len(medium_reviews):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Batched Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batched filtering is faster for simple conditions\n",
    "def filter_by_length_batched(examples):\n",
    "    return [len(t) >= 500 for t in examples['text']]\n",
    "\n",
    "# Compare speeds\n",
    "print(\"Filter speed comparison:\")\n",
    "\n",
    "start = time.time()\n",
    "_ = imdb['train'].filter(lambda x: len(x['text']) >= 500)\n",
    "print(f\"Non-batched: {time.time() - start:.2f}s\")\n",
    "\n",
    "start = time.time()\n",
    "_ = imdb['train'].filter(filter_by_length_batched, batched=True)\n",
    "print(f\"Batched: {time.time() - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Creating Custom Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/validation/test splits\n",
    "print(\"Creating custom splits from IMDB train...\")\n",
    "\n",
    "# Start with train split only\n",
    "full_train = imdb['train']\n",
    "\n",
    "# First split: separate test set (10%)\n",
    "split1 = full_train.train_test_split(test_size=0.1, seed=42)\n",
    "train_val = split1['train']\n",
    "test_set = split1['test']\n",
    "\n",
    "# Second split: separate validation from remaining train (10% of 90% = 9%)\n",
    "split2 = train_val.train_test_split(test_size=0.1, seed=42)\n",
    "train_set = split2['train']\n",
    "val_set = split2['test']\n",
    "\n",
    "print(f\"\\nFinal splits:\")\n",
    "print(f\"  Train: {len(train_set):,} ({100*len(train_set)/len(full_train):.1f}%)\")\n",
    "print(f\"  Val:   {len(val_set):,} ({100*len(val_set)/len(full_train):.1f}%)\")\n",
    "print(f\"  Test:  {len(test_set):,} ({100*len(test_set)/len(full_train):.1f}%)\")\n",
    "\n",
    "# Create a DatasetDict\n",
    "custom_splits = DatasetDict({\n",
    "    'train': train_set,\n",
    "    'validation': val_set,\n",
    "    'test': test_set\n",
    "})\n",
    "\n",
    "print(\"\\nCustom DatasetDict:\")\n",
    "print(custom_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Stratified Splits (Maintaining Label Balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split maintains label proportions\n",
    "print(\"Stratified split (maintains label balance):\")\n",
    "\n",
    "stratified_split = full_train.train_test_split(\n",
    "    test_size=0.2,\n",
    "    stratify_by_column='label',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Check label distribution in both splits\n",
    "for split_name in ['train', 'test']:\n",
    "    labels = stratified_split[split_name]['label']\n",
    "    counts = Counter(labels)\n",
    "    total = len(labels)\n",
    "    print(f\"\\n{split_name}:\")\n",
    "    for label, count in sorted(counts.items()):\n",
    "        print(f\"  Label {label}: {count:,} ({100*count/total:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Working with Large Datasets (Streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming mode: process huge datasets without loading into memory\n",
    "print(\"Loading C4 dataset in streaming mode...\")\n",
    "print(\"(C4 is ~300GB - we don't want to download all of it!)\")\n",
    "\n",
    "# Load in streaming mode\n",
    "c4_stream = load_dataset(\n",
    "    \"allenai/c4\",\n",
    "    \"en\",  # English subset\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"\\nType: {type(c4_stream)}\")\n",
    "print(\"(IterableDataset - data is fetched on-demand)\")\n",
    "\n",
    "# Take a few examples\n",
    "print(\"\\nFirst 3 examples:\")\n",
    "for i, example in enumerate(c4_stream.take(3)):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  URL: {example.get('url', 'N/A')[:50]}...\")\n",
    "    print(f\"  Text: {example['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing streaming datasets\n",
    "print(\"\\nProcessing streaming data:\")\n",
    "\n",
    "# Map operations work on streams too\n",
    "processed_stream = c4_stream.map(\n",
    "    lambda x: {'text_length': len(x['text'])}\n",
    ")\n",
    "\n",
    "# Filter also works\n",
    "long_texts = processed_stream.filter(\n",
    "    lambda x: x['text_length'] > 1000\n",
    ")\n",
    "\n",
    "# Shuffle with a buffer\n",
    "shuffled = long_texts.shuffle(buffer_size=1000, seed=42)\n",
    "\n",
    "# Take samples\n",
    "print(\"\\nSampling from processed stream:\")\n",
    "for i, example in enumerate(shuffled.take(3)):\n",
    "    print(f\"  Sample {i+1}: {example['text_length']} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Preparing for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete pipeline: Load -> Process -> Ready for Trainer\n",
    "print(\"Complete Data Preparation Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Load dataset\n",
    "print(\"\\n1. Loading dataset...\")\n",
    "raw_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# 2. Create validation split\n",
    "print(\"2. Creating validation split...\")\n",
    "split = raw_dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
    "raw_dataset = DatasetDict({\n",
    "    'train': split['train'],\n",
    "    'validation': split['test'],\n",
    "    'test': raw_dataset['test']\n",
    "})\n",
    "\n",
    "# 3. Tokenize\n",
    "print(\"3. Tokenizing...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_and_prepare(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "    # Rename label to labels (required by Trainer)\n",
    "    tokenized['labels'] = examples['label']\n",
    "    return tokenized\n",
    "\n",
    "processed_dataset = raw_dataset.map(\n",
    "    tokenize_and_prepare,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=['text', 'label'],\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "# 4. Set format for PyTorch\n",
    "print(\"4. Setting format for PyTorch...\")\n",
    "processed_dataset.set_format(\"torch\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"READY FOR TRAINING!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataset structure: {processed_dataset}\")\n",
    "print(f\"\\nColumns: {processed_dataset['train'].column_names}\")\n",
    "print(f\"\\nSample tensor shapes:\")\n",
    "sample = processed_dataset['train'][0]\n",
    "for key, val in sample.items():\n",
    "    if hasattr(val, 'shape'):\n",
    "        print(f\"  {key}: {val.shape}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(val).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Using with DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Create a data collator (handles dynamic padding)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    processed_dataset['train'],\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# Get a batch\n",
    "batch = next(iter(train_dataloader))\n",
    "print(\"Batch contents:\")\n",
    "for key, val in batch.items():\n",
    "    print(f\"  {key}: {val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Saving and Loading Processed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed dataset to disk\n",
    "print(\"Saving processed dataset...\")\n",
    "processed_dataset.save_to_disk(\"../data/processed_imdb\")\n",
    "print(\"Saved to ../data/processed_imdb\")\n",
    "\n",
    "# Load it back\n",
    "print(\"\\nLoading from disk...\")\n",
    "from datasets import load_from_disk\n",
    "loaded_dataset = load_from_disk(\"../data/processed_imdb\")\n",
    "print(loaded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to different formats\n",
    "\n",
    "# As Parquet (efficient columnar format)\n",
    "processed_dataset['train'].to_parquet(\"../data/imdb_train.parquet\")\n",
    "print(\"Saved as Parquet\")\n",
    "\n",
    "# As JSON\n",
    "processed_dataset['train'].select(range(100)).to_json(\"../data/imdb_sample.json\")\n",
    "print(\"Saved sample as JSON\")\n",
    "\n",
    "# As CSV (note: tensors will be converted to lists)\n",
    "# raw_dataset['train'].select(range(100)).to_csv(\"../data/imdb_sample.csv\")\n",
    "# print(\"Saved sample as CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Build a Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Build a complete data pipeline for the AG News dataset\n",
    "# Requirements:\n",
    "# 1. Load the AG News dataset\n",
    "# 2. Analyze the label distribution\n",
    "# 3. Filter to only keep articles > 200 characters\n",
    "# 4. Create train/val/test splits (80/10/10)\n",
    "# 5. Tokenize with a transformer tokenizer\n",
    "# 6. Set format for PyTorch\n",
    "\n",
    "# Hint: AG News has 4 categories - World, Sports, Business, Sci/Tech\n",
    "\n",
    "# Your code:\n",
    "# ag_news = load_dataset(\"ag_news\")\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Using Batched Processing\n",
    "\n",
    "```python\n",
    "# Wrong: Processes one example at a time (SLOW)\n",
    "dataset.map(lambda x: tokenizer(x['text']))\n",
    "\n",
    "# Right: Process in batches (FAST)\n",
    "dataset.map(lambda x: tokenizer(x['text']), batched=True)\n",
    "```\n",
    "\n",
    "### Mistake 2: Forgetting `remove_columns`\n",
    "\n",
    "```python\n",
    "# Wrong: Keeps original text (wastes memory)\n",
    "dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "# Right: Remove columns you don't need\n",
    "dataset.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "```\n",
    "\n",
    "### Mistake 3: Wrong Label Column Name\n",
    "\n",
    "```python\n",
    "# Wrong: Trainer expects 'labels' (plural)\n",
    "tokenized['label'] = examples['label']\n",
    "\n",
    "# Right: Use 'labels'\n",
    "tokenized['labels'] = examples['label']\n",
    "```\n",
    "\n",
    "### Mistake 4: Not Setting Format Before Training\n",
    "\n",
    "```python\n",
    "# Wrong: Data is in Arrow format\n",
    "trainer = Trainer(train_dataset=dataset)\n",
    "\n",
    "# Right: Convert to PyTorch tensors\n",
    "dataset.set_format(\"torch\")\n",
    "trainer = Trainer(train_dataset=dataset)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- Loading datasets from the Hub with various options\n",
    "- Analyzing dataset statistics\n",
    "- Applying transformations with `map()` and `filter()`\n",
    "- Creating stratified train/val/test splits\n",
    "- Handling huge datasets with streaming\n",
    "- Preparing datasets for the Trainer API\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Datasets Documentation](https://huggingface.co/docs/datasets)\n",
    "- [Dataset Processing Guide](https://huggingface.co/docs/datasets/process)\n",
    "- [Streaming Datasets](https://huggingface.co/docs/datasets/stream)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import shutil\n",
    "\n",
    "# Clean up saved files\n",
    "# shutil.rmtree(\"../data/processed_imdb\", ignore_errors=True)\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nLab 2.5.3 complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}