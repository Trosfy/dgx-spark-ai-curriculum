{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.5.1: Hugging Face Hub Exploration\n",
    "\n",
    "**Module:** 2.5 - Hugging Face Ecosystem  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐ (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- [ ] Navigate the Hugging Face Hub to discover models\n",
    "- [ ] Understand model cards and evaluate model quality\n",
    "- [ ] Load and test pre-trained models locally\n",
    "- [ ] Document models systematically for your projects\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Module 2.4 (Efficient Architectures)\n",
    "- Knowledge of: PyTorch basics, transformer architecture concepts\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "**The AI Model Marketplace**: Imagine you're building an AI-powered customer service bot. You need models for:\n",
    "- Understanding customer sentiment (Are they happy or frustrated?)\n",
    "- Extracting key information (What product are they asking about?)\n",
    "- Generating helpful responses\n",
    "\n",
    "Instead of training these from scratch (which would take months and millions of dollars), you can find pre-trained models on **Hugging Face Hub** - think of it as the \"GitHub for AI models.\"\n",
    "\n",
    "Today, we'll learn how to navigate this marketplace like a pro!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: The Hugging Face Hub\n",
    "\n",
    "> **Imagine you're building with LEGO...**\n",
    ">\n",
    "> Instead of making every brick from scratch, you go to a store where other builders share their pre-made structures. Someone already built a perfect castle tower? Great, you can use it in your medieval village!\n",
    ">\n",
    "> The Hugging Face Hub is exactly like that - a store where AI researchers share their \"LEGO structures\" (trained models). Each comes with:\n",
    "> - **Instructions** (model cards) - how to use it\n",
    "> - **Reviews** (downloads/likes) - how popular it is\n",
    "> - **Demo** (inference API) - try before you download\n",
    ">\n",
    "> **In AI terms:** It's a platform hosting 500,000+ pre-trained models for tasks like text classification, image generation, speech recognition, and more. You can download any model with one line of code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setting Up\n",
    "\n",
    "### 1.1 Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if not already installed)\n",
    "# !pip install transformers datasets huggingface_hub accelerate -q\n",
    "\n",
    "import torch\n",
    "from huggingface_hub import HfApi, hf_hub_download, list_models\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "import json\n",
    "\n",
    "# Check our environment\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Initialize the Hugging Face API\n",
    "\n",
    "The `HfApi` class is your gateway to programmatically interact with the Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Hugging Face API client\n",
    "api = HfApi()\n",
    "\n",
    "# Test the connection by getting info about a popular model\n",
    "model_info = api.model_info(\"bert-base-uncased\")\n",
    "print(f\"Model ID: {model_info.id}\")\n",
    "print(f\"Downloads: {model_info.downloads:,}\")\n",
    "print(f\"Likes: {model_info.likes}\")\n",
    "print(f\"Library: {model_info.library_name}\")\n",
    "print(f\"\\nConnection successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "We just queried the Hugging Face Hub to get information about the `bert-base-uncased` model. This is one of the most downloaded models - a foundational language model from Google that's been used as a starting point for thousands of NLP applications.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Searching for Models\n",
    "\n",
    "### 2.1 Understanding Model Categories\n",
    "\n",
    "Models on the Hub are organized by **tasks**. Here are the main categories:\n",
    "\n",
    "| Category | Tasks | Examples |\n",
    "|----------|-------|----------|\n",
    "| **NLP** | text-classification, ner, qa, summarization, translation | BERT, GPT, T5 |\n",
    "| **Vision** | image-classification, object-detection, segmentation | ViT, YOLO, SAM |\n",
    "| **Audio** | speech-recognition, text-to-speech | Whisper, Bark |\n",
    "| **Multimodal** | image-to-text, text-to-image | CLIP, Stable Diffusion |\n",
    "| **Generation** | text-generation, image-generation | Llama, Mistral, SDXL |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_models_by_task(\n",
    "    task: str,\n",
    "    limit: int = 10,\n",
    "    sort: str = \"downloads\"\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search for models by task type.\n",
    "    \n",
    "    Args:\n",
    "        task: Task type (e.g., \"text-classification\", \"text-generation\")\n",
    "        limit: Maximum number of results\n",
    "        sort: Sort by \"downloads\" or \"likes\"\n",
    "    \n",
    "    Returns:\n",
    "        List of model dictionaries\n",
    "    \"\"\"\n",
    "    models = api.list_models(\n",
    "        filter=task,\n",
    "        sort=sort,\n",
    "        direction=-1,  # Descending\n",
    "        limit=limit\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    for m in models:\n",
    "        results.append({\n",
    "            \"id\": m.id,\n",
    "            \"author\": m.author,\n",
    "            \"downloads\": m.downloads,\n",
    "            \"likes\": m.likes,\n",
    "            \"tags\": m.tags[:5] if m.tags else []\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Search for sentiment analysis models\n",
    "print(\"Top 10 Sentiment Analysis Models:\")\n",
    "print(\"=\" * 70)\n",
    "models = search_models_by_task(\"text-classification\", limit=10)\n",
    "\n",
    "for i, m in enumerate(models, 1):\n",
    "    print(f\"{i:2}. {m['id'][:50]:<50} | Downloads: {m['downloads']:>12,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Searching by Keywords\n",
    "\n",
    "Sometimes you want to search by model name or specific features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_models_by_keyword(\n",
    "    keyword: str,\n",
    "    author: Optional[str] = None,\n",
    "    limit: int = 10\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search for models by keyword.\n",
    "    \n",
    "    Args:\n",
    "        keyword: Search term\n",
    "        author: Optional author/organization filter\n",
    "        limit: Maximum results\n",
    "    \n",
    "    Returns:\n",
    "        List of model dictionaries\n",
    "    \"\"\"\n",
    "    kwargs = {\n",
    "        \"search\": keyword,\n",
    "        \"sort\": \"downloads\",\n",
    "        \"direction\": -1,\n",
    "        \"limit\": limit\n",
    "    }\n",
    "    \n",
    "    if author:\n",
    "        kwargs[\"author\"] = author\n",
    "    \n",
    "    models = api.list_models(**kwargs)\n",
    "    \n",
    "    return [\n",
    "        {\n",
    "            \"id\": m.id,\n",
    "            \"downloads\": m.downloads,\n",
    "            \"pipeline_tag\": m.pipeline_tag\n",
    "        }\n",
    "        for m in models\n",
    "    ]\n",
    "\n",
    "# Search for Llama models from Meta\n",
    "print(\"\\nLlama models from Meta:\")\n",
    "print(\"=\" * 70)\n",
    "llama_models = search_models_by_keyword(\"llama\", author=\"meta-llama\", limit=5)\n",
    "\n",
    "for m in llama_models:\n",
    "    print(f\"  {m['id']:<45} | {m['pipeline_tag'] or 'N/A':<20}\")\n",
    "\n",
    "# Search for models optimized for DGX Spark (smaller, efficient models)\n",
    "print(\"\\nMistral models (great for DGX Spark):\")\n",
    "print(\"=\" * 70)\n",
    "mistral_models = search_models_by_keyword(\"mistral\", author=\"mistralai\", limit=5)\n",
    "\n",
    "for m in mistral_models:\n",
    "    print(f\"  {m['id']:<45} | {m['pipeline_tag'] or 'N/A':<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try It Yourself: Find Your Own Models\n",
    "\n",
    "Search for models related to a task you're interested in:\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Try tasks like: \"question-answering\", \"summarization\", \"translation\", \"image-classification\"\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Search for models in a task that interests you\n",
    "\n",
    "my_task = \"question-answering\"  # Change this!\n",
    "\n",
    "# Search and print the top 5 models\n",
    "my_models = search_models_by_task(my_task, limit=5)\n",
    "\n",
    "print(f\"\\nTop 5 {my_task} models:\")\n",
    "for i, m in enumerate(my_models, 1):\n",
    "    print(f\"{i}. {m['id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Understanding Model Cards\n",
    "\n",
    "### 3.1 What is a Model Card?\n",
    "\n",
    "A **Model Card** is like a nutritional label for AI models. It tells you:\n",
    "- What the model does\n",
    "- How it was trained\n",
    "- What data was used\n",
    "- Known limitations and biases\n",
    "- How to use it\n",
    "\n",
    "Let's create a comprehensive model documentation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelDocumentation:\n",
    "    \"\"\"Structured documentation for a Hugging Face model.\"\"\"\n",
    "    model_id: str\n",
    "    author: str\n",
    "    task: str\n",
    "    downloads: int\n",
    "    likes: int\n",
    "    library: str\n",
    "    tags: List[str]\n",
    "    created_at: str\n",
    "    last_modified: str\n",
    "    model_size_gb: float = 0.0\n",
    "    notes: str = \"\"\n",
    "    tested_locally: bool = False\n",
    "    local_test_result: str = \"\"\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"model_id\": self.model_id,\n",
    "            \"author\": self.author,\n",
    "            \"task\": self.task,\n",
    "            \"downloads\": self.downloads,\n",
    "            \"likes\": self.likes,\n",
    "            \"library\": self.library,\n",
    "            \"tags\": self.tags,\n",
    "            \"created_at\": self.created_at,\n",
    "            \"last_modified\": self.last_modified,\n",
    "            \"model_size_gb\": self.model_size_gb,\n",
    "            \"notes\": self.notes,\n",
    "            \"tested_locally\": self.tested_locally,\n",
    "            \"local_test_result\": self.local_test_result\n",
    "        }\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"\"\"\n",
    "========================================\n",
    "MODEL: {self.model_id}\n",
    "========================================\n",
    "Author: {self.author}\n",
    "Task: {self.task}\n",
    "Library: {self.library}\n",
    "\n",
    "Popularity:\n",
    "  Downloads: {self.downloads:,}\n",
    "  Likes: {self.likes}\n",
    "\n",
    "Tags: {', '.join(self.tags[:5])}\n",
    "\n",
    "Size: {self.model_size_gb:.2f} GB (estimated)\n",
    "\n",
    "Tested Locally: {'Yes' if self.tested_locally else 'No'}\n",
    "{f'Test Result: {self.local_test_result}' if self.tested_locally else ''}\n",
    "\n",
    "Notes: {self.notes or 'None'}\n",
    "========================================\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def document_model(model_id: str) -> ModelDocumentation:\n",
    "    \"\"\"\n",
    "    Create comprehensive documentation for a model.\n",
    "    \n",
    "    Args:\n",
    "        model_id: Hugging Face model identifier\n",
    "    \n",
    "    Returns:\n",
    "        ModelDocumentation object\n",
    "    \"\"\"\n",
    "    info = api.model_info(model_id)\n",
    "    \n",
    "    # Estimate model size from siblings (files)\n",
    "    total_size = 0\n",
    "    if info.siblings:\n",
    "        for sibling in info.siblings:\n",
    "            if hasattr(sibling, 'size') and sibling.size:\n",
    "                total_size += sibling.size\n",
    "    \n",
    "    return ModelDocumentation(\n",
    "        model_id=model_id,\n",
    "        author=info.author or \"unknown\",\n",
    "        task=info.pipeline_tag or \"unknown\",\n",
    "        downloads=info.downloads or 0,\n",
    "        likes=info.likes or 0,\n",
    "        library=info.library_name or \"unknown\",\n",
    "        tags=info.tags[:10] if info.tags else [],\n",
    "        created_at=str(info.created_at) if info.created_at else \"unknown\",\n",
    "        last_modified=str(info.last_modified) if info.last_modified else \"unknown\",\n",
    "        model_size_gb=total_size / 1e9\n",
    "    )\n",
    "\n",
    "# Document a popular model\n",
    "doc = document_model(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Reading the Model README\n",
    "\n",
    "The README (model card content) contains the most detailed information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_readme(model_id: str, max_chars: int = 2000) -> str:\n",
    "    \"\"\"\n",
    "    Download and return the README content for a model.\n",
    "    \n",
    "    Args:\n",
    "        model_id: Model identifier\n",
    "        max_chars: Maximum characters to return\n",
    "    \n",
    "    Returns:\n",
    "        README content (truncated if needed)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        readme_path = hf_hub_download(repo_id=model_id, filename=\"README.md\")\n",
    "        with open(readme_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        if len(content) > max_chars:\n",
    "            return content[:max_chars] + f\"\\n\\n... [Truncated - {len(content):,} total characters]\"\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        return f\"Could not fetch README: {e}\"\n",
    "\n",
    "# Get the README for DistilBERT sentiment model\n",
    "readme = get_model_readme(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "print(readme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Testing Models Locally\n",
    "\n",
    "### 4.1 Loading and Testing Classification Models\n",
    "\n",
    "Now let's actually load and test some models on our DGX Spark!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classification_model(\n",
    "    model_id: str,\n",
    "    test_texts: List[str],\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load and test a classification model.\n",
    "    \n",
    "    Args:\n",
    "        model_id: Model identifier\n",
    "        test_texts: List of texts to classify\n",
    "        device: Device to use\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results and timing\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"model_id\": model_id,\n",
    "        \"success\": False,\n",
    "        \"load_time_seconds\": 0,\n",
    "        \"inference_time_ms\": 0,\n",
    "        \"memory_gb\": 0,\n",
    "        \"predictions\": [],\n",
    "        \"error\": None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Clear memory\n",
    "        torch.cuda.empty_cache() if device == \"cuda\" else None\n",
    "        initial_memory = torch.cuda.memory_allocated() / 1e9 if device == \"cuda\" else 0\n",
    "        \n",
    "        # Load model with timing\n",
    "        print(f\"Loading {model_id}...\")\n",
    "        start = time.time()\n",
    "        \n",
    "        # Use pipeline for simplicity\n",
    "        classifier = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=model_id,\n",
    "            device=0 if device == \"cuda\" else -1,\n",
    "            torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32\n",
    "        )\n",
    "        \n",
    "        result[\"load_time_seconds\"] = time.time() - start\n",
    "        result[\"memory_gb\"] = (torch.cuda.memory_allocated() / 1e9 - initial_memory) if device == \"cuda\" else 0\n",
    "        \n",
    "        print(f\"  Loaded in {result['load_time_seconds']:.2f}s, using {result['memory_gb']:.2f}GB\")\n",
    "        \n",
    "        # Run inference\n",
    "        print(\"  Running inference...\")\n",
    "        start = time.time()\n",
    "        predictions = classifier(test_texts)\n",
    "        result[\"inference_time_ms\"] = (time.time() - start) * 1000\n",
    "        \n",
    "        result[\"predictions\"] = predictions\n",
    "        result[\"success\"] = True\n",
    "        \n",
    "        # Cleanup\n",
    "        del classifier\n",
    "        torch.cuda.empty_cache() if device == \"cuda\" else None\n",
    "        \n",
    "        print(f\"  Inference took {result['inference_time_ms']:.1f}ms for {len(test_texts)} samples\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        result[\"error\"] = str(e)\n",
    "        print(f\"  ERROR: {e}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test texts for sentiment analysis\n",
    "test_texts = [\n",
    "    \"This product is amazing! Best purchase I've ever made!\",\n",
    "    \"Terrible experience. Complete waste of money.\",\n",
    "    \"It's okay, nothing special but gets the job done.\",\n",
    "    \"I'm absolutely thrilled with this service!\",\n",
    "    \"Disappointed. Would not recommend.\"\n",
    "]\n",
    "\n",
    "# Test the DistilBERT sentiment model\n",
    "result = test_classification_model(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    test_texts\n",
    ")\n",
    "\n",
    "if result[\"success\"]:\n",
    "    print(\"\\nResults:\")\n",
    "    print(\"-\" * 60)\n",
    "    for text, pred in zip(test_texts, result[\"predictions\"]):\n",
    "        sentiment = \"POS\" if pred[\"label\"] == \"POSITIVE\" else \"NEG\"\n",
    "        conf = pred[\"score\"] * 100\n",
    "        print(f\"[{sentiment}] {conf:5.1f}% | {text[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Testing Text Generation Models\n",
    "\n",
    "Let's also test a text generation model - this is where DGX Spark's 128GB memory really shines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generation_model(\n",
    "    model_id: str,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 50,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load and test a text generation model.\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"model_id\": model_id,\n",
    "        \"success\": False,\n",
    "        \"load_time_seconds\": 0,\n",
    "        \"inference_time_ms\": 0,\n",
    "        \"memory_gb\": 0,\n",
    "        \"generated_text\": \"\",\n",
    "        \"error\": None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        torch.cuda.empty_cache() if device == \"cuda\" else None\n",
    "        initial_memory = torch.cuda.memory_allocated() / 1e9 if device == \"cuda\" else 0\n",
    "        \n",
    "        print(f\"Loading {model_id}...\")\n",
    "        start = time.time()\n",
    "        \n",
    "        generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_id,\n",
    "            device=0 if device == \"cuda\" else -1,\n",
    "            torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32\n",
    "        )\n",
    "        \n",
    "        result[\"load_time_seconds\"] = time.time() - start\n",
    "        result[\"memory_gb\"] = (torch.cuda.memory_allocated() / 1e9 - initial_memory) if device == \"cuda\" else 0\n",
    "        \n",
    "        print(f\"  Loaded in {result['load_time_seconds']:.2f}s, using {result['memory_gb']:.2f}GB\")\n",
    "        \n",
    "        print(\"  Generating...\")\n",
    "        start = time.time()\n",
    "        \n",
    "        output = generator(\n",
    "            prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=generator.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        result[\"inference_time_ms\"] = (time.time() - start) * 1000\n",
    "        result[\"generated_text\"] = output[0][\"generated_text\"]\n",
    "        result[\"success\"] = True\n",
    "        \n",
    "        del generator\n",
    "        torch.cuda.empty_cache() if device == \"cuda\" else None\n",
    "        \n",
    "    except Exception as e:\n",
    "        result[\"error\"] = str(e)\n",
    "        print(f\"  ERROR: {e}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test with GPT-2 (small, fast, good for demonstration)\n",
    "result = test_generation_model(\n",
    "    \"gpt2\",\n",
    "    \"The future of artificial intelligence is\",\n",
    "    max_new_tokens=50\n",
    ")\n",
    "\n",
    "if result[\"success\"]:\n",
    "    print(\"\\nGenerated Text:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(result[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Documenting Your Model Selection\n",
    "\n",
    "### 5.1 Creating a Model Catalog\n",
    "\n",
    "For your assignment, you need to document 10 models. Here's a structured way to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to explore (mix of different tasks)\n",
    "models_to_explore = [\n",
    "    # Sentiment/Classification\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    \n",
    "    # Named Entity Recognition\n",
    "    \"dslim/bert-base-NER\",\n",
    "    \n",
    "    # Question Answering\n",
    "    \"deepset/roberta-base-squad2\",\n",
    "    \n",
    "    # Summarization\n",
    "    \"facebook/bart-large-cnn\",\n",
    "    \n",
    "    # Text Generation\n",
    "    \"gpt2\",\n",
    "    \"microsoft/phi-2\",  # Great for DGX Spark!\n",
    "    \n",
    "    # Embeddings\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \n",
    "    # Translation\n",
    "    \"Helsinki-NLP/opus-mt-en-de\",\n",
    "    \n",
    "    # Zero-shot Classification\n",
    "    \"facebook/bart-large-mnli\"\n",
    "]\n",
    "\n",
    "# Document all models\n",
    "print(\"Documenting 10 models...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model_catalog = []\n",
    "\n",
    "for model_id in models_to_explore:\n",
    "    try:\n",
    "        doc = document_model(model_id)\n",
    "        model_catalog.append(doc)\n",
    "        print(f\"[OK] {model_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[FAIL] {model_id}: {e}\")\n",
    "\n",
    "print(f\"\\nSuccessfully documented {len(model_catalog)} models!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display catalog summary\n",
    "print(\"\\nMODEL CATALOG SUMMARY\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Model ID':<45} | {'Task':<20} | {'Downloads':>12}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for doc in sorted(model_catalog, key=lambda x: x.downloads, reverse=True):\n",
    "    model_short = doc.model_id[:44] if len(doc.model_id) > 44 else doc.model_id\n",
    "    task_short = doc.task[:19] if len(doc.task) > 19 else doc.task\n",
    "    print(f\"{model_short:<45} | {task_short:<20} | {doc.downloads:>12,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Testing Your Top 3 Models Locally\n",
    "\n",
    "Now test the 3 models you find most interesting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3 models locally\n",
    "models_to_test = [\n",
    "    (\"distilbert-base-uncased-finetuned-sst-2-english\", \"classification\"),\n",
    "    (\"dslim/bert-base-NER\", \"ner\"),\n",
    "    (\"gpt2\", \"generation\")\n",
    "]\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for model_id, task_type in models_to_test:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {model_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if task_type == \"classification\":\n",
    "        result = test_classification_model(\n",
    "            model_id,\n",
    "            [\"This is wonderful!\", \"This is terrible.\"]\n",
    "        )\n",
    "    elif task_type == \"generation\":\n",
    "        result = test_generation_model(\n",
    "            model_id,\n",
    "            \"Once upon a time\",\n",
    "            max_new_tokens=30\n",
    "        )\n",
    "    elif task_type == \"ner\":\n",
    "        # NER test\n",
    "        try:\n",
    "            ner = pipeline(\n",
    "                \"ner\",\n",
    "                model=model_id,\n",
    "                aggregation_strategy=\"simple\",\n",
    "                device=0 if torch.cuda.is_available() else -1\n",
    "            )\n",
    "            output = ner(\"Apple CEO Tim Cook announced new products in Cupertino.\")\n",
    "            result = {\"success\": True, \"output\": output}\n",
    "            print(\"  Entities found:\")\n",
    "            for ent in output:\n",
    "                print(f\"    - {ent['entity_group']}: '{ent['word']}' ({ent['score']:.2%})\")\n",
    "            del ner\n",
    "            torch.cuda.empty_cache()\n",
    "        except Exception as e:\n",
    "            result = {\"success\": False, \"error\": str(e)}\n",
    "    \n",
    "    test_results.append({\"model_id\": model_id, \"result\": result})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All tests complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Save Your Catalog\n",
    "\n",
    "Save your model catalog for future reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Convert catalog to JSON\n",
    "catalog_data = {\n",
    "    \"created_at\": datetime.now().isoformat(),\n",
    "    \"num_models\": len(model_catalog),\n",
    "    \"models\": [doc.to_dict() for doc in model_catalog]\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "output_path = Path(\"../data/model_catalog.json\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(catalog_data, f, indent=2)\n",
    "\n",
    "print(f\"Catalog saved to {output_path}\")\n",
    "print(f\"Contains {len(model_catalog)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## DGX Spark Model Capacity Reference\n\nWhen selecting models, keep these DGX Spark limits in mind:\n\n| Scenario | Maximum Model Size | Notes |\n|----------|-------------------|-------|\n| BF16 Inference | 50-55B | Native Blackwell support |\n| FP8 Inference | 90-100B | Reduced precision |\n| NVFP4 Inference | ~200B | Blackwell exclusive |\n| Full Fine-Tuning (FP16) | 12-16B | With gradient checkpointing |\n| QLoRA Fine-Tuning | 100-120B | 4-bit quantized + adapters |\n\n---\n\n## Common Mistakes\n\n### Mistake 1: Not Checking Model Size Before Loading\n\n```python\n# Wrong: Trying to load a 70B model in BF16 without checking capacity\n# DGX Spark BF16 limit is 50-55B!\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-70b-hf\",\n    torch_dtype=torch.bfloat16\n)\n# OOM Error!\n\n# Right: Check model size first, use quantization for larger models\ninfo = api.model_info(\"meta-llama/Llama-2-70b-hf\")\nprint(f\"Model size: {sum(s.size for s in info.siblings if s.size) / 1e9:.1f} GB\")\n# 70B requires FP8 or NVFP4 quantization - see Module 3.2\n```\n\n### Mistake 2: Forgetting to Use BFloat16 on DGX Spark\n\n```python\n# Wrong: Default FP32 wastes memory\nmodel = AutoModel.from_pretrained(\"bert-base-uncased\")\n\n# Right: Use BF16 for Blackwell GB10 (native support)\nmodel = AutoModel.from_pretrained(\n    \"bert-base-uncased\",\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\"\n)\n```\n\n### Mistake 3: Not Clearing GPU Memory Between Models\n\n```python\n# Wrong: Memory keeps accumulating\nmodel1 = load_model(\"model1\")\nmodel2 = load_model(\"model2\")  # OOM!\n\n# Right: Clean up between models (standard pattern)\nmodel1 = load_model(\"model1\")\n# ... use model1 ...\ndel model1\ngc.collect()\ntorch.cuda.empty_cache()\nmodel2 = load_model(\"model2\")  # Works!\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- How to search the Hugging Face Hub by task and keyword\n",
    "- How to read and understand model cards\n",
    "- How to load and test models locally on DGX Spark\n",
    "- How to document models systematically\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "**Advanced Exercise**: Create a model comparison tool that:\n",
    "1. Takes a task type as input\n",
    "2. Finds the top 5 models for that task\n",
    "3. Tests each model on the same inputs\n",
    "4. Compares accuracy, speed, and memory usage\n",
    "5. Generates a recommendation\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Combine the search and test functions we created, then add comparison logic.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Hugging Face Hub Documentation](https://huggingface.co/docs/hub)\n",
    "- [Model Cards Guide](https://huggingface.co/docs/hub/model-cards)\n",
    "- [Transformers Quick Tour](https://huggingface.co/docs/transformers/quicktour)\n",
    "- [DGX Spark Model Loading Best Practices](https://developer.nvidia.com/dgx-spark)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "\n",
    "print(\"\\nLab complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}