{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.5.4: Fine-Tuning with the Trainer API\n",
    "\n",
    "**Module:** 2.5 - Hugging Face Ecosystem  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐⭐ (Intermediate-Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- [ ] Configure TrainingArguments for optimal DGX Spark training\n",
    "- [ ] Implement custom metrics for evaluation\n",
    "- [ ] Use callbacks for monitoring training\n",
    "- [ ] Fine-tune a model for text classification\n",
    "- [ ] Evaluate and save your trained model\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Labs 2.5.1 through 2.5.3\n",
    "- Knowledge of: Training loops, loss functions, optimizers\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "**From General to Specialist**: Imagine you're a hospital. BERT is like a general doctor - knows a bit about everything. But you need a specialist who understands medical terminology, diagnoses, and patient sentiment.\n",
    "\n",
    "**Fine-tuning** teaches this general doctor to become a specialist - using YOUR data for YOUR specific task. The Hugging Face **Trainer** makes this process as simple as:\n",
    "\n",
    "```python\n",
    "trainer = Trainer(model, args, train_data, eval_data)\n",
    "trainer.train()  # That's it!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: What is Fine-Tuning?\n",
    "\n",
    "> **Imagine you're learning to play tennis...**\n",
    ">\n",
    "> Pre-training (what BERT learned): General coordination, how to hold a racket, basic movement\n",
    ">\n",
    "> Fine-tuning (what we'll do): Practice YOUR favorite shots, learn YOUR opponent's weaknesses\n",
    ">\n",
    "> The key insight: You don't start from scratch! You keep all the general knowledge and just add specialized skills.\n",
    ">\n",
    "> **In AI terms:**\n",
    "> - Pre-trained BERT knows language structure, grammar, context\n",
    "> - We fine-tune it on sentiment data so it learns \"words like 'amazing' = positive\"\n",
    "> - Training is FAST because we only adjust weights slightly, not learn everything from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import evaluate\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Check environment\n",
    "print(\"Environment Check\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDB dataset\n",
    "print(\"Loading IMDB dataset...\")\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Create validation split from training data\n",
    "split = dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "train_dataset = split['train']\n",
    "eval_dataset = split['test']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "print(f\"\\nDataset splits:\")\n",
    "print(f\"  Train: {len(train_dataset):,}\")\n",
    "print(f\"  Eval:  {len(eval_dataset):,}\")\n",
    "print(f\"  Test:  {len(test_dataset):,}\")\n",
    "\n",
    "# Check label balance\n",
    "from collections import Counter\n",
    "print(f\"\\nLabel distribution (train): {Counter(train_dataset['label'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    torch_dtype=torch.bfloat16  # Use BF16 for DGX Spark\n",
    ")\n",
    "\n",
    "# Check model size\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {num_params:,}\")\n",
    "print(f\"Estimated size: {num_params * 2 / 1e9:.2f} GB (BF16)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize datasets\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=['text']\n",
    ")\n",
    "\n",
    "tokenized_eval = eval_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=['text']\n",
    ")\n",
    "\n",
    "tokenized_test = test_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=['text']\n",
    ")\n",
    "\n",
    "# Rename label to labels (Trainer expects this)\n",
    "tokenized_train = tokenized_train.rename_column(\"label\", \"labels\")\n",
    "tokenized_eval = tokenized_eval.rename_column(\"label\", \"labels\")\n",
    "tokenized_test = tokenized_test.rename_column(\"label\", \"labels\")\n",
    "\n",
    "print(\"\\nTokenized dataset columns:\", tokenized_train.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding TrainingArguments\n",
    "\n",
    "This is where you configure everything about your training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore TrainingArguments step by step\n",
    "print(\"TrainingArguments - Key Parameters\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "config_explanation = \"\"\"\n",
    "OUTPUT:\n",
    "  output_dir         : Where to save checkpoints and results\n",
    "  overwrite_output_dir: Overwrite existing output directory\n",
    "\n",
    "TRAINING DURATION:\n",
    "  num_train_epochs   : Number of training epochs (full passes through data)\n",
    "  max_steps          : Override epochs with exact step count (-1 to disable)\n",
    "\n",
    "BATCH SIZE:\n",
    "  per_device_train_batch_size: Batch size per GPU for training\n",
    "  per_device_eval_batch_size : Batch size per GPU for evaluation\n",
    "  gradient_accumulation_steps: Accumulate gradients over N steps (simulates larger batch)\n",
    "\n",
    "OPTIMIZATION:\n",
    "  learning_rate      : Initial learning rate\n",
    "  weight_decay       : L2 regularization weight\n",
    "  warmup_steps       : Steps for learning rate warmup (or warmup_ratio)\n",
    "  lr_scheduler_type  : \"linear\", \"cosine\", \"polynomial\", etc.\n",
    "\n",
    "EVALUATION & SAVING:\n",
    "  eval_strategy      : \"no\", \"steps\", \"epoch\" - when to evaluate\n",
    "  save_strategy      : \"no\", \"steps\", \"epoch\" - when to save\n",
    "  load_best_model_at_end: Load best checkpoint at end of training\n",
    "  metric_for_best_model : Which metric determines \"best\"\n",
    "\n",
    "DGX SPARK OPTIMIZATION:\n",
    "  bf16               : Use bfloat16 mixed precision (True for Blackwell GPU!)\n",
    "  dataloader_num_workers: Parallel data loading\n",
    "  dataloader_pin_memory : Pin memory for faster GPU transfer\n",
    "\"\"\"\n",
    "print(config_explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimized training arguments for DGX Spark\n",
    "training_args = TrainingArguments(\n",
    "    # Output\n",
    "    output_dir=\"./results/imdb_classifier\",\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # Training duration\n",
    "    num_train_epochs=3,\n",
    "    \n",
    "    # Batch size (with 128GB, we can use larger batches!)\n",
    "    per_device_train_batch_size=32,  # Adjust based on GPU memory\n",
    "    per_device_eval_batch_size=64,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,  # 10% of training for warmup\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    \n",
    "    # Evaluation & Saving\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,  # Keep only 2 best checkpoints\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # DGX Spark Optimizations\n",
    "    bf16=True,  # Blackwell native BF16\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard for now\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"Training Arguments Created!\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  BF16: {training_args.bf16}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute multiple metrics for evaluation.\n",
    "    \n",
    "    Args:\n",
    "        eval_pred: EvalPrediction containing predictions and labels\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of metric names and values\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Compute all metrics\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"f1\": f1[\"f1\"],\n",
    "        \"precision\": precision[\"precision\"],\n",
    "        \"recall\": recall[\"recall\"]\n",
    "    }\n",
    "\n",
    "print(\"Custom metrics function created!\")\n",
    "print(\"Will compute: accuracy, f1, precision, recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Custom Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Callback to monitor GPU memory usage during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, log_every=100):\n",
    "        self.log_every = log_every\n",
    "        self.memory_log = []\n",
    "        \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.log_every == 0:\n",
    "            if torch.cuda.is_available():\n",
    "                allocated = torch.cuda.memory_allocated() / 1e9\n",
    "                reserved = torch.cuda.memory_reserved() / 1e9\n",
    "                self.memory_log.append({\n",
    "                    \"step\": state.global_step,\n",
    "                    \"allocated_gb\": allocated,\n",
    "                    \"reserved_gb\": reserved\n",
    "                })\n",
    "                \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        if self.memory_log:\n",
    "            max_allocated = max(m[\"allocated_gb\"] for m in self.memory_log)\n",
    "            print(f\"\\n[Memory] Peak GPU usage: {max_allocated:.2f} GB\")\n",
    "\n",
    "\n",
    "class TimingCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Callback to track training timing.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.epoch_times = []\n",
    "        \n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.start_time = time.time()\n",
    "        print(f\"\\n[Timing] Training started...\")\n",
    "        \n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        self.epoch_start = time.time()\n",
    "        \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        epoch_time = time.time() - self.epoch_start\n",
    "        self.epoch_times.append(epoch_time)\n",
    "        print(f\"[Timing] Epoch completed in {epoch_time:.1f}s\")\n",
    "        \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        total_time = time.time() - self.start_time\n",
    "        print(f\"\\n[Timing] Total training time: {total_time/60:.1f} minutes\")\n",
    "        print(f\"[Timing] Average epoch time: {np.mean(self.epoch_times):.1f}s\")\n",
    "\n",
    "\n",
    "print(\"Custom callbacks created: MemoryCallback, TimingCallback\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        MemoryCallback(log_every=100),\n",
    "        TimingCallback(),\n",
    "        EarlyStoppingCallback(early_stopping_patience=3)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Trainer created!\")\n",
    "print(f\"\\nTraining dataset: {len(tokenized_train):,} examples\")\n",
    "print(f\"Evaluation dataset: {len(tokenized_eval):,} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate training time\n",
    "total_steps = len(tokenized_train) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
    "print(f\"Total training steps: {total_steps:,}\")\n",
    "print(f\"Steps per epoch: {len(tokenized_train) // training_args.per_device_train_batch_size:,}\")\n",
    "\n",
    "# Clear memory before training\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"\\nGPU memory before training: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training summary\n",
    "print(\"\\nTraining Results:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Final training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Training runtime: {train_result.metrics['train_runtime']:.1f}s\")\n",
    "print(f\"Samples/second: {train_result.metrics['train_samples_per_second']:.1f}\")\n",
    "print(f\"Steps/second: {train_result.metrics['train_steps_per_second']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "print(\"-\" * 40)\n",
    "for key, value in eval_results.items():\n",
    "    if \"eval_\" in key:\n",
    "        metric_name = key.replace(\"eval_\", \"\")\n",
    "        if isinstance(value, float):\n",
    "            print(f\"{metric_name:15}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"{metric_name:15}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_results = trainer.evaluate(tokenized_test)\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(\"-\" * 40)\n",
    "for key, value in test_results.items():\n",
    "    if \"eval_\" in key:\n",
    "        metric_name = key.replace(\"eval_\", \"\")\n",
    "        if isinstance(value, float):\n",
    "            print(f\"{metric_name:15}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "predictions = trainer.predict(tokenized_test)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"-\" * 40)\n",
    "cm = confusion_matrix(labels, preds)\n",
    "print(f\"              Predicted\")\n",
    "print(f\"              Neg    Pos\")\n",
    "print(f\"Actual Neg   {cm[0][0]:5}  {cm[0][1]:5}\")\n",
    "print(f\"       Pos   {cm[1][0]:5}  {cm[1][1]:5}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"-\" * 40)\n",
    "print(classification_report(labels, preds, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Saving and Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_path = \"./results/imdb_classifier_final\"\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Model saved to {save_path}\")\n",
    "\n",
    "# Check what was saved\n",
    "import os\n",
    "print(\"\\nSaved files:\")\n",
    "for f in os.listdir(save_path):\n",
    "    size = os.path.getsize(os.path.join(save_path, f)) / 1e6\n",
    "    print(f\"  {f}: {size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model for inference\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create pipeline from saved model\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=save_path,\n",
    "    tokenizer=save_path,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Test on new examples\n",
    "test_texts = [\n",
    "    \"This movie was absolutely fantastic! I loved every minute of it.\",\n",
    "    \"Worst movie I've ever seen. Complete waste of time.\",\n",
    "    \"It was okay, nothing special but watchable.\"\n",
    "]\n",
    "\n",
    "print(\"\\nInference on new examples:\")\n",
    "print(\"-\" * 60)\n",
    "for text in test_texts:\n",
    "    result = classifier(text)[0]\n",
    "    sentiment = \"POSITIVE\" if result['label'] == 'LABEL_1' else \"NEGATIVE\"\n",
    "    print(f\"{sentiment} ({result['score']:.1%}): {text[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Hyperparameter Tips for DGX Spark\n",
    "\n",
    "### Recommended Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DGX SPARK TRAINING RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "recommendations = \"\"\"\n",
    "PRECISION:\n",
    "  - Use bf16=True (native Blackwell support)\n",
    "  - Avoid fp16 on Blackwell - BF16 is more stable and equally fast\n",
    "\n",
    "BATCH SIZE:\n",
    "  - With 128GB unified memory, you can use larger batches\n",
    "  - DistilBERT: batch_size=64-128\n",
    "  - BERT-base: batch_size=32-64\n",
    "  - Large models (7B+): Use gradient accumulation\n",
    "\n",
    "LEARNING RATE:\n",
    "  - Start with 2e-5 for most transformer fine-tuning\n",
    "  - Larger batches can use slightly higher LR (3e-5)\n",
    "  - Use warmup (10-20% of training)\n",
    "\n",
    "DATA LOADING:\n",
    "  - dataloader_num_workers=4-8\n",
    "  - dataloader_pin_memory=True\n",
    "  - Use batched tokenization with num_proc=4+\n",
    "\n",
    "GRADIENT ACCUMULATION:\n",
    "  - Use when batch_size is limited by memory\n",
    "  - Effective batch = batch_size * gradient_accumulation_steps\n",
    "  - Example: batch=8, accum=4 -> effective batch=32\n",
    "\n",
    "CHECKPOINTING:\n",
    "  - save_total_limit=2-3 (save disk space)\n",
    "  - load_best_model_at_end=True\n",
    "  - Use early stopping for efficiency\n",
    "\"\"\"\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Train on AG News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Fine-tune a model on the AG News dataset (4-class classification)\n",
    "# \n",
    "# Steps:\n",
    "# 1. Load AG News dataset\n",
    "# 2. Prepare train/eval splits\n",
    "# 3. Tokenize with a transformer tokenizer\n",
    "# 4. Create TrainingArguments\n",
    "# 5. Create Trainer\n",
    "# 6. Train and evaluate\n",
    "#\n",
    "# Hint: AG News has 4 classes, so use num_labels=4\n",
    "\n",
    "# Your code:\n",
    "# ag_news = load_dataset(\"ag_news\")\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting to Rename Label Column\n",
    "\n",
    "```python\n",
    "# Wrong: Trainer expects 'labels'\n",
    "tokenized = dataset.map(tokenize_fn, batched=True)\n",
    "# Error: KeyError 'labels'\n",
    "\n",
    "# Right: Rename the column\n",
    "tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
    "```\n",
    "\n",
    "### Mistake 2: Not Setting load_best_model_at_end\n",
    "\n",
    "```python\n",
    "# Wrong: Final model might be overfit\n",
    "args = TrainingArguments(\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Right: Load best checkpoint at end\n",
    "args = TrainingArguments(\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Mistake 3: Mismatched Strategies\n",
    "\n",
    "```python\n",
    "# Wrong: Can't load best model if save strategy doesn't match eval\n",
    "args = TrainingArguments(\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"steps\",  # Mismatch!\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Right: Match strategies\n",
    "args = TrainingArguments(\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",  # Match!\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- How to configure TrainingArguments for DGX Spark\n",
    "- How to implement custom metrics\n",
    "- How to use callbacks for monitoring\n",
    "- How to train and evaluate a text classifier\n",
    "- How to save and load fine-tuned models\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Trainer Documentation](https://huggingface.co/docs/transformers/main_classes/trainer)\n",
    "- [TrainingArguments Reference](https://huggingface.co/docs/transformers/main_classes/trainer#trainingarguments)\n",
    "- [Custom Training Loops](https://huggingface.co/docs/transformers/training)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del model, trainer, classifier\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"GPU memory after cleanup: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "print(\"\\nLab 2.5.4 complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
