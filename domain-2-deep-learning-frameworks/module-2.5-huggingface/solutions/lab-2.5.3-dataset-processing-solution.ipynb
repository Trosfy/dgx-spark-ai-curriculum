{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.5.3 Solution: AG News Data Pipeline\n",
    "\n",
    "This notebook provides the complete solution for the \"Try It Yourself\" exercise in Lab 2.5.3.\n",
    "\n",
    "**Task**: Build a complete data pipeline for the AG News dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "print(\"AG News Data Pipeline Solution\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the AG News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load AG News dataset\n",
    "print(\"\\nStep 1: Loading AG News dataset...\")\n",
    "ag_news = load_dataset(\"ag_news\")\n",
    "\n",
    "print(f\"Splits: {list(ag_news.keys())}\")\n",
    "print(f\"Train examples: {len(ag_news['train']):,}\")\n",
    "print(f\"Test examples: {len(ag_news['test']):,}\")\n",
    "print(f\"\\nFeatures: {ag_news['train'].features}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample:\")\n",
    "print(ag_news['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Analyze Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Analyze label distribution\n",
    "print(\"\\nStep 2: Label Distribution Analysis\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# AG News categories\n",
    "categories = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "\n",
    "for split in ['train', 'test']:\n",
    "    labels = ag_news[split]['label']\n",
    "    counts = Counter(labels)\n",
    "    total = len(labels)\n",
    "    \n",
    "    print(f\"\\n{split.upper()}:\")\n",
    "    for label_id, count in sorted(counts.items()):\n",
    "        pct = 100 * count / total\n",
    "        print(f\"  {label_id} ({categories[label_id]:10}): {count:6,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Filter Articles > 200 Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Filter to only keep articles > 200 characters\n",
    "print(\"\\nStep 3: Filtering short articles...\")\n",
    "\n",
    "def filter_by_length(example):\n",
    "    return len(example['text']) > 200\n",
    "\n",
    "filtered_train = ag_news['train'].filter(filter_by_length, num_proc=4)\n",
    "filtered_test = ag_news['test'].filter(filter_by_length, num_proc=4)\n",
    "\n",
    "print(f\"Train: {len(ag_news['train']):,} -> {len(filtered_train):,} \")\n",
    "print(f\"       (removed {len(ag_news['train']) - len(filtered_train):,} short articles)\")\n",
    "print(f\"Test: {len(ag_news['test']):,} -> {len(filtered_test):,}\")\n",
    "print(f\"      (removed {len(ag_news['test']) - len(filtered_test):,} short articles)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Train/Val/Test Splits (80/10/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create train/val/test splits (80/10/10)\n",
    "print(\"\\nStep 4: Creating stratified splits...\")\n",
    "\n",
    "# First split: 90% train+val, 10% test\n",
    "split1 = filtered_train.train_test_split(\n",
    "    test_size=0.1,\n",
    "    stratify_by_column='label',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Second split: 88.9% train, 11.1% val (of the 90%)\n",
    "# This gives us ~80% train, ~10% val overall\n",
    "split2 = split1['train'].train_test_split(\n",
    "    test_size=0.111,  # 10/90 = 0.111\n",
    "    stratify_by_column='label',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Create DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': split2['train'],\n",
    "    'validation': split2['test'],\n",
    "    'test': split1['test']\n",
    "})\n",
    "\n",
    "total = sum(len(d) for d in dataset.values())\n",
    "print(f\"\\nFinal splits:\")\n",
    "for split, data in dataset.items():\n",
    "    pct = 100 * len(data) / total\n",
    "    print(f\"  {split:12}: {len(data):,} ({pct:.1f}%)\")\n",
    "\n",
    "# Verify stratification\n",
    "print(\"\\nLabel distribution preserved:\")\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    counts = Counter(dataset[split]['label'])\n",
    "    dist = [counts[i] / len(dataset[split]) for i in range(4)]\n",
    "    print(f\"  {split}: {[f'{d:.1%}' for d in dist]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Tokenize with Transformer Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Tokenize with a transformer tokenizer\n",
    "print(\"\\nStep 5: Tokenizing...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "# Tokenize all splits\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=['text'],\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "# Rename label to labels (for Trainer)\n",
    "tokenized_dataset = tokenized_dataset.rename_column('label', 'labels')\n",
    "\n",
    "print(f\"\\nTokenized columns: {tokenized_dataset['train'].column_names}\")\n",
    "print(f\"Sample input_ids length: {len(tokenized_dataset['train'][0]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Set Format for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Set format for PyTorch\n",
    "print(\"\\nStep 6: Setting PyTorch format...\")\n",
    "\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "# Verify format\n",
    "sample = tokenized_dataset['train'][0]\n",
    "print(f\"\\nSample tensor types:\")\n",
    "for key, val in sample.items():\n",
    "    print(f\"  {key}: {type(val).__name__}, shape: {val.shape if hasattr(val, 'shape') else 'scalar'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"AG NEWS DATASET READY FOR TRAINING!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(tokenized_dataset)\n",
    "\n",
    "print(f\"\\nCategories (4 classes):\")\n",
    "for i, cat in enumerate(categories):\n",
    "    print(f\"  {i}: {cat}\")\n",
    "\n",
    "print(f\"\\nReady to use with:\")\n",
    "print(\"  model = AutoModelForSequenceClassification.from_pretrained(\")\n",
    "print(\"      'distilbert-base-uncased', num_labels=4\")\n",
    "print(\"  )\")\n",
    "print(\"  trainer = Trainer(model=model, train_dataset=tokenized_dataset['train'], ...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Quick Training Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Quick training test\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import evaluate\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=4,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Quick training args (1 epoch for verification)\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./ag_news_test\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    eval_strategy=\"epoch\",\n",
    "    bf16=True,\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset['train'].select(range(5000)),  # Subset for quick test\n",
    "    eval_dataset=tokenized_dataset['validation'].select(range(1000)),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"\\nQuick training test (1 epoch, subset)...\")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "results = trainer.evaluate()\n",
    "print(f\"\\nValidation accuracy: {results['eval_accuracy']:.4f}\")\n",
    "print(\"Pipeline is working correctly!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
