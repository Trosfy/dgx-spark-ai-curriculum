{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.5.1 Solution: Model Comparison Tool\n",
    "\n",
    "This notebook provides the complete solution for the Challenge in Lab 2.5.1.\n",
    "\n",
    "**Task**: Create a model comparison tool that:\n",
    "1. Takes a task type as input\n",
    "2. Finds the top 5 models for that task\n",
    "3. Tests each model on the same inputs\n",
    "4. Compares accuracy, speed, and memory usage\n",
    "5. Generates a recommendation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from huggingface_hub import HfApi\n",
    "from transformers import pipeline\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "import gc\n",
    "\n",
    "print(\"Model Comparison Tool Solution\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "DEVICE = 0 if torch.cuda.is_available() else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Model Comparison Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelBenchmark:\n",
    "    \"\"\"Results from benchmarking a model.\"\"\"\n",
    "    model_id: str\n",
    "    downloads: int\n",
    "    load_time_seconds: float\n",
    "    inference_time_ms: float\n",
    "    memory_gb: float\n",
    "    predictions: List[Dict]\n",
    "    success: bool\n",
    "    error: Optional[str] = None\n",
    "\n",
    "\n",
    "class ModelComparisonTool:\n",
    "    \"\"\"\n",
    "    Tool for comparing multiple models on the same task.\n",
    "    \n",
    "    Supports:\n",
    "    - text-classification\n",
    "    - ner\n",
    "    - question-answering\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, device: int = -1):\n",
    "        self.device = device\n",
    "        self.api = HfApi()\n",
    "        \n",
    "        # Default test inputs for each task\n",
    "        self.default_inputs = {\n",
    "            \"text-classification\": [\n",
    "                \"This product is absolutely amazing! Best purchase ever!\",\n",
    "                \"Terrible quality. Complete waste of money.\",\n",
    "                \"It's okay, does the job but nothing special.\"\n",
    "            ],\n",
    "            \"ner\": [\n",
    "                \"Apple CEO Tim Cook announced the new iPhone in Cupertino.\",\n",
    "                \"Microsoft and Google compete in the cloud computing market.\"\n",
    "            ],\n",
    "            \"question-answering\": [\n",
    "                {\n",
    "                    \"question\": \"What is the capital of France?\",\n",
    "                    \"context\": \"Paris is the capital and largest city of France.\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def find_top_models(self, task: str, limit: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Find top models for a given task.\n",
    "        \n",
    "        Args:\n",
    "            task: Pipeline task type\n",
    "            limit: Maximum number of models\n",
    "            \n",
    "        Returns:\n",
    "            List of model info dictionaries\n",
    "        \"\"\"\n",
    "        print(f\"\\nSearching for top {limit} models for '{task}'...\")\n",
    "        \n",
    "        models = self.api.list_models(\n",
    "            filter=task,\n",
    "            sort=\"downloads\",\n",
    "            direction=-1,\n",
    "            limit=limit * 2  # Get more to filter\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for m in models:\n",
    "            # Skip very large models and non-PyTorch\n",
    "            if 'pytorch' in (m.library_name or '').lower() or m.library_name is None:\n",
    "                results.append({\n",
    "                    \"id\": m.id,\n",
    "                    \"downloads\": m.downloads,\n",
    "                    \"likes\": m.likes\n",
    "                })\n",
    "                if len(results) >= limit:\n",
    "                    break\n",
    "        \n",
    "        print(f\"Found {len(results)} models\")\n",
    "        return results\n",
    "    \n",
    "    def benchmark_model(\n",
    "        self,\n",
    "        model_id: str,\n",
    "        task: str,\n",
    "        test_inputs: List,\n",
    "        downloads: int = 0,\n",
    "        warmup_runs: int = 2\n",
    "    ) -> ModelBenchmark:\n",
    "        \"\"\"\n",
    "        Benchmark a single model.\n",
    "        \n",
    "        Args:\n",
    "            model_id: HuggingFace model ID\n",
    "            task: Pipeline task type\n",
    "            test_inputs: List of test inputs\n",
    "            downloads: Model download count\n",
    "            warmup_runs: Number of warmup inferences\n",
    "            \n",
    "        Returns:\n",
    "            ModelBenchmark with results\n",
    "        \"\"\"\n",
    "        # Clear memory\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() if self.device >= 0 else None\n",
    "        initial_memory = torch.cuda.memory_allocated() / 1e9 if self.device >= 0 else 0\n",
    "        \n",
    "        result = ModelBenchmark(\n",
    "            model_id=model_id,\n",
    "            downloads=downloads,\n",
    "            load_time_seconds=0,\n",
    "            inference_time_ms=0,\n",
    "            memory_gb=0,\n",
    "            predictions=[],\n",
    "            success=False\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Load model\n",
    "            print(f\"  Loading {model_id}...\", end=\" \", flush=True)\n",
    "            start = time.time()\n",
    "            \n",
    "            pipe = pipeline(\n",
    "                task,\n",
    "                model=model_id,\n",
    "                device=self.device,\n",
    "                torch_dtype=torch.bfloat16 if self.device >= 0 else torch.float32\n",
    "            )\n",
    "            \n",
    "            result.load_time_seconds = time.time() - start\n",
    "            result.memory_gb = (torch.cuda.memory_allocated() / 1e9 - initial_memory) if self.device >= 0 else 0\n",
    "            print(f\"loaded in {result.load_time_seconds:.1f}s\")\n",
    "            \n",
    "            # Warmup\n",
    "            for _ in range(warmup_runs):\n",
    "                if task == \"question-answering\":\n",
    "                    pipe(**test_inputs[0])\n",
    "                else:\n",
    "                    pipe(test_inputs[0])\n",
    "            \n",
    "            # Benchmark\n",
    "            torch.cuda.synchronize() if self.device >= 0 else None\n",
    "            start = time.time()\n",
    "            \n",
    "            for inp in test_inputs:\n",
    "                if task == \"question-answering\":\n",
    "                    pred = pipe(**inp)\n",
    "                else:\n",
    "                    pred = pipe(inp)\n",
    "                result.predictions.append(pred)\n",
    "            \n",
    "            torch.cuda.synchronize() if self.device >= 0 else None\n",
    "            result.inference_time_ms = (time.time() - start) * 1000 / len(test_inputs)\n",
    "            result.success = True\n",
    "            \n",
    "            # Cleanup\n",
    "            del pipe\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache() if self.device >= 0 else None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"FAILED: {e}\")\n",
    "            result.error = str(e)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def compare_models(\n",
    "        self,\n",
    "        task: str,\n",
    "        num_models: int = 5,\n",
    "        test_inputs: Optional[List] = None\n",
    "    ) -> List[ModelBenchmark]:\n",
    "        \"\"\"\n",
    "        Compare top models for a task.\n",
    "        \n",
    "        Args:\n",
    "            task: Pipeline task type\n",
    "            num_models: Number of models to compare\n",
    "            test_inputs: Custom test inputs (or use defaults)\n",
    "            \n",
    "        Returns:\n",
    "            List of ModelBenchmark results\n",
    "        \"\"\"\n",
    "        # Get test inputs\n",
    "        if test_inputs is None:\n",
    "            test_inputs = self.default_inputs.get(task, [\"Test input\"])\n",
    "        \n",
    "        # Find models\n",
    "        models = self.find_top_models(task, num_models)\n",
    "        \n",
    "        # Benchmark each\n",
    "        print(f\"\\nBenchmarking {len(models)} models...\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        results = []\n",
    "        for model in models:\n",
    "            benchmark = self.benchmark_model(\n",
    "                model[\"id\"],\n",
    "                task,\n",
    "                test_inputs,\n",
    "                downloads=model[\"downloads\"]\n",
    "            )\n",
    "            results.append(benchmark)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_report(self, results: List[ModelBenchmark]) -> str:\n",
    "        \"\"\"\n",
    "        Generate a comparison report with recommendation.\n",
    "        \n",
    "        Args:\n",
    "            results: List of benchmark results\n",
    "            \n",
    "        Returns:\n",
    "            Formatted report string\n",
    "        \"\"\"\n",
    "        successful = [r for r in results if r.success]\n",
    "        \n",
    "        if not successful:\n",
    "            return \"No models completed successfully.\"\n",
    "        \n",
    "        report = []\n",
    "        report.append(\"\\n\" + \"=\" * 80)\n",
    "        report.append(\"MODEL COMPARISON REPORT\")\n",
    "        report.append(\"=\" * 80)\n",
    "        \n",
    "        # Table header\n",
    "        report.append(f\"\\n{'Model':<45} {'Load(s)':<10} {'Infer(ms)':<12} {'Memory(GB)':<12}\")\n",
    "        report.append(\"-\" * 80)\n",
    "        \n",
    "        # Sort by inference time\n",
    "        sorted_results = sorted(successful, key=lambda x: x.inference_time_ms)\n",
    "        \n",
    "        for r in sorted_results:\n",
    "            model_short = r.model_id[:44] if len(r.model_id) > 44 else r.model_id\n",
    "            report.append(\n",
    "                f\"{model_short:<45} {r.load_time_seconds:<10.2f} \"\n",
    "                f\"{r.inference_time_ms:<12.2f} {r.memory_gb:<12.2f}\"\n",
    "            )\n",
    "        \n",
    "        # Recommendations\n",
    "        report.append(\"\\n\" + \"=\" * 80)\n",
    "        report.append(\"RECOMMENDATIONS\")\n",
    "        report.append(\"=\" * 80)\n",
    "        \n",
    "        # Fastest\n",
    "        fastest = min(successful, key=lambda x: x.inference_time_ms)\n",
    "        report.append(f\"\\n FASTEST: {fastest.model_id}\")\n",
    "        report.append(f\"   Inference: {fastest.inference_time_ms:.2f}ms\")\n",
    "        \n",
    "        # Smallest memory\n",
    "        smallest = min(successful, key=lambda x: x.memory_gb if x.memory_gb > 0 else float('inf'))\n",
    "        report.append(f\"\\n SMALLEST: {smallest.model_id}\")\n",
    "        report.append(f\"   Memory: {smallest.memory_gb:.2f} GB\")\n",
    "        \n",
    "        # Most popular (by downloads)\n",
    "        most_popular = max(successful, key=lambda x: x.downloads)\n",
    "        report.append(f\"\\n MOST POPULAR: {most_popular.model_id}\")\n",
    "        report.append(f\"   Downloads: {most_popular.downloads:,}\")\n",
    "        \n",
    "        # Overall recommendation (balance of speed and popularity)\n",
    "        def score(r):\n",
    "            # Lower is better for time, higher is better for downloads\n",
    "            time_score = r.inference_time_ms / max(x.inference_time_ms for x in successful)\n",
    "            pop_score = r.downloads / max(x.downloads for x in successful)\n",
    "            return pop_score - time_score * 0.5  # Weight popularity more\n",
    "        \n",
    "        best_overall = max(successful, key=score)\n",
    "        report.append(f\"\\n OVERALL RECOMMENDATION: {best_overall.model_id}\")\n",
    "        report.append(f\"   Good balance of speed ({best_overall.inference_time_ms:.1f}ms) \")\n",
    "        report.append(f\"   and community trust ({best_overall.downloads:,} downloads)\")\n",
    "        \n",
    "        report.append(\"\\n\" + \"=\" * 80)\n",
    "        \n",
    "        return \"\\n\".join(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "tool = ModelComparisonTool(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare text classification models\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Comparing TEXT CLASSIFICATION models\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = tool.compare_models(\n",
    "    task=\"text-classification\",\n",
    "    num_models=5\n",
    ")\n",
    "\n",
    "print(tool.generate_report(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare NER models\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Comparing NER models\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ner_results = tool.compare_models(\n",
    "    task=\"ner\",\n",
    "    num_models=3  # Fewer for faster demo\n",
    ")\n",
    "\n",
    "print(tool.generate_report(ner_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom test inputs\n",
    "custom_inputs = [\n",
    "    \"I absolutely love this new feature!\",\n",
    "    \"This update broke everything. Very disappointed.\",\n",
    "    \"Works as expected, no complaints.\",\n",
    "    \"Best software I've ever used!\",\n",
    "    \"Buggy and slow. Needs improvement.\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Comparing with CUSTOM test inputs\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "custom_results = tool.compare_models(\n",
    "    task=\"text-classification\",\n",
    "    num_models=3,\n",
    "    test_inputs=custom_inputs\n",
    ")\n",
    "\n",
    "print(tool.generate_report(custom_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "print(\"\\nSolution complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
