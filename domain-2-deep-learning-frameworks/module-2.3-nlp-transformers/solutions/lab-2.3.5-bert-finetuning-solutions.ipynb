{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions: Lab 2.3.5 - BERT Fine-tuning\n",
    "\n",
    "This notebook contains solutions to the exercises from notebook 05.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import AdamW  # Use PyTorch's AdamW (not deprecated transformers.AdamW)\n\ntry:\n    from transformers import BertModel, BertTokenizer, get_linear_schedule_with_warmup\n    HAS_TRANSFORMERS = True\nexcept ImportError:\n    HAS_TRANSFORMERS = False\n    print(\"Please install transformers: pip install transformers\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Multi-class Classification\n",
    "\n",
    "**Task:** Modify the binary classifier to handle multiple classes (e.g., emotion detection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMultiClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT for multi-class classification.\n",
    "    \n",
    "    Modifications from binary classifier:\n",
    "    - Output layer has num_classes instead of 1\n",
    "    - Uses CrossEntropyLoss instead of BCEWithLogitsLoss\n",
    "    - No sigmoid activation (handled by CrossEntropyLoss)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, dropout=0.3, freeze_bert=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, num_classes)  # num_classes outputs\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.pooler_output\n",
    "        return self.classifier(pooled)  # Returns logits for each class\n",
    "\n",
    "# Example usage for emotion classification (6 classes)\n",
    "if HAS_TRANSFORMERS:\n",
    "    EMOTION_LABELS = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
    "    num_classes = len(EMOTION_LABELS)\n",
    "    \n",
    "    model = BertMultiClassifier(num_classes=num_classes, freeze_bert=True)\n",
    "    print(f\"Model for {num_classes}-class classification created\")\n",
    "    print(f\"Labels: {EMOTION_LABELS}\")\n",
    "    \n",
    "    # Loss function for multi-class\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Example forward pass\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    text = \"I am so happy today!\"\n",
    "    encoding = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(encoding['input_ids'], encoding['attention_mask'])\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        pred = probs.argmax(dim=-1).item()\n",
    "    \n",
    "    print(f\"\\nExample prediction for '{text}':\")\n",
    "    print(f\"Predicted: {EMOTION_LABELS[pred]} ({probs[0, pred]:.2%} confidence)\")\n",
    "    print(f\"\\nAll probabilities:\")\n",
    "    for i, label in enumerate(EMOTION_LABELS):\n",
    "        print(f\"  {label}: {probs[0, i]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Learning Rate Scheduling\n",
    "\n",
    "**Task:** Implement different learning rate schedules and compare their effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_lr_schedules(num_epochs=3, steps_per_epoch=100):\n",
    "    \"\"\"\n",
    "    Compare different learning rate schedules.\n",
    "    \"\"\"\n",
    "    total_steps = num_epochs * steps_per_epoch\n",
    "    warmup_steps = int(0.1 * total_steps)  # 10% warmup\n",
    "    \n",
    "    # Create dummy optimizer\n",
    "    model = nn.Linear(10, 2)\n",
    "    base_lr = 2e-5\n",
    "    \n",
    "    schedules = {}\n",
    "    \n",
    "    # 1. Linear warmup + decay\n",
    "    optimizer1 = torch.optim.AdamW(model.parameters(), lr=base_lr)\n",
    "    scheduler1 = get_linear_schedule_with_warmup(\n",
    "        optimizer1, \n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    lrs1 = []\n",
    "    for _ in range(total_steps):\n",
    "        lrs1.append(optimizer1.param_groups[0]['lr'])\n",
    "        scheduler1.step()\n",
    "    schedules['Linear warmup + decay'] = lrs1\n",
    "    \n",
    "    # 2. Cosine annealing\n",
    "    optimizer2 = torch.optim.AdamW(model.parameters(), lr=base_lr)\n",
    "    scheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer2, \n",
    "        T_max=total_steps\n",
    "    )\n",
    "    \n",
    "    lrs2 = []\n",
    "    for _ in range(total_steps):\n",
    "        lrs2.append(optimizer2.param_groups[0]['lr'])\n",
    "        scheduler2.step()\n",
    "    schedules['Cosine annealing'] = lrs2\n",
    "    \n",
    "    # 3. Step decay\n",
    "    optimizer3 = torch.optim.AdamW(model.parameters(), lr=base_lr)\n",
    "    scheduler3 = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer3,\n",
    "        step_size=steps_per_epoch,\n",
    "        gamma=0.5\n",
    "    )\n",
    "    \n",
    "    lrs3 = []\n",
    "    for _ in range(total_steps):\n",
    "        lrs3.append(optimizer3.param_groups[0]['lr'])\n",
    "        scheduler3.step()\n",
    "    schedules['Step decay'] = lrs3\n",
    "    \n",
    "    # 4. Constant LR (baseline)\n",
    "    schedules['Constant'] = [base_lr] * total_steps\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    for name, lrs in schedules.items():\n",
    "        plt.plot(lrs, label=name, linewidth=2)\n",
    "    \n",
    "    # Add epoch markers\n",
    "    for i in range(1, num_epochs):\n",
    "        plt.axvline(x=i * steps_per_epoch, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate Schedules Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Recommendations:\")\n",
    "    print(\"- Linear warmup + decay: Standard for BERT fine-tuning\")\n",
    "    print(\"- Cosine annealing: Good for longer training\")\n",
    "    print(\"- Step decay: Simple but less smooth\")\n",
    "    print(\"- Constant: Usually not recommended for fine-tuning\")\n",
    "\n",
    "if HAS_TRANSFORMERS:\n",
    "    visualize_lr_schedules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Layer-wise Learning Rate Decay\n",
    "\n",
    "**Task:** Implement LLRD (Layer-wise Learning Rate Decay) where earlier layers get lower learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llrd_params(model, base_lr=2e-5, decay_factor=0.9):\n",
    "    \"\"\"\n",
    "    Get parameter groups with layer-wise learning rate decay.\n",
    "    \n",
    "    Earlier layers get lower learning rates:\n",
    "    - Layer 0: base_lr * decay_factor^11\n",
    "    - Layer 11: base_lr * decay_factor^0 = base_lr\n",
    "    - Classifier: base_lr\n",
    "    \n",
    "    Args:\n",
    "        model: BERT-based model\n",
    "        base_lr: Learning rate for top layers\n",
    "        decay_factor: How much to reduce LR for each layer down\n",
    "    \"\"\"\n",
    "    param_groups = []\n",
    "    \n",
    "    # Group parameters by layer\n",
    "    # Embeddings get lowest LR\n",
    "    embeddings_params = []\n",
    "    for name, param in model.bert.embeddings.named_parameters():\n",
    "        embeddings_params.append(param)\n",
    "    \n",
    "    num_layers = 12  # BERT-base has 12 layers\n",
    "    embedding_lr = base_lr * (decay_factor ** num_layers)\n",
    "    param_groups.append({\n",
    "        'params': embeddings_params,\n",
    "        'lr': embedding_lr,\n",
    "        'name': 'embeddings'\n",
    "    })\n",
    "    \n",
    "    # Encoder layers\n",
    "    for layer_idx in range(num_layers):\n",
    "        layer_params = []\n",
    "        for name, param in model.bert.encoder.layer[layer_idx].named_parameters():\n",
    "            layer_params.append(param)\n",
    "        \n",
    "        layer_lr = base_lr * (decay_factor ** (num_layers - 1 - layer_idx))\n",
    "        param_groups.append({\n",
    "            'params': layer_params,\n",
    "            'lr': layer_lr,\n",
    "            'name': f'layer_{layer_idx}'\n",
    "        })\n",
    "    \n",
    "    # Pooler and classifier get full LR\n",
    "    classifier_params = list(model.bert.pooler.parameters()) + list(model.classifier.parameters())\n",
    "    param_groups.append({\n",
    "        'params': classifier_params,\n",
    "        'lr': base_lr,\n",
    "        'name': 'classifier'\n",
    "    })\n",
    "    \n",
    "    return param_groups\n",
    "\n",
    "if HAS_TRANSFORMERS:\n",
    "    # Create model and get LLRD params\n",
    "    model = BertMultiClassifier(num_classes=6, freeze_bert=False)\n",
    "    param_groups = get_llrd_params(model, base_lr=2e-5, decay_factor=0.9)\n",
    "    \n",
    "    print(\"Layer-wise Learning Rate Decay:\")\n",
    "    print(\"=\" * 50)\n",
    "    for group in param_groups:\n",
    "        print(f\"  {group['name']}: lr = {group['lr']:.2e}\")\n",
    "    \n",
    "    # Create optimizer with LLRD\n",
    "    optimizer = torch.optim.AdamW(param_groups, weight_decay=0.01)\n",
    "    print(f\"\\nOptimizer created with {len(param_groups)} parameter groups\")\n",
    "    \n",
    "    # Visualize\n",
    "    lrs = [g['lr'] for g in param_groups]\n",
    "    names = [g['name'] for g in param_groups]\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.bar(range(len(lrs)), lrs)\n",
    "    plt.xticks(range(len(names)), names, rotation=45, ha='right')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Layer-wise Learning Rate Decay (LLRD)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Multi-class classification** uses CrossEntropyLoss and outputs logits for each class\n",
    "2. **Learning rate scheduling** is crucial - linear warmup + decay is standard for BERT\n",
    "3. **LLRD** gives lower learning rates to earlier layers, preserving pre-trained features\n",
    "4. **Freezing** earlier layers can speed up training while maintaining most performance\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}