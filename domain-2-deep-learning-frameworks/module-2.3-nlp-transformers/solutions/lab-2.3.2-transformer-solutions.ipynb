{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions: Lab 2.3.2 - Transformer Block\n",
    "\n",
    "This notebook contains solutions to the exercises from notebook 02.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Decoder Block\n",
    "\n",
    "**Task:** Create a TransformerDecoderBlock with masked self-attention and cross-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention (from notebook 01).\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        seq_q, seq_k = query.size(1), key.size(1)\n",
    "        \n",
    "        Q = self.W_q(query).view(batch_size, seq_q, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, seq_k, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, seq_k, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "        \n",
    "        context = torch.matmul(attention, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_q, self.d_model)\n",
    "        \n",
    "        return self.W_o(context)\n",
    "\n",
    "\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Decoder block with:\n",
    "    1. Masked self-attention\n",
    "    2. Cross-attention to encoder\n",
    "    3. Feed-forward network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "        \n",
    "        # Self-attention (masked)\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Cross-attention\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Feed-forward\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        # Layer norms\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, encoder_output, self_mask=None, cross_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Decoder input (batch, tgt_len, d_model)\n",
    "            encoder_output: Encoder output (batch, src_len, d_model)\n",
    "            self_mask: Causal mask for self-attention\n",
    "            cross_mask: Mask for cross-attention (e.g., padding mask)\n",
    "        \"\"\"\n",
    "        # 1. Masked self-attention\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.self_attn(x, x, x, self_mask)\n",
    "        x = residual + self.dropout(x)\n",
    "        \n",
    "        # 2. Cross-attention\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.cross_attn(x, encoder_output, encoder_output, cross_mask)\n",
    "        x = residual + self.dropout(x)\n",
    "        \n",
    "        # 3. Feed-forward\n",
    "        residual = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.ffn(x)\n",
    "        x = residual + self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test\n",
    "decoder_block = TransformerDecoderBlock(512, 8)\n",
    "decoder_input = torch.randn(2, 10, 512)\n",
    "encoder_output = torch.randn(2, 20, 512)\n",
    "\n",
    "# Create causal mask\n",
    "tgt_len = 10\n",
    "causal_mask = torch.tril(torch.ones(tgt_len, tgt_len)).bool()\n",
    "causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)  # (1, 1, tgt, tgt)\n",
    "\n",
    "out = decoder_block(decoder_input, encoder_output, self_mask=causal_mask)\n",
    "print(f\"Decoder input shape: {decoder_input.shape}\")\n",
    "print(f\"Encoder output shape: {encoder_output.shape}\")\n",
    "print(f\"Decoder block output shape: {out.shape}\")\n",
    "print(\"\\nDecoder block implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Gradient Checkpointing\n",
    "\n",
    "**Task:** Implement gradient checkpointing to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderCheckpointed(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder with gradient checkpointing.\n",
    "    \n",
    "    This saves memory by not storing all activations,\n",
    "    at the cost of recomputing them during backward pass.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            self._make_layer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def _make_layer(self, d_model, num_heads, d_ff, dropout):\n",
    "        return nn.ModuleDict({\n",
    "            'attn': MultiHeadAttention(d_model, num_heads, dropout),\n",
    "            'ffn': nn.Sequential(\n",
    "                nn.Linear(d_model, d_ff),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(d_ff, d_model)\n",
    "            ),\n",
    "            'norm1': nn.LayerNorm(d_model),\n",
    "            'norm2': nn.LayerNorm(d_model),\n",
    "            'dropout': nn.Dropout(dropout)\n",
    "        })\n",
    "    \n",
    "    def _forward_layer(self, layer, x, mask):\n",
    "        \"\"\"Forward pass through a single layer.\"\"\"\n",
    "        # Self-attention\n",
    "        residual = x\n",
    "        x = layer['norm1'](x)\n",
    "        x = layer['attn'](x, x, x, mask)\n",
    "        x = residual + layer['dropout'](x)\n",
    "        \n",
    "        # FFN\n",
    "        residual = x\n",
    "        x = layer['norm2'](x)\n",
    "        x = layer['ffn'](x)\n",
    "        x = residual + layer['dropout'](x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x, mask=None, use_checkpointing=True):\n",
    "        \"\"\"\n",
    "        Forward pass with optional gradient checkpointing.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            mask: Attention mask\n",
    "            use_checkpointing: Whether to use gradient checkpointing\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            if use_checkpointing and self.training:\n",
    "                # Use checkpoint to save memory\n",
    "                # Note: checkpoint requires the function to not have any side effects\n",
    "                x = checkpoint(\n",
    "                    self._forward_layer,\n",
    "                    layer, x, mask,\n",
    "                    use_reentrant=False\n",
    "                )\n",
    "            else:\n",
    "                x = self._forward_layer(layer, x, mask)\n",
    "        \n",
    "        return self.final_norm(x)\n",
    "\n",
    "# Test\n",
    "encoder = TransformerEncoderCheckpointed(\n",
    "    num_layers=6,\n",
    "    d_model=512,\n",
    "    num_heads=8\n",
    ")\n",
    "\n",
    "x = torch.randn(2, 100, 512, requires_grad=True)\n",
    "\n",
    "# With checkpointing\n",
    "encoder.train()\n",
    "out = encoder(x, use_checkpointing=True)\n",
    "loss = out.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Gradient computed: {x.grad is not None}\")\n",
    "print(\"\\nGradient checkpointing implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (Challenge): RMSNorm\n",
    "\n",
    "**Task:** Implement RMSNorm as an alternative to LayerNorm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Root Mean Square Normalization (used in LLaMA).\n",
    "    \n",
    "    RMSNorm(x) = x * scale / sqrt(mean(x^2) + epsilon)\n",
    "    \n",
    "    Slightly faster than LayerNorm since it doesn't compute the mean.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(d_model))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Compute RMS\n",
    "        rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
    "        \n",
    "        # Normalize and scale\n",
    "        return x / rms * self.scale\n",
    "\n",
    "# Test\n",
    "rms = RMSNorm(512)\n",
    "x = torch.randn(2, 10, 512)\n",
    "out = rms(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "\n",
    "# Compare with LayerNorm\n",
    "ln = nn.LayerNorm(512)\n",
    "\n",
    "import time\n",
    "\n",
    "x_large = torch.randn(32, 1000, 512)\n",
    "\n",
    "# Time RMSNorm\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = rms(x_large)\n",
    "rms_time = time.time() - start\n",
    "\n",
    "# Time LayerNorm\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = ln(x_large)\n",
    "ln_time = time.time() - start\n",
    "\n",
    "print(f\"\\nSpeed comparison (100 iterations):\")\n",
    "print(f\"  RMSNorm:   {rms_time*1000:.2f} ms\")\n",
    "print(f\"  LayerNorm: {ln_time*1000:.2f} ms\")\n",
    "print(f\"  Speedup:   {ln_time/rms_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Decoder blocks** have three sublayers: masked self-attention, cross-attention, and FFN\n",
    "2. **Gradient checkpointing** trades compute for memory - useful for large models\n",
    "3. **RMSNorm** is faster than LayerNorm and works well in practice\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}