{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions: Lab 2.3.3 - Positional Encoding Study\n",
    "\n",
    "This notebook contains solutions to the exercises from notebook 03.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Implement RoPE (Rotary Position Embeddings)\n",
    "\n",
    "**Task:** Implement Rotary Position Embeddings used in LLaMA and modern LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE(nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary Position Embeddings.\n",
    "    \n",
    "    Applies rotation to Q and K based on position, making the\n",
    "    dot product naturally encode relative position.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim, max_seq_len=2048, base=10000):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert dim % 2 == 0, \"Dimension must be even for RoPE\"\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.base = base\n",
    "        \n",
    "        # Compute inverse frequencies: theta_i = 1 / (base^(2i/dim))\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        \n",
    "        # Pre-compute cos and sin\n",
    "        self._build_cache(max_seq_len)\n",
    "    \n",
    "    def _build_cache(self, seq_len):\n",
    "        \"\"\"Pre-compute sin/cos values.\"\"\"\n",
    "        t = torch.arange(seq_len, device=self.inv_freq.device)\n",
    "        freqs = torch.outer(t, self.inv_freq)  # (seq_len, dim/2)\n",
    "        \n",
    "        # Create [cos, cos] and [sin, sin] for each position\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)  # (seq_len, dim)\n",
    "        \n",
    "        self.register_buffer('cos_cached', emb.cos(), persistent=False)\n",
    "        self.register_buffer('sin_cached', emb.sin(), persistent=False)\n",
    "    \n",
    "    def _rotate_half(self, x):\n",
    "        \"\"\"Rotate half the hidden dims of x.\"\"\"\n",
    "        x1 = x[..., :x.shape[-1] // 2]\n",
    "        x2 = x[..., x.shape[-1] // 2:]\n",
    "        return torch.cat([-x2, x1], dim=-1)\n",
    "    \n",
    "    def forward(self, q, k, seq_len=None):\n",
    "        \"\"\"\n",
    "        Apply RoPE to queries and keys.\n",
    "        \n",
    "        Args:\n",
    "            q: Query tensor (batch, heads, seq_len, dim)\n",
    "            k: Key tensor (batch, heads, seq_len, dim)\n",
    "            seq_len: Sequence length (inferred if not provided)\n",
    "        \"\"\"\n",
    "        if seq_len is None:\n",
    "            seq_len = q.size(2)\n",
    "        \n",
    "        # Extend cache if needed\n",
    "        if seq_len > self.max_seq_len:\n",
    "            self._build_cache(seq_len)\n",
    "            self.max_seq_len = seq_len\n",
    "        \n",
    "        cos = self.cos_cached[:seq_len].to(q.dtype)\n",
    "        sin = self.sin_cached[:seq_len].to(q.dtype)\n",
    "        \n",
    "        # Reshape for broadcasting: (1, 1, seq, dim)\n",
    "        cos = cos.unsqueeze(0).unsqueeze(0)\n",
    "        sin = sin.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Apply rotation: x * cos + rotate_half(x) * sin\n",
    "        q_embed = (q * cos) + (self._rotate_half(q) * sin)\n",
    "        k_embed = (k * cos) + (self._rotate_half(k) * sin)\n",
    "        \n",
    "        return q_embed, k_embed\n",
    "\n",
    "# Test RoPE\n",
    "rope = RoPE(dim=64, max_seq_len=1024)\n",
    "q = torch.randn(2, 8, 100, 64)  # (batch, heads, seq, dim)\n",
    "k = torch.randn(2, 8, 100, 64)\n",
    "\n",
    "q_rot, k_rot = rope(q, k)\n",
    "\n",
    "print(f\"Input Q shape: {q.shape}\")\n",
    "print(f\"Output Q shape: {q_rot.shape}\")\n",
    "print(f\"\\nRoPE preserves shape and applies position-dependent rotation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Implement ALiBi (Attention with Linear Biases)\n",
    "\n",
    "**Task:** Implement ALiBi which adds linear biases based on relative position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALiBi(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention with Linear Biases.\n",
    "    \n",
    "    Adds a position-dependent bias to attention scores:\n",
    "    attention_scores = Q @ K^T + alibi_bias\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Compute slopes for each head\n",
    "        slopes = self._get_slopes(num_heads)\n",
    "        self.register_buffer('slopes', torch.tensor(slopes).float())\n",
    "    \n",
    "    def _get_slopes(self, num_heads):\n",
    "        \"\"\"Compute ALiBi slopes for each head.\"\"\"\n",
    "        def get_slopes_power_of_2(n):\n",
    "            start = 2 ** (-8 / n)\n",
    "            ratio = start\n",
    "            return [start * (ratio ** i) for i in range(n)]\n",
    "        \n",
    "        if math.log2(num_heads).is_integer():\n",
    "            return get_slopes_power_of_2(num_heads)\n",
    "        else:\n",
    "            # For non-power-of-2, interpolate\n",
    "            closest_power_of_2 = 2 ** math.floor(math.log2(num_heads))\n",
    "            slopes = get_slopes_power_of_2(closest_power_of_2)\n",
    "            extra = get_slopes_power_of_2(2 * closest_power_of_2)[0::2]\n",
    "            return slopes + extra[:num_heads - closest_power_of_2]\n",
    "    \n",
    "    def forward(self, seq_len):\n",
    "        \"\"\"\n",
    "        Compute ALiBi bias matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Bias tensor (num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # Create relative position matrix\n",
    "        positions = torch.arange(seq_len, device=self.slopes.device)\n",
    "        relative_pos = positions.unsqueeze(0) - positions.unsqueeze(1)\n",
    "        \n",
    "        # Apply negative absolute value (closer = higher score)\n",
    "        bias = -torch.abs(relative_pos).float()\n",
    "        \n",
    "        # Scale by head-specific slope\n",
    "        bias = bias.unsqueeze(0) * self.slopes.unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        return bias\n",
    "\n",
    "# Test ALiBi\n",
    "alibi = ALiBi(num_heads=8)\n",
    "bias = alibi(seq_len=10)\n",
    "\n",
    "print(f\"ALiBi bias shape: {bias.shape}\")\n",
    "print(f\"\\nBias for head 0 (first 5x5):\")\n",
    "print(bias[0, :5, :5].numpy().round(3))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.imshow(bias[i].numpy(), cmap='RdBu_r')\n",
    "    plt.title(f'Head {i}')\n",
    "    plt.colorbar()\n",
    "plt.suptitle('ALiBi Bias Patterns for Different Heads')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Compare Position Encoding Methods\n",
    "\n",
    "**Task:** Compare how different position encodings affect attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compare_position_encodings():\n",
    "    \"\"\"\n",
    "    Compare different position encoding methods.\n",
    "    \"\"\"\n",
    "    seq_len = 50\n",
    "    d_model = 64\n",
    "    num_heads = 4\n",
    "    \n",
    "    # Create random Q, K\n",
    "    Q = torch.randn(1, num_heads, seq_len, d_model // num_heads)\n",
    "    K = torch.randn(1, num_heads, seq_len, d_model // num_heads)\n",
    "    \n",
    "    # 1. No position encoding (baseline)\n",
    "    scores_none = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_model // num_heads)\n",
    "    attn_none = F.softmax(scores_none, dim=-1)\n",
    "    \n",
    "    # 2. With RoPE\n",
    "    rope = RoPE(dim=d_model // num_heads)\n",
    "    Q_rope, K_rope = rope(Q, K)\n",
    "    scores_rope = torch.matmul(Q_rope, K_rope.transpose(-2, -1)) / math.sqrt(d_model // num_heads)\n",
    "    attn_rope = F.softmax(scores_rope, dim=-1)\n",
    "    \n",
    "    # 3. With ALiBi\n",
    "    alibi = ALiBi(num_heads)\n",
    "    alibi_bias = alibi(seq_len)\n",
    "    scores_alibi = scores_none + alibi_bias\n",
    "    attn_alibi = F.softmax(scores_alibi, dim=-1)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    axes[0].imshow(attn_none[0, 0].detach().numpy(), cmap='Blues')\n",
    "    axes[0].set_title('No Position Encoding')\n",
    "    axes[0].set_xlabel('Key Position')\n",
    "    axes[0].set_ylabel('Query Position')\n",
    "    \n",
    "    axes[1].imshow(attn_rope[0, 0].detach().numpy(), cmap='Blues')\n",
    "    axes[1].set_title('With RoPE')\n",
    "    axes[1].set_xlabel('Key Position')\n",
    "    \n",
    "    axes[2].imshow(attn_alibi[0, 0].detach().numpy(), cmap='Blues')\n",
    "    axes[2].set_title('With ALiBi')\n",
    "    axes[2].set_xlabel('Key Position')\n",
    "    \n",
    "    plt.suptitle('Attention Patterns with Different Position Encodings (Head 0)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Observations:\")\n",
    "    print(\"- No position: Attention based purely on content\")\n",
    "    print(\"- RoPE: Relative position affects Q-K similarity\")\n",
    "    print(\"- ALiBi: Strong bias toward nearby positions (local attention)\")\n",
    "\n",
    "compare_position_encodings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **RoPE** encodes position by rotating Q and K, making dot product sensitive to relative position\n",
    "2. **ALiBi** adds a linear bias that penalizes distant tokens, promoting local attention\n",
    "3. Both methods enable better length extrapolation than absolute position embeddings\n",
    "4. RoPE is used in LLaMA, Mistral, etc.; ALiBi is used in BLOOM, MPT\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}