{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 2.3.6: GPT Text Generation\n\n**Module:** 2.3 - Natural Language Processing & Transformers  \n**Time:** 2 hours  \n**Difficulty:** â­â­â­â­\n\n---\n\n## Learning Objectives\n\nBy the end of this notebook, you will:\n- [ ] Understand autoregressive language modeling\n- [ ] Load and use GPT-2 for text generation\n- [ ] Implement greedy decoding from scratch\n- [ ] Implement temperature sampling\n- [ ] Implement top-k and top-p (nucleus) sampling\n- [ ] Implement beam search\n- [ ] Compare different decoding strategies\n\n---\n\n## Prerequisites\n\n- Completed: Labs 2.3.1-2.3.5\n- Knowledge of: Probability distributions, sampling\n\n---\n\n## Real-World Context\n\n**Text generation powers:**\n- **ChatGPT, Claude** - Conversational AI\n- **GitHub Copilot** - Code generation\n- **Jasper, Copy.ai** - Marketing copy\n- **AI Dungeon** - Interactive storytelling\n\nThe decoding strategy dramatically affects output quality:\n- Greedy: Fast but repetitive\n- Sampling: Creative but can be random\n- Beam search: High quality but less diverse\n- Top-p: Good balance of quality and creativity"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: How GPT Generates Text\n",
    "\n",
    "> **Imagine a very good game of \"complete the sentence.\"**\n",
    ">\n",
    "> You start with: \"The cat sat on the...\"\n",
    "> GPT thinks: \"What word usually comes next?\"\n",
    "> - \"mat\" - 40% likely\n",
    "> - \"chair\" - 25% likely\n",
    "> - \"floor\" - 20% likely\n",
    "> - \"sofa\" - 10% likely\n",
    "> - ... (thousands of options)\n",
    ">\n",
    "> **Different strategies for picking the next word:**\n",
    "> - **Greedy**: Always pick \"mat\" (highest probability)\n",
    "> - **Sampling**: Randomly pick based on probabilities (might get \"sofa\")\n",
    "> - **Top-k**: Only consider top 50 options, then sample\n",
    "> - **Top-p**: Only consider options until you have 90% probability mass\n",
    ">\n",
    "> Then repeat: \"The cat sat on the mat...\" â†’ what's next?\n",
    "\n",
    "### Key Insight: Autoregressive Generation\n",
    "\n",
    "GPT generates one token at a time, feeding its output back as input:\n",
    "\n",
    "```\n",
    "Input: \"The cat\"     â†’ Output: \"sat\"\n",
    "Input: \"The cat sat\" â†’ Output: \"on\"\n",
    "Input: \"The cat sat on\" â†’ Output: \"the\"\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup and Load GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import time\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 model and tokenizer\n",
    "print(\"Loading GPT-2...\")\n",
    "\n",
    "# Use GPT-2 Medium for better quality (on DGX Spark, we can use larger models!)\n",
    "model_name = \"gpt2\"  # or \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Set pad token (GPT-2 doesn't have one by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"\\nModel: {model_name}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test: get next token probabilities\n",
    "\n",
    "prompt = \"The capital of France is\"\n",
    "\n",
    "# Tokenize\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Token IDs: {input_ids[0].tolist()}\")\n",
    "print(f\"Tokens: {tokenizer.convert_ids_to_tokens(input_ids[0])}\")\n",
    "\n",
    "# Get model output\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits  # (batch, seq_len, vocab_size)\n",
    "\n",
    "# Get probabilities for next token\n",
    "next_token_logits = logits[0, -1, :]  # Last position\n",
    "probs = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "# Top 10 most likely next tokens\n",
    "top_probs, top_indices = probs.topk(10)\n",
    "\n",
    "print(f\"\\nTop 10 most likely next tokens:\")\n",
    "for prob, idx in zip(top_probs, top_indices):\n",
    "    token = tokenizer.decode([idx])\n",
    "    print(f\"  '{token}': {prob.item()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Greedy Decoding\n",
    "\n",
    "The simplest strategy: always pick the most likely next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, tokenizer, prompt, max_length=50, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate text using greedy decoding.\n",
    "    \n",
    "    At each step, pick the token with highest probability.\n",
    "    \n",
    "    Args:\n",
    "        model: GPT-2 model\n",
    "        tokenizer: GPT-2 tokenizer\n",
    "        prompt: Starting text\n",
    "        max_length: Maximum tokens to generate\n",
    "        device: Device to run on\n",
    "        \n",
    "    Returns:\n",
    "        Generated text\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    generated = input_ids[0].tolist()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(torch.tensor([generated]).to(device))\n",
    "            logits = outputs.logits[0, -1, :]  # Last token's predictions\n",
    "        \n",
    "        # Greedy: pick highest probability token\n",
    "        next_token = logits.argmax().item()\n",
    "        \n",
    "        # Stop if we hit end-of-sequence\n",
    "        if next_token == tokenizer.eos_token_id:\n",
    "            break\n",
    "            \n",
    "        generated.append(next_token)\n",
    "    \n",
    "    return tokenizer.decode(generated)\n",
    "\n",
    "# Test greedy decoding\n",
    "prompt = \"Artificial intelligence will\"\n",
    "result = greedy_decode(model, tokenizer, prompt, max_length=30, device=device)\n",
    "\n",
    "print(\"Greedy Decoding:\")\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Output: '{result}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem with greedy: repetition!\n",
    "\n",
    "print(\"Greedy decoding problem - repetition:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"The meaning of life is\",\n",
    "    \"In a galaxy far far away\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    result = greedy_decode(model, tokenizer, prompt, max_length=50, device=device)\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(f\"Output: {result}\")\n",
    "\n",
    "print(\"\\nâš ï¸ Notice the repetition in the outputs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Temperature Sampling\n",
    "\n",
    "### ELI5: What is Temperature?\n",
    "\n",
    "> **Temperature controls \"confidence\" in the distribution.**\n",
    ">\n",
    "> - **Temperature = 1.0**: Use probabilities as-is\n",
    "> - **Temperature < 1.0**: More confident (sharper distribution, more likely to pick top token)\n",
    "> - **Temperature > 1.0**: Less confident (flatter distribution, more random)\n",
    "\n",
    "Mathematically: `probs = softmax(logits / temperature)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temperature effect\n",
    "\n",
    "logits = torch.tensor([5.0, 3.0, 2.0, 1.0, 0.5])\n",
    "\n",
    "temperatures = [0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "\n",
    "for ax, temp in zip(axes, temperatures):\n",
    "    probs = F.softmax(logits / temp, dim=-1)\n",
    "    ax.bar(range(5), probs.numpy())\n",
    "    ax.set_title(f'Temp = {temp}')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel('Token')\n",
    "    if temp == 0.1:\n",
    "        ax.set_ylabel('Probability')\n",
    "\n",
    "plt.suptitle('Effect of Temperature on Probability Distribution', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Lower temperature â†’ More deterministic (like greedy)\")\n",
    "print(\"Higher temperature â†’ More random (more creative but can be nonsense)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_temperature(model, tokenizer, prompt, max_length=50, \n",
    "                           temperature=1.0, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate text using temperature sampling.\n",
    "    \n",
    "    Args:\n",
    "        temperature: Controls randomness (lower = more deterministic)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    generated = input_ids[0].tolist()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(torch.tensor([generated]).to(device))\n",
    "            logits = outputs.logits[0, -1, :]\n",
    "        \n",
    "        # Apply temperature\n",
    "        scaled_logits = logits / temperature\n",
    "        probs = F.softmax(scaled_logits, dim=-1)\n",
    "        \n",
    "        # Sample from distribution\n",
    "        next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "        \n",
    "        if next_token == tokenizer.eos_token_id:\n",
    "            break\n",
    "            \n",
    "        generated.append(next_token)\n",
    "    \n",
    "    return tokenizer.decode(generated)\n",
    "\n",
    "# Compare different temperatures\n",
    "prompt = \"The scientist discovered a\"\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for temp in [0.3, 0.7, 1.0, 1.5]:\n",
    "    result = sample_with_temperature(\n",
    "        model, tokenizer, prompt, \n",
    "        max_length=30, temperature=temp, device=device\n",
    "    )\n",
    "    print(f\"\\nTemp={temp}:\")\n",
    "    print(f\"  {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Top-k Sampling\n",
    "\n",
    "Only consider the top k most likely tokens, then sample from those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_sampling(model, tokenizer, prompt, max_length=50, \n",
    "                   k=50, temperature=1.0, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate text using top-k sampling.\n",
    "    \n",
    "    Only sample from the k most likely tokens.\n",
    "    \n",
    "    Args:\n",
    "        k: Number of top tokens to consider\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    generated = input_ids[0].tolist()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(torch.tensor([generated]).to(device))\n",
    "            logits = outputs.logits[0, -1, :]\n",
    "        \n",
    "        # Apply temperature\n",
    "        scaled_logits = logits / temperature\n",
    "        \n",
    "        # Get top k\n",
    "        top_k_logits, top_k_indices = scaled_logits.topk(k)\n",
    "        \n",
    "        # Sample from top k\n",
    "        probs = F.softmax(top_k_logits, dim=-1)\n",
    "        selected_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "        next_token = top_k_indices[selected_idx].item()\n",
    "        \n",
    "        if next_token == tokenizer.eos_token_id:\n",
    "            break\n",
    "            \n",
    "        generated.append(next_token)\n",
    "    \n",
    "    return tokenizer.decode(generated)\n",
    "\n",
    "# Compare different k values\n",
    "prompt = \"In the year 2050, robots will\"\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for k in [5, 20, 50, 100]:\n",
    "    result = top_k_sampling(\n",
    "        model, tokenizer, prompt,\n",
    "        max_length=30, k=k, temperature=0.8, device=device\n",
    "    )\n",
    "    print(f\"\\nTop-k={k}:\")\n",
    "    print(f\"  {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Top-p (Nucleus) Sampling\n",
    "\n",
    "### ELI5: Top-p Sampling\n",
    "\n",
    "> **Instead of fixed k tokens, include tokens until you have p% of probability mass.**\n",
    ">\n",
    "> If top 3 tokens have 40%, 30%, 20% = 90% total:\n",
    "> - With p=0.9, only consider these 3 tokens\n",
    "> - With p=0.95, might include 4th token too\n",
    ">\n",
    "> **Advantage:** Adapts to the situation!\n",
    "> - Confident prediction (one token is 95%) â†’ only consider that one\n",
    "> - Uncertain (many similar probabilities) â†’ consider many options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_sampling(model, tokenizer, prompt, max_length=50,\n",
    "                   p=0.9, temperature=1.0, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate text using nucleus (top-p) sampling.\n",
    "    \n",
    "    Include smallest set of tokens whose cumulative probability >= p.\n",
    "    \n",
    "    Args:\n",
    "        p: Cumulative probability threshold\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    generated = input_ids[0].tolist()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(torch.tensor([generated]).to(device))\n",
    "            logits = outputs.logits[0, -1, :]\n",
    "        \n",
    "        # Apply temperature\n",
    "        scaled_logits = logits / temperature\n",
    "        probs = F.softmax(scaled_logits, dim=-1)\n",
    "        \n",
    "        # Sort by probability (descending)\n",
    "        sorted_probs, sorted_indices = probs.sort(descending=True)\n",
    "        \n",
    "        # Compute cumulative probabilities\n",
    "        cumsum = sorted_probs.cumsum(dim=-1)\n",
    "        \n",
    "        # Find cutoff (first position where cumsum >= p)\n",
    "        cutoff_idx = (cumsum >= p).nonzero()[0].item() + 1\n",
    "        \n",
    "        # Keep only tokens in the nucleus\n",
    "        nucleus_probs = sorted_probs[:cutoff_idx]\n",
    "        nucleus_indices = sorted_indices[:cutoff_idx]\n",
    "        \n",
    "        # Re-normalize probabilities\n",
    "        nucleus_probs = nucleus_probs / nucleus_probs.sum()\n",
    "        \n",
    "        # Sample\n",
    "        selected_idx = torch.multinomial(nucleus_probs, num_samples=1).item()\n",
    "        next_token = nucleus_indices[selected_idx].item()\n",
    "        \n",
    "        if next_token == tokenizer.eos_token_id:\n",
    "            break\n",
    "            \n",
    "        generated.append(next_token)\n",
    "    \n",
    "    return tokenizer.decode(generated)\n",
    "\n",
    "# Compare different p values\n",
    "prompt = \"The future of technology is\"\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for p in [0.5, 0.8, 0.9, 0.95]:\n",
    "    result = top_p_sampling(\n",
    "        model, tokenizer, prompt,\n",
    "        max_length=30, p=p, temperature=0.8, device=device\n",
    "    )\n",
    "    print(f\"\\nTop-p={p}:\")\n",
    "    print(f\"  {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Beam Search\n",
    "\n",
    "### ELI5: Beam Search\n",
    "\n",
    "> **Instead of keeping just one sequence, keep track of multiple promising sequences.**\n",
    ">\n",
    "> Imagine you're navigating a maze:\n",
    "> - Greedy: Take the best-looking path at each junction\n",
    "> - Beam search: Keep track of top 5 paths, explore all of them\n",
    ">\n",
    "> This finds better overall solutions, not just locally optimal ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(model, tokenizer, prompt, max_length=50,\n",
    "                beam_width=5, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate text using beam search.\n",
    "    \n",
    "    Maintain beam_width best sequences at each step.\n",
    "    \n",
    "    Args:\n",
    "        beam_width: Number of sequences to track\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Each beam: (sequence, log_prob)\n",
    "    beams = [(input_ids[0].tolist(), 0.0)]\n",
    "    completed = []\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        candidates = []\n",
    "        \n",
    "        for seq, score in beams:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(torch.tensor([seq]).to(device))\n",
    "                logits = outputs.logits[0, -1, :]\n",
    "            \n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            \n",
    "            # Get top beam_width tokens\n",
    "            top_log_probs, top_indices = log_probs.topk(beam_width)\n",
    "            \n",
    "            for log_prob, idx in zip(top_log_probs, top_indices):\n",
    "                new_seq = seq + [idx.item()]\n",
    "                new_score = score + log_prob.item()\n",
    "                \n",
    "                if idx.item() == tokenizer.eos_token_id:\n",
    "                    completed.append((new_seq, new_score))\n",
    "                else:\n",
    "                    candidates.append((new_seq, new_score))\n",
    "        \n",
    "        # Keep top beam_width candidates\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        beams = candidates[:beam_width]\n",
    "        \n",
    "        if not beams:\n",
    "            break\n",
    "    \n",
    "    # Add remaining beams to completed\n",
    "    completed.extend(beams)\n",
    "    \n",
    "    # Return best sequence (normalized by length)\n",
    "    if completed:\n",
    "        # Length normalize scores\n",
    "        completed = [(seq, score / len(seq)) for seq, score in completed]\n",
    "        best_seq, _ = max(completed, key=lambda x: x[1])\n",
    "        return tokenizer.decode(best_seq)\n",
    "    else:\n",
    "        return tokenizer.decode(beams[0][0])\n",
    "\n",
    "# Test beam search\n",
    "prompt = \"The best way to learn programming is\"\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for beam_width in [1, 3, 5, 10]:\n",
    "    result = beam_search(\n",
    "        model, tokenizer, prompt,\n",
    "        max_length=30, beam_width=beam_width, device=device\n",
    "    )\n",
    "    print(f\"\\nBeam width={beam_width}:\")\n",
    "    print(f\"  {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Comparison of Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all strategies on the same prompt\n",
    "\n",
    "prompt = \"Once upon a time in a magical kingdom, there lived a\"\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "strategies = [\n",
    "    (\"Greedy\", lambda: greedy_decode(model, tokenizer, prompt, 40, device)),\n",
    "    (\"Temp=0.7\", lambda: sample_with_temperature(model, tokenizer, prompt, 40, 0.7, device)),\n",
    "    (\"Top-k=40\", lambda: top_k_sampling(model, tokenizer, prompt, 40, 40, 0.8, device)),\n",
    "    (\"Top-p=0.9\", lambda: top_p_sampling(model, tokenizer, prompt, 40, 0.9, 0.8, device)),\n",
    "    (\"Beam=5\", lambda: beam_search(model, tokenizer, prompt, 40, 5, device)),\n",
    "]\n",
    "\n",
    "for name, func in strategies:\n",
    "    result = func()\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure generation speed\n",
    "\n",
    "prompt = \"Artificial intelligence is\"\n",
    "max_length = 50\n",
    "num_runs = 3\n",
    "\n",
    "print(\"Generation Speed Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, func in strategies:\n",
    "    times = []\n",
    "    for _ in range(num_runs):\n",
    "        start = time.time()\n",
    "        _ = func()\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    avg_time = sum(times) / len(times)\n",
    "    tokens_per_sec = max_length / avg_time\n",
    "    print(f\"  {name}: {avg_time*1000:.0f}ms ({tokens_per_sec:.1f} tokens/sec)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Using Hugging Face's Generate Method\n",
    "\n",
    "In production, use the optimized built-in method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face's generate method is much faster and more feature-rich\n",
    "\n",
    "prompt = \"The secret to happiness is\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Greedy\n",
    "outputs = model.generate(\n",
    "    input_ids, \n",
    "    max_new_tokens=30,\n",
    "    do_sample=False  # Greedy\n",
    ")\n",
    "print(f\"\\nGreedy: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\")\n",
    "\n",
    "# Top-p sampling\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=30,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.8\n",
    ")\n",
    "print(f\"\\nTop-p: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\")\n",
    "\n",
    "# Top-k sampling\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=30,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    temperature=0.8\n",
    ")\n",
    "print(f\"\\nTop-k: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\")\n",
    "\n",
    "# Beam search\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=30,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2  # Prevent repetition\n",
    ")\n",
    "print(f\"\\nBeam: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Advanced: Repetition Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repetition is a common problem. Let's fix it!\n",
    "\n",
    "def top_p_with_repetition_penalty(model, tokenizer, prompt, max_length=50,\n",
    "                                  p=0.9, temperature=0.8, \n",
    "                                  repetition_penalty=1.2, device='cpu'):\n",
    "    \"\"\"\n",
    "    Top-p sampling with repetition penalty.\n",
    "    \n",
    "    Args:\n",
    "        repetition_penalty: Penalty for repeated tokens (>1 discourages repetition)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    generated = input_ids[0].tolist()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(torch.tensor([generated]).to(device))\n",
    "            logits = outputs.logits[0, -1, :]\n",
    "        \n",
    "        # Apply repetition penalty\n",
    "        for token_id in set(generated):\n",
    "            if logits[token_id] > 0:\n",
    "                logits[token_id] = logits[token_id] / repetition_penalty\n",
    "            else:\n",
    "                logits[token_id] = logits[token_id] * repetition_penalty\n",
    "        \n",
    "        # Apply temperature and top-p\n",
    "        scaled_logits = logits / temperature\n",
    "        probs = F.softmax(scaled_logits, dim=-1)\n",
    "        \n",
    "        sorted_probs, sorted_indices = probs.sort(descending=True)\n",
    "        cumsum = sorted_probs.cumsum(dim=-1)\n",
    "        cutoff_idx = (cumsum >= p).nonzero()[0].item() + 1\n",
    "        \n",
    "        nucleus_probs = sorted_probs[:cutoff_idx]\n",
    "        nucleus_indices = sorted_indices[:cutoff_idx]\n",
    "        nucleus_probs = nucleus_probs / nucleus_probs.sum()\n",
    "        \n",
    "        selected_idx = torch.multinomial(nucleus_probs, num_samples=1).item()\n",
    "        next_token = nucleus_indices[selected_idx].item()\n",
    "        \n",
    "        if next_token == tokenizer.eos_token_id:\n",
    "            break\n",
    "            \n",
    "        generated.append(next_token)\n",
    "    \n",
    "    return tokenizer.decode(generated)\n",
    "\n",
    "# Compare with and without repetition penalty\n",
    "prompt = \"The weather today is very nice because the weather is\"\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result_no_penalty = top_p_sampling(\n",
    "    model, tokenizer, prompt, 40, 0.9, 0.8, device\n",
    ")\n",
    "print(f\"\\nWithout penalty:\")\n",
    "print(f\"  {result_no_penalty}\")\n",
    "\n",
    "result_with_penalty = top_p_with_repetition_penalty(\n",
    "    model, tokenizer, prompt, 40, 0.9, 0.8, 1.5, device\n",
    ")\n",
    "print(f\"\\nWith penalty (1.5):\")\n",
    "print(f\"  {result_with_penalty}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Exercises\n",
    "\n",
    "### Exercise 1: Implement Contrastive Search\n",
    "\n",
    "Contrastive search balances likelihood with diversity:\n",
    "\n",
    "```\n",
    "score(token) = (1-alpha) * log_prob(token) - alpha * max_cosine_sim(token, previous_tokens)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Implement contrastive search decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Story Generator\n",
    "\n",
    "Create an interactive story generator that:\n",
    "1. Takes a story premise\n",
    "2. Generates a paragraph\n",
    "3. Lets user choose what happens next\n",
    "4. Continues the story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create an interactive story generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Not handling special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong: Not checking for EOS token\n",
    "def wrong_generate(model, prompt, length):\n",
    "    for _ in range(length):\n",
    "        # ... generate token ...\n",
    "        # Never checks if it's EOS!\n",
    "        pass\n",
    "\n",
    "# Right: Stop at EOS\n",
    "def right_generate(model, tokenizer, prompt, length):\n",
    "    for _ in range(length):\n",
    "        # ... generate next_token ...\n",
    "        if next_token == tokenizer.eos_token_id:\n",
    "            break  # Stop generation!\n",
    "\n",
    "print(\"Always check for EOS token to stop generation properly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Temperature = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong: Temperature of 0 causes division by zero!\n",
    "# logits / 0 = infinity\n",
    "\n",
    "# If you want deterministic, use:\n",
    "# 1. Very small temperature (0.01)\n",
    "# 2. Greedy decoding (just take argmax)\n",
    "\n",
    "print(\"Never use temperature=0! Use very small values or greedy instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Not using KV cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong: Recompute all tokens at each step (slow!)\n",
    "def slow_generate(model, input_ids):\n",
    "    for _ in range(100):\n",
    "        outputs = model(input_ids)  # Recomputes ALL tokens!\n",
    "        next_token = outputs.logits[0, -1].argmax()\n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "\n",
    "# Right: Use KV cache (Hugging Face does this automatically)\n",
    "# The generate() method uses past_key_values to avoid recomputation\n",
    "\n",
    "print(\"Hugging Face's generate() uses KV cache for speed.\")\n",
    "print(\"For custom implementations, use past_key_values parameter.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Decoding Strategy Summary\n",
    "\n",
    "| Strategy | Pros | Cons | Best For |\n",
    "|----------|------|------|----------|\n",
    "| Greedy | Fast, deterministic | Repetitive, boring | Quick tests |\n",
    "| Temperature | Controls randomness | Can be too random | Creative writing |\n",
    "| Top-k | Removes unlikely tokens | Fixed k not adaptive | General use |\n",
    "| Top-p | Adaptive, high quality | Slightly slower | Production use |\n",
    "| Beam | Finds optimal sequence | Less diverse, slower | Translation, summarization |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- âœ… How autoregressive generation works\n",
    "- âœ… Greedy decoding and its limitations\n",
    "- âœ… Temperature sampling for controlling randomness\n",
    "- âœ… Top-k sampling to limit vocabulary\n",
    "- âœ… Top-p (nucleus) sampling for adaptive selection\n",
    "- âœ… Beam search for finding optimal sequences\n",
    "- âœ… Repetition penalties to avoid boring text\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "Implement **speculative decoding** for faster generation:\n",
    "1. Use a small \"draft\" model to generate multiple tokens quickly\n",
    "2. Use the large model to verify/correct them in parallel\n",
    "3. Accept correct tokens, regenerate incorrect ones\n",
    "\n",
    "This can provide 2-3x speedup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Implement speculative decoding\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751) - Top-p sampling paper\n",
    "- [CTRL: Conditional Transformer LM](https://arxiv.org/abs/1909.05858) - Controllable generation\n",
    "- [Contrastive Search](https://arxiv.org/abs/2202.06417) - Better decoding\n",
    "- [Speculative Decoding](https://arxiv.org/abs/2211.17192) - Fast generation\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clean up\nimport gc\n\ndel model, tokenizer\n\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\ngc.collect()\n\nprint(\"Memory cleared!\")\nprint(\"\\n\" + \"=\" * 60)\nprint(\"ðŸŽ‰ Congratulations! You've completed Module 2.3!\")\nprint(\"=\" * 60)\nprint(\"\\nYou now understand:\")\nprint(\"  âœ… Attention mechanisms\")\nprint(\"  âœ… Transformer architecture\")\nprint(\"  âœ… Positional encodings\")\nprint(\"  âœ… Tokenization\")\nprint(\"  âœ… BERT fine-tuning\")\nprint(\"  âœ… GPT text generation\")\nprint(\"\\nYou're ready for Module 2.4: Efficient Architectures!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}