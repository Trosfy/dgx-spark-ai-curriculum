{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8.1: Attention from Scratch\n",
    "\n",
    "**Module:** 8 - Natural Language Processing & Transformers  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐⭐\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand what attention means and why it revolutionized NLP\n",
    "- [ ] Implement scaled dot-product attention from scratch\n",
    "- [ ] Build multi-head attention and understand why multiple heads matter\n",
    "- [ ] Visualize attention patterns to see what the model \"looks at\"\n",
    "- [ ] Apply masking for causal (autoregressive) models\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Module 6 (PyTorch Deep Learning)\n",
    "- Knowledge of: Matrix multiplication, softmax, neural network basics\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "**Attention is the core innovation behind:**\n",
    "- **Google Translate** - Understanding which words in \"The cat sat on the mat\" correspond to words in \"Le chat s'est assis sur le tapis\"\n",
    "- **ChatGPT/Claude** - Knowing that \"it\" in \"I dropped the glass and it broke\" refers to the glass\n",
    "- **GitHub Copilot** - Understanding context from your entire codebase, not just the current line\n",
    "\n",
    "Before attention (2014), sequence models had to compress entire sentences into fixed-size vectors, losing information. Attention lets models dynamically focus on relevant parts of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: What is Attention?\n",
    "\n",
    "> **Imagine you're at a crowded party.** Dozens of conversations are happening around you. Somehow, when someone across the room says your name, you instantly tune in to *that* conversation. Your brain didn't process every word equally—it learned to **attend** to what matters.\n",
    ">\n",
    "> **Now imagine reading this sentence:** \"The animal didn't cross the street because it was too tired.\"\n",
    ">\n",
    "> What does \"it\" refer to? Your brain instantly knows it's the animal (not the street). How? You're **paying attention** to the relationship between words.\n",
    ">\n",
    "> **In AI terms:**\n",
    "> - Each word asks: \"Which other words should I pay attention to?\"\n",
    "> - The model calculates \"attention scores\" between all word pairs\n",
    "> - Words with high attention scores influence each other more\n",
    "> - This happens in parallel for all words simultaneously!\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "Before attention, models read sentences like a person with memory problems—by the time they reach the end, they've forgotten the beginning. Attention lets models look back at any part of the input at any time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Set up plotting with seaborn if available\ntry:\n    import seaborn as sns\n    # Use set_theme instead of deprecated set_palette\n    sns.set_theme(style=\"whitegrid\", palette=\"husl\")\n    HAS_SEABORN = True\nexcept ImportError:\n    HAS_SEABORN = False\n    print(\"⚠️ seaborn not installed. Using matplotlib defaults.\")\n\n# Check our hardware\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding Q, K, V (Query, Key, Value)\n",
    "\n",
    "Before we implement attention, we need to understand the three key players: **Query**, **Key**, and **Value**.\n",
    "\n",
    "### ELI5: The Library Analogy\n",
    "\n",
    "> **Imagine a library search system:**\n",
    ">\n",
    "> - **Query (Q)**: Your search question - \"books about space\"\n",
    "> - **Key (K)**: The labels on each book - \"Astronomy 101\", \"Cooking for Beginners\", \"Galactic Adventures\"\n",
    "> - **Value (V)**: The actual content of each book\n",
    ">\n",
    "> **How it works:**\n",
    "> 1. You compare your **Query** to all **Keys** (how relevant is each book?)\n",
    "> 2. Books with high relevance get high scores\n",
    "> 3. You blend all **Values** based on those scores\n",
    "> 4. Result: A weighted summary focused on space-related content!\n",
    "\n",
    "### In Mathematical Terms\n",
    "\n",
    "```\n",
    "Attention(Q, K, V) = softmax(Q @ K^T / sqrt(d_k)) @ V\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `Q @ K^T` computes similarity between queries and keys\n",
    "- `sqrt(d_k)` prevents the dot products from getting too large\n",
    "- `softmax` converts scores to probabilities (sum to 1)\n",
    "- Multiply by `V` to get weighted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build intuition with a simple example\n",
    "\n",
    "# Suppose we have 4 words in a sentence\n",
    "sentence = [\"The\", \"cat\", \"sat\", \"down\"]\n",
    "\n",
    "# Each word is represented as a vector (embedding)\n",
    "# In practice, these come from an embedding layer\n",
    "# For now, let's use random vectors of dimension 8\n",
    "\n",
    "seq_len = 4\n",
    "d_model = 8  # Embedding dimension\n",
    "\n",
    "# Random embeddings for our words\n",
    "embeddings = torch.randn(seq_len, d_model)\n",
    "print(f\"Word embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Each word is a vector of {d_model} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In self-attention, Q, K, and V all come from the same input\n",
    "# but are transformed by different weight matrices\n",
    "\n",
    "# Create learnable projections\n",
    "W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "# Project embeddings to Q, K, V\n",
    "Q = W_q(embeddings)  # What am I looking for?\n",
    "K = W_k(embeddings)  # What do I contain?\n",
    "V = W_v(embeddings)  # What's my actual content?\n",
    "\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"K shape: {K.shape}\")\n",
    "print(f\"V shape: {V.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "We created three different \"views\" of our input:\n",
    "1. **Q (Query)**: \"What information am I looking for?\"\n",
    "2. **K (Key)**: \"What information do I have to offer?\"\n",
    "3. **V (Value)**: \"Here's my actual content\"\n",
    "\n",
    "These are learned during training—the model figures out the best way to project words for attention!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Scaled Dot-Product Attention\n",
    "\n",
    "Now let's implement the core attention mechanism step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Query tensor of shape (..., seq_len, d_k)\n",
    "        K: Key tensor of shape (..., seq_len, d_k)  \n",
    "        V: Value tensor of shape (..., seq_len, d_v)\n",
    "        mask: Optional mask tensor (True = keep, False = mask out)\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention-weighted values\n",
    "        attention_weights: The attention pattern (for visualization)\n",
    "    \"\"\"\n",
    "    # Get the dimension of keys for scaling\n",
    "    d_k = K.size(-1)\n",
    "    \n",
    "    # Step 1: Compute attention scores\n",
    "    # Q @ K^T gives us how much each query attends to each key\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "    print(f\"  Raw attention scores shape: {scores.shape}\")\n",
    "    \n",
    "    # Step 2: Scale by sqrt(d_k)\n",
    "    # Without scaling, large d_k leads to huge dot products,\n",
    "    # pushing softmax into regions with tiny gradients\n",
    "    scores = scores / math.sqrt(d_k)\n",
    "    print(f\"  Scaled scores range: [{scores.min():.2f}, {scores.max():.2f}]\")\n",
    "    \n",
    "    # Step 3: Apply mask (if provided)\n",
    "    if mask is not None:\n",
    "        # Replace masked positions with -inf (becomes 0 after softmax)\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    # Step 4: Softmax to get attention weights (probabilities)\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    print(f\"  Attention weights sum per row: {attention_weights.sum(dim=-1)}\")\n",
    "    \n",
    "    # Step 5: Apply attention to values\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    print(f\"  Output shape: {output.shape}\")\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run attention on our example\n",
    "print(\"Computing attention:\")\n",
    "output, attention = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"\\nInput shape: {embeddings.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"\\n✅ Output has same shape as input - each word now contains\")\n",
    "print(\"   information from all other words, weighted by attention!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Attention Patterns\n",
    "\n",
    "One of the beautiful things about attention is we can visualize what the model is \"looking at\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_attention(attention_weights, x_labels, y_labels, title=\"Attention Pattern\"):\n    \"\"\"\n    Visualize attention weights as a heatmap.\n\n    Args:\n        attention_weights: Tensor of shape (seq_len, seq_len)\n        x_labels: Labels for keys (columns)\n        y_labels: Labels for queries (rows)\n        title: Plot title\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(8, 6))\n\n    # Convert to numpy for plotting\n    weights = attention_weights.detach().numpy()\n\n    # Create heatmap - use seaborn if available, otherwise matplotlib\n    if HAS_SEABORN:\n        sns.heatmap(\n            weights,\n            xticklabels=x_labels,\n            yticklabels=y_labels,\n            annot=True,\n            fmt=\".2f\",\n            cmap=\"Blues\",\n            ax=ax,\n            vmin=0,\n            vmax=1\n        )\n    else:\n        im = ax.imshow(weights, cmap=\"Blues\", vmin=0, vmax=1, aspect='auto')\n        ax.set_xticks(range(len(x_labels)))\n        ax.set_xticklabels(x_labels)\n        ax.set_yticks(range(len(y_labels)))\n        ax.set_yticklabels(y_labels)\n        # Add annotations\n        for i in range(len(y_labels)):\n            for j in range(len(x_labels)):\n                ax.text(j, i, f\"{weights[i, j]:.2f}\", ha='center', va='center', fontsize=8)\n        plt.colorbar(im, ax=ax)\n\n    ax.set_xlabel(\"Keys (attending to)\")\n    ax.set_ylabel(\"Queries (from)\")\n    ax.set_title(title)\n    plt.tight_layout()\n    plt.show()\n\n# Visualize our attention pattern\nplot_attention(\n    attention,\n    sentence,\n    sentence,\n    \"Self-Attention Pattern (Random Weights)\"\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What the Heatmap Shows\n",
    "\n",
    "- **Rows** = Query words (\"I am looking...\")\n",
    "- **Columns** = Key words (\"...at these words\")\n",
    "- **Cell value** = How much the query word attends to the key word\n",
    "- Each row sums to 1.0 (it's a probability distribution)\n",
    "\n",
    "With random weights, the attention is essentially random. After training, meaningful patterns emerge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Why Scale by sqrt(d_k)?\n",
    "\n",
    "This is a common interview question! Let's understand why scaling matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: What happens without scaling?\n",
    "\n",
    "def demonstrate_scaling():\n",
    "    \"\"\"Show why scaling matters for attention stability.\"\"\"\n",
    "    \n",
    "    dimensions = [8, 64, 512, 2048]\n",
    "    \n",
    "    print(\"Effect of dimension on dot product magnitude:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for d_k in dimensions:\n",
    "        # Random vectors with unit variance\n",
    "        q = torch.randn(1, d_k)\n",
    "        k = torch.randn(1, d_k)\n",
    "        \n",
    "        # Dot product without scaling\n",
    "        raw_score = (q @ k.T).item()\n",
    "        \n",
    "        # Dot product with scaling\n",
    "        scaled_score = raw_score / math.sqrt(d_k)\n",
    "        \n",
    "        print(f\"d_k={d_k:4d}: raw_score={raw_score:8.2f}, \"\n",
    "              f\"scaled_score={scaled_score:6.2f}, \"\n",
    "              f\"sqrt(d_k)={math.sqrt(d_k):.1f}\")\n",
    "\n",
    "demonstrate_scaling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does this mean for softmax?\n",
    "\n",
    "def demonstrate_softmax_saturation():\n",
    "    \"\"\"Show how large values cause softmax to saturate.\"\"\"\n",
    "    \n",
    "    # Simulated attention scores\n",
    "    small_scores = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "    large_scores = torch.tensor([10.0, 20.0, 30.0, 40.0])\n",
    "    \n",
    "    small_probs = F.softmax(small_scores, dim=-1)\n",
    "    large_probs = F.softmax(large_scores, dim=-1)\n",
    "    \n",
    "    print(\"Softmax behavior with different score magnitudes:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Small scores: {small_scores.tolist()}\")\n",
    "    print(f\"Softmax:      {small_probs.tolist()}\")\n",
    "    print(f\"Gradient OK: Probabilities are distributed\")\n",
    "    print()\n",
    "    print(f\"Large scores: {large_scores.tolist()}\")\n",
    "    print(f\"Softmax:      {large_probs.tolist()}\")\n",
    "    print(f\"Problem: One element takes ~100%, gradients vanish!\")\n",
    "\n",
    "demonstrate_softmax_saturation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Intuition\n",
    "\n",
    "Without scaling:\n",
    "- As dimension increases, dot products get larger (variance = d_k)\n",
    "- Large dot products push softmax to extremes (0 or 1)\n",
    "- Extreme softmax = tiny gradients = slow/no learning\n",
    "\n",
    "With scaling by √d_k:\n",
    "- Dot products have variance ≈ 1 regardless of dimension\n",
    "- Softmax stays in its \"interesting\" region\n",
    "- Gradients flow nicely, training works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Multi-Head Attention\n",
    "\n",
    "### ELI5: Why Multiple Heads?\n",
    "\n",
    "> **Imagine you're analyzing a movie scene.** You might notice:\n",
    "> - The **dialogue** (what characters say)\n",
    "> - The **cinematography** (camera angles, lighting)\n",
    "> - The **music** (emotional cues)\n",
    "> - The **acting** (facial expressions, body language)\n",
    ">\n",
    "> **Each \"head\" looks at the same scene differently!**\n",
    ">\n",
    "> In attention:\n",
    "> - One head might focus on syntax (grammar relationships)\n",
    "> - Another on semantics (meaning relationships)\n",
    "> - Another on coreference (what \"it\" refers to)\n",
    "> - Another on position (nearby words)\n",
    ">\n",
    "> Multi-head attention = multiple perspectives combined!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention mechanism.\n",
    "    \n",
    "    Instead of one attention function, we run h parallel attention \"heads\"\n",
    "    and concatenate their outputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension (embedding size)\n",
    "            num_heads: Number of attention heads\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension per head\n",
    "        \n",
    "        # Linear projections for Q, K, V (all heads combined)\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Final output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Compute multi-head attention.\n",
    "        \n",
    "        Args:\n",
    "            query: Query tensor (batch, seq_len, d_model)\n",
    "            key: Key tensor (batch, seq_len, d_model)\n",
    "            value: Value tensor (batch, seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "            \n",
    "        Returns:\n",
    "            output: Attention output (batch, seq_len, d_model)\n",
    "            attention_weights: Attention patterns per head\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        seq_len = query.size(1)\n",
    "        \n",
    "        # Step 1: Linear projections\n",
    "        Q = self.W_q(query)  # (batch, seq_len, d_model)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "        \n",
    "        # Step 2: Reshape for multi-head: (batch, heads, seq_len, d_k)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Step 3: Scaled dot-product attention (for all heads at once!)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        context = torch.matmul(attention_weights, V)  # (batch, heads, seq_len, d_k)\n",
    "        \n",
    "        # Step 4: Concatenate heads: (batch, seq_len, d_model)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        # Step 5: Final projection\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Create multi-head attention\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "print(f\"Multi-Head Attention Configuration:\")\n",
    "print(f\"  d_model (total): {d_model}\")\n",
    "print(f\"  num_heads: {num_heads}\")\n",
    "print(f\"  d_k (per head): {d_model // num_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a batch of sentences\n",
    "batch_size = 2\n",
    "seq_len = 6\n",
    "\n",
    "# Random embeddings (in practice, these come from an embedding layer)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Self-attention: query, key, value are all the same input\n",
    "output, attention = mha(x, x, x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention.shape}\")\n",
    "print(f\"  (batch={batch_size}, heads={num_heads}, seq={seq_len}, seq={seq_len})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize attention from different heads\nwords = [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \".\"]\n\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\n\nfor head_idx, ax in enumerate(axes.flat):\n    # Get attention for first example, specific head\n    head_attention = attention[0, head_idx].detach().numpy()\n\n    if HAS_SEABORN:\n        sns.heatmap(\n            head_attention,\n            xticklabels=words,\n            yticklabels=words,\n            cmap=\"Blues\",\n            ax=ax,\n            cbar=False,\n            vmin=0,\n            vmax=1\n        )\n    else:\n        im = ax.imshow(head_attention, cmap=\"Blues\", vmin=0, vmax=1, aspect='auto')\n        ax.set_xticks(range(len(words)))\n        ax.set_xticklabels(words)\n        ax.set_yticks(range(len(words)))\n        ax.set_yticklabels(words)\n    ax.set_title(f\"Head {head_idx + 1}\")\n    ax.tick_params(axis='x', rotation=45)\n\nplt.suptitle(\"Different Attention Heads See Different Patterns\\n(Random weights - patterns emerge after training)\", \n             fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "Each head has its own Q, K, V projections and learns to attend differently:\n",
    "- One head might learn to look at the previous word\n",
    "- Another might learn to look at verbs from subjects\n",
    "- Another might learn syntactic dependencies\n",
    "\n",
    "After training on real data, these patterns become meaningful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Causal Masking (for GPT-style models)\n",
    "\n",
    "### ELI5: Why Mask?\n",
    "\n",
    "> **Imagine you're taking a fill-in-the-blank test:**\n",
    "> \"The sun rises in the ____\"\n",
    ">\n",
    "> **If you could see the answer key, that would be cheating!**\n",
    ">\n",
    "> For language models that predict the next word, we need to prevent them from \"peeking ahead.\" This is called **causal** or **autoregressive** masking.\n",
    ">\n",
    "> When predicting word 5, the model can only see words 1-4, not words 6+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_causal_mask(seq_len):\n    \"\"\"\n    Create a causal (look-ahead) mask.\n    \n    Position i can only attend to positions <= i.\n    \n    Returns:\n        mask: Boolean tensor where True = keep, False = mask\n    \"\"\"\n    # Lower triangular matrix\n    mask = torch.tril(torch.ones(seq_len, seq_len)).bool()\n    return mask\n\n# Create and visualize causal mask\nseq_len = 6\ncausal_mask = create_causal_mask(seq_len)\n\nprint(\"Causal Mask (True = can attend, False = masked):\")\nprint(causal_mask.int())\n\n# Visualize\nplt.figure(figsize=(6, 5))\nif HAS_SEABORN:\n    sns.heatmap(\n        causal_mask.int().numpy(),\n        annot=True,\n        fmt=\"d\",\n        cmap=\"RdYlGn\",\n        xticklabels=[f\"pos {i}\" for i in range(seq_len)],\n        yticklabels=[f\"pos {i}\" for i in range(seq_len)],\n        cbar=False\n    )\nelse:\n    plt.imshow(causal_mask.int().numpy(), cmap=\"RdYlGn\", aspect='auto')\n    plt.xticks(range(seq_len), [f\"pos {i}\" for i in range(seq_len)])\n    plt.yticks(range(seq_len), [f\"pos {i}\" for i in range(seq_len)])\n    # Add annotations\n    for i in range(seq_len):\n        for j in range(seq_len):\n            plt.text(j, i, str(causal_mask[i, j].int().item()), ha='center', va='center')\nplt.title(\"Causal Mask\\n(Green=1=can see, Red=0=cannot see)\")\nplt.xlabel(\"Key positions\")\nplt.ylabel(\"Query positions\")\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply causal masking to attention\ndef causal_attention_demo():\n    \"\"\"Demonstrate causal vs. non-causal attention.\"\"\"\n    \n    # Input sequence\n    words = [\"I\", \"love\", \"machine\", \"learning\", \"!\", \"<END>\"]\n    seq_len = len(words)\n    d_model = 32\n    \n    # Random embeddings\n    x = torch.randn(1, seq_len, d_model)\n    \n    # Create Q, K, V\n    Q = x\n    K = x\n    V = x\n    \n    # Compute attention scores\n    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_model)\n    \n    # Create causal mask\n    mask = create_causal_mask(seq_len)\n    \n    # Apply mask\n    masked_scores = scores.masked_fill(~mask.unsqueeze(0), float('-inf'))\n    \n    # Softmax\n    attention_no_mask = F.softmax(scores, dim=-1)\n    attention_with_mask = F.softmax(masked_scores, dim=-1)\n    \n    # Visualize comparison\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    if HAS_SEABORN:\n        sns.heatmap(\n            attention_no_mask[0].detach().numpy(),\n            xticklabels=words,\n            yticklabels=words,\n            annot=True,\n            fmt=\".2f\",\n            cmap=\"Blues\",\n            ax=axes[0]\n        )\n        sns.heatmap(\n            attention_with_mask[0].detach().numpy(),\n            xticklabels=words,\n            yticklabels=words,\n            annot=True,\n            fmt=\".2f\",\n            cmap=\"Blues\",\n            ax=axes[1]\n        )\n    else:\n        # Matplotlib fallback\n        for idx, (attn, ax) in enumerate([(attention_no_mask[0], axes[0]), (attention_with_mask[0], axes[1])]):\n            attn_np = attn.detach().numpy()\n            im = ax.imshow(attn_np, cmap=\"Blues\", vmin=0, vmax=1, aspect='auto')\n            ax.set_xticks(range(len(words)))\n            ax.set_xticklabels(words, rotation=45)\n            ax.set_yticks(range(len(words)))\n            ax.set_yticklabels(words)\n            for i in range(len(words)):\n                for j in range(len(words)):\n                    ax.text(j, i, f\"{attn_np[i, j]:.2f}\", ha='center', va='center', fontsize=7)\n    \n    axes[0].set_title(\"Bidirectional Attention (BERT-style)\\nEach word sees ALL words\")\n    axes[1].set_title(\"Causal Attention (GPT-style)\\nEach word only sees past words\")\n    \n    plt.tight_layout()\n    plt.show()\n\ncausal_attention_demo()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Difference:\n",
    "\n",
    "- **Bidirectional (BERT)**: Word at position 3 can attend to all 6 words\n",
    "- **Causal (GPT)**: Word at position 3 can only attend to positions 0, 1, 2, 3\n",
    "\n",
    "This prevents \"information leakage\" during training for language models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Attention Types Overview\n",
    "\n",
    "There are several types of attention used in different contexts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a comprehensive diagram of attention types\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Self-Attention\nself_attn = torch.tril(torch.ones(5, 5)) + torch.triu(torch.ones(5, 5))\nself_attn = (self_attn > 0).float()\n\n# Causal Self-Attention\ncausal = torch.tril(torch.ones(5, 5))\n\n# Cross-Attention\ncross = torch.ones(5, 7)\n\nif HAS_SEABORN:\n    sns.heatmap(self_attn.numpy(), ax=axes[0], cmap=\"Blues\", cbar=False, \n                annot=False, square=True)\n    sns.heatmap(causal.numpy(), ax=axes[1], cmap=\"Blues\", cbar=False, \n                annot=False, square=True)\n    sns.heatmap(cross.numpy(), ax=axes[2], cmap=\"Blues\", cbar=False, \n                annot=False)\nelse:\n    axes[0].imshow(self_attn.numpy(), cmap=\"Blues\", aspect='auto')\n    axes[1].imshow(causal.numpy(), cmap=\"Blues\", aspect='auto')\n    axes[2].imshow(cross.numpy(), cmap=\"Blues\", aspect='auto')\n\naxes[0].set_title(\"Self-Attention (Encoder)\\nAll positions see all positions\", fontsize=11)\naxes[0].set_xlabel(\"Keys (same sequence)\")\naxes[0].set_ylabel(\"Queries (same sequence)\")\n\naxes[1].set_title(\"Causal Self-Attention (Decoder)\\nEach position sees only past\", fontsize=11)\naxes[1].set_xlabel(\"Keys (same sequence)\")\naxes[1].set_ylabel(\"Queries (same sequence)\")\n\naxes[2].set_title(\"Cross-Attention (Encoder-Decoder)\\nDecoder queries attend to encoder keys\", fontsize=11)\naxes[2].set_xlabel(\"Keys (encoder sequence)\")\naxes[2].set_ylabel(\"Queries (decoder sequence)\")\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Attention Types Summary:\")\nprint(\"=\" * 60)\nprint(\"1. Self-Attention: Q, K, V all from same sequence\")\nprint(\"   - Used in: BERT encoder, GPT (with mask), ViT\")\nprint()\nprint(\"2. Causal Self-Attention: Self-attention + future mask\")\nprint(\"   - Used in: GPT, LLaMA, text generation\")\nprint()\nprint(\"3. Cross-Attention: Q from decoder, K/V from encoder\")\nprint(\"   - Used in: T5, translation, encoder-decoder models\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Exercises\n",
    "\n",
    "### Exercise 1: Implement Attention with Dropout\n",
    "\n",
    "Add dropout to the attention weights (commonly used to prevent overfitting).\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Apply dropout AFTER softmax but BEFORE multiplying with V.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention_with_dropout(Q, K, V, dropout_p=0.1, mask=None, training=True):\n",
    "    \"\"\"\n",
    "    Scaled dot-product attention with dropout.\n",
    "    \n",
    "    TODO: Implement attention with dropout on the attention weights.\n",
    "    \n",
    "    Args:\n",
    "        Q, K, V: Query, Key, Value tensors\n",
    "        dropout_p: Dropout probability\n",
    "        mask: Optional attention mask\n",
    "        training: Whether in training mode (dropout active)\n",
    "    \"\"\"\n",
    "    d_k = K.size(-1)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # 1. Compute attention scores\n",
    "    # 2. Scale by sqrt(d_k)\n",
    "    # 3. Apply mask if provided\n",
    "    # 4. Apply softmax\n",
    "    # 5. Apply dropout to attention weights\n",
    "    # 6. Multiply with values\n",
    "    \n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "# Test your implementation\n",
    "# Q = torch.randn(1, 4, 8)\n",
    "# K = torch.randn(1, 4, 8)\n",
    "# V = torch.randn(1, 4, 8)\n",
    "# output, attention = scaled_dot_product_attention_with_dropout(Q, K, V, dropout_p=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Attention Complexity Analysis\n",
    "\n",
    "What is the time and space complexity of self-attention? Why is this a problem for long sequences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_attention_complexity():\n",
    "    \"\"\"\n",
    "    Measure memory usage for different sequence lengths.\n",
    "    \n",
    "    TODO: \n",
    "    1. Create attention matrices for sequences of length 128, 512, 1024, 2048\n",
    "    2. Measure memory usage\n",
    "    3. Plot the relationship\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Your analysis here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Visualize Trained Attention\n",
    "\n",
    "Let's load a pre-trained model and visualize what its attention heads learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll explore this more in later notebooks, but here's a preview:\n",
    "\n",
    "# TODO: Load a pre-trained BERT model and extract attention weights\n",
    "# from transformers import BertModel, BertTokenizer\n",
    "# \n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
    "# \n",
    "# text = \"The cat sat on the mat because it was tired.\"\n",
    "# inputs = tokenizer(text, return_tensors='pt')\n",
    "# outputs = model(**inputs)\n",
    "# \n",
    "# # Extract attention from the last layer\n",
    "# attention = outputs.attentions[-1]  # (batch, heads, seq, seq)\n",
    "\n",
    "print(\"Exercise: Uncomment and run the code above to visualize BERT attention!\")\n",
    "print(\"You'll see how different heads learn to attend to different relationships.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting to Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong: No scaling\n",
    "def attention_wrong(Q, K, V):\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))  # Missing / sqrt(d_k)!\n",
    "    return F.softmax(scores, dim=-1) @ V\n",
    "\n",
    "# Right: With scaling\n",
    "def attention_right(Q, K, V):\n",
    "    d_k = K.size(-1)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)  # Scaled!\n",
    "    return F.softmax(scores, dim=-1) @ V\n",
    "\n",
    "print(\"Why it matters: Without scaling, attention becomes 'peaky' (almost one-hot)\")\n",
    "print(\"for large d_k, leading to vanishing gradients during training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Wrong Mask Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong: Applying mask AFTER softmax\n",
    "def attention_mask_wrong(Q, K, V, mask):\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(K.size(-1))\n",
    "    attention = F.softmax(scores, dim=-1)\n",
    "    attention = attention * mask  # Wrong! Softmax already computed\n",
    "    return attention @ V\n",
    "\n",
    "# Right: Applying mask BEFORE softmax\n",
    "def attention_mask_right(Q, K, V, mask):\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(K.size(-1))\n",
    "    scores = scores.masked_fill(mask == 0, float('-inf'))  # Before softmax!\n",
    "    attention = F.softmax(scores, dim=-1)  # -inf becomes 0\n",
    "    return attention @ V\n",
    "\n",
    "print(\"Why it matters: Masking after softmax doesn't make the row sum to 1.\")\n",
    "print(\"Masking before softmax with -inf ensures proper probability distribution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Shape Mismatch in Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong: Forgetting to transpose back\n",
    "def multihead_wrong(x, num_heads):\n",
    "    batch, seq_len, d_model = x.shape\n",
    "    d_k = d_model // num_heads\n",
    "    \n",
    "    # Reshape to heads\n",
    "    x = x.view(batch, seq_len, num_heads, d_k)  # (batch, seq, heads, d_k)\n",
    "    x = x.transpose(1, 2)  # (batch, heads, seq, d_k)\n",
    "    \n",
    "    # ... attention computation ...\n",
    "    \n",
    "    # Wrong: Direct view without transpose\n",
    "    # output = x.view(batch, seq_len, d_model)  # Shape mismatch!\n",
    "    pass\n",
    "\n",
    "# Right: Proper transpose then contiguous then view\n",
    "def multihead_right(x, num_heads):\n",
    "    batch, seq_len, d_model = x.shape\n",
    "    d_k = d_model // num_heads\n",
    "    \n",
    "    # Reshape to heads\n",
    "    x = x.view(batch, seq_len, num_heads, d_k)\n",
    "    x = x.transpose(1, 2)  # (batch, heads, seq, d_k)\n",
    "    \n",
    "    # ... attention computation ...\n",
    "    \n",
    "    # Right: transpose, contiguous, then view\n",
    "    x = x.transpose(1, 2)  # (batch, seq, heads, d_k)\n",
    "    x = x.contiguous()  # Make memory contiguous\n",
    "    output = x.view(batch, seq_len, d_model)  # Works!\n",
    "    return output\n",
    "\n",
    "print(\"Why it matters: View requires contiguous memory.\")\n",
    "print(\"After transpose, call .contiguous() before .view()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ✅ The intuition behind attention (library search, party analogy)\n",
    "- ✅ Query, Key, Value projections and their roles\n",
    "- ✅ Scaled dot-product attention implementation\n",
    "- ✅ Why scaling by √d_k prevents gradient problems\n",
    "- ✅ Multi-head attention and why multiple perspectives help\n",
    "- ✅ Causal masking for autoregressive models\n",
    "- ✅ Different attention types (self, causal, cross)\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "Implement **Relative Position Attention** where attention scores are modified based on the relative distance between positions:\n",
    "\n",
    "```\n",
    "score(i, j) = q_i · k_j + q_i · r_(i-j)\n",
    "```\n",
    "\n",
    "Where `r_(i-j)` is a learned embedding for relative position.\n",
    "\n",
    "This is used in models like Transformer-XL and Music Transformer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Implement relative position attention\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - Best visual guide\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original paper\n",
    "- [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/) - Code walkthrough\n",
    "- [BertViz](https://github.com/jessevig/bertviz) - Interactive attention visualization\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "\n",
    "# Delete large tensors\n",
    "del mha, x, output, attention\n",
    "\n",
    "# Clear cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"Memory cleared! Ready for the next notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Up\n",
    "\n",
    "In **Notebook 02: Transformer Block**, we'll combine attention with:\n",
    "- Feed-forward networks\n",
    "- Layer normalization\n",
    "- Residual connections\n",
    "\n",
    "...to build a complete Transformer encoder layer!\n",
    "\n",
    "---\n",
    "\n",
    "*Great job completing your first deep dive into attention! This is the foundation of modern NLP.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}