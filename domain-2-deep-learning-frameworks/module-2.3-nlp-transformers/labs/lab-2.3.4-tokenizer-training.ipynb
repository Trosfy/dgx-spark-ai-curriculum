{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 2.3.4: Tokenization Lab\n\n**Module:** 2.3 - Natural Language Processing & Transformers  \n**Time:** 3 hours  \n**Difficulty:** ⭐⭐\n\n---\n\n## Learning Objectives\n\nBy the end of this notebook, you will:\n- [ ] Understand why tokenization is crucial for NLP\n- [ ] Implement Byte Pair Encoding (BPE) from scratch\n- [ ] Train your own tokenizer on custom text\n- [ ] Compare tokenizers from GPT-2, BERT, and LLaMA\n- [ ] Understand the trade-offs of different vocabulary sizes\n\n---\n\n## Prerequisites\n\n- Completed: Labs 2.3.1-2.3.3\n- Basic Python string manipulation\n- Understanding of dictionaries and collections\n\n---\n\n## Real-World Context\n\n**Tokenization affects everything in NLP:**\n- **Context length**: More efficient tokenization = more text in context window\n- **Cost**: APIs charge per token (ChatGPT: ~$0.002 per 1K tokens)\n- **Multilingual**: Some tokenizers handle languages better than others\n- **Code**: Programming syntax needs special handling\n\nA word like \"unbelievable\" might be:\n- 1 token (word-level)\n- 3 tokens: \"un\" + \"believ\" + \"able\" (subword)\n- 12 tokens (character-level)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: What is Tokenization?\n",
    "\n",
    "> **Imagine you're creating a recipe for a robot chef.**\n",
    ">\n",
    "> You can't just say \"make a sandwich\" - the robot needs specific ingredients:\n",
    "> - Option 1: Individual letters (\"m\", \"a\", \"k\", \"e\", \" \", \"a\", \" \", \"s\", ...)\n",
    ">   - Very flexible, but 100s of steps for simple tasks!\n",
    "> - Option 2: Full recipes (\"make a sandwich\", \"bake a cake\")\n",
    ">   - Fast, but can't combine or create new dishes\n",
    "> - Option 3: **Ingredients** (\"bread\", \"cheese\", \"spread\", \"layer\")\n",
    ">   - Best of both worlds! Can make many dishes efficiently\n",
    ">\n",
    "> **Subword tokenization** is like using ingredients instead of letters or full recipes.\n",
    "> - Common words stay whole: \"the\", \"is\", \"and\"\n",
    "> - Rare words break into pieces: \"un\" + \"believ\" + \"able\"\n",
    "> - New words can be built from known pieces!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import re\nfrom collections import Counter, defaultdict\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport platform\n\n# We'll also use the transformers library for pre-trained tokenizers\nHAS_TRANSFORMERS = False\ntry:\n    from transformers import AutoTokenizer\n    HAS_TRANSFORMERS = True\n    print(\"✅ transformers already installed\")\nexcept ImportError:\n    # Check if we're on ARM64 (DGX Spark)\n    if platform.machine() == 'aarch64':\n        print(\"⚠️ ARM64 detected (DGX Spark). transformers should be pre-installed in NGC container.\")\n        print(\"If missing, restart with: nvcr.io/nvidia/pytorch:25.11-py3 which includes transformers\")\n        print(\"Continuing without transformers library...\")\n    else:\n        print(\"Installing transformers...\")\n        !pip install transformers -q\n        try:\n            from transformers import AutoTokenizer\n            HAS_TRANSFORMERS = True\n            print(\"✅ transformers installed successfully\")\n        except ImportError:\n            print(\"❌ Could not install transformers\")\n\nprint(f\"\\nSetup complete! transformers available: {HAS_TRANSFORMERS}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Tokenization Approaches\n",
    "\n",
    "Let's explore the three main approaches before diving deep into BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"The quick brown fox jumps over the lazy dog. AI is transforming the world!\"\n",
    "\n",
    "# 1. Character-level tokenization\n",
    "char_tokens = list(text)\n",
    "print(f\"Character-level: {len(char_tokens)} tokens\")\n",
    "print(f\"  Tokens: {char_tokens[:20]}...\")\n",
    "print()\n",
    "\n",
    "# 2. Word-level tokenization\n",
    "word_tokens = text.split()\n",
    "print(f\"Word-level: {len(word_tokens)} tokens\")\n",
    "print(f\"  Tokens: {word_tokens}\")\n",
    "print()\n",
    "\n",
    "# 3. Subword tokenization (using GPT-2 tokenizer)\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "subword_tokens = gpt2_tokenizer.tokenize(text)\n",
    "print(f\"Subword (GPT-2): {len(subword_tokens)} tokens\")\n",
    "print(f\"  Tokens: {subword_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The problem with word-level: OOV (Out of Vocabulary)\n",
    "\n",
    "rare_words = \"Transformers use self-attention mechanisms for contextualization.\"\n",
    "\n",
    "# Simple word vocabulary from common words\n",
    "simple_vocab = {\"the\", \"a\", \"is\", \"for\", \"use\", \"and\", \"to\", \"of\"}\n",
    "\n",
    "words = rare_words.lower().split()\n",
    "print(\"Word-level problem:\")\n",
    "for word in words:\n",
    "    # Remove punctuation for checking\n",
    "    clean_word = word.strip(\".,!?\")\n",
    "    if clean_word in simple_vocab:\n",
    "        print(f\"  '{word}' -> ✓ In vocabulary\")\n",
    "    else:\n",
    "        print(f\"  '{word}' -> ✗ OOV (unknown word!)\")\n",
    "\n",
    "print(\"\\nSubword solution:\")\n",
    "tokens = gpt2_tokenizer.tokenize(rare_words)\n",
    "print(f\"  {tokens}\")\n",
    "print(\"  Every piece is known! No OOV problem.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Byte Pair Encoding (BPE) from Scratch\n",
    "\n",
    "### ELI5: How BPE Works\n",
    "\n",
    "> **Start with characters, and repeatedly merge the most common pairs.**\n",
    ">\n",
    "> Imagine you're creating shortcuts for common letter combinations:\n",
    "> 1. Start: \"l o w\" \"l o w e r\" \"n e w e s t\" \"w i d e s t\"\n",
    "> 2. Find most common pair: (\"e\", \"s\") appears 2 times\n",
    "> 3. Create new token: \"es\" and replace all occurrences\n",
    "> 4. Now: \"l o w\" \"l o w e r\" \"n e w es t\" \"w i d es t\"\n",
    "> 5. Repeat until you have enough tokens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBPE:\n",
    "    \"\"\"\n",
    "    Simple Byte Pair Encoding implementation.\n",
    "    \n",
    "    This is a teaching implementation - not optimized for production!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.merges = {}  # (a, b) -> new_token\n",
    "        self.vocab = {}   # token -> id\n",
    "        \n",
    "    def _get_word_freqs(self, text):\n",
    "        \"\"\"Split text into words and count frequencies.\"\"\"\n",
    "        # Simple word splitting\n",
    "        words = re.findall(r\"\\w+|[^\\w\\s]\", text.lower())\n",
    "        word_freqs = Counter(words)\n",
    "        return word_freqs\n",
    "    \n",
    "    def _word_to_chars(self, word):\n",
    "        \"\"\"Convert word to list of characters with end-of-word marker.\"\"\"\n",
    "        return list(word) + [\"</w>\"]\n",
    "    \n",
    "    def _get_pair_freqs(self, word_freqs, word_tokens):\n",
    "        \"\"\"Count frequency of adjacent pairs.\"\"\"\n",
    "        pair_freqs = Counter()\n",
    "        \n",
    "        for word, freq in word_freqs.items():\n",
    "            tokens = word_tokens[word]\n",
    "            for i in range(len(tokens) - 1):\n",
    "                pair = (tokens[i], tokens[i + 1])\n",
    "                pair_freqs[pair] += freq\n",
    "                \n",
    "        return pair_freqs\n",
    "    \n",
    "    def _merge_pair(self, word_tokens, pair):\n",
    "        \"\"\"Merge a pair in all words.\"\"\"\n",
    "        new_word_tokens = {}\n",
    "        \n",
    "        for word, tokens in word_tokens.items():\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == pair:\n",
    "                    # Merge this pair\n",
    "                    new_tokens.append(tokens[i] + tokens[i + 1])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "            new_word_tokens[word] = new_tokens\n",
    "            \n",
    "        return new_word_tokens\n",
    "    \n",
    "    def train(self, text, num_merges=100, verbose=True):\n",
    "        \"\"\"\n",
    "        Train BPE on text.\n",
    "        \n",
    "        Args:\n",
    "            text: Training text\n",
    "            num_merges: Number of merge operations\n",
    "            verbose: Print progress\n",
    "        \"\"\"\n",
    "        # Get word frequencies\n",
    "        word_freqs = self._get_word_freqs(text)\n",
    "        \n",
    "        # Initialize with characters\n",
    "        word_tokens = {word: self._word_to_chars(word) for word in word_freqs}\n",
    "        \n",
    "        # Get initial vocabulary\n",
    "        vocab = set()\n",
    "        for tokens in word_tokens.values():\n",
    "            vocab.update(tokens)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Initial vocabulary size: {len(vocab)}\")\n",
    "            print(f\"Unique words: {len(word_freqs)}\")\n",
    "            print()\n",
    "        \n",
    "        # Perform merges\n",
    "        for i in range(num_merges):\n",
    "            # Find most frequent pair\n",
    "            pair_freqs = self._get_pair_freqs(word_freqs, word_tokens)\n",
    "            \n",
    "            if not pair_freqs:\n",
    "                break\n",
    "                \n",
    "            best_pair = pair_freqs.most_common(1)[0][0]\n",
    "            best_freq = pair_freqs[best_pair]\n",
    "            \n",
    "            # Merge the pair\n",
    "            word_tokens = self._merge_pair(word_tokens, best_pair)\n",
    "            \n",
    "            # Add to merges\n",
    "            new_token = best_pair[0] + best_pair[1]\n",
    "            self.merges[best_pair] = new_token\n",
    "            vocab.add(new_token)\n",
    "            \n",
    "            if verbose and (i + 1) % 10 == 0:\n",
    "                print(f\"Merge {i+1}: {best_pair} -> '{new_token}' (freq: {best_freq})\")\n",
    "        \n",
    "        # Build final vocabulary\n",
    "        self.vocab = {token: i for i, token in enumerate(sorted(vocab))}\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nFinal vocabulary size: {len(self.vocab)}\")\n",
    "            print(f\"Number of merges: {len(self.merges)}\")\n",
    "    \n",
    "    def tokenize(self, word):\n",
    "        \"\"\"Tokenize a single word using learned merges.\"\"\"\n",
    "        tokens = self._word_to_chars(word.lower())\n",
    "        \n",
    "        # Apply merges in order\n",
    "        changed = True\n",
    "        while changed:\n",
    "            changed = False\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if i < len(tokens) - 1:\n",
    "                    pair = (tokens[i], tokens[i + 1])\n",
    "                    if pair in self.merges:\n",
    "                        new_tokens.append(self.merges[pair])\n",
    "                        i += 2\n",
    "                        changed = True\n",
    "                        continue\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "            tokens = new_tokens\n",
    "            \n",
    "        return tokens\n",
    "\n",
    "# Training text (simple example)\n",
    "training_text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog.\n",
    "The dog was very lazy and the fox was quick.\n",
    "Jumping foxes are quicker than sleeping dogs.\n",
    "The lazy dog dreamed of quick brown foxes.\n",
    "\"\"\" * 10  # Repeat for more data\n",
    "\n",
    "# Train BPE\n",
    "bpe = SimpleBPE()\n",
    "bpe.train(training_text, num_merges=30, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our BPE tokenizer\n",
    "\n",
    "test_words = [\"the\", \"quick\", \"jumping\", \"foxes\", \"laziest\", \"unquickly\"]\n",
    "\n",
    "print(\"Testing our BPE tokenizer:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for word in test_words:\n",
    "    tokens = bpe.tokenize(word)\n",
    "    print(f\"  '{word}' -> {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "1. We started with individual characters as our vocabulary\n",
    "2. We found the most frequent pair of adjacent tokens\n",
    "3. We merged them into a new token\n",
    "4. We repeated until we reached our desired vocabulary size\n",
    "\n",
    "Now common subwords like \"the\", \"ing\", \"er\" are single tokens!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Comparing Real Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load different tokenizers\ntokenizers = {}\n\nif HAS_TRANSFORMERS:\n    try:\n        tokenizers[\"GPT-2\"] = AutoTokenizer.from_pretrained(\"gpt2\")\n        print(\"✅ Loaded GPT-2 tokenizer\")\n    except Exception as e:\n        print(f\"⚠️ Could not load GPT-2 tokenizer: {e}\")\n\n    try:\n        tokenizers[\"BERT\"] = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n        print(\"✅ Loaded BERT tokenizer\")\n    except Exception as e:\n        print(f\"⚠️ Could not load BERT tokenizer: {e}\")\n\n    # Try to load LLaMA tokenizer with graceful fallback\n    # Note: LLaMA tokenizers require authentication\n    try:\n        tokenizers[\"LLaMA\"] = AutoTokenizer.from_pretrained(\n            \"meta-llama/Llama-2-7b-hf\",\n            legacy=False,\n            token=True  # Use HF_TOKEN environment variable if set\n        )\n        print(\"✅ Loaded LLaMA tokenizer\")\n    except Exception as e:\n        print(f\"⚠️ Could not load LLaMA tokenizer: {e}\")\n        print(\"   To use LLaMA tokenizer:\")\n        print(\"   1. Create account at huggingface.co\")\n        print(\"   2. Accept LLaMA license at: https://huggingface.co/meta-llama/Llama-2-7b-hf\")\n        print(\"   3. Run: huggingface-cli login\")\n        print(\"   Continuing without LLaMA tokenizer...\")\n\nif tokenizers:\n    print(f\"\\nLoaded {len(tokenizers)} tokenizers:\")\n    for name, tok in tokenizers.items():\n        print(f\"  {name}: vocab size = {tok.vocab_size:,}\")\nelse:\n    print(\"\\n⚠️ No tokenizers loaded. Install transformers to use this section.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tokenization of the same text\n",
    "\n",
    "test_texts = [\n",
    "    \"Hello, world!\",\n",
    "    \"The transformer architecture revolutionized NLP.\",\n",
    "    \"def fibonacci(n): return n if n < 2 else fibonacci(n-1) + fibonacci(n-2)\",\n",
    "    \"Tokenization is crucial for language models.\",\n",
    "    \"supercalifragilisticexpialidocious\",  # Long rare word\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    print(f\"\\nText: '{text}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for name, tokenizer in tokenizers.items():\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "        print(f\"  {name}:\")\n",
    "        print(f\"    Tokens ({len(tokens)}): {tokens}\")\n",
    "        print(f\"    IDs: {ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiency comparison\n",
    "\n",
    "long_text = \"\"\"\n",
    "Artificial intelligence has made remarkable progress in recent years, particularly \n",
    "in the field of natural language processing. Large language models like GPT-4, \n",
    "Claude, and LLaMA have demonstrated impressive capabilities in understanding and \n",
    "generating human-like text. These models use the transformer architecture, which \n",
    "relies on self-attention mechanisms to process sequences of tokens.\n",
    "\n",
    "The tokenization process is crucial for these models. It converts raw text into \n",
    "a sequence of tokens that the model can process. Different tokenization strategies \n",
    "have different trade-offs in terms of vocabulary size, token efficiency, and \n",
    "handling of rare or unknown words.\n",
    "\"\"\" * 3\n",
    "\n",
    "print(f\"Text length: {len(long_text)} characters\")\n",
    "print(\"\\nToken counts:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "token_counts = {}\n",
    "for name, tokenizer in tokenizers.items():\n",
    "    tokens = tokenizer.encode(long_text, add_special_tokens=False)\n",
    "    token_counts[name] = len(tokens)\n",
    "    chars_per_token = len(long_text) / len(tokens)\n",
    "    print(f\"  {name}: {len(tokens)} tokens ({chars_per_token:.1f} chars/token)\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(token_counts.keys(), token_counts.values(), color=['blue', 'green', 'red'][:len(token_counts)])\n",
    "plt.ylabel('Number of Tokens')\n",
    "plt.title('Token Efficiency Comparison\\n(Fewer tokens = more efficient)')\n",
    "plt.xticks(rotation=45)\n",
    "for i, (name, count) in enumerate(token_counts.items()):\n",
    "    plt.text(i, count + 5, str(count), ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Special Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore special tokens\n",
    "\n",
    "for name, tokenizer in tokenizers.items():\n",
    "    print(f\"\\n{name} Special Tokens:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    special_tokens = {\n",
    "        'BOS (Beginning of Sequence)': tokenizer.bos_token,\n",
    "        'EOS (End of Sequence)': tokenizer.eos_token,\n",
    "        'PAD (Padding)': tokenizer.pad_token,\n",
    "        'UNK (Unknown)': tokenizer.unk_token,\n",
    "        'SEP (Separator)': getattr(tokenizer, 'sep_token', None),\n",
    "        'CLS (Classification)': getattr(tokenizer, 'cls_token', None),\n",
    "        'MASK': getattr(tokenizer, 'mask_token', None),\n",
    "    }\n",
    "    \n",
    "    for token_name, token in special_tokens.items():\n",
    "        if token:\n",
    "            token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "            print(f\"  {token_name}: '{token}' (id: {token_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How BERT uses special tokens\n",
    "\n",
    "bert_tokenizer = tokenizers.get(\"BERT\")\n",
    "\n",
    "if bert_tokenizer:\n",
    "    text = \"Hello, how are you?\"\n",
    "    \n",
    "    # Without special tokens\n",
    "    tokens_no_special = bert_tokenizer.tokenize(text)\n",
    "    ids_no_special = bert_tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    # With special tokens\n",
    "    ids_with_special = bert_tokenizer.encode(text, add_special_tokens=True)\n",
    "    tokens_with_special = bert_tokenizer.convert_ids_to_tokens(ids_with_special)\n",
    "    \n",
    "    print(\"BERT tokenization:\")\n",
    "    print(f\"  Text: '{text}'\")\n",
    "    print(f\"  Without special tokens: {tokens_no_special}\")\n",
    "    print(f\"  With special tokens:    {tokens_with_special}\")\n",
    "    print(f\"\\n  [CLS] = start of sequence\")\n",
    "    print(f\"  [SEP] = end of sequence / separator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Training a Custom Tokenizer\n",
    "\n",
    "Let's train a proper tokenizer using the `tokenizers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Try to import tokenizers library\nHAS_TOKENIZERS = False\ntry:\n    from tokenizers import Tokenizer\n    from tokenizers.models import BPE\n    from tokenizers.trainers import BpeTrainer\n    from tokenizers.pre_tokenizers import Whitespace\n    HAS_TOKENIZERS = True\n    print(\"✅ tokenizers library available\")\nexcept ImportError:\n    # Check if we're on ARM64 (DGX Spark)\n    import platform\n    if platform.machine() == 'aarch64':\n        print(\"⚠️ ARM64 detected (DGX Spark).\")\n        print(\"   For NGC container pytorch:25.11-py3, tokenizers should be included.\")\n        print(\"   If missing, ensure you're using the correct container version.\")\n        print(\"   Skipping custom tokenizer training sections...\")\n    else:\n        print(\"Installing tokenizers...\")\n        import subprocess\n        result = subprocess.run(['pip', 'install', 'tokenizers', '-q'], capture_output=True)\n        try:\n            from tokenizers import Tokenizer\n            from tokenizers.models import BPE\n            from tokenizers.trainers import BpeTrainer\n            from tokenizers.pre_tokenizers import Whitespace\n            HAS_TOKENIZERS = True\n            print(\"✅ tokenizers installed successfully\")\n        except ImportError:\n            print(\"❌ Could not install tokenizers\")\n\nif HAS_TOKENIZERS:\n    # Create a larger training corpus\n    training_corpus = [\n        \"Machine learning is a subset of artificial intelligence.\",\n        \"Deep learning uses neural networks with many layers.\",\n        \"Transformers revolutionized natural language processing.\",\n        \"Attention mechanisms allow models to focus on relevant parts.\",\n        \"Pre-training on large datasets improves model performance.\",\n        \"Fine-tuning adapts models to specific tasks.\",\n        \"Tokenization converts text to numerical representations.\",\n        \"Embeddings capture semantic meaning of words.\",\n        \"Self-attention computes relationships between all tokens.\",\n        \"Language models predict the next word in a sequence.\",\n    ] * 100  # Repeat for more data\n\n    # Initialize tokenizer\n    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n    tokenizer.pre_tokenizer = Whitespace()\n\n    # Create trainer\n    trainer = BpeTrainer(\n        vocab_size=500,\n        special_tokens=[\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n    )\n\n    # Train on our corpus\n    tokenizer.train_from_iterator(training_corpus, trainer=trainer)\n\n    print(f\"Trained tokenizer with vocabulary size: {tokenizer.get_vocab_size()}\")\nelse:\n    print(\"⚠️ Skipping custom tokenizer training (tokenizers library not available)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test our custom tokenizer\n\nif HAS_TOKENIZERS:\n    test_texts = [\n        \"Machine learning is fascinating.\",\n        \"Transformers use self-attention mechanisms.\",\n        \"This is a completely new sentence about quantum computing.\",\n    ]\n\n    print(\"Testing custom tokenizer:\")\n    print(\"=\" * 60)\n\n    for text in test_texts:\n        output = tokenizer.encode(text)\n        print(f\"\\nText: '{text}'\")\n        print(f\"  Tokens: {output.tokens}\")\n        print(f\"  IDs:    {output.ids}\")\nelse:\n    print(\"⚠️ Custom tokenizer testing skipped (tokenizers library not available)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Vocabulary Size Trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Experiment with different vocabulary sizes\n\nif HAS_TOKENIZERS:\n    vocab_sizes = [100, 500, 1000, 5000]\n    test_text = \"\"\"\n    Natural language processing has evolved significantly with the introduction of \n    transformer-based architectures. These models leverage self-attention mechanisms \n    to capture long-range dependencies in text, enabling impressive performance on \n    various downstream tasks.\n    \"\"\"\n\n    results = []\n\n    for vocab_size in vocab_sizes:\n        # Train new tokenizer\n        tok = Tokenizer(BPE(unk_token=\"[UNK]\"))\n        tok.pre_tokenizer = Whitespace()\n        trainer = BpeTrainer(vocab_size=vocab_size, special_tokens=[\"[UNK]\"])\n        tok.train_from_iterator(training_corpus, trainer=trainer)\n        \n        # Tokenize test text\n        encoded = tok.encode(test_text)\n        num_tokens = len(encoded.tokens)\n        num_unk = encoded.tokens.count(\"[UNK]\")\n        \n        results.append({\n            'vocab_size': vocab_size,\n            'num_tokens': num_tokens,\n            'num_unk': num_unk,\n            'chars_per_token': len(test_text) / num_tokens\n        })\n        \n        print(f\"Vocab size {vocab_size}: {num_tokens} tokens, {num_unk} [UNK], \"\n              f\"{len(test_text)/num_tokens:.1f} chars/token\")\n\n    # Visualize\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n    ax = axes[0]\n    ax.plot([r['vocab_size'] for r in results], [r['num_tokens'] for r in results], 'bo-')\n    ax.set_xlabel('Vocabulary Size')\n    ax.set_ylabel('Number of Tokens')\n    ax.set_title('Token Count vs Vocabulary Size\\n(Larger vocab = fewer tokens)')\n    ax.grid(True, alpha=0.3)\n\n    ax = axes[1]\n    ax.plot([r['vocab_size'] for r in results], [r['chars_per_token'] for r in results], 'go-')\n    ax.set_xlabel('Vocabulary Size')\n    ax.set_ylabel('Characters per Token')\n    ax.set_title('Token Efficiency vs Vocabulary Size\\n(Higher = more efficient)')\n    ax.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"⚠️ Vocabulary size experiment skipped (tokenizers library not available)\")\n    print(\"This section requires the tokenizers library to train BPE tokenizers.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trade-offs Summary:\n",
    "\n",
    "| Vocab Size | Tokens per Text | Embedding Memory | OOV Risk |\n",
    "|------------|-----------------|------------------|----------|\n",
    "| Small (1K) | Many | Low | High |\n",
    "| Medium (30K) | Balanced | Moderate | Low |\n",
    "| Large (100K+) | Fewer | High | Very Low |\n",
    "\n",
    "Most modern LLMs use 30K-100K vocabulary:\n",
    "- GPT-2: 50,257\n",
    "- BERT: 30,522\n",
    "- LLaMA: 32,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Exercises\n",
    "\n",
    "### Exercise 1: Multilingual Tokenization\n",
    "\n",
    "Compare how different tokenizers handle non-English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multilingual tokenization\n",
    "multilingual_texts = [\n",
    "    \"Hello, world!\",                    # English\n",
    "    \"Bonjour le monde!\",                # French\n",
    "    \"Hola, mundo!\",                     # Spanish\n",
    "    \"Hallo, Welt!\",                     # German\n",
    "    \"Привет мир!\",                      # Russian\n",
    "    \"你好世界！\",                         # Chinese\n",
    "    \"こんにちは世界！\",                    # Japanese\n",
    "]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Compare token counts for each language across different tokenizers\n",
    "# Which tokenizer is most efficient for which languages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Code Tokenization\n",
    "\n",
    "Analyze how different tokenizers handle programming code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_samples = [\n",
    "    'print(\"Hello, World!\")',\n",
    "    'def factorial(n): return 1 if n <= 1 else n * factorial(n-1)',\n",
    "    'class MyClass:\\n    def __init__(self, value):\\n        self.value = value',\n",
    "]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# How do different tokenizers handle code?\n",
    "# Look for patterns in how they split identifiers, keywords, and operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Not handling special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: Special characters can cause issues\n",
    "problematic_text = \"This costs $100 (25% off!) @store #sale\"\n",
    "\n",
    "for name, tok in tokenizers.items():\n",
    "    tokens = tok.tokenize(problematic_text)\n",
    "    print(f\"{name}: {tokens}\")\n",
    "\n",
    "print(\"\\nNote: Different tokenizers handle symbols differently!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Ignoring token limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each model has a maximum token limit!\n",
    "\n",
    "model_limits = {\n",
    "    \"GPT-2\": 1024,\n",
    "    \"BERT\": 512,\n",
    "    \"GPT-4\": 8192,  # or 32K/128K for extended versions\n",
    "    \"Claude\": 100000,\n",
    "}\n",
    "\n",
    "long_text = \"word \" * 1000  # 1000 words\n",
    "\n",
    "gpt2_tok = tokenizers.get(\"GPT-2\")\n",
    "if gpt2_tok:\n",
    "    tokens = gpt2_tok.encode(long_text)\n",
    "    print(f\"Text with 1000 words produces {len(tokens)} tokens\")\n",
    "    \n",
    "    for model, limit in model_limits.items():\n",
    "        status = \"✓ OK\" if len(tokens) <= limit else \"✗ TRUNCATED\"\n",
    "        print(f\"  {model} (limit {limit}): {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Forgetting to add special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT requires [CLS] and [SEP] tokens!\n",
    "\n",
    "bert_tok = tokenizers.get(\"BERT\")\n",
    "if bert_tok:\n",
    "    text = \"This is a test.\"\n",
    "    \n",
    "    # Wrong: no special tokens\n",
    "    wrong_ids = bert_tok.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    # Right: with special tokens\n",
    "    right_ids = bert_tok.encode(text, add_special_tokens=True)\n",
    "    \n",
    "    print(f\"Without special tokens: {bert_tok.convert_ids_to_tokens(wrong_ids)}\")\n",
    "    print(f\"With special tokens:    {bert_tok.convert_ids_to_tokens(right_ids)}\")\n",
    "    print(\"\\n⚠️ BERT expects [CLS] at start and [SEP] at end!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ✅ Why tokenization is necessary for NLP\n",
    "- ✅ Character, word, and subword tokenization\n",
    "- ✅ How Byte Pair Encoding (BPE) works\n",
    "- ✅ Differences between GPT-2, BERT, and LLaMA tokenizers\n",
    "- ✅ Special tokens and their purposes\n",
    "- ✅ Vocabulary size trade-offs\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "Implement **WordPiece** tokenization (used by BERT), which differs from BPE:\n",
    "- BPE merges based on frequency\n",
    "- WordPiece merges based on likelihood improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Implement WordPiece tokenization\n",
    "# Hint: Instead of counting pair frequencies, compute:\n",
    "# score(a, b) = freq(ab) / (freq(a) * freq(b))\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Hugging Face Tokenizers](https://huggingface.co/docs/tokenizers/) - Fast tokenizers library\n",
    "- [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909) - BPE paper\n",
    "- [SentencePiece](https://arxiv.org/abs/1808.06226) - Unsupervised text tokenizer\n",
    "- [Let's Build a Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE) - Andrej Karpathy's video\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clean up\nimport gc\n\n# Safely delete variables that may or may not exist\nfor var_name in ['tokenizers', 'tokenizer', 'bpe']:\n    if var_name in globals():\n        del globals()[var_name]\n\ngc.collect()\n\nprint(\"Memory cleared! Ready for the next notebook.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Up\n",
    "\n",
    "In **Notebook 05: BERT Fine-tuning**, we'll:\n",
    "- Load a pre-trained BERT model\n",
    "- Fine-tune it for sentiment classification\n",
    "- Evaluate performance and understand transfer learning\n",
    "\n",
    "---\n",
    "\n",
    "*Great job! Tokenization is the essential first step for any NLP model.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}