{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.3.5: BERT Fine-tuning for Sentiment Classification\n",
    "\n",
    "**Module:** 2.3 - Natural Language Processing & Transformers  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** â­â­â­\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand what BERT is and how it was pre-trained\n",
    "- [ ] Load and inspect a pre-trained BERT model\n",
    "- [ ] Prepare data for fine-tuning\n",
    "- [ ] Fine-tune BERT for sentiment classification\n",
    "- [ ] Evaluate model performance and interpret results\n",
    "- [ ] Leverage DGX Spark's 128GB memory for larger batch sizes\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Notebooks 01-04 (Attention, Transformer, Position, Tokenization)\n",
    "- Knowledge of: PyTorch training loops, classification metrics\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "**BERT fine-tuning is used everywhere:**\n",
    "- **Spam detection** - Is this email spam?\n",
    "- **Sentiment analysis** - Is this review positive or negative?\n",
    "- **Intent classification** - What does this customer want?\n",
    "- **Named Entity Recognition** - Find names, dates, locations\n",
    "- **Question answering** - Find answers in documents\n",
    "\n",
    "Transfer learning with BERT typically needs:\n",
    "- Only a few thousand labeled examples\n",
    "- Minutes to hours of training (vs weeks from scratch)\n",
    "- State-of-the-art results on most NLP tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: What is BERT?\n",
    "\n",
    "> **Imagine a student who has read every book in the library.**\n",
    ">\n",
    "> They haven't memorized specific answers, but they deeply understand:\n",
    "> - How words relate to each other\n",
    "> - Grammar and sentence structure\n",
    "> - Context and meaning\n",
    ">\n",
    "> **Now you need to teach them to classify movie reviews.**\n",
    ">\n",
    "> Because they already understand language, you only need:\n",
    "> - A few hundred examples of positive/negative reviews\n",
    "> - A few hours of practice\n",
    ">\n",
    "> **That's BERT fine-tuning!**\n",
    "> - Pre-training: Read lots of text, learn language (done by Google)\n",
    "> - Fine-tuning: Learn your specific task (you do this)\n",
    "\n",
    "### BERT's Pre-training Tasks\n",
    "\n",
    "1. **Masked Language Modeling (MLM)**: Predict missing words\n",
    "   - Input: \"The [MASK] sat on the mat\"\n",
    "   - Predict: \"cat\"\n",
    "\n",
    "2. **Next Sentence Prediction (NSP)**: Do sentences follow each other?\n",
    "   - \"The cat sat down.\" + \"It was tired.\" â†’ Yes (follows)\n",
    "   - \"The cat sat down.\" + \"Stocks rose 5%.\" â†’ No (random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import AdamW\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport gc\n\n# Try to import tqdm for progress bars\ntry:\n    from tqdm.auto import tqdm\n    HAS_TQDM = True\nexcept ImportError:\n    HAS_TQDM = False\n    print(\"âš ï¸ tqdm not installed. Progress bars will be disabled.\")\n    print(\"Install with: pip install tqdm\")\n    \n    # Create a fallback class that supports set_postfix\n    class TqdmFallback:\n        \"\"\"Fallback for tqdm that supports set_postfix.\"\"\"\n        def __init__(self, iterable, **kwargs):\n            self.iterable = iterable\n            self.desc = kwargs.get('desc', '')\n        def __iter__(self):\n            return iter(self.iterable)\n        def set_postfix(self, *args, **kwargs):\n            pass  # No-op for fallback\n    \n    def tqdm(iterable, **kwargs):\n        return TqdmFallback(iterable, **kwargs)\n\n# Hugging Face libraries\ntry:\n    from transformers import (\n        BertTokenizer, \n        BertModel, \n        BertForSequenceClassification,\n        get_linear_schedule_with_warmup\n    )\n    from datasets import load_dataset\n    HAS_HF = True\nexcept ImportError:\n    HAS_HF = False\n    print(\"âŒ transformers and/or datasets not installed.\")\n    print(\"Install with: pip install transformers datasets\")\n\n# Check device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"GPU Memory: {gpu_mem:.1f} GB\")\n    \n    # Clear cache before loading large models\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    # DGX Spark optimization: use larger batches based on GPU memory\n    if gpu_mem > 100 or \"GB10\" in torch.cuda.get_device_name(0):\n        print(\"\\nâœ¨ DGX Spark detected! Using optimized settings.\")\n        BATCH_SIZE = 64  # Much larger than typical 16-32\n    elif gpu_mem > 40:\n        BATCH_SIZE = 32\n    elif gpu_mem > 20:\n        BATCH_SIZE = 24\n    else:\n        BATCH_SIZE = 16\nelse:\n    BATCH_SIZE = 8\n\nprint(f\"\\nBatch size: {BATCH_SIZE}\")\n\n# Set seed for reproducibility\ntorch.manual_seed(42)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding BERT Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert = BertModel.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"Max sequence length: {tokenizer.model_max_length}\")\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in bert.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore BERT's architecture\n",
    "print(\"BERT Architecture:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Hidden size: {bert.config.hidden_size}\")\n",
    "print(f\"Number of layers: {bert.config.num_hidden_layers}\")\n",
    "print(f\"Attention heads: {bert.config.num_attention_heads}\")\n",
    "print(f\"FFN dimension: {bert.config.intermediate_size}\")\n",
    "print(f\"\\nThis matches what we built in notebooks 01-02!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how BERT processes text\n",
    "text = \"I love this movie! It's absolutely fantastic.\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "print(\"Tokenized input:\")\n",
    "print(f\"  Input IDs shape: {tokens['input_ids'].shape}\")\n",
    "print(f\"  Tokens: {tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])}\")\n",
    "print(f\"  Attention mask: {tokens['attention_mask'][0].tolist()}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = bert(**tokens)\n",
    "\n",
    "print(f\"\\nBERT outputs:\")\n",
    "print(f\"  Last hidden state: {outputs.last_hidden_state.shape}\")\n",
    "print(f\"  Pooler output:     {outputs.pooler_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Output Explained:\n",
    "\n",
    "- **last_hidden_state**: (batch, seq_len, hidden_size) - contextual embeddings for each token\n",
    "- **pooler_output**: (batch, hidden_size) - the [CLS] token embedding, passed through a dense layer\n",
    "\n",
    "For classification, we typically use the **pooler_output** as the sentence representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDB dataset (movie reviews with sentiment labels)\n",
    "print(\"Loading IMDB dataset...\")\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(dataset)\n",
    "\n",
    "# Look at a sample\n",
    "print(f\"\\nSample review (first 300 chars):\")\n",
    "print(f\"  Text: {dataset['train'][0]['text'][:300]}...\")\n",
    "print(f\"  Label: {dataset['train'][0]['label']} (0=negative, 1=positive)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For faster training, use a subset\n",
    "# With DGX Spark, we can use more data!\n",
    "if torch.cuda.is_available() and torch.cuda.get_device_properties(0).total_memory > 100e9:\n",
    "    NUM_TRAIN = 5000  # Use more with DGX Spark\n",
    "    NUM_TEST = 1000\n",
    "else:\n",
    "    NUM_TRAIN = 2000\n",
    "    NUM_TEST = 500\n",
    "\n",
    "train_dataset = dataset['train'].shuffle(seed=42).select(range(NUM_TRAIN))\n",
    "test_dataset = dataset['test'].shuffle(seed=42).select(range(NUM_TEST))\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch Dataset\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for IMDB reviews.\n",
    "    \n",
    "    Handles tokenization and padding.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_length=256):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]['text']\n",
    "        label = self.data[idx]['label']\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_ds = IMDBDataset(train_dataset, tokenizer, max_length=256)\n",
    "test_ds = IMDBDataset(test_dataset, tokenizer, max_length=256)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Build Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT + Classification head for sentiment analysis.\n",
    "    \n",
    "    Architecture:\n",
    "        BERT -> [CLS] embedding -> Dropout -> Linear -> Labels\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bert_model_name=\"bert-base-uncased\", num_classes=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pre-trained BERT\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs (batch, seq_len)\n",
    "            attention_mask: Attention mask (batch, seq_len)\n",
    "            \n",
    "        Returns:\n",
    "            logits: Class logits (batch, num_classes)\n",
    "        \"\"\"\n",
    "        # Get BERT outputs\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use pooler output ([CLS] token representation)\n",
    "        pooled = outputs.pooler_output\n",
    "        \n",
    "        # Classification\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create model\n",
    "model = BertClassifier(num_classes=2)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model parameters:\")\n",
    "bert_params = sum(p.numel() for p in model.bert.parameters())\n",
    "classifier_params = sum(p.numel() for p in model.classifier.parameters())\n",
    "print(f\"  BERT: {bert_params:,}\")\n",
    "print(f\"  Classifier: {classifier_params:,}\")\n",
    "print(f\"  Total: {bert_params + classifier_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Using Hugging Face's Built-in\n",
    "\n",
    "Hugging Face provides `BertForSequenceClassification` which does the same thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, use the built-in class:\n",
    "# model = BertForSequenceClassification.from_pretrained(\n",
    "#     \"bert-base-uncased\",\n",
    "#     num_labels=2\n",
    "# )\n",
    "\n",
    "# We built our own to understand what's happening!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 5: Training Setup\n\n### Learning Rate Schedulers\n\nWhen fine-tuning Transformers, we use learning rate schedulers with **warmup**:\n1. Start with a small LR and gradually increase (warmup phase)\n2. Then decrease LR over the rest of training\n\n**Key scheduler methods:**\n- `scheduler.step()`: Update the learning rate (call after each optimizer step)\n- `scheduler.get_last_lr()`: Returns a list of current learning rates (useful for logging/visualization)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5  # Small LR for fine-tuning (don't disturb BERT too much)\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "# Optimizer (AdamW is standard for Transformers)\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler with warmup\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "print(f\"  Warmup steps: {warmup_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learning rate schedule\n",
    "\n",
    "lrs = []\n",
    "temp_optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "temp_scheduler = get_linear_schedule_with_warmup(\n",
    "    temp_optimizer, warmup_steps, total_steps\n",
    ")\n",
    "\n",
    "for _ in range(total_steps):\n",
    "    lrs.append(temp_scheduler.get_last_lr()[0])\n",
    "    temp_scheduler.step()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(lrs)\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedule with Warmup')\n",
    "plt.axvline(x=warmup_steps, color='r', linestyle='--', label=f'Warmup ends ({warmup_steps} steps)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Why warmup? Starting with a smaller LR prevents early instability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, criterion, device):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \n",
    "    Returns:\n",
    "        Average loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping (prevents exploding gradients)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        predictions = logits.argmax(dim=-1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'acc': f\"{100*correct/total:.1f}%\"\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model.\n",
    "    \n",
    "    Returns:\n",
    "        Average loss and accuracy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(dataloader), correct / total, all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training!\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, optimizer, scheduler, criterion, device\n",
    "    )\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc, _, _ = evaluate(\n",
    "        model, test_loader, criterion, device\n",
    "    )\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {100*train_acc:.1f}%\")\n",
    "    print(f\"  Test Loss:  {test_loss:.4f}, Test Acc:  {100*test_acc:.1f}%\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nâœ… Training complete in {total_time:.1f} seconds!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss\n",
    "ax = axes[0]\n",
    "ax.plot(range(1, NUM_EPOCHS + 1), train_losses, 'b-o', label='Train')\n",
    "ax.plot(range(1, NUM_EPOCHS + 1), test_losses, 'r-o', label='Test')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training and Test Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax = axes[1]\n",
    "ax.plot(range(1, NUM_EPOCHS + 1), [a*100 for a in train_accs], 'b-o', label='Train')\n",
    "ax.plot(range(1, NUM_EPOCHS + 1), [a*100 for a in test_accs], 'r-o', label='Test')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('Training and Test Accuracy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Test Accuracy: {100*test_accs[-1]:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Detailed Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Try to import sklearn for detailed metrics\nHAS_SKLEARN = False\ntry:\n    from sklearn.metrics import classification_report, confusion_matrix\n    HAS_SKLEARN = True\nexcept ImportError:\n    print(\"âš ï¸ sklearn not installed. Install with: pip install scikit-learn\")\n    print(\"Continuing with basic accuracy metrics...\")\n\n# For visualization\nHAS_SEABORN = False\ntry:\n    import seaborn as sns\n    HAS_SEABORN = True\nexcept ImportError:\n    print(\"âš ï¸ seaborn not installed for confusion matrix visualization\")\n\n# Get final predictions\n_, _, predictions, labels = evaluate(model, test_loader, criterion, device)\n\nif HAS_SKLEARN:\n    # Classification report\n    print(\"Classification Report:\")\n    print(\"=\" * 50)\n    print(classification_report(\n        labels, predictions, \n        target_names=['Negative', 'Positive']\n    ))\n\n    # Confusion matrix\n    cm = confusion_matrix(labels, predictions)\n\n    plt.figure(figsize=(6, 5))\n    if HAS_SEABORN:\n        sns.heatmap(\n            cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=['Negative', 'Positive'],\n            yticklabels=['Negative', 'Positive']\n        )\n    else:\n        plt.imshow(cm, cmap='Blues')\n        plt.colorbar()\n        # Add annotations\n        for i in range(2):\n            for j in range(2):\n                plt.text(j, i, str(cm[i, j]), ha='center', va='center', fontsize=14)\n        plt.xticks([0, 1], ['Negative', 'Positive'])\n        plt.yticks([0, 1], ['Negative', 'Positive'])\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.tight_layout()\n    plt.show()\nelse:\n    # Manual accuracy calculation\n    accuracy = sum(p == l for p, l in zip(predictions, labels)) / len(labels)\n    print(f\"\\nTest Accuracy: {accuracy*100:.1f}%\")\n    \n    # Manual confusion matrix\n    tp = sum(p == 1 and l == 1 for p, l in zip(predictions, labels))\n    tn = sum(p == 0 and l == 0 for p, l in zip(predictions, labels))\n    fp = sum(p == 1 and l == 0 for p, l in zip(predictions, labels))\n    fn = sum(p == 0 and l == 1 for p, l in zip(predictions, labels))\n    \n    print(f\"\\nConfusion Matrix:\")\n    print(f\"  True Negatives:  {tn}\")\n    print(f\"  False Positives: {fp}\")\n    print(f\"  False Negatives: {fn}\")\n    print(f\"  True Positives:  {tp}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Test on Custom Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Predict sentiment for a single text.\n",
    "    \n",
    "    Returns:\n",
    "        Sentiment label and confidence\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        prediction = logits.argmax(dim=-1).item()\n",
    "        confidence = probs[0, prediction].item()\n",
    "    \n",
    "    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "    return sentiment, confidence\n",
    "\n",
    "\n",
    "# Test on custom examples\n",
    "test_reviews = [\n",
    "    \"This movie was absolutely fantastic! I loved every minute of it.\",\n",
    "    \"Terrible film. Waste of time and money. Don't bother watching.\",\n",
    "    \"It was okay, nothing special but not bad either.\",\n",
    "    \"The acting was superb and the plot kept me on the edge of my seat!\",\n",
    "    \"Boring and predictable. I fell asleep halfway through.\",\n",
    "    \"A masterpiece of modern cinema. Truly unforgettable.\",\n",
    "]\n",
    "\n",
    "print(\"Custom Predictions:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for review in test_reviews:\n",
    "    sentiment, confidence = predict_sentiment(review, model, tokenizer, device)\n",
    "    emoji = \"ðŸ˜ƒ\" if sentiment == \"Positive\" else \"ðŸ˜ž\"\n",
    "    print(f\"\\n{emoji} {sentiment} ({confidence*100:.1f}% confident)\")\n",
    "    print(f\"   \\\"{review[:60]}...\\\"\" if len(review) > 60 else f\"   \\\"{review}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Freeze BERT Layers (Optional Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes, we only want to train the classification head\n",
    "# This is faster and works well with limited data\n",
    "\n",
    "def freeze_bert_layers(model, freeze=True):\n",
    "    \"\"\"\n",
    "    Freeze or unfreeze BERT layers.\n",
    "    \n",
    "    When frozen, only the classifier head is trained.\n",
    "    \"\"\"\n",
    "    for param in model.bert.parameters():\n",
    "        param.requires_grad = not freeze\n",
    "    \n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)\")\n",
    "\n",
    "# Example: freeze BERT, only train classifier\n",
    "print(\"Before freezing:\")\n",
    "freeze_bert_layers(model, freeze=False)\n",
    "\n",
    "print(\"\\nAfter freezing:\")\n",
    "freeze_bert_layers(model, freeze=True)\n",
    "\n",
    "# Unfreeze for continued training\n",
    "freeze_bert_layers(model, freeze=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Exercises\n",
    "\n",
    "### Exercise 1: Train on Full Dataset\n",
    "\n",
    "With DGX Spark's memory, try training on the full IMDB dataset (25K samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Use the full training set: dataset['train']\n",
    "# 2. Adjust batch size and epochs\n",
    "# 3. Compare results with the subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Multi-class Classification\n",
    "\n",
    "Adapt the code for a multi-class task like emotion detection (happy, sad, angry, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Load a multi-class dataset: load_dataset(\"emotion\")\n",
    "# 2. Modify BertClassifier for num_classes > 2\n",
    "# 3. Train and evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Too high learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong: Learning rate too high for fine-tuning\n",
    "# optimizer = AdamW(model.parameters(), lr=1e-3)  # 0.001 is WAY too high!\n",
    "\n",
    "# Right: Use small learning rate (2e-5 to 5e-5)\n",
    "# optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "print(\"Fine-tuning learning rates:\")\n",
    "print(\"  Too high (1e-3): Will destroy pre-trained knowledge!\")\n",
    "print(\"  Just right (2e-5 to 5e-5): Gently adapts BERT\")\n",
    "print(\"  Too low (1e-6): Training will be too slow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Forgetting to set model to eval mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong: Evaluating in training mode\n",
    "def wrong_evaluate(model, text):\n",
    "    # model is still in train mode!\n",
    "    # Dropout is active, results will be noisy\n",
    "    pass\n",
    "\n",
    "# Right: Set to eval mode\n",
    "def right_evaluate(model, text):\n",
    "    model.eval()  # Disables dropout!\n",
    "    with torch.no_grad():  # Saves memory\n",
    "        # ... prediction code\n",
    "        pass\n",
    "\n",
    "print(\"model.eval() disables dropout and batch norm updates.\")\n",
    "print(\"model.train() re-enables them for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Not using gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers can have unstable gradients\n",
    "# Always clip gradients during fine-tuning!\n",
    "\n",
    "# In training loop:\n",
    "# loss.backward()\n",
    "# torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Important!\n",
    "# optimizer.step()\n",
    "\n",
    "print(\"Gradient clipping prevents:\")\n",
    "print(\"  - Exploding gradients\")\n",
    "print(\"  - Training instability\")\n",
    "print(\"  - NaN losses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- âœ… What BERT is and how it was pre-trained\n",
    "- âœ… How to load and use pre-trained BERT\n",
    "- âœ… Building a classification head on top of BERT\n",
    "- âœ… Fine-tuning best practices (LR, warmup, gradient clipping)\n",
    "- âœ… Evaluating with classification metrics\n",
    "- âœ… Using predictions on new data\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "Implement **Layer-wise Learning Rate Decay (LLRD)**:\n",
    "- Lower learning rates for earlier BERT layers\n",
    "- Higher learning rates for later layers and classifier\n",
    "- This often improves fine-tuning results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Implement LLRD\n",
    "# Hint: Create parameter groups with different LRs\n",
    "\n",
    "# Example structure:\n",
    "# param_groups = [\n",
    "#     {'params': bert.encoder.layer[0].parameters(), 'lr': 1e-5},\n",
    "#     {'params': bert.encoder.layer[1].parameters(), 'lr': 1.5e-5},\n",
    "#     ...\n",
    "#     {'params': classifier.parameters(), 'lr': 5e-5},\n",
    "# ]\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [BERT Paper](https://arxiv.org/abs/1810.04805) - Original BERT paper\n",
    "- [Hugging Face Course](https://huggingface.co/learn/nlp-course) - Comprehensive tutorial\n",
    "- [How to Fine-Tune BERT](https://mccormickml.com/2019/07/22/BERT-fine-tuning/) - Practical guide\n",
    "- [RoBERTa](https://arxiv.org/abs/1907.11692) - Improved BERT training\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "import gc\n",
    "\n",
    "del model, bert, optimizer, scheduler\n",
    "del train_loader, test_loader, train_ds, test_ds\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"Memory cleared! Ready for the next notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Up\n",
    "\n",
    "In **Notebook 06: GPT Text Generation**, we'll:\n",
    "- Load a GPT-style language model\n",
    "- Implement different decoding strategies\n",
    "- Generate creative text with sampling and beam search\n",
    "\n",
    "---\n",
    "\n",
    "*Congratulations! You've fine-tuned your first BERT model! This is the same technique used in production ML systems worldwide.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}