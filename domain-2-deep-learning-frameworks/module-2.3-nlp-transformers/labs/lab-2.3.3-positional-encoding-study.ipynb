{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.3.3: Positional Encoding Study\n",
    "\n",
    "**Module:** 2.3 - Natural Language Processing & Transformers  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐⭐\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand WHY position information is needed in Transformers\n",
    "- [ ] Implement sinusoidal positional encodings (original Transformer)\n",
    "- [ ] Implement Rotary Position Embeddings (RoPE, used in LLaMA)\n",
    "- [ ] Understand ALiBi and other modern approaches\n",
    "- [ ] Compare different strategies and their trade-offs\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Notebooks 01-02 (Attention and Transformer blocks)\n",
    "- Knowledge of: Trigonometry basics (sin, cos), complex numbers (helpful for RoPE)\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "**Different models use different position strategies:**\n",
    "- **Original Transformer, BERT**: Sinusoidal or learned embeddings\n",
    "- **GPT-2, GPT-3**: Learned positional embeddings\n",
    "- **LLaMA, Mistral, Qwen**: Rotary Position Embeddings (RoPE)\n",
    "- **BLOOM, MPT**: ALiBi (Attention with Linear Biases)\n",
    "\n",
    "The choice affects:\n",
    "- Maximum context length\n",
    "- Ability to extrapolate to longer sequences\n",
    "- Computational efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: Why Do Transformers Need Position Information?\n",
    "\n",
    "> **Imagine reading a shuffled book.**\n",
    ">\n",
    "> Without page numbers, you'd have no idea what order the pages go in! The sentence \"The cat ate the mouse\" would look the same as \"The mouse ate the cat\" to a position-blind model.\n",
    ">\n",
    "> **The problem:** Self-attention treats input as a SET, not a SEQUENCE.\n",
    "> - It computes attention between all pairs of words\n",
    "> - Nothing tells it that word 1 comes before word 2\n",
    "> - \"I like cats\" = \"cats like I\" = \"like I cats\" (all the same!)\n",
    ">\n",
    "> **The solution:** Add position information to each word embedding.\n",
    "> - Word embedding: \"What word is this?\"\n",
    "> - Position encoding: \"Where is this word in the sequence?\"\n",
    "> - Combined: \"This is the word 'cat' at position 3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Try to import seaborn for enhanced visualizations\nHAS_SEABORN = False\ntry:\n    import seaborn as sns\n    HAS_SEABORN = True\nexcept ImportError:\n    print(\"⚠️ seaborn not installed. Using matplotlib defaults.\")\n\n# Set up plotting\nplt.style.use(\"default\")\n\n# ============================================================\n# Visualization Constants (L3 fix: extract magic numbers)\n# ============================================================\nFIGURE_SIZE_SMALL = (10, 4)       # For simple 2-panel plots\nFIGURE_SIZE_MEDIUM = (12, 5)      # For comparison plots\nFIGURE_SIZE_LARGE = (14, 10)      # For multi-panel visualizations\nFIGURE_SIZE_WIDE = (15, 4)        # For wide horizontal layouts\nFIGURE_SIZE_GRID = (16, 8)        # For grid layouts\n\n# Common plot settings\nCOLORMAP_SEQUENTIAL = \"Blues\"\nCOLORMAP_DIVERGING = \"RdBu\"\nCOLORMAP_VIRIDIS = \"viridis\"\nGRID_ALPHA = 0.3\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\ntorch.manual_seed(42)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Demonstrating the Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate that attention is permutation-invariant\n",
    "\n",
    "def simple_attention(Q, K, V):\n",
    "    \"\"\"Simple self-attention without position info.\"\"\"\n",
    "    d_k = K.size(-1)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.matmul(weights, V)\n",
    "\n",
    "# Create some \"word embeddings\"\n",
    "words = [\"The\", \"cat\", \"sat\"]\n",
    "embeddings = torch.randn(1, 3, 8)  # (batch, seq, dim)\n",
    "\n",
    "# Original order\n",
    "out_original = simple_attention(embeddings, embeddings, embeddings)\n",
    "\n",
    "# Shuffled order (swap positions 1 and 2)\n",
    "shuffled = embeddings[:, [0, 2, 1], :]\n",
    "out_shuffled = simple_attention(shuffled, shuffled, shuffled)\n",
    "\n",
    "# Unshuffle the output to compare\n",
    "out_shuffled_unshuffled = out_shuffled[:, [0, 2, 1], :]\n",
    "\n",
    "# Compare\n",
    "print(\"Attention is permutation-equivariant!\")\n",
    "print(f\"Output for 'The cat sat': shape {out_original.shape}\")\n",
    "print(f\"Output for 'The sat cat' (unshuffled): shape {out_shuffled_unshuffled.shape}\")\n",
    "print(f\"Are they the same? {torch.allclose(out_original, out_shuffled_unshuffled, atol=1e-6)}\")\n",
    "print(\"\\n⚠️ Without position info, the model can't tell word order!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Sinusoidal Positional Encoding\n",
    "\n",
    "### ELI5: The Sinusoidal Approach\n",
    "\n",
    "> **Imagine a clock with many hands.**\n",
    ">\n",
    "> - The second hand moves fastest (high frequency)\n",
    "> - The minute hand moves slower\n",
    "> - The hour hand moves slowest (low frequency)\n",
    ">\n",
    "> **By looking at all the hands, you can tell exactly what time it is!**\n",
    ">\n",
    "> Sinusoidal encoding uses the same idea:\n",
    "> - Different dimensions oscillate at different frequencies\n",
    "> - Position 0 has a unique combination of values\n",
    "> - Position 1 has a different combination\n",
    "> - Each position gets a unique \"fingerprint\"\n",
    "\n",
    "### The Formula\n",
    "\n",
    "```\n",
    "PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
    "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal Positional Encoding from \"Attention Is All You Need\".\n",
    "    \n",
    "    Uses sin and cos functions at different frequencies to encode position.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            max_len: Maximum sequence length to pre-compute\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        \n",
    "        # Position indices: [0, 1, 2, ..., max_len-1]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Dimension indices for the exponential term\n",
    "        # div_term = 10000^(2i/d_model)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        \n",
    "        # Apply sin to even indices, cos to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Even dimensions\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd dimensions\n",
    "        \n",
    "        # Add batch dimension and register as buffer (not a parameter)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Add positional encoding to input.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, d_model)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor with position info added\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        # Add position encoding (broadcasting handles batch dimension)\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Create and test\n",
    "d_model = 64\n",
    "pe = SinusoidalPositionalEncoding(d_model, max_len=100)\n",
    "\n",
    "# Test with dummy input\n",
    "x = torch.zeros(1, 20, d_model)  # All zeros, so output = just PE\n",
    "out = pe(x)\n",
    "print(f\"Positional encoding shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the positional encoding\n",
    "\n",
    "def visualize_sinusoidal_pe(d_model=64, max_len=100):\n",
    "    \"\"\"Visualize sinusoidal positional encoding patterns.\"\"\"\n",
    "    \n",
    "    pe = SinusoidalPositionalEncoding(d_model, max_len)\n",
    "    encoding = pe.pe[0].numpy()  # (max_len, d_model)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Full heatmap\n",
    "    ax = axes[0, 0]\n",
    "    im = ax.imshow(encoding[:50, :32], aspect='auto', cmap='RdBu')\n",
    "    ax.set_xlabel('Dimension')\n",
    "    ax.set_ylabel('Position')\n",
    "    ax.set_title('Positional Encoding Heatmap\\n(first 50 positions, 32 dims)')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Show different frequency waves\n",
    "    ax = axes[0, 1]\n",
    "    for dim in [0, 4, 16, 32, 48]:\n",
    "        ax.plot(encoding[:50, dim], label=f'dim {dim}')\n",
    "    ax.set_xlabel('Position')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title('Different Dimensions = Different Frequencies')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Position similarity matrix\n",
    "    ax = axes[1, 0]\n",
    "    positions = encoding[:20]\n",
    "    similarity = np.dot(positions, positions.T)\n",
    "    similarity = similarity / (np.linalg.norm(positions, axis=1, keepdims=True) @ \n",
    "                               np.linalg.norm(positions, axis=1, keepdims=True).T)\n",
    "    im = ax.imshow(similarity, cmap='viridis')\n",
    "    ax.set_xlabel('Position')\n",
    "    ax.set_ylabel('Position')\n",
    "    ax.set_title('Position Similarity\\n(nearby positions are more similar)')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Relative position demo\n",
    "    ax = axes[1, 1]\n",
    "    # Show that PE(pos) and PE(pos+k) have a linear relationship\n",
    "    pos_0 = encoding[0]\n",
    "    pos_5 = encoding[5]\n",
    "    pos_10 = encoding[10]\n",
    "    pos_15 = encoding[15]\n",
    "    \n",
    "    # Difference vectors\n",
    "    diff_0_5 = pos_5 - pos_0\n",
    "    diff_10_15 = pos_15 - pos_10\n",
    "    \n",
    "    ax.plot(diff_0_5[:32], label='PE(5) - PE(0)', alpha=0.8)\n",
    "    ax.plot(diff_10_15[:32], label='PE(15) - PE(10)', alpha=0.8)\n",
    "    ax.set_xlabel('Dimension')\n",
    "    ax.set_ylabel('Difference')\n",
    "    ax.set_title('Relative Position is Consistent\\n(same k-offset has similar pattern)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_sinusoidal_pe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Properties of Sinusoidal PE:\n",
    "\n",
    "1. **Unique encoding**: Each position has a unique pattern\n",
    "2. **Bounded**: Values are always between -1 and 1\n",
    "3. **Relative position**: PE(pos+k) can be expressed as a linear function of PE(pos)\n",
    "4. **Extrapolation**: Can theoretically handle longer sequences than seen during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Learned Positional Embeddings\n",
    "\n",
    "GPT-2 and many models use learned position embeddings instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedPositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Learned positional embeddings.\n",
    "    \n",
    "    Each position has a learnable embedding vector.\n",
    "    Used in GPT-2, GPT-3, BERT.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, max_len: int = 512, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Learnable embedding for each position\n",
    "        self.embedding = nn.Embedding(max_len, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Initialize with small random values\n",
    "        nn.init.normal_(self.embedding.weight, mean=0, std=0.02)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Add position embeddings to input.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Create position indices [0, 1, 2, ..., seq_len-1]\n",
    "        positions = torch.arange(seq_len, device=x.device)\n",
    "        \n",
    "        # Get position embeddings\n",
    "        pos_emb = self.embedding(positions)  # (seq_len, d_model)\n",
    "        \n",
    "        # Add to input\n",
    "        x = x + pos_emb\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Compare parameters\n",
    "d_model = 768\n",
    "max_len = 512\n",
    "\n",
    "sinusoidal = SinusoidalPositionalEncoding(d_model, max_len)\n",
    "learned = LearnedPositionalEmbedding(d_model, max_len)\n",
    "\n",
    "print(\"Parameter comparison:\")\n",
    "print(f\"  Sinusoidal: {sum(p.numel() for p in sinusoidal.parameters()):,} (none! it's fixed)\")\n",
    "print(f\"  Learned:    {sum(p.numel() for p in learned.parameters()):,} ({max_len} x {d_model})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trade-offs:\n",
    "\n",
    "| Aspect | Sinusoidal | Learned |\n",
    "|--------|------------|----------|\n",
    "| Parameters | 0 | max_len × d_model |\n",
    "| Extrapolation | Yes (theoretically) | No (limited to max_len) |\n",
    "| Performance | Good | Sometimes slightly better |\n",
    "| Training | Fixed | Learns from data |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Rotary Position Embeddings (RoPE)\n",
    "\n",
    "### ELI5: RoPE\n",
    "\n",
    "> **Imagine a compass needle.**\n",
    ">\n",
    "> - At position 0, the needle points North\n",
    "> - At position 1, it rotates slightly\n",
    "> - At position 2, it rotates more\n",
    "> - The ANGLE tells you the position!\n",
    ">\n",
    "> **RoPE rotates the query and key vectors by an angle based on position.**\n",
    "> - When computing attention, the dot product naturally encodes relative position\n",
    "> - No need to add anything to the embeddings!\n",
    ">\n",
    "> **The magic:** q(pos_m) · k(pos_n) depends only on (m - n), the relative position!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary Position Embeddings (RoPE).\n",
    "    \n",
    "    Used in LLaMA, Mistral, and many modern LLMs.\n",
    "    \n",
    "    Key insight: Apply rotation to Q and K, making the dot product\n",
    "    naturally depend on relative position.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, max_len: int = 2048, base: int = 10000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: Dimension of the embeddings (must be even)\n",
    "            max_len: Maximum sequence length\n",
    "            base: Base for the frequency calculation\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        assert dim % 2 == 0, \"Dimension must be even for RoPE\"\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.max_len = max_len\n",
    "        self.base = base\n",
    "        \n",
    "        # Precompute the frequency for each dimension pair\n",
    "        # freq_i = 1 / (base^(2i/dim))\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        \n",
    "        # Precompute sin and cos for all positions\n",
    "        self._build_cache(max_len)\n",
    "        \n",
    "    def _build_cache(self, seq_len: int):\n",
    "        \"\"\"Build sin/cos cache for given sequence length.\"\"\"\n",
    "        # Position indices\n",
    "        t = torch.arange(seq_len, device=self.inv_freq.device)\n",
    "        \n",
    "        # Outer product: (seq_len,) x (dim/2,) -> (seq_len, dim/2)\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        \n",
    "        # Duplicate for pairs: (seq_len, dim)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)\n",
    "        \n",
    "        # Cache sin and cos\n",
    "        self.register_buffer('cos_cached', emb.cos(), persistent=False)\n",
    "        self.register_buffer('sin_cached', emb.sin(), persistent=False)\n",
    "        \n",
    "    def forward(self, x, seq_len: int = None):\n",
    "        \"\"\"\n",
    "        Get cos and sin values for rotary embedding.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (used for device/dtype)\n",
    "            seq_len: Sequence length\n",
    "            \n",
    "        Returns:\n",
    "            cos, sin: Tensors of shape (seq_len, dim)\n",
    "        \"\"\"\n",
    "        if seq_len is None:\n",
    "            seq_len = x.size(1)\n",
    "            \n",
    "        if seq_len > self.max_len:\n",
    "            # Extend cache if needed\n",
    "            self._build_cache(seq_len)\n",
    "            \n",
    "        return (\n",
    "            self.cos_cached[:seq_len].to(x.dtype),\n",
    "            self.sin_cached[:seq_len].to(x.dtype)\n",
    "        )\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotate half the hidden dims of x.\"\"\"\n",
    "    x1 = x[..., :x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2:]\n",
    "    return torch.cat([-x2, x1], dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    \"\"\"\n",
    "    Apply rotary position embedding to queries and keys.\n",
    "    \n",
    "    Args:\n",
    "        q: Query tensor (batch, heads, seq_len, dim)\n",
    "        k: Key tensor (batch, heads, seq_len, dim)\n",
    "        cos: Cosine values (seq_len, dim)\n",
    "        sin: Sine values (seq_len, dim)\n",
    "        \n",
    "    Returns:\n",
    "        Rotated q and k\n",
    "    \"\"\"\n",
    "    # Reshape cos/sin for broadcasting\n",
    "    cos = cos.unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, dim)\n",
    "    sin = sin.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    # Apply rotation\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    \n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "# Demonstrate RoPE\n",
    "dim = 64\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "num_heads = 4\n",
    "\n",
    "rope = RotaryPositionalEmbedding(dim)\n",
    "\n",
    "# Create queries and keys\n",
    "q = torch.randn(batch_size, num_heads, seq_len, dim)\n",
    "k = torch.randn(batch_size, num_heads, seq_len, dim)\n",
    "\n",
    "# Get cos/sin\n",
    "cos, sin = rope(q, seq_len)\n",
    "\n",
    "# Apply RoPE\n",
    "q_rot, k_rot = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "\n",
    "print(f\"Original Q shape: {q.shape}\")\n",
    "print(f\"Rotated Q shape:  {q_rot.shape}\")\n",
    "print(f\"cos/sin shape:    {cos.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RoPE rotation\n",
    "\n",
    "def visualize_rope():\n",
    "    \"\"\"Visualize how RoPE rotates vectors.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # 2D visualization of rotation\n",
    "    ax = axes[0]\n",
    "    dim = 2\n",
    "    rope = RotaryPositionalEmbedding(dim, max_len=20)\n",
    "    \n",
    "    # Original vector\n",
    "    v = torch.tensor([[[[1.0, 0.0]]]])  # Unit vector pointing right\n",
    "    \n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, 10))\n",
    "    \n",
    "    for pos in range(10):\n",
    "        cos, sin = rope(v, pos + 1)\n",
    "        cos, sin = cos[-1:], sin[-1:]  # Last position\n",
    "        v_rot, _ = apply_rotary_pos_emb(v, v, cos, sin)\n",
    "        \n",
    "        ax.arrow(0, 0, v_rot[0, 0, 0, 0].item(), v_rot[0, 0, 0, 1].item(),\n",
    "                head_width=0.05, head_length=0.05, fc=colors[pos], ec=colors[pos],\n",
    "                label=f'pos {pos}' if pos % 2 == 0 else None)\n",
    "    \n",
    "    ax.set_xlim(-1.5, 1.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title('RoPE: Vector Rotation by Position')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Show attention pattern with RoPE\n",
    "    ax = axes[1]\n",
    "    dim = 64\n",
    "    seq_len = 20\n",
    "    rope = RotaryPositionalEmbedding(dim)\n",
    "    \n",
    "    # Create Q and K (same values to isolate position effect)\n",
    "    q = torch.ones(1, 1, seq_len, dim)\n",
    "    k = torch.ones(1, 1, seq_len, dim)\n",
    "    \n",
    "    cos, sin = rope(q, seq_len)\n",
    "    q_rot, k_rot = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = torch.matmul(q_rot, k_rot.transpose(-2, -1)) / math.sqrt(dim)\n",
    "    \n",
    "    im = ax.imshow(scores[0, 0].detach().numpy(), cmap='Blues')\n",
    "    ax.set_xlabel('Key Position')\n",
    "    ax.set_ylabel('Query Position')\n",
    "    ax.set_title('Attention Scores with RoPE\\n(diagonal = self, off-diagonal = relative pos)')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Relative position effect\n",
    "    ax = axes[2]\n",
    "    # Show that score depends only on relative position\n",
    "    relative_scores = []\n",
    "    for i in range(seq_len):\n",
    "        relative_scores.append(scores[0, 0, i, :].detach().numpy())\n",
    "    \n",
    "    # Plot score vs relative position for different query positions\n",
    "    for i in [0, 5, 10, 15]:\n",
    "        rel_pos = np.arange(seq_len) - i\n",
    "        ax.plot(rel_pos, relative_scores[i], label=f'query pos {i}', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Relative Position (key - query)')\n",
    "    ax.set_ylabel('Attention Score')\n",
    "    ax.set_title('RoPE: Scores Depend on Relative Position')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_rope()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoPE Advantages:\n",
    "\n",
    "1. **Relative position encoding**: Attention naturally depends on relative position\n",
    "2. **No extra parameters**: Just rotation, no learned embeddings needed\n",
    "3. **Length extrapolation**: Can extend to longer sequences than training (with techniques like YaRN)\n",
    "4. **Efficient**: Applied only to Q and K, not added to embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: ALiBi (Attention with Linear Biases)\n",
    "\n",
    "### ELI5: ALiBi\n",
    "\n",
    "> **Instead of modifying the embeddings, ALiBi adds a penalty to attention scores based on distance.**\n",
    ">\n",
    "> \"The farther apart two words are, the less they should attend to each other.\"\n",
    ">\n",
    "> It's like saying: \"Far-away words get a small penalty, nearby words don't.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALiBi(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention with Linear Biases.\n",
    "    \n",
    "    Adds a linear bias based on relative position to attention scores.\n",
    "    Used in BLOOM, MPT, and other models.\n",
    "    \n",
    "    score(i, j) = q_i · k_j - m * |i - j|\n",
    "    \n",
    "    where m is a per-head slope.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_heads: Number of attention heads\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Compute slopes for each head\n",
    "        # Slopes are geometric sequence: 2^(-8/n), 2^(-16/n), ...\n",
    "        slopes = self._get_slopes(num_heads)\n",
    "        self.register_buffer('slopes', torch.tensor(slopes).float())\n",
    "        \n",
    "    def _get_slopes(self, num_heads: int):\n",
    "        \"\"\"\n",
    "        Get the slope values for each head.\n",
    "        \n",
    "        Returns geometric sequence for power-of-2 num_heads,\n",
    "        interpolated sequence otherwise.\n",
    "        \"\"\"\n",
    "        def get_slopes_power_of_2(n):\n",
    "            start = 2 ** (-8 / n)\n",
    "            ratio = start\n",
    "            return [start * (ratio ** i) for i in range(n)]\n",
    "        \n",
    "        if math.log2(num_heads).is_integer():\n",
    "            return get_slopes_power_of_2(num_heads)\n",
    "        else:\n",
    "            # For non-power-of-2, use closest power of 2 and interpolate\n",
    "            closest_power_of_2 = 2 ** math.floor(math.log2(num_heads))\n",
    "            slopes = get_slopes_power_of_2(closest_power_of_2)\n",
    "            extra_slopes = get_slopes_power_of_2(2 * closest_power_of_2)[0::2]\n",
    "            return slopes + extra_slopes[:num_heads - closest_power_of_2]\n",
    "    \n",
    "    def forward(self, seq_len: int):\n",
    "        \"\"\"\n",
    "        Compute ALiBi bias matrix.\n",
    "        \n",
    "        Args:\n",
    "            seq_len: Sequence length\n",
    "            \n",
    "        Returns:\n",
    "            Bias tensor of shape (num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # Create relative position matrix\n",
    "        # For causal attention, we only look at positions <= current\n",
    "        positions = torch.arange(seq_len)\n",
    "        relative_pos = positions.unsqueeze(0) - positions.unsqueeze(1)  # (seq, seq)\n",
    "        \n",
    "        # For causal: upper triangle should be -inf (future positions)\n",
    "        # Lower triangle: use negative distance as bias\n",
    "        bias = -torch.abs(relative_pos).float()  # (seq, seq)\n",
    "        \n",
    "        # Multiply by slopes for each head\n",
    "        bias = bias.unsqueeze(0) * self.slopes.unsqueeze(-1).unsqueeze(-1)\n",
    "        # (num_heads, seq, seq)\n",
    "        \n",
    "        return bias\n",
    "\n",
    "# Visualize ALiBi\n",
    "num_heads = 8\n",
    "seq_len = 20\n",
    "alibi = ALiBi(num_heads)\n",
    "\n",
    "bias = alibi(seq_len)\n",
    "print(f\"ALiBi bias shape: {bias.shape}\")\n",
    "print(f\"Slopes: {alibi.slopes.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ALiBi patterns\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    head_bias = bias[i].detach().numpy()\n",
    "    \n",
    "    im = ax.imshow(head_bias, cmap='RdBu', vmin=-5, vmax=0)\n",
    "    ax.set_title(f'Head {i+1}\\nSlope: {alibi.slopes[i]:.4f}')\n",
    "    ax.set_xlabel('Key Position')\n",
    "    ax.set_ylabel('Query Position')\n",
    "\n",
    "plt.suptitle('ALiBi Bias Patterns per Head\\n(Darker = More Penalty for Distance)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: Each head has a different slope, so different heads attend to different ranges!\")\n",
    "print(\"Head 1 (large slope): Strong local attention\")\n",
    "print(\"Head 8 (small slope): Weak distance penalty, more global attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALiBi Advantages:\n",
    "\n",
    "1. **No learned parameters**: Just a fixed bias\n",
    "2. **Excellent extrapolation**: Works well on sequences longer than training\n",
    "3. **Simple implementation**: Just add bias to attention scores\n",
    "4. **Multi-scale attention**: Different heads attend at different ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Comparison of Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison\n",
    "\n",
    "comparison = {\n",
    "    \"Method\": [\"Sinusoidal\", \"Learned\", \"RoPE\", \"ALiBi\"],\n",
    "    \"Parameters\": [\"0\", \"max_len × d_model\", \"0\", \"0\"],\n",
    "    \"Applied to\": [\"Embeddings\", \"Embeddings\", \"Q and K\", \"Attention scores\"],\n",
    "    \"Position type\": [\"Absolute\", \"Absolute\", \"Relative\", \"Relative\"],\n",
    "    \"Extrapolation\": [\"Limited\", \"Poor\", \"Good (with NTK/YaRN)\", \"Excellent\"],\n",
    "    \"Used in\": [\"Original Transformer\", \"GPT-2, BERT\", \"LLaMA, Mistral\", \"BLOOM, MPT\"]\n",
    "}\n",
    "\n",
    "print(\"Positional Encoding Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Print as table\n",
    "max_lens = [max(len(str(v)) for v in [k] + vals) for k, vals in comparison.items()]\n",
    "\n",
    "# Header\n",
    "header = \" | \".join(k.ljust(l) for k, l in zip(comparison.keys(), max_lens))\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "# Rows\n",
    "for i in range(len(comparison[\"Method\"])):\n",
    "    row = \" | \".join(str(comparison[k][i]).ljust(l) for k, l in zip(comparison.keys(), max_lens))\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test extrapolation\n",
    "\n",
    "def test_extrapolation():\n",
    "    \"\"\"Test how different methods handle longer sequences than training.\"\"\"\n",
    "    \n",
    "    d_model = 64\n",
    "    train_len = 128\n",
    "    test_lens = [64, 128, 256, 512, 1024]\n",
    "    \n",
    "    results = {\"Sinusoidal\": [], \"Learned\": [], \"RoPE\": []}\n",
    "    \n",
    "    # Create models \"trained\" on max_len=128\n",
    "    sinusoidal = SinusoidalPositionalEncoding(d_model, max_len=train_len)\n",
    "    learned = LearnedPositionalEmbedding(d_model, max_len=train_len)\n",
    "    rope = RotaryPositionalEmbedding(d_model, max_len=train_len)\n",
    "    \n",
    "    print(f\"Testing extrapolation (trained on max_len={train_len}):\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for test_len in test_lens:\n",
    "        # Sinusoidal can extrapolate\n",
    "        try:\n",
    "            # Extend the buffer if needed\n",
    "            if test_len > sinusoidal.pe.size(1):\n",
    "                # Recompute for longer sequence\n",
    "                new_sinusoidal = SinusoidalPositionalEncoding(d_model, max_len=test_len)\n",
    "                x = torch.zeros(1, test_len, d_model)\n",
    "                _ = new_sinusoidal(x)\n",
    "            else:\n",
    "                x = torch.zeros(1, test_len, d_model)\n",
    "                _ = sinusoidal(x)\n",
    "            results[\"Sinusoidal\"].append(\"✓\")\n",
    "        except Exception as e:\n",
    "            results[\"Sinusoidal\"].append(f\"✗ ({type(e).__name__})\")\n",
    "        \n",
    "        # Learned cannot extrapolate beyond max_len\n",
    "        try:\n",
    "            x = torch.zeros(1, test_len, d_model)\n",
    "            _ = learned(x)\n",
    "            results[\"Learned\"].append(\"✓\")\n",
    "        except Exception as e:\n",
    "            results[\"Learned\"].append(f\"✗ ({type(e).__name__})\")\n",
    "        \n",
    "        # RoPE can extrapolate (cache is extended)\n",
    "        try:\n",
    "            x = torch.zeros(1, 1, test_len, d_model)\n",
    "            cos, sin = rope(x, test_len)\n",
    "            results[\"RoPE\"].append(\"✓\")\n",
    "        except Exception as e:\n",
    "            results[\"RoPE\"].append(f\"✗ ({type(e).__name__})\")\n",
    "    \n",
    "    print(f\"\\n{'Length':<10} {'Sinusoidal':<15} {'Learned':<20} {'RoPE':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, test_len in enumerate(test_lens):\n",
    "        print(f\"{test_len:<10} {results['Sinusoidal'][i]:<15} {results['Learned'][i]:<20} {results['RoPE'][i]:<15}\")\n",
    "\n",
    "test_extrapolation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Exercises\n",
    "\n",
    "### Exercise 1: Implement YaRN (Yet another RoPE extensioN)\n",
    "\n",
    "YaRN modifies RoPE to better handle very long sequences. The key idea is to scale the rotary frequencies.\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Modify the frequency computation: `inv_freq = inv_freq / scale`\n",
    "where scale depends on the ratio of target length to training length.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YaRNPositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    YaRN: Yet another RoPE extensioN\n",
    "    \n",
    "    Extends RoPE to handle longer sequences through frequency scaling.\n",
    "    \n",
    "    TODO: Implement this!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim, max_len=2048, base=10000, scale=1.0):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, seq_len=None):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "# Test your implementation:\n",
    "# yarn = YaRNPositionalEmbedding(64, max_len=128, scale=2.0)\n",
    "# x = torch.randn(1, 1, 256, 64)\n",
    "# cos, sin = yarn(x, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Relative Position Bias (T5 style)\n",
    "\n",
    "T5 uses learned relative position biases. Implement a simplified version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5RelativePositionBias(nn.Module):\n",
    "    \"\"\"\n",
    "    Relative position bias used in T5.\n",
    "    \n",
    "    Learns a bias for each relative position bucket.\n",
    "    \n",
    "    TODO: Implement this!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, num_buckets=32, max_distance=128):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        # Hint: Use nn.Embedding for the biases\n",
    "        pass\n",
    "    \n",
    "    def _relative_position_bucket(self, relative_position, num_buckets, max_distance):\n",
    "        \"\"\"Convert relative position to bucket index.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def forward(self, seq_len):\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Adding PE to the wrong thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong: Adding position encoding after attention\n",
    "def wrong_order(x, attention, pos_enc):\n",
    "    x = attention(x, x, x)  # No position info!\n",
    "    x = x + pos_enc  # Too late\n",
    "    return x\n",
    "\n",
    "# Right: Adding position encoding before attention\n",
    "def right_order(x, attention, pos_enc):\n",
    "    x = x + pos_enc  # Add position info first\n",
    "    x = attention(x, x, x)  # Now attention sees position\n",
    "    return x\n",
    "\n",
    "print(\"Position encoding must be added BEFORE attention!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Using absolute position with variable-length inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong: Same position for different sequence lengths means different meanings\n",
    "# \"The cat\" -> positions [0, 1]\n",
    "# \"The big fluffy cat\" -> positions [0, 1, 2, 3]\n",
    "# Position 1 means \"cat\" in first, \"big\" in second!\n",
    "\n",
    "print(\"Problem with absolute positions:\")\n",
    "print(\"  'The cat'           -> pos[0]=The, pos[1]=cat\")\n",
    "print(\"  'The big fluffy cat' -> pos[0]=The, pos[1]=big, pos[2]=fluffy, pos[3]=cat\")\n",
    "print(\"\\n  Position 1 has different meanings!\")\n",
    "print(\"\\nSolution: Use relative position methods (RoPE, ALiBi) for better generalization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Forgetting to extend RoPE cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong: Not checking sequence length\n",
    "class WrongRoPE(nn.Module):\n",
    "    def __init__(self, dim, max_len):\n",
    "        super().__init__()\n",
    "        self.cos = torch.randn(max_len, dim)\n",
    "        self.sin = torch.randn(max_len, dim)\n",
    "        \n",
    "    def forward(self, seq_len):\n",
    "        return self.cos[:seq_len], self.sin[:seq_len]  # Error if seq_len > max_len!\n",
    "\n",
    "# Right: Extend cache dynamically\n",
    "# (as shown in our RotaryPositionalEmbedding implementation)\n",
    "\n",
    "print(\"Always handle sequences longer than the initial cache!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ✅ Why Transformers need position information\n",
    "- ✅ Sinusoidal positional encoding (original Transformer)\n",
    "- ✅ Learned positional embeddings (GPT-2, BERT)\n",
    "- ✅ Rotary Position Embeddings (RoPE) - modern LLMs\n",
    "- ✅ ALiBi - attention biases for long sequences\n",
    "- ✅ Trade-offs between different approaches\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "Implement **NTK-aware RoPE** which modifies the base frequency for better extrapolation:\n",
    "\n",
    "```python\n",
    "# Original: base = 10000\n",
    "# NTK-aware: base = 10000 * (scale ** (dim / (dim - 2)))\n",
    "```\n",
    "\n",
    "Where `scale` is the ratio of target sequence length to training length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your challenge implementation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [RoFormer Paper](https://arxiv.org/abs/2104.09864) - Original RoPE paper\n",
    "- [ALiBi Paper](https://arxiv.org/abs/2108.12409) - Attention with Linear Biases\n",
    "- [YaRN Paper](https://arxiv.org/abs/2309.00071) - Yet another RoPE extensioN\n",
    "- [Extending Context with RoPE](https://kaiokendev.github.io/context) - Great blog post\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Clean up\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"Memory cleared! Ready for the next notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Up\n",
    "\n",
    "In **Notebook 04: Tokenization Lab**, we'll learn how text becomes numbers:\n",
    "- Byte Pair Encoding (BPE)\n",
    "- SentencePiece\n",
    "- Comparing tokenizers from different models\n",
    "\n",
    "---\n",
    "\n",
    "*Excellent work! Position encodings are subtle but crucial for Transformer success.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}