{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 2.3.2: Building a Transformer Block\n\nNow that you understand attention, let's build a complete Transformer encoder block!\nThis is the building block of BERT, GPT, and modern LLMs.\n\n## Learning Objectives\n\nBy the end of this lab, you will:\n1. Build a complete Transformer encoder block with all components\n2. Understand layer normalization and its placement (Pre-LN vs Post-LN)\n3. Implement the feed-forward network (FFN) sublayer\n4. Connect everything with residual connections\n\n## Prerequisites\n\n- Completed: Lab 2.3.1 (Attention Mechanisms)\n- Understanding: Attention, residual connections, layer normalization\n\n**Time:** 2 hours\n**Difficulty:** ⭐⭐⭐ (Intermediate)\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: The Transformer Block\n",
    "\n",
    "> **Imagine a very organized study group:**\n",
    ">\n",
    "> **Step 1: Discussion Round (Attention)**\n",
    "> Everyone shares their ideas. Each person decides which ideas to pay attention to and updates their understanding.\n",
    ">\n",
    "> **Step 2: Personal Reflection (Feed-Forward)**\n",
    "> Each person privately thinks about what they learned and processes it through their own understanding.\n",
    ">\n",
    "> **Step 3: Note-Taking (Residual)**\n",
    "> Instead of replacing all old knowledge, you add the new insights to what you already knew.\n",
    ">\n",
    "> **Step 4: Summary (Layer Norm)**\n",
    "> Everyone normalizes their notes to keep things balanced and prevent some topics from dominating.\n",
    ">\n",
    "> **Repeat 6-24 times** with increasingly refined understanding!\n",
    "\n",
    "### The Architecture\n",
    "\n",
    "```\n",
    "Input\n",
    "  │\n",
    "  ├──────────────┐\n",
    "  │              │\n",
    "  ▼              │\n",
    "Multi-Head      │ (Residual Connection)\n",
    "Attention       │\n",
    "  │              │\n",
    "  ├──────────────┘\n",
    "  │\n",
    "  ▼\n",
    "Layer Norm\n",
    "  │\n",
    "  ├──────────────┐\n",
    "  │              │\n",
    "  ▼              │ (Residual Connection)\n",
    "Feed-Forward    │\n",
    "Network         │\n",
    "  │              │\n",
    "  ├──────────────┘\n",
    "  │\n",
    "  ▼\n",
    "Layer Norm\n",
    "  │\n",
    "  ▼\n",
    "Output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport matplotlib.pyplot as plt\nimport time\n\n# Check device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# DGX Spark detection\nUSE_BFLOAT16 = False\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"Memory: {gpu_mem:.1f} GB\")\n    \n    # Check for DGX Spark (Blackwell GB10) or other bfloat16-capable GPUs\n    if gpu_mem > 100 or \"GB10\" in torch.cuda.get_device_name(0):\n        USE_BFLOAT16 = True\n        print(\"\\n✨ DGX Spark detected! Will use bfloat16 for optimal performance.\")\n        print(\"   Note: bfloat16 will be applied explicitly to models, not globally.\")\n    elif torch.cuda.is_bf16_supported():\n        USE_BFLOAT16 = True\n        print(\"\\n✅ bfloat16 supported on this GPU\")\n\n# Note: We do NOT set torch.set_default_dtype() globally as it can cause issues\n# with pre-trained models. Instead, cast models explicitly when needed:\n#   model = model.to(dtype=torch.bfloat16)\n\ntorch.manual_seed(42)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Multi-Head Attention (Recap)\n",
    "\n",
    "Let's bring in our attention implementation from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention mechanism.\n",
    "    \n",
    "    This is the same implementation from Notebook 01,\n",
    "    cleaned up and production-ready.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        seq_len_q = query.size(1)\n",
    "        seq_len_k = key.size(1)\n",
    "        \n",
    "        # Project and reshape for multi-head\n",
    "        Q = self.W_q(query).view(batch_size, seq_len_q, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, seq_len_k, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, seq_len_k, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        context = torch.matmul(attention, V)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len_q, self.d_model)\n",
    "        \n",
    "        return self.W_o(context)\n",
    "\n",
    "# Quick test\n",
    "mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "x = torch.randn(2, 10, 512)  # (batch, seq_len, d_model)\n",
    "out = mha(x, x, x)\n",
    "print(f\"Multi-Head Attention output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Feed-Forward Network (FFN)\n",
    "\n",
    "### ELI5: The FFN\n",
    "\n",
    "> **After the group discussion (attention), each person needs private thinking time.**\n",
    ">\n",
    "> The Feed-Forward Network is like a two-step thought process:\n",
    "> 1. **Expand**: \"Let me consider MANY possible interpretations\" (expand to 4x dimensions)\n",
    "> 2. **Contract**: \"Now let me focus on what's most important\" (back to original size)\n",
    ">\n",
    "> This happens independently for each word/position!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network.\n",
    "    \n",
    "    FFN(x) = max(0, xW1 + b1)W2 + b2\n",
    "    \n",
    "    Also known as MLP (Multi-Layer Perceptron) in some papers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int = None, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            d_ff: Feed-forward dimension (typically 4 * d_model)\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Default: 4x expansion\n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, d_model)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Expand → ReLU → Contract\n",
    "        x = self.linear1(x)      # (batch, seq, d_ff)\n",
    "        x = F.relu(x)            # Non-linearity\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)      # (batch, seq, d_model)\n",
    "        return x\n",
    "\n",
    "# Visualize the expansion\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "ffn = FeedForward(d_model, d_ff)\n",
    "\n",
    "print(f\"FFN Architecture:\")\n",
    "print(f\"  Input:  {d_model} dimensions\")\n",
    "print(f\"  Hidden: {d_ff} dimensions (4x expansion)\")\n",
    "print(f\"  Output: {d_model} dimensions\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in ffn.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modern FFN Variants\n",
    "\n",
    "The original Transformer uses ReLU, but modern models often use:\n",
    "- **GELU** (used in BERT, GPT-2): Smoother than ReLU\n",
    "- **SwiGLU** (used in LLaMA, PaLM): Gated activation\n",
    "- **GeGLU** (used in some models): GELU-gated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardGELU(nn.Module):\n",
    "    \"\"\"FFN with GELU activation (modern standard).\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int = None, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.gelu(x)  # GELU instead of ReLU\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForwardSwiGLU(nn.Module):\n",
    "    \"\"\"\n",
    "    FFN with SwiGLU activation (LLaMA, PaLM style).\n",
    "    \n",
    "    SwiGLU(x) = (xW1 * Swish(xV)) W2\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int = None, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        if d_ff is None:\n",
    "            # For SwiGLU, we typically use 2/3 of the original d_ff\n",
    "            # because we have an extra gate projection\n",
    "            d_ff = int(4 * d_model * 2 / 3)\n",
    "        \n",
    "        self.w1 = nn.Linear(d_model, d_ff)\n",
    "        self.w2 = nn.Linear(d_ff, d_model)\n",
    "        self.w3 = nn.Linear(d_model, d_ff)  # Gate projection\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # SwiGLU: element-wise multiply with gated activation\n",
    "        gate = F.silu(self.w1(x))  # Swish activation\n",
    "        x = gate * self.w3(x)      # Gating\n",
    "        x = self.dropout(x)\n",
    "        x = self.w2(x)\n",
    "        return x\n",
    "\n",
    "# Compare activations\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x.numpy(), F.relu(x).numpy(), label='ReLU', linewidth=2)\n",
    "plt.plot(x.numpy(), F.gelu(x).numpy(), label='GELU', linewidth=2)\n",
    "plt.plot(x.numpy(), F.silu(x).numpy(), label='SiLU/Swish', linewidth=2)\n",
    "plt.legend()\n",
    "plt.title('Activation Functions')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Gradient comparison\n",
    "x_grad = x.clone().requires_grad_(True)\n",
    "F.relu(x_grad).sum().backward()\n",
    "relu_grad = x_grad.grad.clone()\n",
    "\n",
    "x_grad = x.clone().requires_grad_(True)\n",
    "F.gelu(x_grad).sum().backward()\n",
    "gelu_grad = x_grad.grad.clone()\n",
    "\n",
    "plt.plot(x.numpy(), relu_grad.numpy(), label='ReLU gradient', linewidth=2)\n",
    "plt.plot(x.numpy(), gelu_grad.numpy(), label='GELU gradient', linewidth=2)\n",
    "plt.legend()\n",
    "plt.title('Gradients')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Gradient')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: GELU and SiLU have smooth gradients, which can help training stability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Residual Connections\n",
    "\n",
    "### ELI5: Why Residual Connections?\n",
    "\n",
    "> **Imagine you're learning to cook a new dish.**\n",
    ">\n",
    "> **Without residual connections:**\n",
    "> Each lesson completely replaces what you knew before. If one lesson is bad, you forget everything!\n",
    ">\n",
    "> **With residual connections:**\n",
    "> Each lesson ADDS to your existing knowledge. Bad lessons don't erase good ones.\n",
    "> `new_knowledge = old_knowledge + lesson_learned`\n",
    ">\n",
    "> **Why this matters:**\n",
    "> - Gradients flow easily (no vanishing gradients)\n",
    "> - Easy for the model to learn identity (just output zeros)\n",
    "> - Training is much more stable\n",
    "\n",
    "### Mathematical View\n",
    "\n",
    "```\n",
    "Without residual: output = F(x)\n",
    "With residual:    output = x + F(x)\n",
    "```\n",
    "\n",
    "The model learns F(x) = what to ADD, not what to REPLACE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the gradient flow difference\n",
    "\n",
    "def gradient_flow_demo():\n",
    "    \"\"\"Show how residual connections help gradient flow.\"\"\"\n",
    "    \n",
    "    num_layers = 50\n",
    "    d_model = 64\n",
    "    \n",
    "    # Without residual connections\n",
    "    class DeepNoResidual(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layers = nn.ModuleList([\n",
    "                nn.Linear(d_model, d_model) for _ in range(num_layers)\n",
    "            ])\n",
    "            \n",
    "        def forward(self, x):\n",
    "            for layer in self.layers:\n",
    "                x = torch.tanh(layer(x))  # Tanh saturates easily\n",
    "            return x\n",
    "    \n",
    "    # With residual connections\n",
    "    class DeepWithResidual(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layers = nn.ModuleList([\n",
    "                nn.Linear(d_model, d_model) for _ in range(num_layers)\n",
    "            ])\n",
    "            \n",
    "        def forward(self, x):\n",
    "            for layer in self.layers:\n",
    "                x = x + torch.tanh(layer(x))  # ADD instead of replace\n",
    "            return x\n",
    "    \n",
    "    # Test gradient magnitudes\n",
    "    no_res = DeepNoResidual()\n",
    "    with_res = DeepWithResidual()\n",
    "    \n",
    "    x = torch.randn(1, d_model, requires_grad=True)\n",
    "    \n",
    "    # Forward + backward without residual\n",
    "    out_no_res = no_res(x)\n",
    "    out_no_res.sum().backward()\n",
    "    grad_no_res = x.grad.norm().item()\n",
    "    \n",
    "    x = torch.randn(1, d_model, requires_grad=True)\n",
    "    out_with_res = with_res(x)\n",
    "    out_with_res.sum().backward()\n",
    "    grad_with_res = x.grad.norm().item()\n",
    "    \n",
    "    print(f\"Gradient magnitude through {num_layers} layers:\")\n",
    "    print(f\"  Without residual: {grad_no_res:.2e}\")\n",
    "    print(f\"  With residual:    {grad_with_res:.2e}\")\n",
    "    print(f\"\\n  Ratio: {grad_with_res / (grad_no_res + 1e-10):.1f}x stronger gradients with residual!\")\n",
    "\n",
    "gradient_flow_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Layer Normalization\n",
    "\n",
    "### ELI5: Layer Normalization\n",
    "\n",
    "> **Imagine test scores in a class.**\n",
    "> - One test is out of 100, another out of 10, another out of 1000\n",
    "> - Hard to compare! \"I got 80\" means different things.\n",
    ">\n",
    "> **Layer normalization standardizes:**\n",
    "> - Make all values have mean=0, variance=1\n",
    "> - Now everything is on the same scale\n",
    "> - Training becomes more stable\n",
    "\n",
    "### Pre-LN vs Post-LN\n",
    "\n",
    "The original Transformer used \"Post-LN\" (normalize after), but modern models prefer \"Pre-LN\" (normalize before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-LN (Original Transformer, BERT)\n",
    "class PostLNBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Post-Layer Normalization: normalize AFTER the residual addition.\n",
    "    \n",
    "    output = LayerNorm(x + Sublayer(x))\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, sublayer):\n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.norm(x + self.sublayer(x))\n",
    "\n",
    "\n",
    "# Pre-LN (GPT-2, GPT-3, LLaMA, most modern models)\n",
    "class PreLNBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Pre-Layer Normalization: normalize BEFORE the sublayer.\n",
    "    \n",
    "    output = x + Sublayer(LayerNorm(x))\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, sublayer):\n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.sublayer(self.norm(x))\n",
    "\n",
    "\n",
    "print(\"Post-LN (Original):  output = LayerNorm(x + Sublayer(x))\")\n",
    "print(\"Pre-LN (Modern):     output = x + Sublayer(LayerNorm(x))\")\n",
    "print(\"\\nPre-LN advantages:\")\n",
    "print(\"  - More stable gradients at initialization\")\n",
    "print(\"  - Doesn't need learning rate warmup\")\n",
    "print(\"  - Easier to train very deep models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Complete Transformer Encoder Block\n",
    "\n",
    "Now let's put it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer Encoder block.\n",
    "    \n",
    "    Architecture (Pre-LN):\n",
    "        x -> LayerNorm -> MultiHeadAttention -> + -> LayerNorm -> FFN -> + -> output\n",
    "        |                                      |    |                    |\n",
    "        +--------------------------------------+    +--------------------+\n",
    "                  (residual)                            (residual)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        d_ff: int = None,\n",
    "        dropout: float = 0.1,\n",
    "        activation: str = \"gelu\",\n",
    "        pre_norm: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "            d_ff: Feed-forward dimension (default: 4 * d_model)\n",
    "            dropout: Dropout probability\n",
    "            activation: Activation function (\"relu\", \"gelu\", \"swiglu\")\n",
    "            pre_norm: Use Pre-LN (True) or Post-LN (False)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "        \n",
    "        self.pre_norm = pre_norm\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        if activation == \"swiglu\":\n",
    "            self.ffn = FeedForwardSwiGLU(d_model, d_ff, dropout)\n",
    "        elif activation == \"gelu\":\n",
    "            self.ffn = FeedForwardGELU(d_model, d_ff, dropout)\n",
    "        else:\n",
    "            self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Layer norms\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout for residual\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        if self.pre_norm:\n",
    "            # Pre-LN: Normalize before sublayer\n",
    "            # Attention block\n",
    "            normed = self.norm1(x)\n",
    "            attn_out = self.attention(normed, normed, normed, mask)\n",
    "            x = x + self.dropout(attn_out)\n",
    "            \n",
    "            # FFN block\n",
    "            normed = self.norm2(x)\n",
    "            ffn_out = self.ffn(normed)\n",
    "            x = x + self.dropout(ffn_out)\n",
    "        else:\n",
    "            # Post-LN: Normalize after residual\n",
    "            # Attention block\n",
    "            attn_out = self.attention(x, x, x, mask)\n",
    "            x = self.norm1(x + self.dropout(attn_out))\n",
    "            \n",
    "            # FFN block\n",
    "            ffn_out = self.ffn(x)\n",
    "            x = self.norm2(x + self.dropout(ffn_out))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create and test a single block\n",
    "block = TransformerEncoderBlock(\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048,\n",
    "    dropout=0.1,\n",
    "    activation=\"gelu\",\n",
    "    pre_norm=True\n",
    ")\n",
    "\n",
    "x = torch.randn(2, 10, 512)\n",
    "out = block(x)\n",
    "\n",
    "print(f\"Transformer Encoder Block:\")\n",
    "print(f\"  Input shape:  {x.shape}\")\n",
    "print(f\"  Output shape: {out.shape}\")\n",
    "print(f\"  Parameters:   {sum(p.numel() for p in block.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Stacking Blocks into a Full Encoder\n",
    "\n",
    "Real Transformers stack multiple blocks:\n",
    "- **BERT-base**: 12 blocks\n",
    "- **BERT-large**: 24 blocks\n",
    "- **GPT-3**: 96 blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Full Transformer Encoder: stack of encoder blocks.\n",
    "    \n",
    "    This is what BERT uses for understanding text!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        d_ff: int = None,\n",
    "        dropout: float = 0.1,\n",
    "        activation: str = \"gelu\",\n",
    "        pre_norm: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_layers: Number of encoder blocks to stack\n",
    "            d_model: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "            d_ff: Feed-forward dimension\n",
    "            dropout: Dropout probability\n",
    "            activation: Activation function\n",
    "            pre_norm: Use Pre-LN normalization\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(\n",
    "                d_model=d_model,\n",
    "                num_heads=num_heads,\n",
    "                d_ff=d_ff,\n",
    "                dropout=dropout,\n",
    "                activation=activation,\n",
    "                pre_norm=pre_norm\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm (for Pre-LN architecture)\n",
    "        self.final_norm = nn.LayerNorm(d_model) if pre_norm else nn.Identity()\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        return self.final_norm(x)\n",
    "\n",
    "\n",
    "# Create BERT-base style encoder\n",
    "encoder = TransformerEncoder(\n",
    "    num_layers=12,\n",
    "    d_model=768,\n",
    "    num_heads=12,\n",
    "    d_ff=3072,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(f\"BERT-base style Transformer Encoder:\")\n",
    "print(f\"  Layers: 12\")\n",
    "print(f\"  d_model: 768\")\n",
    "print(f\"  Heads: 12\")\n",
    "print(f\"  d_ff: 3072\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in encoder.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test with a realistic batch\nbatch_size = 8\nseq_len = 128\nd_model = 768\n\nx = torch.randn(batch_size, seq_len, d_model)\n\n# Move to GPU if available\nif torch.cuda.is_available():\n    encoder = encoder.cuda()\n    x = x.cuda()\n\n# Time the forward pass (ensure accurate timing with CUDA sync)\nif torch.cuda.is_available():\n    torch.cuda.synchronize()  # Wait for all pending operations\nstart = time.time()\nwith torch.no_grad():\n    out = encoder(x)\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()  # Wait for GPU computation to finish\nelapsed = time.time() - start\n\nprint(f\"\\nForward pass:\")\nprint(f\"  Input:  {x.shape}\")\nprint(f\"  Output: {out.shape}\")\nprint(f\"  Time:   {elapsed*1000:.2f} ms\")\nprint(f\"  Throughput: {batch_size * seq_len / elapsed:.0f} tokens/sec\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Comparison with PyTorch Built-in\n",
    "\n",
    "Let's verify our implementation matches PyTorch's!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch's built-in TransformerEncoderLayer\n",
    "pytorch_layer = nn.TransformerEncoderLayer(\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.0,  # No dropout for comparison\n",
    "    activation=\"gelu\",\n",
    "    batch_first=True,\n",
    "    norm_first=True  # Pre-LN\n",
    ")\n",
    "\n",
    "# Our implementation\n",
    "our_layer = TransformerEncoderBlock(\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048,\n",
    "    dropout=0.0,\n",
    "    activation=\"gelu\",\n",
    "    pre_norm=True\n",
    ")\n",
    "\n",
    "print(\"Parameter count comparison:\")\n",
    "print(f\"  PyTorch: {sum(p.numel() for p in pytorch_layer.parameters()):,}\")\n",
    "print(f\"  Ours:    {sum(p.numel() for p in our_layer.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed comparison\n",
    "x = torch.randn(32, 128, 512)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    pytorch_layer = pytorch_layer.cuda()\n",
    "    our_layer = our_layer.cuda()\n",
    "    x = x.cuda()\n",
    "\n",
    "# Warmup\n",
    "for _ in range(10):\n",
    "    _ = pytorch_layer(x)\n",
    "    _ = our_layer(x)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# Time PyTorch\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = pytorch_layer(x)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "pytorch_time = time.time() - start\n",
    "\n",
    "# Time ours\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = our_layer(x)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "our_time = time.time() - start\n",
    "\n",
    "print(\"\\nSpeed comparison (100 iterations):\")\n",
    "print(f\"  PyTorch: {pytorch_time*1000:.2f} ms\")\n",
    "print(f\"  Ours:    {our_time*1000:.2f} ms\")\n",
    "print(f\"  Ratio:   {our_time/pytorch_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Memory Analysis for DGX Spark\n",
    "\n",
    "Let's see what we can fit in our 128GB unified memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_transformer_memory(\n",
    "    batch_size: int,\n",
    "    seq_len: int,\n",
    "    d_model: int,\n",
    "    num_layers: int,\n",
    "    num_heads: int,\n",
    "    d_ff: int,\n",
    "    dtype_bytes: int = 2  # bfloat16\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Estimate memory usage for a Transformer encoder.\n",
    "    \n",
    "    Returns dictionary with memory breakdown.\n",
    "    \"\"\"\n",
    "    # Parameters per layer\n",
    "    # Attention: 4 * d_model^2 (W_q, W_k, W_v, W_o)\n",
    "    attention_params = 4 * d_model * d_model\n",
    "    # FFN: d_model * d_ff + d_ff * d_model\n",
    "    ffn_params = 2 * d_model * d_ff\n",
    "    # Layer norms: 2 * 2 * d_model (gamma, beta for 2 norms)\n",
    "    norm_params = 4 * d_model\n",
    "    \n",
    "    total_params = num_layers * (attention_params + ffn_params + norm_params)\n",
    "    param_memory = total_params * dtype_bytes\n",
    "    \n",
    "    # Activations (for backward pass)\n",
    "    # Input to each layer\n",
    "    activation_per_layer = batch_size * seq_len * d_model * dtype_bytes\n",
    "    # Attention scores (batch, heads, seq, seq)\n",
    "    attention_scores = batch_size * num_heads * seq_len * seq_len * dtype_bytes\n",
    "    # FFN hidden\n",
    "    ffn_hidden = batch_size * seq_len * d_ff * dtype_bytes\n",
    "    \n",
    "    activations = num_layers * (activation_per_layer + attention_scores + ffn_hidden)\n",
    "    \n",
    "    # Optimizer states (for Adam: 2x for moment estimates)\n",
    "    optimizer_memory = 2 * param_memory\n",
    "    \n",
    "    total = param_memory + activations + optimizer_memory\n",
    "    \n",
    "    return {\n",
    "        \"parameters\": total_params,\n",
    "        \"param_memory_gb\": param_memory / 1e9,\n",
    "        \"activation_memory_gb\": activations / 1e9,\n",
    "        \"optimizer_memory_gb\": optimizer_memory / 1e9,\n",
    "        \"total_memory_gb\": total / 1e9\n",
    "    }\n",
    "\n",
    "# Test different configurations\n",
    "configs = [\n",
    "    {\"name\": \"BERT-base\", \"layers\": 12, \"d_model\": 768, \"heads\": 12, \"d_ff\": 3072},\n",
    "    {\"name\": \"BERT-large\", \"layers\": 24, \"d_model\": 1024, \"heads\": 16, \"d_ff\": 4096},\n",
    "    {\"name\": \"GPT-2 Medium\", \"layers\": 24, \"d_model\": 1024, \"heads\": 16, \"d_ff\": 4096},\n",
    "    {\"name\": \"GPT-2 Large\", \"layers\": 36, \"d_model\": 1280, \"heads\": 20, \"d_ff\": 5120},\n",
    "]\n",
    "\n",
    "print(\"Memory estimates for batch_size=32, seq_len=512 (bfloat16):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for config in configs:\n",
    "    mem = estimate_transformer_memory(\n",
    "        batch_size=32,\n",
    "        seq_len=512,\n",
    "        d_model=config[\"d_model\"],\n",
    "        num_layers=config[\"layers\"],\n",
    "        num_heads=config[\"heads\"],\n",
    "        d_ff=config[\"d_ff\"]\n",
    "    )\n",
    "    \n",
    "    fits = \"✅\" if mem[\"total_memory_gb\"] < 128 else \"❌\"\n",
    "    \n",
    "    print(f\"\\n{config['name']}:\")\n",
    "    print(f\"  Parameters:  {mem['parameters']/1e6:.1f}M\")\n",
    "    print(f\"  Params mem:  {mem['param_memory_gb']:.2f} GB\")\n",
    "    print(f\"  Activations: {mem['activation_memory_gb']:.2f} GB\")\n",
    "    print(f\"  Optimizer:   {mem['optimizer_memory_gb']:.2f} GB\")\n",
    "    print(f\"  Total:       {mem['total_memory_gb']:.2f} GB {fits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DGX Spark Advantage\n",
    "\n",
    "With 128GB unified memory, you can:\n",
    "- Train BERT-large with large batch sizes\n",
    "- Fine-tune GPT-2 Large without memory tricks\n",
    "- Use longer sequences than typical GPUs allow\n",
    "\n",
    "On a typical 24GB GPU, you'd need gradient checkpointing or very small batches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Exercises\n",
    "\n",
    "### Exercise 1: Add Decoder Block\n",
    "\n",
    "Create a `TransformerDecoderBlock` that includes:\n",
    "1. Masked self-attention (causal)\n",
    "2. Cross-attention to encoder outputs\n",
    "3. Feed-forward network\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "The decoder has two attention layers: one for self-attention with causal mask, and one for cross-attention where queries come from decoder, keys/values from encoder.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Decoder block with:\n",
    "    1. Masked self-attention\n",
    "    2. Cross-attention to encoder\n",
    "    3. Feed-forward network\n",
    "    \n",
    "    TODO: Implement this!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, encoder_output, self_mask=None, cross_mask=None):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "# Test your implementation:\n",
    "# decoder_block = TransformerDecoderBlock(512, 8)\n",
    "# decoder_input = torch.randn(2, 10, 512)\n",
    "# encoder_output = torch.randn(2, 20, 512)\n",
    "# out = decoder_block(decoder_input, encoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement Gradient Checkpointing\n",
    "\n",
    "For very deep models, we can trade compute for memory by recomputing activations during backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class TransformerEncoderCheckpointed(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder with gradient checkpointing.\n",
    "    \n",
    "    This saves memory by not storing all activations,\n",
    "    at the cost of recomputing them during backward pass.\n",
    "    \n",
    "    TODO: Implement using torch.utils.checkpoint.checkpoint\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, mask=None, use_checkpointing=True):\n",
    "        # YOUR CODE HERE\n",
    "        # Hint: use checkpoint(layer, x, mask) instead of layer(x, mask)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Wrong Residual Connection Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong: Adding normalized output to non-normalized input\n",
    "def wrong_residual(x, sublayer, norm):\n",
    "    return x + norm(sublayer(x))  # Inconsistent normalization!\n",
    "\n",
    "# Right (Pre-LN): Normalize before sublayer, add to original\n",
    "def right_residual_preln(x, sublayer, norm):\n",
    "    return x + sublayer(norm(x))  # Consistent: both paths end unnormalized\n",
    "\n",
    "# Right (Post-LN): Normalize after residual\n",
    "def right_residual_postln(x, sublayer, norm):\n",
    "    return norm(x + sublayer(x))  # Consistent: output is always normalized\n",
    "\n",
    "print(\"Pre-LN:  output = x + sublayer(norm(x))\")\n",
    "print(\"Post-LN: output = norm(x + sublayer(x))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Forgetting Final Layer Norm in Pre-LN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong: No final norm in Pre-LN architecture\n",
    "class WrongPreLNEncoder(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        # Missing: self.final_norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x  # Output is not normalized!\n",
    "\n",
    "# Right: Include final layer norm\n",
    "class RightPreLNEncoder(nn.Module):\n",
    "    def __init__(self, layers, d_model):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.final_norm(x)  # Properly normalized output\n",
    "\n",
    "print(\"Pre-LN needs a final LayerNorm after all layers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Wrong Dropout Placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong: Dropout after residual addition\n",
    "def wrong_dropout(x, sublayer_out, norm, dropout):\n",
    "    return dropout(x + sublayer_out)  # Drops both original AND new info!\n",
    "\n",
    "# Right: Dropout on sublayer output before addition\n",
    "def right_dropout(x, sublayer_out, norm, dropout):\n",
    "    return x + dropout(sublayer_out)  # Only drops new info, preserves original\n",
    "\n",
    "print(\"Dropout should be applied to sublayer output BEFORE adding residual.\")\n",
    "print(\"This preserves the information in the skip connection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ✅ How to build a complete Transformer encoder block\n",
    "- ✅ The role of feed-forward networks and different activations\n",
    "- ✅ Why residual connections enable deep networks\n",
    "- ✅ Pre-LN vs Post-LN normalization strategies\n",
    "- ✅ How to stack blocks into a full encoder\n",
    "- ✅ Memory estimation for DGX Spark optimization\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "Implement **RMSNorm** (Root Mean Square Normalization) as an alternative to LayerNorm:\n",
    "\n",
    "```\n",
    "RMSNorm(x) = x * scale / sqrt(mean(x^2) + epsilon)\n",
    "```\n",
    "\n",
    "This is used in LLaMA and is slightly faster than LayerNorm since it doesn't compute the mean!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Root Mean Square Normalization (used in LLaMA).\n",
    "    \n",
    "    TODO: Implement this!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "# Test:\n",
    "# rms = RMSNorm(512)\n",
    "# x = torch.randn(2, 10, 512)\n",
    "# out = rms(x)\n",
    "# print(f\"RMSNorm output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [On Layer Normalization in the Transformer Architecture](https://arxiv.org/abs/2002.04745) - Pre-LN analysis\n",
    "- [GLU Variants Improve Transformer](https://arxiv.org/abs/2002.05202) - SwiGLU and friends\n",
    "- [Root Mean Square Layer Normalization](https://arxiv.org/abs/1910.07467) - RMSNorm paper\n",
    "- [Deep Residual Learning](https://arxiv.org/abs/1512.03385) - Original ResNet paper\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory\n",
    "import gc\n",
    "\n",
    "del encoder, block, pytorch_layer, our_layer\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"Memory cleared! Ready for the next notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Up\n",
    "\n",
    "In **Notebook 03: Positional Encoding Study**, we'll learn how Transformers understand word order:\n",
    "- Sinusoidal positional encodings\n",
    "- Rotary Position Embeddings (RoPE)\n",
    "- ALiBi and other modern approaches\n",
    "\n",
    "---\n",
    "\n",
    "*Excellent work! You've now built the core of modern NLP from scratch!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}