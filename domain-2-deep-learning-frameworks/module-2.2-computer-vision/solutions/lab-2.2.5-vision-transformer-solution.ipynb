{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.2.5 Solution: Vision Transformer\n",
    "\n",
    "**Module:** 2.2 - Computer Vision  \n",
    "**Type:** Solution Notebook\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains solutions for Vision Transformer exercises, including patch size analysis and DeiT implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Solution: Patch Embedding\n",
    "\n",
    "The foundation of Vision Transformers: converting images into sequences of patch embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert image into patch embeddings.\n",
    "    \n",
    "    The image is split into non-overlapping patches, each patch is\n",
    "    linearly projected to an embedding vector.\n",
    "    \n",
    "    Args:\n",
    "        img_size: Input image size (assumed square)\n",
    "        patch_size: Size of each patch (assumed square)\n",
    "        in_channels: Number of input channels\n",
    "        embed_dim: Dimension of patch embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_size: int, patch_size: int, in_channels: int, embed_dim: int):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # Use conv2d as efficient patch extraction + linear projection\n",
    "        self.projection = nn.Conv2d(\n",
    "            in_channels, embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input image [B, C, H, W]\n",
    "        \n",
    "        Returns:\n",
    "            Patch embeddings [B, num_patches, embed_dim]\n",
    "        \"\"\"\n",
    "        x = self.projection(x)  # [B, embed_dim, H/P, W/P]\n",
    "        x = x.flatten(2)  # [B, embed_dim, num_patches]\n",
    "        x = x.transpose(1, 2)  # [B, num_patches, embed_dim]\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test patch embedding\n",
    "patch_embed = PatchEmbedding(img_size=32, patch_size=4, in_channels=3, embed_dim=256)\n",
    "x = torch.randn(1, 3, 32, 32)\n",
    "patches = patch_embed(x)\n",
    "\n",
    "print(f\"Patch Embedding\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {patches.shape}\")\n",
    "print(f\"Number of patches: {patch_embed.num_patches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Solution: Comparing Different Patch Sizes\n",
    "\n",
    "Analyzing the trade-offs between different patch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_patch_sizes(img_size: int = 32, embed_dim: int = 256):\n",
    "    \"\"\"\n",
    "    Compare different patch sizes for ViT.\n",
    "    \n",
    "    Trade-offs:\n",
    "    - Smaller patches = more tokens = more compute but potentially better accuracy\n",
    "    - Larger patches = fewer tokens = less compute but may lose fine details\n",
    "    \n",
    "    Args:\n",
    "        img_size: Input image size\n",
    "        embed_dim: Embedding dimension\n",
    "    \"\"\"\n",
    "    patch_sizes = [2, 4, 8, 16]\n",
    "    \n",
    "    print(f\"Patch Size Comparison for {img_size}x{img_size} images\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Patch Size':<12} {'Num Patches':<15} {'Seq Length':<15} {'Params':<15} {'FLOPs Est.':<15}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for patch_size in patch_sizes:\n",
    "        if img_size % patch_size != 0:\n",
    "            continue\n",
    "        \n",
    "        patch_embed = PatchEmbedding(img_size, patch_size, 3, embed_dim)\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        seq_length = num_patches + 1  # +1 for CLS token\n",
    "        params = sum(p.numel() for p in patch_embed.parameters())\n",
    "        \n",
    "        # Rough FLOPs estimate for attention: O(seq_length^2 * embed_dim)\n",
    "        flops_attention = seq_length ** 2 * embed_dim\n",
    "        \n",
    "        print(f\"{patch_size:<12} {num_patches:<15} {seq_length:<15} {params:,}\")\n",
    "        \n",
    "        results.append({\n",
    "            'patch_size': patch_size,\n",
    "            'num_patches': num_patches,\n",
    "            'seq_length': seq_length,\n",
    "            'params': params\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Compare for CIFAR-10 sized images\n",
    "results = compare_patch_sizes(img_size=32)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Analysis:\")\n",
    "print(\"-\"*70)\n",
    "print(\"- Patch size 2: 256 patches - Very detailed but computationally expensive\")\n",
    "print(\"- Patch size 4: 64 patches - Good balance for small images like CIFAR\")\n",
    "print(\"- Patch size 8: 16 patches - Fast but may miss fine details\")\n",
    "print(\"- Patch size 16: 4 patches - Too few for meaningful attention patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare for ImageNet sized images (224x224)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "results_imagenet = compare_patch_sizes(img_size=224)\n",
    "\n",
    "print(\"\\nFor 224x224 images (ImageNet):\")\n",
    "print(\"- Patch size 14: 256 patches - Common choice (ViT-B/14)\")\n",
    "print(\"- Patch size 16: 196 patches - Standard choice (ViT-B/16)\")\n",
    "print(\"- Patch size 32: 49 patches - Fast but coarse (ViT-B/32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Solution: Simple Vision Transformer\n",
    "\n",
    "A basic ViT implementation from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Vision Transformer (ViT) implementation.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Patch Embedding\n",
    "    2. Positional Embedding (learnable)\n",
    "    3. Transformer Encoder\n",
    "    4. MLP Head\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int = 32,\n",
    "        patch_size: int = 4,\n",
    "        in_channels: int = 3,\n",
    "        num_classes: int = 10,\n",
    "        embed_dim: int = 256,\n",
    "        depth: int = 6,\n",
    "        num_heads: int = 8,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        \n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        \n",
    "        # Learnable CLS token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        # Learnable position embedding\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n",
    "        \n",
    "        self.pos_drop = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=int(embed_dim * mlp_ratio),\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True  # Pre-LayerNorm\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Classification head\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.head.weight, std=0.02)\n",
    "        nn.init.zeros_(self.head.bias)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)  # [B, num_patches, embed_dim]\n",
    "        \n",
    "        # Prepend CLS token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # [B, num_patches + 1, embed_dim]\n",
    "        \n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        x = self.encoder(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Classification from CLS token\n",
    "        cls_output = x[:, 0]\n",
    "        logits = self.head(cls_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "# Test ViT\n",
    "vit = VisionTransformer(\n",
    "    img_size=32,\n",
    "    patch_size=4,\n",
    "    num_classes=10,\n",
    "    embed_dim=256,\n",
    "    depth=6,\n",
    "    num_heads=8\n",
    ")\n",
    "\n",
    "x = torch.randn(2, 3, 32, 32)\n",
    "output = vit(x)\n",
    "\n",
    "print(f\"Vision Transformer\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in vit.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Solution: DeiT with Distillation Token\n",
    "\n",
    "Data-efficient Image Transformer (DeiT) adds a distillation token that learns from a CNN teacher.\n",
    "\n",
    "**Key insight**: The distillation token provides a second \"view\" of the classification that can learn from a CNN's inductive biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeiT(nn.Module):\n",
    "    \"\"\"\n",
    "    Data-efficient Image Transformer (DeiT).\n",
    "    \n",
    "    Adds a distillation token that learns from a CNN teacher.\n",
    "    Paper: \"Training data-efficient image transformers\" (Touvron et al., 2021)\n",
    "    \n",
    "    Key differences from ViT:\n",
    "    1. Distillation token alongside CLS token\n",
    "    2. Two classification heads (one for each token)\n",
    "    3. During inference, average both predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int = 32,\n",
    "        patch_size: int = 4,\n",
    "        in_channels: int = 3,\n",
    "        num_classes: int = 10,\n",
    "        embed_dim: int = 256,\n",
    "        depth: int = 6,\n",
    "        num_heads: int = 8,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super(DeiT, self).__init__()\n",
    "        \n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = nn.Conv2d(in_channels, embed_dim, patch_size, stride=patch_size)\n",
    "        \n",
    "        # CLS token and DISTILLATION token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim))  # New!\n",
    "        \n",
    "        # Position embedding for patches + cls + dist tokens\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 2, embed_dim))\n",
    "        \n",
    "        self.pos_drop = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=int(embed_dim * mlp_ratio),\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Two classification heads\n",
    "        self.head = nn.Linear(embed_dim, num_classes)  # For CLS token\n",
    "        self.dist_head = nn.Linear(embed_dim, num_classes)  # For distillation token\n",
    "        \n",
    "        # Initialize\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.dist_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.head.weight, std=0.02)\n",
    "        nn.init.trunc_normal_(self.dist_head.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x).flatten(2).transpose(1, 2)  # [B, N, D]\n",
    "        \n",
    "        # Prepend CLS and DIST tokens\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        dist_tokens = self.dist_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, dist_tokens, x], dim=1)  # [B, N+2, D]\n",
    "        \n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        # Transformer\n",
    "        x = self.encoder(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Two outputs\n",
    "        cls_output = self.head(x[:, 0])  # From CLS token\n",
    "        dist_output = self.dist_head(x[:, 1])  # From DIST token\n",
    "        \n",
    "        # During inference, average both predictions\n",
    "        if not self.training:\n",
    "            return (cls_output + dist_output) / 2\n",
    "        \n",
    "        return cls_output, dist_output\n",
    "\n",
    "\n",
    "# Test DeiT\n",
    "deit = DeiT(img_size=32, patch_size=4, num_classes=10)\n",
    "x = torch.randn(2, 3, 32, 32)\n",
    "\n",
    "# Training mode (returns two outputs)\n",
    "deit.train()\n",
    "cls_out, dist_out = deit(x)\n",
    "print(f\"DeiT (Training Mode)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"CLS output shape: {cls_out.shape}\")\n",
    "print(f\"DIST output shape: {dist_out.shape}\")\n",
    "\n",
    "# Eval mode (returns averaged output)\n",
    "deit.eval()\n",
    "out = deit(x)\n",
    "print(f\"\\nDeiT (Inference Mode)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Combined output shape: {out.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in deit.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Solution: Distillation Training\n",
    "\n",
    "How to train DeiT with knowledge distillation from a CNN teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Distillation loss for DeiT training.\n",
    "    \n",
    "    Combines:\n",
    "    1. Hard target loss (cross-entropy with true labels)\n",
    "    2. Soft target loss (KL divergence with teacher outputs)\n",
    "    \n",
    "    The distillation token learns from the teacher,\n",
    "    while the CLS token learns from the ground truth.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        teacher_model: nn.Module,\n",
    "        temperature: float = 3.0,\n",
    "        alpha: float = 0.5\n",
    "    ):\n",
    "        super(DistillationLoss, self).__init__()\n",
    "        self.teacher = teacher_model\n",
    "        self.teacher.eval()\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.kl = nn.KLDivLoss(reduction='batchmean')\n",
    "    \n",
    "    def forward(self, student_cls: torch.Tensor, student_dist: torch.Tensor, \n",
    "                images: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            student_cls: CLS token output from student\n",
    "            student_dist: DIST token output from student\n",
    "            images: Input images (for teacher inference)\n",
    "            labels: Ground truth labels\n",
    "        \"\"\"\n",
    "        # Get teacher predictions\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = self.teacher(images)\n",
    "        \n",
    "        # Hard loss: CLS token vs ground truth\n",
    "        hard_loss = self.ce(student_cls, labels)\n",
    "        \n",
    "        # Soft loss: DIST token vs teacher (with temperature)\n",
    "        soft_student = F.log_softmax(student_dist / self.temperature, dim=-1)\n",
    "        soft_teacher = F.softmax(teacher_logits / self.temperature, dim=-1)\n",
    "        soft_loss = self.kl(soft_student, soft_teacher) * (self.temperature ** 2)\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = self.alpha * hard_loss + (1 - self.alpha) * soft_loss\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "\n",
    "print(\"DistillationLoss defined!\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  teacher = load_pretrained_resnet()  # CNN teacher\")\n",
    "print(\"  criterion = DistillationLoss(teacher, temperature=3.0, alpha=0.5)\")\n",
    "print(\"  \")\n",
    "print(\"  # In training loop:\")\n",
    "print(\"  cls_out, dist_out = deit(images)\")\n",
    "print(\"  loss = criterion(cls_out, dist_out, images, labels)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Solution: Position Embedding Visualization\n",
    "\n",
    "Visualizing what position embeddings learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_position_embeddings(pos_embed: torch.Tensor, grid_size: int = 8):\n",
    "    \"\"\"\n",
    "    Visualize position embedding similarities.\n",
    "    \n",
    "    Shows how similar each position's embedding is to every other position.\n",
    "    Ideally, nearby positions should have similar embeddings.\n",
    "    \"\"\"\n",
    "    # Remove CLS token embedding (first position)\n",
    "    patch_pos_embed = pos_embed[0, 1:, :].detach()  # [num_patches, embed_dim]\n",
    "    \n",
    "    # Compute cosine similarity matrix\n",
    "    patch_pos_embed = F.normalize(patch_pos_embed, dim=-1)\n",
    "    similarity = torch.mm(patch_pos_embed, patch_pos_embed.T)  # [N, N]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Full similarity matrix\n",
    "    ax = axes[0]\n",
    "    im = ax.imshow(similarity.cpu().numpy(), cmap='viridis')\n",
    "    ax.set_title('Position Embedding Similarity Matrix')\n",
    "    ax.set_xlabel('Position')\n",
    "    ax.set_ylabel('Position')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Similarity from center patch\n",
    "    center_idx = (grid_size * grid_size) // 2 + grid_size // 2\n",
    "    center_sim = similarity[center_idx].cpu().numpy().reshape(grid_size, grid_size)\n",
    "    \n",
    "    ax = axes[1]\n",
    "    im = ax.imshow(center_sim, cmap='viridis')\n",
    "    ax.set_title(f'Similarity to Center Patch (idx={center_idx})')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Similarity from corner patch\n",
    "    corner_sim = similarity[0].cpu().numpy().reshape(grid_size, grid_size)\n",
    "    \n",
    "    ax = axes[2]\n",
    "    im = ax.imshow(corner_sim, cmap='viridis')\n",
    "    ax.set_title('Similarity to Top-Left Corner (idx=0)')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create a ViT and visualize its initial position embeddings\n",
    "vit = VisionTransformer(img_size=32, patch_size=4, embed_dim=256)\n",
    "print(\"Visualizing initial (random) position embeddings:\")\n",
    "print(\"(After training, nearby positions should show higher similarity)\")\n",
    "visualize_position_embeddings(vit.pos_embed, grid_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key concepts covered:\n",
    "\n",
    "1. **Patch Embedding**: Converting images to sequences via non-overlapping patches\n",
    "2. **Patch Size Trade-offs**: Smaller patches = more detail but more compute\n",
    "3. **Vision Transformer**: Complete ViT implementation with:\n",
    "   - CLS token for classification\n",
    "   - Learnable position embeddings\n",
    "   - Transformer encoder\n",
    "4. **DeiT**: Data-efficient ViT with distillation token\n",
    "5. **Position Embeddings**: Learnable spatial information for patches\n",
    "\n",
    "Recommended patch sizes:\n",
    "- CIFAR (32x32): patch_size=4 (64 patches)\n",
    "- ImageNet (224x224): patch_size=16 (196 patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}