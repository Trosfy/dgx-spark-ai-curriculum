{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7.2 Solution: Transfer Learning Project\n",
    "\n",
    "**Module:** 7 - Computer Vision  \n",
    "**Type:** Solution Notebook\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains solutions for achieving >90% accuracy on CIFAR-100 using transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Achieving >90% Accuracy on CIFAR-100\n",
    "\n",
    "Key strategies:\n",
    "1. Use a larger pretrained model (EfficientNet-B3 or ConvNeXt)\n",
    "2. Train on full dataset (not subset)\n",
    "3. Longer training (50+ epochs)\n",
    "4. Advanced techniques: Mixup, Label Smoothing, Gradual Unfreezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for >90% accuracy on CIFAR-100\n",
    "training_config = {\n",
    "    'model': 'efficientnet_b3',  # Larger model for better capacity\n",
    "    'dataset_subset': None,  # Full dataset (50,000 images)\n",
    "    'epochs': 50,\n",
    "    'batch_size': 32,\n",
    "    'base_lr': 1e-4,\n",
    "    'weight_decay': 0.1,\n",
    "    'warmup_epochs': 5,\n",
    "    'label_smoothing': 0.1,\n",
    "    'mixup_alpha': 0.2,\n",
    "    'strategy': 'gradual_unfreeze',\n",
    "}\n",
    "\n",
    "print(\"Configuration for >90% accuracy on CIFAR-100:\")\n",
    "print(\"=\"*50)\n",
    "for key, value in training_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixup Augmentation Implementation\n",
    "\n",
    "Mixup creates virtual training examples by combining pairs of images and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x: torch.Tensor, y: torch.Tensor, alpha: float = 0.2) -> Tuple:\n",
    "    \"\"\"\n",
    "    Apply Mixup augmentation.\n",
    "    \n",
    "    Mixup creates virtual training examples by:\n",
    "    x_new = lambda * x_i + (1 - lambda) * x_j\n",
    "    y_new = lambda * y_i + (1 - lambda) * y_j\n",
    "    \n",
    "    Args:\n",
    "        x: Input images [B, C, H, W]\n",
    "        y: Labels [B]\n",
    "        alpha: Mixup interpolation strength\n",
    "    \n",
    "    Returns:\n",
    "        mixed_x: Mixed images\n",
    "        y_a, y_b: Original labels for loss computation\n",
    "        lam: Mixing coefficient\n",
    "    \"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion: nn.Module, pred: torch.Tensor, \n",
    "                    y_a: torch.Tensor, y_b: torch.Tensor, lam: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute loss for Mixup.\n",
    "    \n",
    "    The loss is a weighted combination of losses for both labels.\n",
    "    \"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "\n",
    "# Demonstration\n",
    "print(\"Mixup Augmentation Demo\")\n",
    "print(\"=\"*50)\n",
    "x = torch.randn(4, 3, 32, 32)\n",
    "y = torch.tensor([0, 1, 2, 3])\n",
    "\n",
    "mixed_x, y_a, y_b, lam = mixup_data(x, y, alpha=0.2)\n",
    "print(f\"Original labels: {y.tolist()}\")\n",
    "print(f\"y_a (first set): {y_a.tolist()}\")\n",
    "print(f\"y_b (shuffled): {y_b.tolist()}\")\n",
    "print(f\"Lambda (mixing coefficient): {lam:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Smoothing Implementation\n",
    "\n",
    "Label smoothing prevents overconfidence by softening the target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-entropy loss with label smoothing.\n",
    "    \n",
    "    Instead of hard labels [0, 0, 1, 0], uses soft labels:\n",
    "    [eps/K, eps/K, 1-eps+eps/K, eps/K]\n",
    "    \n",
    "    This prevents overconfidence and improves generalization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, smoothing: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.confidence = 1.0 - smoothing\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        num_classes = pred.size(-1)\n",
    "        \n",
    "        # Log softmax for numerical stability\n",
    "        log_probs = torch.log_softmax(pred, dim=-1)\n",
    "        \n",
    "        # Create smooth labels\n",
    "        smooth_labels = torch.full_like(log_probs, self.smoothing / num_classes)\n",
    "        smooth_labels.scatter_(1, target.unsqueeze(1), self.confidence)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = -torch.sum(smooth_labels * log_probs, dim=-1)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "# Demonstration\n",
    "criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
    "pred = torch.randn(4, 100)  # 100 classes for CIFAR-100\n",
    "target = torch.tensor([0, 25, 50, 75])\n",
    "\n",
    "loss = criterion(pred, target)\n",
    "print(f\"Label Smoothing Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradual Unfreezing Strategy\n",
    "\n",
    "Gradually unfreeze layers from top to bottom during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradual_unfreeze(model: nn.Module, epoch: int, total_epochs: int, num_stages: int = 4):\n",
    "    \"\"\"\n",
    "    Gradually unfreeze model layers during training.\n",
    "    \n",
    "    Starts with only classifier trainable, then progressively unfreezes\n",
    "    deeper layers as training progresses.\n",
    "    \n",
    "    Args:\n",
    "        model: The model with 'features' and 'classifier' attributes\n",
    "        epoch: Current epoch\n",
    "        total_epochs: Total training epochs\n",
    "        num_stages: Number of unfreezing stages\n",
    "    \"\"\"\n",
    "    # Get all layers\n",
    "    if hasattr(model, 'features'):\n",
    "        layers = list(model.features.children())\n",
    "    else:\n",
    "        layers = list(model.children())[:-1]  # Exclude classifier\n",
    "    \n",
    "    epochs_per_stage = total_epochs // num_stages\n",
    "    current_stage = min(epoch // epochs_per_stage, num_stages - 1)\n",
    "    \n",
    "    # Calculate how many layers to unfreeze\n",
    "    layers_to_unfreeze = (current_stage + 1) * (len(layers) // num_stages)\n",
    "    \n",
    "    # Freeze all layers first\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Unfreeze classifier (always trainable)\n",
    "    if hasattr(model, 'classifier'):\n",
    "        for param in model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "    elif hasattr(model, 'fc'):\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    # Unfreeze top N layers\n",
    "    for layer in layers[-layers_to_unfreeze:]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"Stage {current_stage + 1}/{num_stages}: {trainable:,}/{total:,} params trainable ({100*trainable/total:.1f}%)\")\n",
    "\n",
    "\n",
    "# Demonstration\n",
    "print(\"Gradual Unfreezing Strategy\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nThis strategy trains classifier first, then progressively\")\n",
    "print(\"unfreezes deeper layers for fine-grained adaptation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_with_mixup(\n",
    "    model: nn.Module,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device: torch.device,\n",
    "    mixup_alpha: float = 0.2\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Train one epoch with Mixup augmentation.\n",
    "    \n",
    "    Returns:\n",
    "        Average loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Apply Mixup\n",
    "        mixed_images, y_a, y_b, lam = mixup_data(images, labels, mixup_alpha)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(mixed_images)\n",
    "        \n",
    "        # Mixup loss\n",
    "        loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "print(\"Training functions defined!\")\n",
    "print(\"\\nTo achieve >90% on CIFAR-100:\")\n",
    "print(\"1. Use EfficientNet-B3 or ConvNeXt-Tiny as backbone\")\n",
    "print(\"2. Train for 50+ epochs with gradual unfreezing\")\n",
    "print(\"3. Apply Mixup (alpha=0.2) and Label Smoothing (0.1)\")\n",
    "print(\"4. Use cosine annealing LR schedule with warmup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key techniques for achieving >90% on CIFAR-100:\n",
    "\n",
    "1. **Larger Model**: EfficientNet-B3 has more capacity than ResNet-18\n",
    "2. **Mixup**: Creates virtual training samples, reduces overfitting\n",
    "3. **Label Smoothing**: Prevents overconfident predictions\n",
    "4. **Gradual Unfreezing**: Starts from pretrained weights, adapts carefully\n",
    "5. **Longer Training**: 50+ epochs with proper LR schedule\n",
    "\n",
    "Expected results:\n",
    "- ResNet-18 baseline: ~75-80%\n",
    "- With techniques above: ~90-92%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
