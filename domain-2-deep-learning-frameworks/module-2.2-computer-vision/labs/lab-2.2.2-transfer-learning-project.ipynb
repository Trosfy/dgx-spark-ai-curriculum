{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.2.2: Transfer Learning Project\n",
    "\n",
    "**Module:** 2.2 - Computer Vision  \n",
    "**Time:** 3 hours  \n",
    "**Difficulty:** â­â­â­\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand when and why to use transfer learning\n",
    "- [ ] Compare feature extraction vs fine-tuning approaches\n",
    "- [ ] Fine-tune EfficientNet to achieve >90% accuracy on custom data\n",
    "- [ ] Implement proper learning rate strategies for transfer learning\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Prerequisites\n",
    "\n",
    "- Completed: Lab 2.2.1 (CNN Architecture Study)\n",
    "- Knowledge of: PyTorch training loops, data augmentation\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ Real-World Context\n",
    "\n",
    "**Transfer learning is how most real AI systems are built:**\n",
    "\n",
    "- ğŸ¥ **Medical imaging**: Start with ImageNet model, fine-tune on X-rays\n",
    "- ğŸ›ï¸ **E-commerce**: Classify products using pre-trained features\n",
    "- ğŸŒ¾ **Agriculture**: Detect crop diseases with minimal training data\n",
    "- ğŸ­ **Manufacturing**: Quality control with few defect examples\n",
    "\n",
    "**Why?** Training from scratch requires millions of images. Transfer learning needs just hundreds!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§’ ELI5: What is Transfer Learning?\n",
    "\n",
    "> **Imagine you already know how to ride a bicycle...** ğŸš²\n",
    ">\n",
    "> When you learn to ride a motorcycle, do you start from zero? No! You already know:\n",
    "> - How to balance\n",
    "> - How to steer\n",
    "> - How to watch the road\n",
    ">\n",
    "> You just need to learn the new parts (throttle, clutch, extra weight).\n",
    ">\n",
    "> **Transfer learning is the same for neural networks:**\n",
    "> - A model trained on ImageNet already knows edges, textures, shapes, objects\n",
    "> - To classify YOUR images, you just teach it what YOUR specific categories look like\n",
    "> - This is way faster than teaching it everything from scratch!\n",
    ">\n",
    "> **In AI terms:** Pre-trained networks have learned universal visual features. We leverage these features and only train the task-specific layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding Transfer Learning Strategies\n",
    "\n",
    "### Two Main Approaches\n",
    "\n",
    "```\n",
    "Feature Extraction:                    Fine-Tuning:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Pre-trained     â”‚ â„ï¸ FROZEN        â”‚  Pre-trained     â”‚ ğŸ”¥ TRAINABLE\n",
    "â”‚  Backbone        â”‚ (no training)    â”‚  Backbone        â”‚ (small LR)\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  New Classifier  â”‚ ğŸ”¥ TRAINABLE     â”‚  New Classifier  â”‚ ğŸ”¥ TRAINABLE\n",
    "â”‚  Head            â”‚                  â”‚  Head            â”‚ (larger LR)\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "When to use:                          When to use:\n",
    "- Small dataset (<1000 images)        - Medium dataset (1000+)\n",
    "- Similar to ImageNet                 - Different from ImageNet\n",
    "- Limited compute                     - Better accuracy needed\n",
    "```\n",
    "\n",
    "### The Transfer Learning Spectrum\n",
    "\n",
    "```\n",
    "Your data is like ImageNet    â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’    Your data is very different\n",
    "        â”‚                                                              â”‚\n",
    "        â–¼                                                              â–¼\n",
    "  Feature Extraction                                              Full Fine-tuning\n",
    "  (freeze everything)                                            (train everything)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict, List, Optional\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# DGX Spark optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸ–¥ï¸  Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ® GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The CIFAR-100 Dataset\n",
    "\n",
    "We'll use CIFAR-100, which has 100 classes - a more challenging task than CIFAR-10. This simulates real-world scenarios where you have many categories and limited training data per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar100_loaders(\n",
    "    batch_size: int = 64,\n",
    "    num_workers: int = 4,\n",
    "    subset_size: Optional[int] = None\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Create CIFAR-100 data loaders with ImageNet-style preprocessing.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Batch size for training\n",
    "        num_workers: Number of data loading workers\n",
    "        subset_size: If specified, use only this many training samples\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_loader, test_loader)\n",
    "    \"\"\"\n",
    "    # ImageNet normalization (required for pre-trained models!)\n",
    "    imagenet_mean = [0.485, 0.456, 0.406]\n",
    "    imagenet_std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    # Training transforms with augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(256),                          # Upscale for better features\n",
    "        transforms.RandomResizedCrop(224),               # Standard ImageNet input size\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),     # Color augmentation\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "    ])\n",
    "    \n",
    "    # Validation transforms (no augmentation)\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "    ])\n",
    "    \n",
    "    # Load datasets\n",
    "    train_dataset = torchvision.datasets.CIFAR100(\n",
    "        root='../data', train=True, download=True, transform=train_transform\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.CIFAR100(\n",
    "        root='../data', train=False, download=True, transform=val_transform\n",
    "    )\n",
    "    \n",
    "    # Optionally use subset for faster experiments\n",
    "    if subset_size:\n",
    "        indices = torch.randperm(len(train_dataset))[:subset_size]\n",
    "        train_dataset = Subset(train_dataset, indices)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size,\n",
    "        shuffle=True, num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size,\n",
    "        shuffle=False, num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Load data (using subset for demonstration)\n",
    "train_loader, test_loader = get_cifar100_loaders(batch_size=64, subset_size=10000)\n",
    "\n",
    "print(f\"ğŸ“Š CIFAR-100 Dataset:\")\n",
    "print(f\"   Training samples: {len(train_loader.dataset):,}\")\n",
    "print(f\"   Test samples:     {len(test_loader.dataset):,}\")\n",
    "print(f\"   Classes:          100\")\n",
    "print(f\"   Batch size:       64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some samples\n",
    "def show_batch(loader, classes_map=None):\n",
    "    \"\"\"Display a batch of images with labels.\"\"\"\n",
    "    images, labels = next(iter(loader))\n",
    "    \n",
    "    # Denormalize for visualization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    images_vis = images[:16] * std + mean\n",
    "    images_vis = images_vis.clamp(0, 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "    for idx, ax in enumerate(axes.flat):\n",
    "        img = images_vis[idx].permute(1, 2, 0).numpy()\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f'Class: {labels[idx].item()}', fontsize=10)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('ğŸ–¼ï¸ CIFAR-100 Sample Batch (Augmented)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_batch(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Exploring Pre-trained Models\n",
    "\n",
    "The `torchvision.models` and `timm` libraries provide many pre-trained models. Let's explore what's available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install timm if not available\n# Note: For DGX Spark, timm is recommended to be installed in your NGC container\n# See README.md for Quick Start instructions\n\ntry:\n    import timm\n    print(\"âœ… timm library available\")\nexcept ImportError:\n    print(\"âš ï¸ timm not found. Installing...\")\n    print(\"   ğŸ’¡ TIP: For DGX Spark ARM64, pre-install in NGC container for best results:\")\n    print(\"      pip install timm\")\n    !pip install timm -q\n    import timm\n    print(\"âœ… timm installed successfully!\")\n\n# List popular models\nprint(\"\\nğŸ“š Popular Pre-trained Models for Transfer Learning:\")\nprint(\"=\"*60)\n\npopular_models = {\n    'ResNet-50': ('resnet50', 25.6),\n    'EfficientNet-B0': ('efficientnet_b0', 5.3),\n    'EfficientNet-B3': ('efficientnet_b3', 12.2),\n    'ConvNeXt-Tiny': ('convnext_tiny', 28.6),\n    'ViT-B/16': ('vit_base_patch16_224', 86.6),\n}\n\nprint(f\"{'Model':<20} {'Parameters':>15} {'ImageNet Top-1':>15}\")\nprint(\"-\"*60)\n\n# Top-1 accuracies (approximate)\naccuracies = {\n    'ResNet-50': 76.1,\n    'EfficientNet-B0': 77.1,\n    'EfficientNet-B3': 81.6,\n    'ConvNeXt-Tiny': 82.1,\n    'ViT-B/16': 81.8,\n}\n\nfor name, (model_name, params) in popular_models.items():\n    acc = accuracies.get(name, 'N/A')\n    print(f\"{name:<20} {params:>12.1f}M {acc:>14.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§’ ELI5: Why EfficientNet?\n",
    "\n",
    "> **Imagine you're building a LEGO tower...**\n",
    ">\n",
    "> - **ResNet**: Makes the tower taller (more layers)\n",
    "> - **Wide ResNet**: Makes each floor bigger (more channels)\n",
    "> - **EfficientNet**: Carefully balances height, width, AND image resolution\n",
    ">\n",
    "> The EfficientNet authors used neural architecture search to find the optimal balance. Result: **Better accuracy with fewer parameters!**\n",
    "\n",
    "### EfficientNet Scaling\n",
    "\n",
    "```\n",
    "EfficientNet-B0 (baseline):  224Ã—224 input,  5.3M params\n",
    "EfficientNet-B1:             240Ã—240 input,  7.8M params\n",
    "EfficientNet-B2:             260Ã—260 input,  9.2M params  \n",
    "EfficientNet-B3:             300Ã—300 input, 12.2M params  â† Sweet spot!\n",
    "EfficientNet-B4:             380Ã—380 input, 19.3M params\n",
    "...\n",
    "EfficientNet-B7:             600Ã—600 input, 66.3M params\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Feature Extraction (Frozen Backbone)\n",
    "\n",
    "First, let's try the simplest approach: freeze the pre-trained backbone and only train a new classifier head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_extractor(num_classes: int = 100) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Create EfficientNet-B0 with frozen backbone for feature extraction.\n",
    "    \n",
    "    Args:\n",
    "        num_classes: Number of output classes\n",
    "    \n",
    "    Returns:\n",
    "        Model with frozen backbone and new classifier\n",
    "    \"\"\"\n",
    "    # Load pre-trained EfficientNet-B0\n",
    "    model = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "    \n",
    "    # Freeze ALL backbone parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Replace classifier head (this stays trainable)\n",
    "    num_features = model.classifier.in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(num_features, num_classes)\n",
    "    )\n",
    "    \n",
    "    # Count trainable parameters\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"ğŸ“Š Feature Extractor Created:\")\n",
    "    print(f\"   Total parameters:     {total:,}\")\n",
    "    print(f\"   Trainable parameters: {trainable:,}\")\n",
    "    print(f\"   Frozen parameters:    {total - trainable:,}\")\n",
    "    print(f\"   Trainable ratio:      {100*trainable/total:.2f}%\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "feature_extractor = create_feature_extractor(num_classes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0, 0, 0\n",
    "    \n",
    "    for inputs, targets in tqdm(loader, desc='Training', leave=False):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return running_loss / total, 100 * correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate model accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(loader, desc='Evaluating', leave=False):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return running_loss / total, 100 * correct / total\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    epochs: int = 10,\n",
    "    lr: float = 0.001,\n",
    "    device: torch.device = device\n",
    ") -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Train a model with standard settings.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of training history\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Only optimize trainable parameters\n",
    "    optimizer = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=lr, weight_decay=0.01\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"   Train: Loss={train_loss:.4f}, Acc={train_acc:.1f}%\")\n",
    "        print(f\"   Val:   Loss={val_loss:.4f}, Acc={val_acc:.1f}%\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train feature extractor (fast, but limited accuracy)\n",
    "print(\"ğŸ§Š Training Feature Extractor (Frozen Backbone)...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "feature_extractor = create_feature_extractor(num_classes=100)\n",
    "start_time = time.time()\n",
    "\n",
    "fe_history = train_model(\n",
    "    feature_extractor, \n",
    "    train_loader, \n",
    "    test_loader,\n",
    "    epochs=5,  # Quick training since backbone is frozen\n",
    "    lr=0.01    # Can use higher LR since only training head\n",
    ")\n",
    "\n",
    "fe_time = time.time() - start_time\n",
    "print(f\"\\nâœ… Feature extraction complete in {fe_time:.1f}s\")\n",
    "print(f\"   Final accuracy: {fe_history['val_acc'][-1]:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Full Fine-Tuning\n",
    "\n",
    "Now let's train the entire network, including the backbone. This requires careful learning rate management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_finetune_model(num_classes: int = 100) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Create EfficientNet-B0 for full fine-tuning.\n",
    "    \n",
    "    All parameters are trainable, but we'll use different learning rates.\n",
    "    \"\"\"\n",
    "    # Load pre-trained model\n",
    "    model = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "    \n",
    "    # Replace classifier\n",
    "    num_features = model.classifier.in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(num_features, num_classes)\n",
    "    )\n",
    "    \n",
    "    # ALL parameters are trainable\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"ğŸ“Š Fine-tuning Model Created:\")\n",
    "    print(f\"   Total/Trainable parameters: {total:,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "finetune_model = create_finetune_model(num_classes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§’ ELI5: Differential Learning Rates\n",
    "\n",
    "> **Imagine you're renovating a house...** ğŸ \n",
    ">\n",
    "> - The **foundation** (early layers): Already solid! Only minor adjustments needed.\n",
    "> - The **walls** (middle layers): Might need some changes to fit your style.\n",
    "> - The **furniture** (classifier head): Brand new! Built specifically for you.\n",
    ">\n",
    "> Similarly, we use **smaller learning rates for early layers** (small adjustments to solid foundation) and **larger learning rates for the classifier** (learning from scratch).\n",
    "\n",
    "```\n",
    "Learning Rate Strategy:\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Classifier     â”‚  LR = 0.001   (large, learning new task)\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Late Layers    â”‚  LR = 0.0001  (medium, adapt features)\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Early Layers   â”‚  LR = 0.00001 (tiny, preserve general features)\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_discriminative_lr(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    epochs: int = 10,\n",
    "    base_lr: float = 0.0001,\n",
    "    device: torch.device = device\n",
    ") -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Train with discriminative (layer-wise) learning rates.\n",
    "    \n",
    "    Backbone gets smaller LR, classifier gets larger LR.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Create parameter groups with different LRs\n",
    "    # EfficientNet structure: conv_stem -> blocks -> conv_head -> classifier\n",
    "    \n",
    "    # Separate backbone and classifier parameters\n",
    "    backbone_params = []\n",
    "    classifier_params = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'classifier' in name:\n",
    "            classifier_params.append(param)\n",
    "        else:\n",
    "            backbone_params.append(param)\n",
    "    \n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': backbone_params, 'lr': base_lr},\n",
    "        {'params': classifier_params, 'lr': base_lr * 10}  # 10Ã— for classifier\n",
    "    ], weight_decay=0.01)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        current_lrs = [group['lr'] for group in optimizer.param_groups]\n",
    "        print(f\"   Train: Loss={train_loss:.4f}, Acc={train_acc:.1f}%\")\n",
    "        print(f\"   Val:   Loss={val_loss:.4f}, Acc={val_acc:.1f}%\")\n",
    "        print(f\"   LRs:   backbone={current_lrs[0]:.6f}, classifier={current_lrs[1]:.6f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with full fine-tuning\n",
    "print(\"ğŸ”¥ Training Full Fine-Tuning (Discriminative LR)...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "finetune_model = create_finetune_model(num_classes=100)\n",
    "start_time = time.time()\n",
    "\n",
    "ft_history = train_with_discriminative_lr(\n",
    "    finetune_model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=10,\n",
    "    base_lr=0.0001\n",
    ")\n",
    "\n",
    "ft_time = time.time() - start_time\n",
    "print(f\"\\nâœ… Fine-tuning complete in {ft_time:.1f}s\")\n",
    "print(f\"   Final accuracy: {ft_history['val_acc'][-1]:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Gradual Unfreezing Strategy\n",
    "\n",
    "A more sophisticated approach: start with frozen backbone, then progressively unfreeze layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_gradual_unfreeze(\n",
    "    num_classes: int = 100,\n",
    "    train_loader: DataLoader = train_loader,\n",
    "    val_loader: DataLoader = test_loader,\n",
    "    device: torch.device = device\n",
    ") -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Train using gradual unfreezing:\n",
    "    1. Train only classifier (2 epochs)\n",
    "    2. Unfreeze last 2 blocks + classifier (3 epochs)\n",
    "    3. Unfreeze all (5 epochs)\n",
    "    \"\"\"\n",
    "    # Create model with frozen backbone\n",
    "    model = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "    \n",
    "    # Replace classifier\n",
    "    num_features = model.classifier.in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(num_features, num_classes)\n",
    "    )\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'phase': []}\n",
    "    \n",
    "    # ========== Phase 1: Train classifier only ==========\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ§Š Phase 1: Training classifier only (backbone frozen)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Freeze everything\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Unfreeze classifier\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    optimizer = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=0.01\n",
    "    )\n",
    "    \n",
    "    for epoch in range(2):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['phase'].append('classifier')\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Acc={train_acc:.1f}%, Val Acc={val_acc:.1f}%\")\n",
    "    \n",
    "    # ========== Phase 2: Unfreeze last 2 blocks ==========\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸŒ¡ï¸ Phase 2: Unfreezing last 2 blocks\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # EfficientNet-B0 has blocks named 'blocks.X'\n",
    "    # Unfreeze blocks 5 and 6 (last two)\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'blocks.5' in name or 'blocks.6' in name or 'conv_head' in name:\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    # New optimizer with lower LR for unfrozen layers\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': [p for n, p in model.named_parameters() \n",
    "                    if ('blocks.5' in n or 'blocks.6' in n or 'conv_head' in n) and p.requires_grad], \n",
    "         'lr': 0.0001},\n",
    "        {'params': model.classifier.parameters(), 'lr': 0.001}\n",
    "    ])\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['phase'].append('partial')\n",
    "        \n",
    "        print(f\"Epoch {epoch+3}: Train Acc={train_acc:.1f}%, Val Acc={val_acc:.1f}%\")\n",
    "    \n",
    "    # ========== Phase 3: Unfreeze all ==========\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ”¥ Phase 3: Full fine-tuning\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Unfreeze everything\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # Very small LR for early layers\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.00001)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['phase'].append('full')\n",
    "        \n",
    "        print(f\"Epoch {epoch+6}: Train Acc={train_acc:.1f}%, Val Acc={val_acc:.1f}%\")\n",
    "    \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with gradual unfreezing\n",
    "print(\"ğŸ“ Training with Gradual Unfreezing Strategy...\")\n",
    "start_time = time.time()\n",
    "\n",
    "gu_history, best_model = train_with_gradual_unfreeze()\n",
    "\n",
    "gu_time = time.time() - start_time\n",
    "print(f\"\\nâœ… Gradual unfreezing complete in {gu_time:.1f}s\")\n",
    "print(f\"   Final accuracy: {gu_history['val_acc'][-1]:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Compare All Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Validation accuracy\n",
    "axes[0].plot(fe_history['val_acc'], label='Feature Extraction', linewidth=2, marker='o')\n",
    "axes[0].plot(ft_history['val_acc'], label='Full Fine-tuning', linewidth=2, marker='s')\n",
    "axes[0].plot(gu_history['val_acc'], label='Gradual Unfreezing', linewidth=2, marker='^')\n",
    "\n",
    "# Mark phase transitions for gradual unfreezing\n",
    "phases = gu_history['phase']\n",
    "for i, phase in enumerate(phases):\n",
    "    if i > 0 and phases[i] != phases[i-1]:\n",
    "        axes[0].axvline(x=i, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Validation Accuracy (%)')\n",
    "axes[0].set_title('ğŸ“ˆ Validation Accuracy Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Training loss\n",
    "axes[1].plot(fe_history['train_loss'], label='Feature Extraction', linewidth=2)\n",
    "axes[1].plot(ft_history['train_loss'], label='Full Fine-tuning', linewidth=2)\n",
    "axes[1].plot(gu_history['train_loss'], label='Gradual Unfreezing', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Training Loss')\n",
    "axes[1].set_title('ğŸ“‰ Training Loss Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š TRANSFER LEARNING COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Approach':<25} {'Final Acc':>12} {'Best Acc':>12} {'Time':>10}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Feature Extraction':<25} {fe_history['val_acc'][-1]:>11.1f}% {max(fe_history['val_acc']):>11.1f}% {fe_time:>9.1f}s\")\n",
    "print(f\"{'Full Fine-tuning':<25} {ft_history['val_acc'][-1]:>11.1f}% {max(ft_history['val_acc']):>11.1f}% {ft_time:>9.1f}s\")\n",
    "print(f\"{'Gradual Unfreezing':<25} {gu_history['val_acc'][-1]:>11.1f}% {max(gu_history['val_acc']):>11.1f}% {gu_time:>9.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ‹ Try It Yourself\n",
    "\n",
    "Experiment with different configurations:\n",
    "\n",
    "1. **Try a larger model**: Replace `efficientnet_b0` with `efficientnet_b3`\n",
    "2. **Adjust augmentation**: Add/remove transforms and see the effect\n",
    "3. **Use the full dataset**: Remove the `subset_size` parameter\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Hint for achieving >90% accuracy</summary>\n",
    "\n",
    "To hit 90%+ on CIFAR-100:\n",
    "- Use the full 50,000 training images\n",
    "- Try EfficientNet-B3 or ConvNeXt\n",
    "- Train for 20+ epochs with gradual unfreezing\n",
    "- Add stronger augmentation (Mixup, CutMix)\n",
    "- Use label smoothing: `nn.CrossEntropyLoss(label_smoothing=0.1)`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš ï¸ Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting to normalize inputs\n",
    "\n",
    "```python\n",
    "# âŒ Wrong: No normalization\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# âœ… Right: Use ImageNet normalization for pre-trained models\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "```\n",
    "**Why:** Pre-trained models expect inputs normalized to ImageNet statistics.\n",
    "\n",
    "### Mistake 2: Same learning rate for all layers\n",
    "\n",
    "```python\n",
    "# âŒ Wrong: Same high LR for everything\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# âœ… Right: Lower LR for pretrained layers\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.backbone.parameters(), 'lr': 1e-5},\n",
    "    {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
    "])\n",
    "```\n",
    "**Why:** High LR destroys pretrained weights. Lower LR preserves good features.\n",
    "\n",
    "### Mistake 3: Not replacing the classifier head\n",
    "\n",
    "```python\n",
    "# âŒ Wrong: Training on 1000-class head\n",
    "model = models.resnet50(pretrained=True)  # 1000 classes!\n",
    "\n",
    "# âœ… Right: Replace with correct number of classes\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)  # Your classes\n",
    "```\n",
    "**Why:** ImageNet has 1000 classes. You need to match your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- âœ… When to use feature extraction vs fine-tuning\n",
    "- âœ… How to load and modify pre-trained models\n",
    "- âœ… Discriminative learning rates for transfer learning\n",
    "- âœ… Gradual unfreezing strategy for best results\n",
    "- âœ… Proper normalization for pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ Challenge (Optional)\n",
    "\n",
    "**Beat 90% accuracy on CIFAR-100!**\n",
    "\n",
    "Techniques to try:\n",
    "1. **Mixup/CutMix augmentation**\n",
    "2. **Label smoothing**\n",
    "3. **Larger models** (EfficientNet-B4 or ConvNeXt-Small)\n",
    "4. **Longer training** with warmup\n",
    "5. **Test-time augmentation** (TTA)\n",
    "\n",
    "Record your best accuracy and the techniques used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE\n",
    "# Goal: >90% accuracy on CIFAR-100\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“– Further Reading\n",
    "\n",
    "- [timm Library Documentation](https://huggingface.co/docs/timm/index)\n",
    "- [EfficientNet Paper](https://arxiv.org/abs/1905.11946)\n",
    "- [How to Train Your ViT](https://arxiv.org/abs/2106.10270)\n",
    "- [Transfer Learning in Computer Vision](https://cs231n.github.io/transfer-learning/)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "\n",
    "del feature_extractor, finetune_model, best_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… Cleanup complete!\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ’¾ GPU Memory Free: {torch.cuda.mem_get_info()[0] / 1e9:.1f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}