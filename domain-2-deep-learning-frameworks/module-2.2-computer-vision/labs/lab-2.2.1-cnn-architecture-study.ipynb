{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.2.1: CNN Architecture Study\n",
    "\n",
    "**Module:** 2.2 - Computer Vision  \n",
    "**Time:** 3 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand how convolution operations extract features from images\n",
    "- [ ] Implement LeNet-5, AlexNet, VGG-11, and ResNet-18 from scratch\n",
    "- [ ] Compare architecture performance on CIFAR-10\n",
    "- [ ] Understand the evolution of CNN design principles\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Module 6 (PyTorch Deep Learning)\n",
    "- Knowledge of: Neural networks, backpropagation, PyTorch basics\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**CNNs power the visual AI all around us:**\n",
    "\n",
    "- üì∏ **Your phone's camera** uses CNNs for face detection, portrait mode, and scene recognition\n",
    "- üöó **Self-driving cars** use CNNs to identify pedestrians, signs, and lane markings\n",
    "- üè• **Medical imaging** CNNs detect tumors in X-rays with superhuman accuracy\n",
    "- üõí **Amazon Go** stores use CNNs to track what you pick up\n",
    "\n",
    "The architectures you'll learn today form the foundation of all these systems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What is a Convolutional Neural Network?\n",
    "\n",
    "> **Imagine you're playing \"I Spy\" with a friend...** üîç\n",
    ">\n",
    "> When you look for something specific (like a red ball), you don't examine every single pixel of your vision at once. Instead, your eyes scan across the scene, looking for specific patterns:\n",
    "> - First, you look for anything red\n",
    "> - Then, you look for round shapes\n",
    "> - Finally, you check if the red round thing is the right size\n",
    ">\n",
    "> **A CNN works exactly the same way!**\n",
    ">\n",
    "> - **Convolutional layers** are like pattern detectors that slide across the image\n",
    "> - **Early layers** detect simple patterns (edges, colors)\n",
    "> - **Deeper layers** combine simple patterns into complex ones (eyes ‚Üí faces)\n",
    "> - **Pooling layers** are like squinting - you lose detail but see the big picture\n",
    ">\n",
    "> **In AI terms:** A CNN applies learnable filters across an image to detect hierarchical features, building from edges to textures to object parts to whole objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding Convolution Operations\n",
    "\n",
    "### The Core Operation\n",
    "\n",
    "A convolution is like a magnifying glass that looks at small patches of an image and produces a single number representing \"how much does this patch match my pattern?\"\n",
    "\n",
    "```\n",
    "Input Image          Filter (Kernel)       Output Feature Map\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 1  2  3  4  ‚îÇ      ‚îÇ 1  0  ‚îÇ            ‚îÇ 14  20  ‚îÇ\n",
    "‚îÇ 5  6  7  8  ‚îÇ  √ó   ‚îÇ 0  1  ‚îÇ    =       ‚îÇ 30  36  ‚îÇ\n",
    "‚îÇ 9  10 11 12 ‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "‚îÇ 13 14 15 16 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "Let's see this in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - Run this first!\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Dict\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# DGX Spark optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Check our hardware\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è  Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize what convolution does\n",
    "def visualize_convolution():\n",
    "    \"\"\"\n",
    "    Show how different filters detect different features.\n",
    "    \"\"\"\n",
    "    # Load a sample image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    dataset = torchvision.datasets.CIFAR10(root='../data', train=True, \n",
    "                                            download=True, transform=transform)\n",
    "    \n",
    "    # Get a nice image (let's find a car)\n",
    "    for img, label in dataset:\n",
    "        if label == 1:  # Car class\n",
    "            break\n",
    "    \n",
    "    # Convert to grayscale for clearer visualization\n",
    "    gray_img = img.mean(dim=0, keepdim=True).unsqueeze(0)  # [1, 1, 32, 32]\n",
    "    \n",
    "    # Define edge detection filters\n",
    "    filters = {\n",
    "        'Horizontal Edges': torch.tensor([[[[-1., -1., -1.],\n",
    "                                            [ 0.,  0.,  0.],\n",
    "                                            [ 1.,  1.,  1.]]]]),\n",
    "        'Vertical Edges': torch.tensor([[[[-1., 0., 1.],\n",
    "                                          [-1., 0., 1.],\n",
    "                                          [-1., 0., 1.]]]]),\n",
    "        'Corners': torch.tensor([[[[ 0., -1., 0.],\n",
    "                                   [-1.,  4., -1.],\n",
    "                                   [ 0., -1., 0.]]]]),\n",
    "        'Blur': torch.ones(1, 1, 3, 3) / 9\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0, 0].imshow(img.permute(1, 2, 0).numpy())\n",
    "    axes[0, 0].set_title('Original (Color)', fontsize=12)\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[0, 1].imshow(gray_img.squeeze().numpy(), cmap='gray')\n",
    "    axes[0, 1].set_title('Grayscale', fontsize=12)\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    # Apply each filter\n",
    "    for idx, (name, kernel) in enumerate(filters.items()):\n",
    "        output = F.conv2d(gray_img, kernel, padding=1)\n",
    "        row, col = divmod(idx + 2, 3)\n",
    "        axes[row, col].imshow(output.squeeze().detach().numpy(), cmap='gray')\n",
    "        axes[row, col].set_title(f'{name}', fontsize=12)\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.suptitle('üîç How Convolution Filters Detect Features', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_convolution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "Each filter detected different features:\n",
    "- **Horizontal edges**: Bright where there are horizontal lines (like car rooflines)\n",
    "- **Vertical edges**: Bright where there are vertical lines (like car sides)\n",
    "- **Corners**: Bright at corners and detailed areas\n",
    "- **Blur**: Smoothed out the image (useful for noise reduction)\n",
    "\n",
    "**Key insight**: In a CNN, the network *learns* what filters to use! It discovers the most useful patterns automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The CNN Architecture Timeline\n",
    "\n",
    "Let's trace the evolution of CNNs:\n",
    "\n",
    "```\n",
    "1998: LeNet-5         2012: AlexNet        2014: VGG           2015: ResNet\n",
    "  ‚îÇ                      ‚îÇ                   ‚îÇ                    ‚îÇ\n",
    "  ‚ñº                      ‚ñº                   ‚ñº                    ‚ñº\n",
    "60K params           62M params          138M params         11M params\n",
    "7 layers             8 layers            16-19 layers        50-152 layers!\n",
    "Digit recognition    ImageNet winner     Deeper = Better?    Skip connections!\n",
    "```\n",
    "\n",
    "Each architecture solved a critical problem:\n",
    "- **LeNet**: Proved CNNs work for visual recognition\n",
    "- **AlexNet**: Made them work at scale with GPUs\n",
    "- **VGG**: Showed that depth matters (using 3√ó3 filters)\n",
    "- **ResNet**: Solved vanishing gradients with skip connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: LeNet-5 (1998) - The Pioneer üèõÔ∏è\n",
    "\n",
    "### üßí ELI5: LeNet\n",
    "\n",
    "> **Imagine you're sorting letters by their zip codes...**\n",
    ">\n",
    "> Before LeNet, computers couldn't reliably read handwritten numbers. Yann LeCun created a network that looks at digits the way you might:\n",
    "> 1. First, spot the curves and straight lines\n",
    "> 2. Then, combine them into recognizable patterns (loops, crosses)\n",
    "> 3. Finally, decide which digit it most looks like\n",
    ">\n",
    "> **LeNet was the first CNN to be used commercially** - it processed millions of checks at banks!\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "Input (32√ó32√ó1)\n",
    "    ‚îÇ\n",
    "    ‚ñº\n",
    "Conv1 (5√ó5, 6 filters) ‚Üí 28√ó28√ó6\n",
    "    ‚îÇ\n",
    "    ‚ñº\n",
    "AvgPool (2√ó2) ‚Üí 14√ó14√ó6\n",
    "    ‚îÇ\n",
    "    ‚ñº\n",
    "Conv2 (5√ó5, 16 filters) ‚Üí 10√ó10√ó16\n",
    "    ‚îÇ\n",
    "    ‚ñº\n",
    "AvgPool (2√ó2) ‚Üí 5√ó5√ó16\n",
    "    ‚îÇ\n",
    "    ‚ñº\n",
    "Flatten ‚Üí 400\n",
    "    ‚îÇ\n",
    "    ‚ñº\n",
    "FC1 ‚Üí 120 ‚Üí FC2 ‚Üí 84 ‚Üí Output ‚Üí 10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    \"\"\"\n",
    "    LeNet-5 implementation (adapted for CIFAR-10's 3 channels).\n",
    "    \n",
    "    Original paper: \"Gradient-Based Learning Applied to Document Recognition\"\n",
    "    by Yann LeCun et al., 1998\n",
    "    \n",
    "    Key innovations:\n",
    "    - Convolutional layers for spatial feature extraction\n",
    "    - Subsampling (pooling) for translation invariance\n",
    "    - Tanh activation (original), we use ReLU here\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int = 10):\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)      # 32‚Üí28\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2) # 28‚Üí14\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)     # 14‚Üí10\n",
    "        \n",
    "        # Classification layers\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Conv block 1\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # [B, 6, 14, 14]\n",
    "        \n",
    "        # Conv block 2\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # [B, 16, 5, 5]\n",
    "        \n",
    "        # Flatten and classify\n",
    "        x = x.view(x.size(0), -1)  # [B, 400]\n",
    "        x = F.relu(self.fc1(x))    # [B, 120]\n",
    "        x = F.relu(self.fc2(x))    # [B, 84]\n",
    "        x = self.fc3(x)            # [B, 10]\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test it!\n",
    "model = LeNet5()\n",
    "dummy_input = torch.randn(1, 3, 32, 32)\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(f\"üìä LeNet-5 Architecture\")\n",
    "print(f\"   Input shape:  {dummy_input.shape}\")\n",
    "print(f\"   Output shape: {output.shape}\")\n",
    "print(f\"   Parameters:   {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself #1\n",
    "\n",
    "Modify LeNet to use **MaxPooling** instead of **AvgPooling**. Which do you think will work better for CIFAR-10 and why?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "- MaxPool keeps the strongest activation in each region (\"Was there an edge here?\")\n",
    "- AvgPool smooths activations (\"How much edge on average?\")\n",
    "- For detecting objects, strong features often matter more than average features\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Create LeNet5_MaxPool\n",
    "# Tip: Just change nn.AvgPool2d to nn.MaxPool2d\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: AlexNet (2012) - The GPU Revolution üöÄ\n",
    "\n",
    "### üßí ELI5: AlexNet\n",
    "\n",
    "> **Imagine trying to spot Waldo in a tiny phone screen vs. a huge wall mural...**\n",
    ">\n",
    "> Before AlexNet, neural networks looked at tiny images because bigger ones took forever to process. Alex Krizhevsky's breakthrough was using GPUs (designed for video games!) to crunch through millions of large images.\n",
    ">\n",
    "> Key innovations:\n",
    "> - **ReLU activation**: Faster than tanh (the old standard)\n",
    "> - **Dropout**: Prevents memorization\n",
    "> - **Data augmentation**: More training variety\n",
    "> - **GPU training**: 1000√ó speedup!\n",
    ">\n",
    "> **AlexNet won ImageNet 2012 by a HUGE margin** - shocking the computer vision world and starting the deep learning revolution!\n",
    "\n",
    "### Architecture (adapted for CIFAR-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    \"\"\"\n",
    "    AlexNet implementation (adapted for 32√ó32 CIFAR-10 images).\n",
    "    \n",
    "    Original paper: \"ImageNet Classification with Deep Convolutional Neural Networks\"\n",
    "    by Alex Krizhevsky et al., 2012\n",
    "    \n",
    "    Key innovations:\n",
    "    - ReLU activation (faster training than sigmoid/tanh)\n",
    "    - Dropout regularization\n",
    "    - Local Response Normalization (we use BatchNorm instead)\n",
    "    - Overlapping max pooling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int = 10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        \n",
    "        # Feature extraction (adapted kernel sizes for 32√ó32 input)\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv1: Large receptive field to capture broad patterns\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 32‚Üí16\n",
    "            \n",
    "            # Conv2: More filters for richer features\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 16‚Üí8\n",
    "            \n",
    "            # Conv3-5: Deeper feature extraction\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 8‚Üí4\n",
    "        )\n",
    "        \n",
    "        # Classifier with dropout\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256 * 4 * 4, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Test it!\n",
    "model = AlexNet()\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(f\"üìä AlexNet Architecture (CIFAR-10 adapted)\")\n",
    "print(f\"   Input shape:  {dummy_input.shape}\")\n",
    "print(f\"   Output shape: {output.shape}\")\n",
    "print(f\"   Parameters:   {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight: Dropout as Regularization\n",
    "\n",
    "```\n",
    "Training (Dropout ON):          Inference (Dropout OFF):\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ ‚óè‚îÄ‚óã‚îÄ‚óè‚îÄ‚óè‚îÄ‚óã‚îÄ‚óè     ‚îÇ            ‚îÇ ‚óè‚îÄ‚óè‚îÄ‚óè‚îÄ‚óè‚îÄ‚óè‚îÄ‚óè     ‚îÇ\n",
    "‚îÇ ‚îÇ   ‚îÇ ‚îÇ   ‚îÇ     ‚îÇ     ‚Üí      ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ     ‚îÇ\n",
    "‚îÇ ‚óè‚îÄ‚óè‚îÄ‚óã‚îÄ‚óè‚îÄ‚óè‚îÄ‚óã     ‚îÇ            ‚îÇ ‚óè‚îÄ‚óè‚îÄ‚óè‚îÄ‚óè‚îÄ‚óè‚îÄ‚óè     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "‚óã = dropped (random)           All neurons active\n",
    "```\n",
    "\n",
    "**Why it works**: Forces the network to not rely on any single neuron. Like training for a group project where members randomly miss meetings - everyone learns to contribute!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: VGG (2014) - The Power of Depth üìè\n",
    "\n",
    "### üßí ELI5: VGG\n",
    "\n",
    "> **Imagine building with LEGO blocks...**\n",
    ">\n",
    "> Previous CNNs used different sized bricks (5√ó5, 7√ó7, 11√ó11 filters). VGG's insight was: **just use tiny 3√ó3 bricks everywhere!**\n",
    ">\n",
    "> Why? Two 3√ó3 layers see the same area as one 5√ó5 layer, but:\n",
    "> - **More layers** = more ReLUs = more non-linearity = richer features\n",
    "> - **Fewer parameters** = less memory\n",
    ">\n",
    "> **VGG's motto**: \"Keep it simple, make it deep!\"\n",
    "\n",
    "### Why 3√ó3 Works Better\n",
    "\n",
    "```\n",
    "One 5√ó5 filter:                 Two 3√ó3 filters stacked:\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ ‚óè ‚óè ‚óè ‚óè ‚óè ‚îÇ                   ‚îÇ ‚óè ‚óè ‚óè ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ ‚óè ‚óè ‚óè ‚óè ‚óè ‚îÇ   Same            ‚îÇ ‚óè ‚óè ‚óè ‚îÇ ‚Üí ‚îÇ ‚óè ‚óè ‚óè ‚îÇ\n",
    "‚îÇ ‚óè ‚óè ‚óè ‚óè ‚óè ‚îÇ   receptive   =   ‚îÇ ‚óè ‚óè ‚óè ‚îÇ   ‚îÇ ‚óè ‚óè ‚óè ‚îÇ ‚Üí Output\n",
    "‚îÇ ‚óè ‚óè ‚óè ‚óè ‚óè ‚îÇ   field!          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ ‚óè ‚óè ‚óè ‚îÇ\n",
    "‚îÇ ‚óè ‚óè ‚óè ‚óè ‚óè ‚îÇ                               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "25 parameters                   9 + 9 = 18 parameters!\n",
    "1 ReLU                          2 ReLUs (more expressive!)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG11(nn.Module):\n",
    "    \"\"\"\n",
    "    VGG-11 implementation (Configuration A from the paper).\n",
    "    \n",
    "    Original paper: \"Very Deep Convolutional Networks for Large-Scale Image Recognition\"\n",
    "    by Karen Simonyan and Andrew Zisserman, 2014\n",
    "    \n",
    "    Key innovations:\n",
    "    - Uniform 3√ó3 convolutions throughout\n",
    "    - Doubling filters after each pooling\n",
    "    - Very deep networks (11-19 layers)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int = 10):\n",
    "        super(VGG11, self).__init__()\n",
    "        \n",
    "        # VGG block helper\n",
    "        def vgg_block(in_channels: int, out_channels: int, num_convs: int):\n",
    "            layers = []\n",
    "            for i in range(num_convs):\n",
    "                layers.append(nn.Conv2d(\n",
    "                    in_channels if i == 0 else out_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=3, padding=1\n",
    "                ))\n",
    "                layers.append(nn.BatchNorm2d(out_channels))\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            return nn.Sequential(*layers)\n",
    "        \n",
    "        # VGG-11 configuration: [64, M, 128, M, 256, 256, M, 512, 512, M, 512, 512, M]\n",
    "        self.features = nn.Sequential(\n",
    "            vgg_block(3, 64, 1),     # 32‚Üí16\n",
    "            vgg_block(64, 128, 1),   # 16‚Üí8\n",
    "            vgg_block(128, 256, 2),  # 8‚Üí4\n",
    "            vgg_block(256, 512, 2),  # 4‚Üí2\n",
    "            vgg_block(512, 512, 2),  # 2‚Üí1\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Test it!\n",
    "model = VGG11()\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(f\"üìä VGG-11 Architecture\")\n",
    "print(f\"   Input shape:  {dummy_input.shape}\")\n",
    "print(f\"   Output shape: {output.shape}\")\n",
    "print(f\"   Parameters:   {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: ResNet (2015) - Skip Connections to the Rescue ü¶∏\n",
    "\n",
    "### üßí ELI5: ResNet\n",
    "\n",
    "> **Imagine playing the telephone game with 150 people...**\n",
    ">\n",
    "> In a regular deep network, information passes through many layers. Like the telephone game, the message gets distorted with each step. By the time gradients reach the first layers during training, they've essentially vanished!\n",
    ">\n",
    "> **ResNet's solution: Skip connections (shortcuts!)**\n",
    ">\n",
    "> Instead of just passing the message along, you also whisper the original message directly to people further down the line. Now even if the telephone game distorts things, the original message still gets through!\n",
    ">\n",
    "> ```\n",
    "> Regular Network:     ResNet:\n",
    "> A ‚Üí B ‚Üí C ‚Üí D        A ‚Üí B ‚Üí C ‚Üí D\n",
    ">                       ‚Üò‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üó\n",
    ">                       (shortcut!)\n",
    "> ```\n",
    "\n",
    "### The Residual Block\n",
    "\n",
    "```\n",
    "Input (x)\n",
    "    ‚îÇ\n",
    "    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚ñº                ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ\n",
    "‚îÇ Conv   ‚îÇ           ‚îÇ  (identity shortcut)\n",
    "‚îÇ BN     ‚îÇ           ‚îÇ\n",
    "‚îÇ ReLU   ‚îÇ           ‚îÇ\n",
    "‚îÇ Conv   ‚îÇ           ‚îÇ\n",
    "‚îÇ BN     ‚îÇ           ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ\n",
    "    ‚îÇ                ‚îÇ\n",
    "    ‚ñº                ‚îÇ\n",
    "   (+)‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚Üê Add the input directly!\n",
    "    ‚îÇ\n",
    "    ‚ñº\n",
    "  ReLU\n",
    "    ‚îÇ\n",
    "    ‚ñº\n",
    "  Output = F(x) + x      ‚Üê \"Learn the residual\"\n",
    "```\n",
    "\n",
    "**Why it works**: Instead of learning `H(x)`, the network learns `H(x) - x` (the residual). If the optimal transformation is close to identity, learning a near-zero residual is easier than learning identity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic residual block for ResNet-18/34.\n",
    "    \n",
    "    Two 3√ó3 convolutions with a skip connection.\n",
    "    \"\"\"\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        # Main path\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Shortcut path (identity or projection)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            # Need to match dimensions with 1√ó1 conv\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Main path\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        # Add shortcut (THIS IS THE MAGIC!)\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet-18 implementation.\n",
    "    \n",
    "    Original paper: \"Deep Residual Learning for Image Recognition\"\n",
    "    by Kaiming He et al., 2015\n",
    "    \n",
    "    Key innovations:\n",
    "    - Skip connections (identity shortcuts)\n",
    "    - Batch normalization throughout\n",
    "    - Global average pooling instead of FC layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int = 10):\n",
    "        super(ResNet18, self).__init__()\n",
    "        \n",
    "        self.in_channels = 64\n",
    "        \n",
    "        # Initial convolution (adapted for 32√ó32 input)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Residual layers\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1)   # 32√ó32\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2)  # 16√ó16\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2)  # 8√ó8\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2)  # 4√ó4\n",
    "        \n",
    "        # Global average pooling + classifier\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def _make_layer(self, out_channels: int, num_blocks: int, stride: int):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(BasicBlock(self.in_channels, out_channels, s))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Initial conv\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        # Residual blocks\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        # Classifier\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test it!\n",
    "model = ResNet18()\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(f\"üìä ResNet-18 Architecture\")\n",
    "print(f\"   Input shape:  {dummy_input.shape}\")\n",
    "print(f\"   Output shape: {output.shape}\")\n",
    "print(f\"   Parameters:   {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself #2\n",
    "\n",
    "Visualize the gradient flow through a ResNet block vs a plain block. Create a simple experiment:\n",
    "\n",
    "1. Create a plain conv block (2 convs, no skip)\n",
    "2. Create a residual block (same, but with skip)\n",
    "3. Pass random input through each\n",
    "4. Compute gradient of output w.r.t. input\n",
    "5. Compare gradient magnitudes\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "```python\n",
    "# Compute gradients\n",
    "x.requires_grad = True\n",
    "output = block(x)\n",
    "output.sum().backward()\n",
    "grad_magnitude = x.grad.abs().mean()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Compare gradient flow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Architecture Comparison on CIFAR-10\n",
    "\n",
    "Now let's train all architectures and compare them! We'll use a consistent training setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ‚ö†Ô∏è DGX SPARK NOTE: When using Docker with num_workers > 0, ensure --ipc=host flag is set\n# Example: docker run --gpus all --ipc=host -it nvcr.io/nvidia/pytorch:25.11-py3\n# Without this flag, DataLoader may hang or crash due to shared memory issues.\n\ndef get_cifar10_loaders(batch_size: int = 128) -> Tuple[DataLoader, DataLoader]:\n    \"\"\"\n    Create CIFAR-10 data loaders with standard augmentation.\n    \n    Args:\n        batch_size: Batch size for training and testing.\n                   DGX Spark can handle larger batches (256-512) due to 128GB memory.\n    \n    Note:\n        When running in Docker, use --ipc=host flag for num_workers > 0.\n    \"\"\"\n    # Training transforms (with augmentation)\n    train_transform = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), \n                           (0.2023, 0.1994, 0.2010))\n    ])\n    \n    # Test transforms (no augmentation)\n    test_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), \n                           (0.2023, 0.1994, 0.2010))\n    ])\n    \n    train_dataset = torchvision.datasets.CIFAR10(\n        root='../data', train=True, download=True, transform=train_transform\n    )\n    test_dataset = torchvision.datasets.CIFAR10(\n        root='../data', train=False, download=True, transform=test_transform\n    )\n    \n    # num_workers=4 requires --ipc=host when running in Docker\n    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n                              shuffle=True, num_workers=4, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size,\n                             shuffle=False, num_workers=4, pin_memory=True)\n    \n    return train_loader, test_loader\n\ntrain_loader, test_loader = get_cifar10_loaders()\nprint(f\"üìä Dataset loaded:\")\nprint(f\"   Training samples: {len(train_loader.dataset):,}\")\nprint(f\"   Test samples:     {len(test_loader.dataset):,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    epochs: int = 10,\n",
    "    lr: float = 0.01\n",
    ") -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Train a model and track metrics.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with train_loss, train_acc, test_loss, test_acc histories\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'test_loss': [], 'test_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for inputs, targets in pbar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{train_loss/total:.4f}', \n",
    "                            'acc': f'{100.*correct/total:.1f}%'})\n",
    "        \n",
    "        history['train_loss'].append(train_loss / len(train_loader))\n",
    "        history['train_acc'].append(100. * correct / total)\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        test_loss, correct, total = 0, 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        history['test_loss'].append(test_loss / len(test_loader))\n",
    "        history['test_acc'].append(100. * correct / total)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"   Test: Loss={history['test_loss'][-1]:.4f}, Acc={history['test_acc'][-1]:.1f}%\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all architectures (this takes ~15-20 minutes)\n",
    "architectures = {\n",
    "    'LeNet-5': LeNet5(),\n",
    "    'AlexNet': AlexNet(),\n",
    "    'VGG-11': VGG11(),\n",
    "    'ResNet-18': ResNet18()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "epochs = 10  # Quick comparison; use 50+ for best results\n",
    "\n",
    "for name, model in architectures.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"üèãÔ∏è Training {name}...\")\n",
    "    print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = train_model(model, train_loader, test_loader, epochs=epochs)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    results[name] = {\n",
    "        'history': history,\n",
    "        'params': sum(p.numel() for p in model.parameters()),\n",
    "        'train_time': train_time,\n",
    "        'final_acc': history['test_acc'][-1]\n",
    "    }\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Training curves\n",
    "for name, data in results.items():\n",
    "    axes[0].plot(data['history']['test_acc'], label=name, linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Test Accuracy (%)')\n",
    "axes[0].set_title('üìà Test Accuracy Over Training')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter count vs accuracy\n",
    "names = list(results.keys())\n",
    "params = [results[n]['params'] / 1e6 for n in names]\n",
    "accs = [results[n]['final_acc'] for n in names]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "\n",
    "axes[1].bar(names, params, color=colors)\n",
    "axes[1].set_ylabel('Parameters (Millions)')\n",
    "axes[1].set_title('üìä Model Size Comparison')\n",
    "for i, (n, p, a) in enumerate(zip(names, params, accs)):\n",
    "    axes[1].annotate(f'{a:.1f}%', (i, p), ha='center', va='bottom')\n",
    "\n",
    "# Efficiency: Accuracy per million parameters\n",
    "efficiency = [a / p for a, p in zip(accs, params)]\n",
    "axes[2].bar(names, efficiency, color=colors)\n",
    "axes[2].set_ylabel('Accuracy / Million Params')\n",
    "axes[2].set_title('‚ö° Parameter Efficiency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìã FINAL COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Model':<15} {'Parameters':>12} {'Final Acc':>12} {'Train Time':>12}\")\n",
    "print(\"-\"*70)\n",
    "for name, data in results.items():\n",
    "    print(f\"{name:<15} {data['params']:>12,} {data['final_acc']:>11.1f}% {data['train_time']:>10.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting to adjust for input size\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Using ImageNet-sized kernels on CIFAR-10\n",
    "self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2)  # 32‚Üí13 (not nice!)\n",
    "\n",
    "# ‚úÖ Right: Adapt kernel size for small images\n",
    "self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1)  # 32‚Üí32\n",
    "```\n",
    "**Why:** CIFAR-10 images (32√ó32) are much smaller than ImageNet (224√ó224). Large kernels and strides destroy too much information.\n",
    "\n",
    "### Mistake 2: Missing normalization\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Raw transforms\n",
    "transform = transforms.ToTensor()  # Values 0-1\n",
    "\n",
    "# ‚úÖ Right: Normalize to match training distribution\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "```\n",
    "**Why:** Pre-trained models expect normalized inputs. Without it, activations explode.\n",
    "\n",
    "### Mistake 3: Wrong residual connection dimensions\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Adding tensors of different sizes\n",
    "out = self.conv_block(x) + x  # Fails if shapes differ!\n",
    "\n",
    "# ‚úÖ Right: Use projection shortcut when dimensions change\n",
    "if stride != 1 or in_channels != out_channels:\n",
    "    self.shortcut = nn.Conv2d(in_channels, out_channels, \n",
    "                               kernel_size=1, stride=stride)\n",
    "out = self.conv_block(x) + self.shortcut(x)\n",
    "```\n",
    "**Why:** Skip connections must have matching dimensions to add."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How convolution operations detect features in images\n",
    "- ‚úÖ The evolution from LeNet (1998) to ResNet (2015)\n",
    "- ‚úÖ Why 3√ó3 convolutions are preferred (VGG's insight)\n",
    "- ‚úÖ How skip connections solve vanishing gradients (ResNet's breakthrough)\n",
    "- ‚úÖ How to compare architectures fairly on a benchmark dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "Implement **ResNet with Squeeze-and-Excitation (SE) blocks** - a 2017 improvement that adds channel attention:\n",
    "\n",
    "```\n",
    "Input ‚Üí Conv ‚Üí SE Block ‚Üí Output\n",
    "              ‚Üì\n",
    "         [Global Pool]\n",
    "              ‚Üì\n",
    "         [FC ‚Üí ReLU ‚Üí FC ‚Üí Sigmoid]\n",
    "              ‚Üì\n",
    "         [Scale each channel]\n",
    "```\n",
    "\n",
    "The SE block learns to weight channels by their importance. Can you beat vanilla ResNet?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Starting Code</summary>\n",
    "\n",
    "```python\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excite = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.squeeze(x).view(b, c)\n",
    "        y = self.excite(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [Original LeNet Paper](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) - Yann LeCun's classic\n",
    "- [AlexNet Paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) - The GPU revolution\n",
    "- [VGG Paper](https://arxiv.org/abs/1409.1556) - The depth experiments\n",
    "- [ResNet Paper](https://arxiv.org/abs/1512.03385) - Skip connections explained\n",
    "- [CS231n CNN Notes](http://cs231n.github.io/convolutional-networks/) - Excellent visualizations\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üíæ GPU Memory Free: {torch.cuda.mem_get_info()[0] / 1e9:.1f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}