{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.2.6: Segment Anything Model (SAM) Integration\n",
    "\n",
    "**Module:** 2.2 - Computer Vision  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand what makes SAM a \"foundation model\" for segmentation\n",
    "- [ ] Use SAM for automatic mask generation\n",
    "- [ ] Create interactive segmentation with point/box prompts\n",
    "- [ ] Leverage DGX Spark's 128GB memory for efficient SAM inference\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Tasks 7.1-7.5\n",
    "- Knowledge of: Image segmentation, transformers\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**SAM is revolutionizing image editing and analysis:**\n",
    "\n",
    "- üì∏ **Photo editing**: One-click background removal, object selection\n",
    "- üè• **Medical imaging**: Quickly annotate organs, tumors, cells\n",
    "- üõ∞Ô∏è **Satellite imagery**: Segment buildings, roads, vegetation\n",
    "- üé¨ **Video production**: Object tracking and rotoscoping\n",
    "- ü§ñ **Robotics**: Understanding scene for manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What is Segment Anything Model?\n",
    "\n",
    "> **Imagine a magical magnifying glass...** üîç\n",
    ">\n",
    "> When you point it at anything in a picture and say \"this one\", it instantly draws a perfect outline around that thing - whether it's a cat, a cup, or a cloud!\n",
    ">\n",
    "> **What makes SAM special:**\n",
    "> 1. **Zero-shot**: Works on objects it's never seen before\n",
    "> 2. **Promptable**: Tell it WHAT to segment (point, box, or text)\n",
    "> 3. **Foundation model**: Trained on 11 million images, 1.1 billion masks\n",
    ">\n",
    "> It's like having an expert annotator who can instantly segment anything you point at!\n",
    "\n",
    "### SAM Architecture Overview\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    Segment Anything Model                    ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  Image ‚îÄ‚îÄ‚ñ∫ Image Encoder ‚îÄ‚îÄ‚ñ∫ Image Embedding                ‚îÇ\n",
    "‚îÇ            (ViT-H/14)          (256√ó64√ó64)                  ‚îÇ\n",
    "‚îÇ                                      ‚îÇ                      ‚îÇ\n",
    "‚îÇ                                      ‚ñº                      ‚îÇ\n",
    "‚îÇ  Prompt ‚îÄ‚îÄ‚ñ∫ Prompt Encoder ‚îÄ‚îÄ‚ñ∫ Prompt Embedding             ‚îÇ\n",
    "‚îÇ  (points, boxes, masks)                ‚îÇ                    ‚îÇ\n",
    "‚îÇ                                        ‚ñº                    ‚îÇ\n",
    "‚îÇ                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ\n",
    "‚îÇ                              ‚îÇ  Mask Decoder   ‚îÇ            ‚îÇ\n",
    "‚îÇ                              ‚îÇ  (lightweight)  ‚îÇ            ‚îÇ\n",
    "‚îÇ                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ\n",
    "‚îÇ                                       ‚îÇ                     ‚îÇ\n",
    "‚îÇ                                       ‚ñº                     ‚îÇ\n",
    "‚îÇ                              Predicted Masks + IoU Scores   ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Key insight**: The image encoder is heavy (runs once), but the mask decoder is lightweight (runs many times for different prompts on the same image)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport urllib.request\nfrom pathlib import Path\nimport time\nfrom typing import List, Tuple, Optional, Dict\n\n# Check cv2 installation\ntry:\n    import cv2\nexcept ImportError:\n    print(\"‚ö†Ô∏è OpenCV not found. Installing...\")\n    import subprocess\n    import sys\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"opencv-python\", \"-q\"])\n    import cv2\n\n# Check device and memory\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"üñ•Ô∏è  Device: {device}\")\nif torch.cuda.is_available():\n    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"üíæ GPU Memory: {gpu_mem:.1f} GB\")\n    \n    # DGX Spark advantage!\n    if gpu_mem > 100:\n        print(f\"üöÄ DGX Spark detected! You can load the largest SAM model (ViT-H)!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install segment-anything if needed (with ARM64/DGX Spark compatibility)\nimport sys\nimport subprocess\n\ntry:\n    from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n    print(\"‚úÖ SAM library already installed!\")\nexcept ImportError:\n    print(\"‚ö†Ô∏è segment-anything not found.\")\n    print(\"üì¶ Installing segment-anything...\")\n    print(\"   (This may take a moment on first run)\")\n    \n    try:\n        # Install segment-anything from GitHub\n        result = subprocess.run(\n            [sys.executable, \"-m\", \"pip\", \"install\", \n             \"git+https://github.com/facebookresearch/segment-anything.git\", \"-q\"],\n            capture_output=True, text=True, timeout=300\n        )\n        \n        if result.returncode != 0:\n            print(f\"‚ùå Installation failed: {result.stderr}\")\n            print(\"\\nüîß For DGX Spark ARM64, try installing manually in your NGC container:\")\n            print(\"   pip install git+https://github.com/facebookresearch/segment-anything.git\")\n            raise ImportError(\"segment-anything installation failed\")\n        \n        from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n        print(\"‚úÖ SAM library installed successfully!\")\n        \n    except subprocess.TimeoutExpired:\n        print(\"‚ùå Installation timed out.\")\n        print(\"\\nüîß Please install manually before running this notebook:\")\n        print(\"   pip install git+https://github.com/facebookresearch/segment-anything.git\")\n        raise ImportError(\"segment-anything installation timed out\")\n    except Exception as e:\n        print(f\"‚ùå Installation error: {e}\")\n        print(\"\\nüîß Please install manually before running this notebook:\")\n        print(\"   pip install git+https://github.com/facebookresearch/segment-anything.git\")\n        raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Loading SAM Models\n",
    "\n",
    "SAM comes in three sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAM model variants\n",
    "sam_models = {\n",
    "    'vit_b': {'checkpoint': 'sam_vit_b_01ec64.pth', 'size': '375 MB', 'speed': 'Fast'},\n",
    "    'vit_l': {'checkpoint': 'sam_vit_l_0b3195.pth', 'size': '1.2 GB', 'speed': 'Medium'},\n",
    "    'vit_h': {'checkpoint': 'sam_vit_h_4b8939.pth', 'size': '2.5 GB', 'speed': 'Slow (Best quality)'},\n",
    "}\n",
    "\n",
    "print(\"üìä SAM Model Variants:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<12} {'Checkpoint Size':>18} {'Speed':>25}\")\n",
    "print(\"-\"*60)\n",
    "for name, info in sam_models.items():\n",
    "    print(f\"{name:<12} {info['size']:>18} {info['speed']:>25}\")\n",
    "\n",
    "print(\"\\nüí° For DGX Spark with 128GB, we can easily use ViT-H!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sam_checkpoint(model_type: str = 'vit_b') -> Path:\n",
    "    \"\"\"\n",
    "    Download SAM checkpoint if not present.\n",
    "    \n",
    "    Args:\n",
    "        model_type: One of 'vit_b', 'vit_l', 'vit_h'\n",
    "    \"\"\"\n",
    "    checkpoints = {\n",
    "        'vit_b': 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth',\n",
    "        'vit_l': 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth',\n",
    "        'vit_h': 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth',\n",
    "    }\n",
    "    \n",
    "    checkpoint_dir = Path('../data/sam_checkpoints')\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    url = checkpoints[model_type]\n",
    "    filename = url.split('/')[-1]\n",
    "    filepath = checkpoint_dir / filename\n",
    "    \n",
    "    if not filepath.exists():\n",
    "        print(f\"üì• Downloading {model_type} checkpoint...\")\n",
    "        urllib.request.urlretrieve(url, filepath)\n",
    "        print(f\"‚úÖ Downloaded to {filepath}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Checkpoint already exists: {filepath}\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "# Download the base model (fast to download, good for demos)\n",
    "# For best quality on DGX Spark, use 'vit_h'\n",
    "model_type = 'vit_b'  # Change to 'vit_h' for best quality\n",
    "checkpoint_path = download_sam_checkpoint(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clear buffer cache before loading large model (DGX Spark best practice)\nimport subprocess\nimport gc\n\nprint(\"üßπ Preparing for model load...\")\n\n# Clear Python garbage\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\n# Try to clear system buffer cache (requires sudo, optional)\ntry:\n    subprocess.run(['sudo', 'sh', '-c', 'sync; echo 3 > /proc/sys/vm/drop_caches'], \n                   check=True, capture_output=True, timeout=10)\n    print(\"‚úÖ System buffer cache cleared\")\nexcept Exception as e:\n    print(f\"‚ÑπÔ∏è  Buffer cache clear skipped (optional): {type(e).__name__}\")\n\n# Load SAM model\nprint(f\"\\nüîß Loading SAM {model_type}...\")\nstart_time = time.time()\n\nsam = sam_model_registry[model_type](checkpoint=str(checkpoint_path))\nsam.to(device=device)\n\nload_time = time.time() - start_time\nprint(f\"‚úÖ Model loaded in {load_time:.1f}s\")\n\n# Memory usage\nif torch.cuda.is_available():\n    mem_allocated = torch.cuda.memory_allocated() / 1e9\n    mem_reserved = torch.cuda.memory_reserved() / 1e9\n    print(f\"üíæ GPU Memory Used: {mem_allocated:.1f} GB (Reserved: {mem_reserved:.1f} GB)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Download Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def download_sample_images():\n    \"\"\"\n    Download sample images for SAM demos with fallback support.\n    \n    Uses multiple URL sources for reliability and creates placeholder\n    images if all downloads fail.\n    \"\"\"\n    sample_dir = Path('../data/sam_samples')\n    sample_dir.mkdir(parents=True, exist_ok=True)\n    \n    # URLs with fallbacks for reliability\n    urls = {\n        'dogs.jpg': [\n            'https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Collage_of_Nine_Dogs.jpg/800px-Collage_of_Nine_Dogs.jpg',\n            'https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/YellowLabradorLooking_new.jpg/1200px-YellowLabradorLooking_new.jpg',\n        ],\n        'groceries.jpg': [\n            'https://images.unsplash.com/photo-1542838132-92c53300491e?w=800',\n            'https://upload.wikimedia.org/wikipedia/commons/thumb/5/5a/Fruits_and_vegetables.jpg/800px-Fruits_and_vegetables.jpg',\n        ],\n        'room.jpg': [\n            'https://images.unsplash.com/photo-1586023492125-27b2c045efd7?w=800',\n            'https://upload.wikimedia.org/wikipedia/commons/thumb/5/5e/Guestroom_at_the_Westin_Seattle.jpg/1200px-Guestroom_at_the_Westin_Seattle.jpg',\n        ],\n    }\n    \n    # Custom headers to avoid blocking\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n    }\n    \n    images = {}\n    for name, url_list in urls.items():\n        filepath = sample_dir / name\n        \n        if filepath.exists():\n            print(f\"‚úÖ {name} already exists\")\n            images[name] = filepath\n            continue\n        \n        downloaded = False\n        for url in url_list:\n            try:\n                print(f\"üì• Downloading {name}...\")\n                request = urllib.request.Request(url, headers=headers)\n                with urllib.request.urlopen(request, timeout=30) as response:\n                    with open(filepath, 'wb') as f:\n                        f.write(response.read())\n                \n                # Verify it's a valid image\n                img = Image.open(filepath)\n                img.verify()\n                downloaded = True\n                print(f\"   ‚úÖ Downloaded from {url[:50]}...\")\n                break\n            except Exception as e:\n                print(f\"   ‚ö†Ô∏è Failed: {type(e).__name__}\")\n                if filepath.exists():\n                    filepath.unlink()  # Remove corrupted file\n        \n        if not downloaded:\n            print(f\"   üìù Creating placeholder for {name}\")\n            # Create a colorful placeholder image\n            h, w = 600, 800\n            placeholder = np.zeros((h, w, 3), dtype=np.uint8)\n            # Add gradient background\n            for i in range(h):\n                placeholder[i, :, 0] = int(255 * i / h)  # Red gradient\n                placeholder[i, :, 2] = int(255 * (1 - i / h))  # Blue gradient\n            placeholder[:, :, 1] = 128  # Green constant\n            # Add some shapes for segmentation testing\n            cv2.rectangle(placeholder, (100, 100), (300, 300), (255, 255, 255), -1)\n            cv2.circle(placeholder, (500, 300), 100, (0, 255, 0), -1)\n            cv2.rectangle(placeholder, (400, 400), (700, 550), (255, 0, 255), -1)\n            Image.fromarray(placeholder).save(filepath)\n        \n        images[name] = filepath\n    \n    return images\n\nsample_images = download_sample_images()\nprint(f\"\\n‚úÖ Sample images ready: {list(sample_images.keys())}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path: Path) -> Tuple[np.ndarray, Image.Image]:\n",
    "    \"\"\"Load image and return both numpy array (for SAM) and PIL image (for display).\"\"\"\n",
    "    pil_image = Image.open(path).convert('RGB')\n",
    "    np_image = np.array(pil_image)\n",
    "    return np_image, pil_image\n",
    "\n",
    "# Load a sample image\n",
    "sample_path = list(sample_images.values())[0]\n",
    "image_np, image_pil = load_image(sample_path)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(image_np)\n",
    "plt.title(f'Sample Image: {sample_path.name}\\nSize: {image_np.shape}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Automatic Mask Generation\n",
    "\n",
    "SAM can automatically segment everything in an image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create automatic mask generator\n",
    "mask_generator = SamAutomaticMaskGenerator(\n",
    "    sam,\n",
    "    points_per_side=32,  # Grid density for point prompts\n",
    "    pred_iou_thresh=0.88,  # Confidence threshold\n",
    "    stability_score_thresh=0.95,\n",
    "    crop_n_layers=1,  # Multi-scale\n",
    "    crop_n_points_downscale_factor=2,\n",
    "    min_mask_region_area=100,  # Filter tiny masks\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Automatic mask generator created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate masks automatically\n",
    "print(\"üîç Generating masks automatically...\")\n",
    "start_time = time.time()\n",
    "\n",
    "masks = mask_generator.generate(image_np)\n",
    "\n",
    "gen_time = time.time() - start_time\n",
    "print(f\"‚úÖ Generated {len(masks)} masks in {gen_time:.1f}s\")\n",
    "\n",
    "# Print mask statistics\n",
    "print(f\"\\nüìä Mask Statistics:\")\n",
    "areas = [m['area'] for m in masks]\n",
    "print(f\"   Area range: {min(areas):,} - {max(areas):,} pixels\")\n",
    "print(f\"   Average area: {np.mean(areas):,.0f} pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_masks(image: np.ndarray, masks: List[Dict], figsize: Tuple = (12, 10)):\n",
    "    \"\"\"\n",
    "    Visualize automatically generated masks.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    # Sort masks by area (largest first)\n",
    "    sorted_masks = sorted(masks, key=lambda x: x['area'], reverse=True)\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    for mask_data in sorted_masks:\n",
    "        mask = mask_data['segmentation']\n",
    "        color = np.random.random(3)\n",
    "        \n",
    "        # Create colored mask\n",
    "        colored_mask = np.zeros((*mask.shape, 4))\n",
    "        colored_mask[mask] = [*color, 0.5]  # RGBA with alpha\n",
    "        \n",
    "        ax.imshow(colored_mask)\n",
    "    \n",
    "    plt.title(f'üé® Automatic Segmentation: {len(masks)} objects found')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_masks(image_np, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_individual_masks(image: np.ndarray, masks: List[Dict], num_show: int = 9):\n",
    "    \"\"\"\n",
    "    Show individual masks in a grid.\n",
    "    \"\"\"\n",
    "    # Sort by area and take top N\n",
    "    sorted_masks = sorted(masks, key=lambda x: x['area'], reverse=True)[:num_show]\n",
    "    \n",
    "    rows = int(np.ceil(np.sqrt(num_show)))\n",
    "    cols = int(np.ceil(num_show / rows))\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (ax, mask_data) in enumerate(zip(axes, sorted_masks)):\n",
    "        mask = mask_data['segmentation']\n",
    "        \n",
    "        # Create masked image\n",
    "        masked_image = image.copy()\n",
    "        masked_image[~mask] = 255  # White background\n",
    "        \n",
    "        ax.imshow(masked_image)\n",
    "        ax.set_title(f\"Area: {mask_data['area']:,}\\nIoU: {mask_data['predicted_iou']:.2f}\", fontsize=9)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for ax in axes[len(sorted_masks):]:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('üîç Top Individual Masks (by area)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_individual_masks(image_np, masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Interactive Segmentation with Prompts\n",
    "\n",
    "SAM's real power: segment specific objects by pointing at them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictor for interactive prompts\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "# Set the image (encode it once, reuse for multiple prompts)\n",
    "print(\"üîß Encoding image...\")\n",
    "start_time = time.time()\n",
    "\n",
    "predictor.set_image(image_np)\n",
    "\n",
    "encode_time = time.time() - start_time\n",
    "print(f\"‚úÖ Image encoded in {encode_time:.1f}s\")\n",
    "print(\"   Now we can make fast predictions with different prompts!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point Prompts\n",
    "\n",
    "Click on an object to segment it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_with_point(\n",
    "    predictor: SamPredictor,\n",
    "    image: np.ndarray,\n",
    "    point: Tuple[int, int],\n",
    "    point_label: int = 1  # 1 for foreground, 0 for background\n",
    "):\n",
    "    \"\"\"\n",
    "    Segment object at a given point.\n",
    "    \n",
    "    Args:\n",
    "        point: (x, y) coordinates\n",
    "        point_label: 1 for \"include this\", 0 for \"exclude this\"\n",
    "    \"\"\"\n",
    "    input_point = np.array([point])\n",
    "    input_label = np.array([point_label])\n",
    "    \n",
    "    masks, scores, logits = predictor.predict(\n",
    "        point_coords=input_point,\n",
    "        point_labels=input_label,\n",
    "        multimask_output=True  # Return multiple masks with different granularity\n",
    "    )\n",
    "    \n",
    "    return masks, scores\n",
    "\n",
    "def visualize_point_segmentation(\n",
    "    image: np.ndarray,\n",
    "    point: Tuple[int, int],\n",
    "    masks: np.ndarray,\n",
    "    scores: np.ndarray\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize segmentation results from a point prompt.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    # Original with point\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].scatter(point[0], point[1], c='red', s=200, marker='*', edgecolors='white', linewidths=2)\n",
    "    axes[0].set_title('Input Point')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Show all 3 masks\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    titles = ['Small', 'Medium', 'Large']\n",
    "    \n",
    "    for idx, (mask, score, ax) in enumerate(zip(masks, scores, axes[1:])):\n",
    "        ax.imshow(image)\n",
    "        \n",
    "        # Create colored mask\n",
    "        colored_mask = np.zeros((*mask.shape, 4))\n",
    "        color_rgb = plt.cm.colors.to_rgb(colors[idx])\n",
    "        colored_mask[mask] = [*color_rgb, 0.6]\n",
    "        ax.imshow(colored_mask)\n",
    "        \n",
    "        ax.scatter(point[0], point[1], c='red', s=100, marker='*', edgecolors='white')\n",
    "        ax.set_title(f'{titles[idx]} mask\\nScore: {score:.2f}')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('üëÜ Point Prompt ‚Üí Multiple Mask Options', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Try a point prompt (adjust coordinates based on your image)\n",
    "h, w = image_np.shape[:2]\n",
    "point = (w // 2, h // 2)  # Center of image\n",
    "\n",
    "masks, scores = segment_with_point(predictor, image_np, point)\n",
    "visualize_point_segmentation(image_np, point, masks, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Prompts\n",
    "\n",
    "Draw a bounding box to segment the object inside!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_with_box(\n",
    "    predictor: SamPredictor,\n",
    "    box: Tuple[int, int, int, int]  # x1, y1, x2, y2\n",
    "):\n",
    "    \"\"\"\n",
    "    Segment object within a bounding box.\n",
    "    \"\"\"\n",
    "    input_box = np.array(box)\n",
    "    \n",
    "    masks, scores, logits = predictor.predict(\n",
    "        box=input_box,\n",
    "        multimask_output=False  # Single mask for boxes\n",
    "    )\n",
    "    \n",
    "    return masks[0], scores[0]\n",
    "\n",
    "def visualize_box_segmentation(\n",
    "    image: np.ndarray,\n",
    "    box: Tuple[int, int, int, int],\n",
    "    mask: np.ndarray,\n",
    "    score: float\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize box prompt segmentation.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    x1, y1, x2, y2 = box\n",
    "    \n",
    "    # Original with box\n",
    "    axes[0].imshow(image)\n",
    "    rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, fill=False, edgecolor='lime', linewidth=3)\n",
    "    axes[0].add_patch(rect)\n",
    "    axes[0].set_title('Input Box')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Mask overlay\n",
    "    axes[1].imshow(image)\n",
    "    colored_mask = np.zeros((*mask.shape, 4))\n",
    "    colored_mask[mask] = [0.2, 0.8, 0.8, 0.6]\n",
    "    axes[1].imshow(colored_mask)\n",
    "    rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, fill=False, edgecolor='lime', linewidth=2)\n",
    "    axes[1].add_patch(rect)\n",
    "    axes[1].set_title(f'Segmentation (Score: {score:.2f})')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Extracted object\n",
    "    extracted = image.copy()\n",
    "    extracted[~mask] = 255\n",
    "    axes[2].imshow(extracted)\n",
    "    axes[2].set_title('Extracted Object')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.suptitle('üì¶ Box Prompt ‚Üí Precise Segmentation', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Try a box prompt (adjust based on your image)\n",
    "h, w = image_np.shape[:2]\n",
    "box = (w//4, h//4, 3*w//4, 3*h//4)  # Central box\n",
    "\n",
    "mask, score = segment_with_box(predictor, box)\n",
    "visualize_box_segmentation(image_np, box, mask, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Multiple Prompts\n",
    "\n",
    "Use positive and negative points together for precise control!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_with_multi_prompts(\n",
    "    predictor: SamPredictor,\n",
    "    positive_points: List[Tuple[int, int]],\n",
    "    negative_points: List[Tuple[int, int]] = None,\n",
    "    box: Optional[Tuple[int, int, int, int]] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Segment with multiple prompt types.\n",
    "    \n",
    "    Args:\n",
    "        positive_points: Points that should be included\n",
    "        negative_points: Points that should be excluded\n",
    "        box: Optional bounding box\n",
    "    \"\"\"\n",
    "    if negative_points is None:\n",
    "        negative_points = []\n",
    "    \n",
    "    all_points = positive_points + negative_points\n",
    "    labels = [1] * len(positive_points) + [0] * len(negative_points)\n",
    "    \n",
    "    input_points = np.array(all_points)\n",
    "    input_labels = np.array(labels)\n",
    "    input_box = np.array(box) if box else None\n",
    "    \n",
    "    masks, scores, _ = predictor.predict(\n",
    "        point_coords=input_points if len(all_points) > 0 else None,\n",
    "        point_labels=input_labels if len(all_points) > 0 else None,\n",
    "        box=input_box,\n",
    "        multimask_output=False\n",
    "    )\n",
    "    \n",
    "    return masks[0], scores[0]\n",
    "\n",
    "def visualize_multi_prompt(\n",
    "    image: np.ndarray,\n",
    "    positive_points: List[Tuple[int, int]],\n",
    "    negative_points: List[Tuple[int, int]],\n",
    "    mask: np.ndarray\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize multi-prompt segmentation.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Prompts\n",
    "    axes[0].imshow(image)\n",
    "    for p in positive_points:\n",
    "        axes[0].scatter(p[0], p[1], c='green', s=200, marker='*', edgecolors='white', linewidths=2)\n",
    "    for p in negative_points:\n",
    "        axes[0].scatter(p[0], p[1], c='red', s=200, marker='x', linewidths=3)\n",
    "    axes[0].set_title('‚úì Positive (green) ‚úó Negative (red) points')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Result\n",
    "    axes[1].imshow(image)\n",
    "    colored_mask = np.zeros((*mask.shape, 4))\n",
    "    colored_mask[mask] = [0.3, 0.7, 0.3, 0.6]\n",
    "    axes[1].imshow(colored_mask)\n",
    "    axes[1].set_title('Segmentation Result')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.suptitle('üéØ Multi-Prompt Segmentation', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example with positive and negative points\n",
    "h, w = image_np.shape[:2]\n",
    "positive = [(w//2, h//2)]  # Include center\n",
    "negative = [(50, 50), (w-50, h-50)]  # Exclude corners\n",
    "\n",
    "mask, _ = segment_with_multi_prompts(predictor, positive, negative)\n",
    "visualize_multi_prompt(image_np, positive, negative, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Benchmarking on DGX Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_sam(predictor: SamPredictor, image: np.ndarray, num_runs: int = 10):\n",
    "    \"\"\"\n",
    "    Benchmark SAM inference speed.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'encode_time': [],\n",
    "        'point_inference': [],\n",
    "        'box_inference': [],\n",
    "    }\n",
    "    \n",
    "    h, w = image.shape[:2]\n",
    "    point = np.array([[w//2, h//2]])\n",
    "    label = np.array([1])\n",
    "    box = np.array([w//4, h//4, 3*w//4, 3*h//4])\n",
    "    \n",
    "    # Benchmark image encoding\n",
    "    print(\"üèéÔ∏è Benchmarking image encoding...\")\n",
    "    for _ in range(num_runs):\n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        start = time.perf_counter()\n",
    "        predictor.set_image(image)\n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        results['encode_time'].append(time.perf_counter() - start)\n",
    "    \n",
    "    # Benchmark point prediction (after encoding)\n",
    "    print(\"üèéÔ∏è Benchmarking point prediction...\")\n",
    "    predictor.set_image(image)  # Encode once\n",
    "    for _ in range(num_runs):\n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        start = time.perf_counter()\n",
    "        predictor.predict(point_coords=point, point_labels=label, multimask_output=True)\n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        results['point_inference'].append(time.perf_counter() - start)\n",
    "    \n",
    "    # Benchmark box prediction\n",
    "    print(\"üèéÔ∏è Benchmarking box prediction...\")\n",
    "    for _ in range(num_runs):\n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        start = time.perf_counter()\n",
    "        predictor.predict(box=box, multimask_output=False)\n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        results['box_inference'].append(time.perf_counter() - start)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_results = benchmark_sam(predictor, image_np, num_runs=10)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä SAM BENCHMARK RESULTS ON DGX SPARK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Image size: {image_np.shape}\")\n",
    "print(f\"Model: SAM {model_type}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Operation':<25} {'Mean (ms)':>15} {'Std (ms)':>15}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for op, times in benchmark_results.items():\n",
    "    mean_ms = np.mean(times) * 1000\n",
    "    std_ms = np.std(times) * 1000\n",
    "    print(f\"{op:<25} {mean_ms:>14.1f} {std_ms:>14.1f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüí° Key insight: After encoding ({np.mean(benchmark_results['encode_time'])*1000:.0f}ms),\")\n",
    "print(f\"   predictions are fast ({np.mean(benchmark_results['point_inference'])*1000:.0f}ms)!\")\n",
    "print(f\"   This enables interactive applications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Practical Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def background_removal(image: np.ndarray, predictor: SamPredictor, point: Tuple[int, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Remove background from image using SAM.\n",
    "    \n",
    "    Returns:\n",
    "        RGBA image with transparent background\n",
    "    \"\"\"\n",
    "    predictor.set_image(image)\n",
    "    \n",
    "    masks, scores, _ = predictor.predict(\n",
    "        point_coords=np.array([point]),\n",
    "        point_labels=np.array([1]),\n",
    "        multimask_output=True\n",
    "    )\n",
    "    \n",
    "    # Use highest scoring mask\n",
    "    best_mask = masks[scores.argmax()]\n",
    "    \n",
    "    # Create RGBA image\n",
    "    rgba = np.zeros((*image.shape[:2], 4), dtype=np.uint8)\n",
    "    rgba[:, :, :3] = image\n",
    "    rgba[:, :, 3] = (best_mask * 255).astype(np.uint8)\n",
    "    \n",
    "    return rgba, best_mask\n",
    "\n",
    "# Demo: Background removal\n",
    "h, w = image_np.shape[:2]\n",
    "rgba_result, mask = background_removal(image_np, predictor, (w//2, h//2))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(image_np)\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(mask, cmap='gray')\n",
    "axes[1].set_title('Mask')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Show with checkerboard background for transparency\n",
    "checkerboard = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "checkerboard[::20, ::20] = 200\n",
    "checkerboard[10::20, 10::20] = 200\n",
    "result_vis = checkerboard.copy()\n",
    "result_vis[mask] = image_np[mask]\n",
    "\n",
    "axes[2].imshow(result_vis)\n",
    "axes[2].set_title('Background Removed')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.suptitle('‚úÇÔ∏è One-Click Background Removal with SAM', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself\n",
    "\n",
    "1. **Try your own images**: Load an image of your choice and segment objects\n",
    "2. **Compare model sizes**: If you have time, compare ViT-B vs ViT-H quality\n",
    "3. **Build an annotation tool**: Create a function that takes multiple box prompts and segments all objects\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint for annotation tool</summary>\n",
    "\n",
    "```python\n",
    "def annotate_multiple_objects(image, boxes):\n",
    "    predictor.set_image(image)  # Encode once\n",
    "    masks = []\n",
    "    for box in boxes:\n",
    "        mask, _ = segment_with_box(predictor, box)\n",
    "        masks.append(mask)\n",
    "    return masks\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Re-encoding for each prompt\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Encoding image every time (slow!)\n",
    "for point in points:\n",
    "    predictor.set_image(image)  # Expensive!\n",
    "    predictor.predict(point=point)\n",
    "\n",
    "# ‚úÖ Right: Encode once, predict many times\n",
    "predictor.set_image(image)  # Do this once\n",
    "for point in points:\n",
    "    predictor.predict(point=point)  # Fast!\n",
    "```\n",
    "**Why:** Image encoding is the expensive operation (~1s). Predictions are fast (~50ms).\n",
    "\n",
    "### Mistake 2: Wrong input format\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Normalized tensor\n",
    "image = transforms.ToTensor()(pil_image)  # [0, 1] range\n",
    "predictor.set_image(image.numpy())\n",
    "\n",
    "# ‚úÖ Right: RGB numpy array with 0-255 values\n",
    "image = np.array(pil_image)  # [0, 255] range\n",
    "predictor.set_image(image)\n",
    "```\n",
    "**Why:** SAM expects raw 8-bit RGB images.\n",
    "\n",
    "### Mistake 3: Not using multimask_output correctly\n",
    "\n",
    "```python\n",
    "# For points: Use multimask=True (get options)\n",
    "masks, scores, _ = predictor.predict(point_coords=point, multimask_output=True)\n",
    "best_mask = masks[scores.argmax()]\n",
    "\n",
    "# For boxes: Use multimask=False (single precise mask)\n",
    "masks, _, _ = predictor.predict(box=box, multimask_output=False)\n",
    "mask = masks[0]\n",
    "```\n",
    "**Why:** Points are ambiguous (small/medium/large objects), boxes are precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ What makes SAM a foundation model for segmentation\n",
    "- ‚úÖ Automatic mask generation for entire images\n",
    "- ‚úÖ Interactive segmentation with points and boxes\n",
    "- ‚úÖ Combining positive and negative prompts\n",
    "- ‚úÖ Practical applications like background removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Build a simple \"Magic Wand\" tool:**\n",
    "\n",
    "Create a function that:\n",
    "1. Takes an image and a click position\n",
    "2. Returns the best mask for that click\n",
    "3. Allows iterative refinement with more clicks\n",
    "\n",
    "Bonus: Add a \"grow\" and \"shrink\" function that returns different granularity masks.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Starting Code</summary>\n",
    "\n",
    "```python\n",
    "class MagicWand:\n",
    "    def __init__(self, sam_model):\n",
    "        self.predictor = SamPredictor(sam_model)\n",
    "        self.current_mask = None\n",
    "        self.positive_points = []\n",
    "        self.negative_points = []\n",
    "    \n",
    "    def set_image(self, image):\n",
    "        self.predictor.set_image(image)\n",
    "        self.current_mask = None\n",
    "        self.positive_points = []\n",
    "        self.negative_points = []\n",
    "    \n",
    "    def click(self, point, is_positive=True):\n",
    "        if is_positive:\n",
    "            self.positive_points.append(point)\n",
    "        else:\n",
    "            self.negative_points.append(point)\n",
    "        self._update_mask()\n",
    "        return self.current_mask\n",
    "    \n",
    "    def _update_mask(self):\n",
    "        # ... predict with all accumulated points\n",
    "        pass\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [SAM Paper](https://arxiv.org/abs/2304.02643) - \"Segment Anything\"\n",
    "- [SAM GitHub](https://github.com/facebookresearch/segment-anything)\n",
    "- [SAM Demo](https://segment-anything.com/demo)\n",
    "- [SAM 2](https://ai.meta.com/sam2/) - Video segmentation\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "\n",
    "del sam, predictor, mask_generator\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üíæ GPU Memory Free: {torch.cuda.mem_get_info()[0] / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Module 7 Complete!\n",
    "\n",
    "Congratulations! You've completed the Computer Vision module. You've learned:\n",
    "\n",
    "1. **CNN Architectures**: From LeNet to ResNet\n",
    "2. **Transfer Learning**: Fine-tuning pre-trained models\n",
    "3. **Object Detection**: Using YOLOv8 for real-time detection\n",
    "4. **Segmentation**: U-Net for semantic segmentation\n",
    "5. **Vision Transformers**: ViT from scratch\n",
    "6. **Foundation Models**: SAM for zero-shot segmentation\n",
    "\n",
    "These skills form the backbone of modern computer vision applications!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}