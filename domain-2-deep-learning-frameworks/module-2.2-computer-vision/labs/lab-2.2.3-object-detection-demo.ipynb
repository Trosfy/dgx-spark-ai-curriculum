{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.2.3: Object Detection Demo\n",
    "\n",
    "**Module:** 2.2 - Computer Vision  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** â­â­â­\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand the difference between classification, detection, and segmentation\n",
    "- [ ] Know the evolution from R-CNN to YOLO\n",
    "- [ ] Use YOLOv8 for real-time object detection\n",
    "- [ ] Benchmark detection speed on DGX Spark\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Prerequisites\n",
    "\n",
    "- Completed: Tasks 7.1-7.2\n",
    "- Knowledge of: CNNs, transfer learning\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ Real-World Context\n",
    "\n",
    "**Object detection is everywhere:**\n",
    "\n",
    "- ğŸš— **Autonomous vehicles**: Detecting pedestrians, other cars, traffic signs\n",
    "- ğŸ“· **Security cameras**: Identifying suspicious activities\n",
    "- ğŸ­ **Manufacturing**: Quality control and defect detection\n",
    "- ğŸ›’ **Retail**: Inventory management and checkout automation\n",
    "- ğŸ¥ **Medical imaging**: Detecting tumors, lesions, anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§’ ELI5: What is Object Detection?\n",
    "\n",
    "> **Imagine you're playing \"I Spy\" but with a twist...** ğŸ”\n",
    ">\n",
    "> - **Classification**: \"I spy something... it's a CAT!\" (What is in the image?)\n",
    "> - **Detection**: \"I spy a CAT at (x=100, y=200)!\" (What AND where?)\n",
    "> - **Segmentation**: \"I spy a CAT and here's its exact silhouette!\" (Pixel-perfect boundary)\n",
    ">\n",
    "> Object detection answers: **What objects are in the image, and WHERE are they?**\n",
    "\n",
    "### Visual Comparison\n",
    "\n",
    "```\n",
    "Classification:        Detection:              Segmentation:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              â”‚       â”‚ â”Œâ”€â”€â”€â”€â”       â”‚        â”‚    â–“â–“â–“       â”‚\n",
    "â”‚    ğŸ±        â”‚       â”‚ â”‚ ğŸ± â”‚       â”‚        â”‚   â–“â–“â–“â–“â–“      â”‚\n",
    "â”‚              â”‚       â”‚ â””â”€â”€â”€â”€â”˜       â”‚        â”‚  â–“â–“â–“â–“â–“â–“â–“     â”‚\n",
    "â”‚    ğŸ•        â”‚       â”‚    â”Œâ”€â”€â”€â”€â”    â”‚        â”‚      â–ˆâ–ˆâ–ˆâ–ˆ    â”‚\n",
    "â”‚              â”‚       â”‚    â”‚ ğŸ• â”‚    â”‚        â”‚     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "Output: \"cat, dog\"     Output: boxes +         Output: pixel\n",
    "                       labels + confidence     masks for each\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Evolution of Object Detection\n",
    "\n",
    "### The Two Families of Detectors\n",
    "\n",
    "```\n",
    "TWO-STAGE DETECTORS:                    ONE-STAGE DETECTORS:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                           â”‚           â”‚                           â”‚\n",
    "â”‚  Step 1: Propose regions  â”‚           â”‚  Single pass: detect      â”‚\n",
    "â”‚  \"Where might objects be?\"â”‚           â”‚  everything at once!      â”‚\n",
    "â”‚            â”‚              â”‚           â”‚                           â”‚\n",
    "â”‚            â–¼              â”‚           â”‚  Input â†’ CNN â†’ Boxes      â”‚\n",
    "â”‚  Step 2: Classify regions â”‚           â”‚                           â”‚\n",
    "â”‚  \"What is in each region?\"â”‚           â”‚                           â”‚\n",
    "â”‚                           â”‚           â”‚                           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Examples:                               Examples:\n",
    "- R-CNN (2014)                          - YOLO (2016)\n",
    "- Fast R-CNN (2015)                     - SSD (2016)\n",
    "- Faster R-CNN (2016)                   - RetinaNet (2017)\n",
    "\n",
    "Pros: More accurate                     Pros: Much faster!\n",
    "Cons: Slower (two steps)                Cons: Slightly less accurate\n",
    "```\n",
    "\n",
    "### YOLO: \"You Only Look Once\"\n",
    "\n",
    "The key insight of YOLO: **Detection can be a single regression problem!**\n",
    "\n",
    "```\n",
    "Image â†’ CNN â†’ Grid predictions â†’ Filter by confidence â†’ Final boxes\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚ x, y, w, h  â”‚  (box coordinates)\n",
    "              â”‚ confidence  â”‚  (objectness score)\n",
    "              â”‚ class probs â”‚  (80 COCO classes)\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport time\nfrom pathlib import Path\nimport urllib.request\nfrom typing import List, Tuple, Dict\n\n# Install ultralytics for YOLOv8\n# Note: For DGX Spark ARM64, pre-install in NGC container for best results\n# See README.md for Quick Start instructions\n\ntry:\n    from ultralytics import YOLO\n    print(\"âœ… Ultralytics YOLOv8 available\")\nexcept ImportError:\n    print(\"âš ï¸ ultralytics not found. Installing...\")\n    print(\"   ğŸ’¡ TIP: For DGX Spark ARM64, this may take a moment to compile native extensions.\")\n    print(\"   For faster startup, pre-install in your NGC container: pip install ultralytics\")\n    !pip install ultralytics -q\n    from ultralytics import YOLO\n    print(\"âœ… ultralytics installed successfully!\")\n\n# Check device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"\\nğŸ–¥ï¸  Device: {device}\")\nif torch.cuda.is_available():\n    print(f\"ğŸ® GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: YOLOv8 Model Zoo\n",
    "\n",
    "YOLOv8 comes in different sizes for different use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOv8 model variants\n",
    "yolov8_models = {\n",
    "    'yolov8n': {'params': 3.2, 'mAP': 37.3, 'speed': 'Fastest'},\n",
    "    'yolov8s': {'params': 11.2, 'mAP': 44.9, 'speed': 'Fast'},\n",
    "    'yolov8m': {'params': 25.9, 'mAP': 50.2, 'speed': 'Medium'},\n",
    "    'yolov8l': {'params': 43.7, 'mAP': 52.9, 'speed': 'Slow'},\n",
    "    'yolov8x': {'params': 68.2, 'mAP': 53.9, 'speed': 'Slowest'},\n",
    "}\n",
    "\n",
    "print(\"ğŸ“Š YOLOv8 Model Variants:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<12} {'Parameters':>12} {'mAP@0.5':>12} {'Speed':>12}\")\n",
    "print(\"-\"*60)\n",
    "for name, info in yolov8_models.items():\n",
    "    print(f\"{name:<12} {info['params']:>10.1f}M {info['mAP']:>11.1f}% {info['speed']:>12}\")\n",
    "print(\"\\nğŸ’¡ Tip: Start with yolov8s for a good balance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample images for detection\n",
    "def download_sample_images():\n",
    "    \"\"\"Download sample images for detection demos.\"\"\"\n",
    "    data_dir = Path('../data/detection_samples')\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Sample images from COCO dataset\n",
    "    urls = {\n",
    "        'street.jpg': 'https://ultralytics.com/images/bus.jpg',\n",
    "        'zidane.jpg': 'https://ultralytics.com/images/zidane.jpg',\n",
    "    }\n",
    "    \n",
    "    for filename, url in urls.items():\n",
    "        filepath = data_dir / filename\n",
    "        if not filepath.exists():\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            urllib.request.urlretrieve(url, filepath)\n",
    "    \n",
    "    return data_dir\n",
    "\n",
    "sample_dir = download_sample_images()\n",
    "print(f\"âœ… Sample images ready in {sample_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Running YOLOv8 Inference\n",
    "\n",
    "Let's detect objects in sample images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv8 model (downloads automatically)\n",
    "print(\"Loading YOLOv8s model...\")\n",
    "model = YOLO('yolov8s.pt')  # 's' for small, good balance\n",
    "\n",
    "print(f\"âœ… Model loaded!\")\n",
    "print(f\"   Model type: {model.task}\")\n",
    "print(f\"   Classes: {len(model.names)} (COCO dataset)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run detection on sample image\n",
    "image_path = sample_dir / 'street.jpg'\n",
    "\n",
    "# Inference\n",
    "results = model(str(image_path), verbose=False)\n",
    "\n",
    "# Access first result (we only passed one image)\n",
    "result = results[0]\n",
    "\n",
    "print(f\"ğŸ“Š Detection Results:\")\n",
    "print(f\"   Image size: {result.orig_shape}\")\n",
    "print(f\"   Objects found: {len(result.boxes)}\")\n",
    "print(f\"   Inference time: {result.speed['inference']:.1f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections(result, figsize=(12, 8)):\n",
    "    \"\"\"\n",
    "    Visualize detection results with bounding boxes.\n",
    "    \"\"\"\n",
    "    # Get annotated image\n",
    "    annotated = result.plot()\n",
    "    \n",
    "    # Convert BGR to RGB\n",
    "    annotated = annotated[:, :, ::-1]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(result.orig_img[:, :, ::-1])\n",
    "    axes[0].set_title('Original Image', fontsize=12)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Detections\n",
    "    axes[1].imshow(annotated)\n",
    "    axes[1].set_title(f'Detections ({len(result.boxes)} objects)', fontsize=12)\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detection details\n",
    "    print(\"\\nğŸ” Detection Details:\")\n",
    "    print(\"-\"*50)\n",
    "    for i, box in enumerate(result.boxes):\n",
    "        cls_id = int(box.cls[0])\n",
    "        cls_name = result.names[cls_id]\n",
    "        conf = float(box.conf[0])\n",
    "        x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "        print(f\"   {i+1}. {cls_name}: {conf:.1%} @ [{x1:.0f}, {y1:.0f}, {x2:.0f}, {y2:.0f}]\")\n",
    "\n",
    "visualize_detections(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another image\n",
    "image_path2 = sample_dir / 'zidane.jpg'\n",
    "results2 = model(str(image_path2), verbose=False)\n",
    "visualize_detections(results2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Understanding Detection Outputs\n",
    "\n",
    "Let's dive deeper into what YOLO outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_detections(result) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze detection results in detail.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with detection statistics\n",
    "    \"\"\"\n",
    "    boxes = result.boxes\n",
    "    \n",
    "    # Extract data\n",
    "    class_ids = boxes.cls.cpu().numpy().astype(int)\n",
    "    confidences = boxes.conf.cpu().numpy()\n",
    "    xyxy = boxes.xyxy.cpu().numpy()  # [x1, y1, x2, y2]\n",
    "    \n",
    "    # Class distribution\n",
    "    class_counts = {}\n",
    "    for cls_id in class_ids:\n",
    "        cls_name = result.names[cls_id]\n",
    "        class_counts[cls_name] = class_counts.get(cls_name, 0) + 1\n",
    "    \n",
    "    # Box sizes\n",
    "    widths = xyxy[:, 2] - xyxy[:, 0]\n",
    "    heights = xyxy[:, 3] - xyxy[:, 1]\n",
    "    areas = widths * heights\n",
    "    \n",
    "    stats = {\n",
    "        'num_detections': len(boxes),\n",
    "        'class_distribution': class_counts,\n",
    "        'confidence_range': (confidences.min(), confidences.max()),\n",
    "        'avg_confidence': confidences.mean(),\n",
    "        'box_area_range': (areas.min(), areas.max()),\n",
    "        'avg_box_area': areas.mean(),\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "stats = analyze_detections(result)\n",
    "\n",
    "print(\"ğŸ“Š Detection Analysis:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total detections: {stats['num_detections']}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "for cls_name, count in stats['class_distribution'].items():\n",
    "    print(f\"   {cls_name}: {count}\")\n",
    "print(f\"\\nConfidence: {stats['confidence_range'][0]:.1%} - {stats['confidence_range'][1]:.1%}\")\n",
    "print(f\"Average confidence: {stats['avg_confidence']:.1%}\")\n",
    "print(f\"\\nBox area: {stats['box_area_range'][0]:.0f} - {stats['box_area_range'][1]:.0f} pixelsÂ²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§’ ELI5: Non-Maximum Suppression (NMS)\n",
    "\n",
    "> **Imagine you're looking for faces in a crowd photo...**\n",
    ">\n",
    "> The model might draw 10 slightly overlapping boxes around the same face because it's not sure exactly where the face boundaries are.\n",
    ">\n",
    "> **NMS (Non-Maximum Suppression)** cleans this up:\n",
    "> 1. Keep the most confident box\n",
    "> 2. Remove any overlapping boxes (above IoU threshold)\n",
    "> 3. Repeat for remaining boxes\n",
    ">\n",
    "> Result: One clean box per object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate confidence threshold effect\n",
    "def compare_confidence_thresholds(image_path, thresholds=[0.1, 0.25, 0.5, 0.75]):\n",
    "    \"\"\"\n",
    "    Show how confidence threshold affects detection.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(thresholds), figsize=(4*len(thresholds), 4))\n",
    "    \n",
    "    for ax, conf_thresh in zip(axes, thresholds):\n",
    "        results = model(str(image_path), conf=conf_thresh, verbose=False)\n",
    "        annotated = results[0].plot()\n",
    "        \n",
    "        ax.imshow(annotated[:, :, ::-1])\n",
    "        ax.set_title(f'Confidence > {conf_thresh}\\n({len(results[0].boxes)} objects)', fontsize=10)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('ğŸ¯ Effect of Confidence Threshold', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_confidence_thresholds(sample_dir / 'street.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: DGX Spark Benchmarks\n",
    "\n",
    "Let's benchmark YOLOv8 performance on DGX Spark!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Benchmark configuration\nBENCHMARK_WARMUP = 5    # Number of warmup iterations\nBENCHMARK_RUNS = 20     # Number of timed iterations\n\ndef benchmark_yolo(\n    model_size: str = 'yolov8s',\n    image_sizes: List[int] = [320, 640, 1280],\n    num_warmup: int = BENCHMARK_WARMUP,\n    num_runs: int = BENCHMARK_RUNS\n) -> Dict:\n    \"\"\"\n    Benchmark YOLO model at different image sizes.\n    \n    Args:\n        model_size: YOLO model variant (yolov8n, yolov8s, yolov8m, etc.)\n        image_sizes: List of image sizes to test\n        num_warmup: Number of warmup iterations before timing\n        num_runs: Number of timed iterations for averaging\n    \n    Returns:\n        Dictionary of benchmarks by image size\n    \"\"\"\n    print(f\"ğŸï¸ Benchmarking {model_size}...\")\n    model = YOLO(f'{model_size}.pt')\n    \n    # Generate random image\n    results = {}\n    \n    for img_size in image_sizes:\n        print(f\"\\n   Testing {img_size}Ã—{img_size}...\")\n        \n        # Create dummy image\n        dummy_img = np.random.randint(0, 255, (img_size, img_size, 3), dtype=np.uint8)\n        \n        # Warmup\n        for _ in range(num_warmup):\n            model(dummy_img, imgsz=img_size, verbose=False)\n        \n        # Benchmark\n        torch.cuda.synchronize() if torch.cuda.is_available() else None\n        times = []\n        \n        for _ in range(num_runs):\n            start = time.perf_counter()\n            model(dummy_img, imgsz=img_size, verbose=False)\n            torch.cuda.synchronize() if torch.cuda.is_available() else None\n            times.append(time.perf_counter() - start)\n        \n        avg_time = np.mean(times) * 1000  # Convert to ms\n        fps = 1000 / avg_time\n        \n        results[img_size] = {\n            'avg_time_ms': avg_time,\n            'fps': fps,\n            'times': times\n        }\n        \n        print(f\"      Avg time: {avg_time:.1f}ms, FPS: {fps:.1f}\")\n    \n    return results\n\nbenchmarks = benchmark_yolo('yolov8s')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare all model sizes\ndef benchmark_all_models(\n    models: List[str] = ['yolov8n', 'yolov8s', 'yolov8m'],\n    img_size: int = 640,\n    num_warmup: int = BENCHMARK_WARMUP,\n    num_runs: int = BENCHMARK_RUNS\n) -> Dict:\n    \"\"\"\n    Benchmark multiple YOLO variants.\n    \n    Args:\n        models: List of YOLO model variants to benchmark\n        img_size: Image size for benchmarking\n        num_warmup: Number of warmup iterations before timing\n        num_runs: Number of timed iterations for averaging\n    \n    Returns:\n        Dictionary of benchmark results per model\n    \"\"\"\n    results = {}\n    dummy_img = np.random.randint(0, 255, (img_size, img_size, 3), dtype=np.uint8)\n    \n    for model_name in models:\n        print(f\"\\nğŸï¸ Benchmarking {model_name}...\")\n        model = YOLO(f'{model_name}.pt')\n        \n        # Warmup\n        for _ in range(num_warmup):\n            model(dummy_img, imgsz=img_size, verbose=False)\n        \n        # Benchmark\n        torch.cuda.synchronize() if torch.cuda.is_available() else None\n        times = []\n        \n        for _ in range(num_runs):\n            start = time.perf_counter()\n            model(dummy_img, imgsz=img_size, verbose=False)\n            torch.cuda.synchronize() if torch.cuda.is_available() else None\n            times.append(time.perf_counter() - start)\n        \n        avg_time = np.mean(times) * 1000\n        results[model_name] = {\n            'avg_time_ms': avg_time,\n            'fps': 1000 / avg_time,\n            'params': yolov8_models[model_name]['params'],\n            'mAP': yolov8_models[model_name]['mAP']\n        }\n        \n        print(f\"   âœ… {avg_time:.1f}ms ({1000/avg_time:.1f} FPS)\")\n        \n        # Cleanup\n        del model\n        torch.cuda.empty_cache()\n    \n    return results\n\nall_benchmarks = benchmark_all_models()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize benchmark results\n",
    "def plot_benchmarks(benchmarks: Dict):\n",
    "    \"\"\"Create benchmark visualization.\"\"\"\n",
    "    models = list(benchmarks.keys())\n",
    "    fps = [benchmarks[m]['fps'] for m in models]\n",
    "    mAP = [benchmarks[m]['mAP'] for m in models]\n",
    "    params = [benchmarks[m]['params'] for m in models]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(models)))\n",
    "    \n",
    "    # FPS comparison\n",
    "    bars = axes[0].bar(models, fps, color=colors)\n",
    "    axes[0].set_ylabel('Frames Per Second')\n",
    "    axes[0].set_title('âš¡ Speed (Higher is Better)')\n",
    "    for bar, f in zip(bars, fps):\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                    f'{f:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # mAP comparison\n",
    "    bars = axes[1].bar(models, mAP, color=colors)\n",
    "    axes[1].set_ylabel('mAP@0.5 (%)')\n",
    "    axes[1].set_title('ğŸ¯ Accuracy (Higher is Better)')\n",
    "    for bar, m in zip(bars, mAP):\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                    f'{m:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Speed vs Accuracy scatter\n",
    "    scatter = axes[2].scatter(fps, mAP, c=params, s=[p*20 for p in params],\n",
    "                              cmap='viridis', alpha=0.7)\n",
    "    for m, f, a in zip(models, fps, mAP):\n",
    "        axes[2].annotate(m, (f, a), textcoords='offset points', \n",
    "                        xytext=(5, 5), fontsize=9)\n",
    "    axes[2].set_xlabel('FPS')\n",
    "    axes[2].set_ylabel('mAP@0.5 (%)')\n",
    "    axes[2].set_title('ğŸ“Š Speed vs Accuracy Trade-off')\n",
    "    cbar = plt.colorbar(scatter, ax=axes[2])\n",
    "    cbar.set_label('Parameters (M)')\n",
    "    \n",
    "    plt.suptitle('ğŸï¸ YOLOv8 Model Comparison on DGX Spark', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_benchmarks(all_benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š DGX SPARK BENCHMARK SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Model':<12} {'Parameters':>12} {'mAP@0.5':>12} {'Time (ms)':>12} {'FPS':>12}\")\n",
    "print(\"-\"*70)\n",
    "for name, data in all_benchmarks.items():\n",
    "    print(f\"{name:<12} {data['params']:>10.1f}M {data['mAP']:>11.1f}% {data['avg_time_ms']:>11.1f} {data['fps']:>11.1f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Recommendations\n",
    "fastest = max(all_benchmarks, key=lambda x: all_benchmarks[x]['fps'])\n",
    "most_accurate = max(all_benchmarks, key=lambda x: all_benchmarks[x]['mAP'])\n",
    "\n",
    "print(f\"\\nğŸ’¡ Recommendations:\")\n",
    "print(f\"   - For real-time video (30+ FPS): Use {fastest}\")\n",
    "print(f\"   - For best accuracy: Use {most_accurate}\")\n",
    "print(f\"   - For balanced performance: Use yolov8s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Batch Processing\n",
    "\n",
    "Process multiple images efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(model, image_paths: List[str], batch_size: int = 4):\n",
    "    \"\"\"\n",
    "    Process images in batches for efficiency.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch = image_paths[i:i+batch_size]\n",
    "        results = model(batch, verbose=False)\n",
    "        all_results.extend(results)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Demo with both sample images\n",
    "image_paths = [str(sample_dir / 'street.jpg'), str(sample_dir / 'zidane.jpg')]\n",
    "model = YOLO('yolov8s.pt')\n",
    "\n",
    "start = time.time()\n",
    "batch_results = process_batch(model, image_paths * 5)  # Process 10 images\n",
    "batch_time = time.time() - start\n",
    "\n",
    "print(f\"\\nâš¡ Batch Processing:\")\n",
    "print(f\"   Images processed: {len(batch_results)}\")\n",
    "print(f\"   Total time: {batch_time:.2f}s\")\n",
    "print(f\"   Throughput: {len(batch_results)/batch_time:.1f} images/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Custom Detection Filtering\n",
    "\n",
    "Sometimes you only care about specific classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all COCO classes\n",
    "print(\"ğŸ“‹ All COCO Classes:\")\n",
    "print(\"=\"*60)\n",
    "for i, name in model.names.items():\n",
    "    print(f\"{i:>3}: {name:<20}\", end='')\n",
    "    if (i + 1) % 4 == 0:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_specific_classes(image_path: str, classes: List[str]):\n",
    "    \"\"\"\n",
    "    Detect only specific classes.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to image\n",
    "        classes: List of class names to detect\n",
    "    \"\"\"\n",
    "    # Get class indices\n",
    "    class_to_idx = {v: k for k, v in model.names.items()}\n",
    "    class_indices = [class_to_idx[c] for c in classes if c in class_to_idx]\n",
    "    \n",
    "    print(f\"ğŸ” Looking for: {classes}\")\n",
    "    print(f\"   Class indices: {class_indices}\")\n",
    "    \n",
    "    # Run detection with class filter\n",
    "    results = model(image_path, classes=class_indices, verbose=False)\n",
    "    \n",
    "    # Visualize\n",
    "    annotated = results[0].plot()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(annotated[:, :, ::-1])\n",
    "    plt.title(f\"Detecting: {', '.join(classes)} ({len(results[0].boxes)} found)\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Detect only people\n",
    "detect_specific_classes(str(sample_dir / 'street.jpg'), ['person', 'car', 'bus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ‹ Try It Yourself\n",
    "\n",
    "1. Download an image from the internet and run detection on it\n",
    "2. Filter for specific objects relevant to your use case\n",
    "3. Compare YOLOv8n vs YOLOv8x on the same image\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Hint</summary>\n",
    "\n",
    "```python\n",
    "# Download image\n",
    "url = \"your_image_url\"\n",
    "urllib.request.urlretrieve(url, \"my_image.jpg\")\n",
    "\n",
    "# Detect\n",
    "results = model(\"my_image.jpg\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš ï¸ Common Mistakes\n",
    "\n",
    "### Mistake 1: Wrong input format\n",
    "\n",
    "```python\n",
    "# âŒ Wrong: Normalized tensor input\n",
    "img_tensor = transforms.ToTensor()(img)  # Values 0-1\n",
    "results = model(img_tensor)  # May give wrong results!\n",
    "\n",
    "# âœ… Right: Use raw image (PIL, numpy, or path)\n",
    "results = model(\"image.jpg\")  # Path\n",
    "results = model(np.array(img))  # NumPy array (0-255)\n",
    "```\n",
    "**Why:** YOLOv8 handles preprocessing internally.\n",
    "\n",
    "### Mistake 2: Ignoring confidence threshold\n",
    "\n",
    "```python\n",
    "# âŒ Wrong: Too low threshold (many false positives)\n",
    "results = model(img, conf=0.1)\n",
    "\n",
    "# âœ… Right: Appropriate threshold for your use case\n",
    "results = model(img, conf=0.5)  # Higher precision\n",
    "```\n",
    "**Why:** Low threshold gives many false positives. Tune for your needs.\n",
    "\n",
    "### Mistake 3: Not using GPU\n",
    "\n",
    "```python\n",
    "# âŒ Wrong: CPU inference (slow!)\n",
    "model = YOLO('yolov8s.pt')\n",
    "# No explicit device set, may default to CPU\n",
    "\n",
    "# âœ… Right: Ensure GPU inference\n",
    "model = YOLO('yolov8s.pt')\n",
    "model.to('cuda')  # Or check model.device\n",
    "```\n",
    "**Why:** GPU is 10-100Ã— faster for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- âœ… Difference between classification, detection, and segmentation\n",
    "- âœ… Evolution from R-CNN to YOLO\n",
    "- âœ… Using YOLOv8 for object detection\n",
    "- âœ… Benchmarking detection speed on DGX Spark\n",
    "- âœ… Filtering detections by confidence and class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ Challenge (Optional)\n",
    "\n",
    "**Create a simple video processing pipeline:**\n",
    "\n",
    "1. Download a short video or use webcam\n",
    "2. Process frame-by-frame with YOLOv8\n",
    "3. Count unique objects over time\n",
    "4. Save annotated video\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Starting Code</summary>\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture('video.mp4')\n",
    "model = YOLO('yolov8n.pt')  # Use nano for speed\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    results = model(frame, verbose=False)\n",
    "    annotated = results[0].plot()\n",
    "    \n",
    "    cv2.imshow('Detection', annotated)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“– Further Reading\n",
    "\n",
    "- [Ultralytics YOLOv8 Documentation](https://docs.ultralytics.com/)\n",
    "- [YOLOv8 GitHub](https://github.com/ultralytics/ultralytics)\n",
    "- [Original YOLO Paper](https://arxiv.org/abs/1506.02640)\n",
    "- [COCO Dataset](https://cocodataset.org/)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… Cleanup complete!\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ’¾ GPU Memory Free: {torch.cuda.mem_get_info()[0] / 1e9:.1f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}