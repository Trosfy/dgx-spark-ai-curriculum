{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 2.2.5: Vision Transformer (ViT)\n\n**Module:** 2.2 - Computer Vision  \n**Time:** 3 hours  \n**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n\n---\n\n## üéØ Learning Objectives\n\nBy the end of this notebook, you will:\n- [ ] Understand how Vision Transformers adapt NLP techniques for images\n- [ ] Implement ViT from scratch, including patch embedding and self-attention\n- [ ] Train ViT on CIFAR-10\n- [ ] Compare ViT with CNN architectures\n\n---\n\n## üìö Prerequisites\n\n- Completed: Labs 2.2.1-2.2.4\n- Knowledge of: Transformers (from NLP), self-attention, CNNs\n\n---\n\n## üåç Real-World Context\n\n**Vision Transformers are revolutionizing computer vision:**\n\n- üñºÔ∏è **State-of-the-art**: ViT and variants (Swin, DeiT) now top ImageNet leaderboards\n- ü§ñ **Unified architecture**: Same transformer can process text, images, audio\n- üß† **Foundation models**: CLIP, DALL-E, Stable Diffusion all use ViT variants\n- üìä **Scalability**: ViT scales better than CNNs with more data and compute"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What is a Vision Transformer?\n",
    "\n",
    "> **Imagine you're reading a book with pictures...**\n",
    ">\n",
    "> When you read, you don't look at every letter individually. You see words and sentences as chunks.\n",
    ">\n",
    "> **Vision Transformers treat images the same way:**\n",
    "> 1. **Cut the image into patches** (like words in a sentence)\n",
    "> 2. **Arrange patches in a sequence** (like words in order)\n",
    "> 3. **Let patches \"talk\" to each other** through attention (like understanding context)\n",
    ">\n",
    "> The magic: Instead of looking at local neighborhoods (like CNNs), every patch can directly attend to every other patch!\n",
    "\n",
    "### From CNN to ViT\n",
    "\n",
    "```\n",
    "CNN approach:                           ViT approach:\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ üîç ‚Üí üîç ‚Üí üîç‚îÇ  Local filters         ‚îÇ ‚ñ¢ ‚ñ¢ ‚ñ¢ ‚ñ¢    ‚îÇ  Cut into patches\n",
    "‚îÇ üîç ‚Üí üîç ‚Üí üîç‚îÇ  slide across          ‚îÇ ‚ñ¢ ‚ñ¢ ‚ñ¢ ‚ñ¢    ‚îÇ  \n",
    "‚îÇ üîç ‚Üí üîç ‚Üí üîç‚îÇ  the image             ‚îÇ ‚ñ¢ ‚ñ¢ ‚ñ¢ ‚ñ¢    ‚îÇ  \n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îÇ ‚ñ¢ ‚ñ¢ ‚ñ¢ ‚ñ¢    ‚îÇ  \n",
    "                                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ                                    ‚îÇ\n",
    "         ‚ñº                                    ‚ñº\n",
    "   Hierarchical                        [CLS] P1 P2 P3 ... P16\n",
    "   feature maps                              ‚Üì\n",
    "         ‚îÇ                             Transformer Layers\n",
    "         ‚ñº                                   ‚Üì\n",
    "      Output                           Classify from [CLS]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Tuple, Optional, Dict, List\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import math\n",
    "\n",
    "# DGX Spark optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è  Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Patch Embedding\n",
    "\n",
    "The first step is converting an image into a sequence of patch embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert image into sequence of patch embeddings.\n",
    "    \n",
    "    Input:  [B, C, H, W]  - Batch of images\n",
    "    Output: [B, N, D]     - Sequence of N patch embeddings of dimension D\n",
    "    \n",
    "    where N = (H * W) / (patch_size^2)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        in_channels: int = 3,\n",
    "        embed_dim: int = 768\n",
    "    ):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # Linear projection of flattened patches\n",
    "        # Using Conv2d with kernel_size=patch_size is equivalent to:\n",
    "        # 1. Split image into patches\n",
    "        # 2. Flatten each patch\n",
    "        # 3. Apply linear projection\n",
    "        self.projection = nn.Conv2d(\n",
    "            in_channels,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, C, H, W]\n",
    "        Returns:\n",
    "            [B, num_patches, embed_dim]\n",
    "        \"\"\"\n",
    "        # Project and reshape: [B, embed_dim, H/P, W/P] -> [B, embed_dim, num_patches]\n",
    "        x = self.projection(x)\n",
    "        x = x.flatten(2)  # [B, embed_dim, num_patches]\n",
    "        x = x.transpose(1, 2)  # [B, num_patches, embed_dim]\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test patch embedding\n",
    "patch_embed = PatchEmbedding(img_size=32, patch_size=4, embed_dim=256)\n",
    "dummy_img = torch.randn(1, 3, 32, 32)\n",
    "patches = patch_embed(dummy_img)\n",
    "\n",
    "print(f\"üìä Patch Embedding:\")\n",
    "print(f\"   Input image:    {dummy_img.shape}\")\n",
    "print(f\"   Patch size:     {patch_embed.patch_size}\")\n",
    "print(f\"   Number patches: {patch_embed.num_patches} ({32//4} √ó {32//4})\")\n",
    "print(f\"   Output:         {patches.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_patches(img: torch.Tensor, patch_size: int = 4):\n",
    "    \"\"\"\n",
    "    Visualize how an image is split into patches.\n",
    "    \"\"\"\n",
    "    img_np = img.squeeze().permute(1, 2, 0).numpy()\n",
    "    H, W = img_np.shape[:2]\n",
    "    num_patches_h = H // patch_size\n",
    "    num_patches_w = W // patch_size\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Original image with grid\n",
    "    axes[0].imshow(img_np.clip(0, 1))\n",
    "    for i in range(num_patches_h + 1):\n",
    "        axes[0].axhline(y=i*patch_size - 0.5, color='r', linewidth=0.5)\n",
    "    for j in range(num_patches_w + 1):\n",
    "        axes[0].axvline(x=j*patch_size - 0.5, color='r', linewidth=0.5)\n",
    "    axes[0].set_title(f'Image with {num_patches_h}√ó{num_patches_w} patch grid')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Individual patches\n",
    "    num_show = min(16, num_patches_h * num_patches_w)\n",
    "    patches_grid = np.zeros((4 * patch_size, 4 * patch_size, 3))\n",
    "    \n",
    "    for idx in range(num_show):\n",
    "        i = idx // num_patches_w\n",
    "        j = idx % num_patches_w\n",
    "        patch = img_np[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n",
    "        \n",
    "        grid_i = idx // 4\n",
    "        grid_j = idx % 4\n",
    "        patches_grid[grid_i*patch_size:(grid_i+1)*patch_size,\n",
    "                    grid_j*patch_size:(grid_j+1)*patch_size] = patch\n",
    "    \n",
    "    axes[1].imshow(patches_grid.clip(0, 1))\n",
    "    axes[1].set_title(f'First 16 patches (each {patch_size}√ó{patch_size})')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.suptitle('üß© Image to Patch Conversion', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load a sample image\n",
    "transform = transforms.ToTensor()\n",
    "dataset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform)\n",
    "sample_img, label = dataset[0]\n",
    "\n",
    "visualize_patches(sample_img.unsqueeze(0), patch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Multi-Head Self-Attention\n",
    "\n",
    "### üßí ELI5: Self-Attention\n",
    "\n",
    "> **Imagine you're in a classroom and need to answer a question...**\n",
    ">\n",
    "> You look around and decide who to pay attention to:\n",
    "> - **Query (Q)**: \"I need information about X\"\n",
    "> - **Key (K)**: Each classmate holds up a sign: \"I know about Y\"\n",
    "> - **Value (V)**: The actual information each classmate has\n",
    ">\n",
    "> You compare your Query to everyone's Keys, then take a weighted average of their Values!\n",
    ">\n",
    "> **Multi-head**: Instead of asking one question, you ask 8 different questions in parallel (like having 8 TAs helping you). Each head learns to look for different things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention mechanism.\n",
    "    \n",
    "    Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 768,\n",
    "        num_heads: int = 12,\n",
    "        dropout: float = 0.0\n",
    "    ):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        \n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5  # 1/sqrt(d_k)\n",
    "        \n",
    "        # Combined QKV projection (more efficient)\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, N, D] - Input sequence\n",
    "        Returns:\n",
    "            output: [B, N, D] - Attended sequence\n",
    "            attention: [B, H, N, N] - Attention weights\n",
    "        \"\"\"\n",
    "        B, N, D = x.shape\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        qkv = self.qkv(x)  # [B, N, 3*D]\n",
    "        qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, B, H, N, head_dim]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Each: [B, H, N, head_dim]\n",
    "        \n",
    "        # Attention scores: [B, H, N, N]\n",
    "        attention = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attention = attention.softmax(dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = attention @ v  # [B, H, N, head_dim]\n",
    "        out = out.transpose(1, 2).reshape(B, N, D)  # [B, N, D]\n",
    "        out = self.proj(out)\n",
    "        \n",
    "        return out, attention\n",
    "\n",
    "\n",
    "# Test attention\n",
    "mhsa = MultiHeadSelfAttention(embed_dim=256, num_heads=8)\n",
    "dummy_seq = torch.randn(1, 64, 256)  # [B, N, D]\n",
    "out, attn = mhsa(dummy_seq)\n",
    "\n",
    "print(f\"üìä Multi-Head Self-Attention:\")\n",
    "print(f\"   Input:    {dummy_seq.shape}\")\n",
    "print(f\"   Output:   {out.shape}\")\n",
    "print(f\"   Attention weights: {attn.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(attn_weights: torch.Tensor, num_heads: int = 4):\n",
    "    \"\"\"\n",
    "    Visualize attention patterns from different heads.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, num_heads//2, figsize=(12, 8))\n",
    "    \n",
    "    for idx, ax in enumerate(axes.flat):\n",
    "        if idx >= num_heads:\n",
    "            break\n",
    "        \n",
    "        attn = attn_weights[0, idx].detach().numpy()  # [N, N]\n",
    "        im = ax.imshow(attn, cmap='viridis')\n",
    "        ax.set_title(f'Head {idx+1}', fontsize=10)\n",
    "        ax.set_xlabel('Key position')\n",
    "        ax.set_ylabel('Query position')\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.suptitle('üîç Attention Patterns Across Heads', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_attention(attn, num_heads=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Transformer Block\n",
    "\n",
    "A Transformer block combines self-attention with a feedforward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"MLP (Feed-Forward Network) in Transformer block.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 768,\n",
    "        hidden_dim: int = 3072,\n",
    "        dropout: float = 0.0\n",
    "    ):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.act = nn.GELU()  # ViT uses GELU activation\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder block.\n",
    "    \n",
    "    Structure:\n",
    "        x ‚Üí LayerNorm ‚Üí MHSA ‚Üí + ‚Üí LayerNorm ‚Üí MLP ‚Üí +\n",
    "            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                (residual)              (residual)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 768,\n",
    "        num_heads: int = 12,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        dropout: float = 0.0\n",
    "    ):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, int(embed_dim * mlp_ratio), dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Self-attention with residual\n",
    "        attn_out, _ = self.attn(self.norm1(x))\n",
    "        x = x + attn_out\n",
    "        \n",
    "        # MLP with residual\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Complete Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer (ViT) for image classification.\n",
    "    \n",
    "    Original paper: \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"\n",
    "    by Alexey Dosovitskiy et al., 2020\n",
    "    \n",
    "    Architecture:\n",
    "        Image ‚Üí Patch Embed ‚Üí [CLS] + Patches + Pos Embed ‚Üí Transformer Layers ‚Üí [CLS] ‚Üí MLP Head\n",
    "    \n",
    "    Args:\n",
    "        img_size: Input image size\n",
    "        patch_size: Size of each patch\n",
    "        in_channels: Number of input channels\n",
    "        num_classes: Number of output classes\n",
    "        embed_dim: Embedding dimension\n",
    "        depth: Number of transformer layers\n",
    "        num_heads: Number of attention heads\n",
    "        mlp_ratio: MLP hidden dim = embed_dim * mlp_ratio\n",
    "        dropout: Dropout rate\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        in_channels: int = 3,\n",
    "        num_classes: int = 1000,\n",
    "        embed_dim: int = 768,\n",
    "        depth: int = 12,\n",
    "        num_heads: int = 12,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        dropout: float = 0.0\n",
    "    ):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        \n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        \n",
    "        # Learnable [CLS] token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        # Learnable positional embeddings\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n",
    "        \n",
    "        # Dropout after embedding\n",
    "        self.pos_drop = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Classification head\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights like in the original ViT paper.\"\"\"\n",
    "        # Initialize cls_token and pos_embed\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        \n",
    "        # Initialize other layers\n",
    "        self.apply(self._init_module_weights)\n",
    "    \n",
    "    def _init_module_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.zeros_(m.bias)\n",
    "            nn.init.ones_(m.weight)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, C, H, W] - Input images\n",
    "        Returns:\n",
    "            [B, num_classes] - Class logits\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Patch embedding: [B, num_patches, embed_dim]\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Prepend [CLS] token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # [B, 1, embed_dim]\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # [B, num_patches + 1, embed_dim]\n",
    "        \n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Final norm\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Classification from [CLS] token\n",
    "        cls_output = x[:, 0]  # [B, embed_dim]\n",
    "        logits = self.head(cls_output)  # [B, num_classes]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "# Test the model\n",
    "model = VisionTransformer(\n",
    "    img_size=32,\n",
    "    patch_size=4,\n",
    "    num_classes=10,\n",
    "    embed_dim=256,\n",
    "    depth=6,\n",
    "    num_heads=8\n",
    ")\n",
    "\n",
    "dummy_img = torch.randn(1, 3, 32, 32)\n",
    "output = model(dummy_img)\n",
    "\n",
    "print(f\"üìä Vision Transformer (ViT-Tiny for CIFAR-10):\")\n",
    "print(f\"   Input shape:  {dummy_img.shape}\")\n",
    "print(f\"   Output shape: {output.shape}\")\n",
    "print(f\"   Parameters:   {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"\\n   Configuration:\")\n",
    "print(f\"   - Patch size:    4√ó4\")\n",
    "print(f\"   - Num patches:   {model.num_patches} ({32//4}√ó{32//4})\")\n",
    "print(f\"   - Embed dim:     256\")\n",
    "print(f\"   - Num layers:    6\")\n",
    "print(f\"   - Num heads:     8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Training ViT on CIFAR-10\n",
    "\n",
    "Training ViT from scratch is tricky - they typically need lots of data or strong regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Data loading with strong augmentation (important for ViT!)\n# NOTE: When using num_workers > 0 in Docker, use --ipc=host flag\n# Example: docker run --gpus all --ipc=host ...\n\ndef get_cifar10_loaders_vit(batch_size: int = 128) -> Tuple[DataLoader, DataLoader]:\n    \"\"\"\n    Create CIFAR-10 loaders with strong augmentation for ViT.\n    \n    ViT is data-hungry, so we use aggressive augmentation.\n    \n    Args:\n        batch_size: Batch size. DGX Spark can handle 256+ due to 128GB memory.\n    \"\"\"\n    # Strong augmentation for training\n    train_transform = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandAugment(num_ops=2, magnitude=9),  # Strong augmentation\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n        transforms.RandomErasing(p=0.25),  # Cutout-like\n    ])\n    \n    test_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n    \n    train_dataset = torchvision.datasets.CIFAR10(\n        root='../data', train=True, download=True, transform=train_transform\n    )\n    test_dataset = torchvision.datasets.CIFAR10(\n        root='../data', train=False, download=True, transform=test_transform\n    )\n    \n    # num_workers=4 requires --ipc=host when running in Docker\n    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n                              shuffle=True, num_workers=4, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size,\n                             shuffle=False, num_workers=4, pin_memory=True)\n    \n    return train_loader, test_loader\n\ntrain_loader, test_loader = get_cifar10_loaders_vit(batch_size=128)\nprint(f\"üìä Dataset loaded with strong augmentation\")\nprint(f\"   Training:   {len(train_loader.dataset):,} images\")\nprint(f\"   Test:       {len(test_loader.dataset):,} images\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vit(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    epochs: int = 50,\n",
    "    lr: float = 0.001,\n",
    "    weight_decay: float = 0.1,\n",
    "    warmup_epochs: int = 5,\n",
    "    device: torch.device = device\n",
    ") -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Train Vision Transformer with warmup and cosine annealing.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Use label smoothing (helps regularization)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    \n",
    "    # AdamW optimizer (important for transformers!)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Warmup + cosine annealing scheduler\n",
    "    warmup_scheduler = optim.lr_scheduler.LinearLR(\n",
    "        optimizer, start_factor=0.01, total_iters=warmup_epochs * len(train_loader)\n",
    "    )\n",
    "    cosine_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=(epochs - warmup_epochs) * len(train_loader)\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.SequentialLR(\n",
    "        optimizer, [warmup_scheduler, cosine_scheduler],\n",
    "        milestones=[warmup_epochs * len(train_loader)]\n",
    "    )\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping (important for transformers!)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{train_loss/total:.4f}',\n",
    "                'acc': f'{100.*correct/total:.1f}%',\n",
    "                'lr': f'{scheduler.get_last_lr()[0]:.6f}'\n",
    "            })\n",
    "        \n",
    "        history['train_loss'].append(train_loss / len(train_loader))\n",
    "        history['train_acc'].append(100. * correct / total)\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        test_loss, correct, total = 0, 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        history['test_loss'].append(test_loss / len(test_loader))\n",
    "        history['test_acc'].append(100. * correct / total)\n",
    "        \n",
    "        print(f\"   Test: Loss={history['test_loss'][-1]:.4f}, Acc={history['test_acc'][-1]:.1f}%\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ViT\n",
    "print(\"üèãÔ∏è Training Vision Transformer on CIFAR-10...\")\n",
    "print(\"=\"*50)\n",
    "print(\"This may take a while - ViT needs careful training!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "vit_model = VisionTransformer(\n",
    "    img_size=32,\n",
    "    patch_size=4,\n",
    "    num_classes=10,\n",
    "    embed_dim=256,\n",
    "    depth=6,\n",
    "    num_heads=8,\n",
    "    mlp_ratio=4.0,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "vit_history = train_vit(\n",
    "    vit_model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=20,  # Use more epochs (50+) for better results\n",
    "    lr=0.001,\n",
    "    weight_decay=0.1,\n",
    "    warmup_epochs=2\n",
    ")\n",
    "\n",
    "vit_time = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Training complete in {vit_time/60:.1f} minutes\")\n",
    "print(f\"   Best test accuracy: {max(vit_history['test_acc']):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(vit_history['train_loss'], label='Train', linewidth=2)\n",
    "axes[0].plot(vit_history['test_loss'], label='Test', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('üìâ Training and Test Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(vit_history['train_acc'], label='Train', linewidth=2)\n",
    "axes[1].plot(vit_history['test_acc'], label='Test', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('üìà Training and Test Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Vision Transformer Training on CIFAR-10', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Visualizing What ViT Learns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def visualize_positional_embeddings(model: VisionTransformer):\n    \"\"\"\n    Visualize the learned positional embeddings.\n    \n    Each patch position learns a unique embedding that encodes spatial information.\n    \"\"\"\n    # Check for sklearn dependency\n    try:\n        from sklearn.decomposition import PCA\n        has_sklearn = True\n    except ImportError:\n        print(\"‚ö†Ô∏è scikit-learn not installed. PCA visualization will be skipped.\")\n        print(\"   Install with: pip install scikit-learn\")\n        has_sklearn = False\n    \n    pos_embed = model.pos_embed[0, 1:].detach().cpu()  # Exclude [CLS]\n    \n    # Compute similarity between positional embeddings\n    pos_embed_norm = F.normalize(pos_embed, dim=-1)\n    similarity = pos_embed_norm @ pos_embed_norm.T\n    \n    num_patches = int(math.sqrt(pos_embed.shape[0]))\n    \n    # Adjust figure layout based on sklearn availability\n    num_cols = 3 if has_sklearn else 2\n    fig, axes = plt.subplots(1, num_cols, figsize=(5 * num_cols, 4))\n    \n    # Similarity matrix\n    im0 = axes[0].imshow(similarity.numpy(), cmap='viridis')\n    axes[0].set_title('Positional Embedding Similarity')\n    axes[0].set_xlabel('Patch position')\n    axes[0].set_ylabel('Patch position')\n    plt.colorbar(im0, ax=axes[0])\n    \n    # Show similarity to center patch\n    center_idx = (num_patches * num_patches) // 2\n    center_sim = similarity[center_idx].reshape(num_patches, num_patches).numpy()\n    im1 = axes[1].imshow(center_sim, cmap='RdBu_r', vmin=-1, vmax=1)\n    axes[1].set_title(f'Similarity to center patch (idx={center_idx})')\n    plt.colorbar(im1, ax=axes[1])\n    \n    # PCA visualization of embeddings (if sklearn available)\n    if has_sklearn:\n        pca = PCA(n_components=3)\n        pos_pca = pca.fit_transform(pos_embed.numpy())\n        pos_pca_img = pos_pca.reshape(num_patches, num_patches, 3)\n        pos_pca_img = (pos_pca_img - pos_pca_img.min()) / (pos_pca_img.max() - pos_pca_img.min())\n        \n        axes[2].imshow(pos_pca_img)\n        axes[2].set_title('Position Embeddings (PCA ‚Üí RGB)')\n        axes[2].axis('off')\n    \n    axes[0].set_xticks([])\n    axes[0].set_yticks([])\n    axes[1].axis('off')\n    \n    plt.suptitle('üß† What ViT Learns: Positional Embeddings', fontsize=14)\n    plt.tight_layout()\n    plt.show()\n\n# Visualize positional embeddings\nvisualize_positional_embeddings(vit_model)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_maps(model: VisionTransformer, image: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Visualize attention maps from the last transformer layer.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Hook to capture attention\n",
    "    attention_maps = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        _, attn = output\n",
    "        attention_maps.append(attn.detach().cpu())\n",
    "    \n",
    "    # Register hook on last attention layer\n",
    "    hook = model.blocks[-1].attn.register_forward_hook(hook_fn)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _ = model(image.unsqueeze(0).to(device))\n",
    "    \n",
    "    hook.remove()\n",
    "    \n",
    "    if not attention_maps:\n",
    "        print(\"No attention captured\")\n",
    "        return\n",
    "    \n",
    "    # Get attention from [CLS] token to patches\n",
    "    attn = attention_maps[0][0]  # [num_heads, num_tokens, num_tokens]\n",
    "    cls_attn = attn[:, 0, 1:]  # [num_heads, num_patches]\n",
    "    \n",
    "    num_patches = int(math.sqrt(cls_attn.shape[1]))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    \n",
    "    # Original image\n",
    "    img_np = image.permute(1, 2, 0).numpy()\n",
    "    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "    \n",
    "    axes[0, 0].imshow(img_np)\n",
    "    axes[0, 0].set_title('Input Image')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Attention heads\n",
    "    for idx, ax in enumerate(axes.flat[1:]):\n",
    "        if idx >= cls_attn.shape[0]:\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "        \n",
    "        attn_map = cls_attn[idx].reshape(num_patches, num_patches).numpy()\n",
    "        im = ax.imshow(attn_map, cmap='hot')\n",
    "        ax.set_title(f'Head {idx+1}')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('üëÅÔ∏è Attention from [CLS] Token to Image Patches', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize on a sample\n",
    "sample_img, _ = test_loader.dataset[0]\n",
    "visualize_attention_maps(vit_model, sample_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself\n",
    "\n",
    "1. **Try different patch sizes**: 2√ó2, 4√ó4, 8√ó8 - how does this affect accuracy and speed?\n",
    "2. **Experiment with depth**: 4 vs 8 vs 12 layers\n",
    "3. **Compare with ResNet**: Train a ResNet-18 and compare\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "Smaller patches = more tokens = more compute but potentially better accuracy:\n",
    "\n",
    "```python\n",
    "# Patch size 2 (256 patches for 32√ó32 image)\n",
    "model_p2 = VisionTransformer(patch_size=2, ...)\n",
    "\n",
    "# Patch size 8 (16 patches for 32√ó32 image)\n",
    "model_p8 = VisionTransformer(patch_size=8, ...)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Training without warmup\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: No warmup (training can diverge)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ‚úÖ Right: Linear warmup for transformers\n",
    "warmup_scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=0.01, total_iters=warmup_steps)\n",
    "```\n",
    "**Why:** Transformers are sensitive to learning rate at the start. Warmup stabilizes training.\n",
    "\n",
    "### Mistake 2: Not using gradient clipping\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: No gradient clipping\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# ‚úÖ Right: Clip gradients\n",
    "loss.backward()\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "optimizer.step()\n",
    "```\n",
    "**Why:** Attention can have large gradients, especially early in training.\n",
    "\n",
    "### Mistake 3: Insufficient data/augmentation\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Weak augmentation\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# ‚úÖ Right: Strong augmentation for ViT\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandAugment(num_ops=2, magnitude=9),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.25),\n",
    "])\n",
    "```\n",
    "**Why:** ViT lacks inductive biases of CNNs. It needs more data or stronger augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How ViT converts images into sequences via patch embedding\n",
    "- ‚úÖ Multi-head self-attention mechanism\n",
    "- ‚úÖ Complete ViT architecture with [CLS] token and positional embeddings\n",
    "- ‚úÖ Training techniques for transformers (warmup, gradient clipping, strong augmentation)\n",
    "- ‚úÖ Visualizing attention patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Implement DeiT (Data-efficient Image Transformer) improvements:**\n",
    "\n",
    "DeiT adds a \"distillation token\" that learns from a CNN teacher:\n",
    "\n",
    "1. Train a ResNet teacher\n",
    "2. Add a distillation token to ViT (similar to [CLS])\n",
    "3. Train ViT to match both:\n",
    "   - True labels (cross-entropy loss)\n",
    "   - Teacher predictions (distillation loss)\n",
    "\n",
    "<details>\n",
    "<summary>üí° Starting Code</summary>\n",
    "\n",
    "```python\n",
    "class DeiT(VisionTransformer):\n",
    "    def __init__(self, ...):\n",
    "        super().__init__(...)\n",
    "        # Add distillation token\n",
    "        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        # New position embedding for +1 token\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 2, embed_dim))\n",
    "        # Distillation head\n",
    "        self.dist_head = nn.Linear(embed_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # ... similar to ViT but includes dist_token\n",
    "        cls_output = x[:, 0]\n",
    "        dist_output = x[:, 1]\n",
    "        return self.head(cls_output), self.dist_head(dist_output)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [ViT Paper](https://arxiv.org/abs/2010.11929) - Original Vision Transformer\n",
    "- [DeiT Paper](https://arxiv.org/abs/2012.12877) - Data-efficient training\n",
    "- [Swin Transformer](https://arxiv.org/abs/2103.14030) - Hierarchical ViT\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - Great visual explanation\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "\n",
    "del vit_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üíæ GPU Memory Free: {torch.cuda.mem_get_info()[0] / 1e9:.1f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}