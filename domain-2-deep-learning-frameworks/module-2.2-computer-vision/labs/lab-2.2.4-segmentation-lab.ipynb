{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 2.2.4: Segmentation Lab\n\n**Module:** 2.2 - Computer Vision  \n**Time:** 3 hours  \n**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê\n\n---\n\n## üéØ Learning Objectives\n\nBy the end of this notebook, you will:\n- [ ] Understand semantic vs instance vs panoptic segmentation\n- [ ] Implement the U-Net architecture from scratch\n- [ ] Train a segmentation model on real data\n- [ ] Evaluate segmentation using IoU (Intersection over Union)\n\n---\n\n## üìö Prerequisites\n\n- Completed: Labs 2.2.1-2.2.3\n- Knowledge of: CNNs, skip connections, encoder-decoder architectures\n\n---\n\n## üåç Real-World Context\n\n**Image segmentation is crucial for:**\n\n- üè• **Medical imaging**: Outlining tumors, organs, cell structures\n- üöó **Autonomous driving**: Understanding road, lane, sidewalk boundaries\n- üõ∞Ô∏è **Satellite imagery**: Land use classification, disaster assessment\n- üì∏ **Photo editing**: Background removal, portrait mode\n- ü§ñ **Robotics**: Understanding what can be grasped or navigated"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What is Image Segmentation?\n",
    "\n",
    "> **Imagine coloring in a coloring book...** üé®\n",
    ">\n",
    "> - **Classification**: \"This coloring book page has a cat\" ‚úì/‚úó\n",
    "> - **Detection**: \"There's a cat shape starting here and ending there\" ‚¨ú\n",
    "> - **Segmentation**: \"Color every pixel that belongs to the cat\" üé®\n",
    ">\n",
    "> Segmentation gives you the **exact shape** of every object, pixel by pixel!\n",
    "\n",
    "### Three Types of Segmentation\n",
    "\n",
    "```\n",
    "Input Image:          Semantic:             Instance:             Panoptic:\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ    üê±  üê±    ‚îÇ      ‚îÇ    ‚ñì‚ñì  ‚ñì‚ñì    ‚îÇ      ‚îÇ    ‚ñà‚ñà  ‚ñí‚ñí    ‚îÇ      ‚îÇ    ‚ñà‚ñà  ‚ñí‚ñí    ‚îÇ\n",
    "‚îÇ              ‚îÇ  ‚Üí   ‚îÇ              ‚îÇ  ‚Üí   ‚îÇ              ‚îÇ  ‚Üí   ‚îÇ              ‚îÇ\n",
    "‚îÇ  Background  ‚îÇ      ‚îÇ  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  ‚îÇ      ‚îÇ  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  ‚îÇ      ‚îÇ  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                      All cats = same       Cat 1 ‚â† Cat 2         Cats + Background\n",
    "                      color (class)         (different IDs)       all labeled\n",
    "```\n",
    "\n",
    "- **Semantic**: All pixels of same CLASS get same label\n",
    "- **Instance**: Each OBJECT gets unique ID (separates individual cats)\n",
    "- **Panoptic**: Semantic + Instance combined (the full picture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# DGX Spark optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è  Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding U-Net Architecture\n",
    "\n",
    "### üßí ELI5: U-Net\n",
    "\n",
    "> **Imagine describing a picture to a friend over the phone...**\n",
    ">\n",
    "> 1. **First**, you give a high-level summary: \"It's a beach scene\"\n",
    "> 2. **Then**, you add details: \"There's an umbrella on the left\"\n",
    "> 3. **Finally**, you get specific: \"The umbrella is red with white stripes, positioned at...\"\n",
    ">\n",
    "> **U-Net works similarly:**\n",
    "> - **Encoder (going down)**: Captures \"what\" is in the image (abstract features)\n",
    "> - **Decoder (going up)**: Recovers \"where\" things are (spatial details)\n",
    "> - **Skip connections**: Pass spatial details directly to help localization!\n",
    "\n",
    "### U-Net Architecture\n",
    "\n",
    "```\n",
    "Input (256√ó256√ó3)\n",
    "    ‚îÇ\n",
    "    ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Conv√ó2  ‚îÇ 64 filters                                           ‚îÇ Skip 1\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                      ‚îÇ\n",
    "     ‚ñº MaxPool                                                   ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ\n",
    "‚îÇ Conv√ó2  ‚îÇ 128 filters                                  ‚îÇ Skip 2 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                              ‚îÇ        ‚îÇ\n",
    "     ‚ñº MaxPool                                           ‚îÇ        ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ        ‚îÇ\n",
    "‚îÇ Conv√ó2  ‚îÇ 256 filters                     ‚îÇ Skip 3     ‚îÇ        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                 ‚îÇ             ‚îÇ        ‚îÇ\n",
    "     ‚ñº MaxPool                              ‚îÇ             ‚îÇ        ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ             ‚îÇ        ‚îÇ\n",
    "‚îÇ Conv√ó2  ‚îÇ 512 filters        ‚îÇ Skip 4     ‚îÇ             ‚îÇ        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ             ‚îÇ             ‚îÇ        ‚îÇ\n",
    "     ‚ñº MaxPool                 ‚îÇ             ‚îÇ             ‚îÇ        ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ             ‚îÇ             ‚îÇ        ‚îÇ\n",
    "‚îÇ Conv√ó2  ‚îÇ 1024 (bottleneck)  ‚îÇ             ‚îÇ             ‚îÇ        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ             ‚îÇ             ‚îÇ        ‚îÇ\n",
    "     ‚ñº UpConv                  ‚îÇ             ‚îÇ             ‚îÇ        ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ             ‚îÇ        ‚îÇ\n",
    "‚îÇ Conv√ó2  ‚îÇ + Concatenate Skip 4             ‚îÇ             ‚îÇ        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                 ‚îÇ             ‚îÇ        ‚îÇ\n",
    "     ‚ñº UpConv                               ‚îÇ             ‚îÇ        ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ        ‚îÇ\n",
    "‚îÇ Conv√ó2  ‚îÇ + Concatenate Skip 3                          ‚îÇ        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                              ‚îÇ        ‚îÇ\n",
    "     ‚ñº UpConv                                            ‚îÇ        ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ\n",
    "‚îÇ Conv√ó2  ‚îÇ + Concatenate Skip 2                                  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                      ‚îÇ\n",
    "     ‚ñº UpConv                                                    ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "‚îÇ Conv√ó2  ‚îÇ + Concatenate Skip 1\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "     ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Conv 1√ó1‚îÇ ‚Üí num_classes channels\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "    ‚îÇ\n",
    "    ‚ñº\n",
    "Output (256√ó256√ónum_classes)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Double convolution block: (Conv ‚Üí BN ‚Üí ReLU) √ó 2\n",
    "    \n",
    "    This is the basic building block of U-Net.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downsampling: MaxPool ‚Üí DoubleConv\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super(Down, self).__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upsampling: UpConv ‚Üí Concat skip ‚Üí DoubleConv\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, bilinear: bool = True):\n",
    "        super(Up, self).__init__()\n",
    "        \n",
    "        if bilinear:\n",
    "            # Use bilinear upsampling (less parameters)\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "        else:\n",
    "            # Use transposed convolution (learnable upsampling)\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"x1: from decoder, x2: skip connection from encoder\"\"\"\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # Handle size mismatch (input might not be perfectly divisible by 2)\n",
    "        diff_y = x2.size()[2] - x1.size()[2]\n",
    "        diff_x = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diff_x // 2, diff_x - diff_x // 2,\n",
    "                       diff_y // 2, diff_y - diff_y // 2])\n",
    "        \n",
    "        # Concatenate skip connection\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net for semantic segmentation.\n",
    "    \n",
    "    Original paper: \"U-Net: Convolutional Networks for Biomedical Image Segmentation\"\n",
    "    by Olaf Ronneberger et al., 2015\n",
    "    \n",
    "    Key innovations:\n",
    "    - Encoder-decoder with skip connections\n",
    "    - Works with limited training data\n",
    "    - Precise localization through skip connections\n",
    "    \n",
    "    Args:\n",
    "        n_channels: Number of input channels (3 for RGB)\n",
    "        n_classes: Number of output classes\n",
    "        bilinear: Use bilinear upsampling (vs transposed conv)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_channels: int = 3, n_classes: int = 21, bilinear: bool = True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "        \n",
    "        # Encoder (downsampling path)\n",
    "        self.inc = DoubleConv(n_channels, 64)    # Initial conv\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)   # Bottleneck\n",
    "        \n",
    "        # Decoder (upsampling path)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        \n",
    "        # Output layer (1√ó1 conv to reduce to num_classes)\n",
    "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Encoder\n",
    "        x1 = self.inc(x)      # [B, 64, H, W]\n",
    "        x2 = self.down1(x1)   # [B, 128, H/2, W/2]\n",
    "        x3 = self.down2(x2)   # [B, 256, H/4, W/4]\n",
    "        x4 = self.down3(x3)   # [B, 512, H/8, W/8]\n",
    "        x5 = self.down4(x4)   # [B, 512, H/16, W/16] (bottleneck)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        x = self.up1(x5, x4)  # [B, 256, H/8, W/8]\n",
    "        x = self.up2(x, x3)   # [B, 128, H/4, W/4]\n",
    "        x = self.up3(x, x2)   # [B, 64, H/2, W/2]\n",
    "        x = self.up4(x, x1)   # [B, 64, H, W]\n",
    "        \n",
    "        # Output\n",
    "        logits = self.outc(x) # [B, n_classes, H, W]\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Test the model\n",
    "model = UNet(n_channels=3, n_classes=21)\n",
    "dummy_input = torch.randn(1, 3, 256, 256)\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(f\"üìä U-Net Architecture:\")\n",
    "print(f\"   Input shape:  {dummy_input.shape}\")\n",
    "print(f\"   Output shape: {output.shape}\")\n",
    "print(f\"   Parameters:   {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Data Preparation (Pascal VOC)\n",
    "\n",
    "We'll use the Pascal VOC 2012 segmentation dataset - a classic benchmark with 21 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC class names and colors\n",
    "VOC_CLASSES = [\n",
    "    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "]\n",
    "\n",
    "# VOC color palette\n",
    "VOC_COLORMAP = np.array([\n",
    "    [0, 0, 0],        # background\n",
    "    [128, 0, 0],      # aeroplane\n",
    "    [0, 128, 0],      # bicycle\n",
    "    [128, 128, 0],    # bird\n",
    "    [0, 0, 128],      # boat\n",
    "    [128, 0, 128],    # bottle\n",
    "    [0, 128, 128],    # bus\n",
    "    [128, 128, 128],  # car\n",
    "    [64, 0, 0],       # cat\n",
    "    [192, 0, 0],      # chair\n",
    "    [64, 128, 0],     # cow\n",
    "    [192, 128, 0],    # diningtable\n",
    "    [64, 0, 128],     # dog\n",
    "    [192, 0, 128],    # horse\n",
    "    [64, 128, 128],   # motorbike\n",
    "    [192, 128, 128],  # person\n",
    "    [0, 64, 0],       # pottedplant\n",
    "    [128, 64, 0],     # sheep\n",
    "    [0, 192, 0],      # sofa\n",
    "    [128, 192, 0],    # train\n",
    "    [0, 64, 128],     # tvmonitor\n",
    "], dtype=np.uint8)\n",
    "\n",
    "print(\"üìã Pascal VOC Classes:\")\n",
    "for i, cls in enumerate(VOC_CLASSES):\n",
    "    color = VOC_COLORMAP[i]\n",
    "    print(f\"   {i:2d}: {cls:<15} RGB{tuple(color)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCSegmentationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Pascal VOC Segmentation dataset with proper transforms.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str = '../data',\n",
    "        image_set: str = 'train',\n",
    "        image_size: int = 256,\n",
    "        download: bool = True\n",
    "    ):\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Load VOC dataset\n",
    "        self.dataset = VOCSegmentation(\n",
    "            root=root,\n",
    "            year='2012',\n",
    "            image_set=image_set,\n",
    "            download=download\n",
    "        )\n",
    "        \n",
    "        # Transforms for image\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # Transforms for mask (no normalization!)\n",
    "        self.mask_transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size), \n",
    "                            interpolation=transforms.InterpolationMode.NEAREST),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        image, mask = self.dataset[idx]\n",
    "        \n",
    "        # Transform image\n",
    "        image = self.image_transform(image)\n",
    "        \n",
    "        # Transform mask\n",
    "        mask = self.mask_transform(mask)\n",
    "        mask = torch.from_numpy(np.array(mask)).long()\n",
    "        \n",
    "        # VOC uses 255 for boundary/ignore - set to 0 (background) for simplicity\n",
    "        mask[mask == 255] = 0\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load datasets\n# Note: Pascal VOC 2012 is ~2GB download. First run may take several minutes.\nprint(\"üìÇ Loading Pascal VOC 2012...\")\nprint(\"   ‚ö†Ô∏è First run will download ~2GB. This may take several minutes.\")\n\ntrain_dataset = VOCSegmentationDataset(image_set='train')\nval_dataset = VOCSegmentationDataset(image_set='val')\n\n# ‚ö†Ô∏è DGX SPARK NOTE: When using Docker with num_workers > 0, use --ipc=host flag\n# Example: docker run --gpus all -it --rm --ipc=host nvcr.io/nvidia/pytorch:25.11-py3\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4, pin_memory=True)\n\nprint(f\"‚úÖ Dataset loaded:\")\nprint(f\"   Training:   {len(train_dataset):,} images\")\nprint(f\"   Validation: {len(val_dataset):,} images\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_segmentation(image: torch.Tensor, mask: torch.Tensor, prediction: Optional[torch.Tensor] = None):\n",
    "    \"\"\"\n",
    "    Visualize image, ground truth mask, and optionally prediction.\n",
    "    \"\"\"\n",
    "    # Denormalize image\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    image_vis = image.cpu() * std + mean\n",
    "    image_vis = image_vis.permute(1, 2, 0).numpy().clip(0, 1)\n",
    "    \n",
    "    # Convert mask to colored image\n",
    "    mask_np = mask.cpu().numpy()\n",
    "    mask_colored = VOC_COLORMAP[mask_np]\n",
    "    \n",
    "    if prediction is not None:\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "        \n",
    "        pred_np = prediction.cpu().numpy()\n",
    "        pred_colored = VOC_COLORMAP[pred_np]\n",
    "        \n",
    "        axes[0].imshow(image_vis)\n",
    "        axes[0].set_title('Input Image')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(mask_colored)\n",
    "        axes[1].set_title('Ground Truth')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        axes[2].imshow(pred_colored)\n",
    "        axes[2].set_title('Prediction')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        # Overlay\n",
    "        axes[3].imshow(image_vis)\n",
    "        axes[3].imshow(pred_colored, alpha=0.5)\n",
    "        axes[3].set_title('Overlay')\n",
    "        axes[3].axis('off')\n",
    "    else:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "        \n",
    "        axes[0].imshow(image_vis)\n",
    "        axes[0].set_title('Input Image')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(mask_colored)\n",
    "        axes[1].set_title('Segmentation Mask')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # Overlay\n",
    "        axes[2].imshow(image_vis)\n",
    "        axes[2].imshow(mask_colored, alpha=0.5)\n",
    "        axes[2].set_title('Overlay')\n",
    "        axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize samples\n",
    "images, masks = next(iter(train_loader))\n",
    "for i in range(3):\n",
    "    visualize_segmentation(images[i], masks[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 3: Segmentation Loss Functions\n\nSegmentation uses special loss functions to handle class imbalance.\n\n### üìù Key Function: F.one_hot()\n\nBefore we define our loss functions, let's understand `F.one_hot()` which converts class indices to one-hot encoded tensors:\n\n```python\nimport torch.nn.functional as F\n\n# Class indices for 3 pixels (classes 0, 2, 1)\nindices = torch.tensor([0, 2, 1])\n\n# Convert to one-hot with 4 classes\none_hot = F.one_hot(indices, num_classes=4)\n# Result: tensor([[1, 0, 0, 0],   <- class 0\n#                 [0, 0, 1, 0],   <- class 2\n#                 [0, 1, 0, 0]])  <- class 1\n```\n\nThis is essential for computing per-class losses in segmentation!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Dice Loss for segmentation.\n",
    "    \n",
    "    Dice = 2 * |A ‚à© B| / (|A| + |B|)\n",
    "    \n",
    "    Handles class imbalance better than cross-entropy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, smooth: float = 1.0):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: [B, C, H, W] - raw model output\n",
    "            targets: [B, H, W] - class indices\n",
    "        \"\"\"\n",
    "        num_classes = logits.shape[1]\n",
    "        \n",
    "        # Convert logits to probabilities\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        # One-hot encode targets\n",
    "        targets_one_hot = F.one_hot(targets, num_classes).permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        # Compute Dice per class\n",
    "        dims = (0, 2, 3)  # Batch, H, W\n",
    "        intersection = (probs * targets_one_hot).sum(dims)\n",
    "        union = probs.sum(dims) + targets_one_hot.sum(dims)\n",
    "        \n",
    "        dice = (2 * intersection + self.smooth) / (union + self.smooth)\n",
    "        \n",
    "        # Average over classes (excluding background if desired)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combination of Cross-Entropy and Dice Loss.\n",
    "    \n",
    "    CE helps with hard examples, Dice handles class imbalance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ce_weight: float = 0.5, dice_weight: float = 0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.ce = nn.CrossEntropyLoss(ignore_index=255)  # Ignore boundary\n",
    "        self.dice = DiceLoss()\n",
    "        self.ce_weight = ce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "    \n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        ce_loss = self.ce(logits, targets)\n",
    "        dice_loss = self.dice(logits, targets)\n",
    "        return self.ce_weight * ce_loss + self.dice_weight * dice_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßí ELI5: Dice Loss vs Cross-Entropy\n",
    "\n",
    "> **Imagine you're grading an artist's drawing of a cat...**\n",
    ">\n",
    "> **Cross-Entropy**: \"How confident were you at each pixel?\"\n",
    "> - Penalizes wrong predictions strongly\n",
    "> - Can be dominated by background (if 90% of pixels are background)\n",
    ">\n",
    "> **Dice Loss**: \"How much overlap between your drawing and the real cat?\"\n",
    "> - Directly measures segmentation quality\n",
    "> - Naturally handles class imbalance\n",
    ">\n",
    "> **Combined**: Get the best of both worlds!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Evaluation Metrics (IoU/mIoU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(pred: torch.Tensor, target: torch.Tensor, num_classes: int = 21) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute Intersection over Union (IoU) per class and mean IoU.\n",
    "    \n",
    "    IoU = |A ‚à© B| / |A ‚à™ B|\n",
    "    \n",
    "    Args:\n",
    "        pred: [B, H, W] - predicted class indices\n",
    "        target: [B, H, W] - ground truth class indices\n",
    "        num_classes: Number of classes\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with per-class IoU and mIoU\n",
    "    \"\"\"\n",
    "    ious = []\n",
    "    \n",
    "    for cls in range(num_classes):\n",
    "        pred_cls = (pred == cls)\n",
    "        target_cls = (target == cls)\n",
    "        \n",
    "        intersection = (pred_cls & target_cls).sum().float()\n",
    "        union = (pred_cls | target_cls).sum().float()\n",
    "        \n",
    "        if union > 0:\n",
    "            iou = intersection / union\n",
    "            ious.append(iou.item())\n",
    "        else:\n",
    "            ious.append(float('nan'))  # Class not present\n",
    "    \n",
    "    # Compute mIoU (ignoring classes not present)\n",
    "    valid_ious = [iou for iou in ious if not np.isnan(iou)]\n",
    "    miou = np.mean(valid_ious) if valid_ious else 0.0\n",
    "    \n",
    "    return {\n",
    "        'per_class_iou': ious,\n",
    "        'miou': miou\n",
    "    }\n",
    "\n",
    "# Demo\n",
    "pred = torch.argmax(torch.randn(1, 21, 256, 256), dim=1)\n",
    "target = torch.randint(0, 21, (1, 256, 256))\n",
    "metrics = compute_iou(pred, target)\n",
    "\n",
    "print(f\"üìä IoU Metrics (random predictions):\")\n",
    "print(f\"   mIoU: {metrics['miou']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Training U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_segmentation(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    epochs: int = 10,\n",
    "    lr: float = 0.001,\n",
    "    device: torch.device = device\n",
    ") -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Train a segmentation model.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = CombinedLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_miou': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for images, masks in pbar:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds, all_targets = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = outputs.argmax(dim=1)\n",
    "                all_preds.append(preds.cpu())\n",
    "                all_targets.append(masks.cpu())\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        # Compute mIoU\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_targets = torch.cat(all_targets)\n",
    "        metrics = compute_iou(all_preds, all_targets)\n",
    "        history['val_miou'].append(metrics['miou'])\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"   Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, mIoU: {metrics['miou']:.1%}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"üèãÔ∏è Training U-Net on Pascal VOC...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "model = UNet(n_channels=3, n_classes=21)\n",
    "start_time = time.time()\n",
    "\n",
    "history = train_segmentation(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader,\n",
    "    epochs=10,\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Training complete in {train_time/60:.1f} minutes\")\n",
    "print(f\"   Final mIoU: {history['val_miou'][-1]:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='Train', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Validation', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('üìâ Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history['val_miou'], label='mIoU', linewidth=2, color='green')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('mIoU')\n",
    "axes[1].set_title('üìà Validation mIoU')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions on validation set\n",
    "model.eval()\n",
    "\n",
    "for batch_idx, (images, masks) in enumerate(val_loader):\n",
    "    if batch_idx >= 1:  # Just one batch\n",
    "        break\n",
    "    \n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        predictions = outputs.argmax(dim=1)\n",
    "    \n",
    "    # Show first 4 samples\n",
    "    for i in range(min(4, len(images))):\n",
    "        visualize_segmentation(\n",
    "            images[i].cpu(),\n",
    "            masks[i],\n",
    "            predictions[i].cpu()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ‚úã Try It Yourself\n\n1. **Modify U-Net**: Add more channels (128, 256, 512, 1024 instead of 64, 128, 256, 512)\n2. **Add data augmentation**: Random rotation, flipping for both image and mask\n3. **Try different losses**: Pure Dice loss vs pure CE vs combined\n\n<details>\n<summary>üí° Hint for augmentation using torchvision (built-in)</summary>\n\nFor joint image-mask augmentation, you can use torchvision transforms with manual random state:\n\n```python\nimport torchvision.transforms.functional as TF\nimport random\n\ndef joint_transform(image, mask):\n    # Random horizontal flip\n    if random.random() > 0.5:\n        image = TF.hflip(image)\n        mask = TF.hflip(mask)\n    \n    # Random vertical flip\n    if random.random() > 0.5:\n        image = TF.vflip(image)\n        mask = TF.vflip(mask)\n    \n    # Random rotation (90 degrees)\n    if random.random() > 0.5:\n        angle = random.choice([90, 180, 270])\n        image = TF.rotate(image, angle)\n        mask = TF.rotate(mask, angle)\n    \n    return image, mask\n```\n\n</details>\n\n<details>\n<summary>üí° Hint for augmentation using albumentations (advanced)</summary>\n\nThe `albumentations` library provides powerful joint image-mask augmentation. **You'll need to install it first:**\n\n```bash\npip install albumentations\n```\n\nThen use it like this:\n\n```python\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# Define joint transform for image AND mask\ntransform = A.Compose([\n    A.RandomCrop(256, 256),\n    A.HorizontalFlip(p=0.5),\n    A.RandomRotate90(p=0.5),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2(),\n])\n\n# Apply to both image and mask together\ntransformed = transform(image=image, mask=mask)\naug_image = transformed['image']\naug_mask = transformed['mask']\n```\n\n**Key benefit:** Albumentations ensures the same random transformation is applied to both image and mask, maintaining spatial correspondence.\n\n</details>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Using wrong interpolation for masks\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Bilinear interpolation creates invalid class values!\n",
    "mask = F.interpolate(mask.float(), size=256, mode='bilinear')\n",
    "\n",
    "# ‚úÖ Right: Use nearest neighbor for class labels\n",
    "mask = F.interpolate(mask.unsqueeze(0).float(), size=256, mode='nearest').squeeze(0)\n",
    "```\n",
    "**Why:** Masks are class indices (0, 1, 2...). Bilinear creates invalid values (0.5, 1.3...).\n",
    "\n",
    "### Mistake 2: Forgetting ignore index\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Training on boundary pixels\n",
    "criterion = nn.CrossEntropyLoss()  # Treats 255 as a class!\n",
    "\n",
    "# ‚úÖ Right: Ignore boundary/void pixels\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "```\n",
    "**Why:** VOC uses 255 for boundaries. Training on them confuses the model.\n",
    "\n",
    "### Mistake 3: Size mismatch with skip connections\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Direct concatenation without size check\n",
    "x = torch.cat([encoder_feature, decoder_feature], dim=1)  # May crash!\n",
    "\n",
    "# ‚úÖ Right: Pad or crop to match sizes\n",
    "if encoder_feature.size() != decoder_feature.size():\n",
    "    decoder_feature = F.pad(decoder_feature, compute_padding(...))\n",
    "x = torch.cat([encoder_feature, decoder_feature], dim=1)\n",
    "```\n",
    "**Why:** Downsampling/upsampling may not preserve exact dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ Semantic vs instance vs panoptic segmentation\n",
    "- ‚úÖ U-Net encoder-decoder architecture with skip connections\n",
    "- ‚úÖ Dice loss for handling class imbalance\n",
    "- ‚úÖ IoU/mIoU evaluation metrics\n",
    "- ‚úÖ Training and visualizing segmentation models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Implement DeepLabV3+ backbone replacement:**\n",
    "\n",
    "Replace U-Net's encoder with a pre-trained ResNet backbone:\n",
    "\n",
    "1. Use `torchvision.models.segmentation.deeplabv3_resnet50`\n",
    "2. Compare performance with your U-Net\n",
    "3. Fine-tune only the classifier head first, then the full model\n",
    "\n",
    "<details>\n",
    "<summary>üí° Starting Code</summary>\n",
    "\n",
    "```python\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "\n",
    "model = deeplabv3_resnet50(pretrained=True)\n",
    "model.classifier[-1] = nn.Conv2d(256, 21, kernel_size=1)  # 21 VOC classes\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [U-Net Paper](https://arxiv.org/abs/1505.04597)\n",
    "- [DeepLab Series](https://arxiv.org/abs/1706.05587)\n",
    "- [Panoptic Segmentation](https://arxiv.org/abs/1801.00868)\n",
    "- [SegFormer](https://arxiv.org/abs/2105.15203) - Modern transformer-based\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üíæ GPU Memory Free: {torch.cuda.mem_get_info()[0] / 1e9:.1f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}