{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.4.3: LlamaIndex Query Engine with Hybrid Search\n",
    "\n",
    "**Module:** 3.4 - AI Agents & Agentic Systems  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand LlamaIndex's approach to RAG (vs LangChain)\n",
    "- [ ] Build different index types for different use cases\n",
    "- [ ] Implement hybrid search (keyword + semantic)\n",
    "- [ ] Add reranking for improved retrieval quality\n",
    "- [ ] Create query engines with automatic citations\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Lab 3.4.1 (RAG Pipeline)\n",
    "- Knowledge of: Embeddings, vector search basics\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**Why LlamaIndex when we have LangChain?**\n",
    "\n",
    "Think of them like different tools in a toolbox:\n",
    "- **LangChain**: General-purpose framework for LLM applications (agents, chains, tools)\n",
    "- **LlamaIndex**: Specialized for data indexing and retrieval (RAG excellence)\n",
    "\n",
    "**Real-world use cases for LlamaIndex:**\n",
    "- üìö **Enterprise Search**: Searching across millions of documents with citations\n",
    "- üìä **Structured Data QA**: Querying databases with natural language\n",
    "- üî¨ **Research Assistants**: Finding relevant papers with context\n",
    "- üìã **Compliance Tools**: Answering policy questions with exact source references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: LlamaIndex vs LangChain\n",
    "\n",
    "> **Imagine you're building a library system...** üìö\n",
    ">\n",
    "> **LangChain** is like a construction company. They can build:\n",
    "> - Libraries (RAG)\n",
    "> - Offices (agents)\n",
    "> - Factories (chains)\n",
    "> - ...and anything else you need!\n",
    ">\n",
    "> **LlamaIndex** is like a specialized library architect. They ONLY build libraries, but:\n",
    "> - Their catalog systems are amazing\n",
    "> - They have special ways to organize books\n",
    "> - They're experts at helping you find exactly what you need\n",
    ">\n",
    "> **When to use which:**\n",
    "> - Need a general AI application? ‚Üí LangChain\n",
    "> - Need the BEST possible document retrieval? ‚Üí LlamaIndex\n",
    "> - Need both? ‚Üí Use them together! (They work great as partners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install LlamaIndex and dependencies (run once)\n# Note: BM25Retriever requires rank_bm25, SentenceTransformerRerank requires sentence-transformers\n# Pinned versions for reproducibility - update as needed\n# !pip install llama-index>=0.10.0 llama-index-llms-ollama llama-index-embeddings-ollama rank_bm25 sentence-transformers"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "import time\n",
    "\n",
    "# LlamaIndex imports\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Document,\n",
    "    Settings,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "\n",
    "# Local models via Ollama\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "print(\"LlamaIndex imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LlamaIndex to use local Ollama models\n",
    "# This runs everything on DGX Spark - no API calls needed!\n",
    "\n",
    "# Initialize embedding model\n",
    "embed_model = OllamaEmbedding(\n",
    "    model_name=\"nomic-embed-text\",\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = Ollama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0.1,\n",
    "    request_timeout=120.0,\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "# Configure global settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "print(\"LlamaIndex configured with local Ollama models:\")\n",
    "print(f\"  - Embedding: nomic-embed-text\")\n",
    "print(f\"  - LLM: llama3.1:8b\")\n",
    "print(f\"  - Chunk size: 512 chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Loading Documents\n",
    "\n",
    "LlamaIndex has built-in readers for many document types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATA_DIR = Path.cwd().parent / \"data\" / \"sample_documents\"\n",
    "INDEX_DIR = Path.cwd().parent / \"data\" / \"llamaindex_storage\"\n",
    "\n",
    "# Load documents\n",
    "print(f\"Loading documents from: {DATA_DIR}\")\n",
    "\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_dir=str(DATA_DIR),\n",
    "    required_exts=[\".txt\", \".md\"],\n",
    "    recursive=True\n",
    ")\n",
    "\n",
    "documents = reader.load_data()\n",
    "\n",
    "print(f\"\\nLoaded {len(documents)} documents:\")\n",
    "for doc in documents:\n",
    "    filename = Path(doc.metadata.get('file_path', 'unknown')).name\n",
    "    print(f\"  - {filename}: {len(doc.text)} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a sample document\n",
    "print(\"Sample document content:\")\n",
    "print(\"=\" * 60)\n",
    "print(documents[0].text[:1000])\n",
    "print(\"...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nMetadata: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Creating a Vector Store Index\n",
    "\n",
    "The VectorStoreIndex is LlamaIndex's primary index type for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create node parser (chunker)\n",
    "node_parser = SentenceSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "# Parse documents into nodes\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"Created {len(nodes)} nodes from {len(documents)} documents\")\n",
    "print(f\"\\nSample node:\")\n",
    "print(f\"  Text: {nodes[0].text[:200]}...\")\n",
    "print(f\"  Metadata: {nodes[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector index\n",
    "print(\"Creating vector index (this will generate embeddings)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nIndex created in {elapsed:.1f} seconds!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the index for later use\n",
    "INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "index.storage_context.persist(persist_dir=str(INDEX_DIR))\n",
    "print(f\"Index saved to: {INDEX_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Basic Query Engine\n",
    "\n",
    "A query engine combines retrieval with response generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a basic query engine\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,  # Retrieve top 5 nodes\n",
    "    response_mode=\"compact\",  # Compress retrieved text\n",
    ")\n",
    "\n",
    "print(\"Query engine created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a simple query\n",
    "query = \"What is the memory capacity of DGX Spark?\"\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "response = query_engine.query(query)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\nResponse: {response}\")\n",
    "print(f\"\\n(Response time: {elapsed:.2f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the source nodes that were retrieved\n",
    "print(\"\\nSource nodes used:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, node in enumerate(response.source_nodes, 1):\n",
    "    score = node.score if hasattr(node, 'score') else 'N/A'\n",
    "    filename = Path(node.metadata.get('file_path', 'unknown')).name\n",
    "    print(f\"\\n[{i}] Score: {score:.4f} | Source: {filename}\")\n",
    "    print(f\"    {node.text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Query Engine with Citations\n",
    "\n",
    "LlamaIndex can automatically add citations to responses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Handle different LlamaIndex versions for CitationQueryEngine\ntry:\n    from llama_index.core.query_engine import CitationQueryEngine\nexcept ImportError:\n    try:\n        from llama_index.query_engine import CitationQueryEngine\n    except ImportError:\n        print(\"‚ö†Ô∏è CitationQueryEngine not available in this LlamaIndex version\")\n        print(\"   Try: pip install llama-index>=0.10.0\")\n        CitationQueryEngine = None\n\n# Create a citation query engine (if available)\nif CitationQueryEngine is not None:\n    citation_engine = CitationQueryEngine.from_args(\n        index=index,\n        similarity_top_k=5,\n        citation_chunk_size=512,\n    )\n    print(\"‚úÖ Citation query engine created!\")\nelse:\n    citation_engine = query_engine  # Fallback to regular engine\n    print(\"‚ö†Ô∏è Using standard query engine (citations not available)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query with citations\n",
    "query = \"How does the unified memory architecture benefit AI workloads on DGX Spark?\"\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "response = citation_engine.query(query)\n",
    "\n",
    "print(f\"\\nResponse with citations:\")\n",
    "print(response.response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Sources:\")\n",
    "for i, node in enumerate(response.source_nodes, 1):\n",
    "    filename = Path(node.metadata.get('file_path', 'unknown')).name\n",
    "    print(f\"  [{i}] {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Hybrid Search (Keyword + Semantic)\n",
    "\n",
    "### üßí ELI5: Why Hybrid Search?\n",
    "\n",
    "> **Imagine looking for a book about \"transformers\"...** ü§ñ\n",
    ">\n",
    "> **Semantic search** understands you mean machine learning transformers and finds:\n",
    "> - \"Attention mechanisms in neural networks\"\n",
    "> - \"BERT architecture explained\"\n",
    ">\n",
    "> **Keyword search** finds exact matches:\n",
    "> - \"The Transformer Architecture...\"\n",
    "> - \"Building Transformers from scratch\"\n",
    ">\n",
    "> **Hybrid search** combines both:\n",
    "> - Gets the semantic understanding\n",
    "> - PLUS catches exact keyword matches\n",
    "> - Best of both worlds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import BM25Retriever\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "# Create BM25 (keyword) retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    nodes=nodes,\n",
    "    similarity_top_k=5\n",
    ")\n",
    "\n",
    "# Create vector (semantic) retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "print(\"Created BM25 and Vector retrievers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hybrid retriever using QueryFusion\n",
    "hybrid_retriever = QueryFusionRetriever(\n",
    "    retrievers=[vector_retriever, bm25_retriever],\n",
    "    similarity_top_k=5,\n",
    "    num_queries=1,  # Don't generate multiple queries\n",
    "    mode=\"reciprocal_rerank\",  # Use RRF (Reciprocal Rank Fusion)\n",
    ")\n",
    "\n",
    "print(\"Hybrid retriever created using Reciprocal Rank Fusion!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results: Semantic vs Keyword vs Hybrid\n",
    "query = \"What is CUDA and Tensor Core count in Blackwell GB10?\"\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Vector (semantic) results\n",
    "print(\"=\" * 60)\n",
    "print(\"VECTOR (SEMANTIC) RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "vector_results = vector_retriever.retrieve(query)\n",
    "for i, node in enumerate(vector_results[:3], 1):\n",
    "    print(f\"[{i}] Score: {node.score:.4f}\")\n",
    "    print(f\"    {node.text[:150]}...\\n\")\n",
    "\n",
    "# BM25 (keyword) results\n",
    "print(\"=\" * 60)\n",
    "print(\"BM25 (KEYWORD) RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "bm25_results = bm25_retriever.retrieve(query)\n",
    "for i, node in enumerate(bm25_results[:3], 1):\n",
    "    print(f\"[{i}] Score: {node.score:.4f}\")\n",
    "    print(f\"    {node.text[:150]}...\\n\")\n",
    "\n",
    "# Hybrid results\n",
    "print(\"=\" * 60)\n",
    "print(\"HYBRID (FUSED) RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "hybrid_results = hybrid_retriever.retrieve(query)\n",
    "for i, node in enumerate(hybrid_results[:3], 1):\n",
    "    print(f\"[{i}] Score: {node.score:.4f}\")\n",
    "    print(f\"    {node.text[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a query engine with hybrid retrieval\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "\n",
    "# Create response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"compact\"\n",
    ")\n",
    "\n",
    "# Create hybrid query engine\n",
    "hybrid_query_engine = RetrieverQueryEngine(\n",
    "    retriever=hybrid_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "print(\"Hybrid query engine created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the hybrid query engine\n",
    "query = \"What are the Tensor Cores and CUDA cores specifications of DGX Spark?\"\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "response = hybrid_query_engine.query(query)\n",
    "\n",
    "print(f\"\\nResponse: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Adding Reranking\n",
    "\n",
    "Reranking improves retrieval by re-scoring results with a more sophisticated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "# Create a reranker\n",
    "# Note: This uses a cross-encoder model for better ranking\n",
    "reranker = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    top_n=3  # Keep top 3 after reranking\n",
    ")\n",
    "\n",
    "print(\"Reranker initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a query engine with reranking\n",
    "reranked_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,  # Retrieve more initially\n",
    "    node_postprocessors=[reranker],  # Then rerank to top 3\n",
    "    response_mode=\"compact\"\n",
    ")\n",
    "\n",
    "print(\"Reranked query engine created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: With and without reranking\n",
    "query = \"What quantization formats work best on DGX Spark?\"\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Without reranking\n",
    "print(\"=\" * 60)\n",
    "print(\"WITHOUT RERANKING\")\n",
    "print(\"=\" * 60)\n",
    "basic_response = query_engine.query(query)\n",
    "print(f\"Response: {basic_response}\\n\")\n",
    "\n",
    "# With reranking\n",
    "print(\"=\" * 60)\n",
    "print(\"WITH RERANKING\")\n",
    "print(\"=\" * 60)\n",
    "reranked_response = reranked_query_engine.query(query)\n",
    "print(f\"Response: {reranked_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Using Proper Chunk Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Default chunk size might not fit your use case\n",
    "# Settings.chunk_size = 1024  # Too large for precise retrieval\n",
    "\n",
    "# ‚úÖ Right: Tune chunk size for your content\n",
    "# For technical docs: 256-512 chars\n",
    "# For long-form content: 512-1024 chars\n",
    "# For code: 512-768 chars\n",
    "\n",
    "Settings.chunk_size = 512\n",
    "print(\"Chunk size set to 512 - good for technical documentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Retrieving Too Few or Too Many Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Too few nodes\n",
    "# query_engine = index.as_query_engine(similarity_top_k=1)  # Might miss relevant info\n",
    "\n",
    "# ‚ùå Wrong: Too many nodes\n",
    "# query_engine = index.as_query_engine(similarity_top_k=20)  # Dilutes relevance\n",
    "\n",
    "# ‚úÖ Right: Balance based on context window and needs\n",
    "# Without reranking: k=3-5\n",
    "# With reranking: k=10-20, then rerank to top 3-5\n",
    "\n",
    "print(\"Recommended: k=5 for direct retrieval, k=10-20 with reranking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Not Persisting the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Rebuilding index every time (slow!)\n",
    "# index = VectorStoreIndex.from_documents(documents)  # Takes minutes\n",
    "\n",
    "# ‚úÖ Right: Persist and reload\n",
    "def get_or_create_index(documents, index_dir):\n",
    "    \"\"\"Load existing index or create new one.\"\"\"\n",
    "    index_dir = Path(index_dir)\n",
    "    \n",
    "    if (index_dir / \"docstore.json\").exists():\n",
    "        print(\"Loading existing index...\")\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=str(index_dir))\n",
    "        return load_index_from_storage(storage_context)\n",
    "    else:\n",
    "        print(\"Creating new index...\")\n",
    "        index = VectorStoreIndex.from_documents(documents)\n",
    "        index.storage_context.persist(persist_dir=str(index_dir))\n",
    "        return index\n",
    "\n",
    "print(\"Always persist your index for faster startup!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How LlamaIndex differs from LangChain\n",
    "- ‚úÖ Creating vector indices with LlamaIndex\n",
    "- ‚úÖ Building query engines with citations\n",
    "- ‚úÖ Implementing hybrid search (BM25 + vector)\n",
    "- ‚úÖ Adding reranking for better results\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úã Try It Yourself\n",
    "\n",
    "Create a query engine that:\n",
    "1. Uses hybrid retrieval\n",
    "2. Applies reranking\n",
    "3. Returns citations\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "You can combine all three by creating a `RetrieverQueryEngine` with:\n",
    "- `hybrid_retriever` as the retriever\n",
    "- `reranker` in `node_postprocessors`\n",
    "- A response synthesizer configured for citations\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your implementation here\n",
    "# Create the ultimate query engine combining all techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "Implement a **query transformation** that:\n",
    "1. Takes the user's query\n",
    "2. Generates multiple related queries\n",
    "3. Retrieves results for all queries\n",
    "4. Fuses the results together\n",
    "\n",
    "This is called **Multi-Query Retrieval** and can significantly improve results for complex questions.\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [LlamaIndex Documentation](https://docs.llamaindex.ai/)\n",
    "- [Query Engines Guide](https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/)\n",
    "- [Retrieval Strategies](https://docs.llamaindex.ai/en/stable/module_guides/querying/retriever/)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Comprehensive cleanup for DGX Spark\nimport gc\n\n# Clear GPU memory if available\ntry:\n    import torch\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n        torch.cuda.empty_cache()\n        allocated = torch.cuda.memory_allocated() / 1e9\n        print(f\"‚úÖ GPU memory cleared ({allocated:.2f} GB still allocated)\")\nexcept ImportError:\n    pass\n\n# Python garbage collection\ngc.collect()\nprint(\"‚úÖ Cleanup complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Summary\n",
    "\n",
    "In this notebook, you explored LlamaIndex's powerful retrieval capabilities:\n",
    "\n",
    "1. **VectorStoreIndex**: Fast semantic search\n",
    "2. **Query Engine**: Combines retrieval + generation\n",
    "3. **Citations**: Automatic source attribution\n",
    "4. **Hybrid Search**: Best of keyword + semantic\n",
    "5. **Reranking**: Improved result quality\n",
    "\n",
    "**When to choose LlamaIndex over LangChain for RAG:**\n",
    "- Need advanced retrieval strategies\n",
    "- Want built-in citations\n",
    "- Working with structured data\n",
    "- Need hybrid search out of the box\n",
    "\n",
    "**Next up:** Lab 3.4.4 - LangGraph Workflow with Human-in-the-Loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}