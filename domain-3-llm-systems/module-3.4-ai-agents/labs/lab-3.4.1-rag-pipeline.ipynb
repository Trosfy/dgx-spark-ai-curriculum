{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.4.1: Building a RAG Pipeline with ChromaDB\n",
    "\n",
    "**Module:** 3.4 - AI Agents & Agentic Systems  \n",
    "**Time:** 3 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand the RAG architecture and why it's essential for modern AI\n",
    "- [ ] Load and chunk documents for effective retrieval\n",
    "- [ ] Create embeddings using local models on DGX Spark\n",
    "- [ ] Build a vector store with ChromaDB\n",
    "- [ ] Implement retrieval strategies (dense, sparse, hybrid)\n",
    "- [ ] Create a complete question-answering RAG pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Module 12 (Deployment & Inference)\n",
    "- Knowledge of: Python, basic NLP concepts, embeddings\n",
    "- Running: Ollama with `nomic-embed-text` and `llama3.1` models\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The Problem:** Large Language Models have a knowledge cutoff and can't access your private documents.\n",
    "\n",
    "**Real Examples:**\n",
    "- üìÑ **Customer Support:** Your company's support bot needs to answer questions about YOUR products\n",
    "- üè• **Healthcare:** A medical AI needs access to the latest research papers\n",
    "- üíº **Legal:** An AI assistant needs to reference specific contracts and legal documents\n",
    "- üìä **Finance:** A trading assistant needs real-time market data and company reports\n",
    "\n",
    "**RAG solves this** by giving the LLM the ability to \"look up\" information in your documents before answering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What is RAG?\n",
    "\n",
    "> **Imagine you're taking an open-book test...** üìö\n",
    ">\n",
    "> Without RAG, an AI is like taking a test with just what you memorized. You might remember a lot, but what about things you never learned or details that changed after you studied?\n",
    ">\n",
    "> **With RAG, the AI gets to use the textbook!**\n",
    ">\n",
    "> When you ask a question:\n",
    "> 1. üîç The AI first **looks up** relevant pages in the textbook (retrieval)\n",
    "> 2. üìñ It **reads** those specific pages (context)\n",
    "> 3. ‚úçÔ∏è Then it **writes** an answer using both what it knows AND what it just read (generation)\n",
    ">\n",
    "> **In AI terms:**\n",
    "> - **Retrieval** = Finding relevant document chunks using similarity search\n",
    "> - **Augmented** = Adding those chunks to the prompt as context\n",
    "> - **Generation** = The LLM generates an answer using the augmented context\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                        RAG Pipeline                             ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îÇ   User Question     Document Store        LLM                   ‚îÇ\n",
    "‚îÇ        ‚îÇ                  ‚îÇ                ‚îÇ                    ‚îÇ\n",
    "‚îÇ        ‚ñº                  ‚îÇ                ‚îÇ                    ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ                ‚îÇ                    ‚îÇ\n",
    "‚îÇ   ‚îÇ Embed   ‚îÇ            ‚îÇ                ‚îÇ                    ‚îÇ\n",
    "‚îÇ   ‚îÇ Query   ‚îÇ            ‚îÇ                ‚îÇ                    ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ                ‚îÇ                    ‚îÇ\n",
    "‚îÇ        ‚îÇ                 ‚îÇ                ‚îÇ                    ‚îÇ\n",
    "‚îÇ        ‚ñº                 ‚ñº                ‚îÇ                    ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ                    ‚îÇ\n",
    "‚îÇ   ‚îÇ   Vector Similarity      ‚îÇ           ‚îÇ                    ‚îÇ\n",
    "‚îÇ   ‚îÇ       Search             ‚îÇ           ‚îÇ                    ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ                    ‚îÇ\n",
    "‚îÇ               ‚îÇ                          ‚îÇ                    ‚îÇ\n",
    "‚îÇ               ‚ñº                          ‚îÇ                    ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ                    ‚îÇ\n",
    "‚îÇ   ‚îÇ  Top-K Relevant     ‚îÇ               ‚îÇ                    ‚îÇ\n",
    "‚îÇ   ‚îÇ     Chunks          ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ          ‚îÇ         ‚îÇ\n",
    "‚îÇ                                         ‚ñº          ‚îÇ         ‚îÇ\n",
    "‚îÇ                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ         ‚îÇ\n",
    "‚îÇ                              ‚îÇ Prompt + Context   ‚îÇ‚îÇ         ‚îÇ\n",
    "‚îÇ                              ‚îÇ                    ‚îÇ‚îÇ         ‚îÇ\n",
    "‚îÇ                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ         ‚îÇ\n",
    "‚îÇ                                        ‚îÇ           ‚îÇ         ‚îÇ\n",
    "‚îÇ                                        ‚ñº           ‚îÇ         ‚îÇ\n",
    "‚îÇ                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ\n",
    "‚îÇ                              ‚îÇ   Generate Answer  ‚îÇ          ‚îÇ\n",
    "‚îÇ                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ\n",
    "‚îÇ                                        ‚îÇ                     ‚îÇ\n",
    "‚îÇ                                        ‚ñº                     ‚îÇ\n",
    "‚îÇ                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ\n",
    "‚îÇ                              ‚îÇ   Final Response   ‚îÇ          ‚îÇ\n",
    "‚îÇ                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment Setup\n",
    "\n",
    "Let's start by setting up our environment. We'll use local models on DGX Spark for maximum performance and privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages (run once)\n# Note: In the NGC container, most packages are pre-installed\n# rank_bm25 is required for the hybrid search challenge\n# !pip install langchain langchain-community chromadb sentence-transformers rank_bm25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "# Add scripts to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'scripts'))\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LangChain components\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "print(\"LangChain components imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Ollama is running before proceeding\n",
    "import requests\n",
    "\n",
    "def check_ollama():\n",
    "    \"\"\"Check if Ollama is running and required models are available.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            models = [m['name'] for m in response.json().get('models', [])]\n",
    "            print(\"‚úÖ Ollama is running\")\n",
    "            print(f\"   Available models: {', '.join(models[:5])}\")\n",
    "            \n",
    "            # Check for required models\n",
    "            required = ['llama3.1', 'nomic-embed-text']\n",
    "            for req in required:\n",
    "                if any(req in m for m in models):\n",
    "                    print(f\"   ‚úÖ Found {req}\")\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è  Missing {req} - run: ollama pull {req}\")\n",
    "            return True\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"‚ùå Ollama is not running!\")\n",
    "        print(\"   Start it with: ollama serve\")\n",
    "        print(\"   Then pull models: ollama pull llama3.1:8b nomic-embed-text\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking Ollama: {e}\")\n",
    "        return False\n",
    "\n",
    "check_ollama()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "We imported the core components we'll need:\n",
    "- **TextSplitter**: Breaks documents into smaller chunks\n",
    "- **DocumentLoaders**: Load documents from files\n",
    "- **OllamaEmbeddings**: Create vector embeddings using local Ollama models\n",
    "- **Chroma**: Our vector database for storing and searching embeddings\n",
    "- **Ollama**: The LLM for generating answers\n",
    "- **RetrievalQA**: Chains retrieval and generation together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Loading Documents\n",
    "\n",
    "The first step in any RAG pipeline is loading your documents. Let's load the sample technical documents we've prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATA_DIR = Path.cwd().parent / \"data\" / \"sample_documents\"\n",
    "CHROMA_DIR = Path.cwd().parent / \"data\" / \"chroma_db\"\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"ChromaDB directory: {CHROMA_DIR}\")\n",
    "\n",
    "# List available documents\n",
    "if DATA_DIR.exists():\n",
    "    docs = list(DATA_DIR.glob(\"*.txt\"))\n",
    "    print(f\"\\nFound {len(docs)} documents:\")\n",
    "    for doc in docs:\n",
    "        print(f\"  - {doc.name} ({doc.stat().st_size / 1024:.1f} KB)\")\n",
    "else:\n",
    "    print(\"Data directory not found. Please run the setup script first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all documents from the directory\n",
    "loader = DirectoryLoader(\n",
    "    str(DATA_DIR),\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "print(f\"\\nLoaded {len(documents)} documents\")\n",
    "\n",
    "# Show a preview of the first document\n",
    "if documents:\n",
    "    first_doc = documents[0]\n",
    "    print(f\"\\nFirst document source: {first_doc.metadata['source']}\")\n",
    "    print(f\"Content preview (first 500 chars):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(first_doc.page_content[:500])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Chunking Documents\n",
    "\n",
    "### üßí ELI5: Why Do We Need Chunking?\n",
    "\n",
    "> **Imagine you're looking for a recipe for chocolate chip cookies in a HUGE cookbook...** üìñ\n",
    ">\n",
    "> You wouldn't read the WHOLE cookbook to find it. You'd:\n",
    "> 1. Look at the table of contents (too vague)\n",
    "> 2. Find the \"Cookies\" section (better!)\n",
    "> 3. Find the specific page for chocolate chip cookies (perfect!)\n",
    ">\n",
    "> **Chunking does the same thing!**\n",
    "> - Whole document = cookbook (too big to search efficiently)\n",
    "> - Chunks = individual recipes (just the right size)\n",
    ">\n",
    "> **Key insight:** Chunks should be:\n",
    "> - Small enough to be specific\n",
    "> - Large enough to be meaningful\n",
    "> - Overlapping slightly so we don't miss context at boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,        # Maximum characters per chunk\n",
    "    chunk_overlap=50,      # Characters overlapping between chunks\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Priority of split points\n",
    ")\n",
    "\n",
    "# Split documents into chunks\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks from {len(documents)} documents\")\n",
    "print(f\"Average chunk size: {sum(len(c.page_content) for c in chunks) / len(chunks):.0f} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine some chunks\n",
    "print(\"Sample chunks:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    source = Path(chunk.metadata['source']).name\n",
    "    print(f\"\\nChunk {i+1} (from {source}):\")\n",
    "    print(f\"Length: {len(chunk.page_content)} characters\")\n",
    "    print(\"-\" * 40)\n",
    "    print(chunk.page_content[:300] + \"...\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself: Experiment with Chunk Sizes\n",
    "\n",
    "Change the `chunk_size` and `chunk_overlap` parameters and observe:\n",
    "- How does the number of chunks change?\n",
    "- What happens with very small chunks (100 characters)?\n",
    "- What happens with very large chunks (2000 characters)?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "**Guidelines for chunk sizes:**\n",
    "- **Too small (< 200):** Chunks lack context, retrieval may be meaningless\n",
    "- **Too large (> 1000):** Chunks are too broad, less precise retrieval\n",
    "- **Sweet spot (300-600):** Good balance of specificity and context\n",
    "- **Overlap (10-20%):** Prevents losing information at chunk boundaries\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experimentation code here\n",
    "# Try different chunk_size values: 100, 300, 512, 1000, 2000\n",
    "\n",
    "chunk_sizes = [100, 300, 512, 1000]\n",
    "\n",
    "for size in chunk_sizes:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=size,\n",
    "        chunk_overlap=size // 10,  # 10% overlap\n",
    "    )\n",
    "    test_chunks = splitter.split_documents(documents)\n",
    "    print(f\"Chunk size {size}: {len(test_chunks)} chunks created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Creating Embeddings\n",
    "\n",
    "### üßí ELI5: What Are Embeddings?\n",
    "\n",
    "> **Imagine you could describe ANY concept with a set of coordinates, like a map...** üó∫Ô∏è\n",
    ">\n",
    "> On this magic map:\n",
    "> - \"Dog\" and \"puppy\" would be very close together\n",
    "> - \"Dog\" and \"cat\" would be nearby (both pets)\n",
    "> - \"Dog\" and \"algebra\" would be far apart\n",
    ">\n",
    "> **Embeddings are these coordinates!**\n",
    "> - Each chunk of text ‚Üí a list of numbers (vector)\n",
    "> - Similar text ‚Üí similar numbers (close on the map)\n",
    "> - Different text ‚Üí different numbers (far on the map)\n",
    ">\n",
    "> **Why this matters for RAG:**\n",
    "> When you ask \"What is the GPU in DGX Spark?\", we convert your question to coordinates, then find the document chunks with the closest coordinates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "# We use Ollama's nomic-embed-text which runs locally on DGX Spark\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",\n",
    "    base_url=\"http://localhost:11434\"  # Default Ollama URL\n",
    ")\n",
    "\n",
    "print(\"Embedding model initialized!\")\n",
    "print(\"Model: nomic-embed-text (768 dimensions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what embeddings look like\n",
    "sample_text = \"The DGX Spark has 128GB of unified memory.\"\n",
    "\n",
    "# Create an embedding\n",
    "start_time = time.time()\n",
    "sample_embedding = embeddings.embed_query(sample_text)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"Text: '{sample_text}'\")\n",
    "print(f\"\\nEmbedding:\")\n",
    "print(f\"  - Dimensions: {len(sample_embedding)}\")\n",
    "print(f\"  - First 10 values: {sample_embedding[:10]}\")\n",
    "print(f\"  - Time to embed: {elapsed*1000:.1f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate semantic similarity with embeddings\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "# Create embeddings for similar and different texts\n",
    "texts = [\n",
    "    \"The DGX Spark has 128GB of unified memory.\",  # Original\n",
    "    \"DGX Spark features 128 gigabytes of shared RAM.\",  # Similar meaning\n",
    "    \"The memory capacity is one hundred twenty-eight gigabytes.\",  # Similar topic\n",
    "    \"Machine learning uses neural networks.\",  # Different topic\n",
    "    \"I like pizza with extra cheese.\",  # Completely different\n",
    "]\n",
    "\n",
    "print(\"Semantic Similarity Demo\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Reference: '{texts[0]}'\\n\")\n",
    "\n",
    "ref_embedding = embeddings.embed_query(texts[0])\n",
    "\n",
    "for text in texts[1:]:\n",
    "    text_embedding = embeddings.embed_query(text)\n",
    "    similarity = cosine_similarity(ref_embedding, text_embedding)\n",
    "    print(f\"Similarity: {similarity:.3f} | '{text[:50]}...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "Notice how:\n",
    "- Texts with similar meaning have high similarity scores (close to 1.0)\n",
    "- Texts about different topics have lower scores\n",
    "- Completely unrelated texts have very low scores\n",
    "\n",
    "This is the magic of embeddings - they capture **semantic meaning**, not just word overlap!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Building the Vector Store with ChromaDB\n",
    "\n",
    "Now we'll store our embeddings in ChromaDB, a lightweight vector database perfect for local development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector store\n",
    "# This embeds all chunks and stores them in ChromaDB\n",
    "\n",
    "print(f\"Creating vector store with {len(chunks)} chunks...\")\n",
    "print(\"This may take a minute as we embed each chunk.\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create and persist the vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=str(CHROMA_DIR),\n",
    "    collection_name=\"dgx_spark_docs\"\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Vector store created in {elapsed:.1f} seconds!\")\n",
    "print(f\"Stored at: {CHROMA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the vector store with a simple query\n",
    "query = \"What is the memory capacity of DGX Spark?\"\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Top 3 most relevant chunks:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Perform similarity search\n",
    "results = vectorstore.similarity_search_with_score(query, k=3)\n",
    "\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    source = Path(doc.metadata['source']).name\n",
    "    print(f\"\\n[{i}] Score: {score:.4f} | Source: {source}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(doc.page_content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Creating the RAG Chain\n",
    "\n",
    "Now let's combine retrieval with generation to create a complete RAG pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "# Using Ollama with a local model on DGX Spark\n",
    "llm = Ollama(\n",
    "    model=\"llama3.1:8b\",  # Use 8b for faster responses, or 70b for better quality\n",
    "    temperature=0.3,       # Lower temperature for more factual responses\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "print(\"LLM initialized: llama3.1:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom prompt template for RAG\n",
    "RAG_PROMPT_TEMPLATE = \"\"\"You are a helpful AI assistant with access to technical documentation.\n",
    "Use the following context to answer the question. If the answer cannot be found in the context,\n",
    "say \"I don't have information about that in my knowledge base.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=RAG_PROMPT_TEMPLATE,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "print(\"RAG prompt template created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}  # Retrieve top 5 chunks\n",
    ")\n",
    "\n",
    "# Create the RAG chain\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # \"stuff\" = put all context into prompt\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "print(\"RAG chain created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to query the RAG system\n",
    "def ask_rag(question: str, show_sources: bool = True) -> str:\n",
    "    \"\"\"Query the RAG system and optionally show sources.\"\"\"\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = rag_chain.invoke({\"query\": question})\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nAnswer: {result['result']}\")\n",
    "    print(f\"\\n(Response time: {elapsed:.2f}s)\")\n",
    "    \n",
    "    if show_sources:\n",
    "        print(\"\\nSources used:\")\n",
    "        for i, doc in enumerate(result['source_documents'], 1):\n",
    "            source = Path(doc.metadata['source']).name\n",
    "            print(f\"  [{i}] {source}\")\n",
    "    \n",
    "    return result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test our RAG system!\n",
    "print(\"=\"*60)\n",
    "print(\"Testing the RAG Pipeline\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Test question 1: Factual retrieval\n",
    "ask_rag(\"What is the memory capacity of DGX Spark?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question 2: Technical details\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "ask_rag(\"How does the unified memory architecture benefit AI workloads?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question 3: Multi-hop reasoning\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "ask_rag(\"Can I fine-tune a 70B model on DGX Spark using LoRA without quantization?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question 4: Out of scope (should gracefully handle)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "ask_rag(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Retrieval Strategies\n",
    "\n",
    "Let's explore different retrieval strategies to improve results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Similarity Search (Default)\n",
    "# Simply returns the k most similar chunks\n",
    "\n",
    "similarity_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "\n",
    "print(\"Strategy 1: Similarity Search\")\n",
    "print(\"Returns the k most similar chunks based on cosine similarity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Maximum Marginal Relevance (MMR)\n",
    "# Balances relevance with diversity to avoid redundant results\n",
    "\n",
    "mmr_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 5,\n",
    "        \"fetch_k\": 20,     # Fetch more candidates first\n",
    "        \"lambda_mult\": 0.5  # Balance between relevance (1) and diversity (0)\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Strategy 2: Maximum Marginal Relevance (MMR)\")\n",
    "print(\"Balances relevance with diversity to reduce redundancy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two strategies\n",
    "query = \"What are the key features of DGX Spark?\"\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SIMILARITY SEARCH RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "sim_results = similarity_retriever.invoke(query)\n",
    "for i, doc in enumerate(sim_results, 1):\n",
    "    print(f\"\\n[{i}] {doc.page_content[:150]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MMR SEARCH RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "mmr_results = mmr_retriever.invoke(query)\n",
    "for i, doc in enumerate(mmr_results, 1):\n",
    "    print(f\"\\n[{i}] {doc.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What's the Difference?\n",
    "\n",
    "- **Similarity Search**: May return chunks that are all very similar (redundant)\n",
    "- **MMR**: Returns diverse results that cover different aspects of the query\n",
    "\n",
    "**When to use which:**\n",
    "- **Similarity**: When you want the absolute most relevant chunks\n",
    "- **MMR**: When your query could be answered from multiple perspectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Chunks Too Small or Too Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Chunks too small\n",
    "bad_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,  # Way too small!\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "# This creates chunks like: \"The DGX Spark is NVIDIA's\" - meaningless!\n",
    "\n",
    "# ‚úÖ Right: Reasonable chunk size\n",
    "good_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "print(\"Chunk size should be 300-600 characters for most use cases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Not Including Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Losing track of where chunks came from\n",
    "# chunks_no_metadata = [doc.page_content for doc in documents]\n",
    "\n",
    "# ‚úÖ Right: Keep metadata for source attribution\n",
    "chunks_with_metadata = text_splitter.split_documents(documents)\n",
    "\n",
    "# Now we can always trace back to the source\n",
    "print(f\"Example metadata: {chunks_with_metadata[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Using the Wrong Number of Retrieved Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Too few chunks (might miss relevant info)\n",
    "bad_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# ‚ùå Wrong: Too many chunks (wastes context window, dilutes relevance)\n",
    "also_bad_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 50})\n",
    "\n",
    "# ‚úÖ Right: Balance based on your context window and needs\n",
    "good_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "print(\"k=3-7 is a good starting point for most use cases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How RAG works and why it's important\n",
    "- ‚úÖ Loading and chunking documents with appropriate sizes\n",
    "- ‚úÖ Creating embeddings using local models\n",
    "- ‚úÖ Building a vector store with ChromaDB\n",
    "- ‚úÖ Implementing different retrieval strategies\n",
    "- ‚úÖ Creating a complete RAG pipeline with LangChain\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "### Advanced RAG: Implement a Hybrid Search\n",
    "\n",
    "Combine keyword search (BM25) with semantic search for better results.\n",
    "\n",
    "**Hint:** Use `langchain.retrievers.EnsembleRetriever` to combine multiple retrievers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Challenge: Implement hybrid search\n# Requires: pip install rank_bm25\n# Your code here...\n\n# # Handle import compatibility for BM25Retriever\n# try:\n#     from langchain_community.retrievers import BM25Retriever\n# except ImportError:\n#     from langchain.retrievers import BM25Retriever\n#\n# from langchain.retrievers import EnsembleRetriever\n#\n# # Create BM25 (keyword) retriever\n# bm25_retriever = BM25Retriever.from_documents(chunks)\n# bm25_retriever.k = 5\n#\n# # Create ensemble retriever\n# ensemble_retriever = EnsembleRetriever(\n#     retrievers=[bm25_retriever, mmr_retriever],\n#     weights=[0.3, 0.7]  # 30% keyword, 70% semantic\n# )\n#\n# # Test hybrid retrieval\n# hybrid_results = ensemble_retriever.invoke(\"What are the key features of DGX Spark?\")\n# print(\"Hybrid search results:\", len(hybrid_results))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [LangChain RAG Documentation](https://python.langchain.com/docs/tutorials/rag/)\n",
    "- [ChromaDB Documentation](https://docs.trychroma.com/)\n",
    "- [Ollama Embedding Models](https://ollama.com/library?tag=embedding)\n",
    "- [RAG Best Practices (Anthropic)](https://docs.anthropic.com/claude/docs/retrieval-augmented-generation-rag)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: Free GPU memory and resources\n",
    "import gc\n",
    "\n",
    "# Clear GPU memory if available\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"‚úÖ GPU memory cleared ({allocated:.2f} GB still allocated)\")\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Python garbage collection\n",
    "gc.collect()\n",
    "print(\"‚úÖ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Summary\n",
    "\n",
    "In this notebook, you built a complete RAG pipeline:\n",
    "\n",
    "1. **Loaded documents** from files into LangChain Document objects\n",
    "2. **Chunked documents** using RecursiveCharacterTextSplitter\n",
    "3. **Created embeddings** with local Ollama models\n",
    "4. **Stored vectors** in ChromaDB for efficient similarity search\n",
    "5. **Implemented retrieval** with different strategies (similarity, MMR)\n",
    "6. **Built a QA chain** combining retrieval with LLM generation\n",
    "\n",
    "**Next up:** Lab 3.4.2 - Building Custom Tools for AI Agents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}