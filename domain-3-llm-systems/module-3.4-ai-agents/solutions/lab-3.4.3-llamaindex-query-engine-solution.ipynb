{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.4.3: LlamaIndex Query Engine - Solutions\n",
    "\n",
    "This notebook contains complete solutions for the LlamaIndex exercises.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Combine Hybrid Retrieval + Reranking + Citations\n",
    "\n",
    "**Solution:** Creating the ultimate query engine that combines all three techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.retrievers import BM25Retriever, QueryFusionRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure LlamaIndex\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "Settings.llm = Ollama(model=\"llama3.1:8b\", temperature=0.1)\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "# Load documents\n",
    "DATA_DIR = Path.cwd().parent / \"data\" / \"sample_documents\"\n",
    "documents = SimpleDirectoryReader(input_dir=str(DATA_DIR)).load_data()\n",
    "\n",
    "# Create nodes\n",
    "node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=50)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents, created {len(nodes)} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index\n",
    "index = VectorStoreIndex(nodes=nodes, show_progress=True)\n",
    "\n",
    "# Create component retrievers\n",
    "# 1. BM25 (keyword) retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    nodes=nodes,\n",
    "    similarity_top_k=10  # Get more initially for reranking\n",
    ")\n",
    "\n",
    "# 2. Vector (semantic) retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=10)\n",
    "\n",
    "# 3. Hybrid retriever using QueryFusion\n",
    "hybrid_retriever = QueryFusionRetriever(\n",
    "    retrievers=[vector_retriever, bm25_retriever],\n",
    "    similarity_top_k=10,\n",
    "    num_queries=1,\n",
    "    mode=\"reciprocal_rerank\",  # RRF fusion\n",
    ")\n",
    "\n",
    "print(\"Hybrid retriever created (BM25 + Vector with RRF fusion)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reranker\n",
    "reranker = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    top_n=5  # Keep top 5 after reranking\n",
    ")\n",
    "\n",
    "# Create response synthesizer with citation support\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"compact\",\n",
    "    # The synthesizer will include source info in the response\n",
    ")\n",
    "\n",
    "print(\"Reranker and response synthesizer created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ultimate query engine\n",
    "ultimate_query_engine = RetrieverQueryEngine(\n",
    "    retriever=hybrid_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[reranker],  # Apply reranking\n",
    ")\n",
    "\n",
    "print(\"Ultimate Query Engine created!\")\n",
    "print(\"Features:\")\n",
    "print(\"  ✓ Hybrid retrieval (BM25 + Vector)\")\n",
    "print(\"  ✓ RRF fusion for combining results\")\n",
    "print(\"  ✓ Cross-encoder reranking\")\n",
    "print(\"  ✓ Compact response synthesis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the ultimate query engine\n",
    "test_queries = [\n",
    "    \"What are the memory specifications of DGX Spark?\",\n",
    "    \"How does LoRA enable efficient fine-tuning?\",\n",
    "    \"What quantization formats does Blackwell support?\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    response = ultimate_query_engine.query(query)\n",
    "    \n",
    "    print(f\"\\nResponse: {response}\")\n",
    "    print(f\"\\nSources used: {len(response.source_nodes)}\")\n",
    "    for i, node in enumerate(response.source_nodes[:3], 1):\n",
    "        print(f\"  [{i}] {node.text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Multi-Query Retrieval\n",
    "\n",
    "**Solution:** Generate multiple query variations and combine results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "# The QueryFusionRetriever can generate multiple queries\n",
    "# Let's create one that generates variations\n",
    "\n",
    "multi_query_retriever = QueryFusionRetriever(\n",
    "    retrievers=[vector_retriever],\n",
    "    similarity_top_k=5,\n",
    "    num_queries=4,  # Generate 4 query variations\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=False,\n",
    ")\n",
    "\n",
    "# Create query engine with multi-query retrieval\n",
    "multi_query_engine = RetrieverQueryEngine(\n",
    "    retriever=multi_query_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "print(\"Multi-Query Retriever created!\")\n",
    "print(\"This will generate 4 variations of each query for broader coverage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-query retrieval\n",
    "query = \"What makes DGX Spark good for AI?\"\n",
    "\n",
    "print(f\"Original Query: {query}\")\n",
    "print(\"\\nGenerated Query Variations (by the retriever):\")\n",
    "\n",
    "response = multi_query_engine.query(query)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Hybrid retrieval** catches both exact matches (BM25) and semantic meaning (vector)\n",
    "2. **RRF fusion** effectively combines rankings from multiple retrievers\n",
    "3. **Reranking** with cross-encoders significantly improves result quality\n",
    "4. **Multi-query retrieval** broadens coverage by searching variations\n",
    "5. **The combination** of all techniques provides the most robust retrieval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}