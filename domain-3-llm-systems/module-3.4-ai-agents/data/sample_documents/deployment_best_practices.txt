# AI Model Deployment Best Practices

## Introduction

Deploying AI models from development to production requires careful consideration of performance, reliability, scalability, and cost. This guide covers best practices for deploying large language models and AI systems, with special attention to DGX Spark deployments.

## Deployment Architectures

### Single-Model Serving
Simplest architecture for one model:
```
Client → Load Balancer → Model Server → Response
```

Components:
- FastAPI/Flask for REST API
- Model loaded in memory
- Health check endpoints

### Multi-Model Serving
Multiple models behind one endpoint:
```
Client → API Gateway → Router → Model Pool → Response
```

Benefits:
- A/B testing
- Fallback models
- Specialized models for different tasks

## Inference Servers

### vLLM
High-throughput serving with PagedAttention:
```python
from vllm import LLM, SamplingParams

llm = LLM(model="meta-llama/Llama-3.1-8B-Instruct")
sampling_params = SamplingParams(
    temperature=0.7,
    max_tokens=256
)

outputs = llm.generate(prompts, sampling_params)
```

Features:
- Continuous batching
- Efficient KV-cache management
- OpenAI-compatible API

### TensorRT-LLM
NVIDIA's optimized inference:
- Maximum throughput on NVIDIA GPUs
- Quantization support
- Inflight batching

### Ollama
Easiest local deployment:
```bash
# Start server
ollama serve

# Run model
ollama run llama3.1:70b
```

Perfect for DGX Spark:
- Simple setup
- Automatic memory management
- REST API included

## API Design

### RESTful Endpoints
```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

class ChatRequest(BaseModel):
    messages: list[dict]
    temperature: float = 0.7
    max_tokens: int = 256

class ChatResponse(BaseModel):
    content: str
    usage: dict

@app.post("/v1/chat/completions")
async def chat(request: ChatRequest) -> ChatResponse:
    response = model.generate(
        messages=request.messages,
        temperature=request.temperature,
        max_tokens=request.max_tokens
    )
    return ChatResponse(
        content=response.content,
        usage=response.usage
    )
```

### Streaming Responses
Essential for LLM applications:
```python
from fastapi.responses import StreamingResponse

@app.post("/v1/chat/completions/stream")
async def chat_stream(request: ChatRequest):
    async def generate():
        for chunk in model.generate_stream(request.messages):
            yield f"data: {json.dumps(chunk)}\n\n"
        yield "data: [DONE]\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
```

## Performance Optimization

### Batching Strategies
Dynamic batching for throughput:
```python
class BatchProcessor:
    def __init__(self, model, max_batch_size=8, max_wait_ms=100):
        self.model = model
        self.max_batch_size = max_batch_size
        self.max_wait_ms = max_wait_ms
        self.queue = asyncio.Queue()

    async def process_batches(self):
        while True:
            batch = []
            start_time = time.time()

            while len(batch) < self.max_batch_size:
                try:
                    timeout = self.max_wait_ms/1000 - (time.time() - start_time)
                    if timeout <= 0:
                        break
                    item = await asyncio.wait_for(
                        self.queue.get(),
                        timeout=timeout
                    )
                    batch.append(item)
                except asyncio.TimeoutError:
                    break

            if batch:
                results = self.model.generate_batch([b.prompt for b in batch])
                for item, result in zip(batch, results):
                    item.future.set_result(result)
```

### KV-Cache Optimization
Efficient memory usage for long contexts:
```python
# vLLM handles this automatically
llm = LLM(
    model="model_name",
    gpu_memory_utilization=0.9,  # Use 90% of GPU memory
    max_model_len=4096           # Maximum context length
)
```

### Prefix Caching
Cache common prompts:
```python
# For system prompts that don't change
system_prefix = tokenizer.encode(system_prompt)
cached_kv = model.generate_kv_cache(system_prefix)

# Reuse for all requests
response = model.generate(
    user_input,
    prefix_cache=cached_kv
)
```

## Reliability Patterns

### Health Checks
```python
@app.get("/health")
async def health_check():
    try:
        # Quick inference test
        test_output = model.generate("Hello", max_tokens=1)
        return {"status": "healthy", "model_loaded": True}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}, 503

@app.get("/ready")
async def readiness_check():
    if model_fully_loaded:
        return {"status": "ready"}
    return {"status": "not_ready"}, 503
```

### Timeouts and Retries
```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=1, max=10)
)
async def generate_with_retry(prompt: str) -> str:
    try:
        async with asyncio.timeout(30):  # 30 second timeout
            return await model.generate(prompt)
    except asyncio.TimeoutError:
        logger.warning("Generation timed out, retrying...")
        raise
```

### Graceful Degradation
```python
async def generate_with_fallback(prompt: str) -> str:
    try:
        return await primary_model.generate(prompt)
    except Exception as primary_error:
        logger.error(f"Primary model failed: {primary_error}")
        try:
            return await fallback_model.generate(prompt)
        except Exception as fallback_error:
            return "I'm experiencing technical difficulties. Please try again later."
```

## Monitoring and Observability

### Key Metrics
Track these for LLM deployments:
```python
# Request metrics
request_count = Counter("llm_requests_total", ["model", "status"])
request_latency = Histogram("llm_request_duration_seconds", ["model"])
tokens_generated = Counter("llm_tokens_generated_total", ["model"])

# Resource metrics
gpu_memory_used = Gauge("gpu_memory_used_bytes")
batch_size = Histogram("llm_batch_size", ["model"])
queue_size = Gauge("llm_queue_size")
```

### Logging Best Practices
```python
import structlog

logger = structlog.get_logger()

@app.middleware("http")
async def log_requests(request: Request, call_next):
    request_id = str(uuid.uuid4())
    start_time = time.time()

    logger.info(
        "request_started",
        request_id=request_id,
        method=request.method,
        path=request.url.path
    )

    response = await call_next(request)

    logger.info(
        "request_completed",
        request_id=request_id,
        status_code=response.status_code,
        duration_ms=(time.time() - start_time) * 1000
    )

    return response
```

## Security Considerations

### Input Validation
```python
def validate_input(prompt: str) -> str:
    # Length limits
    if len(prompt) > 10000:
        raise ValueError("Prompt too long")

    # Sanitize potential injection
    # (LLMs can be resistant but be cautious)

    return prompt

@app.post("/generate")
async def generate(request: GenerateRequest):
    validated_prompt = validate_input(request.prompt)
    return await model.generate(validated_prompt)
```

### Rate Limiting
```python
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)

@app.post("/generate")
@limiter.limit("10/minute")  # 10 requests per minute per IP
async def generate(request: Request, body: GenerateRequest):
    return await model.generate(body.prompt)
```

### API Authentication
```python
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials

security = HTTPBearer()

@app.post("/generate")
async def generate(
    request: GenerateRequest,
    credentials: HTTPAuthorizationCredentials = Depends(security)
):
    if not verify_token(credentials.credentials):
        raise HTTPException(status_code=401, detail="Invalid token")
    return await model.generate(request.prompt)
```

## DGX Spark Deployment

### Optimal Configuration
```python
# DGX Spark specific settings
DEPLOYMENT_CONFIG = {
    # Use bfloat16 (native Blackwell support)
    "torch_dtype": "bfloat16",

    # Take advantage of 128GB unified memory
    "gpu_memory_utilization": 0.85,

    # No need for aggressive quantization
    "quantization": None,  # or "fp8" for speed

    # Enable Blackwell optimizations
    "enable_cuda_graph": True
}
```

### Memory Management
```python
def clear_cache_before_loading():
    """Clear system caches before loading large models."""
    import subprocess
    subprocess.run(
        ["sudo", "sh", "-c", "sync; echo 3 > /proc/sys/vm/drop_caches"],
        check=True
    )

def load_model_for_production():
    clear_cache_before_loading()

    model = AutoModelForCausalLM.from_pretrained(
        "meta-llama/Llama-3.1-70B-Instruct",
        torch_dtype=torch.bfloat16,
        device_map="cuda"
    )

    # Compile for faster inference
    model = torch.compile(model)

    return model
```

## Scaling Strategies

### Horizontal Scaling
For high-throughput requirements:
- Multiple DGX Spark units
- Load balancer distribution
- Consistent hashing for cache efficiency

### Vertical Scaling
Maximize single-node performance:
- Larger batch sizes
- KV-cache optimization
- Quantization for speed (not memory)

## Conclusion

Deploying AI models effectively requires balancing performance, reliability, and cost. DGX Spark's 128GB unified memory simplifies deployment of large models, but following these best practices ensures robust production systems. Key takeaways:
- Use appropriate inference servers (vLLM, Ollama)
- Implement proper monitoring and health checks
- Design for graceful degradation
- Secure your endpoints
- Optimize for the unique advantages of DGX Spark
