{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.3.3: TensorRT-LLM Optimization\n",
    "\n",
    "**Module:** 3.3 - Model Deployment & Inference Engines  \n",
    "**Time:** 3 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand TensorRT-LLM's optimization pipeline\n",
    "- [ ] Build optimized TensorRT engines for LLMs\n",
    "- [ ] Benchmark prefill performance (TensorRT-LLM's strength)\n",
    "- [ ] Configure quantization and other optimizations\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Tasks 12.1 and 12.2\n",
    "- Docker with NVIDIA container toolkit\n",
    "- ~3 hours for engine build (can run in background)\n",
    "- HuggingFace access token for gated models\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**When to use TensorRT-LLM:**\n",
    "\n",
    "TensorRT-LLM shines when you need the **fastest possible prefill** (processing the input prompt).\n",
    "This is critical for:\n",
    "\n",
    "- **RAG applications**: Processing long retrieved documents\n",
    "- **Code completion**: Large context windows with existing code\n",
    "- **Summarization**: Long documents need fast input processing\n",
    "- **First-response latency**: Getting that first token FAST\n",
    "\n",
    "Companies like NVIDIA, Meta, and AWS use TensorRT-LLM for production deployments where every millisecond counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What is TensorRT-LLM?\n",
    "\n",
    "> **Imagine you're a chef preparing for a big dinner...**\n",
    ">\n",
    "> **Regular inference** = You cook each dish from scratch when orders come in.\n",
    "> Every time someone orders pasta, you boil water, make sauce, cook noodles.\n",
    ">\n",
    "> **TensorRT-LLM** = You spend HOURS preparing in advance.\n",
    "> - Pre-chop all vegetables (fuse operations)\n",
    "> - Pre-make sauces (optimize kernels)\n",
    "> - Set up assembly lines (operator fusion)\n",
    "> - When orders come in, dishes fly out! (fast inference)\n",
    ">\n",
    "> The **build time is long** (45-90 minutes), but the **serving is blazing fast**.\n",
    ">\n",
    "> **In AI terms:** TensorRT-LLM analyzes your model, fuses operations, generates\n",
    "> custom CUDA kernels for your specific GPU, and creates an optimized \"engine\"\n",
    "> that runs much faster than the original model.\n",
    "\n",
    "---\n",
    "\n",
    "## üîë TensorRT-LLM Optimizations\n",
    "\n",
    "| Optimization | What It Does | Benefit |\n",
    "|--------------|--------------|--------|\n",
    "| **Operator Fusion** | Combines multiple ops into one | Fewer memory transfers |\n",
    "| **Custom Kernels** | GPU-specific code generation | Maximum hardware utilization |\n",
    "| **In-flight Batching** | Like continuous batching | High throughput |\n",
    "| **Fused MLP** | Combines MLP layers | Faster feedforward |\n",
    "| **FP8/FP4 Quantization** | Lower precision math | 2-4x faster on Blackwell |\n",
    "| **Paged KV Cache** | Dynamic memory allocation | Better memory efficiency |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment Setup\n",
    "\n",
    "TensorRT-LLM requires NVIDIA's container for proper setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def run_command(cmd, shell=True):\n",
    "    \"\"\"Run a shell command and return output.\"\"\"\n",
    "    result = subprocess.run(cmd, shell=shell, capture_output=True, text=True)\n",
    "    return result.stdout.strip(), result.stderr.strip(), result.returncode\n",
    "\n",
    "# Check system\n",
    "print(\"üîç System Check for TensorRT-LLM:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Architecture\n",
    "arch, _, _ = run_command(\"uname -m\")\n",
    "print(f\"   Architecture: {arch}\")\n",
    "\n",
    "# GPU info\n",
    "gpu_info, _, _ = run_command(\"nvidia-smi --query-gpu=name,compute_cap --format=csv,noheader\")\n",
    "if gpu_info:\n",
    "    name, compute = gpu_info.split(\",\")\n",
    "    print(f\"   GPU: {name.strip()}\")\n",
    "    print(f\"   Compute Capability: {compute.strip()}\")\n",
    "\n",
    "# Check for TRT-LLM container\n",
    "docker_images, _, _ = run_command(\"docker images --format '{{.Repository}}:{{.Tag}}' | grep tensorrt\")\n",
    "print(f\"   TensorRT-LLM containers: {docker_images if docker_images else 'None found'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üê≥ Setting Up TensorRT-LLM Container\n",
    "\n",
    "TensorRT-LLM is best run in NVIDIA's official container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate container setup commands\nimport platform\n\n# Check architecture first - critical for DGX Spark\narch = platform.machine()\nprint(f\"üîç System Architecture: {arch}\")\n\nif arch == \"aarch64\":\n    print(\"   ‚úì DGX Spark detected (ARM64/aarch64)\")\n    print(\"\")\n    print(\"   ‚ö†Ô∏è  IMPORTANT: Verify container ARM64 support before using!\")\n    print(\"   Check NGC catalog: https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver\")\n    print(\"\")\n    print(\"   If the TensorRT-LLM container doesn't support ARM64, use this alternative:\")\n    print(\"   nvcr.io/nvidia/pytorch:25.11-py3 (then install TensorRT-LLM from source)\")\nelif arch == \"x86_64\":\n    print(\"   ‚úì x86_64 architecture detected\")\nelse:\n    print(f\"   ‚ö†Ô∏è  Unknown architecture: {arch}\")\n\n# TensorRT-LLM container configuration\n# NOTE: Verify ARM64 support at NGC catalog before using on DGX Spark\ntrtllm_container = \"nvcr.io/nvidia/tritonserver:25.11-trtllm-python-py3\"\n\nworkspace_dir = Path.home() / \"trtllm-workspace\"\nmodels_dir = workspace_dir / \"models\"\nengines_dir = workspace_dir / \"engines\"\n\nprint(\"üì¶ TensorRT-LLM Container Setup for DGX Spark\")\nprint(\"=\" * 60)\nprint(f\"\"\"\n# Step 1: Create workspace directories\nmkdir -p {models_dir}\nmkdir -p {engines_dir}\n\n# Step 2: For 70B models, clear buffer cache first\nsudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'\n\n# Step 3: Pull the TensorRT-LLM container\n# NOTE: Verify ARM64 support at NGC catalog before pulling\ndocker pull {trtllm_container}\n\n# Step 4: Start interactive container\n# Key flags for DGX Spark:\n#   --ipc=host       : Required for DataLoader workers\n#   --shm-size=16g   : Shared memory for optimization\n#   --ulimit memlock=-1 : Unlimited locked memory\ndocker run --gpus all -it --rm \\\\\n    -v {workspace_dir}:/workspace \\\\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\\\n    -e HF_TOKEN=$HF_TOKEN \\\\\n    --ipc=host \\\\\n    --shm-size=16g \\\\\n    --ulimit memlock=-1 \\\\\n    {trtllm_container} \\\\\n    bash\n\n# Inside the container, TensorRT-LLM is pre-installed!\n\"\"\")\n\nprint(\"üí° Copy these commands to your terminal to get started.\")\nprint(\"‚ö†Ô∏è Verify container ARM64 support at: https://catalog.ngc.nvidia.com/\")\nprint(\"   If no ARM64 support, use PyTorch NGC container and install TensorRT-LLM from source.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding the TensorRT-LLM Pipeline\n",
    "\n",
    "Building a TensorRT engine involves several steps:\n",
    "\n",
    "```\n",
    "HuggingFace Model ‚Üí Convert to TRT-LLM format ‚Üí Build TensorRT Engine ‚Üí Deploy\n",
    "```\n",
    "\n",
    "### Pipeline Diagram\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  HuggingFace Model  ‚îÇ  (meta-llama/Llama-3.1-8B)\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "           ‚îÇ\n",
    "           ‚ñº convert_checkpoint.py\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  TRT-LLM Checkpoint ‚îÇ  (optimized weights format)\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "           ‚îÇ\n",
    "           ‚ñº trtllm-build\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  TensorRT Engine    ‚îÇ  (GPU-specific binary)\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "           ‚îÇ\n",
    "           ‚ñº Triton or TRT-LLM Server\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Production API     ‚îÇ  (OpenAI-compatible)\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorRT-LLM Build Configuration\n",
    "# This generates the commands you'll run inside the container\n",
    "\n",
    "def generate_trtllm_build_commands(\n",
    "    model_name: str = \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    output_name: str = \"llama-8b-trtllm\",\n",
    "    dtype: str = \"bfloat16\",\n",
    "    max_input_len: int = 4096,\n",
    "    max_output_len: int = 2048,\n",
    "    max_batch_size: int = 8,\n",
    "    use_fused_mlp: bool = True,\n",
    "    quantization: str = None,  # None, \"fp8\", \"int8_sq\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate TensorRT-LLM build commands.\n",
    "    \n",
    "    Args:\n",
    "        model_name: HuggingFace model ID\n",
    "        output_name: Name for the output engine\n",
    "        dtype: Data type (float16, bfloat16)\n",
    "        max_input_len: Maximum input sequence length\n",
    "        max_output_len: Maximum output sequence length\n",
    "        max_batch_size: Maximum batch size\n",
    "        use_fused_mlp: Enable fused MLP optimization\n",
    "        quantization: Quantization method if any\n",
    "    \"\"\"\n",
    "    \n",
    "    checkpoint_dir = f\"/workspace/checkpoints/{output_name}\"\n",
    "    engine_dir = f\"/workspace/engines/{output_name}\"\n",
    "    \n",
    "    # Step 1: Convert checkpoint\n",
    "    convert_cmd = f\"\"\"# Step 1: Convert HuggingFace model to TRT-LLM checkpoint\n",
    "python /opt/TensorRT-LLM/examples/llama/convert_checkpoint.py \\\\\n",
    "    --model_dir {model_name} \\\\\n",
    "    --output_dir {checkpoint_dir} \\\\\n",
    "    --dtype {dtype}\"\"\"\n",
    "    \n",
    "    if quantization == \"fp8\":\n",
    "        convert_cmd += \" \\\\\n",
    "    --use_fp8\"\n",
    "    elif quantization == \"int8_sq\":\n",
    "        convert_cmd += \" \\\\\n",
    "    --use_smooth_quant\"\n",
    "    \n",
    "    # Step 2: Build engine\n",
    "    build_cmd = f\"\"\"\\n\\n# Step 2: Build TensorRT engine (this takes 45-90 minutes)\n",
    "trtllm-build \\\\\n",
    "    --checkpoint_dir {checkpoint_dir} \\\\\n",
    "    --output_dir {engine_dir} \\\\\n",
    "    --max_input_len {max_input_len} \\\\\n",
    "    --max_seq_len {max_input_len + max_output_len} \\\\\n",
    "    --max_batch_size {max_batch_size} \\\\\n",
    "    --gemm_plugin {dtype}\"\"\"\n",
    "    \n",
    "    if use_fused_mlp:\n",
    "        build_cmd += \" \\\\\n",
    "    --use_fused_mlp enable\"\n",
    "    \n",
    "    # Step 3: Run server\n",
    "    server_cmd = f\"\"\"\\n\\n# Step 3: Start the inference server\n",
    "python /opt/TensorRT-LLM/examples/run.py \\\\\n",
    "    --engine_dir {engine_dir} \\\\\n",
    "    --tokenizer_dir {model_name} \\\\\n",
    "    --max_output_len 512 \\\\\n",
    "    --input_text \"Hello, how are you?\"\"\"\n",
    "    \n",
    "    return convert_cmd + build_cmd + server_cmd\n",
    "\n",
    "# Generate commands for Llama 3.1 8B\n",
    "print(\"üîß TensorRT-LLM Build Commands for Llama 3.1 8B\")\n",
    "print(\"=\" * 60)\n",
    "print(generate_trtllm_build_commands())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚è±Ô∏è Build Time Expectations\n",
    "\n",
    "| Model Size | Approximate Build Time | Engine Size |\n",
    "|------------|------------------------|-------------|\n",
    "| 7-8B | 45-60 minutes | ~15 GB |\n",
    "| 13B | 60-90 minutes | ~25 GB |\n",
    "| 70B | 2-3 hours | ~140 GB |\n",
    "\n",
    "**Tip:** Start the build and let it run in the background while you work on other tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Configuration Options Deep Dive\n",
    "\n",
    "Let's explore the key configuration options for TensorRT-LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorRT-LLM Configuration Guide\n",
    "\n",
    "trtllm_configs = {\n",
    "    \"low_latency\": {\n",
    "        \"description\": \"Minimize time-to-first-token\",\n",
    "        \"settings\": {\n",
    "            \"max_batch_size\": 4,\n",
    "            \"max_input_len\": 2048,\n",
    "            \"max_output_len\": 512,\n",
    "            \"use_fused_mlp\": True,\n",
    "            \"paged_kv_cache\": True,\n",
    "            \"dtype\": \"bfloat16\"\n",
    "        },\n",
    "        \"use_case\": \"Interactive chat, real-time applications\"\n",
    "    },\n",
    "    \"high_throughput\": {\n",
    "        \"description\": \"Maximize requests per second\",\n",
    "        \"settings\": {\n",
    "            \"max_batch_size\": 64,\n",
    "            \"max_input_len\": 4096,\n",
    "            \"max_output_len\": 2048,\n",
    "            \"use_fused_mlp\": True,\n",
    "            \"paged_kv_cache\": True,\n",
    "            \"inflight_batching\": True,\n",
    "            \"dtype\": \"bfloat16\"\n",
    "        },\n",
    "        \"use_case\": \"Batch processing, API serving\"\n",
    "    },\n",
    "    \"long_context\": {\n",
    "        \"description\": \"For RAG and document processing\",\n",
    "        \"settings\": {\n",
    "            \"max_batch_size\": 8,\n",
    "            \"max_input_len\": 32768,\n",
    "            \"max_output_len\": 4096,\n",
    "            \"use_fused_mlp\": True,\n",
    "            \"paged_kv_cache\": True,\n",
    "            \"dtype\": \"bfloat16\"\n",
    "        },\n",
    "        \"use_case\": \"RAG, document summarization\"\n",
    "    },\n",
    "    \"fp8_quantized\": {\n",
    "        \"description\": \"FP8 for Blackwell GPU (2x speedup)\",\n",
    "        \"settings\": {\n",
    "            \"max_batch_size\": 32,\n",
    "            \"max_input_len\": 4096,\n",
    "            \"max_output_len\": 2048,\n",
    "            \"use_fused_mlp\": True,\n",
    "            \"paged_kv_cache\": True,\n",
    "            \"quantization\": \"fp8\",\n",
    "            \"dtype\": \"float16\"  # FP8 compute, FP16 I/O\n",
    "        },\n",
    "        \"use_case\": \"Maximum performance on Blackwell\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìã TensorRT-LLM Configuration Profiles\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, config in trtllm_configs.items():\n",
    "    print(f\"\\nüîß {name.upper()}\")\n",
    "    print(f\"   Description: {config['description']}\")\n",
    "    print(f\"   Use case: {config['use_case']}\")\n",
    "    print(f\"   Settings:\")\n",
    "    for key, value in config['settings'].items():\n",
    "        print(f\"      {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîë Key Parameters Explained\n",
    "\n",
    "| Parameter | Description | Trade-off |\n",
    "|-----------|-------------|----------|\n",
    "| `max_batch_size` | Maximum concurrent requests | Higher = more throughput, more memory |\n",
    "| `max_input_len` | Maximum prompt length | Higher = longer context, more memory |\n",
    "| `max_output_len` | Maximum generated length | Higher = longer responses, more memory |\n",
    "| `use_fused_mlp` | Fuse MLP operations | Faster, no downside |\n",
    "| `paged_kv_cache` | Dynamic KV cache allocation | More efficient memory |\n",
    "| `gemm_plugin` | Use optimized GEMM kernels | Faster matrix operations |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Benchmarking TensorRT-LLM\n",
    "\n",
    "Let's set up benchmarking for when your engine is built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from typing import List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class TRTLLMBenchmarkResult:\n",
    "    \"\"\"Result from TensorRT-LLM benchmark.\"\"\"\n",
    "    prompt: str\n",
    "    prompt_tokens: int\n",
    "    output_tokens: int\n",
    "    prefill_time_ms: float\n",
    "    decode_time_ms: float\n",
    "    total_time_ms: float\n",
    "    prefill_tokens_per_sec: float\n",
    "    decode_tokens_per_sec: float\n",
    "    \n",
    "def benchmark_trtllm(\n",
    "    server_url: str,\n",
    "    prompt: str,\n",
    "    max_tokens: int = 100\n",
    ") -> Optional[TRTLLMBenchmarkResult]:\n",
    "    \"\"\"\n",
    "    Benchmark a single request to TensorRT-LLM server.\n",
    "    \n",
    "    Args:\n",
    "        server_url: TRT-LLM server URL\n",
    "        prompt: Input prompt\n",
    "        max_tokens: Maximum tokens to generate\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.perf_counter()\n",
    "        first_token_time = None\n",
    "        output_tokens = 0\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{server_url}/v1/chat/completions\",\n",
    "            json={\n",
    "                \"model\": \"tensorrt_llm\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"stream\": True\n",
    "            },\n",
    "            stream=True,\n",
    "            timeout=120\n",
    "        )\n",
    "        \n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                line_str = line.decode()\n",
    "                if line_str.startswith(\"data: \"):\n",
    "                    data_str = line_str[6:]\n",
    "                    if data_str == \"[DONE]\":\n",
    "                        break\n",
    "                    try:\n",
    "                        chunk = json.loads(data_str)\n",
    "                        if chunk.get(\"choices\", [{}])[0].get(\"delta\", {}).get(\"content\"):\n",
    "                            if first_token_time is None:\n",
    "                                first_token_time = time.perf_counter()\n",
    "                            output_tokens += 1\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        prefill_time = (first_token_time - start_time) * 1000 if first_token_time else 0\n",
    "        total_time = (end_time - start_time) * 1000\n",
    "        decode_time = total_time - prefill_time\n",
    "        \n",
    "        # Estimate prompt tokens (rough)\n",
    "        prompt_tokens = len(prompt.split()) * 1.3\n",
    "        \n",
    "        return TRTLLMBenchmarkResult(\n",
    "            prompt=prompt[:50] + \"...\",\n",
    "            prompt_tokens=int(prompt_tokens),\n",
    "            output_tokens=output_tokens,\n",
    "            prefill_time_ms=prefill_time,\n",
    "            decode_time_ms=decode_time,\n",
    "            total_time_ms=total_time,\n",
    "            prefill_tokens_per_sec=prompt_tokens / (prefill_time / 1000) if prefill_time > 0 else 0,\n",
    "            decode_tokens_per_sec=output_tokens / (decode_time / 1000) if decode_time > 0 else 0\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Benchmark error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefill-focused benchmark prompts (different input lengths)\n",
    "prefill_benchmark_prompts = {\n",
    "    \"short\": \"What is 2+2?\",\n",
    "    \"medium\": \"\"\"Explain the concept of neural networks and how they learn from data. \n",
    "    Include information about backpropagation, gradient descent, and activation functions.\"\"\",\n",
    "    \"long\": \"\"\"The following is an excerpt from a technical document about machine learning:\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and \n",
    "improve from experience without being explicitly programmed. The field has evolved significantly \n",
    "over the past decades, from simple linear regression models to complex deep neural networks \n",
    "that can process images, text, and audio with remarkable accuracy.\n",
    "\n",
    "Deep learning, a subset of machine learning, uses artificial neural networks with multiple \n",
    "layers to progressively extract higher-level features from raw input. For example, in image \n",
    "processing, lower layers may identify edges, while higher layers may identify concepts relevant \n",
    "to humans such as digits or letters or faces.\n",
    "\n",
    "The training process involves feeding large amounts of labeled data through the network, \n",
    "calculating the error between predictions and actual values, and then adjusting the network's \n",
    "parameters to minimize this error. This is typically done using optimization algorithms like \n",
    "stochastic gradient descent (SGD) or Adam.\n",
    "\n",
    "Transformer models, introduced in 2017, have revolutionized natural language processing. \n",
    "These models use self-attention mechanisms to process input sequences in parallel, allowing \n",
    "them to capture long-range dependencies more effectively than previous recurrent architectures.\n",
    "\n",
    "Given this context, please summarize the key points about machine learning evolution.\"\"\",\n",
    "}\n",
    "\n",
    "print(\"üìù Prefill Benchmark Prompts:\")\n",
    "for name, prompt in prefill_benchmark_prompts.items():\n",
    "    word_count = len(prompt.split())\n",
    "    estimated_tokens = int(word_count * 1.3)\n",
    "    print(f\"   {name}: ~{estimated_tokens} tokens ({word_count} words)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prefill benchmark (if TRT-LLM is running)\n",
    "TRTLLM_URL = \"http://localhost:8000\"  # Adjust if using different port\n",
    "\n",
    "def check_trtllm_status(url: str) -> bool:\n",
    "    \"\"\"Check if TensorRT-LLM server is running.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{url}/v1/models\", timeout=5)\n",
    "        return response.status_code == 200\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "if check_trtllm_status(TRTLLM_URL):\n",
    "    print(\"‚úÖ TensorRT-LLM server is running!\")\n",
    "    print(\"\\nüìä Running prefill-focused benchmark...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = []\n",
    "    for name, prompt in prefill_benchmark_prompts.items():\n",
    "        print(f\"\\nTesting {name} prompt...\")\n",
    "        result = benchmark_trtllm(TRTLLM_URL, prompt, max_tokens=100)\n",
    "        if result:\n",
    "            results.append((name, result))\n",
    "            print(f\"   Prefill: {result.prefill_time_ms:.1f}ms \"\n",
    "                  f\"({result.prefill_tokens_per_sec:.0f} tok/s)\")\n",
    "            print(f\"   Decode:  {result.decode_time_ms:.1f}ms \"\n",
    "                  f\"({result.decode_tokens_per_sec:.0f} tok/s)\")\n",
    "    \n",
    "    # Summary\n",
    "    if results:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üìà PREFILL PERFORMANCE SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"{'Prompt':<10} {'Tokens':<10} {'Prefill (ms)':<15} {'Prefill (tok/s)':<15}\")\n",
    "        print(\"-\" * 50)\n",
    "        for name, r in results:\n",
    "            print(f\"{name:<10} {r.prompt_tokens:<10} {r.prefill_time_ms:<15.1f} {r.prefill_tokens_per_sec:<15.0f}\")\n",
    "else:\n",
    "    print(\"‚ùå TensorRT-LLM server is not running\")\n",
    "    print(\"\\nüìù Simulated benchmark results for demonstration:\")\n",
    "    print(\"\")\n",
    "    print(\"TensorRT-LLM excels at prefill (processing input):\")\n",
    "    print(f\"{'Prompt':<10} {'Tokens':<10} {'Prefill (ms)':<15} {'Prefill (tok/s)':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'short':<10} {'10':<10} {'8.5':<15} {'1176':<15}\")\n",
    "    print(f\"{'medium':<10} {'50':<10} {'15.2':<15} {'3289':<15}\")\n",
    "    print(f\"{'long':<10} {'250':<10} {'45.8':<15} {'5459':<15}\")\n",
    "    print(\"\\nüí° Note: TRT-LLM's prefill speed scales well with longer inputs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Understanding Prefill Performance\n",
    "\n",
    "TensorRT-LLM typically achieves:\n",
    "- **Prefill: 3,000-10,000 tokens/second** (vs 500-2,000 for other engines)\n",
    "- **Decode: 50-150 tokens/second** (similar to other engines)\n",
    "\n",
    "This makes TensorRT-LLM ideal for:\n",
    "- Long context applications (RAG)\n",
    "- Latency-sensitive first-token requirements\n",
    "- High-throughput batch processing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Comparing TensorRT-LLM vs Other Engines\n",
    "\n",
    "Let's create a comparison framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison data (from typical benchmarks)\n",
    "# These are representative values - actual results will vary\n",
    "\n",
    "comparison_data = {\n",
    "    \"Ollama\": {\n",
    "        \"prefill_tok_s\": 800,\n",
    "        \"decode_tok_s\": 85,\n",
    "        \"ttft_ms\": 45,\n",
    "        \"setup_time\": \"Minutes\",\n",
    "        \"ease_of_use\": 5,\n",
    "        \"best_for\": \"Development, easy setup\"\n",
    "    },\n",
    "    \"vLLM\": {\n",
    "        \"prefill_tok_s\": 1500,\n",
    "        \"decode_tok_s\": 75,\n",
    "        \"ttft_ms\": 35,\n",
    "        \"setup_time\": \"Minutes\",\n",
    "        \"ease_of_use\": 4,\n",
    "        \"best_for\": \"High throughput, batching\"\n",
    "    },\n",
    "    \"TensorRT-LLM\": {\n",
    "        \"prefill_tok_s\": 5000,\n",
    "        \"decode_tok_s\": 70,\n",
    "        \"ttft_ms\": 20,\n",
    "        \"setup_time\": \"Hours\",\n",
    "        \"ease_of_use\": 2,\n",
    "        \"best_for\": \"Lowest latency, long context\"\n",
    "    },\n",
    "    \"llama.cpp\": {\n",
    "        \"prefill_tok_s\": 600,\n",
    "        \"decode_tok_s\": 95,\n",
    "        \"ttft_ms\": 50,\n",
    "        \"setup_time\": \"Minutes\",\n",
    "        \"ease_of_use\": 3,\n",
    "        \"best_for\": \"Fastest decode, GGUF format\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìä Inference Engine Comparison (8B model, typical values)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Engine':<15} {'Prefill':<15} {'Decode':<12} {'TTFT':<10} {'Setup':<10} {'Best For'}\")\n",
    "print(f\"{'':15} {'(tok/s)':<15} {'(tok/s)':<12} {'(ms)':<10} {'Time':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for engine, data in comparison_data.items():\n",
    "    print(f\"{engine:<15} {data['prefill_tok_s']:<15} {data['decode_tok_s']:<12} \"\n",
    "          f\"{data['ttft_ms']:<10} {data['setup_time']:<10} {data['best_for']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize comparison\ntry:\n    import matplotlib.pyplot as plt\n    import numpy as np\n    \n    engines = list(comparison_data.keys())\n    prefill = [comparison_data[e][\"prefill_tok_s\"] for e in engines]\n    decode = [comparison_data[e][\"decode_tok_s\"] for e in engines]\n    ttft = [comparison_data[e][\"ttft_ms\"] for e in engines]\n    \n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    \n    colors = ['#4C78A8', '#F58518', '#E45756', '#72B7B2']\n    \n    # Prefill speed\n    axes[0].bar(engines, prefill, color=colors)\n    axes[0].set_ylabel('Tokens/second')\n    axes[0].set_title('Prefill Speed (higher is better)')\n    axes[0].tick_params(axis='x', rotation=15)\n    \n    # Decode speed\n    axes[1].bar(engines, decode, color=colors)\n    axes[1].set_ylabel('Tokens/second')\n    axes[1].set_title('Decode Speed (higher is better)')\n    axes[1].tick_params(axis='x', rotation=15)\n    \n    # TTFT\n    axes[2].bar(engines, ttft, color=colors)\n    axes[2].set_ylabel('Milliseconds')\n    axes[2].set_title('Time to First Token (lower is better)')\n    axes[2].tick_params(axis='x', rotation=15)\n    \n    plt.tight_layout()\n    plt.savefig('engine_comparison.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    print(\"\\nüìà Chart saved to engine_comparison.png\")\n    \nexcept ImportError:\n    print(\"‚ö†Ô∏è matplotlib not available for visualization\")\n    print(\"   Install with: pip install matplotlib\")\n    print(\"   Or in NGC container: pip install matplotlib --user\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ‚ö†Ô∏è Common Mistakes\n\n### Mistake 1: Building Without Enough Memory\n\n```bash\n# ‚ùå Wrong - Build will fail or be very slow\ndocker run --gpus all -it nvcr.io/nvidia/tritonserver:25.11-trtllm-python-py3\n\n# ‚úÖ Right - Allocate enough shared memory\ndocker run --gpus all --shm-size=16g -it nvcr.io/nvidia/tritonserver:25.11-trtllm-python-py3\n```\n\n**Why:** TensorRT-LLM's build process needs significant shared memory for graph optimization.\n\n### Mistake 2: Mismatched max_input_len and max_seq_len\n\n```bash\n# ‚ùå Wrong - max_seq_len must be >= max_input_len + max_output_len\ntrtllm-build --max_input_len 4096 --max_seq_len 4096\n\n# ‚úÖ Right - Leave room for output\ntrtllm-build --max_input_len 4096 --max_seq_len 6144  # 4096 + 2048\n```\n\n**Why:** If max_seq_len equals max_input_len, there's no room for output tokens.\n\n### Mistake 3: Not Using gemm_plugin\n\n```bash\n# ‚ùå Wrong - Slower matrix operations\ntrtllm-build --output_dir ./engine\n\n# ‚úÖ Right - Use optimized GEMM kernels\ntrtllm-build --output_dir ./engine --gemm_plugin bfloat16\n```\n\n**Why:** The GEMM plugin provides significant speedups for matrix multiplications.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Build a TensorRT Engine\n",
    "\n",
    "Follow the steps above to build a TensorRT engine for Llama 3.1 8B. Time the build process and note the engine size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Document your build process\n",
    "\n",
    "# Build Configuration:\n",
    "build_config = {\n",
    "    \"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"dtype\": \"bfloat16\",\n",
    "    \"max_input_len\": 4096,\n",
    "    \"max_output_len\": 2048,\n",
    "    \"max_batch_size\": 8,\n",
    "    # Add your other settings...\n",
    "}\n",
    "\n",
    "# Results (fill in after building):\n",
    "build_results = {\n",
    "    \"build_time_minutes\": None,  # TODO: Record this\n",
    "    \"engine_size_gb\": None,      # TODO: Check with ls -lh\n",
    "    \"errors_encountered\": [],     # TODO: Document any issues\n",
    "}\n",
    "\n",
    "print(\"üìù Document your build results above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Prefill Scaling Test\n",
    "\n",
    "Test how TensorRT-LLM's prefill speed scales with input length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Prefill scaling test\n",
    "# Create prompts of lengths: 100, 500, 1000, 2000, 4000 tokens\n",
    "# Measure prefill time for each\n",
    "# Plot the relationship\n",
    "\n",
    "# TODO: Your code here\n",
    "# Hint: Use lorem ipsum or repeated text to create consistent prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How TensorRT-LLM optimizes models for NVIDIA GPUs\n",
    "- ‚úÖ The build pipeline: HuggingFace ‚Üí Checkpoint ‚Üí Engine\n",
    "- ‚úÖ Key configuration options for different use cases\n",
    "- ‚úÖ When to choose TensorRT-LLM over other engines\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Build an FP8 Quantized Engine**\n",
    "\n",
    "DGX Spark's Blackwell GPU supports FP8 inference. Try building an FP8 engine and compare:\n",
    "1. Build time\n",
    "2. Engine size\n",
    "3. Inference speed\n",
    "4. Output quality\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [TensorRT-LLM GitHub](https://github.com/NVIDIA/TensorRT-LLM)\n",
    "- [TensorRT-LLM Performance Guide](https://nvidia.github.io/TensorRT-LLM/performance/perf-overview.html)\n",
    "- [Triton Inference Server Integration](https://github.com/triton-inference-server/tensorrtllm_backend)\n",
    "- [FP8 Training and Inference](https://developer.nvidia.com/blog/nvidia-hopper-architecture-enables-fp8-training-and-inference/)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cleanup\nimport gc\n\ngc.collect()\n\nprint(\"‚úÖ Cleanup complete!\")\nprint(\"\\nüí° To stop TensorRT-LLM container:\")\nprint(\"   docker stop $(docker ps -q --filter ancestor=nvcr.io/nvidia/tritonserver:25.11-trtllm-python-py3)\")\nprint(\"\\n‚ö†Ô∏è Engine files can be large (~15GB per 8B model)\")\nprint(\"   Delete unused engines to save space.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}