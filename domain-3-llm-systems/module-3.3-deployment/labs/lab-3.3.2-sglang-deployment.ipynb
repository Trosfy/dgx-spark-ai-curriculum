{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.3.2: SGLang Deployment with RadixAttention\n",
    "\n",
    "**Module:** 3.3 - Model Deployment & Inference Engines  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand how RadixAttention enables 29-45% faster inference through prefix caching\n",
    "- [ ] Deploy SGLang on DGX Spark with optimal configuration\n",
    "- [ ] Measure and verify prefix cache hit rates\n",
    "- [ ] Build applications that leverage shared system prompts efficiently\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Lab 3.3.1 (Engine Benchmark)\n",
    "- Knowledge of: REST APIs, Python async programming\n",
    "- Having: Hugging Face account with accepted Llama license\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The Problem:** In production chatbots, every user shares the same system prompt:\n",
    "\n",
    "```\n",
    "\"You are a helpful customer service agent for Acme Corp. You help customers with orders, returns, and product questions. Always be polite and professional...\"\n",
    "```\n",
    "\n",
    "**The Waste:** Traditional inference engines recompute this system prompt for EVERY request. If you have 1000 users, you compute the same 500 tokens 1000 times!\n",
    "\n",
    "**SGLang's Solution:** RadixAttention caches the KV (key-value) computations for shared prefixes. Once the system prompt is processed, subsequent users get it \"for free\".\n",
    "\n",
    "**Real Impact:**\n",
    "- OpenAI's API uses similar prefix caching (that's why structured prompts are encouraged)\n",
    "- Production deployments see 29-45% latency reduction\n",
    "- Memory efficiency improves since cached prefixes aren't duplicated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What is RadixAttention?\n",
    "\n",
    "> **Imagine you're a teacher grading homework...**\n",
    ">\n",
    "> Every student's paper starts with the same header:\n",
    "> - \"Name: _____\"\n",
    "> - \"Date: _____\"\n",
    "> - \"Class: Math 101\"\n",
    "> - \"Assignment: Chapter 5 Problems\"\n",
    ">\n",
    "> **The OLD way:** You read this entire header for EVERY paper, even though it's the same.\n",
    ">\n",
    "> **The RadixAttention way:** You read the header ONCE, remember it, and when you see the same header on the next paper, you skip right to the unique part (the actual answers).\n",
    ">\n",
    "> The \"Radix\" in RadixAttention comes from **radix trees** - a special data structure that efficiently stores strings with common prefixes (like how \"apple\", \"application\", and \"apply\" share \"appl\").\n",
    ">\n",
    "> **In AI terms:** SGLang stores the computed attention values (KV cache) for prompts it has seen before. When a new prompt shares a prefix with a cached one, it reuses those computations instead of redoing them.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä When RadixAttention Helps Most\n",
    "\n",
    "| Scenario | Cache Benefit | Example |\n",
    "|----------|---------------|--------|\n",
    "| **Same system prompt** | üî•üî•üî• Huge | Chatbots, assistants |\n",
    "| **Few-shot examples** | üî•üî• Large | When you include examples in every prompt |\n",
    "| **Document QA** | üî•üî• Large | Questions about the same document |\n",
    "| **Continuation** | üî• Moderate | Generating more text from same context |\n",
    "| **Unique prompts** | ‚ùÑÔ∏è None | Every prompt is different |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setting Up SGLang on DGX Spark\n",
    "\n",
    "First, let's understand how to deploy SGLang properly on DGX Spark's ARM64 architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "# Add scripts directory to path\n",
    "scripts_path = Path(\"../scripts\").resolve()\n",
    "sys.path.insert(0, str(scripts_path))\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(f\"üìÅ Scripts path: {scripts_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU status - DGX Spark should show ~128GB unified memory\n",
    "def check_gpu_status():\n",
    "    \"\"\"Check GPU availability and memory on DGX Spark.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"nvidia-smi\", \"--query-gpu=name,memory.total,memory.free,memory.used\",\n",
    "             \"--format=csv,noheader,nounits\"],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            values = result.stdout.strip().split(\",\")\n",
    "            name = values[0].strip()\n",
    "            total_gb = int(values[1]) / 1024\n",
    "            free_gb = int(values[2]) / 1024\n",
    "            used_gb = int(values[3]) / 1024\n",
    "            \n",
    "            print(\"üñ•Ô∏è GPU Status:\")\n",
    "            print(f\"   Name: {name}\")\n",
    "            print(f\"   Memory: {used_gb:.1f}GB used / {total_gb:.1f}GB total\")\n",
    "            print(f\"   Free: {free_gb:.1f}GB available\")\n",
    "            \n",
    "            if \"GB10\" in name or total_gb > 100:\n",
    "                print(\"   ‚úÖ DGX Spark detected!\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è GPU check failed: {e}\")\n",
    "    return False\n",
    "\n",
    "check_gpu_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Starting SGLang Server\n",
    "\n",
    "SGLang can be started in several ways on DGX Spark:\n",
    "\n",
    "**Option 1: Direct installation (recommended for DGX Spark)**\n",
    "```bash\n",
    "# SGLang has native ARM64/Blackwell support\n",
    "pip install sglang[all]\n",
    "\n",
    "# Start the server\n",
    "python -m sglang.launch_server \\\n",
    "    --model-path meta-llama/Llama-3.1-8B-Instruct \\\n",
    "    --port 30000 \\\n",
    "    --dtype bfloat16 \\\n",
    "    --mem-fraction-static 0.85\n",
    "```\n",
    "\n",
    "**Option 2: Using NGC container**\n",
    "```bash\n",
    "docker run --gpus all -p 30000:30000 \\\n",
    "    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n",
    "    -e HF_TOKEN=$HF_TOKEN \\\n",
    "    --ipc=host \\\n",
    "    nvcr.io/nvidia/pytorch:25.11-py3 \\\n",
    "    bash -c \"pip install 'sglang[all]' && \\\n",
    "            python -m sglang.launch_server \\\n",
    "            --model-path meta-llama/Llama-3.1-8B-Instruct \\\n",
    "            --port 30000 \\\n",
    "            --dtype bfloat16\"\n",
    "```\n",
    "\n",
    "**Key flags for DGX Spark:**\n",
    "- `--dtype bfloat16`: Native Blackwell support\n",
    "- `--mem-fraction-static 0.85`: Reserve 85% of GPU memory for KV cache\n",
    "- `--enable-prefix-caching`: Enabled by default, but can be explicitly set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if SGLang server is running\n",
    "SGLANG_URL = \"http://localhost:30000\"\n",
    "\n",
    "def check_sglang_server(url: str = SGLANG_URL) -> bool:\n",
    "    \"\"\"Check if SGLang server is running and accessible.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{url}/v1/models\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get(\"data\", [])\n",
    "            print(f\"‚úÖ SGLang server is running at {url}\")\n",
    "            if models:\n",
    "                for model in models:\n",
    "                    print(f\"   Model: {model.get('id', 'unknown')}\")\n",
    "            return True\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"‚ùå SGLang server not running at {url}\")\n",
    "        print(\"\\nüìù To start SGLang, run in a separate terminal:\")\n",
    "        print(\"   python -m sglang.launch_server \\\\\")\n",
    "        print(\"       --model-path meta-llama/Llama-3.1-8B-Instruct \\\\\")\n",
    "        print(\"       --port 30000 \\\\\")\n",
    "        print(\"       --dtype bfloat16\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking SGLang: {e}\")\n",
    "    return False\n",
    "\n",
    "sglang_available = check_sglang_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding Prefix Caching with RadixAttention\n",
    "\n",
    "Let's visualize how RadixAttention works by comparing requests with and without shared prefixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our test scenario: customer service chatbot\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful customer service assistant for TechCorp Inc.\n",
    "You help customers with:\n",
    "- Product information and specifications\n",
    "- Order status and tracking\n",
    "- Returns and refunds\n",
    "- Technical support\n",
    "\n",
    "Always be polite, professional, and concise.\n",
    "If you don't know something, say so and offer to connect them with a specialist.\n",
    "\"\"\"\n",
    "\n",
    "# Different user questions that all share the same system prompt\n",
    "USER_QUESTIONS = [\n",
    "    \"Where is my order #12345?\",\n",
    "    \"How do I return a product?\",\n",
    "    \"What's the warranty on the X500 laptop?\",\n",
    "    \"My device won't turn on, what should I do?\",\n",
    "    \"Can I change my shipping address?\",\n",
    "    \"What payment methods do you accept?\",\n",
    "    \"Is the Y200 compatible with Mac?\",\n",
    "    \"How long does shipping take?\",\n",
    "]\n",
    "\n",
    "print(f\"üìù System prompt length: {len(SYSTEM_PROMPT)} characters\")\n",
    "print(f\"‚ùì Number of test questions: {len(USER_QUESTIONS)}\")\n",
    "print(\"\\nüîë Key insight: All questions share the SAME system prompt!\")\n",
    "print(\"   With RadixAttention, we only compute the system prompt ONCE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_chat_request(\n",
    "    url: str,\n",
    "    system_prompt: str,\n",
    "    user_message: str,\n",
    "    max_tokens: int = 100,\n",
    "    temperature: float = 0.7\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Send a chat request and measure timing.\n",
    "    \n",
    "    Returns dict with:\n",
    "        - response: The generated text\n",
    "        - ttft: Time to first token (seconds)\n",
    "        - total_time: Total request time (seconds)\n",
    "        - tokens: Number of tokens generated\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    first_token_time = None\n",
    "    chunks = []\n",
    "    \n",
    "    try:\n",
    "        # Use streaming to measure TTFT\n",
    "        response = requests.post(\n",
    "            f\"{url}/v1/chat/completions\",\n",
    "            json={\n",
    "                \"model\": \"default\",\n",
    "                \"messages\": messages,\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"stream\": True\n",
    "            },\n",
    "            stream=True,\n",
    "            timeout=60\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                line_str = line.decode(\"utf-8\")\n",
    "                if line_str.startswith(\"data: \"):\n",
    "                    data_str = line_str[6:]\n",
    "                    if data_str.strip() == \"[DONE]\":\n",
    "                        break\n",
    "                    try:\n",
    "                        chunk = json.loads(data_str)\n",
    "                        delta = chunk.get(\"choices\", [{}])[0].get(\"delta\", {})\n",
    "                        content = delta.get(\"content\", \"\")\n",
    "                        if content:\n",
    "                            if first_token_time is None:\n",
    "                                first_token_time = time.perf_counter()\n",
    "                            chunks.append(content)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "        \n",
    "        end_time = time.perf_counter()\n",
    "        total_time = end_time - start_time\n",
    "        ttft = (first_token_time - start_time) if first_token_time else total_time\n",
    "        \n",
    "        return {\n",
    "            \"response\": \"\".join(chunks),\n",
    "            \"ttft\": ttft,\n",
    "            \"total_time\": total_time,\n",
    "            \"tokens\": len(chunks),  # Approximate\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"response\": \"\",\n",
    "            \"ttft\": 0,\n",
    "            \"total_time\": time.perf_counter() - start_time,\n",
    "            \"tokens\": 0,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Experiment: Measuring Prefix Cache Effect\n",
    "\n",
    "We'll send multiple requests with the same system prompt and observe how TTFT improves after the first request (when the prefix gets cached)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_prefix_caching(url: str, system_prompt: str, questions: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Benchmark prefix caching by sending multiple requests with the same system prompt.\n",
    "    \n",
    "    The first request should be slower (cache miss).\n",
    "    Subsequent requests should be faster (cache hit).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(\"\\nüöÄ Benchmarking prefix caching...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, question in enumerate(questions):\n",
    "        print(f\"\\n[{i+1}/{len(questions)}] {question[:50]}...\")\n",
    "        \n",
    "        result = send_chat_request(\n",
    "            url=url,\n",
    "            system_prompt=system_prompt,\n",
    "            user_message=question,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"is_first\": i == 0,\n",
    "            **result\n",
    "        })\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            cache_status = \"‚ùÑÔ∏è Cold (cache miss)\" if i == 0 else \"üî• Warm (cache hit)\"\n",
    "            print(f\"   {cache_status}\")\n",
    "            print(f\"   TTFT: {result['ttft']*1000:.1f}ms | Total: {result['total_time']*1000:.1f}ms\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Error: {result.get('error', 'Unknown')}\")\n",
    "    \n",
    "    return analyze_prefix_cache_results(results)\n",
    "\n",
    "\n",
    "def analyze_prefix_cache_results(results: List[Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze prefix caching benchmark results.\"\"\"\n",
    "    successful = [r for r in results if r[\"success\"]]\n",
    "    \n",
    "    if len(successful) < 2:\n",
    "        return {\"error\": \"Not enough successful requests to analyze\"}\n",
    "    \n",
    "    # First request (cold cache)\n",
    "    cold = successful[0]\n",
    "    \n",
    "    # Subsequent requests (warm cache)\n",
    "    warm = successful[1:]\n",
    "    \n",
    "    cold_ttft = cold[\"ttft\"] * 1000  # ms\n",
    "    warm_ttfts = [r[\"ttft\"] * 1000 for r in warm]\n",
    "    avg_warm_ttft = np.mean(warm_ttfts)\n",
    "    \n",
    "    speedup = cold_ttft / avg_warm_ttft if avg_warm_ttft > 0 else 0\n",
    "    reduction = (cold_ttft - avg_warm_ttft) / cold_ttft * 100 if cold_ttft > 0 else 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä PREFIX CACHING ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n   Cold cache TTFT (first request): {cold_ttft:.1f}ms\")\n",
    "    print(f\"   Warm cache TTFT (avg of {len(warm)}): {avg_warm_ttft:.1f}ms\")\n",
    "    print(f\"\\n   üöÄ Speedup: {speedup:.2f}x\")\n",
    "    print(f\"   üìâ Latency reduction: {reduction:.1f}%\")\n",
    "    \n",
    "    if reduction > 20:\n",
    "        print(\"\\n   ‚úÖ RadixAttention is working! Prefix caching effective.\")\n",
    "    else:\n",
    "        print(\"\\n   ‚ö†Ô∏è Low reduction. Possible reasons:\")\n",
    "        print(\"      - Prefix caching not enabled\")\n",
    "        print(\"      - System prompt too short to benefit\")\n",
    "        print(\"      - Server under heavy load\")\n",
    "    \n",
    "    return {\n",
    "        \"cold_ttft_ms\": cold_ttft,\n",
    "        \"warm_ttft_ms\": avg_warm_ttft,\n",
    "        \"speedup\": speedup,\n",
    "        \"reduction_percent\": reduction,\n",
    "        \"num_requests\": len(successful)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the benchmark if SGLang is available\n",
    "if sglang_available:\n",
    "    prefix_results = benchmark_prefix_caching(\n",
    "        url=SGLANG_URL,\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        questions=USER_QUESTIONS\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è SGLang not available. Start the server to run this benchmark.\")\n",
    "    print(\"\\nüìä Expected results with RadixAttention:\")\n",
    "    print(\"   - First request (cold): ~200-400ms TTFT\")\n",
    "    print(\"   - Subsequent (warm): ~100-200ms TTFT\")\n",
    "    print(\"   - Speedup: 1.5-2.5x on shared prefixes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "When we sent multiple requests with the same system prompt:\n",
    "\n",
    "1. **First request (cold cache):**\n",
    "   - SGLang computes the full KV cache for the system prompt\n",
    "   - Stores the KV cache in the radix tree indexed by the prompt content\n",
    "   - Higher TTFT because we're doing full prefill\n",
    "\n",
    "2. **Subsequent requests (warm cache):**\n",
    "   - SGLang looks up the system prompt in the radix tree\n",
    "   - Finds a matching prefix ‚Üí Cache hit!\n",
    "   - Reuses the stored KV cache, only computes the new user message\n",
    "   - Lower TTFT because we skip re-computing the system prompt\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Advanced Prefix Caching Patterns\n",
    "\n",
    "Let's explore more sophisticated uses of prefix caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 1: Few-shot learning with cached examples\n",
    "FEW_SHOT_PREFIX = \"\"\"\n",
    "You are a sentiment analyzer. Classify text as positive, negative, or neutral.\n",
    "\n",
    "Examples:\n",
    "Text: \"I love this product! Best purchase ever!\"\n",
    "Sentiment: positive\n",
    "\n",
    "Text: \"This is the worst service I've experienced.\"\n",
    "Sentiment: negative\n",
    "\n",
    "Text: \"The package arrived on time.\"\n",
    "Sentiment: neutral\n",
    "\n",
    "Text: \"Absolutely fantastic! Exceeded all expectations!\"\n",
    "Sentiment: positive\n",
    "\n",
    "Text: \"Terrible quality, broke after one use.\"\n",
    "Sentiment: negative\n",
    "\n",
    "Now classify the following:\n",
    "\"\"\"\n",
    "\n",
    "SENTIMENT_QUERIES = [\n",
    "    \"The food was okay, nothing special.\",\n",
    "    \"I'm so happy with this purchase!\",\n",
    "    \"Complete waste of money.\",\n",
    "    \"It works as described.\",\n",
    "    \"This changed my life for the better!\",\n",
    "]\n",
    "\n",
    "print(f\"üìù Few-shot prefix length: {len(FEW_SHOT_PREFIX)} characters\")\n",
    "print(f\"   This prefix includes 5 training examples\")\n",
    "print(f\"   All {len(SENTIMENT_QUERIES)} queries will share this prefix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 2: Document QA with cached document\n",
    "DOCUMENT_CONTEXT = \"\"\"\n",
    "# DGX Spark Technical Specifications\n",
    "\n",
    "## Overview\n",
    "DGX Spark is NVIDIA's first personal AI computer, bringing AI supercomputing\n",
    "capabilities to your desktop.\n",
    "\n",
    "## Hardware Specifications\n",
    "- **GPU**: NVIDIA Blackwell GB10 Superchip\n",
    "- **CPU**: 20 ARM v9.2 cores (10 Cortex-X925 + 10 Cortex-A725)\n",
    "- **Memory**: 128GB LPDDR5X Unified Memory (shared CPU+GPU)\n",
    "- **Memory Bandwidth**: 273 GB/s\n",
    "- **CUDA Cores**: 6,144\n",
    "- **Tensor Cores**: 192 (5th generation)\n",
    "\n",
    "## Performance\n",
    "- 1 PFLOP FP4 (NVFP4 quantization)\n",
    "- ~209 TFLOPS FP8\n",
    "- ~100 TFLOPS BF16\n",
    "\n",
    "## Model Capacity\n",
    "- FP16 Inference: Up to 50-55B parameters\n",
    "- FP8 Inference: Up to 90-100B parameters\n",
    "- NVFP4 Inference: Up to ~200B parameters\n",
    "- QLoRA Fine-tuning: Up to 100-120B parameters\n",
    "\n",
    "## Key Features\n",
    "- Unified memory eliminates CPU-GPU transfers\n",
    "- Native ARM64 architecture\n",
    "- Desktop form factor\n",
    "- NVLink-C2C for dual-system configurations\n",
    "\n",
    "Based on this document, answer the following question:\n",
    "\"\"\"\n",
    "\n",
    "DOCUMENT_QUESTIONS = [\n",
    "    \"How much memory does DGX Spark have?\",\n",
    "    \"What is the FP4 performance?\",\n",
    "    \"What's the maximum model size for FP16 inference?\",\n",
    "    \"How many CUDA cores does it have?\",\n",
    "    \"What CPU architecture is used?\",\n",
    "]\n",
    "\n",
    "print(f\"üìÑ Document context length: {len(DOCUMENT_CONTEXT)} characters\")\n",
    "print(f\"   Questions that reuse this context: {len(DOCUMENT_QUESTIONS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmarks on both patterns if SGLang is available\n",
    "if sglang_available:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Pattern 1: Few-Shot Learning\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    few_shot_results = benchmark_prefix_caching(\n",
    "        url=SGLANG_URL,\n",
    "        system_prompt=FEW_SHOT_PREFIX,\n",
    "        questions=SENTIMENT_QUERIES\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Pattern 2: Document QA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    doc_qa_results = benchmark_prefix_caching(\n",
    "        url=SGLANG_URL,\n",
    "        system_prompt=DOCUMENT_CONTEXT,\n",
    "        questions=DOCUMENT_QUESTIONS\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è SGLang not available. See expected results in solution notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Comparing SGLang with Other Engines\n",
    "\n",
    "Let's compare SGLang's prefix caching with vLLM (which also supports prefix caching) and Ollama (which doesn't)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_engines_prefix_caching(\n",
    "    engines: Dict[str, str],  # {\"engine_name\": \"url\"}\n",
    "    system_prompt: str,\n",
    "    questions: List[str]\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Compare prefix caching performance across engines.\n",
    "    \n",
    "    Returns dictionary with results for each engine.\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "    \n",
    "    for engine_name, url in engines.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing: {engine_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Check if engine is available\n",
    "        try:\n",
    "            response = requests.get(f\"{url}/v1/models\", timeout=3)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"‚ö†Ô∏è {engine_name} not responding\")\n",
    "                continue\n",
    "        except:\n",
    "            print(f\"‚ö†Ô∏è {engine_name} not available at {url}\")\n",
    "            continue\n",
    "        \n",
    "        # Run benchmark\n",
    "        results = []\n",
    "        for i, question in enumerate(questions):\n",
    "            result = send_chat_request(\n",
    "                url=url,\n",
    "                system_prompt=system_prompt,\n",
    "                user_message=question,\n",
    "                max_tokens=100\n",
    "            )\n",
    "            results.append(result)\n",
    "            \n",
    "            if result[\"success\"]:\n",
    "                status = \"‚ùÑÔ∏è\" if i == 0 else \"üî•\"\n",
    "                print(f\"  {status} [{i+1}] TTFT: {result['ttft']*1000:.1f}ms\")\n",
    "            else:\n",
    "                print(f\"  ‚ùå [{i+1}] Error\")\n",
    "        \n",
    "        # Compute stats\n",
    "        successful = [r for r in results if r[\"success\"]]\n",
    "        if len(successful) >= 2:\n",
    "            cold_ttft = successful[0][\"ttft\"] * 1000\n",
    "            warm_ttfts = [r[\"ttft\"] * 1000 for r in successful[1:]]\n",
    "            avg_warm = np.mean(warm_ttfts)\n",
    "            \n",
    "            all_results[engine_name] = {\n",
    "                \"cold_ttft_ms\": cold_ttft,\n",
    "                \"warm_ttft_ms\": avg_warm,\n",
    "                \"speedup\": cold_ttft / avg_warm if avg_warm > 0 else 1,\n",
    "                \"reduction_percent\": (cold_ttft - avg_warm) / cold_ttft * 100 if cold_ttft > 0 else 0\n",
    "            }\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define engines to compare\n",
    "ENGINES_TO_COMPARE = {\n",
    "    \"SGLang\": \"http://localhost:30000\",\n",
    "    \"vLLM\": \"http://localhost:8000\",\n",
    "}\n",
    "\n",
    "# Run comparison\n",
    "comparison_results = compare_engines_prefix_caching(\n",
    "    engines=ENGINES_TO_COMPARE,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    questions=USER_QUESTIONS[:5]  # Use first 5 for quick comparison\n",
    ")\n",
    "\n",
    "# Print comparison table\n",
    "if comparison_results:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä PREFIX CACHING COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n{'Engine':<15} {'Cold TTFT':>12} {'Warm TTFT':>12} {'Speedup':>10} {'Reduction':>12}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for engine, stats in comparison_results.items():\n",
    "        print(f\"{engine:<15} {stats['cold_ttft_ms']:>10.1f}ms {stats['warm_ttft_ms']:>10.1f}ms \"\n",
    "              f\"{stats['speedup']:>9.2f}x {stats['reduction_percent']:>10.1f}%\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No engines available for comparison.\")\n",
    "    print(\"   Start SGLang and/or vLLM servers to run this comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Expecting Cache Hits with Different Prefixes\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - Different system prompts = no cache reuse\n",
    "prompt1 = \"You are a helpful assistant. Answer questions.\"\n",
    "prompt2 = \"You are a helpful assistant. Answer questions!\"  # Different punctuation!\n",
    "\n",
    "# ‚úÖ Right - Exact same prefix for cache hits\n",
    "SHARED_PROMPT = \"You are a helpful assistant. Answer questions.\"\n",
    "# Use SHARED_PROMPT for ALL requests\n",
    "```\n",
    "\n",
    "**Why:** RadixAttention matches prefixes exactly. Even a single character difference creates a new cache entry.\n",
    "\n",
    "### Mistake 2: Not Warming Up Before Benchmarking\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - First request includes model loading overhead\n",
    "results = benchmark_all_questions()  # First result is artificially slow\n",
    "\n",
    "# ‚úÖ Right - Warm up with unrelated requests first\n",
    "warm_up_request(\"Hello\")  # Load model, initialize KV cache\n",
    "results = benchmark_all_questions()  # Now measuring actual inference\n",
    "```\n",
    "\n",
    "### Mistake 3: Using Too Short Prefixes\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - Short prefixes have minimal cache benefit\n",
    "system_prompt = \"You are helpful.\"  # Only ~5 tokens\n",
    "\n",
    "# ‚úÖ Right - Longer prefixes show more benefit\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful customer service assistant for TechCorp Inc.\n",
    "You help customers with product information, order status,\n",
    "returns, refunds, and technical support. Always be polite,\n",
    "professional, and concise...\n",
    "\"\"\"  # ~100+ tokens - significant cache benefit\n",
    "```\n",
    "\n",
    "**Why:** The overhead of cache lookup vs. computation means very short prefixes don't benefit much.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Design a Production Chatbot Prefix\n",
    "\n",
    "Create an optimized system prompt for a specific use case that maximizes prefix cache benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Design a system prompt for a code review assistant\n",
    "\n",
    "CODE_REVIEW_PROMPT = \"\"\"\n",
    "# TODO: Create a comprehensive system prompt for a code review assistant\n",
    "# Include:\n",
    "# - What languages you specialize in\n",
    "# - What aspects of code you review (style, bugs, performance, security)\n",
    "# - How you format your feedback\n",
    "# - Example reviews (few-shot)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "CODE_REVIEW_QUERIES = [\n",
    "    \"Review this Python function: def add(a, b): return a + b\",\n",
    "    \"Review this: for i in range(len(lst)): print(lst[i])\",\n",
    "    \"Review this: password = input('password: ')\",\n",
    "]\n",
    "\n",
    "# Benchmark your prompt\n",
    "# TODO: Run benchmark_prefix_caching with your prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "A good code review system prompt should:\n",
    "1. Be specific about the reviewer's expertise\n",
    "2. Include 2-3 example reviews to establish the format\n",
    "3. End with a clear instruction like \"Review the following code:\"\n",
    "\n",
    "This creates a long, reusable prefix that benefits from caching.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Exercise 2: Measure Cache Eviction\n",
    "\n",
    "What happens when you have many different prefixes? Test cache eviction behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Create multiple different system prompts and see how the cache handles them\n",
    "\n",
    "DIFFERENT_PROMPTS = [\n",
    "    \"You are a Python expert.\",\n",
    "    \"You are a JavaScript expert.\",\n",
    "    \"You are a Rust expert.\",\n",
    "    \"You are a Go expert.\",\n",
    "    # Add more...\n",
    "]\n",
    "\n",
    "# TODO:\n",
    "# 1. Send requests with each different prompt\n",
    "# 2. Then go back to the first prompt - is it still cached?\n",
    "# 3. At what point does cache eviction happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How RadixAttention works to cache and reuse prefix computations\n",
    "- ‚úÖ How to deploy SGLang on DGX Spark with optimal settings\n",
    "- ‚úÖ How to measure and verify prefix cache hit rates\n",
    "- ‚úÖ Best practices for designing cacheable prompts\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Build a Prefix Cache Monitor**\n",
    "\n",
    "Create a real-time monitoring dashboard that shows:\n",
    "1. Cache hit rate over time\n",
    "2. Memory usage by cached prefixes\n",
    "3. Most frequently cached prefixes\n",
    "4. Alert when cache hit rate drops\n",
    "\n",
    "This would be valuable for production monitoring!\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [SGLang Paper: RadixAttention](https://arxiv.org/abs/2312.07104)\n",
    "- [SGLang GitHub Repository](https://github.com/sgl-project/sglang)\n",
    "- [Efficient Prompt Caching (Anthropic Blog)](https://www.anthropic.com/news/prompt-caching)\n",
    "- [vLLM Automatic Prefix Caching](https://docs.vllm.ai/en/latest/automatic_prefix_caching/)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cleanup\nimport gc\n\n# Clear Python garbage\ngc.collect()\n\n# Clear GPU memory cache if torch is available\ntry:\n    import torch\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n        print(\"‚úÖ GPU memory cache cleared!\")\nexcept ImportError:\n    pass\n\nprint(\"‚úÖ Cleanup complete!\")\nprint(\"\\nüìù Remember: Stop SGLang server when done:\")\nprint(\"   pkill -f 'sglang.launch_server'\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}