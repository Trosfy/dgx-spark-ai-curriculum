{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.3.4: Speculative Decoding with SGLang\n",
    "\n",
    "**Module:** 3.3 - Model Deployment & Inference Engines  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand how speculative decoding accelerates inference\n",
    "- [ ] Set up SGLang with EAGLE-3 speculative decoding\n",
    "- [ ] Measure and analyze the speedup from speculation\n",
    "- [ ] Know when speculative decoding helps vs hurts\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Tasks 12.1-12.3\n",
    "- Understanding of: Autoregressive generation\n",
    "- Hardware: GPU with enough memory for both draft and target models\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The Problem with LLM Generation:**\n",
    "\n",
    "LLMs generate text one token at a time. Each token requires a full forward pass through the model.\n",
    "A 100-token response needs 100 sequential forward passes - that's slow!\n",
    "\n",
    "**Speculative Decoding's Solution:**\n",
    "\n",
    "What if we could \"guess\" the next several tokens quickly, then verify them in parallel?\n",
    "That's exactly what speculative decoding does!\n",
    "\n",
    "**Real-world impact:**\n",
    "- 2-3x faster generation for well-matched tasks\n",
    "- Same output quality (mathematically identical)\n",
    "- Used by Google (Gemini), Meta (Llama), and many production systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: Speculative Decoding\n",
    "\n",
    "> **Imagine you're a slow but careful painter, and you have a fast but less careful assistant...**\n",
    ">\n",
    "> **Without speculative decoding:**\n",
    "> You (the expert) paint one brushstroke, step back, think carefully, paint another brushstroke.\n",
    "> Very slow, but every stroke is perfect.\n",
    ">\n",
    "> **With speculative decoding:**\n",
    "> 1. Your assistant quickly sketches the next 5 brushstrokes (draft)\n",
    "> 2. You look at all 5 at once and say \"Yes, yes, no, yes, yes\" (verify)\n",
    "> 3. You keep the correct ones (3-4 strokes accepted!)\n",
    "> 4. For the wrong one, you paint it correctly\n",
    "> 5. Repeat!\n",
    ">\n",
    "> Even though you reject some guesses, you're still faster because you verified\n",
    "> 5 strokes in the time it would have taken to paint 1!\n",
    ">\n",
    "> **In AI terms:**\n",
    "> - **Draft model** = Fast, smaller model that guesses multiple tokens\n",
    "> - **Target model** = Your actual large model that verifies\n",
    "> - **Acceptance rate** = How often the draft's guesses are accepted\n",
    "> - **Speedup** = Depends on acceptance rate and draft speed\n",
    "\n",
    "---\n",
    "\n",
    "## üîë Key Concepts\n",
    "\n",
    "| Term | Meaning |\n",
    "|------|--------|\n",
    "| **Draft Model** | Small, fast model that proposes tokens |\n",
    "| **Target Model** | Your main model that verifies proposals |\n",
    "| **Speculation Length** | How many tokens to guess at once (typically 3-8) |\n",
    "| **Acceptance Rate** | % of draft tokens accepted by target |\n",
    "| **Wallclock Speedup** | Actual time saved (what you care about) |\n",
    "| **EAGLE** | A learned draft head attached to the target model |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding the Algorithm\n",
    "\n",
    "Let's implement a simplified version to understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class SpeculationResult:\n",
    "    \"\"\"Result of a speculation round.\"\"\"\n",
    "    draft_tokens: List[str]\n",
    "    accepted_tokens: List[str]\n",
    "    acceptance_rate: float\n",
    "    time_saved_factor: float\n",
    "\n",
    "\n",
    "def simulate_speculative_decoding(\n",
    "    target_vocabulary: List[str],\n",
    "    sequence_length: int = 50,\n",
    "    speculation_length: int = 5,\n",
    "    draft_accuracy: float = 0.7,  # How often draft matches target\n",
    "    draft_speed_multiplier: float = 10.0,  # How much faster draft is\n",
    ") -> Tuple[List[str], dict]:\n",
    "    \"\"\"\n",
    "    Simulate speculative decoding to understand the algorithm.\n",
    "    \n",
    "    Args:\n",
    "        target_vocabulary: Possible tokens to generate\n",
    "        sequence_length: Total tokens to generate\n",
    "        speculation_length: Tokens to speculate per round\n",
    "        draft_accuracy: Probability draft matches target\n",
    "        draft_speed_multiplier: How much faster draft model is\n",
    "    \"\"\"\n",
    "    \n",
    "    generated = []\n",
    "    stats = {\n",
    "        \"target_forward_passes\": 0,\n",
    "        \"draft_forward_passes\": 0,\n",
    "        \"tokens_accepted\": 0,\n",
    "        \"tokens_rejected\": 0,\n",
    "        \"speculation_rounds\": 0\n",
    "    }\n",
    "    \n",
    "    while len(generated) < sequence_length:\n",
    "        stats[\"speculation_rounds\"] += 1\n",
    "        \n",
    "        # Step 1: Draft model generates speculation_length tokens quickly\n",
    "        draft_tokens = [random.choice(target_vocabulary) for _ in range(speculation_length)]\n",
    "        stats[\"draft_forward_passes\"] += speculation_length\n",
    "        \n",
    "        # Step 2: Target model verifies all draft tokens in one pass\n",
    "        # (In reality, this is done with careful probability matching)\n",
    "        target_tokens = [random.choice(target_vocabulary) for _ in range(speculation_length)]\n",
    "        stats[\"target_forward_passes\"] += 1  # Just one pass for all tokens!\n",
    "        \n",
    "        # Step 3: Accept tokens until first mismatch\n",
    "        for i, (draft, target) in enumerate(zip(draft_tokens, target_tokens)):\n",
    "            # Simulate acceptance probability\n",
    "            if random.random() < draft_accuracy:\n",
    "                # Draft matches target - accept!\n",
    "                generated.append(draft)\n",
    "                stats[\"tokens_accepted\"] += 1\n",
    "                if len(generated) >= sequence_length:\n",
    "                    break\n",
    "            else:\n",
    "                # Draft wrong - use target's token and stop\n",
    "                generated.append(target)\n",
    "                stats[\"tokens_rejected\"] += 1\n",
    "                break  # Must restart speculation\n",
    "    \n",
    "    # Calculate speedup\n",
    "    # Without speculation: sequence_length target forward passes\n",
    "    # With speculation: target_forward_passes + draft_forward_passes/draft_speed_multiplier\n",
    "    baseline_cost = sequence_length  # 1 target pass per token\n",
    "    speculative_cost = (\n",
    "        stats[\"target_forward_passes\"] + \n",
    "        stats[\"draft_forward_passes\"] / draft_speed_multiplier\n",
    "    )\n",
    "    \n",
    "    stats[\"baseline_cost\"] = baseline_cost\n",
    "    stats[\"speculative_cost\"] = speculative_cost\n",
    "    stats[\"speedup\"] = baseline_cost / speculative_cost\n",
    "    stats[\"acceptance_rate\"] = stats[\"tokens_accepted\"] / (stats[\"tokens_accepted\"] + stats[\"tokens_rejected\"])\n",
    "    \n",
    "    return generated[:sequence_length], stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulation\n",
    "vocabulary = [\"the\", \"a\", \"is\", \"was\", \"it\", \"that\", \"for\", \"on\", \"with\", \"as\", \n",
    "              \"be\", \"at\", \"by\", \"this\", \"have\", \"from\", \"or\", \"but\", \"not\", \"are\"]\n",
    "\n",
    "print(\"üìä Speculative Decoding Simulation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test different draft accuracies\n",
    "for accuracy in [0.5, 0.7, 0.85, 0.95]:\n",
    "    tokens, stats = simulate_speculative_decoding(\n",
    "        target_vocabulary=vocabulary,\n",
    "        sequence_length=100,\n",
    "        speculation_length=5,\n",
    "        draft_accuracy=accuracy,\n",
    "        draft_speed_multiplier=10.0\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDraft Accuracy: {accuracy:.0%}\")\n",
    "    print(f\"   Acceptance Rate: {stats['acceptance_rate']:.1%}\")\n",
    "    print(f\"   Target Forward Passes: {stats['target_forward_passes']} (baseline: 100)\")\n",
    "    print(f\"   Speedup: {stats['speedup']:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Key Insights\n",
    "\n",
    "1. **Acceptance rate is crucial**: Higher acceptance = more speedup\n",
    "2. **Even 50% acceptance helps**: Because verification is parallelized\n",
    "3. **Draft model must be FAST**: The speed multiplier matters a lot\n",
    "4. **Returns diminish at very high accuracy**: You're already accepting most tokens\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Setting Up SGLang with Speculative Decoding\n",
    "\n",
    "SGLang supports several speculative decoding methods:\n",
    "- **EAGLE**: Learned draft head\n",
    "- **EAGLE-3**: Improved version with better acceptance\n",
    "- **Medusa**: Multiple draft heads for different positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Check SGLang installation\n",
    "def check_sglang():\n",
    "    \"\"\"Check if SGLang is available.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"python\", \"-c\", \"import sglang; print(sglang.__version__)\"],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            return result.stdout.strip()\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "sglang_version = check_sglang()\n",
    "if sglang_version:\n",
    "    print(f\"‚úÖ SGLang version: {sglang_version}\")\n",
    "else:\n",
    "    print(\"‚ùå SGLang not installed\")\n",
    "    print(\"\\nüì¶ Install with:\")\n",
    "    print(\"   pip install sglang[all]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöÄ Starting SGLang with EAGLE\n",
    "\n",
    "SGLang supports speculative decoding with EAGLE draft models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sglang_commands(\n",
    "    model: str = \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    eagle_model: str = None,  # EAGLE draft model\n",
    "    port: int = 30000,\n",
    "    speculate_num_tokens: int = 5\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Generate SGLang startup commands.\n",
    "    \n",
    "    Returns dict with 'basic' and 'speculative' commands.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Basic SGLang (no speculation)\n",
    "    basic_cmd = f\"\"\"python -m sglang.launch_server \\\\\n",
    "    --model {model} \\\\\n",
    "    --port {port} \\\\\n",
    "    --trust-remote-code\"\"\"\n",
    "    \n",
    "    # With EAGLE speculative decoding\n",
    "    if eagle_model:\n",
    "        speculative_cmd = f\"\"\"python -m sglang.launch_server \\\\\n",
    "    --model {model} \\\\\n",
    "    --port {port} \\\\\n",
    "    --speculative-algorithm EAGLE \\\\\n",
    "    --speculative-draft-model {eagle_model} \\\\\n",
    "    --speculative-num-draft-tokens {speculate_num_tokens} \\\\\n",
    "    --trust-remote-code\"\"\"\n",
    "    else:\n",
    "        # For models without EAGLE, use smaller model as draft\n",
    "        speculative_cmd = f\"\"\"# Note: EAGLE models available for some popular architectures\n",
    "# Check: https://huggingface.co/collections/yuhuili/eagle-models\n",
    "\n",
    "# Example with custom draft model:\n",
    "python -m sglang.launch_server \\\\\n",
    "    --model {model} \\\\\n",
    "    --port {port} \\\\\n",
    "    --speculative-algorithm EAGLE \\\\\n",
    "    --speculative-draft-model yuhuili/EAGLE-LLaMA3-Instruct-8B \\\\\n",
    "    --speculative-num-draft-tokens {speculate_num_tokens} \\\\\n",
    "    --trust-remote-code\"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"basic\": basic_cmd,\n",
    "        \"speculative\": speculative_cmd\n",
    "    }\n",
    "\n",
    "commands = generate_sglang_commands(\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    eagle_model=\"yuhuili/EAGLE-LLaMA3-Instruct-8B\"\n",
    ")\n",
    "\n",
    "print(\"üìã SGLang Startup Commands\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüê¢ WITHOUT Speculative Decoding:\")\n",
    "print(commands[\"basic\"])\n",
    "print(\"\\nüöÄ WITH EAGLE Speculative Decoding:\")\n",
    "print(commands[\"speculative\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available EAGLE Models\n",
    "\n",
    "EAGLE draft models are pre-trained for specific target models:\n",
    "\n",
    "| Target Model | EAGLE Draft Model |\n",
    "|--------------|-------------------|\n",
    "| Llama-3-8B-Instruct | yuhuili/EAGLE-LLaMA3-Instruct-8B |\n",
    "| Llama-3-70B-Instruct | yuhuili/EAGLE-LLaMA3-Instruct-70B |\n",
    "| Vicuna-7B | yuhuili/EAGLE-Vicuna-7B-v1.3 |\n",
    "| Vicuna-13B | yuhuili/EAGLE-Vicuna-13B-v1.3 |\n",
    "| Mixtral-8x7B | yuhuili/EAGLE-Mixtral-8x7B-Instruct-v0.1 |\n",
    "\n",
    "Check [EAGLE Models Collection](https://huggingface.co/collections/yuhuili/eagle-models) for the latest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Benchmarking Speculative Decoding\n",
    "\n",
    "Let's create benchmarking tools to measure speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class SpeedBenchmarkResult:\n",
    "    \"\"\"Result from a single speed benchmark.\"\"\"\n",
    "    prompt: str\n",
    "    output: str\n",
    "    output_tokens: int\n",
    "    total_time_s: float\n",
    "    tokens_per_second: float\n",
    "    ttft_s: float\n",
    "    is_speculative: bool\n",
    "\n",
    "\n",
    "def benchmark_generation(\n",
    "    server_url: str,\n",
    "    prompt: str,\n",
    "    max_tokens: int = 200,\n",
    "    is_speculative: bool = False\n",
    ") -> Optional[SpeedBenchmarkResult]:\n",
    "    \"\"\"\n",
    "    Benchmark text generation speed.\n",
    "    \n",
    "    Args:\n",
    "        server_url: SGLang server URL\n",
    "        prompt: Input prompt\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        is_speculative: Whether this is speculative mode (for labeling)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.perf_counter()\n",
    "        first_token_time = None\n",
    "        output_chunks = []\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{server_url}/v1/chat/completions\",\n",
    "            json={\n",
    "                \"model\": \"default\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"stream\": True,\n",
    "                \"temperature\": 0.7\n",
    "            },\n",
    "            stream=True,\n",
    "            timeout=120\n",
    "        )\n",
    "        \n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                line_str = line.decode()\n",
    "                if line_str.startswith(\"data: \"):\n",
    "                    data_str = line_str[6:]\n",
    "                    if data_str == \"[DONE]\":\n",
    "                        break\n",
    "                    try:\n",
    "                        chunk = json.loads(data_str)\n",
    "                        content = chunk.get(\"choices\", [{}])[0].get(\"delta\", {}).get(\"content\", \"\")\n",
    "                        if content:\n",
    "                            if first_token_time is None:\n",
    "                                first_token_time = time.perf_counter()\n",
    "                            output_chunks.append(content)\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        end_time = time.perf_counter()\n",
    "        output = \"\".join(output_chunks)\n",
    "        \n",
    "        # Estimate tokens (rough)\n",
    "        output_tokens = len(output.split()) * 1.3\n",
    "        total_time = end_time - start_time\n",
    "        ttft = (first_token_time - start_time) if first_token_time else total_time\n",
    "        \n",
    "        return SpeedBenchmarkResult(\n",
    "            prompt=prompt[:50] + \"...\",\n",
    "            output=output[:100] + \"...\",\n",
    "            output_tokens=int(output_tokens),\n",
    "            total_time_s=total_time,\n",
    "            tokens_per_second=output_tokens / total_time if total_time > 0 else 0,\n",
    "            ttft_s=ttft,\n",
    "            is_speculative=is_speculative\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Benchmark error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark prompts designed to test speculation\n",
    "# Speculation works best when output is somewhat predictable\n",
    "\n",
    "speculation_benchmark_prompts = {\n",
    "    \"predictable\": [\n",
    "        \"Count from 1 to 20.\",\n",
    "        \"List the days of the week.\",\n",
    "        \"Recite the alphabet.\",\n",
    "        \"List the months of the year.\",\n",
    "    ],\n",
    "    \"semi_predictable\": [\n",
    "        \"Write a Python function to calculate factorial.\",\n",
    "        \"Explain the water cycle step by step.\",\n",
    "        \"Describe how to make a peanut butter sandwich.\",\n",
    "    ],\n",
    "    \"creative\": [\n",
    "        \"Write a creative short story about a time-traveling cat.\",\n",
    "        \"Compose a unique poem about the color blue.\",\n",
    "        \"Invent a new word and define it.\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"üìù Speculation Benchmark Prompts:\")\n",
    "for category, prompts in speculation_benchmark_prompts.items():\n",
    "    print(f\"\\n   {category.upper()}:\")\n",
    "    for p in prompts:\n",
    "        print(f\"      - {p[:50]}...\" if len(p) > 50 else f\"      - {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run comparative benchmark\nSGLANG_URL = \"http://localhost:30000\"\n\ndef check_sglang_server(url: str) -> bool:\n    \"\"\"Check if SGLang server is running.\"\"\"\n    try:\n        response = requests.get(f\"{url}/v1/models\", timeout=5)\n        return response.status_code == 200\n    except:\n        return False\n\nif check_sglang_server(SGLANG_URL):\n    print(\"‚úÖ SGLang server is running!\")\n    print(\"\\nüìä Running speculation benchmark...\")\n    print(\"=\" * 60)\n    \n    all_results = []\n    \n    for category, prompts in speculation_benchmark_prompts.items():\n        print(f\"\\nCategory: {category}\")\n        \n        for prompt in prompts[:2]:  # First 2 per category\n            result = benchmark_generation(\n                SGLANG_URL, \n                prompt, \n                max_tokens=100\n            )\n            \n            if result:\n                print(f\"   {prompt[:30]}... ‚Üí {result.tokens_per_second:.1f} tok/s\")\n                all_results.append((category, result))\n    \n    # Summary\n    if all_results:\n        print(\"\\n\" + \"=\" * 60)\n        print(\"üìà SUMMARY BY CATEGORY\")\n        print(\"=\" * 60)\n        \n        for cat in speculation_benchmark_prompts.keys():\n            cat_results = [r for c, r in all_results if c == cat]\n            if cat_results:\n                avg_speed = sum(r.tokens_per_second for r in cat_results) / len(cat_results)\n                print(f\"   {cat}: {avg_speed:.1f} tokens/sec average\")\n\nelse:\n    print(\"‚ùå SGLang server is not running\")\n    print(\"\\n\" + \"=\" * 60)\n    print(\"‚ö†Ô∏è  SIMULATED DATA - For Demonstration Only\")\n    print(\"=\" * 60)\n    print(\"\\nüìù These are typical expected results with speculative decoding.\")\n    print(\"   Run SGLang to get actual measurements on your hardware.\")\n    print(\"\")\n    print(f\"{'Category':<20} {'Without Spec':<15} {'With EAGLE':<15} {'Speedup'}\")\n    print(\"-\" * 60)\n    print(f\"{'predictable':<20} {'45 tok/s':<15} {'110 tok/s':<15} {'2.4x'}\")\n    print(f\"{'semi_predictable':<20} {'45 tok/s':<15} {'85 tok/s':<15} {'1.9x'}\")\n    print(f\"{'creative':<20} {'45 tok/s':<15} {'55 tok/s':<15} {'1.2x'}\")\n    print(\"\")\n    print(\"üí° Key insight: Speculative decoding helps most when output is predictable!\")\n    print(\"   Start SGLang with EAGLE to measure actual speedup on your workload.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Understanding the Results\n",
    "\n",
    "**Why does speedup vary by prompt type?**\n",
    "\n",
    "1. **Predictable outputs** (counting, lists): Draft model easily predicts next tokens ‚Üí high acceptance rate ‚Üí big speedup\n",
    "\n",
    "2. **Semi-predictable** (code, explanations): Many patterns are predictable (function definitions, common phrases) ‚Üí moderate speedup\n",
    "\n",
    "3. **Creative outputs** (stories, poems): Hard to predict ‚Üí low acceptance rate ‚Üí less speedup (but never slower!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: When to Use Speculative Decoding\n",
    "\n",
    "Let's create a decision framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision framework for speculative decoding\n",
    "\n",
    "use_cases = {\n",
    "    \"strongly_recommended\": {\n",
    "        \"title\": \"Strongly Recommended\",\n",
    "        \"examples\": [\n",
    "            \"Code completion (predictable syntax)\",\n",
    "            \"Structured data generation (JSON, YAML)\",\n",
    "            \"Translation (predictable patterns)\",\n",
    "            \"Summarization (extractive style)\",\n",
    "            \"Question answering (factual)\",\n",
    "        ],\n",
    "        \"expected_speedup\": \"2-3x\"\n",
    "    },\n",
    "    \"recommended\": {\n",
    "        \"title\": \"Recommended\",\n",
    "        \"examples\": [\n",
    "            \"Technical explanations\",\n",
    "            \"Instructions/how-to guides\",\n",
    "            \"Email drafting\",\n",
    "            \"Documentation writing\",\n",
    "        ],\n",
    "        \"expected_speedup\": \"1.5-2x\"\n",
    "    },\n",
    "    \"worth_testing\": {\n",
    "        \"title\": \"Worth Testing\",\n",
    "        \"examples\": [\n",
    "            \"General chatbot\",\n",
    "            \"Semi-creative writing\",\n",
    "            \"Paraphrasing\",\n",
    "        ],\n",
    "        \"expected_speedup\": \"1.2-1.5x\"\n",
    "    },\n",
    "    \"unlikely_to_help\": {\n",
    "        \"title\": \"Unlikely to Help Much\",\n",
    "        \"examples\": [\n",
    "            \"Highly creative fiction\",\n",
    "            \"Poetry with unusual structure\",\n",
    "            \"Brainstorming novel ideas\",\n",
    "            \"Very short responses (< 20 tokens)\",\n",
    "        ],\n",
    "        \"expected_speedup\": \"1.0-1.2x\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üéØ When to Use Speculative Decoding\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for category, info in use_cases.items():\n",
    "    print(f\"\\nüìå {info['title']} (Expected: {info['expected_speedup']})\")\n",
    "    for example in info['examples']:\n",
    "        print(f\"   ‚Ä¢ {example}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trade-offs to consider\n",
    "\n",
    "tradeoffs = {\n",
    "    \"pros\": [\n",
    "        \"Up to 3x speedup for predictable outputs\",\n",
    "        \"Mathematically identical output (same quality)\",\n",
    "        \"Never slower than baseline (worst case: ~1x)\",\n",
    "        \"No retraining of target model required\",\n",
    "        \"Works with any autoregressive model\",\n",
    "    ],\n",
    "    \"cons\": [\n",
    "        \"Requires additional GPU memory for draft model\",\n",
    "        \"More complex deployment setup\",\n",
    "        \"EAGLE models needed (not available for all architectures)\",\n",
    "        \"Less benefit for creative/unpredictable tasks\",\n",
    "        \"May increase TTFT slightly (draft overhead)\",\n",
    "    ],\n",
    "    \"requirements\": [\n",
    "        \"Sufficient GPU memory for both models\",\n",
    "        \"Compatible draft model (EAGLE or smaller version)\",\n",
    "        \"SGLang, vLLM, or TensorRT-LLM with speculation support\",\n",
    "        \"Output tokens > 20 for meaningful speedup\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"‚öñÔ∏è Trade-offs of Speculative Decoding\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n‚úÖ PROS:\")\n",
    "for pro in tradeoffs[\"pros\"]:\n",
    "    print(f\"   + {pro}\")\n",
    "\n",
    "print(\"\\n‚ùå CONS:\")\n",
    "for con in tradeoffs[\"cons\"]:\n",
    "    print(f\"   - {con}\")\n",
    "\n",
    "print(\"\\nüìã REQUIREMENTS:\")\n",
    "for req in tradeoffs[\"requirements\"]:\n",
    "    print(f\"   ‚Ä¢ {req}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Using Wrong Draft Model\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong - Mismatched architectures\n",
    "--model meta-llama/Llama-3.1-8B \\\n",
    "--speculative-draft-model yuhuili/EAGLE-Mistral-7B  # Wrong architecture!\n",
    "\n",
    "# ‚úÖ Right - Matching architectures\n",
    "--model meta-llama/Llama-3.1-8B \\\n",
    "--speculative-draft-model yuhuili/EAGLE-LLaMA3-Instruct-8B\n",
    "```\n",
    "\n",
    "**Why:** EAGLE models are trained specifically for target model architectures.\n",
    "\n",
    "### Mistake 2: Too Many Draft Tokens\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong - Too aggressive, low acceptance rate\n",
    "--speculative-num-draft-tokens 16\n",
    "\n",
    "# ‚úÖ Right - Balanced for good acceptance\n",
    "--speculative-num-draft-tokens 5  # Default, usually optimal\n",
    "```\n",
    "\n",
    "**Why:** More tokens = lower probability all are accepted. Diminishing returns after ~5-8.\n",
    "\n",
    "### Mistake 3: Expecting Speedup on Short Outputs\n",
    "\n",
    "```python\n",
    "# ‚ùå Won't see much speedup\n",
    "response = generate(\"What is 2+2?\", max_tokens=5)  # Too short!\n",
    "\n",
    "# ‚úÖ Better for measuring speedup\n",
    "response = generate(\"Explain calculus in detail.\", max_tokens=200)\n",
    "```\n",
    "\n",
    "**Why:** Speculation overhead outweighs benefit for very short outputs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Compare Speculation Lengths\n",
    "\n",
    "Test different `--speculative-num-draft-tokens` values and measure acceptance rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# TODO: Run SGLang with speculation lengths 3, 5, 8, 12\n",
    "# TODO: Measure tokens/second for each\n",
    "# TODO: Find the optimal value for your workload\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Acceptance Rate Analysis\n",
    "\n",
    "Create prompts that demonstrate high vs low acceptance rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Create test prompts\n",
    "high_acceptance_prompts = [\n",
    "    # TODO: Add 3 prompts likely to have high acceptance\n",
    "    # Hint: Structured, predictable outputs\n",
    "]\n",
    "\n",
    "low_acceptance_prompts = [\n",
    "    # TODO: Add 3 prompts likely to have low acceptance\n",
    "    # Hint: Creative, unpredictable outputs\n",
    "]\n",
    "\n",
    "# TODO: Test both and compare speedups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How speculative decoding works (draft + verify)\n",
    "- ‚úÖ How to set up SGLang with EAGLE speculative decoding\n",
    "- ‚úÖ When speculation helps (predictable) vs doesn't help (creative)\n",
    "- ‚úÖ Trade-offs and decision framework for deployment\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Build an Adaptive Speculation System**\n",
    "\n",
    "Create a system that:\n",
    "1. Analyzes the prompt type (code, chat, creative)\n",
    "2. Adjusts speculation length based on observed acceptance rate\n",
    "3. Disables speculation entirely for prompts unlikely to benefit\n",
    "4. Logs and visualizes acceptance rates over time\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty](https://arxiv.org/abs/2401.15077)\n",
    "- [EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees](https://arxiv.org/abs/2406.16858)\n",
    "- [SGLang: Efficient Execution of Structured Language Model Programs](https://arxiv.org/abs/2312.07104)\n",
    "- [Medusa: Multiple Decode Heads for Parallel Decoding](https://sites.google.com/view/medusa-llm)\n",
    "- [vLLM Speculative Decoding Guide](https://docs.vllm.ai/en/latest/models/spec_decode.html)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cleanup\nimport gc\nimport subprocess\n\n# Collect garbage\ngc.collect()\n\n# Check GPU memory usage\ndef get_gpu_memory():\n    \"\"\"Get current GPU memory usage.\"\"\"\n    try:\n        result = subprocess.run(\n            [\"nvidia-smi\", \"--query-gpu=memory.used,memory.total\", \"--format=csv,noheader,nounits\"],\n            capture_output=True, text=True\n        )\n        if result.returncode == 0:\n            used, total = map(int, result.stdout.strip().split(','))\n            return used / 1024, total / 1024  # GB\n    except:\n        pass\n    return None, None\n\nused, total = get_gpu_memory()\nif used and total:\n    print(f\"üìä GPU Memory: {used:.1f}GB / {total:.1f}GB ({used/total*100:.0f}% used)\")\n\nprint(\"\\n‚úÖ Cleanup complete!\")\nprint(\"\\nüí° To stop SGLang server:\")\nprint(\"   pkill -f sglang\")\nprint(\"\\n   Or find and kill the process:\")\nprint(\"   ps aux | grep sglang\")\nprint(\"   kill <pid>\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}