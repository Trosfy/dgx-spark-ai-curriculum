{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.3.3: vLLM Continuous Batching & PagedAttention\n",
    "\n",
    "**Module:** 3.3 - Model Deployment & Inference Engines  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand how PagedAttention eliminates KV cache memory fragmentation\n",
    "- [ ] Configure vLLM's continuous batching for maximum throughput\n",
    "- [ ] Measure throughput under varying concurrent loads\n",
    "- [ ] Tune vLLM parameters for DGX Spark's 128GB unified memory\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Lab 3.3.1 (Engine Benchmark)\n",
    "- Knowledge of: REST APIs, concurrency concepts\n",
    "- Having: Docker with GPU support, HF token for gated models\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The Problem:** When serving LLMs to many users, you face a dilemma:\n",
    "\n",
    "- **Sequential processing:** One request at a time ‚Üí Low GPU utilization, bad for business\n",
    "- **Static batching:** Wait for N requests ‚Üí High latency for early arrivals\n",
    "- **Memory waste:** Each request reserves max possible KV cache ‚Üí Fewer concurrent users\n",
    "\n",
    "**vLLM's Solution:**\n",
    "1. **Continuous Batching:** Dynamically add/remove requests from the batch\n",
    "2. **PagedAttention:** Manage KV cache like OS virtual memory pages\n",
    "\n",
    "**Real Impact:**\n",
    "- 2-4x higher throughput than HuggingFace Transformers\n",
    "- Near-100% GPU utilization under load\n",
    "- Serve 3-4x more users with the same hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What is Continuous Batching?\n",
    "\n",
    "> **Imagine you're running a ferry service...**\n",
    ">\n",
    "> **OLD WAY (Static Batching):**\n",
    "> - Ferry waits until it has 10 passengers\n",
    "> - First passenger might wait 30 minutes for others\n",
    "> - If only 3 people show up, ferry is mostly empty\n",
    ">\n",
    "> **NEW WAY (Continuous Batching):**\n",
    "> - Ferry keeps moving continuously\n",
    "> - Passengers hop on and off at each dock\n",
    "> - Person A gets off at dock 3, Person B gets on at dock 3\n",
    "> - Ferry is always full, everyone arrives faster!\n",
    ">\n",
    "> **In AI terms:** Instead of waiting for a batch of requests, vLLM processes tokens continuously. When one request finishes, a new one immediately takes its place in the batch.\n",
    "\n",
    "---\n",
    "\n",
    "## üßí ELI5: What is PagedAttention?\n",
    "\n",
    "> **Imagine a parking lot for thoughts (KV cache)...**\n",
    ">\n",
    "> **OLD WAY:**\n",
    "> - Each car (request) reserves a HUGE space, just in case it grows\n",
    "> - A Smart car reserves 10 spaces \"in case I become a bus\"\n",
    "> - Parking lot fills up with mostly empty reserved spaces\n",
    ">\n",
    "> **PagedAttention WAY:**\n",
    "> - Parking lot is divided into small, equal-sized pages\n",
    "> - Each car gets exactly the spaces it needs right now\n",
    "> - As a car grows (longer response), it gets more pages\n",
    "> - When a car leaves, its pages are immediately reused\n",
    ">\n",
    "> **In AI terms:** Instead of pre-allocating max KV cache per request, PagedAttention allocates memory in small blocks (pages) on demand. This eliminates fragmentation and allows ~3x more concurrent requests.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting Up vLLM on DGX Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "# Add scripts directory to path\n",
    "scripts_path = Path(\"../scripts\").resolve()\n",
    "sys.path.insert(0, str(scripts_path))\n",
    "\n",
    "try:\n",
    "    from benchmark_utils import InferenceBenchmark, BenchmarkResult, BatchBenchmarkResult\n",
    "    from monitoring import GPUMonitor\n",
    "    print(\"‚úÖ Custom utilities loaded\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load custom utilities: {e}\")\n",
    "\n",
    "print(f\"üìÅ Scripts path: {scripts_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU status\n",
    "def get_gpu_info():\n",
    "    \"\"\"Get GPU information for capacity planning.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"nvidia-smi\", \"--query-gpu=name,memory.total,memory.free\",\n",
    "             \"--format=csv,noheader,nounits\"],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            values = result.stdout.strip().split(\",\")\n",
    "            return {\n",
    "                \"name\": values[0].strip(),\n",
    "                \"total_gb\": int(values[1]) / 1024,\n",
    "                \"free_gb\": int(values[2]) / 1024\n",
    "            }\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "gpu_info = get_gpu_info()\n",
    "if gpu_info:\n",
    "    print(f\"üñ•Ô∏è GPU: {gpu_info['name']}\")\n",
    "    print(f\"   Total: {gpu_info['total_gb']:.1f}GB\")\n",
    "    print(f\"   Free: {gpu_info['free_gb']:.1f}GB\")\n",
    "    \n",
    "    # Estimate max model size\n",
    "    usable = gpu_info['free_gb'] * 0.85  # Leave 15% for overhead\n",
    "    max_model_fp16 = usable / 2  # 2 bytes per parameter for FP16\n",
    "    print(f\"\\nüìä Estimated capacity:\")\n",
    "    print(f\"   Max FP16 model: ~{max_model_fp16:.0f}B parameters\")\n",
    "    print(f\"   Max BF16 model: ~{max_model_fp16:.0f}B parameters\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Could not get GPU info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Starting vLLM on DGX Spark\n",
    "\n",
    "vLLM requires `--enforce-eager` on ARM64 (DGX Spark) to disable CUDA graphs:\n",
    "\n",
    "```bash\n",
    "# Option 1: Using NGC PyTorch container (recommended)\n",
    "docker run --gpus all -p 8000:8000 \\\n",
    "    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n",
    "    -e HF_TOKEN=$HF_TOKEN \\\n",
    "    --ipc=host \\\n",
    "    nvcr.io/nvidia/pytorch:25.11-py3 \\\n",
    "    bash -c \"pip install vllm && python -m vllm.entrypoints.openai.api_server \\\n",
    "        --model meta-llama/Llama-3.1-8B-Instruct \\\n",
    "        --enforce-eager \\\n",
    "        --dtype bfloat16 \\\n",
    "        --max-model-len 8192 \\\n",
    "        --gpu-memory-utilization 0.85\"\n",
    "```\n",
    "\n",
    "**Key vLLM configuration flags:**\n",
    "\n",
    "| Flag | Description | DGX Spark Value |\n",
    "|------|-------------|----------------|\n",
    "| `--enforce-eager` | Disable CUDA graphs | **Required** on ARM64 |\n",
    "| `--dtype` | Model precision | `bfloat16` (native support) |\n",
    "| `--max-model-len` | Max sequence length | 8192 (adjust based on needs) |\n",
    "| `--gpu-memory-utilization` | GPU memory to use | 0.85 (leave headroom) |\n",
    "| `--max-num-seqs` | Max concurrent sequences | Default: 256 |\n",
    "| `--block-size` | PagedAttention block size | 16 (default) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check vLLM server status\n",
    "VLLM_URL = \"http://localhost:8000\"\n",
    "\n",
    "def check_vllm_server(url: str = VLLM_URL) -> Dict[str, Any]:\n",
    "    \"\"\"Check vLLM server status and get model info.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{url}/v1/models\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            models = data.get(\"data\", [])\n",
    "            print(f\"‚úÖ vLLM server running at {url}\")\n",
    "            for model in models:\n",
    "                print(f\"   Model: {model.get('id', 'unknown')}\")\n",
    "            return {\"available\": True, \"models\": models}\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"‚ùå vLLM server not running at {url}\")\n",
    "        print(\"\\nüìù Start with the command in the cell above\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    return {\"available\": False, \"models\": []}\n",
    "\n",
    "vllm_status = check_vllm_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding Continuous Batching\n",
    "\n",
    "Let's visualize how continuous batching works compared to static batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize batching strategies\n",
    "print(\"\"\"\n",
    "üìä STATIC BATCHING vs CONTINUOUS BATCHING\n",
    "=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"\n",
    "\n",
    "STATIC BATCHING (Traditional):\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Time:    0   1   2   3   4   5   6   7   8   9   10\n",
    "Request1 [=====]                                    (5 tokens)\n",
    "Request2 [=================]                        (10 tokens)\n",
    "Request3 [===]                                      (3 tokens) WASTED\n",
    "                                                    capacity!\n",
    "         ‚Üë Batch starts    ‚Üë Batch ends (slowest)\n",
    "         \n",
    "Problem: Request1 and Request3 finish early but wait for Request2\n",
    "         GPU sits idle for fast requests\n",
    "\n",
    "CONTINUOUS BATCHING (vLLM):\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Time:    0   1   2   3   4   5   6   7   8   9   10\n",
    "Request1 [=====]                                    (done at 5)\n",
    "Request2 [=================]                        (done at 10)\n",
    "Request3 [===]                                      (done at 3)\n",
    "Request4       [=========]                          (starts at 3, done at 8)\n",
    "Request5             [=======]                      (starts at 5, done at 9)\n",
    "\n",
    "Benefit: As soon as Request3 finishes, Request4 starts\n",
    "         GPU always working at full capacity!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate requests with varying lengths\n",
    "def simulate_request_lengths(num_requests: int = 20) -> List[Dict]:\n",
    "    \"\"\"Generate simulated requests with varying output lengths.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    requests_sim = []\n",
    "    for i in range(num_requests):\n",
    "        # Simulate varying response lengths (some short, some long)\n",
    "        output_tokens = int(np.random.lognormal(4, 1))  # Log-normal distribution\n",
    "        output_tokens = max(10, min(500, output_tokens))  # Clamp to 10-500\n",
    "        \n",
    "        requests_sim.append({\n",
    "            \"id\": i,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"arrival_time\": i * 0.5  # New request every 0.5 seconds\n",
    "        })\n",
    "    \n",
    "    return requests_sim\n",
    "\n",
    "simulated_requests = simulate_request_lengths()\n",
    "print(\"üìä Simulated request distribution:\")\n",
    "print(f\"   Total requests: {len(simulated_requests)}\")\n",
    "print(f\"   Min tokens: {min(r['output_tokens'] for r in simulated_requests)}\")\n",
    "print(f\"   Max tokens: {max(r['output_tokens'] for r in simulated_requests)}\")\n",
    "print(f\"   Median tokens: {np.median([r['output_tokens'] for r in simulated_requests]):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Benchmarking Throughput Under Load\n",
    "\n",
    "Let's measure how vLLM handles increasing concurrent request loads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test prompts\n",
    "TEST_PROMPTS = [\n",
    "    \"What is machine learning? Explain briefly.\",\n",
    "    \"Write a short poem about the ocean.\",\n",
    "    \"List 5 tips for learning a new programming language.\",\n",
    "    \"Explain the water cycle in simple terms.\",\n",
    "    \"What are the benefits of exercise?\",\n",
    "    \"Describe the solar system.\",\n",
    "    \"How does the internet work?\",\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"Explain photosynthesis.\",\n",
    "    \"What causes weather changes?\",\n",
    "    \"Describe the process of making bread.\",\n",
    "    \"How do airplanes fly?\",\n",
    "] * 4  # 48 prompts total\n",
    "\n",
    "print(f\"üìù Prepared {len(TEST_PROMPTS)} test prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_request_sync(url: str, prompt: str, max_tokens: int = 100) -> Dict:\n",
    "    \"\"\"Send a single request and return timing info.\"\"\"\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{url}/v1/chat/completions\",\n",
    "            json={\n",
    "                \"model\": \"default\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": 0.7\n",
    "            },\n",
    "            timeout=120\n",
    "        )\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            usage = data.get(\"usage\", {})\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"latency\": end_time - start_time,\n",
    "                \"tokens\": usage.get(\"completion_tokens\", 0),\n",
    "                \"prompt_tokens\": usage.get(\"prompt_tokens\", 0)\n",
    "            }\n",
    "        else:\n",
    "            return {\"success\": False, \"latency\": end_time - start_time, \"error\": response.status_code}\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"latency\": time.perf_counter() - start_time, \"error\": str(e)}\n",
    "\n",
    "\n",
    "def benchmark_throughput(\n",
    "    url: str,\n",
    "    prompts: List[str],\n",
    "    concurrency: int,\n",
    "    max_tokens: int = 100\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Benchmark throughput at a given concurrency level.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with throughput metrics\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=concurrency) as executor:\n",
    "        futures = [\n",
    "            executor.submit(send_request_sync, url, prompt, max_tokens)\n",
    "            for prompt in prompts\n",
    "        ]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            results.append(future.result())\n",
    "    \n",
    "    total_time = time.perf_counter() - start_time\n",
    "    \n",
    "    # Compute metrics\n",
    "    successful = [r for r in results if r[\"success\"]]\n",
    "    failed = len(results) - len(successful)\n",
    "    \n",
    "    if successful:\n",
    "        latencies = [r[\"latency\"] for r in successful]\n",
    "        total_tokens = sum(r[\"tokens\"] for r in successful)\n",
    "        \n",
    "        return {\n",
    "            \"concurrency\": concurrency,\n",
    "            \"total_requests\": len(prompts),\n",
    "            \"successful\": len(successful),\n",
    "            \"failed\": failed,\n",
    "            \"total_time\": total_time,\n",
    "            \"throughput_rps\": len(successful) / total_time,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"tokens_per_second\": total_tokens / total_time,\n",
    "            \"avg_latency\": np.mean(latencies),\n",
    "            \"p50_latency\": np.percentile(latencies, 50),\n",
    "            \"p90_latency\": np.percentile(latencies, 90),\n",
    "            \"p99_latency\": np.percentile(latencies, 99),\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"concurrency\": concurrency,\n",
    "            \"total_requests\": len(prompts),\n",
    "            \"successful\": 0,\n",
    "            \"failed\": failed,\n",
    "            \"error\": \"All requests failed\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run throughput benchmarks at various concurrency levels\n",
    "CONCURRENCY_LEVELS = [1, 2, 4, 8, 16, 32]\n",
    "NUM_REQUESTS_PER_LEVEL = 24  # Adjust based on patience\n",
    "\n",
    "if vllm_status[\"available\"]:\n",
    "    print(\"üöÄ Running throughput benchmark...\")\n",
    "    print(f\"   Concurrency levels: {CONCURRENCY_LEVELS}\")\n",
    "    print(f\"   Requests per level: {NUM_REQUESTS_PER_LEVEL}\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    throughput_results = []\n",
    "    \n",
    "    for concurrency in CONCURRENCY_LEVELS:\n",
    "        print(f\"\\nTesting concurrency = {concurrency}...\")\n",
    "        \n",
    "        result = benchmark_throughput(\n",
    "            url=VLLM_URL,\n",
    "            prompts=TEST_PROMPTS[:NUM_REQUESTS_PER_LEVEL],\n",
    "            concurrency=concurrency,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        \n",
    "        throughput_results.append(result)\n",
    "        \n",
    "        if \"error\" not in result:\n",
    "            print(f\"   ‚úÖ Throughput: {result['throughput_rps']:.2f} req/s\")\n",
    "            print(f\"      Token throughput: {result['tokens_per_second']:.0f} tok/s\")\n",
    "            print(f\"      Avg latency: {result['avg_latency']*1000:.0f}ms\")\n",
    "            print(f\"      P90 latency: {result['p90_latency']*1000:.0f}ms\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Failed: {result.get('error')}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è vLLM not available. Start the server to run this benchmark.\")\n",
    "    throughput_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize throughput results\n",
    "if throughput_results:\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        successful_results = [r for r in throughput_results if \"error\" not in r]\n",
    "        \n",
    "        if successful_results:\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "            \n",
    "            # Plot 1: Throughput vs Concurrency\n",
    "            concs = [r[\"concurrency\"] for r in successful_results]\n",
    "            throughputs = [r[\"throughput_rps\"] for r in successful_results]\n",
    "            axes[0].plot(concs, throughputs, 'bo-', linewidth=2, markersize=8)\n",
    "            axes[0].set_xlabel(\"Concurrency\")\n",
    "            axes[0].set_ylabel(\"Throughput (req/s)\")\n",
    "            axes[0].set_title(\"Throughput vs Concurrency\")\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot 2: Token throughput\n",
    "            tok_throughputs = [r[\"tokens_per_second\"] for r in successful_results]\n",
    "            axes[1].plot(concs, tok_throughputs, 'go-', linewidth=2, markersize=8)\n",
    "            axes[1].set_xlabel(\"Concurrency\")\n",
    "            axes[1].set_ylabel(\"Token Throughput (tok/s)\")\n",
    "            axes[1].set_title(\"Token Throughput vs Concurrency\")\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot 3: Latency\n",
    "            p50s = [r[\"p50_latency\"]*1000 for r in successful_results]\n",
    "            p90s = [r[\"p90_latency\"]*1000 for r in successful_results]\n",
    "            axes[2].plot(concs, p50s, 'b^-', label='P50', linewidth=2, markersize=8)\n",
    "            axes[2].plot(concs, p90s, 'rs-', label='P90', linewidth=2, markersize=8)\n",
    "            axes[2].set_xlabel(\"Concurrency\")\n",
    "            axes[2].set_ylabel(\"Latency (ms)\")\n",
    "            axes[2].set_title(\"Latency vs Concurrency\")\n",
    "            axes[2].legend()\n",
    "            axes[2].grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\"vllm_throughput.png\", dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"\\nüìà Charts saved to vllm_throughput.png\")\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è matplotlib not available. Install with: pip install matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "We measured how vLLM handles increasing load:\n",
    "\n",
    "1. **Throughput increases with concurrency** (up to a point)\n",
    "   - More concurrent requests = better GPU utilization\n",
    "   - Continuous batching keeps the GPU busy\n",
    "\n",
    "2. **Latency increases with concurrency**\n",
    "   - Individual requests take longer when batched\n",
    "   - But total throughput is higher (good tradeoff for high-load scenarios)\n",
    "\n",
    "3. **Saturation point**\n",
    "   - At some concurrency level, throughput stops increasing\n",
    "   - GPU is fully utilized, adding more requests just adds queue time\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: PagedAttention Memory Efficiency\n",
    "\n",
    "Let's understand how PagedAttention improves memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate memory savings from PagedAttention\n",
    "def calculate_kv_cache_memory(\n",
    "    model_hidden_size: int = 4096,  # Llama 3.1 8B\n",
    "    num_layers: int = 32,\n",
    "    num_kv_heads: int = 8,\n",
    "    head_dim: int = 128,\n",
    "    max_seq_len: int = 8192,\n",
    "    num_sequences: int = 32,\n",
    "    dtype_bytes: int = 2  # bfloat16\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate KV cache memory requirements.\n",
    "    \n",
    "    KV cache size = 2 (K+V) * num_layers * num_kv_heads * head_dim * seq_len * dtype_bytes\n",
    "    \"\"\"\n",
    "    # Per-token KV cache size\n",
    "    per_token_kv = 2 * num_layers * num_kv_heads * head_dim * dtype_bytes\n",
    "    \n",
    "    # Traditional: Reserve max_seq_len for each sequence\n",
    "    traditional_per_seq = per_token_kv * max_seq_len\n",
    "    traditional_total = traditional_per_seq * num_sequences\n",
    "    \n",
    "    # PagedAttention: Allocate as needed (assume average 50% utilization)\n",
    "    avg_utilization = 0.5\n",
    "    paged_per_seq = per_token_kv * max_seq_len * avg_utilization\n",
    "    paged_total = paged_per_seq * num_sequences\n",
    "    \n",
    "    # Also account for fragmentation in traditional (typically 30-50%)\n",
    "    fragmentation = 0.35\n",
    "    traditional_with_frag = traditional_total / (1 - fragmentation)\n",
    "    \n",
    "    return {\n",
    "        \"per_token_bytes\": per_token_kv,\n",
    "        \"traditional_total_gb\": traditional_total / (1024**3),\n",
    "        \"traditional_with_frag_gb\": traditional_with_frag / (1024**3),\n",
    "        \"paged_total_gb\": paged_total / (1024**3),\n",
    "        \"memory_savings\": 1 - (paged_total / traditional_with_frag),\n",
    "        \"max_sequences_traditional\": int(128 * (1024**3) / traditional_with_frag * num_sequences),\n",
    "        \"max_sequences_paged\": int(128 * (1024**3) / paged_total * num_sequences),\n",
    "    }\n",
    "\n",
    "kv_analysis = calculate_kv_cache_memory()\n",
    "\n",
    "print(\"üìä KV Cache Memory Analysis (Llama 3.1 8B, 32 concurrent sequences)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPer-token KV cache: {kv_analysis['per_token_bytes']} bytes\")\n",
    "print(f\"\\nTraditional allocation:\")\n",
    "print(f\"   Total reserved: {kv_analysis['traditional_total_gb']:.1f} GB\")\n",
    "print(f\"   With fragmentation: {kv_analysis['traditional_with_frag_gb']:.1f} GB\")\n",
    "print(f\"\\nPagedAttention:\")\n",
    "print(f\"   Total used: {kv_analysis['paged_total_gb']:.1f} GB\")\n",
    "print(f\"\\nüöÄ Memory savings: {kv_analysis['memory_savings']:.0%}\")\n",
    "print(f\"\\nMax concurrent sequences on 128GB:\")\n",
    "print(f\"   Traditional: ~{kv_analysis['max_sequences_traditional']}\")\n",
    "print(f\"   PagedAttention: ~{kv_analysis['max_sequences_paged']}\")\n",
    "print(f\"\\n   That's {kv_analysis['max_sequences_paged'] / kv_analysis['max_sequences_traditional']:.1f}x more users!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting `--enforce-eager` on ARM64\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong - CUDA graphs fail on ARM64\n",
    "python -m vllm.entrypoints.openai.api_server --model llama3.1\n",
    "\n",
    "# ‚úÖ Right - Disable CUDA graphs for DGX Spark\n",
    "python -m vllm.entrypoints.openai.api_server --model llama3.1 --enforce-eager\n",
    "```\n",
    "\n",
    "### Mistake 2: Setting `--max-model-len` Too High\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong - Uses too much memory for KV cache\n",
    "--max-model-len 131072  # 128K context\n",
    "\n",
    "# ‚úÖ Right - Set based on your actual needs\n",
    "--max-model-len 8192   # Most conversations don't need 128K\n",
    "```\n",
    "\n",
    "**Why:** Higher max_model_len reserves more KV cache memory, reducing concurrent capacity.\n",
    "\n",
    "### Mistake 3: Not Using `--ipc=host` in Docker\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong - Shared memory issues with DataLoader\n",
    "docker run --gpus all ...\n",
    "\n",
    "# ‚úÖ Right - Enable host IPC for shared memory\n",
    "docker run --gpus all --ipc=host ...\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Find the Saturation Point\n",
    "\n",
    "Test higher concurrency levels to find where throughput plateaus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Test concurrency levels: [1, 4, 8, 16, 32, 64, 128]\n",
    "# Find where throughput stops increasing\n",
    "\n",
    "EXTENDED_CONCURRENCY = [1, 4, 8, 16, 32, 64, 128]\n",
    "\n",
    "# TODO: Run benchmarks at these levels\n",
    "# TODO: Plot throughput vs concurrency\n",
    "# TODO: Identify the saturation point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Tune for Your Workload\n",
    "\n",
    "Experiment with different `--max-model-len` values and measure impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Notes for different configurations\n",
    "# \n",
    "# Test these max-model-len values and note the max concurrency:\n",
    "# - 2048: Short conversations\n",
    "# - 4096: Medium conversations\n",
    "# - 8192: Long conversations\n",
    "# - 16384: Very long conversations\n",
    "#\n",
    "# For each, restart vLLM with the new value and measure:\n",
    "# 1. Maximum concurrent sequences before OOM\n",
    "# 2. Throughput at concurrency=16\n",
    "# 3. Memory usage (nvidia-smi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How continuous batching maximizes GPU utilization\n",
    "- ‚úÖ How PagedAttention eliminates memory fragmentation\n",
    "- ‚úÖ How to benchmark throughput under varying loads\n",
    "- ‚úÖ How to configure vLLM for DGX Spark\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Build an Adaptive Concurrency Controller**\n",
    "\n",
    "Create a system that:\n",
    "1. Monitors current P90 latency\n",
    "2. Automatically adjusts max concurrency to meet SLA\n",
    "3. Alerts when capacity is exceeded\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [vLLM Paper: Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)\n",
    "- [vLLM Documentation](https://docs.vllm.ai/)\n",
    "- [Continuous Batching Explained (Anyscale Blog)](https://www.anyscale.com/blog/continuous-batching-llm-inference)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cleanup\nimport gc\n\n# Clear Python garbage\ngc.collect()\n\n# Clear GPU memory cache if torch is available\ntry:\n    import torch\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n        print(\"‚úÖ GPU memory cache cleared!\")\nexcept ImportError:\n    pass\n\nprint(\"‚úÖ Cleanup complete!\")\nprint(\"\\nüìù To stop vLLM server:\")\nprint(\"   docker stop $(docker ps -q --filter ancestor=nvcr.io/nvidia/pytorch:25.11-py3)\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}