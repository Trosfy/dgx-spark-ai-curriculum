{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.3.6: TensorRT-LLM Optimization\n",
    "\n",
    "**Module:** 3.3 - Model Deployment & Inference Engines  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand TensorRT-LLM's architecture and optimization pipeline\n",
    "- [ ] Build an optimized TRT engine for Llama models\n",
    "- [ ] Benchmark prefill and decode performance\n",
    "- [ ] Know when TensorRT-LLM is the right choice\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Labs 3.3.1-3.3.5\n",
    "- Knowledge of: Model quantization, CUDA basics\n",
    "- Having: NGC container access, 128GB+ free disk for engine builds\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**When Maximum Performance Matters:**\n",
    "\n",
    "TensorRT-LLM is NVIDIA's highest-performance inference engine, designed for:\n",
    "- Production deployments where every millisecond counts\n",
    "- High-throughput batch processing\n",
    "- Applications with long input prompts (RAG, document analysis)\n",
    "\n",
    "**Trade-offs:**\n",
    "- Best performance ‚Üí Requires engine compilation (45-90 minutes)\n",
    "- NVIDIA optimized ‚Üí Less portable than other solutions\n",
    "- Most complex ‚Üí Steeper learning curve\n",
    "\n",
    "**Real Impact:**\n",
    "- 2-5x faster prefill than PyTorch\n",
    "- Best FP8/NVFP4 support on Blackwell\n",
    "- Used by major cloud providers for LLM serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What is TensorRT-LLM?\n",
    "\n",
    "> **Imagine you have a recipe for a fancy cake...**\n",
    ">\n",
    "> **PyTorch/HuggingFace:** You follow the recipe step by step, measuring each ingredient as you go.\n",
    "> This is flexible - you can adjust on the fly - but not the fastest.\n",
    ">\n",
    "> **TensorRT-LLM:** Before baking, you spend an hour organizing:\n",
    "> - Pre-measure ALL ingredients\n",
    "> - Arrange tools in optimal order\n",
    "> - Figure out which steps can happen simultaneously\n",
    "> - Create a highly optimized \"production line\"\n",
    ">\n",
    "> Now when it's time to bake, everything flows perfectly - much faster!\n",
    ">\n",
    "> **The downside?** You spent an hour planning. And if you want a different cake,\n",
    "> you need a new plan. But for making 1000 identical cakes? Way faster!\n",
    ">\n",
    "> **In AI terms:** TensorRT-LLM pre-compiles the model into a highly optimized \"engine\"\n",
    "> that runs on NVIDIA GPUs with maximum efficiency. The compilation takes time,\n",
    "> but inference is blazing fast.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä TensorRT-LLM Optimization Pipeline\n",
    "\n",
    "```\n",
    "HuggingFace Model ‚Üí Convert ‚Üí Quantize ‚Üí Build Engine ‚Üí Deploy\n",
    "     (FP16)          (TRT)     (FP8)       (TRT-LLM)     (Triton)\n",
    "     \n",
    "Time:  ~10min        ~5min     ~15min      ~60min        ~2min\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding TensorRT-LLM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "# Add scripts directory to path\n",
    "scripts_path = Path(\"../scripts\").resolve()\n",
    "sys.path.insert(0, str(scripts_path))\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(f\"üìÅ Scripts path: {scripts_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize TensorRT-LLM components\n",
    "print(\"\"\"\n",
    "üìä TENSORRT-LLM ARCHITECTURE\n",
    "=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    TensorRT-LLM Stack                           ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ\n",
    "‚îÇ  ‚îÇ  Model API    ‚îÇ    ‚îÇ   Executor    ‚îÇ    ‚îÇ   Runtime     ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  (Python)     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ   (Batch Mgr) ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ   (C++/CUDA)  ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ\n",
    "‚îÇ         ‚îÇ                     ‚îÇ                    ‚îÇ           ‚îÇ\n",
    "‚îÇ         ‚îÇ                     ‚îÇ                    ‚îÇ           ‚îÇ\n",
    "‚îÇ         ‚ñº                     ‚ñº                    ‚ñº           ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
    "‚îÇ  ‚îÇ              TensorRT Engine (Compiled)               ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ  - Fused CUDA kernels                                 ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ  - Optimized memory layout                            ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ  - Hardware-specific tuning                           ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
    "‚îÇ                              ‚îÇ                                  ‚îÇ\n",
    "‚îÇ                              ‚ñº                                  ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
    "‚îÇ  ‚îÇ              NVIDIA GPU (Blackwell GB10)              ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ  - 6,144 CUDA cores                                   ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ  - 192 Tensor Cores (5th gen)                         ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ  - Native FP8/NVFP4 support                           ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key optimizations in TensorRT-LLM\n",
    "print(\"\"\"\n",
    "üìä KEY TENSORRT-LLM OPTIMIZATIONS\n",
    "=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"\n",
    "\n",
    "1. KERNEL FUSION\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "   Before: MatMul ‚Üí Add Bias ‚Üí Activation (3 kernel launches)\n",
    "   After:  FusedLinear (1 kernel launch)\n",
    "   \n",
    "   Benefit: ~30% less kernel launch overhead\n",
    "\n",
    "2. FLASH ATTENTION INTEGRATION\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "   - Fused multi-head attention kernel\n",
    "   - Memory-efficient O(N) instead of O(N¬≤)\n",
    "   - Specialized for GQA/MQA architectures\n",
    "   \n",
    "   Benefit: ~40% faster attention, ~60% less memory\n",
    "\n",
    "3. IN-FLIGHT BATCHING\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "   Like vLLM's continuous batching, but at the engine level:\n",
    "   - Dynamic batch management\n",
    "   - Request-level scheduling\n",
    "   \n",
    "   Benefit: Higher throughput under varied load\n",
    "\n",
    "4. QUANTIZATION (FP8/NVFP4)\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "   - Native FP8 on Blackwell (no emulation)\n",
    "   - NVFP4 exclusive to Blackwell architecture\n",
    "   - Per-channel or per-block scaling\n",
    "   \n",
    "   Benefit: 2-4x memory reduction, 1.5-2x faster compute\n",
    "\n",
    "5. PAGED KV CACHE\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "   - Similar to vLLM's PagedAttention\n",
    "   - Managed at the engine level\n",
    "   \n",
    "   Benefit: More concurrent sequences\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Building a TensorRT-LLM Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if TensorRT-LLM is available\n",
    "def check_trt_llm_installation():\n",
    "    \"\"\"Check if TensorRT-LLM is installed and accessible.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"python\", \"-c\", \"import tensorrt_llm; print(tensorrt_llm.__version__)\"],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            version = result.stdout.strip()\n",
    "            print(f\"‚úÖ TensorRT-LLM installed: v{version}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå TensorRT-LLM not installed\")\n",
    "            print(\"\\nüìù To install via NGC container:\")\n",
    "            print(\"   docker pull nvcr.io/nvidia/tritonserver:25.11-trtllm-python-py3\")\n",
    "            return False\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Python not found\")\n",
    "        return False\n",
    "\n",
    "trt_llm_available = check_trt_llm_installation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engine build configuration\n",
    "print(\"\"\"\n",
    "üìä TENSORRT-LLM ENGINE BUILD OPTIONS\n",
    "=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"\n",
    "\n",
    "Key Build Parameters:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "--dtype:\n",
    "  ‚Ä¢ float16  - Standard half precision\n",
    "  ‚Ä¢ bfloat16 - Recommended for Blackwell (native support)\n",
    "  ‚Ä¢ float8   - FP8 E4M3 for inference (2x memory reduction)\n",
    "  ‚Ä¢ fp4      - NVFP4 for Blackwell only (4x memory reduction)\n",
    "\n",
    "--max_input_len:\n",
    "  ‚Ä¢ Maximum input prompt length engine will accept\n",
    "  ‚Ä¢ Affects memory allocation\n",
    "  ‚Ä¢ Default: 2048, can set to 8192 for RAG\n",
    "\n",
    "--max_output_len:\n",
    "  ‚Ä¢ Maximum tokens to generate\n",
    "  ‚Ä¢ Affects KV cache allocation\n",
    "  ‚Ä¢ Set based on your use case\n",
    "\n",
    "--max_batch_size:\n",
    "  ‚Ä¢ Maximum concurrent sequences\n",
    "  ‚Ä¢ DGX Spark with 128GB can handle 32-64 for 8B model\n",
    "\n",
    "--use_fused_mlp:\n",
    "  ‚Ä¢ Fuse gate/up/down projections\n",
    "  ‚Ä¢ ~15% speedup, slightly more memory\n",
    "\n",
    "--enable_context_fmha:\n",
    "  ‚Ä¢ Flash attention for context (prefill)\n",
    "  ‚Ä¢ Essential for long input performance\n",
    "\n",
    "Example Build Command:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "python -m tensorrt_llm.commands.build \\\\\n",
    "    --model_dir ./llama-3.1-8b-hf \\\\\n",
    "    --output_dir ./llama-3.1-8b-trt \\\\\n",
    "    --dtype bfloat16 \\\\\n",
    "    --max_input_len 4096 \\\\\n",
    "    --max_output_len 2048 \\\\\n",
    "    --max_batch_size 32 \\\\\n",
    "    --use_fused_mlp \\\\\n",
    "    --enable_context_fmha\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate engine build time and size\n",
    "def estimate_build_resources(\n",
    "    model_size_b: float,\n",
    "    dtype: str = \"bfloat16\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Estimate build time and disk space for TensorRT-LLM engine.\n",
    "    \n",
    "    Args:\n",
    "        model_size_b: Model size in billions of parameters\n",
    "        dtype: Target data type\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with estimates\n",
    "    \"\"\"\n",
    "    # Bytes per parameter\n",
    "    dtype_bytes = {\n",
    "        \"float32\": 4,\n",
    "        \"float16\": 2,\n",
    "        \"bfloat16\": 2,\n",
    "        \"float8\": 1,\n",
    "        \"fp4\": 0.5\n",
    "    }\n",
    "    \n",
    "    bytes_per_param = dtype_bytes.get(dtype, 2)\n",
    "    \n",
    "    # Model file size\n",
    "    model_size_gb = (model_size_b * 1e9 * bytes_per_param) / (1024**3)\n",
    "    \n",
    "    # Engine is typically 1.1-1.3x model size\n",
    "    engine_size_gb = model_size_gb * 1.2\n",
    "    \n",
    "    # Temp space during build\n",
    "    temp_space_gb = model_size_gb * 3  # Need space for intermediate files\n",
    "    \n",
    "    # Build time estimate (roughly 5 min per billion params)\n",
    "    build_time_min = model_size_b * 5\n",
    "    \n",
    "    return {\n",
    "        \"model_size_gb\": round(model_size_gb, 1),\n",
    "        \"engine_size_gb\": round(engine_size_gb, 1),\n",
    "        \"temp_space_gb\": round(temp_space_gb, 1),\n",
    "        \"total_disk_needed_gb\": round(model_size_gb + engine_size_gb + temp_space_gb, 1),\n",
    "        \"build_time_min\": round(build_time_min),\n",
    "        \"build_time_formatted\": f\"{int(build_time_min // 60)}h {int(build_time_min % 60)}m\"\n",
    "    }\n",
    "\n",
    "# Estimate for common models\n",
    "print(\"üìä Engine Build Estimates\\n\")\n",
    "print(f\"{'Model':<20} {'Dtype':<10} {'Model Size':<12} {'Engine Size':<12} {'Build Time'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "models_to_estimate = [\n",
    "    (\"Llama 3.1 8B\", 8, \"bfloat16\"),\n",
    "    (\"Llama 3.1 8B\", 8, \"float8\"),\n",
    "    (\"Llama 3.1 70B\", 70, \"bfloat16\"),\n",
    "    (\"Llama 3.1 70B\", 70, \"float8\"),\n",
    "    (\"Llama 3.1 70B\", 70, \"fp4\"),\n",
    "]\n",
    "\n",
    "for model_name, size_b, dtype in models_to_estimate:\n",
    "    est = estimate_build_resources(size_b, dtype)\n",
    "    print(f\"{model_name:<20} {dtype:<10} {est['model_size_gb']:<10}GB {est['engine_size_gb']:<10}GB {est['build_time_formatted']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Building a TensorRT-LLM Engine\n",
    "\n",
    "Here's the complete workflow for building an engine on DGX Spark:\n",
    "\n",
    "```bash\n",
    "# Step 1: Start the TensorRT-LLM container\n",
    "docker run --gpus all -it --rm \\\n",
    "    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n",
    "    -v ~/trt-engines:/workspace/engines \\\n",
    "    -e HF_TOKEN=$HF_TOKEN \\\n",
    "    --ipc=host \\\n",
    "    nvcr.io/nvidia/tritonserver:25.11-trtllm-python-py3\n",
    "\n",
    "# Step 2: Convert HuggingFace model to TensorRT-LLM format\n",
    "python -m tensorrt_llm.commands.convert_checkpoint \\\n",
    "    --model_dir Qwen/Qwen3-8B-Instruct \\\n",
    "    --output_dir /workspace/engines/llama-3.1-8b-ckpt \\\n",
    "    --dtype bfloat16\n",
    "\n",
    "# Step 3: Build the TensorRT engine\n",
    "python -m tensorrt_llm.commands.build \\\n",
    "    --checkpoint_dir /workspace/engines/llama-3.1-8b-ckpt \\\n",
    "    --output_dir /workspace/engines/llama-3.1-8b-trt \\\n",
    "    --max_input_len 4096 \\\n",
    "    --max_output_len 2048 \\\n",
    "    --max_batch_size 32 \\\n",
    "    --use_fused_mlp \\\n",
    "    --enable_context_fmha\n",
    "\n",
    "# Step 4: Run the server\n",
    "python -m tensorrt_llm.commands.serve \\\n",
    "    --engine_dir /workspace/engines/llama-3.1-8b-trt \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Benchmarking TensorRT-LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if TensorRT-LLM server is running\n",
    "TRT_LLM_URL = \"http://localhost:8000\"\n",
    "\n",
    "def check_trt_server(url: str) -> bool:\n",
    "    \"\"\"Check if TensorRT-LLM server is running.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{url}/v1/models\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"‚úÖ TensorRT-LLM server running at {url}\")\n",
    "            models = response.json().get(\"data\", [])\n",
    "            for model in models:\n",
    "                print(f\"   Model: {model.get('id', 'unknown')}\")\n",
    "            return True\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"‚ùå No TensorRT-LLM server at {url}\")\n",
    "        print(\"\\nüìù See build instructions above to start a server\")\n",
    "    return False\n",
    "\n",
    "trt_server_available = check_trt_server(TRT_LLM_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark prompts for prefill testing\n",
    "PREFILL_TEST_PROMPTS = {\n",
    "    \"short\": \"What is 2+2?\",\n",
    "    \"medium\": \"Explain the concept of machine learning in 2-3 sentences. \" * 10,  # ~100 tokens\n",
    "    \"long\": \"\"\"Analyze the following text and provide insights:\n",
    "\n",
    "Machine learning (ML) is a branch of artificial intelligence (AI) and computer science that \n",
    "focuses on using data and algorithms to enable AI to imitate the way that humans learn, \n",
    "gradually improving its accuracy. Machine learning is an important component of the growing \n",
    "field of data science. Through the use of statistical methods, algorithms are trained to make \n",
    "classifications or predictions, and to uncover key insights in data mining projects.\n",
    "\n",
    "These insights subsequently drive decision making within applications and businesses, ideally \n",
    "impacting key growth metrics. As big data continues to expand and grow, the market demand for \n",
    "data scientists will increase. They will be required to help identify the most relevant business \n",
    "questions and the data to answer them.\n",
    "\n",
    "Machine learning algorithms are typically created using frameworks that accelerate solution \n",
    "development, such as TensorFlow and PyTorch. Machine learning models are improving in accuracy, \n",
    "thanks to the vast amounts of data now available and to the increasing power of computers. \n",
    "\"\"\" + \"Please summarize the key points. \" * 5,  # ~500 tokens\n",
    "}\n",
    "\n",
    "# Estimate token counts\n",
    "for name, prompt in PREFILL_TEST_PROMPTS.items():\n",
    "    token_estimate = len(prompt.split()) * 1.3  # Rough estimate\n",
    "    print(f\"{name}: ~{int(token_estimate)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_prefill(\n",
    "    url: str,\n",
    "    prompts: Dict[str, str],\n",
    "    max_tokens: int = 50,\n",
    "    num_runs: int = 3\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Benchmark prefill performance for different prompt lengths.\n",
    "    \n",
    "    Prefill (TTFT) is TensorRT-LLM's strength - measures time to process input.\n",
    "    \"\"\"\n",
    "    print(\"\\nüß™ Benchmarking prefill performance...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, prompt in prompts.items():\n",
    "        print(f\"\\n{name} prompt ({len(prompt.split())} words):\")\n",
    "        \n",
    "        ttfts = []\n",
    "        \n",
    "        for run in range(num_runs):\n",
    "            start_time = time.perf_counter()\n",
    "            first_token_time = None\n",
    "            \n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    f\"{url}/v1/chat/completions\",\n",
    "                    json={\n",
    "                        \"model\": \"default\",\n",
    "                        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                        \"max_tokens\": max_tokens,\n",
    "                        \"temperature\": 0.7,\n",
    "                        \"stream\": True\n",
    "                    },\n",
    "                    stream=True,\n",
    "                    timeout=120\n",
    "                )\n",
    "                \n",
    "                for line in response.iter_lines():\n",
    "                    if line and first_token_time is None:\n",
    "                        line_str = line.decode(\"utf-8\")\n",
    "                        if \"content\" in line_str:\n",
    "                            first_token_time = time.perf_counter()\n",
    "                            break\n",
    "                \n",
    "                if first_token_time:\n",
    "                    ttft = (first_token_time - start_time) * 1000  # ms\n",
    "                    ttfts.append(ttft)\n",
    "                    print(f\"   Run {run+1}: TTFT = {ttft:.1f}ms\")\n",
    "                else:\n",
    "                    print(f\"   Run {run+1}: No response received\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   Run {run+1}: Error - {e}\")\n",
    "        \n",
    "        if ttfts:\n",
    "            results[name] = {\n",
    "                \"avg_ttft_ms\": np.mean(ttfts),\n",
    "                \"min_ttft_ms\": np.min(ttfts),\n",
    "                \"max_ttft_ms\": np.max(ttfts),\n",
    "                \"word_count\": len(prompt.split())\n",
    "            }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prefill benchmark if server is available\n",
    "if trt_server_available:\n",
    "    prefill_results = benchmark_prefill(\n",
    "        url=TRT_LLM_URL,\n",
    "        prompts=PREFILL_TEST_PROMPTS,\n",
    "        max_tokens=50,\n",
    "        num_runs=3\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä PREFILL (TTFT) BENCHMARK RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n{'Prompt':<10} {'Words':<8} {'Avg TTFT':<12} {'Min':<10} {'Max'}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for name, stats in prefill_results.items():\n",
    "        print(f\"{name:<10} {stats['word_count']:<8} \"\n",
    "              f\"{stats['avg_ttft_ms']:.1f}ms{'':>4} \"\n",
    "              f\"{stats['min_ttft_ms']:.1f}ms{'':>3} \"\n",
    "              f\"{stats['max_ttft_ms']:.1f}ms\")\n",
    "else:\n",
    "    print(\"\\nüìä Expected TensorRT-LLM prefill performance (Llama 3.1 8B, BF16):\")\n",
    "    print(\"   Short (10 tokens): ~25ms TTFT\")\n",
    "    print(\"   Medium (100 tokens): ~50ms TTFT\")\n",
    "    print(\"   Long (500 tokens): ~120ms TTFT\")\n",
    "    print(\"\\n   Compare to vLLM:\")\n",
    "    print(\"   Short: ~35ms, Medium: ~80ms, Long: ~250ms\")\n",
    "    print(\"\\n   TensorRT-LLM prefill is 1.5-2x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: When to Choose TensorRT-LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision matrix\n",
    "print(\"\"\"\n",
    "üìä INFERENCE ENGINE DECISION MATRIX\n",
    "=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"\n",
    "\n",
    "| Use Case                      | Best Choice   | Why                         |\n",
    "|-------------------------------|---------------|-----------------------------|\n",
    "| Local development             | Ollama        | Easy setup, model mgmt      |\n",
    "| Quick prototyping             | vLLM          | Fast to start, good perf    |\n",
    "| Interactive chat (latency)    | SGLang+Medusa | Lowest decode latency       |\n",
    "| Long inputs (RAG, docs)       | TensorRT-LLM  | Best prefill performance    |\n",
    "| Max throughput (batch)        | TensorRT-LLM  | Best GPU utilization        |\n",
    "| FP8/NVFP4 quantization        | TensorRT-LLM  | Native support on Blackwell |\n",
    "| Production (NVIDIA GPUs)      | TensorRT-LLM  | Fully optimized             |\n",
    "| Edge deployment               | llama.cpp     | Smallest footprint          |\n",
    "\n",
    "TensorRT-LLM Trade-offs:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "‚úÖ PROS:\n",
    "  ‚Ä¢ Best raw performance on NVIDIA hardware\n",
    "  ‚Ä¢ Best prefill speed (crucial for long prompts)\n",
    "  ‚Ä¢ Native FP8/NVFP4 support\n",
    "  ‚Ä¢ Tight integration with Triton Inference Server\n",
    "\n",
    "‚ùå CONS:\n",
    "  ‚Ä¢ Long engine build time (45-90 minutes)\n",
    "  ‚Ä¢ Engine tied to specific hardware\n",
    "  ‚Ä¢ More complex setup\n",
    "  ‚Ä¢ Less flexible for experimentation\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Building Engine on Wrong Hardware\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong - Building on different GPU than deployment target\n",
    "# Built on A100, deploying to DGX Spark (Blackwell)\n",
    "\n",
    "# ‚úÖ Right - Build on target hardware or specify target\n",
    "python -m tensorrt_llm.commands.build \\\n",
    "    --target_architecture blackwell  # Specify target\n",
    "```\n",
    "\n",
    "### Mistake 2: Setting max_input_len Too High\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - Wastes memory on unused context length\n",
    "max_input_len = 131072  # 128K context - but you only use 4K\n",
    "\n",
    "# ‚úÖ Right - Set based on actual usage\n",
    "max_input_len = 8192   # Sufficient for most RAG applications\n",
    "```\n",
    "\n",
    "### Mistake 3: Forgetting --enable_context_fmha\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong - Missing flash attention for prefill\n",
    "python -m tensorrt_llm.commands.build --model_dir ...\n",
    "\n",
    "# ‚úÖ Right - Enable context FMHA for fast prefill\n",
    "python -m tensorrt_llm.commands.build \\\n",
    "    --model_dir ... \\\n",
    "    --enable_context_fmha\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Compare Different dtypes\n",
    "\n",
    "Build engines with different dtypes and compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# If you have TensorRT-LLM access, build engines with:\n",
    "# - bfloat16\n",
    "# - float8\n",
    "# - fp4 (if on Blackwell)\n",
    "#\n",
    "# For each, measure:\n",
    "# - Build time\n",
    "# - Engine size\n",
    "# - Prefill latency\n",
    "# - Decode speed\n",
    "# - Output quality (perplexity if possible)\n",
    "\n",
    "# TODO: Document your findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Optimize for Your Workload\n",
    "\n",
    "Tune TensorRT-LLM build parameters for a specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Choose one of these workloads:\n",
    "# A) Customer support chatbot (short prompts, short responses)\n",
    "# B) Document QA (long prompts, medium responses)\n",
    "# C) Batch summarization (long prompts, long responses, high batch)\n",
    "#\n",
    "# For your chosen workload, determine optimal:\n",
    "# - max_input_len\n",
    "# - max_output_len\n",
    "# - max_batch_size\n",
    "# - dtype\n",
    "#\n",
    "# Document your reasoning\n",
    "\n",
    "# TODO: Implement your optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ TensorRT-LLM's architecture and optimization pipeline\n",
    "- ‚úÖ How to build optimized TRT engines for LLMs\n",
    "- ‚úÖ When TensorRT-LLM is the right choice (long inputs, max throughput)\n",
    "- ‚úÖ Key build parameters and their effects\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [TensorRT-LLM GitHub](https://github.com/NVIDIA/TensorRT-LLM)\n",
    "- [TensorRT-LLM Documentation](https://nvidia.github.io/TensorRT-LLM/)\n",
    "- [NVIDIA NGC Container Catalog](https://catalog.ngc.nvidia.com/)\n",
    "- [Triton Inference Server](https://github.com/triton-inference-server)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cleanup\nimport gc\n\n# Clear Python garbage\ngc.collect()\n\n# Clear GPU memory cache if torch is available\ntry:\n    import torch\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n        print(\"‚úÖ GPU memory cache cleared!\")\nexcept ImportError:\n    pass\n\nprint(\"‚úÖ Cleanup complete!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}