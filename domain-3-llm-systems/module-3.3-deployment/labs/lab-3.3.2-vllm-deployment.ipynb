{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.3.2: vLLM Deployment with Continuous Batching\n",
    "\n",
    "**Module:** 3.3 - Model Deployment & Inference Engines  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand what makes vLLM special (PagedAttention, continuous batching)\n",
    "- [ ] Deploy vLLM on DGX Spark with optimal configuration\n",
    "- [ ] Implement and test continuous batching under load\n",
    "- [ ] Monitor and tune vLLM for your workload\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Lab 3.3.1 (Engine Benchmark)\n",
    "- Access to: HuggingFace models (HF_TOKEN set if using gated models)\n",
    "- Docker installed and configured for GPU access\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**Why vLLM matters for production deployments:**\n",
    "\n",
    "Imagine you're running a customer service chatbot that needs to handle 100 concurrent conversations. Traditional inference would:\n",
    "- Process one request at a time ‚Üí 100 users waiting\n",
    "- Or batch requests ‚Üí Users wait until batch is full\n",
    "\n",
    "**vLLM's continuous batching** lets you:\n",
    "- Add new requests to an ongoing batch dynamically\n",
    "- Each user starts getting responses immediately\n",
    "- GPU stays busy 100% of the time\n",
    "\n",
    "Companies like Anyscale, Modal, and Replicate use vLLM to serve millions of requests efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: Continuous Batching & PagedAttention\n",
    "\n",
    "### Continuous Batching\n",
    "\n",
    "> **Imagine you're a DJ at a party...**\n",
    ">\n",
    "> **Traditional batching** = You wait until 10 people request songs, then play them all at once.\n",
    "> Everyone waits, and some people leave frustrated.\n",
    ">\n",
    "> **Continuous batching** = You're mixing multiple songs simultaneously. As soon as someone requests\n",
    "> a song, you blend it into the current mix. No one waits!\n",
    ">\n",
    "> **In AI terms:** vLLM can add new requests to an in-progress batch. When a request finishes,\n",
    "> a new one immediately takes its place - the GPU never sits idle.\n",
    "\n",
    "### PagedAttention\n",
    "\n",
    "> **Imagine your GPU memory is like a parking lot...**\n",
    ">\n",
    "> **Traditional KV cache** = Each car (request) needs to reserve a parking LANE from start to end.\n",
    "> Even if the lane is mostly empty, no one else can use it. Wastes 60-80% of space!\n",
    ">\n",
    "> **PagedAttention** = Cars park in individual SPOTS. A request might use spots scattered around\n",
    "> the lot. When a spot is freed, anyone can use it. Near 0% waste!\n",
    ">\n",
    "> **In AI terms:** PagedAttention manages KV cache in fixed-size \"pages\" that can be allocated\n",
    "> and freed dynamically, dramatically improving memory efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setting Up vLLM on DGX Spark\n",
    "\n",
    "DGX Spark has some special requirements for vLLM due to its ARM64 architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check our system\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def run_command(cmd):\n",
    "    \"\"\"Run a shell command and return output.\"\"\"\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    return result.stdout.strip() if result.returncode == 0 else f\"Error: {result.stderr}\"\n",
    "\n",
    "print(\"üîç System Information:\")\n",
    "print(f\"   Architecture: {run_command('uname -m')}\")\n",
    "print(f\"   Python: {sys.version.split()[0]}\")\n",
    "print(f\"   Docker: {run_command('docker --version').split(',')[0] if 'Error' not in run_command('docker --version') else 'Not installed'}\")\n",
    "\n",
    "# Check NVIDIA driver\n",
    "nvidia_output = run_command('nvidia-smi --query-gpu=driver_version,name,memory.total --format=csv,noheader')\n",
    "if 'Error' not in nvidia_output:\n",
    "    driver, gpu, memory = nvidia_output.split(',')\n",
    "    print(f\"   GPU: {gpu.strip()}\")\n",
    "    print(f\"   Memory: {memory.strip()}\")\n",
    "    print(f\"   Driver: {driver.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üöÄ Starting vLLM\n\nThere are two ways to run vLLM on DGX Spark:\n\n#### Option 1: PyTorch NGC Container with vLLM (Recommended for DGX Spark)\n\n```bash\n# For DGX Spark ARM64, use the PyTorch NGC container and install vLLM\ndocker run --gpus all -p 8000:8000 \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    -e HF_TOKEN=$HF_TOKEN \\\n    --ipc=host \\\n    nvcr.io/nvidia/pytorch:25.11-py3 \\\n    bash -c \"pip install vllm && python -m vllm.entrypoints.openai.api_server \\\n        --model Qwen/Qwen3-8B-Instruct \\\n        --enforce-eager \\\n        --dtype bfloat16 \\\n        --max-model-len 4096 \\\n        --gpu-memory-utilization 0.9\"\n```\n\n#### Option 2: Official vLLM Container (verify ARM64 support)\n\n```bash\n# Check https://hub.docker.com/r/vllm/vllm-openai for ARM64 availability\ndocker run --gpus all -p 8000:8000 \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    --ipc=host \\\n    vllm/vllm-openai:latest \\\n    --model Qwen/Qwen3-8B-Instruct \\\n    --enforce-eager\n```\n\n#### Important Flags for DGX Spark:\n\n| Flag | Purpose |\n|------|--------|\n| `--enforce-eager` | Disable CUDA graphs (required for ARM64) |\n| `--max-model-len` | Limit context to save memory |\n| `--gpu-memory-utilization` | How much GPU memory to use (0.9 = 90%) |\n| `--dtype bfloat16` | Use BF16 for Blackwell optimization |\n| `--ipc=host` | Required for DataLoader workers (docker flag) |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate the vLLM startup command\ndef generate_vllm_command(\n    model: str = \"Qwen/Qwen3-8B-Instruct\",\n    max_model_len: int = 4096,\n    gpu_memory_utilization: float = 0.9,\n    port: int = 8000,\n    dtype: str = \"bfloat16\"\n) -> str:\n    \"\"\"\n    Generate the docker command to start vLLM on DGX Spark.\n    \n    Args:\n        model: HuggingFace model ID\n        max_model_len: Maximum context length\n        gpu_memory_utilization: Fraction of GPU memory to use (0.0-1.0)\n        port: Port to expose the API on\n        dtype: Data type for inference (bfloat16 recommended for Blackwell)\n        \n    Returns:\n        Docker command string ready to execute\n    \"\"\"\n    hf_token = os.environ.get(\"HF_TOKEN\", \"\")\n    token_flag = f'-e HF_TOKEN={hf_token}' if hf_token else '-e HF_TOKEN=$HF_TOKEN'\n    \n    # Use PyTorch NGC container for ARM64 compatibility\n    cmd = f\"\"\"docker run --gpus all -p {port}:8000 \\\\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\\\n    {token_flag} \\\\\n    --ipc=host \\\\\n    nvcr.io/nvidia/pytorch:25.11-py3 \\\\\n    bash -c \"pip install vllm && python -m vllm.entrypoints.openai.api_server \\\\\n        --model {model} \\\\\n        --enforce-eager \\\\\n        --max-model-len {max_model_len} \\\\\n        --gpu-memory-utilization {gpu_memory_utilization} \\\\\n        --dtype {dtype}\"\n\"\"\"\n    \n    return cmd\n\nprint(\"üöÄ vLLM Startup Command for DGX Spark:\")\nprint(\"=\" * 60)\nprint(generate_vllm_command())\nprint(\"=\" * 60)\nprint(\"\\nüí° Copy and run this in a separate terminal!\")\nprint(\"   Note: First run will install vLLM (takes a few minutes)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if vLLM is Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "VLLM_URL = \"http://localhost:8000\"\n",
    "\n",
    "def check_vllm_status(url: str = VLLM_URL, timeout: int = 5) -> dict:\n",
    "    \"\"\"Check if vLLM server is running and get its status.\"\"\"\n",
    "    try:\n",
    "        # Check models endpoint\n",
    "        response = requests.get(f\"{url}/v1/models\", timeout=timeout)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json()\n",
    "            model_list = [m[\"id\"] for m in models.get(\"data\", [])]\n",
    "            return {\n",
    "                \"status\": \"running\",\n",
    "                \"models\": model_list,\n",
    "                \"url\": url\n",
    "            }\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"error\": str(e)}\n",
    "    \n",
    "    return {\"status\": \"not_running\"}\n",
    "\n",
    "status = check_vllm_status()\n",
    "\n",
    "if status[\"status\"] == \"running\":\n",
    "    print(f\"‚úÖ vLLM is running at {status['url']}\")\n",
    "    print(f\"   Available models: {', '.join(status['models'])}\")\n",
    "else:\n",
    "    print(\"‚ùå vLLM is not running\")\n",
    "    print(\"   Please start vLLM using the command above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding Continuous Batching\n",
    "\n",
    "Let's visualize how continuous batching works compared to static batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation of static vs continuous batching\n",
    "import random\n",
    "\n",
    "def simulate_batching(n_requests: int = 10, batch_size: int = 4):\n",
    "    \"\"\"\n",
    "    Simulate static vs continuous batching to show the difference.\n",
    "    \"\"\"\n",
    "    # Request arrival times (random within first 5 seconds)\n",
    "    arrivals = sorted([random.uniform(0, 5) for _ in range(n_requests)])\n",
    "    # Processing time per request (varies based on output length)\n",
    "    process_times = [random.uniform(0.5, 2.0) for _ in range(n_requests)]\n",
    "    \n",
    "    # STATIC BATCHING\n",
    "    # Wait for batch_size requests, process together, repeat\n",
    "    static_completions = []\n",
    "    current_batch = []\n",
    "    batch_start = 0\n",
    "    \n",
    "    for i, (arrival, proc_time) in enumerate(zip(arrivals, process_times)):\n",
    "        current_batch.append((i, arrival, proc_time))\n",
    "        \n",
    "        if len(current_batch) >= batch_size or i == n_requests - 1:\n",
    "            # Process batch: starts when last request arrives\n",
    "            batch_start = max(batch_start, max(x[1] for x in current_batch))\n",
    "            max_time = max(x[2] for x in current_batch)\n",
    "            \n",
    "            for req_id, arr_time, _ in current_batch:\n",
    "                completion = batch_start + max_time\n",
    "                wait_time = completion - arr_time\n",
    "                static_completions.append((req_id, arr_time, completion, wait_time))\n",
    "            \n",
    "            batch_start = completion\n",
    "            current_batch = []\n",
    "    \n",
    "    # CONTINUOUS BATCHING\n",
    "    # Process requests as they arrive, overlapping execution\n",
    "    continuous_completions = []\n",
    "    for i, (arrival, proc_time) in enumerate(zip(arrivals, process_times)):\n",
    "        completion = arrival + proc_time  # Simplified: starts immediately\n",
    "        wait_time = completion - arrival\n",
    "        continuous_completions.append((i, arrival, completion, wait_time))\n",
    "    \n",
    "    return static_completions, continuous_completions, arrivals\n",
    "\n",
    "# Run simulation\n",
    "static, continuous, arrivals = simulate_batching(n_requests=8, batch_size=4)\n",
    "\n",
    "print(\"üìä Batching Comparison (8 requests, batch_size=4)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüî¥ STATIC BATCHING:\")\n",
    "print(f\"{'Request':<10} {'Arrival':<10} {'Complete':<10} {'Wait Time':<10}\")\n",
    "print(\"-\" * 40)\n",
    "for req_id, arr, comp, wait in static:\n",
    "    print(f\"{req_id:<10} {arr:<10.2f} {comp:<10.2f} {wait:<10.2f}\")\n",
    "avg_static = sum(w for _, _, _, w in static) / len(static)\n",
    "print(f\"\\nAverage wait time: {avg_static:.2f}s\")\n",
    "\n",
    "print(\"\\nüü¢ CONTINUOUS BATCHING:\")\n",
    "print(f\"{'Request':<10} {'Arrival':<10} {'Complete':<10} {'Wait Time':<10}\")\n",
    "print(\"-\" * 40)\n",
    "for req_id, arr, comp, wait in continuous:\n",
    "    print(f\"{req_id:<10} {arr:<10.2f} {comp:<10.2f} {wait:<10.2f}\")\n",
    "avg_continuous = sum(w for _, _, _, w in continuous) / len(continuous)\n",
    "print(f\"\\nAverage wait time: {avg_continuous:.2f}s\")\n",
    "\n",
    "print(f\"\\nüéØ Improvement: {(avg_static - avg_continuous) / avg_static * 100:.1f}% lower latency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "In the simulation:\n",
    "- **Static batching** waits to fill batches before processing. Late-arriving requests in a batch get processed with early ones, but early arrivers wait unnecessarily.\n",
    "- **Continuous batching** starts processing each request immediately. As requests complete, new ones join the active batch.\n",
    "\n",
    "The real magic happens when combined with PagedAttention - memory is used efficiently even with variable-length sequences.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Testing vLLM Under Load\n",
    "\n",
    "Now let's send real requests to vLLM and see continuous batching in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "@dataclass\n",
    "class RequestResult:\n",
    "    \"\"\"Result from a single request.\"\"\"\n",
    "    request_id: int\n",
    "    start_time: float\n",
    "    first_token_time: Optional[float]\n",
    "    end_time: float\n",
    "    tokens_generated: int\n",
    "    success: bool\n",
    "    error: Optional[str] = None\n",
    "    \n",
    "    @property\n",
    "    def ttft(self) -> float:\n",
    "        \"\"\"Time to first token in seconds.\"\"\"\n",
    "        if self.first_token_time:\n",
    "            return self.first_token_time - self.start_time\n",
    "        return 0.0\n",
    "    \n",
    "    @property\n",
    "    def total_time(self) -> float:\n",
    "        return self.end_time - self.start_time\n",
    "    \n",
    "    @property\n",
    "    def tokens_per_second(self) -> float:\n",
    "        decode_time = self.total_time - self.ttft\n",
    "        if decode_time > 0:\n",
    "            return self.tokens_generated / decode_time\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "async def send_vllm_request(\n",
    "    session: aiohttp.ClientSession,\n",
    "    request_id: int,\n",
    "    prompt: str,\n",
    "    model: str,\n",
    "    max_tokens: int = 100\n",
    ") -> RequestResult:\n",
    "    \"\"\"\n",
    "    Send a single request to vLLM and measure timing.\n",
    "    \"\"\"\n",
    "    start_time = time.perf_counter()\n",
    "    first_token_time = None\n",
    "    tokens = 0\n",
    "    \n",
    "    try:\n",
    "        async with session.post(\n",
    "            f\"{VLLM_URL}/v1/chat/completions\",\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"stream\": True\n",
    "            },\n",
    "            timeout=aiohttp.ClientTimeout(total=60)\n",
    "        ) as response:\n",
    "            async for line in response.content:\n",
    "                line_str = line.decode().strip()\n",
    "                if line_str.startswith(\"data: \"):\n",
    "                    data_str = line_str[6:]\n",
    "                    if data_str == \"[DONE]\":\n",
    "                        break\n",
    "                    try:\n",
    "                        import json\n",
    "                        chunk = json.loads(data_str)\n",
    "                        delta = chunk.get(\"choices\", [{}])[0].get(\"delta\", {})\n",
    "                        if delta.get(\"content\"):\n",
    "                            if first_token_time is None:\n",
    "                                first_token_time = time.perf_counter()\n",
    "                            tokens += 1\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        return RequestResult(\n",
    "            request_id=request_id,\n",
    "            start_time=start_time,\n",
    "            first_token_time=first_token_time,\n",
    "            end_time=time.perf_counter(),\n",
    "            tokens_generated=tokens,\n",
    "            success=True\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        return RequestResult(\n",
    "            request_id=request_id,\n",
    "            start_time=start_time,\n",
    "            first_token_time=None,\n",
    "            end_time=time.perf_counter(),\n",
    "            tokens_generated=0,\n",
    "            success=False,\n",
    "            error=str(e)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_load_test(\n",
    "    prompts: List[str],\n",
    "    model: str,\n",
    "    concurrency: int = 4,\n",
    "    max_tokens: int = 100\n",
    ") -> List[RequestResult]:\n",
    "    \"\"\"\n",
    "    Run a load test with specified concurrency.\n",
    "    \"\"\"\n",
    "    semaphore = asyncio.Semaphore(concurrency)\n",
    "    \n",
    "    async def limited_request(session, req_id, prompt):\n",
    "        async with semaphore:\n",
    "            return await send_vllm_request(session, req_id, prompt, model, max_tokens)\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [\n",
    "            limited_request(session, i, prompt)\n",
    "            for i, prompt in enumerate(prompts)\n",
    "        ]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def analyze_load_test(results: List[RequestResult]) -> dict:\n",
    "    \"\"\"Analyze load test results.\"\"\"\n",
    "    successful = [r for r in results if r.success]\n",
    "    \n",
    "    if not successful:\n",
    "        return {\"error\": \"No successful requests\"}\n",
    "    \n",
    "    ttfts = [r.ttft * 1000 for r in successful]  # ms\n",
    "    latencies = [r.total_time * 1000 for r in successful]  # ms\n",
    "    speeds = [r.tokens_per_second for r in successful if r.tokens_per_second > 0]\n",
    "    \n",
    "    total_time = max(r.end_time for r in results) - min(r.start_time for r in results)\n",
    "    \n",
    "    return {\n",
    "        \"total_requests\": len(results),\n",
    "        \"successful\": len(successful),\n",
    "        \"failed\": len(results) - len(successful),\n",
    "        \"total_time_s\": total_time,\n",
    "        \"throughput_rps\": len(successful) / total_time if total_time > 0 else 0,\n",
    "        \"avg_ttft_ms\": sum(ttfts) / len(ttfts) if ttfts else 0,\n",
    "        \"p50_ttft_ms\": sorted(ttfts)[len(ttfts)//2] if ttfts else 0,\n",
    "        \"p90_ttft_ms\": sorted(ttfts)[int(len(ttfts)*0.9)] if ttfts else 0,\n",
    "        \"avg_latency_ms\": sum(latencies) / len(latencies) if latencies else 0,\n",
    "        \"p90_latency_ms\": sorted(latencies)[int(len(latencies)*0.9)] if latencies else 0,\n",
    "        \"avg_tokens_per_sec\": sum(speeds) / len(speeds) if speeds else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run load test (only if vLLM is running)\n",
    "status = check_vllm_status()\n",
    "\n",
    "if status[\"status\"] == \"running\":\n",
    "    # Prepare test prompts\n",
    "    test_prompts = [\n",
    "        \"What is the capital of France?\",\n",
    "        \"Explain quantum computing in simple terms.\",\n",
    "        \"Write a haiku about programming.\",\n",
    "        \"What are the benefits of exercise?\",\n",
    "        \"How does photosynthesis work?\",\n",
    "        \"Describe the water cycle.\",\n",
    "        \"What is machine learning?\",\n",
    "        \"Name three famous scientists.\",\n",
    "        \"What causes earthquakes?\",\n",
    "        \"How do vaccines work?\",\n",
    "        \"Explain the theory of relativity.\",\n",
    "        \"What is artificial intelligence?\",\n",
    "    ]\n",
    "    \n",
    "    model = status[\"models\"][0] if status[\"models\"] else \"Qwen/Qwen3-8B-Instruct\"\n",
    "    \n",
    "    print(f\"üöÄ Running load test against {model}\")\n",
    "    print(f\"   Requests: {len(test_prompts)}\")\n",
    "    print(f\"   Concurrency levels: [1, 2, 4, 8]\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    load_test_results = {}\n",
    "    \n",
    "    for concurrency in [1, 2, 4, 8]:\n",
    "        print(f\"\\nüìä Testing concurrency={concurrency}...\")\n",
    "        \n",
    "        # Run the test\n",
    "        results = asyncio.run(run_load_test(\n",
    "            prompts=test_prompts,\n",
    "            model=model,\n",
    "            concurrency=concurrency,\n",
    "            max_tokens=100\n",
    "        ))\n",
    "        \n",
    "        analysis = analyze_load_test(results)\n",
    "        load_test_results[concurrency] = analysis\n",
    "        \n",
    "        print(f\"   ‚úÖ Throughput: {analysis['throughput_rps']:.2f} req/s\")\n",
    "        print(f\"   ‚úÖ Avg TTFT: {analysis['avg_ttft_ms']:.0f}ms\")\n",
    "        print(f\"   ‚úÖ P90 Latency: {analysis['p90_latency_ms']:.0f}ms\")\n",
    "        print(f\"   ‚úÖ Success Rate: {analysis['successful']}/{analysis['total_requests']}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è vLLM is not running. Please start it first.\")\n",
    "    print(\"   Simulating results for demonstration...\")\n",
    "    \n",
    "    # Simulated results for demonstration\n",
    "    load_test_results = {\n",
    "        1: {\"throughput_rps\": 1.2, \"avg_ttft_ms\": 45, \"p90_latency_ms\": 850},\n",
    "        2: {\"throughput_rps\": 2.3, \"avg_ttft_ms\": 52, \"p90_latency_ms\": 920},\n",
    "        4: {\"throughput_rps\": 4.1, \"avg_ttft_ms\": 68, \"p90_latency_ms\": 1100},\n",
    "        8: {\"throughput_rps\": 6.5, \"avg_ttft_ms\": 95, \"p90_latency_ms\": 1450},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize load test results\ntry:\n    import matplotlib.pyplot as plt\n    import numpy as np\n    \n    concurrencies = sorted(load_test_results.keys())\n    throughputs = [load_test_results[c][\"throughput_rps\"] for c in concurrencies]\n    ttfts = [load_test_results[c][\"avg_ttft_ms\"] for c in concurrencies]\n    latencies = [load_test_results[c][\"p90_latency_ms\"] for c in concurrencies]\n    \n    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n    \n    # Throughput\n    axes[0].bar(concurrencies, throughputs, color='steelblue')\n    axes[0].set_xlabel('Concurrency')\n    axes[0].set_ylabel('Throughput (req/s)')\n    axes[0].set_title('Throughput vs Concurrency')\n    axes[0].set_xticks(concurrencies)\n    \n    # TTFT\n    axes[1].plot(concurrencies, ttfts, 'o-', color='green', linewidth=2, markersize=8)\n    axes[1].set_xlabel('Concurrency')\n    axes[1].set_ylabel('Avg TTFT (ms)')\n    axes[1].set_title('Time to First Token vs Concurrency')\n    axes[1].set_xticks(concurrencies)\n    axes[1].grid(True, alpha=0.3)\n    \n    # Latency\n    axes[2].plot(concurrencies, latencies, 's-', color='red', linewidth=2, markersize=8)\n    axes[2].set_xlabel('Concurrency')\n    axes[2].set_ylabel('P90 Latency (ms)')\n    axes[2].set_title('P90 Latency vs Concurrency')\n    axes[2].set_xticks(concurrencies)\n    axes[2].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig('vllm_load_test.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    print(\"\\nüìà Chart saved to vllm_load_test.png\")\n    \nexcept ImportError:\n    print(\"‚ö†Ô∏è matplotlib not available for visualization\")\n    print(\"   Install with: pip install matplotlib\")\n    print(\"   Or in NGC container: pip install matplotlib --user\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Understanding the Results\n",
    "\n",
    "What we observe with continuous batching:\n",
    "\n",
    "1. **Throughput increases** with concurrency (more requests/second)\n",
    "2. **TTFT slightly increases** due to batching overhead\n",
    "3. **Latency increases** but sub-linearly (the key benefit!)\n",
    "\n",
    "Without continuous batching, latency would increase linearly with concurrency.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: vLLM Configuration Tuning\n",
    "\n",
    "Let's explore key vLLM parameters for DGX Spark optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vLLM Configuration Guide for DGX Spark\n",
    "\n",
    "vllm_configs = {\n",
    "    \"basic\": {\n",
    "        \"description\": \"Simple setup for testing\",\n",
    "        \"flags\": {\n",
    "            \"--model\": \"Qwen/Qwen3-8B-Instruct\",\n",
    "            \"--enforce-eager\": True,  # Required for ARM64\n",
    "            \"--max-model-len\": 4096,\n",
    "        },\n",
    "        \"use_case\": \"Development, testing\"\n",
    "    },\n",
    "    \"high_throughput\": {\n",
    "        \"description\": \"Maximize concurrent request handling\",\n",
    "        \"flags\": {\n",
    "            \"--model\": \"Qwen/Qwen3-8B-Instruct\",\n",
    "            \"--enforce-eager\": True,\n",
    "            \"--max-model-len\": 4096,\n",
    "            \"--gpu-memory-utilization\": 0.95,  # Use more memory\n",
    "            \"--max-num-seqs\": 256,  # More concurrent sequences\n",
    "            \"--max-num-batched-tokens\": 8192,\n",
    "        },\n",
    "        \"use_case\": \"Batch processing, high load\"\n",
    "    },\n",
    "    \"low_latency\": {\n",
    "        \"description\": \"Minimize time to first token\",\n",
    "        \"flags\": {\n",
    "            \"--model\": \"Qwen/Qwen3-8B-Instruct\",\n",
    "            \"--enforce-eager\": True,\n",
    "            \"--max-model-len\": 2048,  # Smaller context = faster prefill\n",
    "            \"--gpu-memory-utilization\": 0.8,\n",
    "            \"--max-num-seqs\": 32,  # Fewer concurrent = faster per-request\n",
    "        },\n",
    "        \"use_case\": \"Interactive chat, real-time\"\n",
    "    },\n",
    "    \"large_context\": {\n",
    "        \"description\": \"For long documents and RAG\",\n",
    "        \"flags\": {\n",
    "            \"--model\": \"Qwen/Qwen3-8B-Instruct\",\n",
    "            \"--enforce-eager\": True,\n",
    "            \"--max-model-len\": 32768,  # Full context length\n",
    "            \"--gpu-memory-utilization\": 0.95,\n",
    "            \"--max-num-seqs\": 16,  # Fewer sequences due to memory\n",
    "        },\n",
    "        \"use_case\": \"RAG, document QA\"\n",
    "    },\n",
    "    \"70b_model\": {\n",
    "        \"description\": \"Running 70B models on 128GB\",\n",
    "        \"flags\": {\n",
    "            \"--model\": \"Qwen/Qwen3-32B-Instruct\",\n",
    "            \"--enforce-eager\": True,\n",
    "            \"--max-model-len\": 4096,\n",
    "            \"--gpu-memory-utilization\": 0.98,  # Max memory\n",
    "            \"--dtype\": \"bfloat16\",\n",
    "            \"--max-num-seqs\": 8,  # Limited by memory\n",
    "        },\n",
    "        \"use_case\": \"Highest quality responses\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìã vLLM Configuration Profiles for DGX Spark\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, config in vllm_configs.items():\n",
    "    print(f\"\\nüîß {name.upper()}\")\n",
    "    print(f\"   {config['description']}\")\n",
    "    print(f\"   Use case: {config['use_case']}\")\n",
    "    print(f\"   Flags:\")\n",
    "    for flag, value in config['flags'].items():\n",
    "        if value is True:\n",
    "            print(f\"      {flag}\")\n",
    "        else:\n",
    "            print(f\"      {flag} {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting `--enforce-eager` on ARM64\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong - Will crash on DGX Spark (ARM64)\n",
    "python -m vllm.entrypoints.openai.api_server --model llama\n",
    "\n",
    "# ‚úÖ Right - Disable CUDA graphs for ARM compatibility\n",
    "python -m vllm.entrypoints.openai.api_server --model llama --enforce-eager\n",
    "```\n",
    "\n",
    "**Why:** CUDA graphs have limited ARM64 support. `--enforce-eager` uses standard execution.\n",
    "\n",
    "### Mistake 2: Setting max-model-len Too High\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong - May OOM with many concurrent requests\n",
    "--max-model-len 131072\n",
    "\n",
    "# ‚úÖ Right - Balance context length with concurrency\n",
    "--max-model-len 8192 --max-num-seqs 64\n",
    "```\n",
    "\n",
    "**Why:** KV cache memory = max_model_len √ó num_sequences. Don't allocate more than you'll use.\n",
    "\n",
    "### Mistake 3: Using Wrong Model Format\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - GGUF is for llama.cpp, not vLLM\n",
    "model = \"TheBloke/Llama-2-7B-GGUF\"\n",
    "\n",
    "# ‚úÖ Right - Use HuggingFace format\n",
    "model = \"Qwen/Qwen3-8B-Instruct\"\n",
    "```\n",
    "\n",
    "**Why:** vLLM loads HuggingFace transformers format directly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Find the Optimal Batch Size\n",
    "\n",
    "Test different `--max-num-seqs` values (8, 16, 32, 64, 128) and find the optimal setting for your workload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# TODO: For each max-num-seqs value:\n",
    "#   1. Start vLLM with that configuration\n",
    "#   2. Run the load test\n",
    "#   3. Record throughput and latency\n",
    "#   4. Find the sweet spot\n",
    "\n",
    "# Hint: Create a function that generates the vLLM command\n",
    "# and run load tests at different concurrency levels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Compare Streaming vs Non-Streaming\n",
    "\n",
    "Measure the throughput difference between streaming and non-streaming requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# TODO: Modify send_vllm_request to support non-streaming\n",
    "# TODO: Compare throughput for streaming vs non-streaming\n",
    "# TODO: When would you use each?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How continuous batching works and why it's powerful\n",
    "- ‚úÖ How to deploy vLLM on DGX Spark with optimal settings\n",
    "- ‚úÖ How to measure and analyze performance under load\n",
    "- ‚úÖ Key configuration parameters for different use cases\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Build an Auto-Scaling vLLM Deployment**\n",
    "\n",
    "Create a system that:\n",
    "1. Monitors request queue depth\n",
    "2. Dynamically adjusts `--max-num-seqs` based on load\n",
    "3. Alerts when latency exceeds thresholds\n",
    "4. Logs performance metrics for analysis\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [vLLM: Easy, Fast, and Cheap LLM Serving (Paper)](https://arxiv.org/abs/2309.06180)\n",
    "- [PagedAttention Explained](https://blog.vllm.ai/2023/06/20/vllm.html)\n",
    "- [vLLM Performance Tuning Guide](https://docs.vllm.ai/en/latest/serving/performance.html)\n",
    "- [Continuous Batching vs Static Batching](https://www.anyscale.com/blog/continuous-batching-llm-inference)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cleanup\nimport gc\n\n# Clear variables\nload_test_results = None\n\ngc.collect()\n\nprint(\"‚úÖ Cleanup complete!\")\nprint(\"\\nüí° To stop vLLM container:\")\nprint(\"   docker ps  # Find the container ID\")\nprint(\"   docker stop <container_id>\")\nprint(\"\\n   Or to stop all PyTorch containers:\")\nprint(\"   docker stop $(docker ps -q --filter ancestor=nvcr.io/nvidia/pytorch:25.11-py3)\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}