{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 3.3.1: Inference Engine Benchmark\n\n**Module:** 3.3 - Model Deployment & Inference Engines  \n**Time:** 3 hours  \n**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê\n\n---\n\n## üéØ Learning Objectives\n\nBy the end of this notebook, you will:\n- [ ] Understand the key metrics for comparing inference engines (TTFT, tokens/sec, throughput)\n- [ ] Set up and benchmark Ollama, llama.cpp, vLLM, and TensorRT-LLM\n- [ ] Know when to use each engine based on your requirements\n- [ ] Create comprehensive benchmark reports with visualizations\n\n---\n\n## üìö Prerequisites\n\n- Completed: Module 3.2 (Quantization & Optimization) or equivalent knowledge\n- Knowledge of: Basic Python, REST APIs, understanding of LLM inference\n- Having at least Ollama installed (`ollama serve`)\n\n---\n\n## üåç Real-World Context\n\n**Choosing the right inference engine is like choosing the right vehicle for a journey:**\n\n| If you need... | Choose... | Like... |\n|----------------|-----------|--------|\n| Quick local testing | Ollama | A bicycle - easy to start, good for short trips |\n| Maximum decode speed | llama.cpp | A sports car - fastest on open roads |\n| High concurrent users | vLLM | A bus - carries many passengers efficiently |\n| Best first-token latency | TensorRT-LLM | A rocket - fastest acceleration |\n| Speculative decoding | SGLang | A teleporter - skip the boring parts |\n\n**Real deployment scenarios:**\n- **Interactive chatbot** ‚Üí Optimize for TTFT (time to first token) - users want instant response\n- **Batch processing** ‚Üí Optimize for throughput (requests/sec) - you want maximum efficiency\n- **Code completion** ‚Üí Optimize for decode speed (tokens/sec) - fast completions feel snappy\n- **Document summarization** ‚Üí Optimize for prefill speed - long inputs need fast processing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What Are Inference Engines?\n",
    "\n",
    "> **Imagine you're running a restaurant kitchen...**\n",
    ">\n",
    "> The **LLM model** is your recipe book - it contains all the knowledge about how to make dishes.\n",
    ">\n",
    "> The **inference engine** is your kitchen setup - the stove, ovens, and how you organize your cooks.\n",
    ">\n",
    "> Different kitchen setups work better for different situations:\n",
    "> - **Ollama** = A home kitchen. Easy to use, great for trying recipes, but not built for 100 customers.\n",
    "> - **llama.cpp** = A food truck. Super efficient, serves one customer at a time REALLY fast.\n",
    "> - **vLLM** = A commercial kitchen. Can handle many orders at once by sharing the grill.\n",
    "> - **TensorRT-LLM** = A high-tech kitchen designed specifically for your stove brand. Optimized for NVIDIA GPUs.\n",
    ">\n",
    "> **In AI terms:** The inference engine determines HOW your model runs - how memory is managed, how requests are batched, and how fast tokens are generated.\n",
    "\n",
    "---\n",
    "\n",
    "## üîë Key Metrics Explained\n",
    "\n",
    "| Metric | What It Measures | Why It Matters |\n",
    "|--------|------------------|----------------|\n",
    "| **TTFT** (Time To First Token) | How long until you see the first word | Users perceive this as \"response time\" |\n",
    "| **Prefill Speed** | Tokens/sec processing the input | Important for long prompts (RAG, documents) |\n",
    "| **Decode Speed** | Tokens/sec generating output | How fast text appears after it starts |\n",
    "| **Throughput** | Requests/sec the system handles | How many users you can serve simultaneously |\n",
    "| **Latency (P50/P90/P99)** | Time to complete requests | P99 = slowest 1% of requests (worst case) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment Setup\n",
    "\n",
    "Let's start by setting up our benchmarking environment and checking what's available on your DGX Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "# Add scripts directory to path\n",
    "scripts_path = Path(\"../scripts\").resolve()\n",
    "sys.path.insert(0, str(scripts_path))\n",
    "\n",
    "# Import our custom utilities\n",
    "from benchmark_utils import (\n",
    "    InferenceBenchmark,\n",
    "    BenchmarkResult,\n",
    "    BatchBenchmarkResult,\n",
    "    load_benchmark_prompts,\n",
    "    compare_engines,\n",
    "    format_comparison_table,\n",
    "    get_gpu_memory_usage\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(f\"üìÅ Scripts path: {scripts_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check GPU availability and status\ndef check_gpu_status() -> bool:\n    \"\"\"\n    Check GPU availability and memory.\n    \n    Returns:\n        bool: True if GPU is available and accessible, False otherwise\n    \"\"\"\n    try:\n        result = subprocess.run(\n            [\"nvidia-smi\", \"--query-gpu=name,memory.total,memory.free,memory.used,utilization.gpu\",\n             \"--format=csv,noheader,nounits\"],\n            capture_output=True, text=True\n        )\n        if result.returncode == 0:\n            values = result.stdout.strip().split(\",\")\n            name = values[0].strip()\n            total = int(values[1]) / 1024  # Convert to GB\n            free = int(values[2]) / 1024\n            used = int(values[3]) / 1024\n            util = values[4].strip()\n            \n            print(\"üñ•Ô∏è GPU Status:\")\n            print(f\"   Name: {name}\")\n            print(f\"   Memory: {used:.1f}GB used / {total:.1f}GB total ({free:.1f}GB free)\")\n            print(f\"   Utilization: {util}%\")\n            return True\n    except Exception as e:\n        print(f\"‚ö†Ô∏è GPU check failed: {e}\")\n    return False\n\ncheck_gpu_status()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check which inference engines are available\ndef check_engine_availability() -> Dict[str, str]:\n    \"\"\"\n    Check which inference engines are running and accessible.\n    \n    Returns:\n        Dict[str, str]: Dictionary mapping engine names to their base URLs\n    \"\"\"\n    engines = {\n        \"ollama\": \"http://localhost:11434\",\n        \"vllm\": \"http://localhost:8000\",\n        \"sglang\": \"http://localhost:30000\",\n        \"tensorrt-llm\": \"http://localhost:8000\",\n    }\n    \n    health_endpoints = {\n        \"ollama\": \"/api/tags\",\n        \"vllm\": \"/v1/models\",\n        \"sglang\": \"/v1/models\",\n        \"tensorrt-llm\": \"/v1/models\",\n    }\n    \n    available = {}\n    \n    print(\"üîç Checking inference engine availability...\\n\")\n    \n    for engine, base_url in engines.items():\n        endpoint = health_endpoints[engine]\n        try:\n            response = requests.get(f\"{base_url}{endpoint}\", timeout=3)\n            if response.status_code == 200:\n                print(f\"   ‚úÖ {engine.ljust(15)} Available at {base_url}\")\n                available[engine] = base_url\n                \n                # For Ollama, list available models\n                if engine == \"ollama\":\n                    models = response.json().get(\"models\", [])\n                    model_names = [m[\"name\"] for m in models[:3]]  # First 3\n                    if model_names:\n                        print(f\"                       Models: {', '.join(model_names)}\")\n            else:\n                print(f\"   ‚ùå {engine.ljust(15)} Not responding (status {response.status_code})\")\n        except requests.exceptions.ConnectionError:\n            print(f\"   ‚ùå {engine.ljust(15)} Not running\")\n        except Exception as e:\n            print(f\"   ‚ùå {engine.ljust(15)} Error: {e}\")\n    \n    print(f\"\\nüìä Available engines: {len(available)}\")\n    return available\n\navailable_engines = check_engine_availability()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üîß Starting Inference Engines\n\nIf you need to start any engines, here are the commands:\n\n**Ollama** (simplest to start):\n```bash\n# In a separate terminal\nollama serve\n\n# Pull a model if you haven't already\nollama pull llama3.1:8b\n```\n\n**vLLM** (using PyTorch NGC container):\n```bash\n# Start vLLM with Llama 3.1 8B\n# Note: Use PyTorch NGC container and install vLLM for DGX Spark ARM64 compatibility\ndocker run --gpus all -p 8000:8000 \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    -e HF_TOKEN=$HF_TOKEN \\\n    --ipc=host \\\n    nvcr.io/nvidia/pytorch:25.11-py3 \\\n    bash -c \"pip install vllm && python -m vllm.entrypoints.openai.api_server \\\n        --model meta-llama/Llama-3.1-8B-Instruct \\\n        --enforce-eager \\\n        --dtype bfloat16 \\\n        --max-model-len 4096\"\n```\n\n**SGLang**:\n```bash\npython -m sglang.launch_server \\\n    --model meta-llama/Llama-3.1-8B-Instruct \\\n    --port 30000\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Load Benchmark Prompts\n",
    "\n",
    "We'll use a variety of prompts to test different aspects of inference performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load benchmark prompts from our data file\n",
    "data_path = Path(\"../data/benchmark_prompts.json\")\n",
    "\n",
    "if data_path.exists():\n",
    "    with open(data_path) as f:\n",
    "        all_prompts = json.load(f)\n",
    "    \n",
    "    print(\"üìù Loaded benchmark prompts:\")\n",
    "    for category, prompts in all_prompts.items():\n",
    "        if category != \"chat\":  # Skip chat format for now\n",
    "            print(f\"   ‚Ä¢ {category}: {len(prompts)} prompts\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Benchmark prompts file not found. Creating sample prompts...\")\n",
    "    all_prompts = {\n",
    "        \"short\": [\n",
    "            {\"id\": \"s1\", \"text\": \"What is the capital of France?\", \"expected_tokens\": 10},\n",
    "            {\"id\": \"s2\", \"text\": \"What is 2 + 2?\", \"expected_tokens\": 5},\n",
    "        ],\n",
    "        \"medium\": [\n",
    "            {\"id\": \"m1\", \"text\": \"Explain machine learning in 3 sentences.\", \"expected_tokens\": 100},\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select prompts for benchmarking\n",
    "# We'll use a mix of short and medium prompts for this benchmark\n",
    "\n",
    "benchmark_prompts = []\n",
    "\n",
    "# Add short prompts (for latency testing)\n",
    "for p in all_prompts.get(\"short\", [])[:3]:\n",
    "    benchmark_prompts.append({\n",
    "        \"id\": p[\"id\"],\n",
    "        \"text\": p[\"text\"],\n",
    "        \"category\": \"short\",\n",
    "        \"max_tokens\": 50\n",
    "    })\n",
    "\n",
    "# Add medium prompts (for throughput testing)\n",
    "for p in all_prompts.get(\"medium\", [])[:2]:\n",
    "    benchmark_prompts.append({\n",
    "        \"id\": p[\"id\"],\n",
    "        \"text\": p[\"text\"],\n",
    "        \"category\": \"medium\",\n",
    "        \"max_tokens\": 200\n",
    "    })\n",
    "\n",
    "# Add a code prompt\n",
    "for p in all_prompts.get(\"code\", [])[:1]:\n",
    "    benchmark_prompts.append({\n",
    "        \"id\": p[\"id\"],\n",
    "        \"text\": p[\"text\"],\n",
    "        \"category\": \"code\",\n",
    "        \"max_tokens\": 300\n",
    "    })\n",
    "\n",
    "print(f\"üìä Selected {len(benchmark_prompts)} prompts for benchmarking:\")\n",
    "for p in benchmark_prompts:\n",
    "    print(f\"   [{p['category']}] {p['text'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Single Request Benchmarks\n",
    "\n",
    "Let's start by benchmarking single requests to understand baseline latency.\n",
    "\n",
    "### üßí ELI5: Latency vs Throughput\n",
    "\n",
    "> Think of a water slide at a water park:\n",
    ">\n",
    "> **Latency** = How long it takes YOU to go from top to bottom (your wait time)\n",
    "> **Throughput** = How many people go down the slide per hour (total capacity)\n",
    ">\n",
    "> You can have a fast slide (low latency) but if only one person can go at a time, throughput is low.\n",
    "> Or you can have multiple lanes (high throughput) but each lane might be a bit slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark a single engine with single requests\n",
    "def benchmark_single_requests(\n",
    "    engine: str,\n",
    "    model: str,\n",
    "    prompts: List[Dict],\n",
    "    base_url: Optional[str] = None\n",
    ") -> List[BenchmarkResult]:\n",
    "    \"\"\"\n",
    "    Run single-request benchmarks for an engine.\n",
    "    \n",
    "    Args:\n",
    "        engine: Engine name (\"ollama\", \"vllm\", etc.)\n",
    "        model: Model identifier\n",
    "        prompts: List of prompt dictionaries\n",
    "        base_url: Optional custom URL\n",
    "    \n",
    "    Returns:\n",
    "        List of BenchmarkResult objects\n",
    "    \"\"\"\n",
    "    print(f\"\\nüöÄ Benchmarking {engine} ({model})...\")\n",
    "    \n",
    "    try:\n",
    "        benchmark = InferenceBenchmark(engine=engine, model=model, base_url=base_url)\n",
    "        \n",
    "        # Warmup\n",
    "        print(\"   Warming up (3 requests)...\")\n",
    "        benchmark.warmup(3)\n",
    "        \n",
    "        results = []\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            print(f\"   Running prompt {i+1}/{len(prompts)}: {prompt['text'][:40]}...\", end=\"\")\n",
    "            \n",
    "            result = benchmark.run_single(\n",
    "                prompt[\"text\"],\n",
    "                max_tokens=prompt.get(\"max_tokens\", 100),\n",
    "                temperature=0.7,\n",
    "                stream=True\n",
    "            )\n",
    "            results.append(result)\n",
    "            \n",
    "            if result.error:\n",
    "                print(f\" ‚ùå Error: {result.error}\")\n",
    "            else:\n",
    "                print(f\" ‚úÖ TTFT: {result.time_to_first_token*1000:.0f}ms, \"\n",
    "                      f\"{result.tokens_per_second:.1f} tok/s\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Failed: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmarks on available engines\n",
    "all_results = {}\n",
    "\n",
    "# Define engine configurations\n",
    "engine_configs = {\n",
    "    \"ollama\": {\n",
    "        \"model\": \"llama3.1:8b\",  # Change this to your installed model\n",
    "        \"base_url\": \"http://localhost:11434\"\n",
    "    },\n",
    "    \"vllm\": {\n",
    "        \"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        \"base_url\": \"http://localhost:8000\"\n",
    "    },\n",
    "    \"sglang\": {\n",
    "        \"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        \"base_url\": \"http://localhost:30000\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run benchmarks only on available engines\n",
    "for engine, config in engine_configs.items():\n",
    "    if engine in available_engines:\n",
    "        results = benchmark_single_requests(\n",
    "            engine=engine,\n",
    "            model=config[\"model\"],\n",
    "            prompts=benchmark_prompts,\n",
    "            base_url=config[\"base_url\"]\n",
    "        )\n",
    "        if results:\n",
    "            all_results[engine] = results\n",
    "\n",
    "print(f\"\\nüìä Completed benchmarks for {len(all_results)} engines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze and compare results\ndef analyze_results(results_by_engine: Dict[str, List[BenchmarkResult]]) -> Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Compute statistics for each engine.\n    \n    Args:\n        results_by_engine: Dictionary mapping engine names to lists of BenchmarkResult\n        \n    Returns:\n        Dictionary with computed statistics for each engine\n    \"\"\"\n    import numpy as np  # Ensure numpy is available even if run out of order\n    \n    analysis = {}\n    \n    for engine, results in results_by_engine.items():\n        successful = [r for r in results if r.error is None]\n        \n        if not successful:\n            continue\n            \n        ttfts = [r.time_to_first_token * 1000 for r in successful]  # in ms\n        speeds = [r.tokens_per_second for r in successful]\n        latencies = [r.total_time * 1000 for r in successful]  # in ms\n        \n        analysis[engine] = {\n            \"count\": len(successful),\n            \"ttft_avg_ms\": np.mean(ttfts),\n            \"ttft_p50_ms\": np.percentile(ttfts, 50),\n            \"ttft_p90_ms\": np.percentile(ttfts, 90),\n            \"speed_avg_tps\": np.mean(speeds),\n            \"speed_max_tps\": np.max(speeds),\n            \"latency_avg_ms\": np.mean(latencies),\n            \"latency_p90_ms\": np.percentile(latencies, 90),\n        }\n    \n    return analysis\n\nif all_results:\n    analysis = analyze_results(all_results)\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"üìä SINGLE REQUEST BENCHMARK RESULTS\")\n    print(\"=\"*70)\n    \n    # Create comparison table\n    print(f\"\\n{'Engine':<15} {'Avg TTFT':>12} {'P90 TTFT':>12} {'Avg Speed':>12} {'P90 Latency':>12}\")\n    print(\"-\"*70)\n    \n    for engine, stats in analysis.items():\n        print(f\"{engine:<15} \"\n              f\"{stats['ttft_avg_ms']:>10.1f}ms \"\n              f\"{stats['ttft_p90_ms']:>10.1f}ms \"\n              f\"{stats['speed_avg_tps']:>10.1f}/s \"\n              f\"{stats['latency_p90_ms']:>10.1f}ms\")\nelse:\n    print(\"\\n‚ö†Ô∏è No results to analyze. Make sure at least one engine is running.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "We just measured three key metrics for each engine:\n",
    "\n",
    "1. **TTFT (Time To First Token)**: How quickly the model starts responding\n",
    "   - Lower is better for interactive applications\n",
    "   - Ollama and llama.cpp typically excel here\n",
    "\n",
    "2. **Speed (tokens/second)**: How fast tokens are generated\n",
    "   - Higher is better for long responses\n",
    "   - llama.cpp typically leads in single-request decode speed\n",
    "\n",
    "3. **Latency (total request time)**: End-to-end request completion\n",
    "   - Combines TTFT + (output_length / speed)\n",
    "   - Important for batch processing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Concurrent Request Benchmarks\n",
    "\n",
    "Now let's test how each engine handles multiple simultaneous requests.\n",
    "\n",
    "### üßí ELI5: Why Concurrency Matters\n",
    "\n",
    "> Imagine you're a chef:\n",
    ">\n",
    "> **Single request** = Making one sandwich at a time\n",
    "> **Concurrent requests** = Making 10 sandwiches at once\n",
    ">\n",
    "> Some chefs (engines) can juggle many sandwiches efficiently by reusing ingredients (KV cache sharing).\n",
    "> Others might get overwhelmed and slow down for everyone.\n",
    ">\n",
    "> vLLM is like a chef with a special system for managing multiple orders efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concurrent benchmark settings\n",
    "CONCURRENCY_LEVELS = [1, 2, 4, 8]  # Number of simultaneous requests\n",
    "REQUESTS_PER_LEVEL = 12  # Total requests at each concurrency level\n",
    "\n",
    "# Prepare prompts for concurrent testing (repeat as needed)\n",
    "concurrent_prompts = [p[\"text\"] for p in benchmark_prompts] * 4  # Repeat to get enough\n",
    "concurrent_prompts = concurrent_prompts[:REQUESTS_PER_LEVEL]\n",
    "\n",
    "print(f\"üìä Concurrent Benchmark Configuration:\")\n",
    "print(f\"   Concurrency levels: {CONCURRENCY_LEVELS}\")\n",
    "print(f\"   Requests per level: {REQUESTS_PER_LEVEL}\")\n",
    "print(f\"   Prompts prepared: {len(concurrent_prompts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run concurrent benchmarks\n",
    "concurrent_results = {}\n",
    "\n",
    "for engine, config in engine_configs.items():\n",
    "    if engine not in available_engines:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nüöÄ Concurrent benchmark: {engine}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    concurrent_results[engine] = {}\n",
    "    \n",
    "    try:\n",
    "        benchmark = InferenceBenchmark(\n",
    "            engine=engine,\n",
    "            model=config[\"model\"],\n",
    "            base_url=config[\"base_url\"]\n",
    "        )\n",
    "        \n",
    "        for concurrency in CONCURRENCY_LEVELS:\n",
    "            print(f\"   Testing concurrency={concurrency}...\", end=\" \")\n",
    "            \n",
    "            result = benchmark.run_batch(\n",
    "                prompts=concurrent_prompts,\n",
    "                max_tokens=100,\n",
    "                concurrency=concurrency,\n",
    "                stream=False  # Faster for batch testing\n",
    "            )\n",
    "            \n",
    "            concurrent_results[engine][concurrency] = result\n",
    "            \n",
    "            print(f\"‚úÖ {result.throughput_rps:.2f} req/s, \"\n",
    "                  f\"avg latency: {result.avg_ttft*1000:.0f}ms\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Concurrent benchmarks complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize concurrent benchmark results\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    if concurrent_results:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Plot 1: Throughput vs Concurrency\n",
    "        ax1 = axes[0]\n",
    "        for engine, results in concurrent_results.items():\n",
    "            concurrencies = sorted(results.keys())\n",
    "            throughputs = [results[c].throughput_rps for c in concurrencies]\n",
    "            ax1.plot(concurrencies, throughputs, 'o-', label=engine, linewidth=2, markersize=8)\n",
    "        \n",
    "        ax1.set_xlabel(\"Concurrency (simultaneous requests)\", fontsize=12)\n",
    "        ax1.set_ylabel(\"Throughput (requests/second)\", fontsize=12)\n",
    "        ax1.set_title(\"Throughput vs Concurrency\", fontsize=14)\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_xticks(CONCURRENCY_LEVELS)\n",
    "        \n",
    "        # Plot 2: Latency vs Concurrency\n",
    "        ax2 = axes[1]\n",
    "        for engine, results in concurrent_results.items():\n",
    "            concurrencies = sorted(results.keys())\n",
    "            latencies = [results[c].p90_latency * 1000 for c in concurrencies]  # in ms\n",
    "            ax2.plot(concurrencies, latencies, 's-', label=engine, linewidth=2, markersize=8)\n",
    "        \n",
    "        ax2.set_xlabel(\"Concurrency (simultaneous requests)\", fontsize=12)\n",
    "        ax2.set_ylabel(\"P90 Latency (ms)\", fontsize=12)\n",
    "        ax2.set_title(\"Latency vs Concurrency\", fontsize=14)\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.set_xticks(CONCURRENCY_LEVELS)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"benchmark_results.png\", dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nüìà Chart saved to benchmark_results.png\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No results to visualize\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è matplotlib not available for visualization\")\n",
    "    print(\"   Install with: pip install matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Generate Benchmark Report\n",
    "\n",
    "Let's create a comprehensive report of our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive benchmark report\n",
    "def generate_report(single_results, concurrent_results, engine_configs):\n",
    "    \"\"\"Generate a markdown benchmark report.\"\"\"\n",
    "    \n",
    "    report = []\n",
    "    report.append(\"# Inference Engine Benchmark Report\")\n",
    "    report.append(f\"\\n**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report.append(f\"**Platform:** DGX Spark (128GB Unified Memory)\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # GPU Info\n",
    "    report.append(\"## Hardware Configuration\")\n",
    "    report.append(\"\")\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"nvidia-smi\", \"--query-gpu=name,memory.total\", \"--format=csv,noheader\"],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            report.append(f\"- **GPU:** {result.stdout.strip()}\")\n",
    "    except:\n",
    "        pass\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Engines tested\n",
    "    report.append(\"## Engines Tested\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"| Engine | Model | URL |\")\n",
    "    report.append(\"|--------|-------|-----|\")\n",
    "    for engine, config in engine_configs.items():\n",
    "        if engine in single_results or engine in concurrent_results:\n",
    "            report.append(f\"| {engine} | {config['model']} | {config['base_url']} |\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Single request results\n",
    "    if single_results:\n",
    "        analysis = analyze_results(single_results)\n",
    "        \n",
    "        report.append(\"## Single Request Performance\")\n",
    "        report.append(\"\")\n",
    "        report.append(\"| Engine | Avg TTFT (ms) | P90 TTFT (ms) | Avg Speed (tok/s) | P90 Latency (ms) |\")\n",
    "        report.append(\"|--------|---------------|---------------|-------------------|------------------|\")\n",
    "        \n",
    "        for engine, stats in analysis.items():\n",
    "            report.append(\n",
    "                f\"| {engine} | {stats['ttft_avg_ms']:.1f} | {stats['ttft_p90_ms']:.1f} | \"\n",
    "                f\"{stats['speed_avg_tps']:.1f} | {stats['latency_p90_ms']:.1f} |\"\n",
    "            )\n",
    "        report.append(\"\")\n",
    "    \n",
    "    # Concurrent results\n",
    "    if concurrent_results:\n",
    "        report.append(\"## Concurrent Request Performance\")\n",
    "        report.append(\"\")\n",
    "        report.append(\"| Engine | Concurrency | Throughput (req/s) | Avg TTFT (ms) | P90 Latency (ms) |\")\n",
    "        report.append(\"|--------|-------------|-------------------|---------------|------------------|\")\n",
    "        \n",
    "        for engine, results in concurrent_results.items():\n",
    "            for conc, result in sorted(results.items()):\n",
    "                report.append(\n",
    "                    f\"| {engine} | {conc} | {result.throughput_rps:.2f} | \"\n",
    "                    f\"{result.avg_ttft*1000:.1f} | {result.p90_latency*1000:.1f} |\"\n",
    "                )\n",
    "        report.append(\"\")\n",
    "    \n",
    "    # Recommendations\n",
    "    report.append(\"## Recommendations\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"Based on the benchmark results:\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    if single_results:\n",
    "        analysis = analyze_results(single_results)\n",
    "        \n",
    "        # Find best for TTFT\n",
    "        best_ttft = min(analysis.items(), key=lambda x: x[1]['ttft_avg_ms'])\n",
    "        report.append(f\"- **Best for interactive chat (lowest TTFT):** {best_ttft[0]} \"\n",
    "                     f\"({best_ttft[1]['ttft_avg_ms']:.0f}ms average)\")\n",
    "        \n",
    "        # Find best for speed\n",
    "        best_speed = max(analysis.items(), key=lambda x: x[1]['speed_avg_tps'])\n",
    "        report.append(f\"- **Best for long responses (highest speed):** {best_speed[0]} \"\n",
    "                     f\"({best_speed[1]['speed_avg_tps']:.0f} tokens/sec)\")\n",
    "    \n",
    "    if concurrent_results:\n",
    "        # Find best throughput at highest concurrency\n",
    "        max_conc = max(CONCURRENCY_LEVELS)\n",
    "        best_throughput = None\n",
    "        best_engine = None\n",
    "        for engine, results in concurrent_results.items():\n",
    "            if max_conc in results:\n",
    "                if best_throughput is None or results[max_conc].throughput_rps > best_throughput:\n",
    "                    best_throughput = results[max_conc].throughput_rps\n",
    "                    best_engine = engine\n",
    "        \n",
    "        if best_engine:\n",
    "            report.append(f\"- **Best for high load (concurrency={max_conc}):** {best_engine} \"\n",
    "                         f\"({best_throughput:.1f} req/sec)\")\n",
    "    \n",
    "    report.append(\"\")\n",
    "    \n",
    "    return \"\\n\".join(report)\n",
    "\n",
    "# Generate and save report\n",
    "report = generate_report(all_results, concurrent_results, engine_configs)\n",
    "print(report)\n",
    "\n",
    "# Save report\n",
    "report_path = Path(\"benchmark_report.md\")\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(report)\n",
    "print(f\"\\nüìÑ Report saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Warming Up the Engine\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - First request is slow due to model loading\n",
    "result = benchmark.run_single(\"Hello\")  # Includes model load time!\n",
    "\n",
    "# ‚úÖ Right - Warm up first\n",
    "benchmark.warmup(3)  # Model is loaded and cached\n",
    "result = benchmark.run_single(\"Hello\")  # Now measures actual inference\n",
    "```\n",
    "\n",
    "**Why:** The first request often includes model loading and JIT compilation overhead. Warming up ensures you're measuring steady-state performance.\n",
    "\n",
    "### Mistake 2: Comparing Different Model Sizes\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - Not a fair comparison\n",
    "ollama_8b = InferenceBenchmark(engine=\"ollama\", model=\"llama3.1:8b\")\n",
    "vllm_70b = InferenceBenchmark(engine=\"vllm\", model=\"Llama-3.1-70B\")  # Different size!\n",
    "\n",
    "# ‚úÖ Right - Same model size\n",
    "ollama_8b = InferenceBenchmark(engine=\"ollama\", model=\"llama3.1:8b\")\n",
    "vllm_8b = InferenceBenchmark(engine=\"vllm\", model=\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "```\n",
    "\n",
    "**Why:** Larger models are inherently slower. Compare apples to apples.\n",
    "\n",
    "### Mistake 3: Ignoring Quantization Differences\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - Different quantization levels\n",
    "ollama_q4 = \"llama3.1:8b\"  # Usually Q4_K_M\n",
    "vllm_fp16 = \"Llama-3.1-8B\"  # Usually FP16\n",
    "\n",
    "# ‚úÖ Right - Document the quantization\n",
    "# Note: Ollama uses Q4 GGUF, vLLM uses FP16/BF16\n",
    "# This explains some performance differences\n",
    "```\n",
    "\n",
    "**Why:** Quantized models (Q4) are faster but less accurate than full precision (FP16).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úã Try It Yourself\n",
    "\n",
    "Now it's your turn! Complete these exercises:\n",
    "\n",
    "### Exercise 1: Test Different Prompt Lengths\n",
    "\n",
    "Create a benchmark that tests how different engines handle:\n",
    "- Very short prompts (5-10 tokens)\n",
    "- Medium prompts (50-100 tokens)\n",
    "- Long prompts (500+ tokens)\n",
    "\n",
    "Which engine handles long contexts best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Hint: Create prompts of different lengths and compare prefill times\n",
    "\n",
    "short_prompt = \"Hi!\"\n",
    "medium_prompt = \"Explain the theory of relativity and its implications for modern physics.\" * 5\n",
    "long_prompt = \"Write a detailed analysis of the following text: \" + (\"Lorem ipsum dolor sit amet. \" * 50)\n",
    "\n",
    "# TODO: Run benchmarks on each prompt length\n",
    "# TODO: Compare TTFT (prefill) performance\n",
    "# TODO: Which engine is best for long contexts?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "Focus on the `time_to_first_token` metric, which measures prefill speed. Create a table like:\n",
    "\n",
    "```python\n",
    "prompt_lengths = [\n",
    "    (\"short\", short_prompt),\n",
    "    (\"medium\", medium_prompt),\n",
    "    (\"long\", long_prompt)\n",
    "]\n",
    "\n",
    "for name, prompt in prompt_lengths:\n",
    "    result = benchmark.run_single(prompt, max_tokens=50)\n",
    "    print(f\"{name}: TTFT={result.time_to_first_token*1000:.0f}ms\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Find the Saturation Point\n",
    "\n",
    "At what concurrency level does each engine start to struggle? Find the \"knee\" in the latency curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Hint: Test concurrency levels [1, 2, 4, 8, 16, 32, 64] and look for when latency spikes\n",
    "\n",
    "# TODO: Run concurrent benchmarks at higher levels\n",
    "# TODO: Plot latency vs concurrency to find the saturation point\n",
    "# TODO: What's the optimal concurrency for each engine?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ The key metrics for LLM inference: TTFT, tokens/sec, throughput, latency\n",
    "- ‚úÖ How to benchmark Ollama, vLLM, and other inference engines\n",
    "- ‚úÖ The trade-offs between single-request latency and concurrent throughput\n",
    "- ‚úÖ How to generate comprehensive benchmark reports\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Create an Automated Benchmark Suite**\n",
    "\n",
    "Build a script that:\n",
    "1. Automatically detects which engines are running\n",
    "2. Runs a comprehensive benchmark suite\n",
    "3. Generates a HTML report with interactive charts\n",
    "4. Sends alerts if performance drops below thresholds\n",
    "\n",
    "This is useful for production monitoring!\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [vLLM Paper: Efficient Memory Management for LLM Serving](https://arxiv.org/abs/2309.06180)\n",
    "- [TensorRT-LLM Optimization Guide](https://nvidia.github.io/TensorRT-LLM/)\n",
    "- [llama.cpp Performance Tips](https://github.com/ggerganov/llama.cpp/blob/master/docs/development.md)\n",
    "- [Continuous Batching Explained](https://www.anyscale.com/blog/continuous-batching-llm-inference)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cleanup\nimport gc\n\n# Clear results if they're taking too much memory\n# all_results = None\n# concurrent_results = None\n\n# Clear Python garbage\ngc.collect()\n\n# Clear GPU memory cache if torch is available\ntry:\n    import torch\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n        print(\"‚úÖ GPU memory cache cleared!\")\nexcept ImportError:\n    pass\n\nprint(\"‚úÖ Cleanup complete!\")\nprint(f\"\\nüìä GPU Memory: {get_gpu_memory_usage():.2f} GB used\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}