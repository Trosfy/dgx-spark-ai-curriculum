{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.3.4: Medusa Speculative Decoding\n",
    "\n",
    "**Module:** 3.3 - Model Deployment & Inference Engines  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** â­â­â­â­â­\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand the draft-verify paradigm of speculative decoding\n",
    "- [ ] Know how Medusa adds prediction heads without a separate draft model\n",
    "- [ ] Measure acceptance rates and speedups on different types of prompts\n",
    "- [ ] Configure Medusa for optimal performance on DGX Spark\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Prerequisites\n",
    "\n",
    "- Completed: Labs 3.3.1-3.3.3\n",
    "- Knowledge of: Transformer architecture basics, attention mechanism\n",
    "- Having: SGLang or vLLM with Medusa support\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ Real-World Context\n",
    "\n",
    "**The Problem:** LLMs generate tokens one at a time (autoregressive). Each token requires a full forward pass through the model. This is inherently sequential and slow.\n",
    "\n",
    "**The Opportunity:** GPUs are massively parallel but underutilized during autoregressive generation. Can we do useful parallel work?\n",
    "\n",
    "**Speculative Decoding Solution:**\n",
    "1. Quickly guess multiple future tokens (speculation)\n",
    "2. Verify all guesses in parallel with the real model\n",
    "3. Accept correct guesses, regenerate incorrect ones\n",
    "\n",
    "**Real Impact:**\n",
    "- 2-3x speedup for interactive chat\n",
    "- No quality degradation (mathematically equivalent outputs)\n",
    "- Especially effective for predictable outputs (code, structured data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§’ ELI5: What is Speculative Decoding?\n",
    "\n",
    "> **Imagine you're playing a word guessing game with a very slow, very smart friend...**\n",
    ">\n",
    "> **OLD WAY:**\n",
    "> - You: \"What's the first word of your sentence?\"\n",
    "> - Friend: (thinks for 10 seconds) \"The\"\n",
    "> - You: \"What's the second word?\"\n",
    "> - Friend: (thinks for 10 seconds) \"quick\"\n",
    "> - (This takes forever!)\n",
    ">\n",
    "> **SPECULATIVE DECODING WAY:**\n",
    "> - You: \"Is your sentence 'The quick brown fox'?\"\n",
    "> - Friend: (checks all 4 words at once in 10 seconds) \"Yes to 'The quick brown', no to 'fox' - it's 'lazy'\"\n",
    "> - You got 3 words verified in the time of 1!\n",
    ">\n",
    "> **In AI terms:** A fast \"draft\" model (or heads) proposes multiple tokens. The slow \"target\" model verifies them in parallel. If the drafts are often correct, you get a big speedup!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§’ ELI5: What is Medusa?\n",
    "\n",
    "> **Traditional speculative decoding needs TWO models:**\n",
    "> - A small, fast \"draft\" model to guess tokens\n",
    "> - The real model to verify guesses\n",
    "> - Problem: You need to store and run two models!\n",
    ">\n",
    "> **Medusa is smarter:**\n",
    "> - Add extra \"prediction heads\" to the existing model\n",
    "> - Head 1 predicts the next token (normal)\n",
    "> - Head 2 predicts the token after that\n",
    "> - Head 3 predicts 2 tokens ahead\n",
    "> - And so on...\n",
    ">\n",
    "> **Like having multiple crystal balls** - each one sees a different distance into the future. All using the same model!\n",
    ">\n",
    "> **Named after Medusa** from Greek mythology who had multiple snake heads - each Medusa head predicts a different future token.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Medusa Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "# Add scripts directory to path\n",
    "scripts_path = Path(\"../scripts\").resolve()\n",
    "sys.path.insert(0, str(scripts_path))\n",
    "\n",
    "try:\n",
    "    from speculative_decoding import (\n",
    "        MedusaConfig,\n",
    "        SGLangSpeculativeClient,\n",
    "        measure_acceptance_rate,\n",
    "        get_optimal_speculation_config,\n",
    "        format_speculation_report\n",
    "    )\n",
    "    print(\"âœ… Speculative decoding utilities loaded\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Could not load utilities: {e}\")\n",
    "\n",
    "print(f\"ğŸ“ Scripts path: {scripts_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Medusa architecture\n",
    "print(\"\"\"\n",
    "ğŸ“Š MEDUSA ARCHITECTURE\n",
    "=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"\n",
    "\n",
    "Traditional Transformer (one head):\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  Hidden States â”€â”€â”€â–ºâ”‚  LM Head    â”‚â”€â”€â”€â–º Token t+1\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Medusa (multiple heads):\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                â”Œâ”€â”€â–ºâ”‚  LM Head 0  â”‚â”€â”€â”€â–º Token t+1 (original)\n",
    "                â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  Hidden    â”€â”€â”€â”€â”¼â”€â”€â–ºâ”‚ Medusa H1   â”‚â”€â”€â”€â–º Token t+2 (speculation)\n",
    "  States        â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                â”œâ”€â”€â–ºâ”‚ Medusa H2   â”‚â”€â”€â”€â–º Token t+3 (speculation)\n",
    "                â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                â””â”€â”€â–ºâ”‚ Medusa H3   â”‚â”€â”€â”€â–º Token t+4 (speculation)\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Each Medusa head is a small MLP trained to predict future tokens.\n",
    "Heads share the base model's hidden states - no extra forward passes!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the verification process\n",
    "print(\"\"\"\n",
    "ğŸ“Š MEDUSA VERIFICATION PROCESS\n",
    "=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"\n",
    "\n",
    "Step 1: SPECULATION\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Given current tokens: [\"The\", \"quick\"]\n",
    "\n",
    "Head 0 predicts: \"brown\"  (next token)\n",
    "Head 1 predicts: \"fox\"    (t+2)\n",
    "Head 2 predicts: \"jumps\"  (t+3)\n",
    "Head 3 predicts: \"over\"   (t+4)\n",
    "\n",
    "Candidate sequence: [\"brown\", \"fox\", \"jumps\", \"over\"]\n",
    "\n",
    "Step 2: TREE-STYLE VERIFICATION\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Medusa uses \"tree attention\" to verify multiple candidates:\n",
    "\n",
    "       \"brown\"\n",
    "      /       \\\n",
    "   \"fox\"     \"dog\"     (multiple candidates per position)\n",
    "   /   \\     /    \\\n",
    "\"jumps\" \"runs\"  ...   (branch for each possibility)\n",
    "\n",
    "The target model evaluates ALL branches in ONE forward pass!\n",
    "\n",
    "Step 3: ACCEPTANCE\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Compare Medusa predictions with target model:\n",
    "- \"brown\" âœ… Accepted (matches target)\n",
    "- \"fox\"   âœ… Accepted\n",
    "- \"jumps\" âŒ Rejected (target says \"leaps\")\n",
    "\n",
    "Result: Accept [\"brown\", \"fox\"], regenerate from \"leaps\"\n",
    "Speedup: Generated 2 tokens in time of ~1!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Medusa Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Medusa configuration options\n",
    "print(\"ğŸ“Š Medusa Configuration Options\\n\")\n",
    "\n",
    "configs = {\n",
    "    \"Conservative\": MedusaConfig(num_heads=3, max_speculation_length=3),\n",
    "    \"Balanced\": MedusaConfig(num_heads=4, max_speculation_length=5),\n",
    "    \"Aggressive\": MedusaConfig(num_heads=5, max_speculation_length=8),\n",
    "}\n",
    "\n",
    "print(f\"{'Config':<15} {'Heads':<8} {'Max Spec':<10} {'Best For'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, config in configs.items():\n",
    "    if name == \"Conservative\":\n",
    "        use_case = \"Unpredictable outputs (creative writing)\"\n",
    "    elif name == \"Balanced\":\n",
    "        use_case = \"General-purpose chat\"\n",
    "    else:\n",
    "        use_case = \"Predictable outputs (code, lists)\"\n",
    "    \n",
    "    print(f\"{name:<15} {config.num_heads:<8} {config.max_speculation_length:<10} {use_case}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show recommended configs for different prompt types\n",
    "print(\"\\nğŸ“Š Optimal Configuration by Prompt Type\\n\")\n",
    "\n",
    "prompt_types = [\"predictable\", \"code\", \"general\", \"creative\"]\n",
    "\n",
    "for ptype in prompt_types:\n",
    "    config = get_optimal_speculation_config(ptype)\n",
    "    print(f\"\\n{ptype.upper()}:\")\n",
    "    print(f\"  Method: {config['method']}\")\n",
    "    print(f\"  Heads: {config['num_heads']}\")\n",
    "    print(f\"  Max speculation: {config['max_speculation_length']}\")\n",
    "    print(f\"  Expected acceptance: {config['expected_acceptance_rate']:.0%}\")\n",
    "    print(f\"  Expected speedup: {config['expected_speedup']:.1f}x\")\n",
    "    print(f\"  Notes: {config['notes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Measuring Acceptance Rates\n",
    "\n",
    "The key metric for speculative decoding is the **acceptance rate** - what percentage of speculated tokens are correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load benchmark prompts\n",
    "data_path = Path(\"../data/benchmark_prompts.json\")\n",
    "\n",
    "if data_path.exists():\n",
    "    with open(data_path) as f:\n",
    "        all_prompts = json.load(f)\n",
    "    \n",
    "    # Get speculative decoding test prompts\n",
    "    spec_prompts = all_prompts.get(\"speculative_decoding_test\", [])\n",
    "    print(f\"ğŸ“ Loaded {len(spec_prompts)} speculative decoding test prompts\")\n",
    "    \n",
    "    for p in spec_prompts:\n",
    "        print(f\"   â€¢ [{p['category']}] {p['text'][:50]}...\")\n",
    "else:\n",
    "    print(\"âš ï¸ Benchmark prompts not found. Creating test prompts...\")\n",
    "    spec_prompts = [\n",
    "        {\"text\": \"Count from 1 to 10.\", \"category\": \"predictable\"},\n",
    "        {\"text\": \"Write a creative story.\", \"category\": \"creative\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test prompts by category\n",
    "PREDICTABLE_PROMPTS = [\n",
    "    \"Count from 1 to 20, one number per line.\",\n",
    "    \"List the days of the week.\",\n",
    "    \"Write the alphabet from A to Z.\",\n",
    "    \"List the months of the year.\",\n",
    "    \"Count by 5s from 5 to 50.\",\n",
    "]\n",
    "\n",
    "CODE_PROMPTS = [\n",
    "    \"Write a Python function to calculate factorial.\",\n",
    "    \"Write a Python class for a linked list node.\",\n",
    "    \"Write a Python function to check if a string is a palindrome.\",\n",
    "    \"Write a Python function to merge two sorted lists.\",\n",
    "]\n",
    "\n",
    "CREATIVE_PROMPTS = [\n",
    "    \"Write a haiku about artificial intelligence.\",\n",
    "    \"Create a short story about a robot who learns to paint.\",\n",
    "    \"Describe an alien world in vivid detail.\",\n",
    "    \"Write a poem about the ocean at sunset.\",\n",
    "]\n",
    "\n",
    "print(f\"ğŸ“ Test prompts prepared:\")\n",
    "print(f\"   Predictable: {len(PREDICTABLE_PROMPTS)}\")\n",
    "print(f\"   Code: {len(CODE_PROMPTS)}\")\n",
    "print(f\"   Creative: {len(CREATIVE_PROMPTS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if server with Medusa is available\n",
    "SGLANG_URL = \"http://localhost:30000\"\n",
    "\n",
    "def check_medusa_server(url: str) -> bool:\n",
    "    \"\"\"Check if a server with Medusa support is available.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{url}/v1/models\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"âœ… Server available at {url}\")\n",
    "            # Check for Medusa-specific endpoints if available\n",
    "            return True\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"âŒ No server at {url}\")\n",
    "        print(\"\\nğŸ“ To run Medusa with SGLang:\")\n",
    "        print(\"   python -m sglang.launch_server \\\\\")\n",
    "        print(\"       --model-path lmms-lab/llama-3.1-8b-medusa \\\\\")\n",
    "        print(\"       --port 30000 \\\\\")\n",
    "        print(\"       --speculative-algorithm medusa\")\n",
    "    return False\n",
    "\n",
    "medusa_available = check_medusa_server(SGLANG_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_acceptance_rate(\n",
    "    url: str,\n",
    "    prompts: List[str],\n",
    "    category: str,\n",
    "    max_tokens: int = 100\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Benchmark speculative decoding on a set of prompts.\n",
    "    \n",
    "    Returns acceptance rate and timing statistics.\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ§ª Benchmarking {category} prompts ({len(prompts)} total)...\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, prompt in enumerate(prompts):\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{url}/v1/chat/completions\",\n",
    "                json={\n",
    "                    \"model\": \"default\",\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"temperature\": 0.7\n",
    "                },\n",
    "                timeout=60\n",
    "            )\n",
    "            \n",
    "            elapsed = time.perf_counter() - start_time\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                usage = data.get(\"usage\", {})\n",
    "                tokens = usage.get(\"completion_tokens\", 0)\n",
    "                \n",
    "                # Get speculation stats if available\n",
    "                spec_stats = data.get(\"speculation_stats\", {})\n",
    "                accepted = spec_stats.get(\"accepted\", tokens)\n",
    "                rejected = spec_stats.get(\"rejected\", 0)\n",
    "                \n",
    "                result = {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"tokens\": tokens,\n",
    "                    \"time\": elapsed,\n",
    "                    \"tokens_per_second\": tokens / elapsed if elapsed > 0 else 0,\n",
    "                    \"accepted\": accepted,\n",
    "                    \"rejected\": rejected,\n",
    "                    \"acceptance_rate\": accepted / (accepted + rejected) if (accepted + rejected) > 0 else 1.0,\n",
    "                    \"success\": True\n",
    "                }\n",
    "                results.append(result)\n",
    "                print(f\"   [{i+1}] {tokens} tokens, {result['tokens_per_second']:.0f} tok/s\")\n",
    "            else:\n",
    "                print(f\"   [{i+1}] âŒ Error: {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   [{i+1}] âŒ Error: {e}\")\n",
    "    \n",
    "    # Compute aggregate stats\n",
    "    if results:\n",
    "        return {\n",
    "            \"category\": category,\n",
    "            \"num_prompts\": len(prompts),\n",
    "            \"successful\": len(results),\n",
    "            \"avg_tokens_per_second\": np.mean([r[\"tokens_per_second\"] for r in results]),\n",
    "            \"avg_acceptance_rate\": np.mean([r[\"acceptance_rate\"] for r in results]),\n",
    "            \"total_tokens\": sum(r[\"tokens\"] for r in results),\n",
    "            \"total_time\": sum(r[\"time\"] for r in results),\n",
    "        }\n",
    "    else:\n",
    "        return {\"category\": category, \"error\": \"No successful requests\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmarks if server is available\n",
    "if medusa_available:\n",
    "    all_category_results = {}\n",
    "    \n",
    "    # Benchmark each category\n",
    "    categories = [\n",
    "        (\"predictable\", PREDICTABLE_PROMPTS),\n",
    "        (\"code\", CODE_PROMPTS),\n",
    "        (\"creative\", CREATIVE_PROMPTS),\n",
    "    ]\n",
    "    \n",
    "    for category, prompts in categories:\n",
    "        result = benchmark_acceptance_rate(\n",
    "            url=SGLANG_URL,\n",
    "            prompts=prompts,\n",
    "            category=category,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        all_category_results[category] = result\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“Š ACCEPTANCE RATE BY CATEGORY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n{'Category':<15} {'Acceptance':<12} {'Speed (tok/s)':<15} {'Expected'}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    expected_rates = {\"predictable\": 0.8, \"code\": 0.65, \"creative\": 0.35}\n",
    "    \n",
    "    for category, result in all_category_results.items():\n",
    "        if \"error\" not in result:\n",
    "            expected = expected_rates.get(category, 0.5)\n",
    "            print(f\"{category:<15} {result['avg_acceptance_rate']:.1%}{'':>5} \"\n",
    "                  f\"{result['avg_tokens_per_second']:.0f}{'':>10} ~{expected:.0%}\")\n",
    "        else:\n",
    "            print(f\"{category:<15} Error\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Server not available.\")\n",
    "    print(\"\\nğŸ“Š Expected results with Medusa:\")\n",
    "    print(\"   Predictable prompts: ~80% acceptance, 2.5x speedup\")\n",
    "    print(\"   Code prompts: ~65% acceptance, 2.0x speedup\")\n",
    "    print(\"   Creative prompts: ~35% acceptance, 1.3x speedup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” What Just Happened?\n",
    "\n",
    "We measured acceptance rates across different prompt types:\n",
    "\n",
    "1. **Predictable prompts** (counting, lists) â†’ **High acceptance (~80%)**\n",
    "   - Output is largely deterministic\n",
    "   - Medusa heads can easily predict next tokens\n",
    "   - Best speedup (2-3x)\n",
    "\n",
    "2. **Code prompts** â†’ **Medium acceptance (~65%)**\n",
    "   - Syntax is predictable (brackets, keywords)\n",
    "   - Logic is less predictable\n",
    "   - Good speedup (1.5-2x)\n",
    "\n",
    "3. **Creative prompts** â†’ **Lower acceptance (~35%)**\n",
    "   - Output is intentionally unpredictable\n",
    "   - Many valid next tokens possible\n",
    "   - Modest speedup (1.1-1.4x)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Speedup Analysis\n",
    "\n",
    "Let's understand the relationship between acceptance rate and speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_theoretical_speedup(\n",
    "    acceptance_rate: float,\n",
    "    num_heads: int,\n",
    "    overhead: float = 0.1  # Overhead from tree verification\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate theoretical speedup from speculative decoding.\n",
    "    \n",
    "    Speedup â‰ˆ 1 / (1 - acceptance_rate * (num_heads - 1) / num_heads) - overhead\n",
    "    \n",
    "    Simplified: Expected tokens accepted per step / cost per step\n",
    "    \"\"\"\n",
    "    # Expected tokens per speculation round\n",
    "    expected_accepted = 1 + acceptance_rate * (num_heads - 1)\n",
    "    \n",
    "    # Cost is approximately 1 + small overhead for verification\n",
    "    cost = 1 + overhead\n",
    "    \n",
    "    return expected_accepted / cost\n",
    "\n",
    "\n",
    "# Visualize speedup vs acceptance rate\n",
    "print(\"ğŸ“Š Theoretical Speedup vs Acceptance Rate\\n\")\n",
    "print(f\"{'Acceptance':<12} {'3 Heads':<10} {'4 Heads':<10} {'5 Heads':<10}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for acc_rate in [0.2, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    speedups = [\n",
    "        calculate_theoretical_speedup(acc_rate, 3),\n",
    "        calculate_theoretical_speedup(acc_rate, 4),\n",
    "        calculate_theoretical_speedup(acc_rate, 5),\n",
    "    ]\n",
    "    print(f\"{acc_rate:.0%}{'':>6} {speedups[0]:.2f}x{'':>5} {speedups[1]:.2f}x{'':>5} {speedups[2]:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    acceptance_rates = np.linspace(0.1, 0.95, 50)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    for num_heads in [3, 4, 5]:\n",
    "        speedups = [calculate_theoretical_speedup(a, num_heads) for a in acceptance_rates]\n",
    "        ax.plot(acceptance_rates * 100, speedups, label=f'{num_heads} Medusa heads', linewidth=2)\n",
    "    \n",
    "    # Mark typical acceptance rates\n",
    "    typical_points = {\n",
    "        \"Predictable\": 0.80,\n",
    "        \"Code\": 0.65,\n",
    "        \"General\": 0.50,\n",
    "        \"Creative\": 0.35\n",
    "    }\n",
    "    \n",
    "    for label, rate in typical_points.items():\n",
    "        speedup = calculate_theoretical_speedup(rate, 4)\n",
    "        ax.scatter([rate * 100], [speedup], s=100, zorder=5)\n",
    "        ax.annotate(label, (rate * 100, speedup), \n",
    "                   xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    ax.set_xlabel('Acceptance Rate (%)', fontsize=12)\n",
    "    ax.set_ylabel('Speedup Factor (x)', fontsize=12)\n",
    "    ax.set_title('Medusa Speedup vs Acceptance Rate', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=1.0, color='r', linestyle='--', alpha=0.5, label='No speedup')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('medusa_speedup.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ Chart saved to medusa_speedup.png\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"âš ï¸ matplotlib not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš ï¸ Common Mistakes\n",
    "\n",
    "### Mistake 1: Too Many Medusa Heads\n",
    "\n",
    "```python\n",
    "# âŒ Wrong - Too many heads with low acceptance rate\n",
    "config = MedusaConfig(num_heads=8, max_speculation_length=10)\n",
    "# On creative prompts, most speculations are wrong â†’ wasted compute\n",
    "\n",
    "# âœ… Right - Match heads to prompt type\n",
    "config = MedusaConfig(num_heads=3, max_speculation_length=4)  # For creative\n",
    "config = MedusaConfig(num_heads=5, max_speculation_length=8)  # For predictable\n",
    "```\n",
    "\n",
    "### Mistake 2: Expecting Speedup on All Prompts\n",
    "\n",
    "```python\n",
    "# âŒ Wrong - Same config for everything\n",
    "# Creative writing with aggressive speculation = minimal speedup\n",
    "\n",
    "# âœ… Right - Adaptive configuration\n",
    "if prompt_is_structured(prompt):\n",
    "    use_aggressive_speculation()\n",
    "else:\n",
    "    use_conservative_speculation()  # Or disable speculation\n",
    "```\n",
    "\n",
    "### Mistake 3: Not Accounting for Temperature\n",
    "\n",
    "```python\n",
    "# âŒ Wrong - High temperature = unpredictable output\n",
    "temperature = 1.5  # Very random â†’ low acceptance rate\n",
    "\n",
    "# âœ… Right - Lower temperature improves acceptance\n",
    "temperature = 0.7  # Balanced\n",
    "temperature = 0.0  # Greedy (highest acceptance, but less diverse)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ‹ Try It Yourself\n",
    "\n",
    "### Exercise 1: Find the Optimal Number of Heads\n",
    "\n",
    "For a specific prompt type, find the optimal number of Medusa heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Test different numbers of heads (if you can configure your server)\n",
    "# and measure actual speedup vs theoretical\n",
    "\n",
    "# Heads to test: [2, 3, 4, 5, 6]\n",
    "# For each, measure:\n",
    "# - Actual tokens/second\n",
    "# - Acceptance rate\n",
    "# - Compare to baseline (1 head)\n",
    "\n",
    "# TODO: Implement the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Analyze Temperature Impact\n",
    "\n",
    "Test how temperature affects acceptance rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Test temperatures: [0.0, 0.3, 0.5, 0.7, 1.0, 1.5]\n",
    "# For each, measure acceptance rate on the same prompts\n",
    "# Plot temperature vs acceptance rate\n",
    "\n",
    "# TODO: Implement the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- âœ… How speculative decoding uses draft-verify to speed up generation\n",
    "- âœ… How Medusa adds prediction heads without needing a separate model\n",
    "- âœ… The relationship between acceptance rate and speedup\n",
    "- âœ… How to configure Medusa for different prompt types\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Challenge (Optional)\n",
    "\n",
    "**Build an Adaptive Speculation System**\n",
    "\n",
    "Create a system that:\n",
    "1. Classifies incoming prompts (predictable vs creative)\n",
    "2. Dynamically adjusts speculation depth\n",
    "3. Monitors acceptance rate in real-time\n",
    "4. Falls back to normal decoding when acceptance is too low\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– Further Reading\n",
    "\n",
    "- [Medusa Paper: Simple Framework for Fast LLM Inference](https://arxiv.org/abs/2401.10774)\n",
    "- [Speculative Decoding Survey](https://arxiv.org/abs/2401.07851)\n",
    "- [Medusa GitHub Repository](https://github.com/FasterDecoding/Medusa)\n",
    "- [SGLang Speculative Decoding Docs](https://sgl-project.github.io/)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
