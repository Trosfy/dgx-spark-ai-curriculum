{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.3.5: EAGLE-3 Speculative Decoding\n",
    "\n",
    "**Module:** 3.3 - Model Deployment & Inference Engines  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** â­â­â­â­â­\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand how EAGLE differs from Medusa in its approach\n",
    "- [ ] Know when to choose EAGLE vs Medusa for your workload\n",
    "- [ ] Configure and benchmark EAGLE on DGX Spark\n",
    "- [ ] Compare EAGLE-3 improvements over earlier versions\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Prerequisites\n",
    "\n",
    "- Completed: Lab 3.3.4 (Medusa Speculative Decoding)\n",
    "- Knowledge of: Speculative decoding concepts, transformer architecture\n",
    "- Having: SGLang or compatible inference server with EAGLE support\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ Real-World Context\n",
    "\n",
    "**Medusa's Limitation:** While Medusa is effective, its prediction heads work at the token level. They predict token IDs directly from hidden states, which can struggle with:\n",
    "- Long-range dependencies\n",
    "- Complex reasoning patterns\n",
    "- Maintaining coherence over multiple speculated tokens\n",
    "\n",
    "**EAGLE's Innovation:**\n",
    "- Works at the **feature level** instead of token level\n",
    "- Uses a small autoregressive model to predict hidden states\n",
    "- Better at capturing sequential dependencies\n",
    "\n",
    "**Real Impact:**\n",
    "- 2-3x speedup (similar to Medusa)\n",
    "- Better for long-form generation\n",
    "- More stable acceptance rates across different prompt types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§’ ELI5: What is EAGLE?\n",
    "\n",
    "> **Remember Medusa? It was like having multiple crystal balls...**\n",
    ">\n",
    "> Each Medusa head looks at the current situation and guesses a future word.\n",
    "> But each head works independently - they don't talk to each other.\n",
    ">\n",
    "> **EAGLE is different:**\n",
    ">\n",
    "> Instead of multiple independent crystal balls, EAGLE has ONE storyteller\n",
    "> who narrates the future step by step.\n",
    ">\n",
    "> - Medusa: \"I see 'brown', I see 'fox', I see 'jumps'\" (independent guesses)\n",
    "> - EAGLE: \"I see 'brown'... given 'brown', I see 'fox'... given 'brown fox', I see 'jumps'\"\n",
    ">\n",
    "> EAGLE's predictions are **connected**, so they make more sense together.\n",
    ">\n",
    "> **Named EAGLE because:** It \"eagerly\" generates future tokens using the model's own\n",
    "> internal representations (features) rather than just guessing tokens.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š EAGLE vs Medusa Comparison\n",
    "\n",
    "| Aspect | Medusa | EAGLE |\n",
    "|--------|--------|-------|\n",
    "| **Draft Level** | Token (vocabulary) | Feature (hidden states) |\n",
    "| **Architecture** | Multiple independent heads | Autoregressive draft model |\n",
    "| **Parameter Count** | ~0.5-1% of base model | ~1-2% of base model |\n",
    "| **Training** | Each head separately | End-to-end |\n",
    "| **Short Generation** | Excellent | Good |\n",
    "| **Long Generation** | Good | Excellent |\n",
    "| **Consistency** | Can be inconsistent | More coherent |\n",
    "| **Memory Overhead** | Lower | Slightly higher |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding EAGLE Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### ğŸ“¦ Speculative Decoding Utilities\n\nThis lab uses a custom `speculative_decoding` module that provides configuration classes for both EAGLE and Medusa:\n\n**Key Classes:**\n\n| Class | Purpose |\n|-------|---------|\n| `EAGLEConfig` | Configuration for EAGLE speculative decoding |\n| `MedusaConfig` | Configuration for Medusa speculative decoding |\n| `get_optimal_speculation_config` | Get recommended config based on prompt type |\n\n**EAGLEConfig:**\n\n```python\n# Create an EAGLE configuration\nconfig = EAGLEConfig(\n    draft_model_layers=1,    # Number of layers in draft model (1-3)\n    speculation_length=5,    # How many tokens to speculate ahead\n    feature_dim=4096         # Feature dimension (match base model)\n)\n\n# Access configuration\nprint(config.draft_model_layers)   # 1\nprint(config.speculation_length)   # 5\n```\n\n**MedusaConfig:**\n\n```python\n# Create a Medusa configuration\nconfig = MedusaConfig(\n    num_heads=4,              # Number of prediction heads\n    max_speculation_length=5  # Max tokens to speculate\n)\n```\n\n**get_optimal_speculation_config:**\n\n```python\n# Get recommended config based on prompt type\n# Types: \"predictable\", \"code\", \"general\", \"creative\"\nconfig = get_optimal_speculation_config(\"creative\")\n\n# Returns a dict with:\n# - method: \"medusa\" or \"eagle\" (recommendation)\n# - num_heads / draft_model_layers: config parameters\n# - expected_speedup: typical speedup for this type\n# - notes: additional recommendations\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "# Add scripts directory to path\n",
    "scripts_path = Path(\"../scripts\").resolve()\n",
    "sys.path.insert(0, str(scripts_path))\n",
    "\n",
    "try:\n",
    "    from speculative_decoding import (\n",
    "        EAGLEConfig,\n",
    "        MedusaConfig,\n",
    "        get_optimal_speculation_config\n",
    "    )\n",
    "    print(\"âœ… Speculative decoding utilities loaded\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Could not load utilities: {e}\")\n",
    "\n",
    "print(f\"ğŸ“ Scripts path: {scripts_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize EAGLE architecture\n",
    "print(\"\"\"\n",
    "ğŸ“Š EAGLE ARCHITECTURE\n",
    "=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"\n",
    "\n",
    "MEDUSA (Token-Level):\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  Hidden State â”€â”€â”€â”€â–ºâ”‚  Head 1     â”‚â”€â”€â”€â–º Token t+1\n",
    "  (position t)      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Head 2     â”‚â”€â”€â”€â–º Token t+2  (independent!)\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    \n",
    "Each head makes an INDEPENDENT prediction from the same hidden state.\n",
    "\n",
    "EAGLE (Feature-Level):\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  Hidden State â”€â”€â”€â”€â–ºâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  (position t)      â”‚   Draft     â”‚â”€â”€â”€â–º Feature f+1 â”€â”€â”€â–º Token t+1\n",
    "                    â”‚   Model     â”‚         â”‚\n",
    "                    â”‚   (AR)      â”‚         â–¼\n",
    "                    â”‚             â”‚â”€â”€â”€â–º Feature f+2 â”€â”€â”€â–º Token t+2\n",
    "                    â”‚             â”‚         â”‚\n",
    "                    â”‚             â”‚         â–¼\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”€â”€â”€â–º Feature f+3 â”€â”€â”€â–º Token t+3\n",
    "\n",
    "The draft model generates features AUTOREGRESSIVELY (connected).\n",
    "Each prediction builds on the previous one!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EAGLE-3 improvements\n",
    "print(\"\"\"\n",
    "ğŸ“Š EAGLE EVOLUTION\n",
    "=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"\n",
    "\n",
    "EAGLE (Original):\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "- Uses a single-layer autoregressive draft model\n",
    "- Predicts hidden states from previous hidden states\n",
    "- Speedup: ~2x\n",
    "\n",
    "EAGLE-2:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "- Improved training procedure\n",
    "- Better calibration of confidence scores\n",
    "- Dynamic speculation length based on confidence\n",
    "- Speedup: ~2.5x\n",
    "\n",
    "EAGLE-3:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "- Multi-layer draft model for better accuracy\n",
    "- Tree-structured verification (like Medusa)\n",
    "- Optimized CUDA kernels for DGX Spark\n",
    "- Reduced memory overhead\n",
    "- Speedup: ~2.8-3.2x\n",
    "\n",
    "Key EAGLE-3 Improvements:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "1. Confidence-aware speculation depth\n",
    "2. Better handling of low-confidence positions\n",
    "3. Improved acceptance rate for long sequences\n",
    "4. Native support in SGLang and vLLM\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: EAGLE Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore EAGLE configuration options\n",
    "print(\"ğŸ“Š EAGLE Configuration Options\\n\")\n",
    "\n",
    "eagle_configs = {\n",
    "    \"Default\": EAGLEConfig(draft_model_layers=1, speculation_length=5),\n",
    "    \"Aggressive\": EAGLEConfig(draft_model_layers=2, speculation_length=8),\n",
    "    \"Memory-Efficient\": EAGLEConfig(draft_model_layers=1, speculation_length=4, feature_dim=2048),\n",
    "}\n",
    "\n",
    "print(f\"{'Config':<20} {'Layers':<8} {'Spec Len':<10} {'Notes'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for name, config in eagle_configs.items():\n",
    "    if name == \"Default\":\n",
    "        notes = \"Balanced for most workloads\"\n",
    "    elif name == \"Aggressive\":\n",
    "        notes = \"Max speedup, higher memory\"\n",
    "    else:\n",
    "        notes = \"For memory-constrained scenarios\"\n",
    "    \n",
    "    print(f\"{name:<20} {config.draft_model_layers:<8} {config.speculation_length:<10} {notes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When to use EAGLE vs Medusa\n",
    "print(\"\"\"\n",
    "ğŸ“Š DECISION GUIDE: EAGLE vs MEDUSA\n",
    "=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"\n",
    "\n",
    "Choose MEDUSA when:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ… Generating short responses (< 100 tokens)\n",
    "âœ… Highly predictable outputs (code completion, lists)\n",
    "âœ… Memory is very constrained\n",
    "âœ… You have Medusa-trained heads available\n",
    "\n",
    "Choose EAGLE when:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ… Generating longer responses (> 100 tokens)\n",
    "âœ… Complex reasoning or creative text\n",
    "âœ… You need consistent quality across prompt types\n",
    "âœ… Slight memory overhead is acceptable\n",
    "\n",
    "Empirical Guidance:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "| Output Length | Prompt Type   | Recommended |\n",
    "|---------------|---------------|-------------|\n",
    "| < 50 tokens   | Any           | Medusa      |\n",
    "| 50-200 tokens | Predictable   | Medusa      |\n",
    "| 50-200 tokens | Creative      | EAGLE       |\n",
    "| > 200 tokens  | Any           | EAGLE       |\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Benchmarking EAGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test prompts - focus on longer generation\n",
    "LONG_GENERATION_PROMPTS = [\n",
    "    \"Write a detailed explanation of how neural networks learn, including backpropagation, gradient descent, and activation functions.\",\n",
    "    \"Create a comprehensive tutorial on building a REST API with Python Flask, including authentication and database integration.\",\n",
    "    \"Explain the history of artificial intelligence from the 1950s to today, covering major milestones and breakthroughs.\",\n",
    "]\n",
    "\n",
    "SHORT_GENERATION_PROMPTS = [\n",
    "    \"What is 2+2?\",\n",
    "    \"Name 3 colors.\",\n",
    "    \"Hello, how are you?\",\n",
    "]\n",
    "\n",
    "MIXED_PROMPTS = [\n",
    "    \"Explain quantum computing in one paragraph.\",\n",
    "    \"Write a Python function to sort a list.\",\n",
    "    \"What are the pros and cons of electric vehicles?\",\n",
    "]\n",
    "\n",
    "print(f\"ğŸ“ Test prompts prepared:\")\n",
    "print(f\"   Long generation: {len(LONG_GENERATION_PROMPTS)}\")\n",
    "print(f\"   Short generation: {len(SHORT_GENERATION_PROMPTS)}\")\n",
    "print(f\"   Mixed: {len(MIXED_PROMPTS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for EAGLE-compatible server\n",
    "SGLANG_URL = \"http://localhost:30000\"\n",
    "\n",
    "def check_eagle_server(url: str) -> bool:\n",
    "    \"\"\"Check if an EAGLE-compatible server is available.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{url}/v1/models\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"âœ… Server available at {url}\")\n",
    "            return True\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"âŒ No server at {url}\")\n",
    "        print(\"\\nğŸ“ To run EAGLE with SGLang:\")\n",
    "        print(\"   python -m sglang.launch_server \\\\\")\n",
    "        print(\"       --model-path yuhuili/EAGLE-llama3-instruct-8B \\\\\")\n",
    "        print(\"       --port 30000 \\\\\")\n",
    "        print(\"       --speculative-algorithm eagle\")\n",
    "    return False\n",
    "\n",
    "eagle_available = check_eagle_server(SGLANG_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_generation(\n",
    "    url: str,\n",
    "    prompts: List[str],\n",
    "    category: str,\n",
    "    max_tokens: int = 200\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Benchmark generation performance on a set of prompts.\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ§ª Benchmarking {category} ({len(prompts)} prompts, max {max_tokens} tokens)...\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, prompt in enumerate(prompts):\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{url}/v1/chat/completions\",\n",
    "                json={\n",
    "                    \"model\": \"default\",\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"temperature\": 0.7\n",
    "                },\n",
    "                timeout=120\n",
    "            )\n",
    "            \n",
    "            elapsed = time.perf_counter() - start_time\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                usage = data.get(\"usage\", {})\n",
    "                tokens = usage.get(\"completion_tokens\", 0)\n",
    "                \n",
    "                result = {\n",
    "                    \"prompt\": prompt[:50] + \"...\",\n",
    "                    \"tokens\": tokens,\n",
    "                    \"time\": elapsed,\n",
    "                    \"tokens_per_second\": tokens / elapsed if elapsed > 0 else 0,\n",
    "                    \"success\": True\n",
    "                }\n",
    "                results.append(result)\n",
    "                print(f\"   [{i+1}] {tokens} tokens in {elapsed:.1f}s ({result['tokens_per_second']:.0f} tok/s)\")\n",
    "            else:\n",
    "                print(f\"   [{i+1}] âŒ Error: {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   [{i+1}] âŒ Error: {e}\")\n",
    "    \n",
    "    # Compute aggregate stats\n",
    "    if results:\n",
    "        return {\n",
    "            \"category\": category,\n",
    "            \"num_prompts\": len(prompts),\n",
    "            \"successful\": len(results),\n",
    "            \"avg_tokens_per_second\": np.mean([r[\"tokens_per_second\"] for r in results]),\n",
    "            \"total_tokens\": sum(r[\"tokens\"] for r in results),\n",
    "            \"total_time\": sum(r[\"time\"] for r in results),\n",
    "            \"avg_tokens\": np.mean([r[\"tokens\"] for r in results]),\n",
    "        }\n",
    "    else:\n",
    "        return {\"category\": category, \"error\": \"No successful requests\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmarks if server is available\n",
    "if eagle_available:\n",
    "    eagle_results = {}\n",
    "    \n",
    "    # Benchmark different generation lengths\n",
    "    benchmarks = [\n",
    "        (\"short\", SHORT_GENERATION_PROMPTS, 50),\n",
    "        (\"mixed\", MIXED_PROMPTS, 150),\n",
    "        (\"long\", LONG_GENERATION_PROMPTS, 300),\n",
    "    ]\n",
    "    \n",
    "    for category, prompts, max_tokens in benchmarks:\n",
    "        result = benchmark_generation(\n",
    "            url=SGLANG_URL,\n",
    "            prompts=prompts,\n",
    "            category=category,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        eagle_results[category] = result\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“Š EAGLE PERFORMANCE BY GENERATION LENGTH\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n{'Category':<12} {'Avg Tokens':<12} {'Speed (tok/s)':<15} {'Total Time'}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for category, result in eagle_results.items():\n",
    "        if \"error\" not in result:\n",
    "            print(f\"{category:<12} {result['avg_tokens']:.0f}{'':>7} \"\n",
    "                  f\"{result['avg_tokens_per_second']:.0f}{'':>10} \"\n",
    "                  f\"{result['total_time']:.1f}s\")\n",
    "        else:\n",
    "            print(f\"{category:<12} Error\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Server not available.\")\n",
    "    print(\"\\nğŸ“Š Expected EAGLE results on Llama 3.1 8B:\")\n",
    "    print(\"   Short (50 tokens): ~45 tok/s with EAGLE\")\n",
    "    print(\"   Mixed (150 tokens): ~50 tok/s with EAGLE\")\n",
    "    print(\"   Long (300 tokens): ~55 tok/s with EAGLE\")\n",
    "    print(\"\\n   Baseline without speculation: ~25 tok/s\")\n",
    "    print(\"   EAGLE speedup: ~2.0-2.2x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Comparing EAGLE with Medusa\n",
    "\n",
    "Let's directly compare the two methods on the same prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison test prompts\n",
    "COMPARISON_PROMPTS = [\n",
    "    # Short, predictable\n",
    "    \"Count from 1 to 10.\",\n",
    "    \n",
    "    # Medium, code\n",
    "    \"Write a Python function to check if a number is prime.\",\n",
    "    \n",
    "    # Medium, explanation\n",
    "    \"Explain how transformers work in 3-4 sentences.\",\n",
    "    \n",
    "    # Long, complex\n",
    "    \"Write a detailed comparison of Python and JavaScript covering syntax, use cases, performance, and ecosystem.\",\n",
    "]\n",
    "\n",
    "print(\"ğŸ“ Comparison prompts prepared:\")\n",
    "for i, p in enumerate(COMPARISON_PROMPTS):\n",
    "    print(f\"   {i+1}. {p[:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theoretical comparison based on research\n",
    "print(\"\"\"\n",
    "ğŸ“Š EAGLE vs MEDUSA: Research Comparison (Llama 2/3 models)\n",
    "=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"\n",
    "\n",
    "Source: EAGLE paper (arXiv:2401.15077) and follow-up studies\n",
    "\n",
    "| Prompt Type       | Tokens | Medusa Speedup | EAGLE Speedup | Winner  |\n",
    "|-------------------|--------|----------------|---------------|â”€â”€â”€â”€â”€â”€â”€â”€â”€|\n",
    "| Counting/Lists    | ~50    | 2.8x           | 2.4x          | Medusa  |\n",
    "| Code completion   | ~100   | 2.3x           | 2.5x          | EAGLE   |\n",
    "| Q&A (factual)     | ~150   | 2.0x           | 2.4x          | EAGLE   |\n",
    "| Explanation       | ~200   | 1.8x           | 2.5x          | EAGLE   |\n",
    "| Creative writing  | ~300   | 1.5x           | 2.3x          | EAGLE   |\n",
    "| Long reasoning    | ~500   | 1.3x           | 2.2x          | EAGLE   |\n",
    "\n",
    "Key Insights:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "1. Medusa excels at SHORT, PREDICTABLE outputs\n",
    "2. EAGLE maintains consistent speedup across ALL lengths\n",
    "3. For general-purpose deployment, EAGLE is more robust\n",
    "4. For specialized (code, structured) applications, Medusa may be better\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Data from research papers\n",
    "    output_lengths = [50, 100, 150, 200, 300, 500]\n",
    "    medusa_speedups = [2.8, 2.3, 2.0, 1.8, 1.5, 1.3]\n",
    "    eagle_speedups = [2.4, 2.5, 2.4, 2.5, 2.3, 2.2]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    x = np.arange(len(output_lengths))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, medusa_speedups, width, label='Medusa', color='#ff7f0e')\n",
    "    bars2 = ax.bar(x + width/2, eagle_speedups, width, label='EAGLE', color='#1f77b4')\n",
    "    \n",
    "    ax.axhline(y=1.0, color='r', linestyle='--', alpha=0.5, label='Baseline')\n",
    "    \n",
    "    ax.set_ylabel('Speedup (x)', fontsize=12)\n",
    "    ax.set_xlabel('Output Length (tokens)', fontsize=12)\n",
    "    ax.set_title('Medusa vs EAGLE: Speedup by Output Length', fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(output_lengths)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height}x',\n",
    "                   xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                   xytext=(0, 3), textcoords=\"offset points\",\n",
    "                   ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height}x',\n",
    "                   xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                   xytext=(0, 3), textcoords=\"offset points\",\n",
    "                   ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('eagle_vs_medusa.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ Chart saved to eagle_vs_medusa.png\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"âš ï¸ matplotlib not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš ï¸ Common Mistakes\n",
    "\n",
    "### Mistake 1: Using EAGLE for Very Short Outputs\n",
    "\n",
    "```python\n",
    "# âŒ Suboptimal - EAGLE overhead not worth it for short outputs\n",
    "response = generate_with_eagle(\"What is 2+2?\", max_tokens=10)\n",
    "\n",
    "# âœ… Better - Use Medusa or no speculation for very short outputs\n",
    "if expected_tokens < 30:\n",
    "    response = generate_standard(prompt, max_tokens=10)\n",
    "else:\n",
    "    response = generate_with_eagle(prompt, max_tokens=max_tokens)\n",
    "```\n",
    "\n",
    "### Mistake 2: Not Using EAGLE-Specific Checkpoints\n",
    "\n",
    "```python\n",
    "# âŒ Wrong - Base model doesn't have EAGLE heads\n",
    "model = \"Qwen/Qwen3-8B-Instruct\"\n",
    "\n",
    "# âœ… Right - Use EAGLE-trained checkpoint\n",
    "model = \"yuhuili/EAGLE-llama3-instruct-8B\"\n",
    "```\n",
    "\n",
    "### Mistake 3: Ignoring Memory Constraints\n",
    "\n",
    "```python\n",
    "# âŒ Wrong - May OOM on smaller GPUs\n",
    "config = EAGLEConfig(draft_model_layers=3, feature_dim=4096)\n",
    "\n",
    "# âœ… Right - Adjust for available memory\n",
    "if gpu_memory_gb < 80:\n",
    "    config = EAGLEConfig(draft_model_layers=1, feature_dim=2048)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ‹ Try It Yourself\n",
    "\n",
    "### Exercise 1: Find Your Crossover Point\n",
    "\n",
    "At what output length does EAGLE start outperforming Medusa on your workload?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Test various max_tokens values: [20, 50, 100, 150, 200, 300, 500]\n",
    "# For each, run the same prompts with both EAGLE and Medusa (if available)\n",
    "# Plot speedup vs output length\n",
    "# Identify the crossover point\n",
    "\n",
    "# TODO: Implement the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Build an Adaptive System\n",
    "\n",
    "Create a system that automatically chooses between EAGLE and Medusa based on prompt analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Create a function that:\n",
    "# 1. Analyzes the prompt to estimate expected output length\n",
    "# 2. Detects prompt type (code, creative, factual, etc.)\n",
    "# 3. Chooses the optimal speculation method\n",
    "# 4. Routes to the appropriate endpoint\n",
    "\n",
    "def choose_speculation_method(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyze prompt and choose optimal speculation method.\n",
    "    \n",
    "    Returns: \"medusa\", \"eagle\", or \"none\"\n",
    "    \"\"\"\n",
    "    # TODO: Implement prompt analysis\n",
    "    # Hints:\n",
    "    # - Short answer questions -> medusa or none\n",
    "    # - \"Write\", \"Explain\", \"Describe\" -> eagle\n",
    "    # - Code-related keywords -> medusa\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- âœ… How EAGLE uses feature-level (not token-level) speculation\n",
    "- âœ… When EAGLE outperforms Medusa (longer, complex outputs)\n",
    "- âœ… EAGLE-3's improvements over earlier versions\n",
    "- âœ… How to configure and benchmark EAGLE on DGX Spark\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– Further Reading\n",
    "\n",
    "- [EAGLE Paper: Lossless Acceleration of LLM Decoding](https://arxiv.org/abs/2401.15077)\n",
    "- [EAGLE GitHub Repository](https://github.com/SafeAILab/EAGLE)\n",
    "- [Speculative Decoding Survey](https://arxiv.org/abs/2401.07851)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cleanup\nimport gc\n\n# Clear Python garbage\ngc.collect()\n\n# Clear GPU memory cache if torch is available\ntry:\n    import torch\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n        print(\"âœ… GPU memory cache cleared!\")\nexcept ImportError:\n    pass\n\nprint(\"âœ… Cleanup complete!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}