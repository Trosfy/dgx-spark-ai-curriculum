{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.3.7: Production API with FastAPI\n",
    "\n",
    "**Module:** 3.3 - Model Deployment & Inference Engines  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Build a production-ready LLM API with FastAPI\n",
    "- [ ] Implement Server-Sent Events (SSE) for streaming responses\n",
    "- [ ] Add health checks, rate limiting, and monitoring\n",
    "- [ ] Deploy with proper error handling and logging\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Labs 3.3.1-3.3.6\n",
    "- Knowledge of: Python async programming, REST APIs, HTTP\n",
    "- Having: At least one inference engine running (Ollama, vLLM, SGLang)\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The Problem:** You have an inference engine running, but:\n",
    "- It's not exposed securely to the internet\n",
    "- No rate limiting = one user can DoS your system\n",
    "- No monitoring = you don't know when it's failing\n",
    "- No authentication = anyone can use your expensive GPU\n",
    "\n",
    "**The Solution:** A production API layer that:\n",
    "- Handles authentication and rate limiting\n",
    "- Provides streaming for real-time responses\n",
    "- Monitors health and performance\n",
    "- Gracefully handles errors\n",
    "\n",
    "**Real Impact:**\n",
    "- OpenAI, Anthropic, and all major LLM providers use similar patterns\n",
    "- This is exactly how production LLM APIs are built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What is a Production API?\n",
    "\n",
    "> **Imagine you built an amazing lemonade stand...**\n",
    ">\n",
    "> Your lemonade (LLM) is great! But right now:\n",
    "> - Anyone can walk up and take infinite lemonade (no rate limiting)\n",
    "> - You don't know how much lemonade you've served (no monitoring)\n",
    "> - If you run out of lemons, you just stare blankly (no error handling)\n",
    "> - Anyone can claim to be a paying customer (no authentication)\n",
    ">\n",
    "> A **production API** is like hiring a professional manager:\n",
    "> - They check customers' membership cards (authentication)\n",
    "> - They limit each customer to 5 cups per hour (rate limiting)\n",
    "> - They track sales and inventory (monitoring)\n",
    "> - They politely explain when you're out of lemons (error handling)\n",
    "> - They pour lemonade into cups as it's ready (streaming)\n",
    ">\n",
    "> **In AI terms:** FastAPI helps us build a professional \"manager\" that sits between\n",
    "> users and our inference engine, handling all the production concerns.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting Up FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install fastapi uvicorn sse-starlette python-multipart aiohttp\n",
    "\n",
    "# Standard imports\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, AsyncGenerator\n",
    "from dataclasses import dataclass, field\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import aiohttp\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production API architecture\n",
    "print(\"\"\"\n",
    "üìä PRODUCTION LLM API ARCHITECTURE\n",
    "=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                         CLIENTS                                  ‚îÇ\n",
    "‚îÇ    (Web Apps, Mobile Apps, CLI Tools, Other Services)           ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                           ‚îÇ\n",
    "                           ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    LOAD BALANCER (Optional)                      ‚îÇ\n",
    "‚îÇ              (nginx, HAProxy, or cloud LB)                       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                           ‚îÇ\n",
    "                           ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                     FastAPI APPLICATION                          ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ\n",
    "‚îÇ  ‚îÇ   Auth      ‚îÇ  ‚îÇ Rate Limit  ‚îÇ  ‚îÇ  Logging    ‚îÇ              ‚îÇ\n",
    "‚îÇ  ‚îÇ Middleware  ‚îÇ  ‚îÇ Middleware  ‚îÇ  ‚îÇ Middleware  ‚îÇ              ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
    "‚îÇ  ‚îÇ                    API ENDPOINTS                          ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îÇ  POST /v1/chat/completions    (OpenAI compatible)        ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îÇ  GET  /health                 (Health check)              ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îÇ  GET  /metrics                (Prometheus metrics)        ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                           ‚îÇ\n",
    "                           ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                   INFERENCE ENGINE                               ‚îÇ\n",
    "‚îÇ          (vLLM, SGLang, TensorRT-LLM, Ollama)                   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Building the API Server\n",
    "\n",
    "Let's build a complete production API. We'll create the code here, then save it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# API server code\napi_server_code = '''\n\"\"\"\nProduction LLM API Server\n\nA production-ready FastAPI server for LLM inference with:\n- OpenAI-compatible API endpoints\n- Server-Sent Events (SSE) streaming\n- Rate limiting\n- Health checks\n- Request logging\n- Error handling\n\nUsage:\n    uvicorn api_server:app --host 0.0.0.0 --port 8080 --workers 1\n\nEnvironment Variables:\n    BACKEND_URL: URL of the inference backend (default: http://localhost:8000)\n    API_KEY: Required API key for authentication (optional)\n    RATE_LIMIT: Requests per minute per client (default: 60)\n\"\"\"\n\nimport asyncio\nimport json\nimport logging\nimport os\nimport time\nimport uuid\nfrom collections import defaultdict\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import Any, AsyncGenerator, Dict, List, Optional\n\nimport aiohttp\nfrom fastapi import FastAPI, HTTPException, Request, Depends\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse\nfrom pydantic import BaseModel, Field\nfrom sse_starlette.sse import EventSourceResponse\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\n# Configuration\nBACKEND_URL = os.getenv(\"BACKEND_URL\", \"http://localhost:8000\")\nAPI_KEY = os.getenv(\"API_KEY\", None)\nRATE_LIMIT = int(os.getenv(\"RATE_LIMIT\", \"60\"))\n\n\n# ============================================================================\n# Pydantic Models (OpenAI-compatible)\n# ============================================================================\n\nclass Message(BaseModel):\n    role: str = Field(..., description=\"Role: system, user, or assistant\")\n    content: str = Field(..., description=\"Message content\")\n\n\nclass ChatCompletionRequest(BaseModel):\n    model: str = Field(default=\"default\", description=\"Model to use\")\n    messages: List[Message] = Field(..., description=\"Conversation messages\")\n    max_tokens: int = Field(default=512, ge=1, le=4096, description=\"Max tokens to generate\")\n    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description=\"Sampling temperature\")\n    top_p: float = Field(default=0.9, ge=0.0, le=1.0, description=\"Nucleus sampling\")\n    stream: bool = Field(default=False, description=\"Enable streaming\")\n    stop: Optional[List[str]] = Field(default=None, description=\"Stop sequences\")\n\n\nclass HealthResponse(BaseModel):\n    status: str\n    backend_status: str\n    uptime_seconds: float\n    total_requests: int\n    active_requests: int\n\n\n# ============================================================================\n# Metrics & Rate Limiting\n# ============================================================================\n\n@dataclass\nclass ServerMetrics:\n    \"\"\"Track server metrics.\"\"\"\n    start_time: float = field(default_factory=time.time)\n    total_requests: int = 0\n    successful_requests: int = 0\n    failed_requests: int = 0\n    active_requests: int = 0\n    total_tokens_generated: int = 0\n    total_latency_ms: float = 0.0\n    \n    @property\n    def uptime_seconds(self) -> float:\n        return time.time() - self.start_time\n    \n    @property\n    def avg_latency_ms(self) -> float:\n        if self.successful_requests == 0:\n            return 0.0\n        return self.total_latency_ms / self.successful_requests\n\n\nclass RateLimiter:\n    \"\"\"Simple in-memory rate limiter.\"\"\"\n    \n    def __init__(self, requests_per_minute: int = 60):\n        self.requests_per_minute = requests_per_minute\n        self.requests: Dict[str, List[float]] = defaultdict(list)\n    \n    def is_allowed(self, client_id: str) -> bool:\n        \"\"\"Check if client is within rate limit.\"\"\"\n        now = time.time()\n        minute_ago = now - 60\n        \n        # Clean old requests\n        self.requests[client_id] = [\n            t for t in self.requests[client_id] if t > minute_ago\n        ]\n        \n        if len(self.requests[client_id]) >= self.requests_per_minute:\n            return False\n        \n        self.requests[client_id].append(now)\n        return True\n    \n    def get_remaining(self, client_id: str) -> int:\n        \"\"\"Get remaining requests for client.\"\"\"\n        now = time.time()\n        minute_ago = now - 60\n        current = len([t for t in self.requests[client_id] if t > minute_ago])\n        return max(0, self.requests_per_minute - current)\n\n\n# Global instances\nmetrics = ServerMetrics()\nrate_limiter = RateLimiter(RATE_LIMIT)\n\n\n# ============================================================================\n# FastAPI App\n# ============================================================================\n\napp = FastAPI(\n    title=\"LLM Inference API\",\n    description=\"Production-ready API for LLM inference\",\n    version=\"1.0.0\",\n    docs_url=\"/docs\",\n    redoc_url=\"/redoc\",\n)\n\n# CORS middleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n\n# ============================================================================\n# Dependencies\n# ============================================================================\n\nasync def verify_api_key(request: Request) -> str:\n    \"\"\"Verify API key if configured.\"\"\"\n    if API_KEY is None:\n        return \"anonymous\"\n    \n    auth_header = request.headers.get(\"Authorization\", \"\")\n    if not auth_header.startswith(\"Bearer \"):\n        raise HTTPException(\n            status_code=401,\n            detail=\"Missing or invalid Authorization header\"\n        )\n    \n    token = auth_header[7:]\n    if token != API_KEY:\n        raise HTTPException(status_code=401, detail=\"Invalid API key\")\n    \n    return token[:8]  # Return truncated for logging\n\n\nasync def check_rate_limit(request: Request):\n    \"\"\"Check rate limit for client.\"\"\"\n    client_id = request.client.host if request.client else \"unknown\"\n    \n    if not rate_limiter.is_allowed(client_id):\n        remaining = rate_limiter.get_remaining(client_id)\n        raise HTTPException(\n            status_code=429,\n            detail=f\"Rate limit exceeded. Try again later. Remaining: {remaining}\"\n        )\n\n\n# ============================================================================\n# Backend Communication\n# ============================================================================\n\nasync def check_backend_health() -> bool:\n    \"\"\"Check if backend is healthy.\"\"\"\n    try:\n        async with aiohttp.ClientSession() as session:\n            async with session.get(\n                f\"{BACKEND_URL}/v1/models\",\n                timeout=aiohttp.ClientTimeout(total=5)\n            ) as response:\n                return response.status == 200\n    except Exception:\n        return False\n\n\nasync def stream_from_backend(\n    request_data: Dict[str, Any]\n) -> AsyncGenerator[str, None]:\n    \"\"\"Stream response from backend.\"\"\"\n    try:\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                f\"{BACKEND_URL}/v1/chat/completions\",\n                json=request_data,\n                timeout=aiohttp.ClientTimeout(total=120)\n            ) as response:\n                if response.status != 200:\n                    error_text = await response.text()\n                    yield f\"data: {{\\\"error\\\": \\\"{error_text}\\\"}}\\\\n\\\\n\"\n                    return\n                \n                async for line in response.content:\n                    if line:\n                        yield line.decode(\"utf-8\")\n                        \n    except asyncio.TimeoutError:\n        yield \"data: {\\\"error\\\": \\\"Request timeout\\\"}\\\\n\\\\n\"\n    except Exception as e:\n        yield f\"data: {{\\\"error\\\": \\\"{str(e)}\\\"}}\\\\n\\\\n\"\n\n\nasync def forward_to_backend(request_data: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Forward non-streaming request to backend.\"\"\"\n    try:\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                f\"{BACKEND_URL}/v1/chat/completions\",\n                json=request_data,\n                timeout=aiohttp.ClientTimeout(total=120)\n            ) as response:\n                if response.status != 200:\n                    error_text = await response.text()\n                    raise HTTPException(\n                        status_code=response.status,\n                        detail=f\"Backend error: {error_text}\"\n                    )\n                return await response.json()\n                \n    except asyncio.TimeoutError:\n        raise HTTPException(status_code=504, detail=\"Backend timeout\")\n    except aiohttp.ClientError as e:\n        raise HTTPException(status_code=502, detail=f\"Backend error: {str(e)}\")\n\n\n# ============================================================================\n# API Endpoints\n# ============================================================================\n\n@app.get(\"/health\", response_model=HealthResponse)\nasync def health_check():\n    \"\"\"Health check endpoint.\"\"\"\n    backend_healthy = await check_backend_health()\n    \n    return HealthResponse(\n        status=\"healthy\" if backend_healthy else \"degraded\",\n        backend_status=\"connected\" if backend_healthy else \"disconnected\",\n        uptime_seconds=metrics.uptime_seconds,\n        total_requests=metrics.total_requests,\n        active_requests=metrics.active_requests\n    )\n\n\n@app.get(\"/metrics\")\nasync def get_metrics():\n    \"\"\"Get server metrics.\"\"\"\n    return {\n        \"uptime_seconds\": metrics.uptime_seconds,\n        \"total_requests\": metrics.total_requests,\n        \"successful_requests\": metrics.successful_requests,\n        \"failed_requests\": metrics.failed_requests,\n        \"active_requests\": metrics.active_requests,\n        \"total_tokens_generated\": metrics.total_tokens_generated,\n        \"avg_latency_ms\": metrics.avg_latency_ms\n    }\n\n\n@app.get(\"/v1/models\")\nasync def list_models():\n    \"\"\"List available models (OpenAI compatible).\"\"\"\n    return {\n        \"object\": \"list\",\n        \"data\": [\n            {\n                \"id\": \"default\",\n                \"object\": \"model\",\n                \"created\": int(metrics.start_time),\n                \"owned_by\": \"local\"\n            }\n        ]\n    }\n\n\n@app.post(\"/v1/chat/completions\")\nasync def chat_completions(\n    request: ChatCompletionRequest,\n    raw_request: Request,\n    _: str = Depends(verify_api_key),\n    __: None = Depends(check_rate_limit)\n):\n    \"\"\"Chat completions endpoint (OpenAI compatible).\"\"\"\n    request_id = str(uuid.uuid4())[:8]\n    start_time = time.time()\n    \n    # Update metrics\n    metrics.total_requests += 1\n    metrics.active_requests += 1\n    \n    client_ip = raw_request.client.host if raw_request.client else \"unknown\"\n    logger.info(f\"[{request_id}] Request from {client_ip}: {len(request.messages)} messages\")\n    \n    try:\n        # Prepare request for backend\n        backend_request = {\n            \"model\": request.model,\n            \"messages\": [m.dict() for m in request.messages],\n            \"max_tokens\": request.max_tokens,\n            \"temperature\": request.temperature,\n            \"top_p\": request.top_p,\n            \"stream\": request.stream\n        }\n        \n        if request.stop:\n            backend_request[\"stop\"] = request.stop\n        \n        if request.stream:\n            # Streaming response\n            async def event_generator():\n                try:\n                    async for chunk in stream_from_backend(backend_request):\n                        yield chunk\n                finally:\n                    metrics.active_requests -= 1\n                    metrics.successful_requests += 1\n                    latency = (time.time() - start_time) * 1000\n                    metrics.total_latency_ms += latency\n                    logger.info(f\"[{request_id}] Streaming completed in {latency:.0f}ms\")\n            \n            return EventSourceResponse(event_generator(), media_type=\"text/event-stream\")\n        \n        else:\n            # Non-streaming response\n            response = await forward_to_backend(backend_request)\n            \n            # Update metrics\n            latency = (time.time() - start_time) * 1000\n            metrics.successful_requests += 1\n            metrics.total_latency_ms += latency\n            \n            usage = response.get(\"usage\", {})\n            metrics.total_tokens_generated += usage.get('completion_tokens', 0)\n            \n            completion_tokens = usage.get('completion_tokens', 0)\n            logger.info(f\"[{request_id}] Completed in {latency:.0f}ms, {completion_tokens} tokens\")\n            \n            return response\n            \n    except HTTPException:\n        metrics.failed_requests += 1\n        raise\n    except Exception as e:\n        metrics.failed_requests += 1\n        logger.error(f\"[{request_id}] Error: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n    finally:\n        if not request.stream:\n            metrics.active_requests -= 1\n\n\n@app.exception_handler(Exception)\nasync def global_exception_handler(request: Request, exc: Exception):\n    \"\"\"Global exception handler.\"\"\"\n    logger.error(f\"Unhandled exception: {str(exc)}\")\n    return JSONResponse(\n        status_code=500,\n        content={\"error\": {\"message\": \"Internal server error\", \"type\": \"server_error\"}}\n    )\n\n\n# ============================================================================\n# Startup/Shutdown\n# ============================================================================\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    \"\"\"Run on startup.\"\"\"\n    logger.info(\"Starting LLM API server...\")\n    logger.info(f\"Backend URL: {BACKEND_URL}\")\n    logger.info(f\"Rate limit: {RATE_LIMIT} requests/minute\")\n    logger.info(f\"API key required: {API_KEY is not None}\")\n    \n    # Check backend\n    backend_healthy = await check_backend_health()\n    if backend_healthy:\n        logger.info(\"Backend is healthy\")\n    else:\n        logger.warning(\"Backend is not responding - will retry on requests\")\n\n\n@app.on_event(\"shutdown\")\nasync def shutdown_event():\n    \"\"\"Run on shutdown.\"\"\"\n    logger.info(f\"Shutting down... Total requests served: {metrics.total_requests}\")\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8080)\n'''\n\n# Save to file\napi_path = Path(\"../api/api_server.py\")\napi_path.parent.mkdir(exist_ok=True)\napi_path.write_text(api_server_code)\n\nprint(f\"‚úÖ API server code saved to: {api_path.resolve()}\")\nprint(f\"\\nüìù File size: {len(api_server_code)} bytes\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain key components\n",
    "print(\"\"\"\n",
    "üìä API SERVER KEY COMPONENTS\n",
    "=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"\n",
    "\n",
    "1. PYDANTIC MODELS\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "   - ChatCompletionRequest: Validates incoming requests\n",
    "   - HealthResponse: Health check response structure\n",
    "   - Automatic OpenAPI documentation\n",
    "\n",
    "2. RATE LIMITING\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "   - In-memory sliding window counter\n",
    "   - Per-client (by IP) rate limiting\n",
    "   - Configurable via RATE_LIMIT env var\n",
    "\n",
    "3. AUTHENTICATION\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "   - Bearer token authentication\n",
    "   - Optional (disabled if API_KEY not set)\n",
    "   - Standard \"Authorization: Bearer <token>\" header\n",
    "\n",
    "4. STREAMING (SSE)\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "   - Server-Sent Events for real-time responses\n",
    "   - Compatible with OpenAI client libraries\n",
    "   - Proper cleanup on disconnect\n",
    "\n",
    "5. METRICS\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "   - Request counts (total, success, failed)\n",
    "   - Latency tracking\n",
    "   - Token counting\n",
    "   - Exposed via /metrics endpoint\n",
    "\n",
    "6. ERROR HANDLING\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "   - Structured error responses\n",
    "   - Proper HTTP status codes\n",
    "   - Global exception handler\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Running and Testing the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to run the API server\n",
    "print(\"\"\"\n",
    "üìù RUNNING THE API SERVER\n",
    "=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"\n",
    "\n",
    "1. Start your inference backend (choose one):\n",
    "   \n",
    "   # Ollama\n",
    "   ollama serve\n",
    "   \n",
    "   # vLLM\n",
    "   python -m vllm.entrypoints.openai.api_server \\\\\n",
    "       --model Qwen/Qwen3-8B-Instruct \\\\\n",
    "       --port 8000\n",
    "   \n",
    "   # SGLang\n",
    "   python -m sglang.launch_server \\\\\n",
    "       --model-path Qwen/Qwen3-8B-Instruct \\\\\n",
    "       --port 8000\n",
    "\n",
    "2. Start the API server:\n",
    "   \n",
    "   cd api/\n",
    "   \n",
    "   # Basic (no auth)\n",
    "   BACKEND_URL=http://localhost:8000 uvicorn api_server:app --port 8080\n",
    "   \n",
    "   # With authentication\n",
    "   BACKEND_URL=http://localhost:8000 \\\\\n",
    "   API_KEY=your-secret-key \\\\\n",
    "   RATE_LIMIT=30 \\\\\n",
    "   uvicorn api_server:app --port 8080\n",
    "\n",
    "3. Access the API:\n",
    "   \n",
    "   # Health check\n",
    "   curl http://localhost:8080/health\n",
    "   \n",
    "   # API docs\n",
    "   open http://localhost:8080/docs\n",
    "   \n",
    "   # Chat completion\n",
    "   curl http://localhost:8080/v1/chat/completions \\\\\n",
    "       -H \"Content-Type: application/json\" \\\\\n",
    "       -H \"Authorization: Bearer your-secret-key\" \\\\\n",
    "       -d '{\n",
    "           \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "           \"max_tokens\": 100\n",
    "       }'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function for the API\n",
    "def test_api_endpoint(url: str = \"http://localhost:8080\", api_key: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Test the production API endpoints.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüß™ Testing API at {url}...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    if api_key:\n",
    "        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n",
    "    \n",
    "    # Test 1: Health check\n",
    "    print(\"\\n1. Health check...\")\n",
    "    try:\n",
    "        response = requests.get(f\"{url}/health\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(f\"   ‚úÖ Status: {data['status']}\")\n",
    "            print(f\"   Backend: {data['backend_status']}\")\n",
    "            print(f\"   Uptime: {data['uptime_seconds']:.0f}s\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Test 2: List models\n",
    "    print(\"\\n2. List models...\")\n",
    "    try:\n",
    "        response = requests.get(f\"{url}/v1/models\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(f\"   ‚úÖ Models: {[m['id'] for m in data['data']]}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    # Test 3: Chat completion (non-streaming)\n",
    "    print(\"\\n3. Chat completion (non-streaming)...\")\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{url}/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            json={\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": \"Say 'Hello, World!' and nothing else.\"}],\n",
    "                \"max_tokens\": 20,\n",
    "                \"temperature\": 0.1\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            content = data['choices'][0]['message']['content']\n",
    "            print(f\"   ‚úÖ Response: {content[:50]}...\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Status code: {response.status_code}\")\n",
    "            print(f\"   Response: {response.text[:100]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    # Test 4: Metrics\n",
    "    print(\"\\n4. Metrics...\")\n",
    "    try:\n",
    "        response = requests.get(f\"{url}/metrics\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(f\"   ‚úÖ Total requests: {data['total_requests']}\")\n",
    "            print(f\"   Successful: {data['successful_requests']}\")\n",
    "            print(f\"   Avg latency: {data['avg_latency_ms']:.0f}ms\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Tests complete!\")\n",
    "\n",
    "\n",
    "# Uncomment to test (requires API server running)\n",
    "# test_api_endpoint(\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python client example\n",
    "client_code = '''\n",
    "\"\"\"\n",
    "Example Python client for the LLM API.\n",
    "\n",
    "This client is compatible with the OpenAI Python library,\n",
    "so you can also use:\n",
    "\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(base_url=\"http://localhost:8080/v1\", api_key=\"your-key\")\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from typing import Iterator, Optional\n",
    "\n",
    "\n",
    "class LLMClient:\n",
    "    \"\"\"Simple client for the LLM API.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str = \"http://localhost:8080\", api_key: Optional[str] = None):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.headers = {\"Content-Type\": \"application/json\"}\n",
    "        if api_key:\n",
    "            self.headers[\"Authorization\"] = f\"Bearer {api_key}\"\n",
    "    \n",
    "    def chat(self, message: str, max_tokens: int = 256, temperature: float = 0.7) -> str:\n",
    "        \"\"\"Send a chat message and get a response.\"\"\"\n",
    "        response = requests.post(\n",
    "            f\"{self.base_url}/v1/chat/completions\",\n",
    "            headers=self.headers,\n",
    "            json={\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": message}],\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"stream\": False\n",
    "            }\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    def stream_chat(self, message: str, max_tokens: int = 256) -> Iterator[str]:\n",
    "        \"\"\"Stream a chat response token by token.\"\"\"\n",
    "        response = requests.post(\n",
    "            f\"{self.base_url}/v1/chat/completions\",\n",
    "            headers=self.headers,\n",
    "            json={\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": message}],\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"stream\": True\n",
    "            },\n",
    "            stream=True\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                line = line.decode(\"utf-8\")\n",
    "                if line.startswith(\"data: \"):\n",
    "                    data = line[6:]\n",
    "                    if data == \"[DONE]\":\n",
    "                        break\n",
    "                    try:\n",
    "                        import json\n",
    "                        chunk = json.loads(data)\n",
    "                        content = chunk.get(\"choices\", [{}])[0].get(\"delta\", {}).get(\"content\", \"\")\n",
    "                        if content:\n",
    "                            yield content\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "    \n",
    "    def health(self) -> dict:\n",
    "        \"\"\"Check API health.\"\"\"\n",
    "        response = requests.get(f\"{self.base_url}/health\")\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    client = LLMClient(api_key=\"your-api-key\")\n",
    "    \n",
    "    # Health check\n",
    "    print(\"Health:\", client.health())\n",
    "    \n",
    "    # Simple chat\n",
    "    response = client.chat(\"What is the capital of France?\")\n",
    "    print(f\"Response: {response}\")\n",
    "    \n",
    "    # Streaming chat\n",
    "    print(\"Streaming: \", end=\"\")\n",
    "    for chunk in client.stream_chat(\"Tell me a short joke.\"):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    print()\n",
    "'''\n",
    "\n",
    "# Save client code\n",
    "client_path = Path(\"../api/client.py\")\n",
    "client_path.write_text(client_code)\n",
    "print(f\"‚úÖ Client code saved to: {client_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Production Deployment Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "üìä PRODUCTION DEPLOYMENT CHECKLIST\n",
    "=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"=\"\"\n",
    "\n",
    "‚ñ° SECURITY\n",
    "  ‚îú‚îÄ ‚ñ° Enable HTTPS (use nginx/caddy as reverse proxy)\n",
    "  ‚îú‚îÄ ‚ñ° Set strong API keys\n",
    "  ‚îú‚îÄ ‚ñ° Rate limit per API key, not just per IP\n",
    "  ‚îú‚îÄ ‚ñ° Input validation (max prompt length)\n",
    "  ‚îî‚îÄ ‚ñ° Output sanitization if needed\n",
    "\n",
    "‚ñ° RELIABILITY\n",
    "  ‚îú‚îÄ ‚ñ° Health checks with proper timeouts\n",
    "  ‚îú‚îÄ ‚ñ° Graceful shutdown handling\n",
    "  ‚îú‚îÄ ‚ñ° Request timeout configuration\n",
    "  ‚îú‚îÄ ‚ñ° Retry logic for transient failures\n",
    "  ‚îî‚îÄ ‚ñ° Circuit breaker for backend failures\n",
    "\n",
    "‚ñ° OBSERVABILITY\n",
    "  ‚îú‚îÄ ‚ñ° Structured logging (JSON format)\n",
    "  ‚îú‚îÄ ‚ñ° Request tracing (correlation IDs)\n",
    "  ‚îú‚îÄ ‚ñ° Prometheus metrics export\n",
    "  ‚îú‚îÄ ‚ñ° Alerting on error rates\n",
    "  ‚îî‚îÄ ‚ñ° Dashboard for monitoring\n",
    "\n",
    "‚ñ° PERFORMANCE\n",
    "  ‚îú‚îÄ ‚ñ° Connection pooling\n",
    "  ‚îú‚îÄ ‚ñ° Async request handling\n",
    "  ‚îú‚îÄ ‚ñ° Proper worker configuration\n",
    "  ‚îú‚îÄ ‚ñ° Load testing before launch\n",
    "  ‚îî‚îÄ ‚ñ° Caching for repeated requests (optional)\n",
    "\n",
    "‚ñ° DEPLOYMENT\n",
    "  ‚îú‚îÄ ‚ñ° Docker containerization\n",
    "  ‚îú‚îÄ ‚ñ° Environment variable configuration\n",
    "  ‚îú‚îÄ ‚ñ° Health check in container spec\n",
    "  ‚îú‚îÄ ‚ñ° Resource limits (memory, CPU)\n",
    "  ‚îî‚îÄ ‚ñ° Horizontal scaling plan\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker deployment example\n",
    "dockerfile_content = '''\n",
    "# Dockerfile for LLM API Server\n",
    "FROM python:3.11-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install --no-cache-dir \\\\\n",
    "    fastapi \\\\\n",
    "    uvicorn \\\\\n",
    "    sse-starlette \\\\\n",
    "    aiohttp \\\\\n",
    "    python-multipart\n",
    "\n",
    "# Copy application\n",
    "COPY api_server.py .\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8080\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\\\n",
    "    CMD curl -f http://localhost:8080/health || exit 1\n",
    "\n",
    "# Run\n",
    "CMD [\"uvicorn\", \"api_server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n",
    "'''\n",
    "\n",
    "docker_compose_content = '''\n",
    "# docker-compose.yml for complete LLM stack\n",
    "version: \"3.8\"\n",
    "\n",
    "services:\n",
    "  # Inference backend (choose one)\n",
    "  vllm:\n",
    "    image: nvcr.io/nvidia/pytorch:25.11-py3\n",
    "    command: >\n",
    "      bash -c \"pip install vllm &&\n",
    "      python -m vllm.entrypoints.openai.api_server\n",
    "      --model Qwen/Qwen3-8B-Instruct\n",
    "      --port 8000\n",
    "      --enforce-eager\n",
    "      --dtype bfloat16\"\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    volumes:\n",
    "      - ~/.cache/huggingface:/root/.cache/huggingface\n",
    "    environment:\n",
    "      - HF_TOKEN=${HF_TOKEN}\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              count: 1\n",
    "              capabilities: [gpu]\n",
    "    ipc: host\n",
    "  \n",
    "  # API server\n",
    "  api:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8080:8080\"\n",
    "    environment:\n",
    "      - BACKEND_URL=http://vllm:8000\n",
    "      - API_KEY=${API_KEY}\n",
    "      - RATE_LIMIT=60\n",
    "    depends_on:\n",
    "      - vllm\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8080/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "'''\n",
    "\n",
    "# Save Docker files\n",
    "dockerfile_path = Path(\"../api/Dockerfile\")\n",
    "dockerfile_path.write_text(dockerfile_content)\n",
    "\n",
    "compose_path = Path(\"../api/docker-compose.yml\")\n",
    "compose_path.write_text(docker_compose_content)\n",
    "\n",
    "print(f\"‚úÖ Dockerfile saved to: {dockerfile_path.resolve()}\")\n",
    "print(f\"‚úÖ docker-compose.yml saved to: {compose_path.resolve()}\")\n",
    "\n",
    "print(\"\\nüìù To deploy with Docker Compose:\")\n",
    "print(\"   cd api/\")\n",
    "print(\"   export HF_TOKEN=your-huggingface-token\")\n",
    "print(\"   export API_KEY=your-api-key\")\n",
    "print(\"   docker-compose up -d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Using Async Properly\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - Blocking the event loop\n",
    "@app.post(\"/chat\")\n",
    "def chat(request: ChatRequest):\n",
    "    response = requests.post(backend_url, json=request.dict())  # Blocking!\n",
    "    return response.json()\n",
    "\n",
    "# ‚úÖ Right - Async all the way\n",
    "@app.post(\"/chat\")\n",
    "async def chat(request: ChatRequest):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(backend_url, json=request.dict()) as resp:\n",
    "            return await resp.json()\n",
    "```\n",
    "\n",
    "### Mistake 2: No Request Timeouts\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - Request hangs forever if backend is slow\n",
    "async with session.post(url, json=data) as resp:\n",
    "    return await resp.json()\n",
    "\n",
    "# ‚úÖ Right - Always set timeouts\n",
    "timeout = aiohttp.ClientTimeout(total=120, connect=10)\n",
    "async with session.post(url, json=data, timeout=timeout) as resp:\n",
    "    return await resp.json()\n",
    "```\n",
    "\n",
    "### Mistake 3: Exposing Internal Errors\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - Leaks internal details\n",
    "@app.exception_handler(Exception)\n",
    "async def handle_error(request, exc):\n",
    "    return JSONResponse({\"error\": str(exc)})  # Full stack trace!\n",
    "\n",
    "# ‚úÖ Right - Generic error for clients, log details\n",
    "@app.exception_handler(Exception)\n",
    "async def handle_error(request, exc):\n",
    "    logger.error(f\"Internal error: {exc}\")  # Log full error\n",
    "    return JSONResponse(\n",
    "        status_code=500,\n",
    "        content={\"error\": \"Internal server error\"}  # Generic for client\n",
    "    )\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Add Request Caching\n",
    "\n",
    "Implement caching for repeated identical requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Add a simple in-memory cache for non-streaming requests\n",
    "# Key: hash of (messages, temperature)\n",
    "# Value: response + timestamp\n",
    "# TTL: 5 minutes\n",
    "\n",
    "# TODO: Implement the cache\n",
    "# Hints:\n",
    "# - Use hashlib to hash the request\n",
    "# - Store (response, timestamp) tuples\n",
    "# - Check TTL before returning cached response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Add Prometheus Metrics\n",
    "\n",
    "Expose metrics in Prometheus format for monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Add a /metrics endpoint that returns Prometheus format:\n",
    "# \n",
    "# # HELP llm_requests_total Total requests\n",
    "# # TYPE llm_requests_total counter\n",
    "# llm_requests_total{status=\"success\"} 100\n",
    "# llm_requests_total{status=\"error\"} 5\n",
    "# \n",
    "# # HELP llm_request_duration_seconds Request duration\n",
    "# # TYPE llm_request_duration_seconds histogram\n",
    "# llm_request_duration_seconds_bucket{le=\"0.1\"} 50\n",
    "# llm_request_duration_seconds_bucket{le=\"0.5\"} 80\n",
    "# ...\n",
    "\n",
    "# TODO: Implement Prometheus metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How to build a production-ready LLM API with FastAPI\n",
    "- ‚úÖ How to implement SSE streaming for real-time responses\n",
    "- ‚úÖ How to add rate limiting, authentication, and monitoring\n",
    "- ‚úÖ How to deploy with Docker and handle errors gracefully\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [FastAPI Documentation](https://fastapi.tiangolo.com/)\n",
    "- [Server-Sent Events (MDN)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)\n",
    "- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)\n",
    "- [Uvicorn Deployment](https://www.uvicorn.org/deployment/)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cleanup\nimport gc\n\n# Clear Python garbage\ngc.collect()\n\n# Clear GPU memory cache if torch is available\ntry:\n    import torch\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n        print(\"‚úÖ GPU memory cache cleared!\")\nexcept ImportError:\n    pass\n\nprint(\"‚úÖ Cleanup complete!\")\nprint(\"\\nüìÅ Files created in ../api/:\")\nprint(\"   - api_server.py  (Main API server)\")\nprint(\"   - client.py      (Python client)\")\nprint(\"   - Dockerfile     (Container image)\")\nprint(\"   - docker-compose.yml (Full stack deployment)\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}