{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 12.6: Ollama Web UI Integration\n",
    "\n",
    "**Module:** 12 - Model Deployment & Inference Engines  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Set up Open WebUI for Ollama\n",
    "- [ ] Configure and customize the interface\n",
    "- [ ] Add model presets and system prompts\n",
    "- [ ] Enable advanced features like RAG and web search\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Previous tasks in this module\n",
    "- Running: Ollama with at least one model pulled\n",
    "- Docker installed (for Open WebUI)\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**Why use a Web UI?**\n",
    "\n",
    "While APIs are great for integration, a web interface is valuable for:\n",
    "- **Team use**: Non-technical colleagues can interact with models\n",
    "- **Experimentation**: Quickly test different models and prompts\n",
    "- **Documentation**: Conversation history is preserved\n",
    "- **RAG demos**: Upload documents and query them\n",
    "- **Model comparison**: Side-by-side model testing\n",
    "\n",
    "**Open WebUI** (formerly Ollama WebUI) is the most popular choice - it's feature-rich, actively maintained, and designed specifically for Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: Web UI vs API\n",
    "\n",
    "> **Think of ordering food...**\n",
    ">\n",
    "> **API** = Ordering through a restaurant's phone line.\n",
    "> You need to know the exact format: \"I'd like one pepperoni pizza, medium, extra cheese.\"\n",
    "> Perfect for apps that order automatically.\n",
    ">\n",
    "> **Web UI** = Ordering at the restaurant with a menu.\n",
    "> You see pictures, read descriptions, and click what you want.\n",
    "> Perfect for humans exploring options.\n",
    ">\n",
    "> **Both talk to the same kitchen (Ollama)**, just with different interfaces!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setting Up Open WebUI\n",
    "\n",
    "Open WebUI is the leading interface for Ollama. Let's set it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Check Ollama status\n",
    "def check_ollama():\n",
    "    \"\"\"Check if Ollama is running and what models are available.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get(\"models\", [])\n",
    "            return True, [m[\"name\"] for m in models]\n",
    "    except:\n",
    "        pass\n",
    "    return False, []\n",
    "\n",
    "ollama_running, models = check_ollama()\n",
    "\n",
    "print(\"üîç System Check\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if ollama_running:\n",
    "    print(\"‚úÖ Ollama is running\")\n",
    "    print(f\"   Available models: {', '.join(models[:5])}...\" if len(models) > 5 else f\"   Available models: {', '.join(models)}\")\n",
    "else:\n",
    "    print(\"‚ùå Ollama is not running\")\n",
    "    print(\"   Start with: ollama serve\")\n",
    "    print(\"   Then pull a model: ollama pull llama3.1:8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üê≥ Installing Open WebUI\n",
    "\n",
    "Open WebUI runs as a Docker container. Here are the setup options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate Open WebUI Docker commands\n\ndef generate_webui_commands(port: int = 3000, data_dir: str = \"~/.open-webui\"):\n    \"\"\"Generate Docker commands for Open WebUI.\"\"\"\n    \n    basic = f\"\"\"\n# Basic Setup (connects to Ollama on host)\n# Note: Ollama runs separately on host, models stored in ~/.ollama\ndocker run -d -p {port}:8080 \\\\\n    --add-host=host.docker.internal:host-gateway \\\\\n    -v {data_dir}:/app/backend/data \\\\\n    --name open-webui \\\\\n    --restart always \\\\\n    ghcr.io/open-webui/open-webui:main\n\"\"\"\n    \n    with_gpu = f\"\"\"\n# With GPU Support (for embeddings/local processing)\ndocker run -d -p {port}:8080 \\\\\n    --gpus all \\\\\n    --add-host=host.docker.internal:host-gateway \\\\\n    -v {data_dir}:/app/backend/data \\\\\n    --name open-webui \\\\\n    --restart always \\\\\n    ghcr.io/open-webui/open-webui:cuda\n\"\"\"\n    \n    with_bundled_ollama = f\"\"\"\n# All-in-One (Open WebUI + Ollama bundled)\n# IMPORTANT: Mount ~/.ollama to persist downloaded models\ndocker run -d -p {port}:8080 \\\\\n    --gpus all \\\\\n    -v {data_dir}:/app/backend/data \\\\\n    -v ~/.ollama:/root/.ollama \\\\\n    --name open-webui \\\\\n    --restart always \\\\\n    ghcr.io/open-webui/open-webui:ollama\n\"\"\"\n    \n    return {\n        \"basic\": basic,\n        \"with_gpu\": with_gpu,\n        \"bundled\": with_bundled_ollama\n    }\n\ncommands = generate_webui_commands()\n\nprint(\"üê≥ Open WebUI Docker Setup Options\")\nprint(\"=\" * 60)\nprint(\"\\nüì¶ OPTION 1: Basic (Ollama already running on host)\")\nprint(commands[\"basic\"])\nprint(\"\\nüì¶ OPTION 2: With GPU (for local embeddings)\")\nprint(commands[\"with_gpu\"])\nprint(\"\\nüì¶ OPTION 3: All-in-One (includes Ollama)\")\nprint(commands[\"bundled\"])\nprint(\"\\nüí° Key volumes to persist data:\")\nprint(\"   ‚Ä¢ ~/.open-webui:/app/backend/data - Chat history, settings\")\nprint(\"   ‚Ä¢ ~/.ollama:/root/.ollama - Downloaded models (for bundled option)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Open WebUI is already running\n",
    "def check_webui(port: int = 3000):\n",
    "    \"\"\"Check if Open WebUI is running.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"http://localhost:{port}/\", timeout=5)\n",
    "        return response.status_code == 200\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "if check_webui():\n",
    "    print(\"‚úÖ Open WebUI is running!\")\n",
    "    print(\"   Access it at: http://localhost:3000\")\n",
    "else:\n",
    "    print(\"‚ùå Open WebUI is not running\")\n",
    "    print(\"   Use one of the Docker commands above to start it\")\n",
    "    print(\"   After starting, access at: http://localhost:3000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Initial Configuration\n",
    "\n",
    "After starting Open WebUI for the first time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß First-Time Setup\n",
    "\n",
    "1. **Create Admin Account**\n",
    "   - Navigate to `http://localhost:3000`\n",
    "   - Create your admin account (first user is admin)\n",
    "   - This account controls all settings\n",
    "\n",
    "2. **Verify Ollama Connection**\n",
    "   - Go to Settings ‚Üí Connections\n",
    "   - Ollama URL should be: `http://host.docker.internal:11434`\n",
    "   - Click \"Verify\" to test the connection\n",
    "\n",
    "3. **Configure Models**\n",
    "   - Go to Settings ‚Üí Models\n",
    "   - Your Ollama models should appear automatically\n",
    "   - Set a default model for new chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration recommendations\n",
    "\n",
    "config_recommendations = {\n",
    "    \"General Settings\": [\n",
    "        \"Enable 'Show Username in Chat' for multi-user setups\",\n",
    "        \"Set a default system prompt for consistent behavior\",\n",
    "        \"Configure chat history retention\",\n",
    "    ],\n",
    "    \"Model Settings\": [\n",
    "        \"Set default parameters (temperature: 0.7, max tokens: 2048)\",\n",
    "        \"Create model presets for different use cases\",\n",
    "        \"Enable 'Show Response Stats' to see tokens/sec\",\n",
    "    ],\n",
    "    \"Interface Settings\": [\n",
    "        \"Enable dark mode for long sessions\",\n",
    "        \"Configure keyboard shortcuts\",\n",
    "        \"Enable code syntax highlighting\",\n",
    "    ],\n",
    "    \"Advanced Features\": [\n",
    "        \"Enable RAG for document upload\",\n",
    "        \"Configure web search integration\",\n",
    "        \"Set up custom tools/functions\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Recommended Configuration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for category, recommendations in config_recommendations.items():\n",
    "    print(f\"\\nüìã {category}:\")\n",
    "    for rec in recommendations:\n",
    "        print(f\"   ‚Ä¢ {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Creating Model Presets\n",
    "\n",
    "Model presets let you save configurations for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model preset configurations\n",
    "\n",
    "model_presets = {\n",
    "    \"Code Assistant\": {\n",
    "        \"base_model\": \"llama3.1:8b\",\n",
    "        \"system_prompt\": \"\"\"You are an expert programmer and software engineer. \n",
    "You help with coding tasks by:\n",
    "1. Writing clean, well-documented code\n",
    "2. Explaining concepts clearly\n",
    "3. Suggesting best practices\n",
    "4. Debugging issues step by step\n",
    "\n",
    "Always include code examples when relevant. Use markdown code blocks with language specification.\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"temperature\": 0.3,  # Lower for more precise code\n",
    "            \"top_p\": 0.9,\n",
    "            \"max_tokens\": 2048\n",
    "        }\n",
    "    },\n",
    "    \"Creative Writer\": {\n",
    "        \"base_model\": \"llama3.1:8b\",\n",
    "        \"system_prompt\": \"\"\"You are a creative writing assistant with expertise in storytelling, \n",
    "poetry, and various writing styles. You help users:\n",
    "1. Generate creative content\n",
    "2. Develop characters and plots\n",
    "3. Improve writing style\n",
    "4. Overcome writer's block\n",
    "\n",
    "Be imaginative and expressive. Adapt your style to the user's preferences.\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"temperature\": 0.9,  # Higher for creativity\n",
    "            \"top_p\": 0.95,\n",
    "            \"max_tokens\": 4096\n",
    "        }\n",
    "    },\n",
    "    \"Research Assistant\": {\n",
    "        \"base_model\": \"llama3.1:8b\",\n",
    "        \"system_prompt\": \"\"\"You are a research assistant helping with information synthesis and analysis.\n",
    "You help users:\n",
    "1. Summarize complex topics\n",
    "2. Compare different viewpoints\n",
    "3. Identify key insights\n",
    "4. Suggest further reading\n",
    "\n",
    "Always be accurate and cite limitations when unsure. Structure responses clearly.\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"temperature\": 0.5,\n",
    "            \"top_p\": 0.9,\n",
    "            \"max_tokens\": 2048\n",
    "        }\n",
    "    },\n",
    "    \"Concise Helper\": {\n",
    "        \"base_model\": \"llama3.1:8b\",\n",
    "        \"system_prompt\": \"You are a helpful assistant. Be concise - answer in 1-3 sentences unless more detail is requested.\",\n",
    "        \"parameters\": {\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"max_tokens\": 256\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üé® Model Preset Configurations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, config in model_presets.items():\n",
    "    print(f\"\\nüìå {name}\")\n",
    "    print(f\"   Base Model: {config['base_model']}\")\n",
    "    print(f\"   Temperature: {config['parameters']['temperature']}\")\n",
    "    print(f\"   Max Tokens: {config['parameters']['max_tokens']}\")\n",
    "    print(f\"   System Prompt: {config['system_prompt'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Creating a Preset in Open WebUI\n",
    "\n",
    "1. Go to **Workspace ‚Üí Models**\n",
    "2. Click **Create a model**\n",
    "3. Fill in:\n",
    "   - Name (e.g., \"Code Assistant\")\n",
    "   - Base model (select from dropdown)\n",
    "   - System prompt (paste from above)\n",
    "   - Parameters (temperature, etc.)\n",
    "4. Click **Save**\n",
    "\n",
    "The preset will now appear in your model dropdown!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: RAG (Document Upload) Setup\n",
    "\n",
    "Open WebUI supports RAG - upload documents and chat with them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG configuration guide\n",
    "\n",
    "rag_config = {\n",
    "    \"Embedding Model\": {\n",
    "        \"option\": \"Settings ‚Üí Documents ‚Üí Embedding Model\",\n",
    "        \"recommendation\": \"Use 'nomic-embed-text' for good balance of speed/quality\",\n",
    "        \"setup\": \"ollama pull nomic-embed-text\"\n",
    "    },\n",
    "    \"Chunk Size\": {\n",
    "        \"option\": \"Settings ‚Üí Documents ‚Üí Chunk Size\",\n",
    "        \"recommendation\": \"1000-1500 characters for most documents\",\n",
    "        \"note\": \"Larger = more context but less precision\"\n",
    "    },\n",
    "    \"Top K Results\": {\n",
    "        \"option\": \"Settings ‚Üí Documents ‚Üí Top K\",\n",
    "        \"recommendation\": \"3-5 for focused retrieval, 10+ for broad coverage\",\n",
    "        \"note\": \"More results = slower but potentially more complete\"\n",
    "    },\n",
    "    \"Supported Formats\": {\n",
    "        \"formats\": [\"PDF\", \"TXT\", \"DOCX\", \"MD\", \"CSV\", \"JSON\"],\n",
    "        \"note\": \"PDF requires additional processing\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìö RAG Configuration Guide\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for setting, details in rag_config.items():\n",
    "    print(f\"\\nüîß {setting}:\")\n",
    "    for key, value in details.items():\n",
    "        if key != \"setup\":\n",
    "            print(f\"   {key}: {value}\")\n",
    "        else:\n",
    "            print(f\"   Setup command: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì§ Using Document Upload\n",
    "\n",
    "1. **Pull an embedding model** (if not done):\n",
    "   ```bash\n",
    "   ollama pull nomic-embed-text\n",
    "   ```\n",
    "\n",
    "2. **Configure in Open WebUI**:\n",
    "   - Settings ‚Üí Documents ‚Üí Select embedding model\n",
    "   - Enable RAG\n",
    "\n",
    "3. **Upload documents**:\n",
    "   - Click the **+** button in chat\n",
    "   - Select \"Upload document\"\n",
    "   - Wait for processing\n",
    "\n",
    "4. **Query your documents**:\n",
    "   - Start with \"Based on the uploaded document...\"\n",
    "   - The model will search and cite relevant sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Advanced Features\n",
    "\n",
    "Let's explore some power features of Open WebUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced features overview\n",
    "\n",
    "advanced_features = {\n",
    "    \"Web Search\": {\n",
    "        \"description\": \"Let the model search the web for current information\",\n",
    "        \"setup\": \"Settings ‚Üí Web Search ‚Üí Enable (requires SearXNG or similar)\",\n",
    "        \"use_case\": \"Current events, recent updates, fact-checking\"\n",
    "    },\n",
    "    \"Function Calling\": {\n",
    "        \"description\": \"Define custom tools the model can use\",\n",
    "        \"setup\": \"Workspace ‚Üí Functions ‚Üí Create\",\n",
    "        \"use_case\": \"Calculations, API calls, custom integrations\"\n",
    "    },\n",
    "    \"Pipelines\": {\n",
    "        \"description\": \"Chain multiple models or add preprocessing\",\n",
    "        \"setup\": \"Settings ‚Üí Pipelines ‚Üí Configure\",\n",
    "        \"use_case\": \"Translation ‚Üí Response, RAG ‚Üí Generation\"\n",
    "    },\n",
    "    \"Voice Input\": {\n",
    "        \"description\": \"Speak to the model using Whisper\",\n",
    "        \"setup\": \"Settings ‚Üí Audio ‚Üí Enable STT (Whisper)\",\n",
    "        \"use_case\": \"Hands-free interaction, accessibility\"\n",
    "    },\n",
    "    \"Image Generation\": {\n",
    "        \"description\": \"Generate images with DALL-E or local models\",\n",
    "        \"setup\": \"Settings ‚Üí Images ‚Üí Configure backend\",\n",
    "        \"use_case\": \"Creative content, diagrams, illustrations\"\n",
    "    },\n",
    "    \"Arena Mode\": {\n",
    "        \"description\": \"Compare responses from multiple models\",\n",
    "        \"setup\": \"Toggle in chat interface\",\n",
    "        \"use_case\": \"Model evaluation, quality comparison\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üöÄ Advanced Open WebUI Features\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for feature, details in advanced_features.items():\n",
    "    print(f\"\\n‚ú® {feature}\")\n",
    "    print(f\"   Description: {details['description']}\")\n",
    "    print(f\"   Setup: {details['setup']}\")\n",
    "    print(f\"   Use case: {details['use_case']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Multi-User Setup\n",
    "\n",
    "For team deployments, you may want to enable multi-user features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-user configuration\n",
    "\n",
    "multiuser_config = {\n",
    "    \"User Management\": {\n",
    "        \"Admin Panel\": \"Accessible via Settings ‚Üí Admin (admin users only)\",\n",
    "        \"User Roles\": [\"Admin\", \"User\", \"Pending\"],\n",
    "        \"Registration\": \"Can be enabled/disabled in admin settings\"\n",
    "    },\n",
    "    \"Access Control\": {\n",
    "        \"Model Access\": \"Restrict which models users can access\",\n",
    "        \"Feature Toggles\": \"Enable/disable features per user role\",\n",
    "        \"Rate Limiting\": \"Per-user request limits\"\n",
    "    },\n",
    "    \"Data Isolation\": {\n",
    "        \"Chats\": \"Each user's chats are private by default\",\n",
    "        \"Documents\": \"Uploaded docs can be personal or shared\",\n",
    "        \"Presets\": \"Personal vs shared model presets\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üë• Multi-User Configuration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for category, details in multiuser_config.items():\n",
    "    print(f\"\\nüìå {category}:\")\n",
    "    for key, value in details.items():\n",
    "        if isinstance(value, list):\n",
    "            print(f\"   {key}: {', '.join(value)}\")\n",
    "        else:\n",
    "            print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Ollama Connection Issues\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong - Can't reach Ollama from container\n",
    "OLLAMA_BASE_URL=http://localhost:11434\n",
    "\n",
    "# ‚úÖ Right - Use Docker's host gateway\n",
    "OLLAMA_BASE_URL=http://host.docker.internal:11434\n",
    "\n",
    "# Or on Linux without Docker Desktop:\n",
    "--network=host\n",
    "```\n",
    "\n",
    "### Mistake 2: Data Not Persisting\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong - Data lost when container restarts\n",
    "docker run -d ghcr.io/open-webui/open-webui:main\n",
    "\n",
    "# ‚úÖ Right - Mount a volume\n",
    "docker run -d -v ~/.open-webui:/app/backend/data ghcr.io/open-webui/open-webui:main\n",
    "```\n",
    "\n",
    "### Mistake 3: RAG Not Working\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong - Embedding model not installed\n",
    "# RAG uploads but retrieval fails\n",
    "\n",
    "# ‚úÖ Right - Install embedding model first\n",
    "ollama pull nomic-embed-text\n",
    "# Then configure in Settings ‚Üí Documents\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Create a Custom Preset\n",
    "\n",
    "Create a model preset for a specific use case you care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Design your custom preset\n",
    "\n",
    "my_preset = {\n",
    "    \"name\": \"My Custom Assistant\",  # TODO: Name it\n",
    "    \"base_model\": \"llama3.1:8b\",\n",
    "    \"system_prompt\": \"\"\"TODO: Write your system prompt here.\n",
    "    \n",
    "Think about:\n",
    "- What role should the assistant have?\n",
    "- What style should it use?\n",
    "- What constraints or guidelines?\n",
    "\"\"\",\n",
    "    \"parameters\": {\n",
    "        \"temperature\": 0.7,  # TODO: Adjust for your use case\n",
    "        \"max_tokens\": 1024,  # TODO: Adjust based on expected response length\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print for copying into Open WebUI\n",
    "print(\"üìù Your Custom Preset:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Name: {my_preset['name']}\")\n",
    "print(f\"System Prompt:\\n{my_preset['system_prompt']}\")\n",
    "print(f\"\\nParameters: {my_preset['parameters']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Set Up RAG\n",
    "\n",
    "Configure RAG and upload a document to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: RAG testing checklist\n",
    "\n",
    "rag_checklist = [\n",
    "    \"[ ] Pull nomic-embed-text: ollama pull nomic-embed-text\",\n",
    "    \"[ ] Configure embedding model in Settings ‚Üí Documents\",\n",
    "    \"[ ] Upload a test document (PDF, TXT, or MD)\",\n",
    "    \"[ ] Ask a question that requires document knowledge\",\n",
    "    \"[ ] Verify citations are shown in the response\",\n",
    "]\n",
    "\n",
    "print(\"üìã RAG Setup Checklist:\")\n",
    "for item in rag_checklist:\n",
    "    print(f\"   {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How to set up Open WebUI for Ollama\n",
    "- ‚úÖ Creating and configuring model presets\n",
    "- ‚úÖ Setting up RAG for document uploads\n",
    "- ‚úÖ Advanced features like web search and function calling\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Build a Custom Knowledge Base**\n",
    "\n",
    "Create a specialized assistant by:\n",
    "1. Uploading 10+ related documents\n",
    "2. Creating a custom preset with domain-specific system prompt\n",
    "3. Testing retrieval quality\n",
    "4. Documenting your findings\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [Open WebUI Documentation](https://docs.openwebui.com/)\n",
    "- [Open WebUI GitHub](https://github.com/open-webui/open-webui)\n",
    "- [Ollama Model Library](https://ollama.com/library)\n",
    "- [RAG Best Practices](https://www.llamaindex.ai/blog/a-guide-to-rag-evaluation)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Module 12 Complete!\")\n",
    "print(\"\\nüìã What you've accomplished:\")\n",
    "print(\"   ‚úì Benchmarked inference engines\")\n",
    "print(\"   ‚úì Deployed vLLM with continuous batching\")\n",
    "print(\"   ‚úì Explored TensorRT-LLM optimization\")\n",
    "print(\"   ‚úì Learned speculative decoding\")\n",
    "print(\"   ‚úì Built a production FastAPI server\")\n",
    "print(\"   ‚úì Set up Open WebUI\")\n",
    "print(\"\\nüöÄ You're ready for production deployments!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}