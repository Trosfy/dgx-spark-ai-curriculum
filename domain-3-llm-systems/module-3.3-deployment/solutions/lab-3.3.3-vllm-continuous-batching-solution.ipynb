{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.3.3 Solutions: vLLM Continuous Batching\n",
    "\n",
    "Complete solutions to all exercises from the vLLM continuous batching lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path(\"../scripts\").resolve()))\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import requests\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "from benchmark_utils import InferenceBenchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Analyze PagedAttention Memory Efficiency\n",
    "\n",
    "**Task**: Calculate theoretical memory savings from PagedAttention vs. traditional pre-allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Model configuration for memory calculations.\"\"\"\n",
    "    name: str\n",
    "    hidden_size: int\n",
    "    num_layers: int\n",
    "    num_kv_heads: int\n",
    "    head_dim: int\n",
    "    max_seq_len: int\n",
    "    \n",
    "    @property\n",
    "    def kv_cache_per_token_bytes(self) -> int:\n",
    "        \"\"\"Calculate KV cache bytes per token.\"\"\"\n",
    "        # For each token: 2 (K and V) * num_layers * num_kv_heads * head_dim * 2 (bf16)\n",
    "        return 2 * self.num_layers * self.num_kv_heads * self.head_dim * 2\n",
    "\n",
    "# Common model configurations\n",
    "MODELS = {\n",
    "    \"llama-3.1-8b\": ModelConfig(\n",
    "        name=\"Llama 3.1 8B\",\n",
    "        hidden_size=4096,\n",
    "        num_layers=32,\n",
    "        num_kv_heads=8,  # GQA\n",
    "        head_dim=128,\n",
    "        max_seq_len=131072\n",
    "    ),\n",
    "    \"llama-3.1-70b\": ModelConfig(\n",
    "        name=\"Llama 3.1 70B\",\n",
    "        hidden_size=8192,\n",
    "        num_layers=80,\n",
    "        num_kv_heads=8,  # GQA\n",
    "        head_dim=128,\n",
    "        max_seq_len=131072\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "def calculate_memory_comparison(model: ModelConfig, \n",
    "                                batch_size: int,\n",
    "                                avg_seq_len: int,\n",
    "                                paged_block_size: int = 16) -> dict:\n",
    "    \"\"\"\n",
    "    Compare memory usage: traditional pre-allocation vs PagedAttention.\n",
    "    \n",
    "    Traditional approach: Pre-allocate max_seq_len for every request\n",
    "    PagedAttention: Allocate blocks on-demand as tokens are generated\n",
    "    \"\"\"\n",
    "    bytes_per_token = model.kv_cache_per_token_bytes\n",
    "    \n",
    "    # Traditional: Pre-allocate max length for all requests\n",
    "    traditional_memory = batch_size * model.max_seq_len * bytes_per_token\n",
    "    \n",
    "    # PagedAttention: Only allocate what's actually used\n",
    "    # Plus some overhead for block table management\n",
    "    actual_tokens = batch_size * avg_seq_len\n",
    "    \n",
    "    # Round up to block boundaries\n",
    "    blocks_needed = (avg_seq_len + paged_block_size - 1) // paged_block_size\n",
    "    paged_tokens = batch_size * blocks_needed * paged_block_size\n",
    "    paged_memory = paged_tokens * bytes_per_token\n",
    "    \n",
    "    # Block table overhead (minimal)\n",
    "    block_table_overhead = batch_size * blocks_needed * 8  # 8 bytes per pointer\n",
    "    paged_memory += block_table_overhead\n",
    "    \n",
    "    savings = traditional_memory - paged_memory\n",
    "    savings_ratio = savings / traditional_memory\n",
    "    \n",
    "    # How many more requests could we serve with saved memory?\n",
    "    additional_requests = savings // (paged_memory // batch_size)\n",
    "    \n",
    "    return {\n",
    "        \"model\": model.name,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"avg_seq_len\": avg_seq_len,\n",
    "        \"max_seq_len\": model.max_seq_len,\n",
    "        \"traditional_memory_gb\": traditional_memory / (1024**3),\n",
    "        \"paged_memory_gb\": paged_memory / (1024**3),\n",
    "        \"savings_gb\": savings / (1024**3),\n",
    "        \"savings_percent\": savings_ratio * 100,\n",
    "        \"additional_capacity\": int(additional_requests),\n",
    "        \"throughput_multiplier\": (batch_size + additional_requests) / batch_size\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_paged_attention_benefits():\n",
    "    \"\"\"Analyze PagedAttention benefits across different scenarios.\"\"\"\n",
    "    \n",
    "    print(\"üìä PagedAttention Memory Efficiency Analysis\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    scenarios = [\n",
    "        # (model_key, batch_size, avg_seq_len)\n",
    "        (\"llama-3.1-8b\", 32, 512),    # Short responses\n",
    "        (\"llama-3.1-8b\", 32, 2048),   # Medium responses\n",
    "        (\"llama-3.1-8b\", 32, 8192),   # Long responses\n",
    "        (\"llama-3.1-70b\", 8, 512),    # Larger model, fewer concurrent\n",
    "        (\"llama-3.1-70b\", 8, 4096),   # Larger model, medium length\n",
    "    ]\n",
    "    \n",
    "    for model_key, batch_size, avg_len in scenarios:\n",
    "        model = MODELS[model_key]\n",
    "        result = calculate_memory_comparison(model, batch_size, avg_len)\n",
    "        \n",
    "        print(f\"\\n{model.name} | Batch: {batch_size} | Avg Length: {avg_len:,}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"  Traditional (pre-alloc {model.max_seq_len:,} tokens):\")\n",
    "        print(f\"    Memory: {result['traditional_memory_gb']:.1f} GB\")\n",
    "        print(f\"  PagedAttention (actual usage):\")\n",
    "        print(f\"    Memory: {result['paged_memory_gb']:.2f} GB\")\n",
    "        print(f\"  Savings: {result['savings_gb']:.1f} GB ({result['savings_percent']:.1f}%)\")\n",
    "        print(f\"  Throughput potential: {result['throughput_multiplier']:.1f}x\")\n",
    "        print(f\"    (Could serve {result['additional_capacity']} additional requests)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üí° Key Insights:\")\n",
    "    print(\"  ‚Ä¢ Shorter responses = more memory savings\")\n",
    "    print(\"  ‚Ä¢ Savings increase with max_seq_len / avg_seq_len ratio\")\n",
    "    print(\"  ‚Ä¢ PagedAttention enables higher concurrent request capacity\")\n",
    "    print(\"  ‚Ä¢ Memory fragmentation eliminated by block-based allocation\")\n",
    "\n",
    "analyze_paged_attention_benefits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Implement Batch Size Optimizer\n",
    "\n",
    "**Task**: Create a system that dynamically finds the optimal batch size for maximum throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass BatchTestResult:\n    \"\"\"Results from testing a specific batch size.\"\"\"\n    batch_size: int\n    throughput_tps: float  # tokens per second\n    avg_latency_ms: float\n    p99_latency_ms: float\n    error_rate: float\n    gpu_memory_used_gb: float\n\n\nclass BatchSizeOptimizer:\n    \"\"\"\n    Find optimal batch size for vLLM through binary search.\n    \n    The optimal batch size maximizes throughput while keeping:\n    - Latency below a target threshold\n    - Error rate at zero\n    - GPU memory below limit\n    \"\"\"\n    \n    def __init__(self, \n                 base_url: str = \"http://localhost:8000\",\n                 max_latency_ms: float = 5000,\n                 max_memory_gb: float = 100):\n        self.base_url = base_url\n        self.max_latency_ms = max_latency_ms\n        self.max_memory_gb = max_memory_gb\n        self.results_history: List[BatchTestResult] = []\n    \n    def test_batch_size(self, batch_size: int, \n                        test_prompt: str = \"Explain AI in one sentence.\",\n                        max_tokens: int = 50,\n                        num_requests: int = 100) -> BatchTestResult:\n        \"\"\"\n        Test a specific batch size and measure performance.\n        \"\"\"\n        latencies = []\n        errors = 0\n        total_tokens = 0\n        \n        start_time = time.time()\n        \n        def make_request():\n            try:\n                req_start = time.time()\n                response = requests.post(\n                    f\"{self.base_url}/v1/completions\",\n                    json={\n                        \"model\": \"default\",\n                        \"prompt\": test_prompt,\n                        \"max_tokens\": max_tokens,\n                        \"temperature\": 0.7\n                    },\n                    timeout=30\n                )\n                latency = (time.time() - req_start) * 1000\n                \n                if response.status_code == 200:\n                    data = response.json()\n                    tokens = data.get(\"usage\", {}).get(\"completion_tokens\", max_tokens)\n                    return latency, tokens, None\n                else:\n                    return latency, 0, response.status_code\n            except Exception as e:\n                return 0, 0, str(e)\n        \n        # Run concurrent requests\n        with ThreadPoolExecutor(max_workers=batch_size) as executor:\n            futures = [executor.submit(make_request) for _ in range(num_requests)]\n            \n            for future in as_completed(futures):\n                latency, tokens, error = future.result()\n                if error:\n                    errors += 1\n                else:\n                    latencies.append(latency)\n                    total_tokens += tokens\n        \n        total_time = time.time() - start_time\n        \n        # Calculate metrics\n        result = BatchTestResult(\n            batch_size=batch_size,\n            throughput_tps=total_tokens / total_time if total_time > 0 else 0,\n            avg_latency_ms=np.mean(latencies) if latencies else 0,\n            p99_latency_ms=np.percentile(latencies, 99) if latencies else 0,\n            error_rate=errors / num_requests,\n            gpu_memory_used_gb=0  # Would need nvidia-smi or pynvml\n        )\n        \n        self.results_history.append(result)\n        return result\n    \n    def is_acceptable(self, result: BatchTestResult) -> bool:\n        \"\"\"Check if a batch size result meets our requirements.\"\"\"\n        return (\n            result.error_rate == 0 and\n            result.p99_latency_ms < self.max_latency_ms and\n            result.gpu_memory_used_gb < self.max_memory_gb\n        )\n    \n    def find_optimal(self, min_batch: int = 1, \n                     max_batch: int = 128) -> Tuple[int, BatchTestResult]:\n        \"\"\"\n        Find optimal batch size using binary search.\n        \n        Algorithm:\n        1. Start with a range [min_batch, max_batch]\n        2. Test the midpoint\n        3. If acceptable and throughput increasing, search higher\n        4. If not acceptable or throughput decreasing, search lower\n        5. Continue until range is small\n        \"\"\"\n        best_batch = min_batch\n        best_result = None\n        best_throughput = 0\n        \n        print(f\"üîç Searching for optimal batch size in [{min_batch}, {max_batch}]\")\n        print(\"-\" * 60)\n        \n        while min_batch <= max_batch:\n            mid = (min_batch + max_batch) // 2\n            \n            print(f\"Testing batch_size={mid}...\", end=\" \")\n            result = self.test_batch_size(mid)\n            \n            acceptable = self.is_acceptable(result)\n            print(f\"TPS: {result.throughput_tps:.1f}, \"\n                  f\"P99: {result.p99_latency_ms:.0f}ms, \"\n                  f\"OK: {acceptable}\")\n            \n            if acceptable and result.throughput_tps > best_throughput:\n                best_batch = mid\n                best_result = result\n                best_throughput = result.throughput_tps\n                min_batch = mid + 1  # Try higher\n            else:\n                max_batch = mid - 1  # Try lower\n        \n        return best_batch, best_result\n    \n    def generate_report(self) -> str:\n        \"\"\"Generate a report of all tested configurations.\"\"\"\n        lines = [\"\\nüìä Batch Size Optimization Report\", \"=\" * 60]\n        lines.append(f\"{'Batch':<8} {'TPS':>10} {'Avg Lat':>12} {'P99 Lat':>12} {'Errors':>8}\")\n        lines.append(\"-\" * 60)\n        \n        for r in sorted(self.results_history, key=lambda x: x.batch_size):\n            lines.append(\n                f\"{r.batch_size:<8} {r.throughput_tps:>10.1f} \"\n                f\"{r.avg_latency_ms:>10.0f}ms {r.p99_latency_ms:>10.0f}ms \"\n                f\"{r.error_rate:>7.1%}\"\n            )\n        \n        # Find optimal\n        valid = [r for r in self.results_history if self.is_acceptable(r)]\n        if valid:\n            best = max(valid, key=lambda x: x.throughput_tps)\n            lines.append(\"\\n\" + \"=\" * 60)\n            lines.append(f\"‚úÖ Optimal batch size: {best.batch_size}\")\n            lines.append(f\"   Throughput: {best.throughput_tps:.1f} tok/s\")\n            lines.append(f\"   P99 Latency: {best.p99_latency_ms:.0f}ms\")\n        \n        return \"\\n\".join(lines)\n\n\n# Example usage (with simulated results for demo)\nprint(\"üîß Batch Size Optimizer\")\nprint(\"   Run after starting vLLM server:\")\nprint(\"   vllm serve meta-llama/Llama-3.1-8B-Instruct --enforce-eager\")\nprint(\"\")\n\n# Simulated optimization results\nprint(\"Simulated optimization run:\")\nprint(\"-\" * 60)\n\nsimulated_results = [\n    BatchTestResult(8, 450.5, 142, 285, 0, 12.3),\n    BatchTestResult(16, 782.3, 165, 412, 0, 18.7),\n    BatchTestResult(32, 1105.8, 234, 678, 0, 28.4),\n    BatchTestResult(48, 1289.2, 312, 892, 0, 38.1),\n    BatchTestResult(64, 1356.7, 428, 1456, 0, 48.2),\n    BatchTestResult(96, 1298.4, 687, 2845, 0, 62.5),\n    BatchTestResult(128, 1189.2, 1024, 4521, 0.02, 78.3),\n]\n\nprint(f\"{'Batch':<8} {'TPS':>10} {'Avg Lat':>12} {'P99 Lat':>12}\")\nprint(\"-\" * 45)\nfor r in simulated_results:\n    marker = \"‚Üê\" if r.batch_size == 64 else \"\"\n    print(f\"{r.batch_size:<8} {r.throughput_tps:>10.1f} {r.avg_latency_ms:>10.0f}ms {r.p99_latency_ms:>10.0f}ms {marker}\")\n\nprint(\"\\n‚úÖ Optimal batch size: 64\")\nprint(\"   - Highest throughput with acceptable latency\")\nprint(\"   - Beyond 64, latency increases faster than throughput\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Implement Request Priority Queue\n",
    "\n",
    "**Task**: Create a priority-based request routing system for vLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import threading\n",
    "from enum import IntEnum\n",
    "from dataclasses import dataclass, field\n",
    "import uuid\n",
    "\n",
    "\n",
    "class Priority(IntEnum):\n",
    "    \"\"\"Request priority levels (lower = higher priority).\"\"\"\n",
    "    CRITICAL = 0   # Health checks, system requests\n",
    "    HIGH = 1       # Premium users, time-sensitive\n",
    "    NORMAL = 2     # Standard requests\n",
    "    LOW = 3        # Batch jobs, background tasks\n",
    "    BULK = 4       # Large batch processing\n",
    "\n",
    "\n",
    "@dataclass(order=True)\n",
    "class PrioritizedRequest:\n",
    "    \"\"\"A request with priority for queue ordering.\"\"\"\n",
    "    priority: int\n",
    "    timestamp: float = field(compare=True)  # For FIFO within priority\n",
    "    request_id: str = field(compare=False, default_factory=lambda: str(uuid.uuid4())[:8])\n",
    "    prompt: str = field(compare=False, default=\"\")\n",
    "    max_tokens: int = field(compare=False, default=100)\n",
    "    user_tier: str = field(compare=False, default=\"standard\")\n",
    "    \n",
    "\n",
    "class PriorityRequestQueue:\n",
    "    \"\"\"\n",
    "    Thread-safe priority queue for LLM requests.\n",
    "    \n",
    "    Features:\n",
    "    - Multi-level priority support\n",
    "    - Fair scheduling within priority levels (FIFO)\n",
    "    - Priority boosting for aging requests\n",
    "    - Rate limiting per priority level\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_age_boost_sec: float = 30):\n",
    "        self._queue: List[PrioritizedRequest] = []\n",
    "        self._lock = threading.Lock()\n",
    "        self._max_age_boost = max_age_boost_sec\n",
    "        self._stats = {\n",
    "            Priority.CRITICAL: {\"enqueued\": 0, \"processed\": 0},\n",
    "            Priority.HIGH: {\"enqueued\": 0, \"processed\": 0},\n",
    "            Priority.NORMAL: {\"enqueued\": 0, \"processed\": 0},\n",
    "            Priority.LOW: {\"enqueued\": 0, \"processed\": 0},\n",
    "            Priority.BULK: {\"enqueued\": 0, \"processed\": 0},\n",
    "        }\n",
    "    \n",
    "    def enqueue(self, prompt: str, priority: Priority = Priority.NORMAL,\n",
    "                max_tokens: int = 100, user_tier: str = \"standard\") -> str:\n",
    "        \"\"\"Add a request to the queue.\"\"\"\n",
    "        request = PrioritizedRequest(\n",
    "            priority=priority,\n",
    "            timestamp=time.time(),\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            user_tier=user_tier\n",
    "        )\n",
    "        \n",
    "        with self._lock:\n",
    "            heapq.heappush(self._queue, request)\n",
    "            self._stats[priority][\"enqueued\"] += 1\n",
    "        \n",
    "        return request.request_id\n",
    "    \n",
    "    def dequeue(self) -> Optional[PrioritizedRequest]:\n",
    "        \"\"\"Get the highest priority request.\"\"\"\n",
    "        with self._lock:\n",
    "            if not self._queue:\n",
    "                return None\n",
    "            \n",
    "            # Apply priority boosting for aged requests\n",
    "            self._apply_aging_boost()\n",
    "            \n",
    "            request = heapq.heappop(self._queue)\n",
    "            self._stats[Priority(request.priority)][\"processed\"] += 1\n",
    "            return request\n",
    "    \n",
    "    def _apply_aging_boost(self):\n",
    "        \"\"\"Boost priority of requests that have been waiting too long.\"\"\"\n",
    "        current_time = time.time()\n",
    "        boosted = False\n",
    "        \n",
    "        for request in self._queue:\n",
    "            age = current_time - request.timestamp\n",
    "            if age > self._max_age_boost and request.priority > Priority.CRITICAL:\n",
    "                # Boost by one level\n",
    "                request.priority = max(Priority.CRITICAL, request.priority - 1)\n",
    "                boosted = True\n",
    "        \n",
    "        if boosted:\n",
    "            heapq.heapify(self._queue)\n",
    "    \n",
    "    def get_queue_depth(self) -> Dict[Priority, int]:\n",
    "        \"\"\"Get current queue depth by priority.\"\"\"\n",
    "        with self._lock:\n",
    "            depths = {p: 0 for p in Priority}\n",
    "            for req in self._queue:\n",
    "                depths[Priority(req.priority)] += 1\n",
    "            return depths\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Get queue statistics.\"\"\"\n",
    "        with self._lock:\n",
    "            return {\n",
    "                \"queue_size\": len(self._queue),\n",
    "                \"by_priority\": dict(self._stats)\n",
    "            }\n",
    "\n",
    "\n",
    "class PriorityRouter:\n",
    "    \"\"\"\n",
    "    Routes requests to vLLM with priority handling.\n",
    "    \n",
    "    Implements:\n",
    "    - Weighted fair queuing\n",
    "    - Rate limiting per tier\n",
    "    - Graceful degradation under load\n",
    "    \"\"\"\n",
    "    \n",
    "    # Weights for weighted fair queuing (higher = more share)\n",
    "    PRIORITY_WEIGHTS = {\n",
    "        Priority.CRITICAL: 100,\n",
    "        Priority.HIGH: 50,\n",
    "        Priority.NORMAL: 20,\n",
    "        Priority.LOW: 5,\n",
    "        Priority.BULK: 1,\n",
    "    }\n",
    "    \n",
    "    def __init__(self, vllm_url: str = \"http://localhost:8000\",\n",
    "                 max_concurrent: int = 64):\n",
    "        self.vllm_url = vllm_url\n",
    "        self.max_concurrent = max_concurrent\n",
    "        self.queue = PriorityRequestQueue()\n",
    "        self._active_requests = 0\n",
    "        self._lock = threading.Lock()\n",
    "    \n",
    "    def submit(self, prompt: str, priority: Priority = Priority.NORMAL,\n",
    "               max_tokens: int = 100, user_tier: str = \"standard\") -> str:\n",
    "        \"\"\"Submit a request for processing.\"\"\"\n",
    "        # Determine priority based on user tier if not specified\n",
    "        if user_tier == \"premium\" and priority == Priority.NORMAL:\n",
    "            priority = Priority.HIGH\n",
    "        elif user_tier == \"batch\" and priority == Priority.NORMAL:\n",
    "            priority = Priority.LOW\n",
    "        \n",
    "        return self.queue.enqueue(prompt, priority, max_tokens, user_tier)\n",
    "    \n",
    "    def should_accept_request(self, priority: Priority) -> bool:\n",
    "        \"\"\"\n",
    "        Determine if we should accept a new request based on load.\n",
    "        \n",
    "        Under high load, lower priority requests are rejected.\n",
    "        \"\"\"\n",
    "        with self._lock:\n",
    "            load_ratio = self._active_requests / self.max_concurrent\n",
    "        \n",
    "        # Load shedding thresholds\n",
    "        thresholds = {\n",
    "            Priority.CRITICAL: 1.0,   # Always accept\n",
    "            Priority.HIGH: 0.95,      # Reject at 95% load\n",
    "            Priority.NORMAL: 0.85,    # Reject at 85% load\n",
    "            Priority.LOW: 0.70,       # Reject at 70% load\n",
    "            Priority.BULK: 0.50,      # Reject at 50% load\n",
    "        }\n",
    "        \n",
    "        return load_ratio < thresholds[priority]\n",
    "    \n",
    "    def get_load_status(self) -> dict:\n",
    "        \"\"\"Get current load status.\"\"\"\n",
    "        with self._lock:\n",
    "            load = self._active_requests / self.max_concurrent\n",
    "        \n",
    "        return {\n",
    "            \"active_requests\": self._active_requests,\n",
    "            \"max_concurrent\": self.max_concurrent,\n",
    "            \"load_percent\": load * 100,\n",
    "            \"queue_depth\": self.queue.get_queue_depth(),\n",
    "            \"accepting\": {\n",
    "                p.name: self.should_accept_request(p)\n",
    "                for p in Priority\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "# Demonstrate the priority system\n",
    "print(\"üéØ Priority Request Queue Demo\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "queue = PriorityRequestQueue()\n",
    "\n",
    "# Simulate incoming requests with different priorities\n",
    "requests_to_add = [\n",
    "    (\"What is AI?\", Priority.NORMAL, \"standard\"),\n",
    "    (\"Health check\", Priority.CRITICAL, \"system\"),\n",
    "    (\"Process this batch\", Priority.BULK, \"batch\"),\n",
    "    (\"Urgent query\", Priority.HIGH, \"premium\"),\n",
    "    (\"Another normal request\", Priority.NORMAL, \"standard\"),\n",
    "    (\"Background job\", Priority.LOW, \"batch\"),\n",
    "]\n",
    "\n",
    "print(\"\\nEnqueuing requests:\")\n",
    "for prompt, priority, tier in requests_to_add:\n",
    "    req_id = queue.enqueue(prompt, priority, user_tier=tier)\n",
    "    print(f\"  [{priority.name:<10}] {prompt[:30]} ‚Üí {req_id}\")\n",
    "\n",
    "print(f\"\\nQueue depth by priority: {queue.get_queue_depth()}\")\n",
    "\n",
    "print(\"\\nProcessing order (highest priority first):\")\n",
    "while True:\n",
    "    request = queue.dequeue()\n",
    "    if request is None:\n",
    "        break\n",
    "    priority_name = Priority(request.priority).name\n",
    "    print(f\"  [{priority_name:<10}] {request.prompt[:30]}\")\n",
    "\n",
    "print(\"\\n‚úÖ All requests processed in priority order\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Compare Batching Strategies\n",
    "\n",
    "**Task**: Implement and compare static batching vs continuous batching throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class BatchingSimulator:\n",
    "    \"\"\"\n",
    "    Simulate and compare different batching strategies.\n",
    "    \n",
    "    This helps understand WHY continuous batching is better.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 decode_time_per_token_ms: float = 10,\n",
    "                 prefill_time_per_token_ms: float = 0.5):\n",
    "        self.decode_time = decode_time_per_token_ms\n",
    "        self.prefill_time = prefill_time_per_token_ms\n",
    "    \n",
    "    def simulate_static_batching(self, \n",
    "                                  requests: List[Dict],\n",
    "                                  batch_size: int) -> dict:\n",
    "        \"\"\"\n",
    "        Simulate static batching: wait for full batch, process together.\n",
    "        \n",
    "        Problem: Short requests wait for long ones to complete.\n",
    "        \"\"\"\n",
    "        total_time = 0\n",
    "        total_tokens = 0\n",
    "        latencies = []\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, len(requests), batch_size):\n",
    "            batch = requests[i:i+batch_size]\n",
    "            \n",
    "            # Find the longest request in batch\n",
    "            max_output_tokens = max(r[\"output_tokens\"] for r in batch)\n",
    "            max_input_tokens = max(r[\"input_tokens\"] for r in batch)\n",
    "            \n",
    "            # Batch time = time for longest request\n",
    "            prefill_time = max_input_tokens * self.prefill_time\n",
    "            decode_time = max_output_tokens * self.decode_time\n",
    "            batch_time = prefill_time + decode_time\n",
    "            \n",
    "            total_time += batch_time\n",
    "            \n",
    "            # All requests in batch have same latency (wait for longest)\n",
    "            for r in batch:\n",
    "                latencies.append(batch_time)\n",
    "                total_tokens += r[\"output_tokens\"]\n",
    "        \n",
    "        return {\n",
    "            \"strategy\": \"Static Batching\",\n",
    "            \"total_time_ms\": total_time,\n",
    "            \"throughput_tps\": total_tokens / (total_time / 1000),\n",
    "            \"avg_latency_ms\": np.mean(latencies),\n",
    "            \"p99_latency_ms\": np.percentile(latencies, 99),\n",
    "            \"efficiency\": total_tokens / (total_time * batch_size) * 1000\n",
    "        }\n",
    "    \n",
    "    def simulate_continuous_batching(self,\n",
    "                                      requests: List[Dict],\n",
    "                                      max_batch_size: int) -> dict:\n",
    "        \"\"\"\n",
    "        Simulate continuous batching: requests exit as they complete.\n",
    "        \n",
    "        Advantage: Short requests don't wait for long ones.\n",
    "        \"\"\"\n",
    "        # Sort by output length to simulate scheduling\n",
    "        sorted_requests = sorted(requests, key=lambda x: x[\"output_tokens\"])\n",
    "        \n",
    "        total_time = 0\n",
    "        total_tokens = 0\n",
    "        latencies = []\n",
    "        \n",
    "        # Simulate token-by-token generation with dynamic batching\n",
    "        active_requests = []\n",
    "        request_queue = list(sorted_requests)\n",
    "        \n",
    "        while request_queue or active_requests:\n",
    "            # Add new requests up to batch size\n",
    "            while len(active_requests) < max_batch_size and request_queue:\n",
    "                req = request_queue.pop(0)\n",
    "                active_requests.append({\n",
    "                    \"remaining\": req[\"output_tokens\"],\n",
    "                    \"total\": req[\"output_tokens\"],\n",
    "                    \"start_time\": total_time\n",
    "                })\n",
    "            \n",
    "            if not active_requests:\n",
    "                break\n",
    "            \n",
    "            # Generate one token for all active requests\n",
    "            step_time = self.decode_time  # Parallel decode\n",
    "            total_time += step_time\n",
    "            \n",
    "            # Update all requests\n",
    "            completed = []\n",
    "            for req in active_requests:\n",
    "                req[\"remaining\"] -= 1\n",
    "                total_tokens += 1\n",
    "                \n",
    "                if req[\"remaining\"] <= 0:\n",
    "                    completed.append(req)\n",
    "                    latency = total_time - req[\"start_time\"]\n",
    "                    latencies.append(latency)\n",
    "            \n",
    "            # Remove completed, make room for new\n",
    "            for req in completed:\n",
    "                active_requests.remove(req)\n",
    "        \n",
    "        return {\n",
    "            \"strategy\": \"Continuous Batching\",\n",
    "            \"total_time_ms\": total_time,\n",
    "            \"throughput_tps\": total_tokens / (total_time / 1000),\n",
    "            \"avg_latency_ms\": np.mean(latencies),\n",
    "            \"p99_latency_ms\": np.percentile(latencies, 99),\n",
    "            \"efficiency\": 0.95  # Near-optimal GPU utilization\n",
    "        }\n",
    "\n",
    "\n",
    "# Run comparison\n",
    "print(\"üìä Batching Strategy Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate realistic request distribution\n",
    "# Mix of short (10-50), medium (50-200), and long (200-500) responses\n",
    "random.seed(42)\n",
    "requests = []\n",
    "for _ in range(100):\n",
    "    output_len = random.choice([\n",
    "        random.randint(10, 50),    # 50% short\n",
    "        random.randint(10, 50),\n",
    "        random.randint(50, 200),   # 30% medium\n",
    "        random.randint(200, 500),  # 20% long\n",
    "    ])\n",
    "    requests.append({\n",
    "        \"input_tokens\": random.randint(50, 200),\n",
    "        \"output_tokens\": output_len\n",
    "    })\n",
    "\n",
    "simulator = BatchingSimulator()\n",
    "\n",
    "# Compare strategies\n",
    "static_result = simulator.simulate_static_batching(requests, batch_size=16)\n",
    "continuous_result = simulator.simulate_continuous_batching(requests, max_batch_size=16)\n",
    "\n",
    "print(f\"\\nTest: {len(requests)} requests with mixed output lengths\")\n",
    "print(f\"Output length distribution: 10-500 tokens\\n\")\n",
    "\n",
    "print(f\"{'Metric':<25} {'Static':<20} {'Continuous':<20}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Total Time':<25} {static_result['total_time_ms']:>15,.0f}ms {continuous_result['total_time_ms']:>15,.0f}ms\")\n",
    "print(f\"{'Throughput':<25} {static_result['throughput_tps']:>15,.1f} t/s {continuous_result['throughput_tps']:>15,.1f} t/s\")\n",
    "print(f\"{'Avg Latency':<25} {static_result['avg_latency_ms']:>15,.0f}ms {continuous_result['avg_latency_ms']:>15,.0f}ms\")\n",
    "print(f\"{'P99 Latency':<25} {static_result['p99_latency_ms']:>15,.0f}ms {continuous_result['p99_latency_ms']:>15,.0f}ms\")\n",
    "\n",
    "improvement = (continuous_result['throughput_tps'] / static_result['throughput_tps'] - 1) * 100\n",
    "latency_improvement = (1 - continuous_result['avg_latency_ms'] / static_result['avg_latency_ms']) * 100\n",
    "\n",
    "print(f\"\\n‚úÖ Continuous Batching Improvements:\")\n",
    "print(f\"   ‚Ä¢ {improvement:.1f}% higher throughput\")\n",
    "print(f\"   ‚Ä¢ {latency_improvement:.1f}% lower average latency\")\n",
    "print(f\"\\nüí° Key Insight: Short requests don't wait for long ones!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **PagedAttention saves memory**: Only allocate what you need, when you need it\n",
    "\n",
    "2. **Batch size optimization**: Binary search to find the sweet spot between throughput and latency\n",
    "\n",
    "3. **Priority queuing**: Critical for production - premium users shouldn't wait behind batch jobs\n",
    "\n",
    "4. **Continuous batching wins**: \n",
    "   - Higher throughput (GPU always busy)\n",
    "   - Lower latency (short requests exit early)\n",
    "   - Better user experience\n",
    "\n",
    "5. **vLLM on DGX Spark**:\n",
    "   - Use `--enforce-eager` for ARM64 compatibility\n",
    "   - `--dtype bfloat16` for optimal performance\n",
    "   - Monitor GPU memory to tune batch sizes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}