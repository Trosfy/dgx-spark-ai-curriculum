{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 12.1 Solutions: Engine Benchmark\n",
    "\n",
    "This notebook contains solutions to the exercises from the Engine Benchmark task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Test Different Prompt Lengths\n",
    "\n",
    "Test how different engines handle prompts of varying lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path(\"../scripts\").resolve()))\n",
    "\n",
    "from benchmark_utils import InferenceBenchmark\n",
    "import numpy as np\n",
    "\n",
    "# Create prompts of different lengths\n",
    "def generate_prompt(target_tokens: int) -> str:\n",
    "    \"\"\"Generate a prompt with approximately the target number of tokens.\"\"\"\n",
    "    base = \"Summarize the following text: \"\n",
    "    filler = \"The quick brown fox jumps over the lazy dog. \" * (target_tokens // 10)\n",
    "    return base + filler\n",
    "\n",
    "prompt_lengths = {\n",
    "    \"short\": generate_prompt(20),\n",
    "    \"medium\": generate_prompt(100),\n",
    "    \"long\": generate_prompt(500),\n",
    "}\n",
    "\n",
    "# Benchmark each length\n",
    "def benchmark_prompt_lengths(engine: str, model: str):\n",
    "    \"\"\"Benchmark an engine with different prompt lengths.\"\"\"\n",
    "    benchmark = InferenceBenchmark(engine=engine, model=model)\n",
    "    benchmark.warmup(3)\n",
    "    \n",
    "    results = {}\n",
    "    for name, prompt in prompt_lengths.items():\n",
    "        # Run 3 times and average\n",
    "        ttfts = []\n",
    "        for _ in range(3):\n",
    "            result = benchmark.run_single(prompt, max_tokens=50, stream=True)\n",
    "            if result.error is None:\n",
    "                ttfts.append(result.time_to_first_token * 1000)  # ms\n",
    "        \n",
    "        if ttfts:\n",
    "            results[name] = {\n",
    "                \"avg_ttft_ms\": np.mean(ttfts),\n",
    "                \"prompt_length\": len(prompt.split())\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmarks (if Ollama is running)\n",
    "try:\n",
    "    results = benchmark_prompt_lengths(\"ollama\", \"llama3.1:8b\")\n",
    "    \n",
    "    print(\"ðŸ“Š Prompt Length vs TTFT (Ollama)\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"{'Length':<12} {'Words':<10} {'Avg TTFT (ms)'}\")\n",
    "    print(\"-\" * 40)\n",
    "    for name, data in results.items():\n",
    "        print(f\"{name:<12} {data['prompt_length']:<10} {data['avg_ttft_ms']:.1f}\")\n",
    "    \n",
    "    # Analysis\n",
    "    print(\"\\nðŸ’¡ Insights:\")\n",
    "    print(\"   - TTFT increases with prompt length (prefill takes longer)\")\n",
    "    print(\"   - The relationship is roughly linear for typical prompts\")\n",
    "    print(\"   - TensorRT-LLM would show the smallest increase (best prefill)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not run benchmark: {e}\")\n",
    "    print(\"Make sure Ollama is running with: ollama serve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Find the Saturation Point\n",
    "\n",
    "Find the concurrency level where latency starts to degrade significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_utils import InferenceBenchmark\n",
    "\n",
    "def find_saturation_point(engine: str, model: str, max_concurrency: int = 64):\n",
    "    \"\"\"\n",
    "    Find the concurrency level where latency starts degrading significantly.\n",
    "    \n",
    "    The \"knee\" in the curve is where:\n",
    "    - Throughput gains diminish\n",
    "    - Latency starts increasing faster than linearly\n",
    "    \"\"\"\n",
    "    benchmark = InferenceBenchmark(engine=engine, model=model)\n",
    "    benchmark.warmup(3)\n",
    "    \n",
    "    # Test prompts\n",
    "    prompts = [\"What is AI?\"] * 20\n",
    "    \n",
    "    # Test concurrency levels\n",
    "    concurrency_levels = [1, 2, 4, 8, 16, 32, 64]\n",
    "    concurrency_levels = [c for c in concurrency_levels if c <= max_concurrency]\n",
    "    \n",
    "    results = {}\n",
    "    previous_throughput = 0\n",
    "    saturation_point = None\n",
    "    \n",
    "    for concurrency in concurrency_levels:\n",
    "        result = benchmark.run_batch(\n",
    "            prompts=prompts,\n",
    "            max_tokens=50,\n",
    "            concurrency=concurrency,\n",
    "            stream=False\n",
    "        )\n",
    "        \n",
    "        results[concurrency] = {\n",
    "            \"throughput\": result.throughput_rps,\n",
    "            \"p90_latency_ms\": result.p90_latency * 1000,\n",
    "            \"avg_ttft_ms\": result.avg_ttft * 1000\n",
    "        }\n",
    "        \n",
    "        # Check for saturation (throughput gains < 20%)\n",
    "        if previous_throughput > 0:\n",
    "            gain = (result.throughput_rps - previous_throughput) / previous_throughput\n",
    "            if gain < 0.2 and saturation_point is None:\n",
    "                saturation_point = concurrency\n",
    "        \n",
    "        previous_throughput = result.throughput_rps\n",
    "    \n",
    "    return results, saturation_point\n",
    "\n",
    "# Run the analysis\n",
    "try:\n",
    "    results, saturation = find_saturation_point(\"ollama\", \"llama3.1:8b\", max_concurrency=16)\n",
    "    \n",
    "    print(\"ðŸ“Š Concurrency Saturation Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Concurrency':<12} {'Throughput':>12} {'P90 Latency':>15} {'Avg TTFT':>12}\")\n",
    "    print(\"-\" * 60)\n",
    "    for conc, data in results.items():\n",
    "        print(f\"{conc:<12} {data['throughput']:>10.2f}/s {data['p90_latency_ms']:>12.0f}ms {data['avg_ttft_ms']:>10.0f}ms\")\n",
    "    \n",
    "    if saturation:\n",
    "        print(f\"\\nðŸŽ¯ Saturation Point: {saturation} concurrent requests\")\n",
    "        print(\"   Beyond this, throughput gains diminish significantly.\")\n",
    "        print(f\"   Recommended operating concurrency: {saturation // 2} - {saturation}\")\n",
    "    else:\n",
    "        print(\"\\nðŸ’¡ No clear saturation point found within tested range.\")\n",
    "        print(\"   The engine may be able to handle higher concurrency.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not run analysis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Prompt Length Impact**: Longer prompts increase TTFT due to prefill processing. TensorRT-LLM handles this best.\n",
    "\n",
    "2. **Saturation Point**: Every engine has a point where adding more concurrency provides diminishing returns. Operating below this point ensures stable latency.\n",
    "\n",
    "3. **Trade-offs**:\n",
    "   - Low concurrency: Best per-request latency\n",
    "   - High concurrency (below saturation): Best throughput\n",
    "   - Above saturation: Degraded performance for everyone\n",
    "\n",
    "4. **Engine Comparison**:\n",
    "   - Ollama: Great for development, saturates early\n",
    "   - vLLM: Handles high concurrency well with continuous batching\n",
    "   - TensorRT-LLM: Best prefill speed, moderate decode\n",
    "   - SGLang: Good all-rounder with speculative decoding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
