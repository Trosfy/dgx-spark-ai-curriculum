{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.3.7 Solutions: Production API with FastAPI\n",
    "\n",
    "Complete solutions to all exercises from the production API lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path(\"../scripts\").resolve()))\n",
    "\n",
    "import asyncio\n",
    "import time\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional, AsyncGenerator\n",
    "from datetime import datetime, timedelta\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Implement Rate Limiting\n",
    "\n",
    "**Task**: Create a production-ready rate limiter with multiple strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import threading\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RateLimitConfig:\n",
    "    \"\"\"Configuration for rate limiting.\"\"\"\n",
    "    requests_per_minute: int = 60\n",
    "    requests_per_hour: int = 1000\n",
    "    tokens_per_minute: int = 100000\n",
    "    concurrent_requests: int = 10\n",
    "    burst_multiplier: float = 1.5\n",
    "\n",
    "\n",
    "class TokenBucketRateLimiter:\n",
    "    \"\"\"\n",
    "    Token bucket rate limiter with support for:\n",
    "    - Multiple time windows (per-minute, per-hour)\n",
    "    - Token-based limits (for LLM usage)\n",
    "    - Burst allowance\n",
    "    - Concurrent request limiting\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: RateLimitConfig = None):\n",
    "        self.config = config or RateLimitConfig()\n",
    "        self._lock = threading.Lock()\n",
    "        \n",
    "        # Per-client tracking\n",
    "        self._request_counts: Dict[str, List[float]] = defaultdict(list)\n",
    "        self._token_counts: Dict[str, List[tuple]] = defaultdict(list)\n",
    "        self._concurrent: Dict[str, int] = defaultdict(int)\n",
    "    \n",
    "    def _clean_old_entries(self, entries: list, window_seconds: int) -> list:\n",
    "        \"\"\"Remove entries older than the window.\"\"\"\n",
    "        cutoff = time.time() - window_seconds\n",
    "        return [e for e in entries if (e[0] if isinstance(e, tuple) else e) > cutoff]\n",
    "    \n",
    "    def check_rate_limit(self, client_id: str, \n",
    "                         estimated_tokens: int = 0) -> dict:\n",
    "        \"\"\"\n",
    "        Check if a request should be allowed.\n",
    "        \n",
    "        Returns:\n",
    "            dict with 'allowed', 'reason', and 'retry_after' if rejected\n",
    "        \"\"\"\n",
    "        with self._lock:\n",
    "            now = time.time()\n",
    "            \n",
    "            # Clean old entries\n",
    "            self._request_counts[client_id] = self._clean_old_entries(\n",
    "                self._request_counts[client_id], 3600  # 1 hour\n",
    "            )\n",
    "            self._token_counts[client_id] = self._clean_old_entries(\n",
    "                self._token_counts[client_id], 60  # 1 minute\n",
    "            )\n",
    "            \n",
    "            # Check concurrent requests\n",
    "            if self._concurrent[client_id] >= self.config.concurrent_requests:\n",
    "                return {\n",
    "                    \"allowed\": False,\n",
    "                    \"reason\": \"concurrent_limit\",\n",
    "                    \"message\": f\"Max {self.config.concurrent_requests} concurrent requests\",\n",
    "                    \"retry_after\": 1\n",
    "                }\n",
    "            \n",
    "            # Check requests per minute\n",
    "            minute_ago = now - 60\n",
    "            requests_last_minute = len([r for r in self._request_counts[client_id] if r > minute_ago])\n",
    "            \n",
    "            burst_limit = int(self.config.requests_per_minute * self.config.burst_multiplier)\n",
    "            if requests_last_minute >= burst_limit:\n",
    "                oldest = min([r for r in self._request_counts[client_id] if r > minute_ago])\n",
    "                retry_after = 60 - (now - oldest)\n",
    "                return {\n",
    "                    \"allowed\": False,\n",
    "                    \"reason\": \"rate_limit_minute\",\n",
    "                    \"message\": f\"Rate limit: {self.config.requests_per_minute}/min\",\n",
    "                    \"retry_after\": max(1, int(retry_after))\n",
    "                }\n",
    "            \n",
    "            # Check requests per hour\n",
    "            if len(self._request_counts[client_id]) >= self.config.requests_per_hour:\n",
    "                return {\n",
    "                    \"allowed\": False,\n",
    "                    \"reason\": \"rate_limit_hour\",\n",
    "                    \"message\": f\"Rate limit: {self.config.requests_per_hour}/hour\",\n",
    "                    \"retry_after\": 3600\n",
    "                }\n",
    "            \n",
    "            # Check token limit\n",
    "            tokens_last_minute = sum(t[1] for t in self._token_counts[client_id])\n",
    "            if tokens_last_minute + estimated_tokens > self.config.tokens_per_minute:\n",
    "                return {\n",
    "                    \"allowed\": False,\n",
    "                    \"reason\": \"token_limit\",\n",
    "                    \"message\": f\"Token limit: {self.config.tokens_per_minute}/min\",\n",
    "                    \"retry_after\": 60\n",
    "                }\n",
    "            \n",
    "            # Request allowed\n",
    "            return {\n",
    "                \"allowed\": True,\n",
    "                \"remaining_requests\": self.config.requests_per_minute - requests_last_minute - 1,\n",
    "                \"remaining_tokens\": self.config.tokens_per_minute - tokens_last_minute - estimated_tokens\n",
    "            }\n",
    "    \n",
    "    def record_request(self, client_id: str, tokens_used: int = 0):\n",
    "        \"\"\"Record a completed request.\"\"\"\n",
    "        with self._lock:\n",
    "            now = time.time()\n",
    "            self._request_counts[client_id].append(now)\n",
    "            if tokens_used > 0:\n",
    "                self._token_counts[client_id].append((now, tokens_used))\n",
    "    \n",
    "    def acquire_concurrent(self, client_id: str) -> bool:\n",
    "        \"\"\"Acquire a concurrent request slot.\"\"\"\n",
    "        with self._lock:\n",
    "            if self._concurrent[client_id] < self.config.concurrent_requests:\n",
    "                self._concurrent[client_id] += 1\n",
    "                return True\n",
    "            return False\n",
    "    \n",
    "    def release_concurrent(self, client_id: str):\n",
    "        \"\"\"Release a concurrent request slot.\"\"\"\n",
    "        with self._lock:\n",
    "            self._concurrent[client_id] = max(0, self._concurrent[client_id] - 1)\n",
    "    \n",
    "    def get_client_stats(self, client_id: str) -> dict:\n",
    "        \"\"\"Get usage statistics for a client.\"\"\"\n",
    "        with self._lock:\n",
    "            now = time.time()\n",
    "            minute_ago = now - 60\n",
    "            \n",
    "            requests_minute = len([r for r in self._request_counts[client_id] if r > minute_ago])\n",
    "            tokens_minute = sum(t[1] for t in self._token_counts[client_id] if t[0] > minute_ago)\n",
    "            \n",
    "            return {\n",
    "                \"requests_last_minute\": requests_minute,\n",
    "                \"requests_last_hour\": len(self._request_counts[client_id]),\n",
    "                \"tokens_last_minute\": tokens_minute,\n",
    "                \"concurrent_requests\": self._concurrent[client_id],\n",
    "                \"limits\": {\n",
    "                    \"requests_per_minute\": self.config.requests_per_minute,\n",
    "                    \"requests_per_hour\": self.config.requests_per_hour,\n",
    "                    \"tokens_per_minute\": self.config.tokens_per_minute,\n",
    "                    \"concurrent_requests\": self.config.concurrent_requests\n",
    "                }\n",
    "            }\n",
    "\n",
    "\n",
    "# Demonstrate rate limiter\n",
    "print(\"üö¶ Rate Limiter Demo\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "config = RateLimitConfig(\n",
    "    requests_per_minute=10,\n",
    "    tokens_per_minute=1000,\n",
    "    concurrent_requests=3\n",
    ")\n",
    "limiter = TokenBucketRateLimiter(config)\n",
    "\n",
    "client_id = \"user_123\"\n",
    "\n",
    "# Simulate requests\n",
    "print(\"\\nSimulating requests...\")\n",
    "for i in range(15):\n",
    "    result = limiter.check_rate_limit(client_id, estimated_tokens=100)\n",
    "    \n",
    "    if result[\"allowed\"]:\n",
    "        limiter.record_request(client_id, tokens_used=100)\n",
    "        print(f\"  Request {i+1}: ‚úì Allowed (remaining: {result['remaining_requests']})\")\n",
    "    else:\n",
    "        print(f\"  Request {i+1}: ‚úó Rejected ({result['reason']})\")\n",
    "        print(f\"              Retry after: {result['retry_after']}s\")\n",
    "\n",
    "# Show stats\n",
    "stats = limiter.get_client_stats(client_id)\n",
    "print(f\"\\nüìä Client Stats:\")\n",
    "print(f\"   Requests (last minute): {stats['requests_last_minute']}\")\n",
    "print(f\"   Tokens (last minute): {stats['tokens_last_minute']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Implement SSE Streaming\n",
    "\n",
    "**Task**: Create a robust Server-Sent Events implementation for token streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class StreamChunk:\n",
    "    \"\"\"A chunk in the SSE stream.\"\"\"\n",
    "    id: str\n",
    "    object: str\n",
    "    created: int\n",
    "    model: str\n",
    "    choices: List[dict]\n",
    "    usage: Optional[dict] = None\n",
    "\n",
    "\n",
    "class SSEStreamManager:\n",
    "    \"\"\"\n",
    "    Manage Server-Sent Events streaming for LLM responses.\n",
    "    \n",
    "    Features:\n",
    "    - OpenAI-compatible format\n",
    "    - Heartbeat to keep connection alive\n",
    "    - Graceful error handling\n",
    "    - Usage tracking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\"):\n",
    "        self.model = model\n",
    "        self.request_id = self._generate_id()\n",
    "        self.created = int(time.time())\n",
    "        self.total_tokens = 0\n",
    "        self.prompt_tokens = 0\n",
    "    \n",
    "    def _generate_id(self) -> str:\n",
    "        \"\"\"Generate a unique request ID.\"\"\"\n",
    "        return f\"chatcmpl-{hashlib.md5(str(time.time()).encode()).hexdigest()[:24]}\"\n",
    "    \n",
    "    def format_chunk(self, \n",
    "                     content: str = \"\",\n",
    "                     finish_reason: Optional[str] = None,\n",
    "                     role: Optional[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Format a chunk in OpenAI-compatible SSE format.\n",
    "        \"\"\"\n",
    "        delta = {}\n",
    "        if role:\n",
    "            delta[\"role\"] = role\n",
    "        if content:\n",
    "            delta[\"content\"] = content\n",
    "            self.total_tokens += 1  # Approximate\n",
    "        \n",
    "        chunk = {\n",
    "            \"id\": self.request_id,\n",
    "            \"object\": \"chat.completion.chunk\",\n",
    "            \"created\": self.created,\n",
    "            \"model\": self.model,\n",
    "            \"choices\": [{\n",
    "                \"index\": 0,\n",
    "                \"delta\": delta,\n",
    "                \"finish_reason\": finish_reason\n",
    "            }]\n",
    "        }\n",
    "        \n",
    "        return f\"data: {json.dumps(chunk)}\\n\\n\"\n",
    "    \n",
    "    def format_done(self) -> str:\n",
    "        \"\"\"Format the final [DONE] message.\"\"\"\n",
    "        return \"data: [DONE]\\n\\n\"\n",
    "    \n",
    "    def format_error(self, error: str, code: str = \"internal_error\") -> str:\n",
    "        \"\"\"Format an error message.\"\"\"\n",
    "        error_chunk = {\n",
    "            \"error\": {\n",
    "                \"message\": error,\n",
    "                \"type\": \"api_error\",\n",
    "                \"code\": code\n",
    "            }\n",
    "        }\n",
    "        return f\"data: {json.dumps(error_chunk)}\\n\\n\"\n",
    "    \n",
    "    def format_heartbeat(self) -> str:\n",
    "        \"\"\"Format a heartbeat (comment) to keep connection alive.\"\"\"\n",
    "        return \": heartbeat\\n\\n\"\n",
    "    \n",
    "    async def stream_response(self, \n",
    "                               tokens: List[str],\n",
    "                               delay_ms: float = 50) -> AsyncGenerator[str, None]:\n",
    "        \"\"\"\n",
    "        Stream tokens as SSE events.\n",
    "        \"\"\"\n",
    "        # Send initial role chunk\n",
    "        yield self.format_chunk(role=\"assistant\")\n",
    "        \n",
    "        # Stream content tokens\n",
    "        for i, token in enumerate(tokens):\n",
    "            await asyncio.sleep(delay_ms / 1000)\n",
    "            yield self.format_chunk(content=token)\n",
    "            \n",
    "            # Send heartbeat every 10 tokens\n",
    "            if i > 0 and i % 10 == 0:\n",
    "                yield self.format_heartbeat()\n",
    "        \n",
    "        # Send final chunk with finish_reason\n",
    "        yield self.format_chunk(finish_reason=\"stop\")\n",
    "        \n",
    "        # Send done\n",
    "        yield self.format_done()\n",
    "    \n",
    "    def get_usage(self) -> dict:\n",
    "        \"\"\"Get token usage for this request.\"\"\"\n",
    "        return {\n",
    "            \"prompt_tokens\": self.prompt_tokens,\n",
    "            \"completion_tokens\": self.total_tokens,\n",
    "            \"total_tokens\": self.prompt_tokens + self.total_tokens\n",
    "        }\n",
    "\n",
    "\n",
    "# Demonstrate SSE streaming\n",
    "print(\"üì° SSE Streaming Demo\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "manager = SSEStreamManager(model=\"llama-3.1-8b\")\n",
    "\n",
    "# Simulate a response\n",
    "tokens = [\"Hello\", \"!\", \" I'm\", \" Claude\", \",\", \" an\", \" AI\", \" assistant\", \".\"]\n",
    "\n",
    "print(\"\\nStreaming response:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "async def demo_stream():\n",
    "    async for chunk in manager.stream_response(tokens, delay_ms=10):\n",
    "        # Parse and display\n",
    "        if chunk.startswith(\"data: {\"):\n",
    "            data = json.loads(chunk[6:-2])  # Remove \"data: \" and \"\\n\\n\"\n",
    "            if \"choices\" in data:\n",
    "                delta = data[\"choices\"][0].get(\"delta\", {})\n",
    "                if \"content\" in delta:\n",
    "                    print(delta[\"content\"], end=\"\", flush=True)\n",
    "                if delta.get(\"role\"):\n",
    "                    print(f\"[{delta['role']}] \", end=\"\")\n",
    "        elif \"[DONE]\" in chunk:\n",
    "            print(\"\\n[DONE]\")\n",
    "\n",
    "await demo_stream()\n",
    "\n",
    "print(f\"\\nüìä Usage: {manager.get_usage()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Implement Health Checks and Monitoring\n",
    "\n",
    "**Task**: Create a comprehensive health monitoring system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "class HealthStatus(Enum):\n",
    "    HEALTHY = \"healthy\"\n",
    "    DEGRADED = \"degraded\"\n",
    "    UNHEALTHY = \"unhealthy\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HealthCheckResult:\n",
    "    \"\"\"Result of a health check.\"\"\"\n",
    "    name: str\n",
    "    status: HealthStatus\n",
    "    latency_ms: float\n",
    "    message: str = \"\"\n",
    "    details: dict = field(default_factory=dict)\n",
    "\n",
    "\n",
    "class HealthMonitor:\n",
    "    \"\"\"\n",
    "    Comprehensive health monitoring for production LLM API.\n",
    "    \n",
    "    Monitors:\n",
    "    - Inference engine connectivity\n",
    "    - GPU memory and utilization\n",
    "    - Request latency and error rates\n",
    "    - Queue depth\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.checks: Dict[str, Callable] = {}\n",
    "        self.metrics = {\n",
    "            \"requests_total\": 0,\n",
    "            \"requests_failed\": 0,\n",
    "            \"latency_sum_ms\": 0,\n",
    "            \"tokens_generated\": 0,\n",
    "        }\n",
    "        self._last_check_time = 0\n",
    "        self._cached_status = None\n",
    "    \n",
    "    def register_check(self, name: str, check_fn: Callable):\n",
    "        \"\"\"Register a health check function.\"\"\"\n",
    "        self.checks[name] = check_fn\n",
    "    \n",
    "    def record_request(self, latency_ms: float, tokens: int, success: bool):\n",
    "        \"\"\"Record metrics for a request.\"\"\"\n",
    "        self.metrics[\"requests_total\"] += 1\n",
    "        self.metrics[\"latency_sum_ms\"] += latency_ms\n",
    "        self.metrics[\"tokens_generated\"] += tokens\n",
    "        if not success:\n",
    "            self.metrics[\"requests_failed\"] += 1\n",
    "    \n",
    "    async def check_inference_engine(self, url: str = \"http://localhost:8000\") -> HealthCheckResult:\n",
    "        \"\"\"Check if inference engine is responsive.\"\"\"\n",
    "        start = time.time()\n",
    "        try:\n",
    "            # Simulate health check (would be actual HTTP request)\n",
    "            await asyncio.sleep(0.01)  # Simulate network latency\n",
    "            latency = (time.time() - start) * 1000\n",
    "            \n",
    "            # Simulated response\n",
    "            engine_healthy = True\n",
    "            \n",
    "            if engine_healthy:\n",
    "                return HealthCheckResult(\n",
    "                    name=\"inference_engine\",\n",
    "                    status=HealthStatus.HEALTHY,\n",
    "                    latency_ms=latency,\n",
    "                    message=\"Engine responding normally\"\n",
    "                )\n",
    "            else:\n",
    "                return HealthCheckResult(\n",
    "                    name=\"inference_engine\",\n",
    "                    status=HealthStatus.UNHEALTHY,\n",
    "                    latency_ms=latency,\n",
    "                    message=\"Engine not responding\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            return HealthCheckResult(\n",
    "                name=\"inference_engine\",\n",
    "                status=HealthStatus.UNHEALTHY,\n",
    "                latency_ms=(time.time() - start) * 1000,\n",
    "                message=str(e)\n",
    "            )\n",
    "    \n",
    "    def check_gpu_health(self) -> HealthCheckResult:\n",
    "        \"\"\"Check GPU memory and utilization.\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        # Simulated GPU metrics\n",
    "        gpu_memory_used_gb = 45.2\n",
    "        gpu_memory_total_gb = 128.0\n",
    "        gpu_utilization = 0.65\n",
    "        \n",
    "        memory_ratio = gpu_memory_used_gb / gpu_memory_total_gb\n",
    "        \n",
    "        if memory_ratio > 0.95:\n",
    "            status = HealthStatus.UNHEALTHY\n",
    "            message = \"GPU memory critically low\"\n",
    "        elif memory_ratio > 0.85:\n",
    "            status = HealthStatus.DEGRADED\n",
    "            message = \"GPU memory running low\"\n",
    "        else:\n",
    "            status = HealthStatus.HEALTHY\n",
    "            message = \"GPU memory OK\"\n",
    "        \n",
    "        return HealthCheckResult(\n",
    "            name=\"gpu\",\n",
    "            status=status,\n",
    "            latency_ms=(time.time() - start) * 1000,\n",
    "            message=message,\n",
    "            details={\n",
    "                \"memory_used_gb\": gpu_memory_used_gb,\n",
    "                \"memory_total_gb\": gpu_memory_total_gb,\n",
    "                \"utilization\": gpu_utilization\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def check_error_rate(self) -> HealthCheckResult:\n",
    "        \"\"\"Check request error rate.\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        total = self.metrics[\"requests_total\"]\n",
    "        failed = self.metrics[\"requests_failed\"]\n",
    "        \n",
    "        if total == 0:\n",
    "            error_rate = 0\n",
    "        else:\n",
    "            error_rate = failed / total\n",
    "        \n",
    "        if error_rate > 0.1:\n",
    "            status = HealthStatus.UNHEALTHY\n",
    "            message = f\"High error rate: {error_rate:.1%}\"\n",
    "        elif error_rate > 0.05:\n",
    "            status = HealthStatus.DEGRADED\n",
    "            message = f\"Elevated error rate: {error_rate:.1%}\"\n",
    "        else:\n",
    "            status = HealthStatus.HEALTHY\n",
    "            message = f\"Error rate normal: {error_rate:.1%}\"\n",
    "        \n",
    "        return HealthCheckResult(\n",
    "            name=\"error_rate\",\n",
    "            status=status,\n",
    "            latency_ms=(time.time() - start) * 1000,\n",
    "            message=message,\n",
    "            details={\n",
    "                \"total_requests\": total,\n",
    "                \"failed_requests\": failed,\n",
    "                \"error_rate\": error_rate\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    async def run_all_checks(self) -> dict:\n",
    "        \"\"\"Run all health checks and return combined status.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Run async check\n",
    "        results.append(await self.check_inference_engine())\n",
    "        \n",
    "        # Run sync checks\n",
    "        results.append(self.check_gpu_health())\n",
    "        results.append(self.check_error_rate())\n",
    "        \n",
    "        # Determine overall status\n",
    "        if any(r.status == HealthStatus.UNHEALTHY for r in results):\n",
    "            overall = HealthStatus.UNHEALTHY\n",
    "        elif any(r.status == HealthStatus.DEGRADED for r in results):\n",
    "            overall = HealthStatus.DEGRADED\n",
    "        else:\n",
    "            overall = HealthStatus.HEALTHY\n",
    "        \n",
    "        return {\n",
    "            \"status\": overall.value,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"checks\": [\n",
    "                {\n",
    "                    \"name\": r.name,\n",
    "                    \"status\": r.status.value,\n",
    "                    \"latency_ms\": r.latency_ms,\n",
    "                    \"message\": r.message,\n",
    "                    \"details\": r.details\n",
    "                }\n",
    "                for r in results\n",
    "            ],\n",
    "            \"metrics\": self.metrics\n",
    "        }\n",
    "\n",
    "\n",
    "# Demonstrate health monitoring\n",
    "print(\"üè• Health Monitor Demo\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "monitor = HealthMonitor()\n",
    "\n",
    "# Simulate some requests\n",
    "for i in range(100):\n",
    "    success = i % 20 != 0  # 5% failure rate\n",
    "    monitor.record_request(\n",
    "        latency_ms=50 + (i % 10) * 5,\n",
    "        tokens=100 + i,\n",
    "        success=success\n",
    "    )\n",
    "\n",
    "# Run health checks\n",
    "async def run_health_check():\n",
    "    health = await monitor.run_all_checks()\n",
    "    \n",
    "    print(f\"\\nüìä Overall Status: {health['status'].upper()}\")\n",
    "    print(f\"   Timestamp: {health['timestamp']}\")\n",
    "    \n",
    "    print(\"\\n   Individual Checks:\")\n",
    "    for check in health['checks']:\n",
    "        icon = \"‚úì\" if check['status'] == 'healthy' else \"‚ö†\" if check['status'] == 'degraded' else \"‚úó\"\n",
    "        print(f\"   {icon} {check['name']}: {check['status']} - {check['message']}\")\n",
    "    \n",
    "    print(f\"\\n   Metrics:\")\n",
    "    print(f\"   ‚Ä¢ Total requests: {health['metrics']['requests_total']}\")\n",
    "    print(f\"   ‚Ä¢ Failed requests: {health['metrics']['requests_failed']}\")\n",
    "    print(f\"   ‚Ä¢ Tokens generated: {health['metrics']['tokens_generated']}\")\n",
    "\n",
    "await run_health_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Implement Request Logging and Tracing\n",
    "\n",
    "**Task**: Create a production logging system with distributed tracing support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import logging\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RequestTrace:\n",
    "    \"\"\"Trace information for a request.\"\"\"\n",
    "    trace_id: str\n",
    "    span_id: str\n",
    "    parent_span_id: Optional[str] = None\n",
    "    operation: str = \"\"\n",
    "    start_time: float = field(default_factory=time.time)\n",
    "    end_time: Optional[float] = None\n",
    "    attributes: dict = field(default_factory=dict)\n",
    "    events: List[dict] = field(default_factory=list)\n",
    "    status: str = \"OK\"\n",
    "    \n",
    "    @property\n",
    "    def duration_ms(self) -> float:\n",
    "        if self.end_time:\n",
    "            return (self.end_time - self.start_time) * 1000\n",
    "        return (time.time() - self.start_time) * 1000\n",
    "    \n",
    "    def add_event(self, name: str, attributes: dict = None):\n",
    "        self.events.append({\n",
    "            \"name\": name,\n",
    "            \"timestamp\": time.time(),\n",
    "            \"attributes\": attributes or {}\n",
    "        })\n",
    "    \n",
    "    def finish(self, status: str = \"OK\"):\n",
    "        self.end_time = time.time()\n",
    "        self.status = status\n",
    "\n",
    "\n",
    "class RequestLogger:\n",
    "    \"\"\"\n",
    "    Production request logger with tracing support.\n",
    "    \n",
    "    Features:\n",
    "    - Structured JSON logging\n",
    "    - Distributed tracing (trace_id, span_id)\n",
    "    - Request/response logging\n",
    "    - Performance metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, service_name: str = \"llm-api\"):\n",
    "        self.service_name = service_name\n",
    "        self.traces: Dict[str, RequestTrace] = {}\n",
    "        \n",
    "        # Configure logger\n",
    "        self.logger = logging.getLogger(service_name)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "    \n",
    "    def start_trace(self, operation: str, \n",
    "                    trace_id: str = None,\n",
    "                    parent_span_id: str = None) -> RequestTrace:\n",
    "        \"\"\"Start a new trace span.\"\"\"\n",
    "        trace = RequestTrace(\n",
    "            trace_id=trace_id or str(uuid.uuid4()),\n",
    "            span_id=str(uuid.uuid4())[:16],\n",
    "            parent_span_id=parent_span_id,\n",
    "            operation=operation\n",
    "        )\n",
    "        self.traces[trace.span_id] = trace\n",
    "        return trace\n",
    "    \n",
    "    def log_request(self, trace: RequestTrace, request: dict):\n",
    "        \"\"\"Log an incoming request.\"\"\"\n",
    "        trace.attributes.update({\n",
    "            \"request.model\": request.get(\"model\", \"unknown\"),\n",
    "            \"request.max_tokens\": request.get(\"max_tokens\", 0),\n",
    "            \"request.stream\": request.get(\"stream\", False),\n",
    "        })\n",
    "        \n",
    "        # Calculate prompt size\n",
    "        messages = request.get(\"messages\", [])\n",
    "        prompt_chars = sum(len(m.get(\"content\", \"\")) for m in messages)\n",
    "        trace.attributes[\"request.prompt_chars\"] = prompt_chars\n",
    "        \n",
    "        trace.add_event(\"request_received\", {\"prompt_chars\": prompt_chars})\n",
    "        \n",
    "        self._log(\"info\", \"Request received\", trace)\n",
    "    \n",
    "    def log_response(self, trace: RequestTrace, response: dict):\n",
    "        \"\"\"Log the response.\"\"\"\n",
    "        usage = response.get(\"usage\", {})\n",
    "        trace.attributes.update({\n",
    "            \"response.prompt_tokens\": usage.get(\"prompt_tokens\", 0),\n",
    "            \"response.completion_tokens\": usage.get(\"completion_tokens\", 0),\n",
    "            \"response.total_tokens\": usage.get(\"total_tokens\", 0),\n",
    "        })\n",
    "        \n",
    "        trace.add_event(\"response_sent\", usage)\n",
    "        trace.finish(\"OK\")\n",
    "        \n",
    "        self._log(\"info\", \"Request completed\", trace)\n",
    "    \n",
    "    def log_error(self, trace: RequestTrace, error: str, error_type: str = \"unknown\"):\n",
    "        \"\"\"Log an error.\"\"\"\n",
    "        trace.attributes[\"error.message\"] = error\n",
    "        trace.attributes[\"error.type\"] = error_type\n",
    "        trace.add_event(\"error\", {\"message\": error, \"type\": error_type})\n",
    "        trace.finish(\"ERROR\")\n",
    "        \n",
    "        self._log(\"error\", f\"Request failed: {error}\", trace)\n",
    "    \n",
    "    def _log(self, level: str, message: str, trace: RequestTrace):\n",
    "        \"\"\"Internal logging with structured data.\"\"\"\n",
    "        log_data = {\n",
    "            \"service\": self.service_name,\n",
    "            \"trace_id\": trace.trace_id,\n",
    "            \"span_id\": trace.span_id,\n",
    "            \"operation\": trace.operation,\n",
    "            \"duration_ms\": trace.duration_ms,\n",
    "            \"status\": trace.status,\n",
    "            \"attributes\": trace.attributes,\n",
    "            \"message\": message\n",
    "        }\n",
    "        \n",
    "        # In production, this would go to a log aggregator\n",
    "        print(f\"[{level.upper()}] {json.dumps(log_data, indent=2)}\")\n",
    "    \n",
    "    def get_trace_summary(self, trace_id: str) -> dict:\n",
    "        \"\"\"Get a summary of all spans in a trace.\"\"\"\n",
    "        spans = [t for t in self.traces.values() if t.trace_id == trace_id]\n",
    "        \n",
    "        return {\n",
    "            \"trace_id\": trace_id,\n",
    "            \"span_count\": len(spans),\n",
    "            \"total_duration_ms\": sum(s.duration_ms for s in spans),\n",
    "            \"spans\": [\n",
    "                {\n",
    "                    \"span_id\": s.span_id,\n",
    "                    \"operation\": s.operation,\n",
    "                    \"duration_ms\": s.duration_ms,\n",
    "                    \"status\": s.status,\n",
    "                    \"events\": len(s.events)\n",
    "                }\n",
    "                for s in spans\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "# Demonstrate logging\n",
    "print(\"üìù Request Logger Demo\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "logger = RequestLogger(service_name=\"llm-api\")\n",
    "\n",
    "# Simulate a request\n",
    "trace = logger.start_trace(\"chat.completion\")\n",
    "\n",
    "request = {\n",
    "    \"model\": \"llama-3.1-8b\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What is machine learning?\"}],\n",
    "    \"max_tokens\": 256,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "print(\"\\n1. Logging request:\")\n",
    "logger.log_request(trace, request)\n",
    "\n",
    "# Simulate processing\n",
    "time.sleep(0.1)\n",
    "trace.add_event(\"inference_start\")\n",
    "time.sleep(0.2)\n",
    "trace.add_event(\"inference_complete\")\n",
    "\n",
    "response = {\n",
    "    \"usage\": {\n",
    "        \"prompt_tokens\": 12,\n",
    "        \"completion_tokens\": 156,\n",
    "        \"total_tokens\": 168\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n2. Logging response:\")\n",
    "logger.log_response(trace, response)\n",
    "\n",
    "print(\"\\n3. Trace summary:\")\n",
    "summary = logger.get_trace_summary(trace.trace_id)\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Rate Limiting**:\n",
    "   - Use token bucket for flexible rate limiting\n",
    "   - Track multiple windows (minute, hour)\n",
    "   - Include token-based limits for LLMs\n",
    "   - Support burst traffic with multipliers\n",
    "\n",
    "2. **SSE Streaming**:\n",
    "   - OpenAI-compatible format for drop-in replacement\n",
    "   - Include heartbeats for connection keep-alive\n",
    "   - Handle errors gracefully in stream\n",
    "\n",
    "3. **Health Monitoring**:\n",
    "   - Check inference engine, GPU, and error rates\n",
    "   - Use three-level status (healthy/degraded/unhealthy)\n",
    "   - Expose metrics for alerting\n",
    "\n",
    "4. **Logging & Tracing**:\n",
    "   - Structured JSON logs for aggregation\n",
    "   - Distributed tracing with trace/span IDs\n",
    "   - Track request lifecycle with events"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
