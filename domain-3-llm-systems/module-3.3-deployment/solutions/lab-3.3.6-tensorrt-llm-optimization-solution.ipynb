{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.3.6 Solutions: TensorRT-LLM Optimization\n",
    "\n",
    "Complete solutions to all exercises from the TensorRT-LLM optimization lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path(\"../scripts\").resolve()))\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Calculate Optimal Quantization Settings\n",
    "\n",
    "**Task**: Analyze memory and performance trade-offs for different quantization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class QuantizationConfig:\n",
    "    \"\"\"Configuration for model quantization.\"\"\"\n",
    "    name: str\n",
    "    weight_bits: int\n",
    "    activation_bits: int\n",
    "    kv_cache_bits: int\n",
    "    compute_dtype: str\n",
    "    supported_on_spark: bool  # DGX Spark Blackwell GPU\n",
    "\n",
    "# Available quantization options\n",
    "QUANT_CONFIGS = {\n",
    "    \"fp16\": QuantizationConfig(\n",
    "        name=\"FP16\", weight_bits=16, activation_bits=16, \n",
    "        kv_cache_bits=16, compute_dtype=\"float16\", supported_on_spark=True\n",
    "    ),\n",
    "    \"bf16\": QuantizationConfig(\n",
    "        name=\"BF16\", weight_bits=16, activation_bits=16,\n",
    "        kv_cache_bits=16, compute_dtype=\"bfloat16\", supported_on_spark=True\n",
    "    ),\n",
    "    \"fp8\": QuantizationConfig(\n",
    "        name=\"FP8\", weight_bits=8, activation_bits=8,\n",
    "        kv_cache_bits=8, compute_dtype=\"float8_e4m3fn\", supported_on_spark=True\n",
    "    ),\n",
    "    \"int8\": QuantizationConfig(\n",
    "        name=\"INT8\", weight_bits=8, activation_bits=8,\n",
    "        kv_cache_bits=8, compute_dtype=\"int8\", supported_on_spark=True\n",
    "    ),\n",
    "    \"int4\": QuantizationConfig(\n",
    "        name=\"INT4 (AWQ)\", weight_bits=4, activation_bits=16,\n",
    "        kv_cache_bits=8, compute_dtype=\"int4_awq\", supported_on_spark=True\n",
    "    ),\n",
    "    \"nvfp4\": QuantizationConfig(\n",
    "        name=\"NVFP4 (Blackwell)\", weight_bits=4, activation_bits=8,\n",
    "        kv_cache_bits=4, compute_dtype=\"nvfp4\", supported_on_spark=True\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelSpec:\n",
    "    \"\"\"Model specification for memory calculations.\"\"\"\n",
    "    name: str\n",
    "    params_billions: float\n",
    "    hidden_size: int\n",
    "    num_layers: int\n",
    "    num_kv_heads: int\n",
    "    head_dim: int\n",
    "    max_seq_len: int\n",
    "\n",
    "MODELS = {\n",
    "    \"llama-8b\": ModelSpec(\"Llama 3.1 8B\", 8, 4096, 32, 8, 128, 131072),\n",
    "    \"llama-70b\": ModelSpec(\"Llama 3.1 70B\", 70, 8192, 80, 8, 128, 131072),\n",
    "    \"llama-405b\": ModelSpec(\"Llama 3.1 405B\", 405, 16384, 126, 8, 128, 131072),\n",
    "}\n",
    "\n",
    "\n",
    "def calculate_memory_requirements(model: ModelSpec, \n",
    "                                   quant: QuantizationConfig,\n",
    "                                   batch_size: int = 1,\n",
    "                                   seq_len: int = 4096) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate memory requirements for a model with specific quantization.\n",
    "    \"\"\"\n",
    "    # Weight memory\n",
    "    weight_bytes = model.params_billions * 1e9 * (quant.weight_bits / 8)\n",
    "    weight_gb = weight_bytes / (1024**3)\n",
    "    \n",
    "    # KV cache memory per token\n",
    "    kv_bytes_per_token = (\n",
    "        2 *  # K and V\n",
    "        model.num_layers *\n",
    "        model.num_kv_heads *\n",
    "        model.head_dim *\n",
    "        (quant.kv_cache_bits / 8)\n",
    "    )\n",
    "    \n",
    "    kv_cache_gb = (batch_size * seq_len * kv_bytes_per_token) / (1024**3)\n",
    "    \n",
    "    # Activation memory (approximate)\n",
    "    activation_bytes = (\n",
    "        batch_size * \n",
    "        seq_len * \n",
    "        model.hidden_size * \n",
    "        (quant.activation_bits / 8) *\n",
    "        4  # Approximate multiplier for intermediate activations\n",
    "    )\n",
    "    activation_gb = activation_bytes / (1024**3)\n",
    "    \n",
    "    total_gb = weight_gb + kv_cache_gb + activation_gb\n",
    "    \n",
    "    # Check if fits in DGX Spark (128GB unified memory)\n",
    "    dgx_spark_memory = 128\n",
    "    fits_spark = total_gb < dgx_spark_memory * 0.9  # 90% utilization\n",
    "    \n",
    "    return {\n",
    "        \"weight_gb\": weight_gb,\n",
    "        \"kv_cache_gb\": kv_cache_gb,\n",
    "        \"activation_gb\": activation_gb,\n",
    "        \"total_gb\": total_gb,\n",
    "        \"fits_dgx_spark\": fits_spark,\n",
    "        \"headroom_gb\": dgx_spark_memory - total_gb\n",
    "    }\n",
    "\n",
    "\n",
    "def estimate_performance(quant: QuantizationConfig) -> dict:\n",
    "    \"\"\"\n",
    "    Estimate relative performance for a quantization config.\n",
    "    \"\"\"\n",
    "    # Baseline: FP16\n",
    "    base_tflops = 1.0\n",
    "    \n",
    "    # Performance scaling based on quantization\n",
    "    if quant.weight_bits == 16:\n",
    "        throughput_multiplier = 1.0\n",
    "    elif quant.weight_bits == 8:\n",
    "        throughput_multiplier = 1.8  # ~1.8x faster\n",
    "    elif quant.weight_bits == 4:\n",
    "        throughput_multiplier = 2.5  # ~2.5x faster\n",
    "    else:\n",
    "        throughput_multiplier = 1.0\n",
    "    \n",
    "    # Quality impact (rough estimates)\n",
    "    if quant.weight_bits >= 16:\n",
    "        quality_impact = \"None\"\n",
    "        perplexity_increase = 0.0\n",
    "    elif quant.weight_bits == 8:\n",
    "        quality_impact = \"Minimal (<1%)\"\n",
    "        perplexity_increase = 0.02\n",
    "    elif quant.weight_bits == 4:\n",
    "        quality_impact = \"Small (1-3%)\"\n",
    "        perplexity_increase = 0.05\n",
    "    else:\n",
    "        quality_impact = \"Unknown\"\n",
    "        perplexity_increase = 0.1\n",
    "    \n",
    "    return {\n",
    "        \"throughput_multiplier\": throughput_multiplier,\n",
    "        \"quality_impact\": quality_impact,\n",
    "        \"perplexity_increase\": perplexity_increase\n",
    "    }\n",
    "\n",
    "\n",
    "# Analyze quantization options\n",
    "print(\"ðŸ“Š TensorRT-LLM Quantization Analysis\")\n",
    "print(\"=\" * 85)\n",
    "\n",
    "model = MODELS[\"llama-70b\"]\n",
    "print(f\"\\nModel: {model.name} ({model.params_billions}B parameters)\")\n",
    "print(f\"Target: DGX Spark (128GB unified memory)\")\n",
    "print(f\"Batch size: 8, Sequence length: 4096\\n\")\n",
    "\n",
    "print(f\"{'Quant':<15} {'Weights':>10} {'KV Cache':>10} {'Total':>10} {'Fits?':>8} {'Speedup':>10} {'Quality':>15}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for name, quant in QUANT_CONFIGS.items():\n",
    "    mem = calculate_memory_requirements(model, quant, batch_size=8, seq_len=4096)\n",
    "    perf = estimate_performance(quant)\n",
    "    \n",
    "    fits = \"âœ“\" if mem[\"fits_dgx_spark\"] else \"âœ—\"\n",
    "    print(f\"{quant.name:<15} {mem['weight_gb']:>8.1f}GB {mem['kv_cache_gb']:>8.1f}GB \"\n",
    "          f\"{mem['total_gb']:>8.1f}GB {fits:>8} {perf['throughput_multiplier']:>9.1f}x \"\n",
    "          f\"{perf['quality_impact']:>15}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Recommendations for DGX Spark:\")\n",
    "print(\"   â€¢ Llama 8B: Use BF16 (plenty of headroom)\")\n",
    "print(\"   â€¢ Llama 70B: Use FP8 or INT8 for best quality/speed balance\")\n",
    "print(\"   â€¢ Llama 405B: Requires NVFP4 or distributed inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Build Optimized TensorRT Engine\n",
    "\n",
    "**Task**: Create a script to build and optimize a TensorRT-LLM engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorRT-LLM Build Script Generator\n",
    "\n",
    "def generate_build_script(model_name: str,\n",
    "                          model_path: str,\n",
    "                          output_dir: str,\n",
    "                          quant_type: str = \"fp8\",\n",
    "                          max_batch_size: int = 8,\n",
    "                          max_input_len: int = 2048,\n",
    "                          max_output_len: int = 2048,\n",
    "                          tp_size: int = 1,\n",
    "                          use_paged_kv: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Generate a shell script for building a TensorRT-LLM engine.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Quantization-specific flags\n",
    "    quant_flags = {\n",
    "        \"fp16\": \"--dtype float16\",\n",
    "        \"bf16\": \"--dtype bfloat16\",\n",
    "        \"fp8\": \"--dtype bfloat16 --use_fp8_context_fmha enable --quantization fp8\",\n",
    "        \"int8\": \"--dtype float16 --use_smooth_quant --per_token --per_channel\",\n",
    "        \"int4_awq\": \"--dtype float16 --use_weight_only --weight_only_precision int4_awq\",\n",
    "        \"nvfp4\": \"--dtype bfloat16 --quantization nvfp4\",\n",
    "    }\n",
    "    \n",
    "    quant_flag = quant_flags.get(quant_type, quant_flags[\"bf16\"])\n",
    "    \n",
    "    script = f'''#!/bin/bash\n",
    "# TensorRT-LLM Engine Build Script\n",
    "# Model: {model_name}\n",
    "# Quantization: {quant_type}\n",
    "# Generated for DGX Spark\n",
    "\n",
    "set -e\n",
    "\n",
    "MODEL_PATH=\"{model_path}\"\n",
    "OUTPUT_DIR=\"{output_dir}\"\n",
    "CHECKPOINT_DIR=\"$OUTPUT_DIR/checkpoint\"\n",
    "ENGINE_DIR=\"$OUTPUT_DIR/engine\"\n",
    "\n",
    "echo \"ðŸ”¨ Building TensorRT-LLM Engine\"\n",
    "echo \"   Model: {model_name}\"\n",
    "echo \"   Quantization: {quant_type}\"\n",
    "echo \"   TP Size: {tp_size}\"\n",
    "\n",
    "# Step 1: Convert checkpoint\n",
    "echo \"\\nðŸ“¦ Step 1: Converting checkpoint...\"\n",
    "python3 convert_checkpoint.py \\\\\n",
    "    --model_dir \"$MODEL_PATH\" \\\\\n",
    "    --output_dir \"$CHECKPOINT_DIR\" \\\\\n",
    "    --tp_size {tp_size} \\\\\n",
    "    {quant_flag}\n",
    "\n",
    "# Step 2: Build engine\n",
    "echo \"\\nâš™ï¸ Step 2: Building TensorRT engine...\"\n",
    "trtllm-build \\\\\n",
    "    --checkpoint_dir \"$CHECKPOINT_DIR\" \\\\\n",
    "    --output_dir \"$ENGINE_DIR\" \\\\\n",
    "    --max_batch_size {max_batch_size} \\\\\n",
    "    --max_input_len {max_input_len} \\\\\n",
    "    --max_seq_len {max_input_len + max_output_len} \\\\\n",
    "    --gemm_plugin auto \\\\\n",
    "    --gpt_attention_plugin auto \\\\\n",
    "    {\"--paged_kv_cache enable\" if use_paged_kv else \"\"} \\\\\n",
    "    --remove_input_padding enable \\\\\n",
    "    --enable_xqa enable \\\\\n",
    "    --use_fused_mlp enable \\\\\n",
    "    --workers {tp_size}\n",
    "\n",
    "# Step 3: Verify build\n",
    "echo \"\\nâœ… Step 3: Verifying engine...\"\n",
    "python3 -c \"\n",
    "import tensorrt_llm\n",
    "from tensorrt_llm import ModelRunner\n",
    "\n",
    "runner = ModelRunner.from_dir('$ENGINE_DIR')\n",
    "print(f'Engine loaded successfully!')\n",
    "print(f'Max batch size: {{runner.max_batch_size}}')\n",
    "print(f'Max input length: {{runner.max_input_len}}')\n",
    "\"\n",
    "\n",
    "echo \"\\nðŸŽ‰ Build complete! Engine saved to: $ENGINE_DIR\"\n",
    "'''\n",
    "    \n",
    "    return script\n",
    "\n",
    "\n",
    "# Generate example scripts\n",
    "print(\"ðŸ“ TensorRT-LLM Build Script Generator\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Example 1: Llama 8B with FP8\n",
    "script_8b = generate_build_script(\n",
    "    model_name=\"Llama-3.1-8B-Instruct\",\n",
    "    model_path=\"/models/Meta-Llama-3.1-8B-Instruct\",\n",
    "    output_dir=\"/engines/llama-8b-fp8\",\n",
    "    quant_type=\"fp8\",\n",
    "    max_batch_size=16,\n",
    "    max_input_len=4096,\n",
    "    max_output_len=2048\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“‹ Example Build Script (Llama 8B, FP8):\")\n",
    "print(\"-\" * 60)\n",
    "print(script_8b[:1500] + \"\\n...\")\n",
    "\n",
    "# Key optimization flags explained\n",
    "print(\"\\nðŸ”§ Key Optimization Flags:\")\n",
    "print(\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Flag                       â”‚ Purpose                                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ --gemm_plugin auto         â”‚ Optimized matrix multiplication        â”‚\n",
    "â”‚ --gpt_attention_plugin     â”‚ Fused attention kernels                â”‚\n",
    "â”‚ --paged_kv_cache enable    â”‚ PagedAttention for memory efficiency   â”‚\n",
    "â”‚ --remove_input_padding     â”‚ Skip padding tokens (faster prefill)   â”‚\n",
    "â”‚ --enable_xqa enable        â”‚ Cross-query attention optimization     â”‚\n",
    "â”‚ --use_fused_mlp enable     â”‚ Fused MLP layers                       â”‚\n",
    "â”‚ --use_fp8_context_fmha     â”‚ FP8 flash attention (Blackwell)        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Benchmark TensorRT-LLM vs Other Engines\n",
    "\n",
    "**Task**: Create a comprehensive benchmarking framework for comparing inference engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    \"\"\"Results from benchmarking an inference engine.\"\"\"\n",
    "    engine: str\n",
    "    model: str\n",
    "    batch_size: int\n",
    "    input_len: int\n",
    "    output_len: int\n",
    "    ttft_ms: float\n",
    "    tpot_ms: float  # Time per output token\n",
    "    throughput_tps: float\n",
    "    memory_gb: float\n",
    "    latency_p99_ms: float\n",
    "\n",
    "\n",
    "class InferenceEngineBenchmark:\n",
    "    \"\"\"\n",
    "    Benchmark framework for comparing inference engines.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulated baseline metrics (would be measured in real benchmark)\n",
    "    BASELINE_METRICS = {\n",
    "        \"tensorrt-llm\": {\n",
    "            \"ttft_factor\": 0.7,    # 30% faster TTFT than baseline\n",
    "            \"tpot_factor\": 0.6,    # 40% faster decode\n",
    "            \"memory_factor\": 0.8,  # 20% less memory\n",
    "        },\n",
    "        \"vllm\": {\n",
    "            \"ttft_factor\": 1.0,    # Baseline\n",
    "            \"tpot_factor\": 1.0,\n",
    "            \"memory_factor\": 1.0,\n",
    "        },\n",
    "        \"sglang\": {\n",
    "            \"ttft_factor\": 0.85,   # 15% faster with RadixAttention\n",
    "            \"tpot_factor\": 0.9,\n",
    "            \"memory_factor\": 0.95,\n",
    "        },\n",
    "        \"ollama\": {\n",
    "            \"ttft_factor\": 1.3,    # Slower (optimized for ease of use)\n",
    "            \"tpot_factor\": 1.2,\n",
    "            \"memory_factor\": 1.1,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    def __init__(self, model_name: str = \"llama-8b\"):\n",
    "        self.model = MODELS[model_name]\n",
    "        # Baseline metrics for 8B model\n",
    "        self.base_ttft_ms = 50  # 50ms TTFT baseline\n",
    "        self.base_tpot_ms = 15  # 15ms per token baseline\n",
    "        self.base_memory_gb = 16  # 16GB baseline\n",
    "    \n",
    "    def run_benchmark(self, engine: str, \n",
    "                      batch_size: int = 1,\n",
    "                      input_len: int = 512,\n",
    "                      output_len: int = 256) -> BenchmarkResult:\n",
    "        \"\"\"\n",
    "        Simulate a benchmark run for an engine.\n",
    "        \"\"\"\n",
    "        metrics = self.BASELINE_METRICS.get(engine, self.BASELINE_METRICS[\"vllm\"])\n",
    "        \n",
    "        # Scale by batch size and sequence length\n",
    "        batch_scale = 1 + (batch_size - 1) * 0.1  # ~10% overhead per batch item\n",
    "        seq_scale = 1 + (input_len / 1024) * 0.2  # ~20% overhead per 1K tokens\n",
    "        \n",
    "        ttft_ms = self.base_ttft_ms * metrics[\"ttft_factor\"] * seq_scale\n",
    "        tpot_ms = self.base_tpot_ms * metrics[\"tpot_factor\"] * batch_scale\n",
    "        \n",
    "        # Calculate throughput\n",
    "        total_time_ms = ttft_ms + (output_len * tpot_ms)\n",
    "        total_tokens = batch_size * output_len\n",
    "        throughput_tps = (total_tokens / total_time_ms) * 1000\n",
    "        \n",
    "        # Memory usage\n",
    "        memory_gb = self.base_memory_gb * metrics[\"memory_factor\"] * (batch_size ** 0.5)\n",
    "        \n",
    "        # P99 latency (add some variance)\n",
    "        latency_p99_ms = total_time_ms * 1.3\n",
    "        \n",
    "        return BenchmarkResult(\n",
    "            engine=engine,\n",
    "            model=self.model.name,\n",
    "            batch_size=batch_size,\n",
    "            input_len=input_len,\n",
    "            output_len=output_len,\n",
    "            ttft_ms=ttft_ms,\n",
    "            tpot_ms=tpot_ms,\n",
    "            throughput_tps=throughput_tps,\n",
    "            memory_gb=memory_gb,\n",
    "            latency_p99_ms=latency_p99_ms\n",
    "        )\n",
    "    \n",
    "    def compare_engines(self, engines: List[str],\n",
    "                        batch_sizes: List[int] = [1, 8, 32]) -> Dict[str, List[BenchmarkResult]]:\n",
    "        \"\"\"\n",
    "        Compare multiple engines across different batch sizes.\n",
    "        \"\"\"\n",
    "        results = {engine: [] for engine in engines}\n",
    "        \n",
    "        for engine in engines:\n",
    "            for batch_size in batch_sizes:\n",
    "                result = self.run_benchmark(engine, batch_size=batch_size)\n",
    "                results[engine].append(result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# Run comparison benchmark\n",
    "print(\"ðŸ“Š Inference Engine Comparison Benchmark\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "benchmark = InferenceEngineBenchmark(\"llama-8b\")\n",
    "engines = [\"tensorrt-llm\", \"vllm\", \"sglang\", \"ollama\"]\n",
    "\n",
    "# Single request benchmark\n",
    "print(\"\\nðŸ“‹ Single Request Performance (batch=1, input=512, output=256):\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Engine':<15} {'TTFT':>10} {'TPOT':>10} {'Throughput':>12} {'Memory':>10} {'P99':>12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "single_results = []\n",
    "for engine in engines:\n",
    "    result = benchmark.run_benchmark(engine, batch_size=1)\n",
    "    single_results.append(result)\n",
    "    \n",
    "    print(f\"{engine:<15} {result.ttft_ms:>8.1f}ms {result.tpot_ms:>8.1f}ms \"\n",
    "          f\"{result.throughput_tps:>10.1f}t/s {result.memory_gb:>8.1f}GB \"\n",
    "          f\"{result.latency_p99_ms:>10.1f}ms\")\n",
    "\n",
    "# High throughput benchmark\n",
    "print(\"\\nðŸ“‹ High Throughput Performance (batch=32):\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Engine':<15} {'Throughput':>15} {'Memory':>12} {'Improvement':>15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "baseline_tps = None\n",
    "for engine in engines:\n",
    "    result = benchmark.run_benchmark(engine, batch_size=32)\n",
    "    \n",
    "    if baseline_tps is None:\n",
    "        baseline_tps = result.throughput_tps\n",
    "        improvement = \"(baseline)\"\n",
    "    else:\n",
    "        imp = (result.throughput_tps / baseline_tps - 1) * 100\n",
    "        improvement = f\"{'+' if imp > 0 else ''}{imp:.1f}%\"\n",
    "    \n",
    "    print(f\"{engine:<15} {result.throughput_tps:>13.1f}t/s {result.memory_gb:>10.1f}GB \"\n",
    "          f\"{improvement:>15}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insights:\")\n",
    "print(\"   â€¢ TensorRT-LLM: Best raw performance (optimized CUDA kernels)\")\n",
    "print(\"   â€¢ vLLM: Great balance of performance and ease of use\")\n",
    "print(\"   â€¢ SGLang: Best for workloads with prefix sharing\")\n",
    "print(\"   â€¢ Ollama: Best for development/prototyping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Optimize for Specific Workloads\n",
    "\n",
    "**Task**: Create workload-specific optimization profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class WorkloadProfile:\n",
    "    \"\"\"Profile for a specific workload type.\"\"\"\n",
    "    name: str\n",
    "    description: str\n",
    "    avg_input_len: int\n",
    "    avg_output_len: int\n",
    "    batch_size: int\n",
    "    latency_priority: bool  # True = optimize latency, False = optimize throughput\n",
    "    streaming: bool\n",
    "\n",
    "WORKLOAD_PROFILES = {\n",
    "    \"chatbot\": WorkloadProfile(\n",
    "        name=\"Interactive Chatbot\",\n",
    "        description=\"Low-latency conversational AI\",\n",
    "        avg_input_len=256,\n",
    "        avg_output_len=128,\n",
    "        batch_size=1,\n",
    "        latency_priority=True,\n",
    "        streaming=True\n",
    "    ),\n",
    "    \"batch_processing\": WorkloadProfile(\n",
    "        name=\"Batch Processing\",\n",
    "        description=\"High-throughput document processing\",\n",
    "        avg_input_len=2048,\n",
    "        avg_output_len=512,\n",
    "        batch_size=64,\n",
    "        latency_priority=False,\n",
    "        streaming=False\n",
    "    ),\n",
    "    \"code_completion\": WorkloadProfile(\n",
    "        name=\"Code Completion\",\n",
    "        description=\"IDE code suggestions\",\n",
    "        avg_input_len=512,\n",
    "        avg_output_len=64,\n",
    "        batch_size=1,\n",
    "        latency_priority=True,\n",
    "        streaming=True\n",
    "    ),\n",
    "    \"summarization\": WorkloadProfile(\n",
    "        name=\"Document Summarization\",\n",
    "        description=\"Long document analysis\",\n",
    "        avg_input_len=8192,\n",
    "        avg_output_len=256,\n",
    "        batch_size=8,\n",
    "        latency_priority=False,\n",
    "        streaming=False\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "def generate_optimization_config(profile: WorkloadProfile) -> dict:\n",
    "    \"\"\"\n",
    "    Generate TensorRT-LLM optimization config for a workload.\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        \"workload\": profile.name,\n",
    "        \"build_config\": {},\n",
    "        \"runtime_config\": {},\n",
    "        \"recommendations\": []\n",
    "    }\n",
    "    \n",
    "    # Build configuration\n",
    "    if profile.latency_priority:\n",
    "        # Optimize for low latency\n",
    "        config[\"build_config\"] = {\n",
    "            \"max_batch_size\": min(8, profile.batch_size * 2),\n",
    "            \"max_input_len\": profile.avg_input_len * 2,\n",
    "            \"max_output_len\": profile.avg_output_len * 2,\n",
    "            \"opt_batch_size\": profile.batch_size,\n",
    "            \"builder_opt_level\": 5,  # Maximum optimization\n",
    "            \"strongly_typed\": True,\n",
    "        }\n",
    "        config[\"recommendations\"].append(\"Use FP8 for best latency on Blackwell\")\n",
    "        config[\"recommendations\"].append(\"Enable speculative decoding for further speedup\")\n",
    "    else:\n",
    "        # Optimize for throughput\n",
    "        config[\"build_config\"] = {\n",
    "            \"max_batch_size\": profile.batch_size * 2,\n",
    "            \"max_input_len\": profile.avg_input_len,\n",
    "            \"max_output_len\": profile.avg_output_len,\n",
    "            \"opt_batch_size\": profile.batch_size,\n",
    "            \"builder_opt_level\": 4,\n",
    "            \"strongly_typed\": True,\n",
    "        }\n",
    "        config[\"recommendations\"].append(\"Use INT4 quantization for maximum throughput\")\n",
    "        config[\"recommendations\"].append(\"Enable paged KV cache for memory efficiency\")\n",
    "    \n",
    "    # Runtime configuration\n",
    "    config[\"runtime_config\"] = {\n",
    "        \"kv_cache_free_gpu_mem_fraction\": 0.85 if profile.latency_priority else 0.95,\n",
    "        \"enable_chunked_context\": profile.avg_input_len > 2048,\n",
    "        \"streaming\": profile.streaming,\n",
    "        \"max_tokens_in_paged_kv_cache\": None,  # Auto-calculate\n",
    "    }\n",
    "    \n",
    "    # Additional recommendations based on workload\n",
    "    if profile.avg_input_len > 4096:\n",
    "        config[\"recommendations\"].append(\"Enable chunked prefill for long inputs\")\n",
    "    if profile.streaming:\n",
    "        config[\"recommendations\"].append(\"Use streaming tokenizer for faster TTFT\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "\n",
    "# Generate optimization configs for each workload\n",
    "print(\"ðŸŽ¯ Workload-Specific Optimization Profiles\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, profile in WORKLOAD_PROFILES.items():\n",
    "    config = generate_optimization_config(profile)\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ {profile.name}\")\n",
    "    print(f\"   {profile.description}\")\n",
    "    print(f\"   Input/Output: {profile.avg_input_len}/{profile.avg_output_len} tokens\")\n",
    "    print(f\"   Batch size: {profile.batch_size}\")\n",
    "    print(f\"   Priority: {'Latency' if profile.latency_priority else 'Throughput'}\")\n",
    "    \n",
    "    print(\"\\n   Build Config:\")\n",
    "    for key, value in config[\"build_config\"].items():\n",
    "        print(f\"      {key}: {value}\")\n",
    "    \n",
    "    print(\"\\n   Recommendations:\")\n",
    "    for rec in config[\"recommendations\"]:\n",
    "        print(f\"      â€¢ {rec}\")\n",
    "    \n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Quantization Trade-offs**:\n",
    "   - FP16/BF16: Best quality, highest memory\n",
    "   - FP8: Great balance (Blackwell native)\n",
    "   - INT4: Maximum throughput, slight quality loss\n",
    "\n",
    "2. **Build Optimization**:\n",
    "   - Use all available plugins (GEMM, attention, MLP fusion)\n",
    "   - Enable paged KV cache for memory efficiency\n",
    "   - Set appropriate batch/sequence limits for your workload\n",
    "\n",
    "3. **TensorRT-LLM Advantages**:\n",
    "   - ~30-40% faster than other engines\n",
    "   - Native Blackwell optimizations\n",
    "   - Best for production deployments\n",
    "\n",
    "4. **Workload Optimization**:\n",
    "   - Chatbot: Prioritize TTFT, use FP8\n",
    "   - Batch: Prioritize throughput, use INT4\n",
    "   - Long context: Enable chunked prefill"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
