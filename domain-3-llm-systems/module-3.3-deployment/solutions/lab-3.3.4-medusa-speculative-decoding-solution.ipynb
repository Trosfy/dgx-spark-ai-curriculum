{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.3.4 Solutions: Medusa Speculative Decoding\n",
    "\n",
    "Complete solutions to all exercises from the Medusa speculative decoding lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path(\"../scripts\").resolve()))\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Implement Medusa Head Simulator\n",
    "\n",
    "**Task**: Create a simulation of Medusa heads to understand the tree attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MedusaHeadConfig:\n",
    "    \"\"\"Configuration for Medusa heads.\"\"\"\n",
    "    num_heads: int = 5           # Number of additional prediction heads\n",
    "    top_k_per_head: int = 10     # Top-k candidates per head\n",
    "    tree_depth: int = 5          # Maximum speculation depth\n",
    "\n",
    "\n",
    "class MedusaSimulator:\n",
    "    \"\"\"\n",
    "    Simulate Medusa speculative decoding to understand its mechanics.\n",
    "    \n",
    "    Medusa adds multiple prediction heads that each predict future tokens.\n",
    "    Head 0: predicts token t+1 (same as base model)\n",
    "    Head 1: predicts token t+2\n",
    "    Head 2: predicts token t+3\n",
    "    ...\n",
    "    \n",
    "    These form a tree of possible continuations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: MedusaHeadConfig = None):\n",
    "        self.config = config or MedusaHeadConfig()\n",
    "        self.vocabulary = self._create_mock_vocabulary()\n",
    "        \n",
    "    def _create_mock_vocabulary(self, size: int = 1000) -> List[str]:\n",
    "        \"\"\"Create a mock vocabulary for simulation.\"\"\"\n",
    "        common_words = [\n",
    "            \"the\", \"a\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\",\n",
    "            \"to\", \"of\", \"and\", \"in\", \"that\", \"it\", \"for\", \"on\",\n",
    "            \"with\", \"as\", \"at\", \"by\", \"from\", \"or\", \"an\", \"but\",\n",
    "            \"not\", \"this\", \"have\", \"has\", \"had\", \"do\", \"does\", \"did\",\n",
    "            \"will\", \"would\", \"could\", \"should\", \"may\", \"might\", \"can\",\n",
    "            \"AI\", \"model\", \"data\", \"learning\", \"neural\", \"network\",\n",
    "            \"training\", \"inference\", \"token\", \"embedding\", \"layer\",\n",
    "        ]\n",
    "        return common_words + [f\"word_{i}\" for i in range(size - len(common_words))]\n",
    "    \n",
    "    def simulate_head_predictions(self, \n",
    "                                   context: str,\n",
    "                                   num_heads: int = None) -> Dict[int, List[Tuple[str, float]]]:\n",
    "        \"\"\"\n",
    "        Simulate predictions from each Medusa head.\n",
    "        \n",
    "        Returns: {head_idx: [(token, probability), ...]}\n",
    "        \"\"\"\n",
    "        num_heads = num_heads or self.config.num_heads\n",
    "        predictions = {}\n",
    "        \n",
    "        for head_idx in range(num_heads):\n",
    "            # Simulate probability distribution\n",
    "            # Later heads are less confident (entropy increases with distance)\n",
    "            confidence_decay = 0.8 ** head_idx\n",
    "            \n",
    "            # Generate top-k predictions for this head\n",
    "            probs = np.random.dirichlet(np.ones(self.config.top_k_per_head) * confidence_decay)\n",
    "            probs = sorted(probs, reverse=True)\n",
    "            \n",
    "            tokens = np.random.choice(\n",
    "                self.vocabulary[:100],  # Use common words\n",
    "                size=self.config.top_k_per_head,\n",
    "                replace=False\n",
    "            )\n",
    "            \n",
    "            predictions[head_idx] = list(zip(tokens, probs))\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def build_speculation_tree(self, \n",
    "                                head_predictions: Dict[int, List[Tuple[str, float]]],\n",
    "                                max_candidates: int = 64) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Build tree of candidate sequences from head predictions.\n",
    "        \n",
    "        This is the key Medusa innovation: tree attention allows\n",
    "        verifying multiple paths in parallel.\n",
    "        \"\"\"\n",
    "        # Start with top-1 from each head (the \"main path\")\n",
    "        candidates = []\n",
    "        \n",
    "        # Build tree using top predictions\n",
    "        def build_paths(depth: int, current_path: List[str], \n",
    "                        cumulative_prob: float):\n",
    "            if depth >= len(head_predictions):\n",
    "                if current_path:\n",
    "                    candidates.append((current_path.copy(), cumulative_prob))\n",
    "                return\n",
    "            \n",
    "            # Take top candidates from this head\n",
    "            top_k = min(3, len(head_predictions[depth]))  # Limit branching\n",
    "            for token, prob in head_predictions[depth][:top_k]:\n",
    "                current_path.append(token)\n",
    "                build_paths(depth + 1, current_path, cumulative_prob * prob)\n",
    "                current_path.pop()\n",
    "            \n",
    "            # Also include partial path (stop early)\n",
    "            if current_path:\n",
    "                candidates.append((current_path.copy(), cumulative_prob))\n",
    "        \n",
    "        build_paths(0, [], 1.0)\n",
    "        \n",
    "        # Sort by probability and take top candidates\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [c[0] for c in candidates[:max_candidates]]\n",
    "    \n",
    "    def simulate_verification(self, \n",
    "                               candidates: List[List[str]],\n",
    "                               acceptance_rate: float = 0.7) -> Tuple[List[str], int]:\n",
    "        \"\"\"\n",
    "        Simulate the verification step.\n",
    "        \n",
    "        In real Medusa, the base model verifies all candidates in parallel\n",
    "        using tree attention, accepting the longest matching prefix.\n",
    "        \"\"\"\n",
    "        if not candidates:\n",
    "            return [], 0\n",
    "        \n",
    "        # Simulate: each token has acceptance_rate probability\n",
    "        best_accepted = []\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            accepted = []\n",
    "            for token in candidate:\n",
    "                if np.random.random() < acceptance_rate:\n",
    "                    accepted.append(token)\n",
    "                else:\n",
    "                    break  # Stop at first rejection\n",
    "            \n",
    "            if len(accepted) > len(best_accepted):\n",
    "                best_accepted = accepted\n",
    "        \n",
    "        return best_accepted, len(best_accepted)\n",
    "\n",
    "\n",
    "# Demonstrate Medusa mechanics\n",
    "print(\"ðŸ Medusa Speculative Decoding Simulation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "simulator = MedusaSimulator(MedusaHeadConfig(\n",
    "    num_heads=5,\n",
    "    top_k_per_head=5,\n",
    "    tree_depth=5\n",
    "))\n",
    "\n",
    "# Simulate one step\n",
    "context = \"The future of AI is\"\n",
    "print(f\"\\nContext: '{context}'\")\n",
    "\n",
    "# Get head predictions\n",
    "predictions = simulator.simulate_head_predictions(context)\n",
    "\n",
    "print(\"\\nðŸ“Š Head Predictions (top-3 per head):\")\n",
    "for head_idx, preds in predictions.items():\n",
    "    top_3 = [(t, f\"{p:.2f}\") for t, p in preds[:3]]\n",
    "    print(f\"  Head {head_idx} (t+{head_idx+1}): {top_3}\")\n",
    "\n",
    "# Build speculation tree\n",
    "candidates = simulator.build_speculation_tree(predictions, max_candidates=20)\n",
    "print(f\"\\nðŸŒ² Speculation Tree: {len(candidates)} candidate sequences\")\n",
    "print(\"  Top-5 candidates:\")\n",
    "for i, cand in enumerate(candidates[:5]):\n",
    "    print(f\"    {i+1}. {' â†’ '.join(cand)}\")\n",
    "\n",
    "# Simulate verification\n",
    "accepted, num_accepted = simulator.simulate_verification(candidates, acceptance_rate=0.7)\n",
    "print(f\"\\nâœ… Verification Result:\")\n",
    "print(f\"  Accepted: {' â†’ '.join(accepted) if accepted else '(none)'}\")\n",
    "print(f\"  Tokens accepted: {num_accepted}\")\n",
    "print(f\"  Speedup: ~{max(1, num_accepted)}x (generated {num_accepted} tokens in 1 forward pass)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Calculate Optimal Head Configuration\n",
    "\n",
    "**Task**: Analyze how different head configurations affect speedup and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HeadAnalysisResult:\n",
    "    \"\"\"Results from analyzing a Medusa head configuration.\"\"\"\n",
    "    num_heads: int\n",
    "    avg_accepted: float\n",
    "    theoretical_speedup: float\n",
    "    overhead_factor: float\n",
    "    effective_speedup: float\n",
    "    memory_overhead_mb: float\n",
    "\n",
    "\n",
    "def analyze_head_configuration(num_heads: int,\n",
    "                                base_acceptance_rate: float = 0.8,\n",
    "                                decay_rate: float = 0.9,\n",
    "                                head_params_mb: float = 50,\n",
    "                                overhead_per_head: float = 0.05) -> HeadAnalysisResult:\n",
    "    \"\"\"\n",
    "    Analyze a Medusa head configuration.\n",
    "    \n",
    "    Parameters:\n",
    "        num_heads: Number of speculative heads\n",
    "        base_acceptance_rate: Probability first head is correct\n",
    "        decay_rate: How much acceptance drops per head\n",
    "        head_params_mb: Memory per head (parameters)\n",
    "        overhead_per_head: Computational overhead per head\n",
    "    \"\"\"\n",
    "    # Calculate expected tokens accepted\n",
    "    # Each head has acceptance = base * decay^head_idx\n",
    "    expected_accepted = 0\n",
    "    cumulative_prob = 1.0\n",
    "    \n",
    "    for i in range(num_heads):\n",
    "        head_acceptance = base_acceptance_rate * (decay_rate ** i)\n",
    "        cumulative_prob *= head_acceptance\n",
    "        expected_accepted += cumulative_prob\n",
    "    \n",
    "    # Add 1 for the base model token (always generated)\n",
    "    avg_accepted = 1 + expected_accepted\n",
    "    \n",
    "    # Theoretical speedup = tokens per forward pass\n",
    "    theoretical_speedup = avg_accepted\n",
    "    \n",
    "    # Overhead from running additional heads\n",
    "    overhead_factor = 1 + (num_heads * overhead_per_head)\n",
    "    \n",
    "    # Effective speedup accounts for overhead\n",
    "    effective_speedup = theoretical_speedup / overhead_factor\n",
    "    \n",
    "    # Memory overhead\n",
    "    memory_overhead = num_heads * head_params_mb\n",
    "    \n",
    "    return HeadAnalysisResult(\n",
    "        num_heads=num_heads,\n",
    "        avg_accepted=avg_accepted,\n",
    "        theoretical_speedup=theoretical_speedup,\n",
    "        overhead_factor=overhead_factor,\n",
    "        effective_speedup=effective_speedup,\n",
    "        memory_overhead_mb=memory_overhead\n",
    "    )\n",
    "\n",
    "\n",
    "def find_optimal_configuration(max_heads: int = 10,\n",
    "                                max_memory_mb: float = 500) -> Tuple[int, HeadAnalysisResult]:\n",
    "    \"\"\"\n",
    "    Find the optimal number of Medusa heads.\n",
    "    \n",
    "    Trade-offs:\n",
    "    - More heads = more speculation = higher potential speedup\n",
    "    - More heads = more overhead = diminishing returns\n",
    "    - More heads = more memory usage\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for num_heads in range(1, max_heads + 1):\n",
    "        result = analyze_head_configuration(num_heads)\n",
    "        \n",
    "        # Skip if exceeds memory budget\n",
    "        if result.memory_overhead_mb > max_memory_mb:\n",
    "            break\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    # Find best effective speedup\n",
    "    best = max(results, key=lambda x: x.effective_speedup)\n",
    "    return best.num_heads, best\n",
    "\n",
    "\n",
    "# Analyze different configurations\n",
    "print(\"ðŸ“Š Medusa Head Configuration Analysis\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Heads':>6} {'Avg Accepted':>14} {'Theoretical':>12} {'Overhead':>10} {'Effective':>10} {'Memory':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for num_heads in range(1, 11):\n",
    "    result = analyze_head_configuration(num_heads)\n",
    "    marker = \"â†\" if result.num_heads == 5 else \"\"\n",
    "    print(f\"{result.num_heads:>6} {result.avg_accepted:>14.2f} \"\n",
    "          f\"{result.theoretical_speedup:>11.2f}x {result.overhead_factor:>9.2f}x \"\n",
    "          f\"{result.effective_speedup:>9.2f}x {result.memory_overhead_mb:>8.0f}MB {marker}\")\n",
    "\n",
    "optimal_heads, optimal_result = find_optimal_configuration()\n",
    "print(f\"\\nâœ… Optimal Configuration: {optimal_heads} heads\")\n",
    "print(f\"   Effective speedup: {optimal_result.effective_speedup:.2f}x\")\n",
    "print(f\"   Memory overhead: {optimal_result.memory_overhead_mb:.0f}MB\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insights:\")\n",
    "print(\"   â€¢ 4-6 heads typically optimal for most models\")\n",
    "print(\"   â€¢ Beyond 6 heads, overhead outweighs benefit\")\n",
    "print(\"   â€¢ Acceptance rate drops ~10-20% per head\")\n",
    "print(\"   â€¢ Consider memory budget for production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Benchmark Different Task Types\n",
    "\n",
    "**Task**: Measure how Medusa performs on different types of tasks (creative vs factual)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass \n",
    "class TaskTypeResult:\n",
    "    \"\"\"Benchmark results for a specific task type.\"\"\"\n",
    "    task_type: str\n",
    "    avg_acceptance_rate: float\n",
    "    avg_tokens_per_step: float\n",
    "    effective_speedup: float\n",
    "    sample_prompt: str\n",
    "\n",
    "\n",
    "# Task type characteristics\n",
    "TASK_CHARACTERISTICS = {\n",
    "    \"code_completion\": {\n",
    "        \"description\": \"Complete code snippets\",\n",
    "        \"expected_acceptance\": 0.85,  # High - code is predictable\n",
    "        \"temperature\": 0.1,\n",
    "        \"prompts\": [\n",
    "            \"def fibonacci(n):\\n    if n <= 1:\\n        return n\\n    return\",\n",
    "            \"class DataLoader:\\n    def __init__(self, data):\\n        self.data = data\\n    def __len__(self):\",\n",
    "            \"import numpy as np\\n\\ndef matrix_multiply(a, b):\\n    return\",\n",
    "        ]\n",
    "    },\n",
    "    \"factual_qa\": {\n",
    "        \"description\": \"Answer factual questions\",\n",
    "        \"expected_acceptance\": 0.75,  # Medium-high - common knowledge\n",
    "        \"temperature\": 0.3,\n",
    "        \"prompts\": [\n",
    "            \"What is the capital of France? The capital is\",\n",
    "            \"Python was created by Guido van Rossum in\",\n",
    "            \"The speed of light is approximately\",\n",
    "        ]\n",
    "    },\n",
    "    \"summarization\": {\n",
    "        \"description\": \"Summarize text\",\n",
    "        \"expected_acceptance\": 0.70,  # Medium - some variation\n",
    "        \"temperature\": 0.5,\n",
    "        \"prompts\": [\n",
    "            \"Summarize: Machine learning is a subset of AI. TL;DR:\",\n",
    "            \"The key points are:\",\n",
    "            \"In summary,\",\n",
    "        ]\n",
    "    },\n",
    "    \"creative_writing\": {\n",
    "        \"description\": \"Creative/open-ended writing\",\n",
    "        \"expected_acceptance\": 0.55,  # Lower - many valid continuations\n",
    "        \"temperature\": 0.9,\n",
    "        \"prompts\": [\n",
    "            \"Once upon a time in a galaxy far away,\",\n",
    "            \"Write a poem about the ocean:\",\n",
    "            \"Imagine a world where robots\",\n",
    "        ]\n",
    "    },\n",
    "    \"translation\": {\n",
    "        \"description\": \"Translate between languages\",\n",
    "        \"expected_acceptance\": 0.80,  # High - deterministic mapping\n",
    "        \"temperature\": 0.2,\n",
    "        \"prompts\": [\n",
    "            \"Translate to French: Hello, how are you?\\nFrench:\",\n",
    "            \"English: The cat is on the mat.\\nSpanish:\",\n",
    "            \"æ—¥æœ¬èªž: ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™\\nEnglish:\",\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def simulate_task_benchmark(task_type: str, \n",
    "                            num_samples: int = 100,\n",
    "                            num_heads: int = 5) -> TaskTypeResult:\n",
    "    \"\"\"\n",
    "    Simulate Medusa performance on a specific task type.\n",
    "    \"\"\"\n",
    "    config = TASK_CHARACTERISTICS[task_type]\n",
    "    base_acceptance = config[\"expected_acceptance\"]\n",
    "    \n",
    "    # Simulate multiple generation steps\n",
    "    total_accepted = 0\n",
    "    total_steps = 0\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Simulate one generation step\n",
    "        accepted = 0\n",
    "        cumulative_prob = 1.0\n",
    "        \n",
    "        for head_idx in range(num_heads):\n",
    "            # Acceptance decreases with each head\n",
    "            head_acceptance = base_acceptance * (0.9 ** head_idx)\n",
    "            # Add some randomness\n",
    "            head_acceptance *= np.random.uniform(0.9, 1.1)\n",
    "            \n",
    "            if np.random.random() < head_acceptance * cumulative_prob:\n",
    "                accepted += 1\n",
    "                cumulative_prob *= head_acceptance\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        total_accepted += accepted + 1  # +1 for base model token\n",
    "        total_steps += 1\n",
    "    \n",
    "    avg_tokens = total_accepted / total_steps\n",
    "    \n",
    "    return TaskTypeResult(\n",
    "        task_type=task_type,\n",
    "        avg_acceptance_rate=base_acceptance,\n",
    "        avg_tokens_per_step=avg_tokens,\n",
    "        effective_speedup=avg_tokens / 1.25,  # Account for overhead\n",
    "        sample_prompt=config[\"prompts\"][0][:50] + \"...\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Run benchmarks for all task types\n",
    "print(\"ðŸ“Š Medusa Performance by Task Type\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Task Type':<20} {'Accept Rate':>12} {'Tokens/Step':>12} {'Speedup':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "results = []\n",
    "for task_type in TASK_CHARACTERISTICS:\n",
    "    result = simulate_task_benchmark(task_type)\n",
    "    results.append(result)\n",
    "    \n",
    "    bar = \"â–ˆ\" * int(result.effective_speedup * 3)\n",
    "    print(f\"{task_type:<20} {result.avg_acceptance_rate:>11.0%} \"\n",
    "          f\"{result.avg_tokens_per_step:>12.1f} {result.effective_speedup:>9.2f}x {bar}\")\n",
    "\n",
    "# Analysis\n",
    "best = max(results, key=lambda x: x.effective_speedup)\n",
    "worst = min(results, key=lambda x: x.effective_speedup)\n",
    "\n",
    "print(f\"\\nâœ… Best performance: {best.task_type} ({best.effective_speedup:.2f}x)\")\n",
    "print(f\"âš ï¸  Worst performance: {worst.task_type} ({worst.effective_speedup:.2f}x)\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insights:\")\n",
    "print(\"   â€¢ Code completion benefits most (predictable patterns)\")\n",
    "print(\"   â€¢ Creative writing benefits least (high entropy)\")\n",
    "print(\"   â€¢ Consider temperature: lower temp = better speculation\")\n",
    "print(\"   â€¢ Task-aware routing can optimize overall throughput\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Implement Adaptive Speculation Depth\n",
    "\n",
    "**Task**: Create a system that adjusts speculation depth based on acceptance rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveSpeculationController:\n",
    "    \"\"\"\n",
    "    Dynamically adjust Medusa speculation depth based on observed acceptance.\n",
    "    \n",
    "    Key idea: If acceptance rate is high, use more heads.\n",
    "    If acceptance rate is low, use fewer heads to reduce overhead.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 min_heads: int = 2,\n",
    "                 max_heads: int = 8,\n",
    "                 target_efficiency: float = 0.6,\n",
    "                 window_size: int = 50):\n",
    "        self.min_heads = min_heads\n",
    "        self.max_heads = max_heads\n",
    "        self.target_efficiency = target_efficiency\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        self.current_heads = (min_heads + max_heads) // 2\n",
    "        self.acceptance_history: List[float] = []\n",
    "        self.heads_history: List[int] = []\n",
    "    \n",
    "    def record_step(self, tokens_proposed: int, tokens_accepted: int):\n",
    "        \"\"\"\n",
    "        Record the result of a speculation step.\n",
    "        \n",
    "        tokens_proposed: How many tokens we tried to generate\n",
    "        tokens_accepted: How many were actually correct\n",
    "        \"\"\"\n",
    "        acceptance_rate = tokens_accepted / tokens_proposed if tokens_proposed > 0 else 0\n",
    "        self.acceptance_history.append(acceptance_rate)\n",
    "        self.heads_history.append(self.current_heads)\n",
    "        \n",
    "        # Keep window limited\n",
    "        if len(self.acceptance_history) > self.window_size:\n",
    "            self.acceptance_history.pop(0)\n",
    "            self.heads_history.pop(0)\n",
    "        \n",
    "        # Adjust speculation depth\n",
    "        self._adjust_depth()\n",
    "    \n",
    "    def _adjust_depth(self):\n",
    "        \"\"\"Adjust the number of speculation heads based on recent performance.\"\"\"\n",
    "        if len(self.acceptance_history) < 10:\n",
    "            return  # Need more data\n",
    "        \n",
    "        # Calculate efficiency: accepted / (proposed * overhead)\n",
    "        recent_acceptance = np.mean(self.acceptance_history[-10:])\n",
    "        overhead = 1 + (self.current_heads * 0.05)\n",
    "        efficiency = recent_acceptance / overhead\n",
    "        \n",
    "        # Adjust based on efficiency vs target\n",
    "        if efficiency > self.target_efficiency * 1.1:  # 10% above target\n",
    "            # Acceptance is good, try more heads\n",
    "            self.current_heads = min(self.max_heads, self.current_heads + 1)\n",
    "        elif efficiency < self.target_efficiency * 0.9:  # 10% below target\n",
    "            # Acceptance is poor, use fewer heads\n",
    "            self.current_heads = max(self.min_heads, self.current_heads - 1)\n",
    "    \n",
    "    def get_speculation_config(self) -> dict:\n",
    "        \"\"\"Get current speculation configuration.\"\"\"\n",
    "        recent_acceptance = np.mean(self.acceptance_history[-10:]) if len(self.acceptance_history) >= 10 else 0.5\n",
    "        \n",
    "        return {\n",
    "            \"num_heads\": self.current_heads,\n",
    "            \"recent_acceptance\": recent_acceptance,\n",
    "            \"samples_collected\": len(self.acceptance_history)\n",
    "        }\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Get detailed statistics.\"\"\"\n",
    "        if not self.acceptance_history:\n",
    "            return {\"status\": \"no data\"}\n",
    "        \n",
    "        return {\n",
    "            \"total_steps\": len(self.acceptance_history),\n",
    "            \"current_heads\": self.current_heads,\n",
    "            \"avg_acceptance\": np.mean(self.acceptance_history),\n",
    "            \"recent_acceptance\": np.mean(self.acceptance_history[-10:]),\n",
    "            \"heads_changes\": len(set(self.heads_history)),\n",
    "            \"avg_heads_used\": np.mean(self.heads_history)\n",
    "        }\n",
    "\n",
    "\n",
    "# Simulate adaptive speculation\n",
    "print(\"ðŸŽ¯ Adaptive Speculation Controller Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "controller = AdaptiveSpeculationController(\n",
    "    min_heads=2,\n",
    "    max_heads=8,\n",
    "    target_efficiency=0.6\n",
    ")\n",
    "\n",
    "# Simulate different phases\n",
    "print(\"\\nSimulating generation with varying difficulty...\\n\")\n",
    "\n",
    "phases = [\n",
    "    (\"Code completion (easy)\", 0.85, 30),\n",
    "    (\"Technical writing (medium)\", 0.70, 30),\n",
    "    (\"Creative writing (hard)\", 0.50, 30),\n",
    "    (\"Back to code (easy)\", 0.85, 30),\n",
    "]\n",
    "\n",
    "print(f\"{'Phase':<30} {'Acceptance':>12} {'Heads Used':>12}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for phase_name, base_acceptance, steps in phases:\n",
    "    phase_heads = []\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        # Simulate with current configuration\n",
    "        config = controller.get_speculation_config()\n",
    "        proposed = config[\"num_heads\"]\n",
    "        \n",
    "        # Simulate acceptance (with noise)\n",
    "        acceptance = base_acceptance * np.random.uniform(0.8, 1.2)\n",
    "        accepted = int(proposed * min(1.0, acceptance))\n",
    "        \n",
    "        controller.record_step(proposed, accepted)\n",
    "        phase_heads.append(config[\"num_heads\"])\n",
    "    \n",
    "    avg_heads = np.mean(phase_heads)\n",
    "    print(f\"{phase_name:<30} {base_acceptance:>11.0%} {avg_heads:>12.1f}\")\n",
    "\n",
    "# Final stats\n",
    "stats = controller.get_stats()\n",
    "print(f\"\\nðŸ“Š Final Statistics:\")\n",
    "print(f\"   Total steps: {stats['total_steps']}\")\n",
    "print(f\"   Average heads used: {stats['avg_heads_used']:.1f}\")\n",
    "print(f\"   Overall acceptance: {stats['avg_acceptance']:.1%}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Observation: Controller automatically adjusts heads\")\n",
    "print(\"   based on content difficulty!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Medusa Architecture**: Multiple prediction heads speculate future tokens in parallel\n",
    "\n",
    "2. **Tree Attention**: Verify multiple candidate paths in a single forward pass\n",
    "\n",
    "3. **Optimal Heads**: 4-6 heads typically best; beyond that, overhead outweighs benefit\n",
    "\n",
    "4. **Task Dependence**: \n",
    "   - Code completion: High acceptance, big speedup\n",
    "   - Creative writing: Low acceptance, modest speedup\n",
    "\n",
    "5. **Adaptive Depth**: Dynamically adjust based on observed acceptance rates\n",
    "\n",
    "6. **Production Tip**: Match speculation depth to your workload characteristics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
