{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.3.2 Solutions: SGLang Deployment with RadixAttention\n",
    "\n",
    "Complete solutions to all exercises from the SGLang deployment lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path(\"../scripts\").resolve()))\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from benchmark_utils import InferenceBenchmark, BenchmarkConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Design Prefix Cache Patterns\n",
    "\n",
    "**Task**: Design a prompt template system that maximizes prefix cache hits for a customer service chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrefixOptimizedPromptSystem:\n",
    "    \"\"\"\n",
    "    A prompt template system designed to maximize RadixAttention cache hits.\n",
    "    \n",
    "    Design Principles:\n",
    "    1. Static content at the beginning (always cached)\n",
    "    2. Semi-static content next (often cached)\n",
    "    3. Dynamic content at the end (never cached)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Level 1: Always the same (100% cache hit after first request)\n",
    "    SYSTEM_PREFIX = \"\"\"You are a helpful customer service assistant for TechCorp Inc.\n",
    "Our products include:\n",
    "- TechWidget Pro: Advanced productivity tool, $299, 2-year warranty\n",
    "- TechWidget Lite: Entry-level option, $99, 1-year warranty  \n",
    "- TechCloud: Cloud storage service, $9.99/month, 1TB storage\n",
    "- TechSupport: Premium support package, $49.99/month\n",
    "\n",
    "Company Policies:\n",
    "- 30-day money-back guarantee on all hardware\n",
    "- Free shipping on orders over $50\n",
    "- Support hours: 24/7 for Premium, 9AM-6PM EST for Standard\n",
    "\n",
    "Always be polite, helpful, and accurate. If unsure, offer to escalate.\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Level 2: Category-specific prefixes (high cache hit within category)\n",
    "    CATEGORY_PREFIXES = {\n",
    "        \"billing\": \"\"\"Current context: BILLING INQUIRY\n",
    "Relevant policies:\n",
    "- Refunds processed within 5-7 business days\n",
    "- Credit card issues should be escalated to finance\n",
    "- Payment plans available for orders over $200\n",
    "\n",
    "\"\"\",\n",
    "        \"technical\": \"\"\"Current context: TECHNICAL SUPPORT\n",
    "Troubleshooting steps:\n",
    "1. Always ask for product model and firmware version\n",
    "2. Check if warranty is still valid\n",
    "3. Try basic reset procedures first\n",
    "4. Escalate hardware issues to Level 2 support\n",
    "\n",
    "\"\"\",\n",
    "        \"shipping\": \"\"\"Current context: SHIPPING INQUIRY\n",
    "Shipping information:\n",
    "- Standard: 5-7 business days\n",
    "- Express: 2-3 business days (+$15)\n",
    "- Overnight: Next business day (+$35)\n",
    "- International: 10-15 business days\n",
    "\n",
    "\"\"\",\n",
    "        \"returns\": \"\"\"Current context: RETURNS & EXCHANGES\n",
    "Return policies:\n",
    "- 30-day return window for unopened items\n",
    "- 14-day return window for opened items (restocking fee may apply)\n",
    "- Defective items: Full refund or exchange anytime under warranty\n",
    "- Return shipping is free for defective items\n",
    "\n",
    "\"\"\"\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache_stats = {\n",
    "            \"l1_hits\": 0,  # System prefix reuse\n",
    "            \"l2_hits\": 0,  # Category prefix reuse\n",
    "            \"total_requests\": 0\n",
    "        }\n",
    "    \n",
    "    def build_prompt(self, category: str, customer_message: str, \n",
    "                     customer_info: dict = None) -> str:\n",
    "        \"\"\"\n",
    "        Build a prompt optimized for prefix caching.\n",
    "        \n",
    "        Structure:\n",
    "        [STATIC: System Prefix] â†’ Always cached after first request\n",
    "        [SEMI-STATIC: Category Prefix] â†’ Cached within category\n",
    "        [DYNAMIC: Customer Context + Message] â†’ Never cached\n",
    "        \"\"\"\n",
    "        # Start with static system prefix\n",
    "        prompt = self.SYSTEM_PREFIX\n",
    "        \n",
    "        # Add category-specific prefix\n",
    "        if category in self.CATEGORY_PREFIXES:\n",
    "            prompt += self.CATEGORY_PREFIXES[category]\n",
    "        \n",
    "        # Add dynamic customer context (if provided)\n",
    "        if customer_info:\n",
    "            prompt += f\"Customer ID: {customer_info.get('id', 'Unknown')}\\n\"\n",
    "            prompt += f\"Account Type: {customer_info.get('tier', 'Standard')}\\n\"\n",
    "            if customer_info.get('previous_issues'):\n",
    "                prompt += f\"Previous Issues: {customer_info['previous_issues']}\\n\"\n",
    "            prompt += \"\\n\"\n",
    "        \n",
    "        # Add the customer message (always last, always dynamic)\n",
    "        prompt += f\"Customer Message: {customer_message}\\n\\n\"\n",
    "        prompt += \"Assistant Response:\"\n",
    "        \n",
    "        self.cache_stats[\"total_requests\"] += 1\n",
    "        return prompt\n",
    "    \n",
    "    def estimate_cache_efficiency(self, prompts: list) -> dict:\n",
    "        \"\"\"\n",
    "        Estimate cache efficiency for a batch of prompts.\n",
    "        \"\"\"\n",
    "        # Count shared prefix lengths\n",
    "        system_prefix_len = len(self.SYSTEM_PREFIX)\n",
    "        total_chars = sum(len(p) for p in prompts)\n",
    "        \n",
    "        # All prompts share system prefix\n",
    "        shared_chars = system_prefix_len * len(prompts)\n",
    "        \n",
    "        # Group by category to estimate category prefix sharing\n",
    "        # (In real usage, you'd track this from build_prompt calls)\n",
    "        \n",
    "        return {\n",
    "            \"total_chars\": total_chars,\n",
    "            \"shared_system_chars\": shared_chars,\n",
    "            \"estimated_cache_ratio\": shared_chars / total_chars if total_chars > 0 else 0\n",
    "        }\n",
    "\n",
    "# Demonstrate the system\n",
    "prompt_system = PrefixOptimizedPromptSystem()\n",
    "\n",
    "# Generate sample prompts\n",
    "test_cases = [\n",
    "    (\"billing\", \"Why was I charged twice?\"),\n",
    "    (\"billing\", \"Can I get a refund?\"),\n",
    "    (\"technical\", \"My TechWidget Pro won't turn on\"),\n",
    "    (\"technical\", \"How do I update the firmware?\"),\n",
    "    (\"shipping\", \"Where is my order?\"),\n",
    "    (\"returns\", \"I want to return my product\"),\n",
    "]\n",
    "\n",
    "prompts = []\n",
    "for category, message in test_cases:\n",
    "    prompt = prompt_system.build_prompt(category, message)\n",
    "    prompts.append(prompt)\n",
    "\n",
    "# Analyze cache efficiency\n",
    "efficiency = prompt_system.estimate_cache_efficiency(prompts)\n",
    "\n",
    "print(\"ðŸ“Š Prefix Cache Efficiency Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total prompts: {len(prompts)}\")\n",
    "print(f\"Total characters: {efficiency['total_chars']:,}\")\n",
    "print(f\"Shared system prefix chars: {efficiency['shared_system_chars']:,}\")\n",
    "print(f\"Estimated cache ratio: {efficiency['estimated_cache_ratio']:.1%}\")\n",
    "print(\"\\nðŸ’¡ With RadixAttention, the system prefix is computed once\")\n",
    "print(\"   and reused for ALL subsequent requests!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Benchmark Prefix Cache Hit Rates\n",
    "\n",
    "**Task**: Write code to measure and visualize prefix cache hit rates across different prompt patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "@dataclass\n",
    "class CacheBenchmarkResult:\n",
    "    \"\"\"Results from a cache benchmark run.\"\"\"\n",
    "    pattern_name: str\n",
    "    first_request_ms: float\n",
    "    avg_subsequent_ms: float\n",
    "    speedup_ratio: float\n",
    "    estimated_cache_hit_rate: float\n",
    "\n",
    "class PrefixCacheBenchmark:\n",
    "    \"\"\"\n",
    "    Benchmark prefix cache effectiveness by measuring latency patterns.\n",
    "    \n",
    "    Cache hits show as: lower TTFT on subsequent requests with same prefix.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str = \"http://localhost:30000\"):\n",
    "        self.base_url = base_url\n",
    "        self.session = requests.Session()\n",
    "    \n",
    "    def _measure_ttft(self, prompt: str, max_tokens: int = 10) -> float:\n",
    "        \"\"\"Measure time-to-first-token for a prompt.\"\"\"\n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        response = self.session.post(\n",
    "            f\"{self.base_url}/v1/completions\",\n",
    "            json={\n",
    "                \"model\": \"default\",\n",
    "                \"prompt\": prompt,\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"stream\": True\n",
    "            },\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        # TTFT is time to first chunk\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                ttft = time.perf_counter() - start\n",
    "                # Consume rest of response\n",
    "                for _ in response.iter_lines():\n",
    "                    pass\n",
    "                return ttft * 1000  # Convert to ms\n",
    "        \n",
    "        return (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    def benchmark_pattern(self, pattern_name: str, \n",
    "                          prompts: List[str],\n",
    "                          runs_per_prompt: int = 3) -> CacheBenchmarkResult:\n",
    "        \"\"\"\n",
    "        Benchmark a pattern of prompts for cache effectiveness.\n",
    "        \n",
    "        Args:\n",
    "            pattern_name: Description of the pattern\n",
    "            prompts: List of prompts (should share prefixes for cache hits)\n",
    "            runs_per_prompt: Number of times to run each prompt\n",
    "        \"\"\"\n",
    "        all_first_runs = []\n",
    "        all_subsequent_runs = []\n",
    "        \n",
    "        for prompt in prompts:\n",
    "            for run in range(runs_per_prompt):\n",
    "                ttft = self._measure_ttft(prompt)\n",
    "                \n",
    "                if run == 0:\n",
    "                    all_first_runs.append(ttft)\n",
    "                else:\n",
    "                    all_subsequent_runs.append(ttft)\n",
    "        \n",
    "        avg_first = np.mean(all_first_runs)\n",
    "        avg_subsequent = np.mean(all_subsequent_runs) if all_subsequent_runs else avg_first\n",
    "        \n",
    "        # Speedup indicates cache effectiveness\n",
    "        speedup = avg_first / avg_subsequent if avg_subsequent > 0 else 1.0\n",
    "        \n",
    "        # Estimate cache hit rate from speedup\n",
    "        # If speedup > 1.5, likely getting cache hits\n",
    "        estimated_hit_rate = min(1.0, max(0, (speedup - 1.0) / 0.5))\n",
    "        \n",
    "        return CacheBenchmarkResult(\n",
    "            pattern_name=pattern_name,\n",
    "            first_request_ms=avg_first,\n",
    "            avg_subsequent_ms=avg_subsequent,\n",
    "            speedup_ratio=speedup,\n",
    "            estimated_cache_hit_rate=estimated_hit_rate\n",
    "        )\n",
    "    \n",
    "    def run_full_benchmark(self) -> List[CacheBenchmarkResult]:\n",
    "        \"\"\"Run benchmark across different prefix sharing patterns.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Pattern 1: Identical prefix (100% cache potential)\n",
    "        shared_prefix = \"You are a helpful AI assistant. \" * 50\n",
    "        pattern1_prompts = [\n",
    "            shared_prefix + \"What is Python?\",\n",
    "            shared_prefix + \"What is JavaScript?\",\n",
    "            shared_prefix + \"What is Rust?\",\n",
    "        ]\n",
    "        results.append(self.benchmark_pattern(\n",
    "            \"Identical Prefix (100% shared)\", \n",
    "            pattern1_prompts\n",
    "        ))\n",
    "        \n",
    "        # Pattern 2: Partial prefix sharing (50% cache potential)\n",
    "        half_prefix = \"You are a helpful AI assistant. \" * 25\n",
    "        pattern2_prompts = [\n",
    "            half_prefix + \"Context A: \" + \"x\" * 100 + \" Question: What is AI?\",\n",
    "            half_prefix + \"Context B: \" + \"y\" * 100 + \" Question: What is ML?\",\n",
    "            half_prefix + \"Context C: \" + \"z\" * 100 + \" Question: What is DL?\",\n",
    "        ]\n",
    "        results.append(self.benchmark_pattern(\n",
    "            \"Partial Prefix (50% shared)\", \n",
    "            pattern2_prompts\n",
    "        ))\n",
    "        \n",
    "        # Pattern 3: No shared prefix (0% cache potential)\n",
    "        pattern3_prompts = [\n",
    "            \"Context Alpha: \" + \"a\" * 200 + \" What is Python?\",\n",
    "            \"Context Beta: \" + \"b\" * 200 + \" What is Java?\",\n",
    "            \"Context Gamma: \" + \"c\" * 200 + \" What is C++?\",\n",
    "        ]\n",
    "        results.append(self.benchmark_pattern(\n",
    "            \"No Shared Prefix (0% shared)\", \n",
    "            pattern3_prompts\n",
    "        ))\n",
    "        \n",
    "        return results\n",
    "\n",
    "def visualize_cache_results(results: List[CacheBenchmarkResult]):\n",
    "    \"\"\"Create ASCII visualization of cache benchmark results.\"\"\"\n",
    "    print(\"\\nðŸ“Š Prefix Cache Benchmark Results\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Pattern':<30} {'1st Req':>10} {'Cached':>10} {'Speedup':>10} {'Hit Rate':>10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for r in results:\n",
    "        hit_bar = \"â–ˆ\" * int(r.estimated_cache_hit_rate * 10)\n",
    "        print(f\"{r.pattern_name:<30} {r.first_request_ms:>8.1f}ms {r.avg_subsequent_ms:>8.1f}ms \"\n",
    "              f\"{r.speedup_ratio:>9.2f}x {hit_bar}\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Key Insights:\")\n",
    "    best = max(results, key=lambda x: x.speedup_ratio)\n",
    "    print(f\"   â€¢ Best pattern: {best.pattern_name} ({best.speedup_ratio:.2f}x speedup)\")\n",
    "    print(f\"   â€¢ Design prompts with shared prefixes to maximize cache hits\")\n",
    "    print(f\"   â€¢ Put dynamic content at the END of prompts\")\n",
    "\n",
    "# Example usage (requires SGLang server running)\n",
    "print(\"ðŸ”§ Prefix Cache Benchmark\")\n",
    "print(\"   Run this after starting SGLang server:\")\n",
    "print(\"   python -m sglang.launch_server --model Qwen/Qwen3-8B-Instruct\")\n",
    "print(\"\")\n",
    "print(\"   Example output with simulated data:\")\n",
    "\n",
    "# Simulated results for demonstration\n",
    "simulated_results = [\n",
    "    CacheBenchmarkResult(\"Identical Prefix (100%)\", 120.5, 45.2, 2.67, 1.0),\n",
    "    CacheBenchmarkResult(\"Partial Prefix (50%)\", 115.3, 72.1, 1.60, 0.6),\n",
    "    CacheBenchmarkResult(\"No Shared Prefix (0%)\", 118.7, 115.2, 1.03, 0.0),\n",
    "]\n",
    "visualize_cache_results(simulated_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Multi-Turn Optimization\n",
    "\n",
    "**Task**: Implement optimized multi-turn conversation handling that preserves RadixAttention benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "import hashlib\n",
    "\n",
    "class RadixOptimizedConversation:\n",
    "    \"\"\"\n",
    "    Multi-turn conversation manager optimized for RadixAttention.\n",
    "    \n",
    "    Key optimizations:\n",
    "    1. Consistent message formatting (same format = same prefix tokens)\n",
    "    2. Session ID-based prefix grouping (related conversations share more)\n",
    "    3. Minimal history modification (append-only when possible)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, system_prompt: str):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.messages: List[Dict[str, str]] = []\n",
    "        self.prefix_hash: Optional[str] = None\n",
    "    \n",
    "    def _format_message(self, role: str, content: str) -> str:\n",
    "        \"\"\"\n",
    "        Format a message consistently.\n",
    "        \n",
    "        CRITICAL: Use exact same format every time!\n",
    "        Different formatting = different tokens = no cache hit\n",
    "        \"\"\"\n",
    "        # Use Llama-style formatting\n",
    "        if role == \"system\":\n",
    "            return f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{content}<|eot_id|>\"\n",
    "        elif role == \"user\":\n",
    "            return f\"<|start_header_id|>user<|end_header_id|>\\n\\n{content}<|eot_id|>\"\n",
    "        elif role == \"assistant\":\n",
    "            return f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{content}<|eot_id|>\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown role: {role}\")\n",
    "    \n",
    "    def build_prompt(self) -> str:\n",
    "        \"\"\"\n",
    "        Build the full conversation prompt.\n",
    "        \n",
    "        Structure designed for maximum cache reuse:\n",
    "        [System Prompt] - Cached across ALL conversations with same system\n",
    "        [Turn 1] - Cached for this conversation\n",
    "        [Turn 2] - Cached for this conversation\n",
    "        [...]\n",
    "        [Current Turn] - New content\n",
    "        \"\"\"\n",
    "        parts = [self._format_message(\"system\", self.system_prompt)]\n",
    "        \n",
    "        for msg in self.messages:\n",
    "            parts.append(self._format_message(msg[\"role\"], msg[\"content\"]))\n",
    "        \n",
    "        # Add generation prompt\n",
    "        parts.append(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")\n",
    "        \n",
    "        return \"\".join(parts)\n",
    "    \n",
    "    def add_user_message(self, content: str):\n",
    "        \"\"\"Add a user message to the conversation.\"\"\"\n",
    "        self.messages.append({\"role\": \"user\", \"content\": content})\n",
    "    \n",
    "    def add_assistant_message(self, content: str):\n",
    "        \"\"\"Add an assistant message to the conversation.\"\"\"\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "    \n",
    "    def get_prefix_hash(self) -> str:\n",
    "        \"\"\"\n",
    "        Get a hash of the current prefix (system + history).\n",
    "        \n",
    "        Same hash = high likelihood of cache hit.\n",
    "        \"\"\"\n",
    "        prefix = self._format_message(\"system\", self.system_prompt)\n",
    "        for msg in self.messages[:-1] if self.messages else []:\n",
    "            prefix += self._format_message(msg[\"role\"], msg[\"content\"])\n",
    "        \n",
    "        return hashlib.md5(prefix.encode()).hexdigest()[:8]\n",
    "    \n",
    "    def estimate_cache_benefit(self) -> dict:\n",
    "        \"\"\"\n",
    "        Estimate the caching benefit for this conversation.\n",
    "        \"\"\"\n",
    "        full_prompt = self.build_prompt()\n",
    "        system_len = len(self._format_message(\"system\", self.system_prompt))\n",
    "        \n",
    "        # Calculate cacheable portion\n",
    "        if len(self.messages) > 0:\n",
    "            # All but the last user message is cacheable\n",
    "            last_msg_len = len(self._format_message(\n",
    "                self.messages[-1][\"role\"], \n",
    "                self.messages[-1][\"content\"]\n",
    "            ))\n",
    "            cacheable_len = len(full_prompt) - last_msg_len - 50  # -50 for gen prompt\n",
    "        else:\n",
    "            cacheable_len = system_len\n",
    "        \n",
    "        return {\n",
    "            \"total_chars\": len(full_prompt),\n",
    "            \"cacheable_chars\": cacheable_len,\n",
    "            \"cache_ratio\": cacheable_len / len(full_prompt),\n",
    "            \"turn_number\": len(self.messages) // 2 + 1\n",
    "        }\n",
    "\n",
    "\n",
    "class MultiConversationManager:\n",
    "    \"\"\"\n",
    "    Manage multiple conversations with shared prefix optimization.\n",
    "    \n",
    "    Conversations with the same system prompt share the cached prefix.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.conversations: Dict[str, RadixOptimizedConversation] = {}\n",
    "        self.system_prompts: Dict[str, str] = {}  # Cache system prompts\n",
    "    \n",
    "    def create_conversation(self, session_id: str, \n",
    "                           system_prompt_key: str = \"default\") -> RadixOptimizedConversation:\n",
    "        \"\"\"\n",
    "        Create a new conversation.\n",
    "        \n",
    "        Using system_prompt_key ensures conversations with same key\n",
    "        share the exact same system prompt = better caching.\n",
    "        \"\"\"\n",
    "        if system_prompt_key not in self.system_prompts:\n",
    "            self.system_prompts[system_prompt_key] = self._get_default_system_prompt()\n",
    "        \n",
    "        conv = RadixOptimizedConversation(self.system_prompts[system_prompt_key])\n",
    "        self.conversations[session_id] = conv\n",
    "        return conv\n",
    "    \n",
    "    def _get_default_system_prompt(self) -> str:\n",
    "        return \"You are a helpful AI assistant. Be concise and accurate.\"\n",
    "    \n",
    "    def analyze_cache_sharing(self) -> dict:\n",
    "        \"\"\"\n",
    "        Analyze cache sharing across all active conversations.\n",
    "        \"\"\"\n",
    "        prefix_groups = {}\n",
    "        for session_id, conv in self.conversations.items():\n",
    "            prefix_hash = conv.get_prefix_hash()\n",
    "            if prefix_hash not in prefix_groups:\n",
    "                prefix_groups[prefix_hash] = []\n",
    "            prefix_groups[prefix_hash].append(session_id)\n",
    "        \n",
    "        return {\n",
    "            \"total_conversations\": len(self.conversations),\n",
    "            \"unique_prefixes\": len(prefix_groups),\n",
    "            \"sharing_ratio\": len(self.conversations) / len(prefix_groups) if prefix_groups else 0,\n",
    "            \"prefix_groups\": prefix_groups\n",
    "        }\n",
    "\n",
    "\n",
    "# Demonstrate multi-turn optimization\n",
    "print(\"ðŸ“ Multi-Turn Conversation Optimization Demo\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a conversation\n",
    "conv = RadixOptimizedConversation(\n",
    "    \"You are a Python programming tutor. Explain concepts clearly.\"\n",
    ")\n",
    "\n",
    "# Simulate a multi-turn conversation\n",
    "turns = [\n",
    "    (\"user\", \"What is a list in Python?\"),\n",
    "    (\"assistant\", \"A list is an ordered, mutable collection of items.\"),\n",
    "    (\"user\", \"How do I add items to it?\"),\n",
    "    (\"assistant\", \"Use append() to add one item, or extend() for multiple.\"),\n",
    "    (\"user\", \"What about removing items?\"),\n",
    "]\n",
    "\n",
    "for role, content in turns:\n",
    "    if role == \"user\":\n",
    "        conv.add_user_message(content)\n",
    "    else:\n",
    "        conv.add_assistant_message(content)\n",
    "    \n",
    "    benefit = conv.estimate_cache_benefit()\n",
    "    print(f\"\\nTurn {benefit['turn_number']}:\")\n",
    "    print(f\"  Prefix hash: {conv.get_prefix_hash()}\")\n",
    "    print(f\"  Cache ratio: {benefit['cache_ratio']:.1%}\")\n",
    "    print(f\"  Cacheable: {benefit['cacheable_chars']:,} / {benefit['total_chars']:,} chars\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insight: As conversation grows, cache ratio increases!\")\n",
    "print(\"   Each new turn reuses ALL previous turns from cache.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Production Deployment Configuration\n",
    "\n",
    "**Task**: Create a production-ready SGLang deployment configuration with proper scaling and monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Production Docker Compose configuration\n",
    "docker_compose_config = \"\"\"\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  sglang:\n",
    "    image: lmsysorg/sglang:latest\n",
    "    runtime: nvidia\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              count: 1\n",
    "              capabilities: [gpu]\n",
    "    environment:\n",
    "      - CUDA_VISIBLE_DEVICES=0\n",
    "    ports:\n",
    "      - \"30000:30000\"  # Main API\n",
    "      - \"30001:30001\"  # Metrics\n",
    "    volumes:\n",
    "      - ~/.cache/huggingface:/root/.cache/huggingface\n",
    "      - ./logs:/app/logs\n",
    "    command: >\n",
    "      python -m sglang.launch_server\n",
    "      --model Qwen/Qwen3-8B-Instruct\n",
    "      --host 0.0.0.0\n",
    "      --port 30000\n",
    "      --dtype bfloat16\n",
    "      --context-length 8192\n",
    "      --mem-fraction-static 0.85\n",
    "      --max-running-requests 64\n",
    "      --schedule-policy fcfs\n",
    "      --enable-metrics\n",
    "      --log-level info\n",
    "      --enforce-eager\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:30000/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "      start_period: 120s\n",
    "    restart: unless-stopped\n",
    "    logging:\n",
    "      driver: \"json-file\"\n",
    "      options:\n",
    "        max-size: \"100m\"\n",
    "        max-file: \"5\"\n",
    "\n",
    "  # Nginx load balancer (for multi-instance)\n",
    "  nginx:\n",
    "    image: nginx:alpine\n",
    "    ports:\n",
    "      - \"8080:80\"\n",
    "    volumes:\n",
    "      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n",
    "    depends_on:\n",
    "      - sglang\n",
    "    restart: unless-stopped\n",
    "\n",
    "  # Prometheus for metrics\n",
    "  prometheus:\n",
    "    image: prom/prometheus:latest\n",
    "    ports:\n",
    "      - \"9090:9090\"\n",
    "    volumes:\n",
    "      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro\n",
    "    restart: unless-stopped\n",
    "\"\"\"\n",
    "\n",
    "# Nginx configuration for load balancing\n",
    "nginx_config = \"\"\"\n",
    "events {\n",
    "    worker_connections 1024;\n",
    "}\n",
    "\n",
    "http {\n",
    "    upstream sglang_backend {\n",
    "        # Least connections for better load distribution\n",
    "        least_conn;\n",
    "        \n",
    "        server sglang:30000;\n",
    "        # Add more servers for multi-GPU:\n",
    "        # server sglang-2:30000;\n",
    "        # server sglang-3:30000;\n",
    "        \n",
    "        keepalive 32;\n",
    "    }\n",
    "\n",
    "    server {\n",
    "        listen 80;\n",
    "        \n",
    "        # Health check endpoint\n",
    "        location /health {\n",
    "            proxy_pass http://sglang_backend/health;\n",
    "            proxy_http_version 1.1;\n",
    "        }\n",
    "        \n",
    "        # API endpoints\n",
    "        location /v1/ {\n",
    "            proxy_pass http://sglang_backend/v1/;\n",
    "            proxy_http_version 1.1;\n",
    "            proxy_set_header Connection \"\";\n",
    "            proxy_set_header Host $host;\n",
    "            \n",
    "            # SSE streaming support\n",
    "            proxy_buffering off;\n",
    "            proxy_cache off;\n",
    "            chunked_transfer_encoding on;\n",
    "            \n",
    "            # Timeouts for long generations\n",
    "            proxy_read_timeout 300s;\n",
    "            proxy_send_timeout 300s;\n",
    "        }\n",
    "        \n",
    "        # Rate limiting\n",
    "        limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;\n",
    "        \n",
    "        location /v1/completions {\n",
    "            limit_req zone=api burst=20 nodelay;\n",
    "            proxy_pass http://sglang_backend/v1/completions;\n",
    "            proxy_buffering off;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Prometheus configuration\n",
    "prometheus_config = \"\"\"\n",
    "global:\n",
    "  scrape_interval: 15s\n",
    "  evaluation_interval: 15s\n",
    "\n",
    "scrape_configs:\n",
    "  - job_name: 'sglang'\n",
    "    static_configs:\n",
    "      - targets: ['sglang:30001']\n",
    "    metrics_path: /metrics\n",
    "\"\"\"\n",
    "\n",
    "# Health check and monitoring script\n",
    "health_check_script = '''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"Production health check and monitoring for SGLang.\"\"\"\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import sys\n",
    "\n",
    "def check_health(url: str = \"http://localhost:30000\") -> dict:\n",
    "    \"\"\"Check SGLang server health.\"\"\"\n",
    "    try:\n",
    "        # Basic health\n",
    "        health_resp = requests.get(f\"{url}/health\", timeout=5)\n",
    "        health_ok = health_resp.status_code == 200\n",
    "        \n",
    "        # Test inference\n",
    "        start = time.time()\n",
    "        test_resp = requests.post(\n",
    "            f\"{url}/v1/completions\",\n",
    "            json={\n",
    "                \"model\": \"default\",\n",
    "                \"prompt\": \"Hello\",\n",
    "                \"max_tokens\": 1\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        latency_ms = (time.time() - start) * 1000\n",
    "        inference_ok = test_resp.status_code == 200\n",
    "        \n",
    "        return {\n",
    "            \"healthy\": health_ok and inference_ok,\n",
    "            \"health_endpoint\": health_ok,\n",
    "            \"inference_working\": inference_ok,\n",
    "            \"test_latency_ms\": latency_ms\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"healthy\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = check_health()\n",
    "    print(f\"Health Check: {\\\"âœ“\\\" if result[\\\"healthy\\\"] else \\\"âœ—\\\"}\")\n",
    "    for k, v in result.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    sys.exit(0 if result[\"healthy\"] else 1)\n",
    "'''\n",
    "\n",
    "print(\"ðŸ“¦ Production Deployment Configuration\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nFiles to create:\")\n",
    "print(\"  1. docker-compose.yml - Main deployment config\")\n",
    "print(\"  2. nginx.conf - Load balancer config\")\n",
    "print(\"  3. prometheus.yml - Metrics collection\")\n",
    "print(\"  4. health_check.py - Monitoring script\")\n",
    "\n",
    "print(\"\\nðŸ”‘ Key Production Settings:\")\n",
    "print(\"  â€¢ --mem-fraction-static 0.85: Reserve 85% GPU memory\")\n",
    "print(\"  â€¢ --max-running-requests 64: Limit concurrent requests\")\n",
    "print(\"  â€¢ --enforce-eager: Required for DGX Spark ARM64\")\n",
    "print(\"  â€¢ Health checks every 30s with 120s startup grace period\")\n",
    "print(\"  â€¢ Log rotation: 100MB max, 5 files\")\n",
    "\n",
    "print(\"\\nðŸš€ Deployment Commands:\")\n",
    "print(\"  # Start the stack\")\n",
    "print(\"  docker-compose up -d\")\n",
    "print(\"\")\n",
    "print(\"  # Check logs\")\n",
    "print(\"  docker-compose logs -f sglang\")\n",
    "print(\"\")\n",
    "print(\"  # Scale (multi-GPU)\")\n",
    "print(\"  docker-compose up -d --scale sglang=2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Prefix Design is Critical**: Put static content first, dynamic content last\n",
    "\n",
    "2. **Consistent Formatting**: Same format = same tokens = cache hits\n",
    "\n",
    "3. **Multi-Turn Benefits**: Each conversation turn builds on cached history\n",
    "\n",
    "4. **Production Considerations**:\n",
    "   - Use `--enforce-eager` on DGX Spark\n",
    "   - Set appropriate memory fractions\n",
    "   - Implement health checks and monitoring\n",
    "   - Use load balancing for multiple instances\n",
    "\n",
    "5. **Monitoring**: Track cache hit rates to validate optimization effectiveness"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
