{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.3.4: Speculative Decoding - Solutions\n",
    "\n",
    "This notebook provides solutions for the exercises in the speculative decoding notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Compare Speculation Lengths\n",
    "\n",
    "Test different `--speculative-num-draft-tokens` values (3, 5, 8, 12) and measure tokens/second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Testing different speculation lengths\n",
    "\n",
    "# You would run SGLang with different configurations and measure performance:\n",
    "# python -m sglang.launch_server ... --speculative-num-draft-tokens 3\n",
    "# python -m sglang.launch_server ... --speculative-num-draft-tokens 5\n",
    "# etc.\n",
    "\n",
    "# Expected results for predictable prompts (e.g., \"Count from 1 to 50\"):\n",
    "speculation_length_results = {\n",
    "    \"no_speculation\": {\n",
    "        \"tokens_per_sec\": 45,\n",
    "        \"acceptance_rate\": None,\n",
    "        \"speedup\": 1.0\n",
    "    },\n",
    "    \"draft_tokens_3\": {\n",
    "        \"tokens_per_sec\": 85,\n",
    "        \"acceptance_rate\": 0.82,\n",
    "        \"speedup\": 1.9\n",
    "    },\n",
    "    \"draft_tokens_5\": {\n",
    "        \"tokens_per_sec\": 110,\n",
    "        \"acceptance_rate\": 0.78,\n",
    "        \"speedup\": 2.4\n",
    "    },\n",
    "    \"draft_tokens_8\": {\n",
    "        \"tokens_per_sec\": 105,\n",
    "        \"acceptance_rate\": 0.72,\n",
    "        \"speedup\": 2.3\n",
    "    },\n",
    "    \"draft_tokens_12\": {\n",
    "        \"tokens_per_sec\": 95,\n",
    "        \"acceptance_rate\": 0.65,\n",
    "        \"speedup\": 2.1\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Speculation Length Comparison (Predictable Prompt):\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Configuration':<20} {'Tokens/sec':<15} {'Accept Rate':<15} {'Speedup':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for config, result in speculation_length_results.items():\n",
    "    accept = f\"{result['acceptance_rate']:.0%}\" if result['acceptance_rate'] else \"N/A\"\n",
    "    print(f\"{config:<20} {result['tokens_per_sec']:<15} {accept:<15} {result['speedup']:.1f}x\")\n",
    "\n",
    "print(\"\\nConclusion:\")\n",
    "print(\"- draft_tokens=5 is optimal for this workload (highest speedup)\")\n",
    "print(\"- Higher values (8, 12) have lower acceptance rates\")\n",
    "print(\"- The 'sweet spot' depends on your prompt type and draft model quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Acceptance Rate Analysis\n",
    "\n",
    "Create prompts that demonstrate high vs low acceptance rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: High vs Low Acceptance Rate Prompts\n",
    "\n",
    "# HIGH ACCEPTANCE prompts (predictable outputs)\n",
    "high_acceptance_prompts = [\n",
    "    \"Count from 1 to 30.\",\n",
    "    \"List all months of the year.\",\n",
    "    \"Write the first 10 lines of 'Twinkle Twinkle Little Star'.\",\n",
    "]\n",
    "\n",
    "# LOW ACCEPTANCE prompts (creative/unpredictable outputs)\n",
    "low_acceptance_prompts = [\n",
    "    \"Invent a completely new word and give it a creative definition.\",\n",
    "    \"Write an absurdist poem about quantum physics and breakfast.\",\n",
    "    \"Create a unique fictional language greeting with pronunciation guide.\",\n",
    "]\n",
    "\n",
    "# Expected results\n",
    "acceptance_comparison = {\n",
    "    \"High Acceptance (Predictable)\": {\n",
    "        \"prompts\": high_acceptance_prompts,\n",
    "        \"expected_acceptance_rate\": 0.85,\n",
    "        \"expected_speedup\": 2.4,\n",
    "        \"why\": \"Draft model easily predicts next tokens (sequences, patterns)\"\n",
    "    },\n",
    "    \"Low Acceptance (Creative)\": {\n",
    "        \"prompts\": low_acceptance_prompts,\n",
    "        \"expected_acceptance_rate\": 0.35,\n",
    "        \"expected_speedup\": 1.1,\n",
    "        \"why\": \"Creative output is hard to predict; most drafts rejected\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Acceptance Rate Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for category, data in acceptance_comparison.items():\n",
    "    print(f\"\\n{category}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Expected Acceptance Rate: {data['expected_acceptance_rate']:.0%}\")\n",
    "    print(f\"Expected Speedup: {data['expected_speedup']:.1f}x\")\n",
    "    print(f\"Why: {data['why']}\")\n",
    "    print(\"\\nExample prompts:\")\n",
    "    for prompt in data['prompts']:\n",
    "        print(f\"  - {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation for measuring acceptance rate\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "SGLANG_URL = \"http://localhost:30000\"\n",
    "\n",
    "def measure_speedup(prompt: str, max_tokens: int = 100) -> dict:\n",
    "    \"\"\"\n",
    "    Measure tokens/second for a prompt.\n",
    "    \n",
    "    Note: SGLang provides acceptance rate in response metadata\n",
    "    when using speculative decoding.\n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "    tokens_generated = 0\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{SGLANG_URL}/v1/chat/completions\",\n",
    "            json={\n",
    "                \"model\": \"default\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"stream\": True\n",
    "            },\n",
    "            stream=True,\n",
    "            timeout=60\n",
    "        )\n",
    "        \n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                line_str = line.decode()\n",
    "                if line_str.startswith(\"data: \") and \"[DONE]\" not in line_str:\n",
    "                    try:\n",
    "                        chunk = json.loads(line_str[6:])\n",
    "                        if chunk.get(\"choices\", [{}])[0].get(\"delta\", {}).get(\"content\"):\n",
    "                            tokens_generated += 1\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        elapsed = time.perf_counter() - start\n",
    "        return {\n",
    "            \"prompt\": prompt[:40] + \"...\",\n",
    "            \"tokens\": tokens_generated,\n",
    "            \"tokens_per_sec\": tokens_generated / elapsed if elapsed > 0 else 0\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "print(\"Test Function Ready\")\n",
    "print(\"Run measure_speedup(prompt) for each prompt type to compare speedups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Optimal speculation length** is typically 5-8 tokens\n",
    "2. **Predictable outputs** benefit most (2-3x speedup)\n",
    "3. **Creative outputs** see minimal benefit but never slow down\n",
    "4. **Test your workload** to find the right configuration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}