{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.3.3: TensorRT-LLM Optimization - Solutions\n",
    "\n",
    "This notebook provides solutions for the exercises in the TensorRT-LLM optimization notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Build a TensorRT Engine\n",
    "\n",
    "Document your build process for Llama 3.1 8B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solution: TensorRT-LLM Build Process Documentation\n\nbuild_config = {\n    \"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n    \"dtype\": \"bfloat16\",\n    \"max_input_len\": 4096,\n    \"max_output_len\": 2048,\n    \"max_batch_size\": 8,\n    \"plugins\": [\"gemm_plugin\", \"use_fused_mlp\"],\n    \"paged_kv_cache\": True,\n}\n\n# Step 1: Start the container\ncontainer_cmd = \"\"\"\ndocker run --gpus all -it --rm \\\\\n    -v ~/trtllm-workspace:/workspace \\\\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\\\n    -e HF_TOKEN=$HF_TOKEN \\\\\n    --ipc=host \\\\\n    --shm-size=16g \\\\\n    --ulimit memlock=-1 \\\\\n    nvcr.io/nvidia/tritonserver:25.11-trtllm-python-py3 \\\\\n    bash\n\"\"\"\n\n# Step 2: Convert checkpoint (inside container)\nconvert_cmd = \"\"\"\npython /opt/TensorRT-LLM/examples/llama/convert_checkpoint.py \\\\\n    --model_dir meta-llama/Llama-3.1-8B-Instruct \\\\\n    --output_dir /workspace/checkpoints/llama-8b \\\\\n    --dtype bfloat16\n\"\"\"\n\n# Step 3: Build engine (inside container)\nbuild_cmd = \"\"\"\ntrtllm-build \\\\\n    --checkpoint_dir /workspace/checkpoints/llama-8b \\\\\n    --output_dir /workspace/engines/llama-8b \\\\\n    --max_input_len 4096 \\\\\n    --max_seq_len 6144 \\\\\n    --max_batch_size 8 \\\\\n    --gemm_plugin bfloat16 \\\\\n    --use_fused_mlp enable \\\\\n    --paged_kv_cache enable\n\"\"\"\n\nprint(\"TensorRT-LLM Build Configuration:\")\nprint(\"=\" * 50)\nfor key, value in build_config.items():\n    print(f\"  {key}: {value}\")\n\nprint(\"\\nCommands:\")\nprint(\"1. Container:\", container_cmd.strip()[:60] + \"...\")\nprint(\"2. Convert:\", convert_cmd.strip()[:60] + \"...\")\nprint(\"3. Build:\", build_cmd.strip()[:60] + \"...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results template - fill in after building\n",
    "\n",
    "build_results = {\n",
    "    \"build_time_minutes\": 55,  # Example: Llama 8B on DGX Spark\n",
    "    \"engine_size_gb\": 15.2,    # Approximate for bfloat16\n",
    "    \"checkpoint_size_gb\": 16.0,\n",
    "    \"errors_encountered\": [],   # Any issues during build\n",
    "    \"notes\": [\n",
    "        \"Build completed successfully\",\n",
    "        \"Used --shm-size=16g to avoid memory issues\",\n",
    "        \"Engine is ~5% smaller than raw checkpoint due to optimization\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Build Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Build Time: {build_results['build_time_minutes']} minutes\")\n",
    "print(f\"Engine Size: {build_results['engine_size_gb']} GB\")\n",
    "print(f\"Checkpoint Size: {build_results['checkpoint_size_gb']} GB\")\n",
    "print(\"\\nNotes:\")\n",
    "for note in build_results['notes']:\n",
    "    print(f\"  - {note}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Prefill Scaling Test\n",
    "\n",
    "Test how TensorRT-LLM's prefill speed scales with input length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Prefill Scaling Test\n",
    "\n",
    "def create_prompt_of_length(target_tokens: int) -> str:\n",
    "    \"\"\"\n",
    "    Create a prompt with approximately target_tokens tokens.\n",
    "    Using repeated text for consistent behavior.\n",
    "    \"\"\"\n",
    "    base_text = \"The quick brown fox jumps over the lazy dog. \"\n",
    "    words_per_token = 0.75  # Approximate for Llama tokenizer\n",
    "    words_needed = int(target_tokens * words_per_token)\n",
    "    \n",
    "    # Repeat base text\n",
    "    repeated = (base_text * (words_needed // 9 + 1))[:words_needed * 5]\n",
    "    \n",
    "    prompt = f\"Summarize the following text in one sentence:\\n\\n{repeated}\\n\\nSummary:\"\n",
    "    return prompt\n",
    "\n",
    "# Create prompts of different lengths\n",
    "test_lengths = [100, 500, 1000, 2000, 4000]\n",
    "test_prompts = {length: create_prompt_of_length(length) for length in test_lengths}\n",
    "\n",
    "print(\"Prompt lengths created:\")\n",
    "for length, prompt in test_prompts.items():\n",
    "    actual_words = len(prompt.split())\n",
    "    print(f\"  {length} tokens target -> {actual_words} words (~{int(actual_words/0.75)} tokens)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark prefill at different lengths\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "\n",
    "TRTLLM_URL = \"http://localhost:8000\"\n",
    "\n",
    "def benchmark_prefill(prompt: str, max_tokens: int = 50) -> dict:\n",
    "    \"\"\"\n",
    "    Measure prefill time (time to first token).\n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "    first_token_time = None\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{TRTLLM_URL}/v1/chat/completions\",\n",
    "            json={\n",
    "                \"model\": \"tensorrt_llm\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"stream\": True\n",
    "            },\n",
    "            stream=True,\n",
    "            timeout=120\n",
    "        )\n",
    "        \n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                line_str = line.decode()\n",
    "                if line_str.startswith(\"data: \") and \"[DONE]\" not in line_str:\n",
    "                    try:\n",
    "                        chunk = json.loads(line_str[6:])\n",
    "                        if chunk.get(\"choices\", [{}])[0].get(\"delta\", {}).get(\"content\"):\n",
    "                            if first_token_time is None:\n",
    "                                first_token_time = time.perf_counter()\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        end = time.perf_counter()\n",
    "        prefill_time = (first_token_time - start) if first_token_time else None\n",
    "        \n",
    "        return {\n",
    "            \"prefill_time_ms\": prefill_time * 1000 if prefill_time else None,\n",
    "            \"total_time_ms\": (end - start) * 1000\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Expected results (simulated for demonstration)\n",
    "expected_results = {\n",
    "    100: {\"prefill_time_ms\": 15, \"prefill_tokens_per_sec\": 6667},\n",
    "    500: {\"prefill_time_ms\": 42, \"prefill_tokens_per_sec\": 11905},\n",
    "    1000: {\"prefill_time_ms\": 75, \"prefill_tokens_per_sec\": 13333},\n",
    "    2000: {\"prefill_time_ms\": 140, \"prefill_tokens_per_sec\": 14286},\n",
    "    4000: {\"prefill_time_ms\": 270, \"prefill_tokens_per_sec\": 14815},\n",
    "}\n",
    "\n",
    "print(\"Expected Prefill Scaling Results (TensorRT-LLM):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Tokens':<10} {'Prefill Time (ms)':<20} {'Speed (tok/s)':<15}\")\n",
    "print(\"-\" * 50)\n",
    "for tokens, result in expected_results.items():\n",
    "    print(f\"{tokens:<10} {result['prefill_time_ms']:<20} {result['prefill_tokens_per_sec']:<15}\")\n",
    "\n",
    "print(\"\\nKey Insight: TRT-LLM prefill speed INCREASES with longer inputs!\")\n",
    "print(\"This is because the GPU can process more tokens in parallel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Build time** is significant (~45-90 min for 8B) but engine runs much faster\n",
    "2. **Prefill scales well** - longer inputs get higher tokens/second\n",
    "3. **TRT-LLM is ideal** for RAG and long-context applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}