{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.5.4: Hybrid Search Implementation\n",
    "\n",
    "**Module:** 3.5 - RAG Systems & Vector Databases  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê (Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "- [ ] Understand the limitations of pure dense or sparse retrieval\n",
    "- [ ] Implement BM25 sparse retrieval from scratch\n",
    "- [ ] Combine dense and sparse retrieval with Reciprocal Rank Fusion (RRF)\n",
    "- [ ] Find the optimal fusion weights for your use case\n",
    "- [ ] Measure the improvement from hybrid search\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Labs 3.5.1-3.5.3\n",
    "- Understanding of: Embeddings, vector search, basic statistics\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The Problem:** Your semantic search is great for conceptual queries (\"how does memory work?\") but fails for specific terms (\"LPDDR5X\" or \"bge-large-en-v1.5\"). Keyword search is the opposite - great for exact terms, poor for concepts.\n",
    "\n",
    "**The Solution:** Hybrid search combines the best of both worlds. Google, Pinecone, and enterprise search all use this approach.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßí ELI5: Hybrid Search\n",
    "\n",
    "> **Imagine you're looking for a book in a library with two librarians:**\n",
    ">\n",
    "> **Librarian A (Semantic/Dense)**: \"You want a book about love? Let me find you romance novels, relationship guides, poetry about affection...\" - They understand MEANING.\n",
    ">\n",
    "> **Librarian B (Keyword/Sparse)**: \"You want 'Romeo and Juliet'? It's in aisle 5, shelf 3, exactly where that title is.\" - They find EXACT MATCHES.\n",
    ">\n",
    "> **Hybrid Search**: Ask BOTH librarians! Librarian A finds conceptually similar books, Librarian B finds exact matches. Then combine their recommendations.\n",
    ">\n",
    "> For \"CUDA memory management\", the keyword librarian finds docs with \"CUDA\" exactly, while the semantic librarian finds docs about \"GPU programming\" and \"device memory.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q \\\n",
    "    langchain langchain-community langchain-huggingface \\\n",
    "    chromadb sentence-transformers \\\n",
    "    rank_bm25 \\\n",
    "    nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and chunk documents\n",
    "DOCS_PATH = Path(\"../data/sample_documents\")\n",
    "\n",
    "documents = []\n",
    "for file_path in sorted(DOCS_PATH.glob(\"*.md\")):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    documents.append(Document(\n",
    "        page_content=content,\n",
    "        metadata={\"source\": file_path.name}\n",
    "    ))\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(f\"üìö Loaded {len(documents)} documents ‚Üí {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "print(\"üîÑ Loading embedding model...\")\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True, \"batch_size\": 32}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding Dense vs Sparse Retrieval\n",
    "\n",
    "### Dense Retrieval (Embeddings)\n",
    "- Maps text to dense vectors (e.g., 1024 dimensions)\n",
    "- Every dimension has a value\n",
    "- Captures semantic meaning\n",
    "- Good for: conceptual queries, paraphrases\n",
    "\n",
    "### Sparse Retrieval (BM25)\n",
    "- Maps text to sparse vectors (vocabulary size dimensions)\n",
    "- Most dimensions are zero\n",
    "- Based on word frequency (TF-IDF style)\n",
    "- Good for: exact terms, rare words, technical jargon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the difference\n",
    "sample_text = \"The DGX Spark uses LPDDR5X unified memory\"\n",
    "\n",
    "# Dense representation\n",
    "dense_vec = embedding_model.embed_query(sample_text)\n",
    "print(\"üìä Dense Representation:\")\n",
    "print(f\"   Dimensions: {len(dense_vec)}\")\n",
    "print(f\"   Non-zero values: {sum(1 for v in dense_vec if abs(v) > 0.001)}\")\n",
    "print(f\"   First 10 values: {[f'{v:.3f}' for v in dense_vec[:10]]}\")\n",
    "\n",
    "# Sparse representation (conceptual)\n",
    "words = sample_text.lower().split()\n",
    "print(f\"\\nüìä Sparse Representation (BM25 style):\")\n",
    "print(f\"   Vocabulary terms: {words}\")\n",
    "print(f\"   Non-zero dimensions: {len(set(words))} (only terms in document)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Implementing BM25 Sparse Retrieval\n",
    "\n",
    "### üßí ELI5: BM25\n",
    "\n",
    "> **BM25 is like counting word importance:**\n",
    ">\n",
    "> 1. Words that appear often in a document are more important FOR that document\n",
    "> 2. Words that appear in MANY documents are less important overall (like \"the\", \"is\")\n",
    "> 3. Short documents get a boost (word density matters)\n",
    ">\n",
    "> It's basically: \"How much does this document talk about these specific words compared to other documents?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Retriever:\n",
    "    \"\"\"\n",
    "    BM25 sparse retrieval implementation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, documents: List[Document], remove_stopwords: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize BM25 with documents.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain Documents\n",
    "            remove_stopwords: Whether to remove common words\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        \n",
    "        if remove_stopwords:\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "        else:\n",
    "            self.stop_words = set()\n",
    "        \n",
    "        # Tokenize all documents\n",
    "        self.tokenized_docs = [self._tokenize(doc.page_content) for doc in documents]\n",
    "        \n",
    "        # Build BM25 index\n",
    "        self.bm25 = BM25Okapi(self.tokenized_docs)\n",
    "        \n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize and optionally remove stopwords.\"\"\"\n",
    "        # Lowercase and tokenize\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        \n",
    "        # Remove stopwords and non-alphanumeric\n",
    "        tokens = [\n",
    "            t for t in tokens \n",
    "            if t.isalnum() and t not in self.stop_words\n",
    "        ]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        Search for documents matching the query.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (document, score) tuples\n",
    "        \"\"\"\n",
    "        tokenized_query = self._tokenize(query)\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        # Get top-k indices\n",
    "        top_indices = np.argsort(scores)[-k:][::-1]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            if scores[idx] > 0:  # Only include matches\n",
    "                results.append((self.documents[idx], scores[idx]))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_scores(self, query: str) -> np.ndarray:\n",
    "        \"\"\"Get BM25 scores for all documents.\"\"\"\n",
    "        tokenized_query = self._tokenize(query)\n",
    "        return self.bm25.get_scores(tokenized_query)\n",
    "\n",
    "\n",
    "# Build BM25 index\n",
    "print(\"üìù Building BM25 index...\")\n",
    "start = time.time()\n",
    "bm25_retriever = BM25Retriever(chunks)\n",
    "print(f\"‚úÖ BM25 index built in {time.time() - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test BM25 retrieval\n",
    "test_query = \"LPDDR5X memory bandwidth\"\n",
    "\n",
    "print(f\"üîç BM25 Search: '{test_query}'\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "bm25_results = bm25_retriever.search(test_query, k=3)\n",
    "\n",
    "for i, (doc, score) in enumerate(bm25_results):\n",
    "    print(f\"\\nüîπ Result {i+1} (BM25 Score: {score:.2f}):\")\n",
    "    print(f\"   Source: {doc.metadata['source']}\")\n",
    "    print(f\"   Content: {doc.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Building Dense Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseRetriever:\n",
    "    \"\"\"\n",
    "    Dense (embedding-based) retrieval implementation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, documents: List[Document], embedding_model: HuggingFaceEmbeddings):\n",
    "        \"\"\"\n",
    "        Initialize with documents and embedding model.\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        self.embedding_model = embedding_model\n",
    "        \n",
    "        # Pre-compute document embeddings\n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        self.embeddings = np.array(embedding_model.embed_documents(texts))\n",
    "        \n",
    "    def search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        Search for semantically similar documents.\n",
    "        \"\"\"\n",
    "        query_emb = np.array(self.embedding_model.embed_query(query))\n",
    "        \n",
    "        # Cosine similarity (embeddings are normalized)\n",
    "        scores = np.dot(self.embeddings, query_emb)\n",
    "        \n",
    "        # Get top-k\n",
    "        top_indices = np.argsort(scores)[-k:][::-1]\n",
    "        \n",
    "        return [(self.documents[idx], scores[idx]) for idx in top_indices]\n",
    "    \n",
    "    def get_scores(self, query: str) -> np.ndarray:\n",
    "        \"\"\"Get similarity scores for all documents.\"\"\"\n",
    "        query_emb = np.array(self.embedding_model.embed_query(query))\n",
    "        return np.dot(self.embeddings, query_emb)\n",
    "\n",
    "\n",
    "# Build dense retriever\n",
    "print(\"üîÑ Building dense retriever (computing embeddings)...\")\n",
    "start = time.time()\n",
    "dense_retriever = DenseRetriever(chunks, embedding_model)\n",
    "print(f\"‚úÖ Dense retriever built in {time.time() - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dense retrieval\n",
    "print(f\"üîç Dense Search: '{test_query}'\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "dense_results = dense_retriever.search(test_query, k=3)\n",
    "\n",
    "for i, (doc, score) in enumerate(dense_results):\n",
    "    print(f\"\\nüîπ Result {i+1} (Similarity: {score:.3f}):\")\n",
    "    print(f\"   Source: {doc.metadata['source']}\")\n",
    "    print(f\"   Content: {doc.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Implementing Hybrid Search\n",
    "\n",
    "### Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "RRF combines rankings from multiple retrievers:\n",
    "\n",
    "```\n",
    "RRF_score(d) = Œ£ 1 / (k + rank_i(d))\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `d` is a document\n",
    "- `k` is a constant (usually 60)\n",
    "- `rank_i(d)` is the rank of document `d` from retriever `i`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRetriever:\n",
    "    \"\"\"\n",
    "    Hybrid retrieval combining dense and sparse methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dense_retriever: DenseRetriever,\n",
    "        sparse_retriever: BM25Retriever,\n",
    "        alpha: float = 0.5,\n",
    "        fusion_method: str = \"rrf\"  # \"rrf\" or \"linear\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize hybrid retriever.\n",
    "        \n",
    "        Args:\n",
    "            dense_retriever: Dense (embedding) retriever\n",
    "            sparse_retriever: Sparse (BM25) retriever\n",
    "            alpha: Weight for dense scores (1-alpha for sparse)\n",
    "            fusion_method: \"rrf\" (Reciprocal Rank Fusion) or \"linear\" (weighted sum)\n",
    "        \"\"\"\n",
    "        self.dense = dense_retriever\n",
    "        self.sparse = sparse_retriever\n",
    "        self.alpha = alpha\n",
    "        self.fusion_method = fusion_method\n",
    "        \n",
    "        # Ensure same documents\n",
    "        assert len(dense_retriever.documents) == len(sparse_retriever.documents)\n",
    "        self.documents = dense_retriever.documents\n",
    "        \n",
    "    def search(self, query: str, k: int = 5, first_stage_k: int = 50) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        Hybrid search combining dense and sparse retrieval.\n",
    "        \"\"\"\n",
    "        if self.fusion_method == \"rrf\":\n",
    "            return self._rrf_search(query, k, first_stage_k)\n",
    "        else:\n",
    "            return self._linear_search(query, k)\n",
    "    \n",
    "    def _rrf_search(self, query: str, k: int, first_stage_k: int) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        Reciprocal Rank Fusion.\n",
    "        \"\"\"\n",
    "        rrf_k = 60  # Standard RRF constant\n",
    "        \n",
    "        # Get rankings from both retrievers\n",
    "        dense_results = self.dense.search(query, k=first_stage_k)\n",
    "        sparse_results = self.sparse.search(query, k=first_stage_k)\n",
    "        \n",
    "        # Build document to rank mapping\n",
    "        doc_to_id = {id(doc): i for i, doc in enumerate(self.documents)}\n",
    "        \n",
    "        # Calculate RRF scores\n",
    "        rrf_scores = {}\n",
    "        \n",
    "        for rank, (doc, _) in enumerate(dense_results):\n",
    "            doc_id = doc_to_id[id(doc)]\n",
    "            rrf_scores[doc_id] = rrf_scores.get(doc_id, 0) + 1 / (rrf_k + rank + 1)\n",
    "        \n",
    "        for rank, (doc, _) in enumerate(sparse_results):\n",
    "            doc_id = doc_to_id[id(doc)]\n",
    "            rrf_scores[doc_id] = rrf_scores.get(doc_id, 0) + 1 / (rrf_k + rank + 1)\n",
    "        \n",
    "        # Sort by RRF score\n",
    "        sorted_docs = sorted(rrf_scores.items(), key=lambda x: -x[1])\n",
    "        \n",
    "        return [(self.documents[doc_id], score) for doc_id, score in sorted_docs[:k]]\n",
    "    \n",
    "    def _linear_search(self, query: str, k: int) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        Linear combination of normalized scores.\n",
    "        \"\"\"\n",
    "        # Get all scores\n",
    "        dense_scores = self.dense.get_scores(query)\n",
    "        sparse_scores = self.sparse.get_scores(query)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        def normalize(scores):\n",
    "            min_s, max_s = scores.min(), scores.max()\n",
    "            if max_s - min_s < 1e-6:\n",
    "                return np.zeros_like(scores)\n",
    "            return (scores - min_s) / (max_s - min_s)\n",
    "        \n",
    "        dense_norm = normalize(dense_scores)\n",
    "        sparse_norm = normalize(sparse_scores)\n",
    "        \n",
    "        # Combine with weights\n",
    "        hybrid_scores = self.alpha * dense_norm + (1 - self.alpha) * sparse_norm\n",
    "        \n",
    "        # Get top-k\n",
    "        top_indices = np.argsort(hybrid_scores)[-k:][::-1]\n",
    "        \n",
    "        return [(self.documents[idx], hybrid_scores[idx]) for idx in top_indices]\n",
    "\n",
    "\n",
    "# Create hybrid retriever\n",
    "print(\"üîÄ Creating hybrid retriever...\")\n",
    "hybrid_retriever = HybridRetriever(\n",
    "    dense_retriever=dense_retriever,\n",
    "    sparse_retriever=bm25_retriever,\n",
    "    alpha=0.5,  # Equal weight to dense and sparse\n",
    "    fusion_method=\"rrf\"\n",
    ")\n",
    "print(\"‚úÖ Hybrid retriever ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three retrieval methods\n",
    "test_queries = [\n",
    "    \"LPDDR5X memory bandwidth\",  # Technical term - sparse should help\n",
    "    \"How does GPU memory work?\",  # Conceptual - dense should help\n",
    "    \"bge-large-en-v1.5 embedding model\",  # Specific model name\n",
    "    \"advantages of unified memory architecture\",  # Conceptual + specific\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üîç Query: '{query}'\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Get top result from each method\n",
    "    dense_top = dense_retriever.search(query, k=1)[0]\n",
    "    sparse_top = bm25_retriever.search(query, k=1)\n",
    "    sparse_top = sparse_top[0] if sparse_top else (None, 0)\n",
    "    hybrid_top = hybrid_retriever.search(query, k=1)[0]\n",
    "    \n",
    "    print(f\"\\nüîµ Dense Top: {dense_top[0].metadata['source'] if dense_top[0] else 'N/A'}\")\n",
    "    print(f\"   Score: {dense_top[1]:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüü¢ Sparse Top: {sparse_top[0].metadata['source'] if sparse_top[0] else 'N/A'}\")\n",
    "    print(f\"   Score: {sparse_top[1]:.2f}\")\n",
    "    \n",
    "    print(f\"\\nüü£ Hybrid Top: {hybrid_top[0].metadata['source']}\")\n",
    "    print(f\"   Score: {hybrid_top[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Finding Optimal Fusion Weights\n",
    "\n",
    "Let's evaluate different alpha values to find the best balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation dataset\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"question\": \"What is the memory capacity of DGX Spark?\",\n",
    "        \"expected_source\": \"dgx_spark_technical_guide.md\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"LPDDR5X bandwidth specifications\",\n",
    "        \"expected_source\": \"dgx_spark_technical_guide.md\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does self-attention work in transformers?\",\n",
    "        \"expected_source\": \"transformer_architecture_explained.md\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"QK^T divided by sqrt(d_k)\",\n",
    "        \"expected_source\": \"transformer_architecture_explained.md\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is low-rank adaptation?\",\n",
    "        \"expected_source\": \"lora_finetuning_guide.md\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"target_modules q_proj k_proj\",\n",
    "        \"expected_source\": \"lora_finetuning_guide.md\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does GPTQ quantization work?\",\n",
    "        \"expected_source\": \"quantization_methods.md\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Q4_K_M GGUF format\",\n",
    "        \"expected_source\": \"quantization_methods.md\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Benefits of retrieval augmented generation\",\n",
    "        \"expected_source\": \"rag_architecture_patterns.md\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"ChromaDB vs FAISS performance\",\n",
    "        \"expected_source\": \"vector_database_comparison.md\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"üìã Evaluation dataset: {len(eval_dataset)} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retriever(retriever, eval_dataset: List[Dict], k: int = 5) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate retriever on the evaluation dataset.\n",
    "    \"\"\"\n",
    "    correct_at_1 = 0\n",
    "    correct_at_3 = 0\n",
    "    correct_at_5 = 0\n",
    "    \n",
    "    for item in eval_dataset:\n",
    "        question = item[\"question\"]\n",
    "        expected = item[\"expected_source\"]\n",
    "        \n",
    "        results = retriever.search(question, k=k)\n",
    "        sources = [r[0].metadata.get('source') for r in results]\n",
    "        \n",
    "        if sources and sources[0] == expected:\n",
    "            correct_at_1 += 1\n",
    "        if expected in sources[:3]:\n",
    "            correct_at_3 += 1\n",
    "        if expected in sources[:5]:\n",
    "            correct_at_5 += 1\n",
    "    \n",
    "    n = len(eval_dataset)\n",
    "    return {\n",
    "        \"recall@1\": correct_at_1 / n,\n",
    "        \"recall@3\": correct_at_3 / n,\n",
    "        \"recall@5\": correct_at_5 / n,\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate baseline retrievers\n",
    "print(\"üìä Evaluating baseline retrievers...\")\n",
    "\n",
    "dense_metrics = evaluate_retriever(dense_retriever, eval_dataset)\n",
    "sparse_metrics = evaluate_retriever(bm25_retriever, eval_dataset)\n",
    "\n",
    "print(f\"\\nüîµ Dense Retriever:\")\n",
    "print(f\"   Recall@1: {dense_metrics['recall@1']:.0%}\")\n",
    "print(f\"   Recall@5: {dense_metrics['recall@5']:.0%}\")\n",
    "\n",
    "print(f\"\\nüü¢ Sparse Retriever (BM25):\")\n",
    "print(f\"   Recall@1: {sparse_metrics['recall@1']:.0%}\")\n",
    "print(f\"   Recall@5: {sparse_metrics['recall@5']:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal alpha\n",
    "print(\"üî¨ Finding optimal alpha for hybrid search...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "alphas = [0.0, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]\n",
    "results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    # RRF hybrid\n",
    "    hybrid = HybridRetriever(\n",
    "        dense_retriever, bm25_retriever,\n",
    "        alpha=alpha, fusion_method=\"rrf\"\n",
    "    )\n",
    "    metrics = evaluate_retriever(hybrid, eval_dataset)\n",
    "    results.append((alpha, \"RRF\", metrics))\n",
    "    \n",
    "    # Linear hybrid\n",
    "    hybrid_linear = HybridRetriever(\n",
    "        dense_retriever, bm25_retriever,\n",
    "        alpha=alpha, fusion_method=\"linear\"\n",
    "    )\n",
    "    metrics_linear = evaluate_retriever(hybrid_linear, eval_dataset)\n",
    "    results.append((alpha, \"Linear\", metrics_linear))\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n{'Alpha':<8} {'Method':<10} {'R@1':<10} {'R@3':<10} {'R@5':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "best_result = None\n",
    "best_score = 0\n",
    "\n",
    "for alpha, method, metrics in results:\n",
    "    print(f\"{alpha:<8} {method:<10} {metrics['recall@1']:<10.0%} \"\n",
    "          f\"{metrics['recall@3']:<10.0%} {metrics['recall@5']:<10.0%}\")\n",
    "    \n",
    "    score = metrics['recall@1'] + metrics['recall@5']\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_result = (alpha, method, metrics)\n",
    "\n",
    "print(f\"\\nüèÜ Best Configuration:\")\n",
    "print(f\"   Alpha: {best_result[0]}\")\n",
    "print(f\"   Method: {best_result[1]}\")\n",
    "print(f\"   Recall@5: {best_result[2]['recall@5']:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Improvement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimized hybrid retriever\n",
    "optimal_alpha, optimal_method = best_result[0], best_result[1]\n",
    "\n",
    "optimal_hybrid = HybridRetriever(\n",
    "    dense_retriever, bm25_retriever,\n",
    "    alpha=optimal_alpha, \n",
    "    fusion_method=optimal_method.lower()\n",
    ")\n",
    "\n",
    "# Compare improvements\n",
    "print(\"\\nüìà IMPROVEMENT ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n{'Method':<20} {'R@1':<10} {'R@5':<10}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Dense Only':<20} {dense_metrics['recall@1']:<10.0%} {dense_metrics['recall@5']:<10.0%}\")\n",
    "print(f\"{'Sparse Only':<20} {sparse_metrics['recall@1']:<10.0%} {sparse_metrics['recall@5']:<10.0%}\")\n",
    "\n",
    "hybrid_metrics = evaluate_retriever(optimal_hybrid, eval_dataset)\n",
    "print(f\"{'Hybrid (Optimal)':<20} {hybrid_metrics['recall@1']:<10.0%} {hybrid_metrics['recall@5']:<10.0%}\")\n",
    "\n",
    "# Calculate improvement\n",
    "dense_r5 = dense_metrics['recall@5']\n",
    "hybrid_r5 = hybrid_metrics['recall@5']\n",
    "improvement = (hybrid_r5 - dense_r5) / dense_r5 * 100 if dense_r5 > 0 else 0\n",
    "\n",
    "print(f\"\\nüéØ Improvement over Dense: {improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Normalizing Scores Before Linear Fusion\n",
    "```python\n",
    "# ‚ùå Wrong: Raw scores have different scales\n",
    "hybrid_score = alpha * dense_score + (1-alpha) * bm25_score\n",
    "\n",
    "# ‚úÖ Right: Normalize to [0, 1] first\n",
    "dense_norm = (dense_score - min) / (max - min)\n",
    "sparse_norm = (sparse_score - min) / (max - min)\n",
    "hybrid_score = alpha * dense_norm + (1-alpha) * sparse_norm\n",
    "```\n",
    "\n",
    "### Mistake 2: Using RRF with Too Few Candidates\n",
    "```python\n",
    "# ‚ùå Wrong: Only get top 5 from each, then RRF\n",
    "dense_top5 = dense.search(query, k=5)\n",
    "sparse_top5 = sparse.search(query, k=5)\n",
    "\n",
    "# ‚úÖ Right: Get more candidates for better RRF\n",
    "dense_top50 = dense.search(query, k=50)\n",
    "sparse_top50 = sparse.search(query, k=50)\n",
    "# Then RRF and take top 5\n",
    "```\n",
    "\n",
    "### Mistake 3: Same Alpha for All Query Types\n",
    "```python\n",
    "# ‚ùå Wrong: Fixed alpha for everything\n",
    "hybrid = HybridRetriever(alpha=0.5)\n",
    "\n",
    "# ‚úÖ Better: Adjust based on query type\n",
    "if looks_like_keyword_query(query):  # e.g., contains model names\n",
    "    alpha = 0.3  # Weight sparse more\n",
    "else:\n",
    "    alpha = 0.7  # Weight dense more\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Query-Adaptive Alpha\n",
    "Implement a function that adjusts alpha based on whether the query contains technical jargon.\n",
    "\n",
    "### Exercise 2: Weighted RRF\n",
    "Modify the RRF implementation to accept different weights for dense and sparse.\n",
    "\n",
    "### Exercise 3: Three-Way Hybrid\n",
    "Add a third retriever (e.g., based on document titles only) to the fusion.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint for Exercise 1</summary>\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "def adaptive_alpha(query: str) -> float:\n",
    "    # Check for technical patterns\n",
    "    technical_patterns = [\n",
    "        r'\\b[A-Z]{2,}\\d*\\b',  # Acronyms like LPDDR5X, GPU\n",
    "        r'\\b\\d+GB\\b',          # Memory sizes\n",
    "        r'\\b[a-z]+-[a-z]+',    # Hyphenated terms\n",
    "    ]\n",
    "    \n",
    "    for pattern in technical_patterns:\n",
    "        if re.search(pattern, query):\n",
    "            return 0.3  # More weight to sparse\n",
    "    \n",
    "    return 0.7  # Default: more weight to dense\n",
    "```\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ The difference between dense (semantic) and sparse (keyword) retrieval\n",
    "- ‚úÖ How to implement BM25 from scratch\n",
    "- ‚úÖ How to combine retrieval methods with RRF and linear fusion\n",
    "- ‚úÖ How to find optimal fusion weights for your data\n",
    "\n",
    "**Key Insight:** Hybrid search often outperforms either method alone, especially for queries mixing concepts and specific terms!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del embedding_model, dense_retriever, bm25_retriever, hybrid_retriever\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next lab, we'll add **reranking** to further improve retrieval quality!\n",
    "\n",
    "‚û°Ô∏è Continue to [Lab 3.5.5: Reranking Pipeline](./lab-3.5.5-reranking.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
