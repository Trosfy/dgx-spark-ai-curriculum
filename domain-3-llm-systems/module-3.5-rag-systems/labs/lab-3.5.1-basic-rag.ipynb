{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 3.5.1: Basic RAG Pipeline\n\n**Module:** 3.5 - RAG Systems & Vector Databases  \n**Time:** 3 hours  \n**Difficulty:** ‚≠ê‚≠ê‚≠ê (Intermediate)\n\n---\n\n## üéØ Learning Objectives\n\nBy the end of this notebook, you will:\n\n- [ ] Understand the complete RAG (Retrieval-Augmented Generation) architecture\n- [ ] Load and process documents for a knowledge base\n- [ ] Implement effective chunking with overlap\n- [ ] Generate embeddings using GPU-accelerated models\n- [ ] Store and query vectors in ChromaDB\n- [ ] Build an end-to-end RAG pipeline that answers questions\n\n---\n\n## üìö Prerequisites\n\n- Completed: Module 3.4 (Test-Time Compute)\n- Knowledge of: Python, basic NLP concepts, LLM usage\n- Ollama running with a model installed (we'll use `qwen3:8b` or similar)\n\n---\n\n## üåç Real-World Context\n\n**The Problem:** You're a developer at a company with thousands of internal documents (policies, technical guides, FAQs). Employees waste hours searching for information. A chatbot that just uses an LLM hallucinates answers because it wasn't trained on your company's data.\n\n**The Solution:** RAG! Build a system that retrieves relevant internal documents and uses them to generate accurate, grounded answers. This is how ChatGPT Enterprise, Microsoft Copilot, and Google's NotebookLM work.\n\n**Why This Matters:** RAG is the #1 most requested skill in LLM job postings. It's how you make LLMs actually useful for business applications.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßí ELI5: What is RAG?\n",
    "\n",
    "> **Imagine you're taking an open-book exam.**\n",
    ">\n",
    "> Without the book, you'd have to answer every question from memory. Sometimes you'd remember wrong, sometimes you'd just make things up (that's \"hallucination\" in AI terms!).\n",
    ">\n",
    "> With the book, you can look up the right information before answering. You find the relevant pages, read them, and then write your answer based on what you just read.\n",
    ">\n",
    "> **RAG gives AI a library card.** Instead of the AI trying to remember everything from its training, it can look up relevant information in YOUR documents before answering.\n",
    ">\n",
    "> The \"R\" in RAG is \"Retrieval\" - finding the right pages in the book.  \n",
    "> The \"A\" is \"Augmented\" - adding that information to the prompt.  \n",
    "> The \"G\" is \"Generation\" - writing the answer based on what was found.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup\n",
    "\n",
    "### Install Required Packages\n",
    "\n",
    "First, let's install everything we need for our RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Note: Run this cell once, then restart the kernel if needed\n",
    "\n",
    "!pip install -q \\\n",
    "    langchain==0.3.14 \\\n",
    "    langchain-community==0.3.14 \\\n",
    "    langchain-huggingface==0.1.2 \\\n",
    "    chromadb==0.5.23 \\\n",
    "    sentence-transformers==3.3.1 \\\n",
    "    ollama==0.4.4\n",
    "\n",
    "print(\"‚úÖ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# LangChain for document processing\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import (\n",
    "    DirectoryLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredMarkdownLoader\n",
    ")\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Embeddings and Vector Store\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# LLM interaction\n",
    "import ollama\n",
    "\n",
    "# GPU and memory utilities\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "We imported our core tools:\n",
    "- **LangChain**: Framework for building LLM applications\n",
    "- **ChromaDB**: Vector database for storing embeddings\n",
    "- **HuggingFaceEmbeddings**: GPU-accelerated embedding models\n",
    "- **Ollama**: Local LLM inference\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding the RAG Architecture\n",
    "\n",
    "### The RAG Pipeline\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    INDEXING PHASE (Done Once)                   ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Documents ‚Üí Chunking ‚Üí Embedding ‚Üí Vector Database            ‚îÇ\n",
    "‚îÇ    üìÑüìÑüìÑ      ‚úÇÔ∏è         üî¢üî¢üî¢       üíæ                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                   QUERY PHASE (Each Question)                   ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Question ‚Üí Embed ‚Üí Search ‚Üí Retrieve Top-K ‚Üí Generate Answer  ‚îÇ\n",
    "‚îÇ     ‚ùì        üî¢      üîç         üìÑüìÑüìÑ           üí¨             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "Let's build each component step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Loading Documents\n",
    "\n",
    "### üßí ELI5: Document Loading\n",
    "\n",
    "> **Think of this like organizing your study materials.**\n",
    ">\n",
    "> Before you can study, you need to gather all your textbooks, notes, and handouts. Some are PDFs, some are Word docs, some are web pages. The document loader reads all these different formats and converts them into a standard format we can work with.\n",
    "\n",
    "Let's load our sample documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to our sample documents\n",
    "DOCS_PATH = Path(\"../data/sample_documents\")\n",
    "\n",
    "# Check what documents we have\n",
    "if DOCS_PATH.exists():\n",
    "    docs_list = list(DOCS_PATH.glob(\"*.md\"))\n",
    "    print(f\"üìÅ Found {len(docs_list)} markdown documents:\")\n",
    "    for doc in docs_list:\n",
    "        size_kb = doc.stat().st_size / 1024\n",
    "        print(f\"   - {doc.name} ({size_kb:.1f} KB)\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Documents directory not found at {DOCS_PATH}\")\n",
    "    print(\"Please ensure you have the sample_documents folder in ../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(docs_path: Path) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load all markdown documents from a directory.\n",
    "    \n",
    "    Args:\n",
    "        docs_path: Path to directory containing documents\n",
    "        \n",
    "    Returns:\n",
    "        List of LangChain Document objects\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # Load each markdown file\n",
    "    for file_path in docs_path.glob(\"*.md\"):\n",
    "        try:\n",
    "            # Read the file content\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # Create a Document object with metadata\n",
    "            doc = Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    \"source\": file_path.name,\n",
    "                    \"file_path\": str(file_path),\n",
    "                    \"file_size\": file_path.stat().st_size\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "            print(f\"‚úÖ Loaded: {file_path.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {file_path.name}: {e}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Load our documents\n",
    "raw_documents = load_documents(DOCS_PATH)\n",
    "print(f\"\\nüìö Total documents loaded: {len(raw_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's peek at one document\n",
    "if raw_documents:\n",
    "    sample_doc = raw_documents[0]\n",
    "    print(f\"üìÑ Sample Document: {sample_doc.metadata['source']}\")\n",
    "    print(f\"   Length: {len(sample_doc.page_content):,} characters\")\n",
    "    print(f\"\\n   First 500 characters:\")\n",
    "    print(\"   \" + \"-\" * 60)\n",
    "    print(sample_doc.page_content[:500])\n",
    "    print(\"   ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "We loaded our documents into LangChain's `Document` format, which has:\n",
    "- `page_content`: The actual text\n",
    "- `metadata`: Information about the source (filename, path, etc.)\n",
    "\n",
    "Metadata is crucial for RAG because it lets us cite sources!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Chunking Documents\n",
    "\n",
    "### üßí ELI5: Why Chunk?\n",
    "\n",
    "> **Imagine you're looking for a recipe in a giant cookbook.**\n",
    ">\n",
    "> You wouldn't read the entire 500-page book to find how to make cookies. You'd look in the \"Desserts\" chapter, find the \"Cookies\" section, and read just those 2-3 pages.\n",
    ">\n",
    "> Chunking is breaking our documents into \"cookie-sized\" pieces so we can retrieve just the relevant parts. Too small = missing context. Too big = including irrelevant info.\n",
    "\n",
    "### Chunking Strategy\n",
    "\n",
    "We'll use **RecursiveCharacterTextSplitter** which:\n",
    "1. Tries to split on double newlines (paragraph breaks)\n",
    "2. Falls back to single newlines\n",
    "3. Falls back to sentences\n",
    "4. Falls back to words\n",
    "\n",
    "This preserves semantic boundaries as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure our chunking strategy\n",
    "CHUNK_SIZE = 512       # Target size in characters\n",
    "CHUNK_OVERLAP = 50     # Overlap between chunks\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Priority order\n",
    ")\n",
    "\n",
    "print(f\"üìè Chunk configuration:\")\n",
    "print(f\"   Chunk size: {CHUNK_SIZE} characters\")\n",
    "print(f\"   Overlap: {CHUNK_OVERLAP} characters\")\n",
    "print(f\"   Separators: paragraph ‚Üí line ‚Üí sentence ‚Üí word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split all documents into chunks\n",
    "chunks = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "print(f\"‚úÇÔ∏è Chunking Results:\")\n",
    "print(f\"   Original documents: {len(raw_documents)}\")\n",
    "print(f\"   Total chunks: {len(chunks)}\")\n",
    "print(f\"   Average chunks per document: {len(chunks) / len(raw_documents):.1f}\")\n",
    "\n",
    "# Analyze chunk sizes\n",
    "chunk_sizes = [len(c.page_content) for c in chunks]\n",
    "print(f\"\\nüìä Chunk Size Statistics:\")\n",
    "print(f\"   Min: {min(chunk_sizes)} chars\")\n",
    "print(f\"   Max: {max(chunk_sizes)} chars\")\n",
    "print(f\"   Average: {sum(chunk_sizes)/len(chunk_sizes):.0f} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine a few chunks to see how splitting worked\n",
    "print(\"üìù Sample Chunks:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i in [0, 5, 10]:  # Look at chunks 0, 5, and 10\n",
    "    if i < len(chunks):\n",
    "        chunk = chunks[i]\n",
    "        print(f\"\\nüîπ Chunk {i} (from {chunk.metadata['source']}):\")\n",
    "        print(f\"   Length: {len(chunk.page_content)} chars\")\n",
    "        print(f\"   Content (first 200 chars):\")\n",
    "        print(f\"   '{chunk.page_content[:200]}...'\")\n",
    "        print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself: Experiment with Chunk Sizes\n",
    "\n",
    "What happens if you use different chunk sizes?\n",
    "\n",
    "1. Try `CHUNK_SIZE = 256` (smaller chunks)\n",
    "2. Try `CHUNK_SIZE = 1024` (larger chunks)\n",
    "\n",
    "What do you notice about the number of chunks and their content?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "- Smaller chunks = more chunks, more precise retrieval, but less context\n",
    "- Larger chunks = fewer chunks, more context, but might include irrelevant info\n",
    "- The sweet spot is usually 256-512 tokens (roughly 512-1500 characters)\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Creating Embeddings\n",
    "\n",
    "### üßí ELI5: What are Embeddings?\n",
    "\n",
    "> **Imagine organizing books in a library by \"vibes\" instead of alphabetically.**\n",
    ">\n",
    "> Books about love go near each other, books about war go near each other, sci-fi books cluster together. Even though \"Romeo and Juliet\" and \"Pride and Prejudice\" have different titles, they'd be placed near each other because they're both love stories.\n",
    ">\n",
    "> Embeddings convert text into numbers (a long list of numbers called a \"vector\") that capture its meaning. Similar meanings ‚Üí similar numbers ‚Üí close together in \"embedding space.\"\n",
    ">\n",
    "> When you search for \"How does the GPU work?\", the embedding of your question will be close to chunks about GPU architecture, even if they don't use the exact words \"GPU work.\"\n",
    "\n",
    "### Loading the Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use BGE-large, one of the best open-source embedding models\n",
    "EMBEDDING_MODEL = \"BAAI/bge-large-en-v1.5\"\n",
    "\n",
    "print(f\"üîÑ Loading embedding model: {EMBEDDING_MODEL}\")\n",
    "print(\"   This may take a minute on first run (downloading ~1.3GB)...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize with GPU support\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    encode_kwargs={\n",
    "        \"normalize_embeddings\": True,  # For cosine similarity\n",
    "        \"batch_size\": 32  # Process multiple chunks at once\n",
    "    }\n",
    ")\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"‚úÖ Model loaded in {load_time:.1f} seconds\")\n",
    "print(f\"   Running on: {'GPU üöÄ' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the embedding model\n",
    "test_texts = [\n",
    "    \"How do I train a neural network?\",\n",
    "    \"What is deep learning?\",\n",
    "    \"The weather is nice today.\"\n",
    "]\n",
    "\n",
    "test_embeddings = embedding_model.embed_documents(test_texts)\n",
    "\n",
    "print(f\"üìä Embedding Dimensions: {len(test_embeddings[0])}\")\n",
    "print(f\"   (Each chunk becomes a vector of {len(test_embeddings[0])} numbers)\")\n",
    "\n",
    "# Calculate similarity between our test texts\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "print(f\"\\nüîó Similarity Scores:\")\n",
    "print(f\"   '{test_texts[0]}' ‚Üî '{test_texts[1]}'\")\n",
    "print(f\"   Similarity: {cosine_similarity(test_embeddings[0], test_embeddings[1]):.3f}\")\n",
    "print(f\"   (High = similar topics)\")\n",
    "\n",
    "print(f\"\\n   '{test_texts[0]}' ‚Üî '{test_texts[2]}'\")\n",
    "print(f\"   Similarity: {cosine_similarity(test_embeddings[0], test_embeddings[2]):.3f}\")\n",
    "print(f\"   (Low = different topics)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "The embedding model converted each text into a 1024-dimensional vector. Notice:\n",
    "- \"Neural network training\" and \"deep learning\" are semantically similar ‚Üí high cosine similarity (~0.8+)\n",
    "- \"Neural network training\" and \"weather\" are unrelated ‚Üí low cosine similarity (~0.3)\n",
    "\n",
    "This is how RAG finds relevant documents even when they don't share exact keywords!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Storing in Vector Database\n",
    "\n",
    "### üßí ELI5: Vector Database\n",
    "\n",
    "> **Remember our library organized by \"vibes\"?**\n",
    ">\n",
    "> A vector database is like a magical librarian who knows exactly where every book is in this vibes-based organization. When you say \"I want a book about romance\", they instantly point you to the right section.\n",
    ">\n",
    "> Technically, vector databases use special algorithms (like HNSW or IVF) to quickly find the nearest neighbors to your query vector without checking every single document.\n",
    "\n",
    "### Creating the ChromaDB Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to store our vector database\n",
    "CHROMA_PATH = \"./chroma_db\"\n",
    "\n",
    "# Clean up any existing database (for fresh start)\n",
    "import shutil\n",
    "if Path(CHROMA_PATH).exists():\n",
    "    shutil.rmtree(CHROMA_PATH)\n",
    "    print(f\"üóëÔ∏è Removed existing database at {CHROMA_PATH}\")\n",
    "\n",
    "print(f\"\\nüì¶ Creating vector database...\")\n",
    "print(f\"   Embedding {len(chunks)} chunks...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create the vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=CHROMA_PATH,\n",
    "    collection_name=\"rag_documents\"\n",
    ")\n",
    "\n",
    "index_time = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Vector database created in {index_time:.1f} seconds!\")\n",
    "print(f\"   Stored at: {CHROMA_PATH}\")\n",
    "print(f\"   Collection: rag_documents\")\n",
    "print(f\"   Chunks indexed: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test retrieval with a sample query\n",
    "test_query = \"How much memory does DGX Spark have?\"\n",
    "\n",
    "print(f\"üîç Test Query: '{test_query}'\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Search for similar chunks\n",
    "results = vectorstore.similarity_search_with_score(test_query, k=3)\n",
    "\n",
    "print(f\"\\nüìÑ Top 3 Retrieved Chunks:\")\n",
    "for i, (doc, score) in enumerate(results):\n",
    "    print(f\"\\nüîπ Result {i+1} (Similarity: {1-score:.3f}):\")\n",
    "    print(f\"   Source: {doc.metadata['source']}\")\n",
    "    print(f\"   Content: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "ChromaDB:\n",
    "1. Took each chunk and computed its embedding\n",
    "2. Stored both the text and embedding\n",
    "3. Built an index for fast similarity search\n",
    "\n",
    "When we queried \"How much memory does DGX Spark have?\", it:\n",
    "1. Embedded our question\n",
    "2. Found the 3 most similar chunks\n",
    "3. Returned them with similarity scores\n",
    "\n",
    "Notice the retrieved chunks are about DGX Spark memory, even though we didn't use the exact same words!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Setting Up the LLM\n",
    "\n",
    "Now we need an LLM to generate answers based on the retrieved context. We'll use Ollama for local inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check Ollama connection and available models\ntry:\n    models = ollama.list()\n    print(\"‚úÖ Ollama is running!\")\n    print(f\"\\nüìã Available models:\")\n    for model in models.get('models', []):\n        name = model.get('name', 'unknown')\n        size = model.get('size', 0) / 1e9\n        print(f\"   - {name} ({size:.1f} GB)\")\nexcept Exception as e:\n    print(f\"‚ùå Ollama not running: {e}\")\n    print(\"\\nüí° To start Ollama:\")\n    print(\"   1. Open a terminal\")\n    print(\"   2. Run: ollama serve\")\n    print(\"   3. In another terminal: ollama pull qwen3:8b\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Select model to use (modify based on what you have installed)\nLLM_MODEL = \"qwen3:8b\"  # Good balance of quality and speed\n# Alternatives: \"qwen3:32b\" (better quality), \"nemotron-3-nano\" (faster)\n\nprint(f\"üìù Using model: {LLM_MODEL}\")\n\n# Test the model with a simple query\ntry:\n    test_response = ollama.chat(\n        model=LLM_MODEL,\n        messages=[{\"role\": \"user\", \"content\": \"Say 'Hello!' in exactly one word.\"}]\n    )\n    print(f\"‚úÖ Model response: {test_response['message']['content']}\")\nexcept Exception as e:\n    print(f\"‚ùå Error with model {LLM_MODEL}: {e}\")\n    print(f\"\\nüí° Try: ollama pull {LLM_MODEL}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Building the RAG Pipeline\n",
    "\n",
    "Now we'll combine everything into a complete RAG system!\n",
    "\n",
    "### The RAG Query Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def rag_query(\n    question: str,\n    vectorstore: Chroma,\n    model: str = \"qwen3:8b\",\n    k: int = 5,\n    verbose: bool = True\n) -> Dict[str, Any]:\n    \"\"\"\n    Complete RAG query: retrieve relevant chunks and generate an answer.\n    \n    Args:\n        question: User's question\n        vectorstore: ChromaDB vector store\n        model: Ollama model to use\n        k: Number of chunks to retrieve\n        verbose: Print intermediate steps\n        \n    Returns:\n        Dictionary with answer, sources, and timing info\n    \"\"\"\n    start_time = time.time()\n    \n    # Step 1: Retrieve relevant chunks\n    if verbose:\n        print(f\"üîç Retrieving top {k} relevant chunks...\")\n    \n    retrieval_start = time.time()\n    results = vectorstore.similarity_search_with_score(question, k=k)\n    retrieval_time = time.time() - retrieval_start\n    \n    if verbose:\n        print(f\"   Retrieved {len(results)} chunks in {retrieval_time*1000:.0f}ms\")\n    \n    # Step 2: Build context from retrieved chunks\n    context_parts = []\n    sources = []\n    \n    for doc, score in results:\n        context_parts.append(doc.page_content)\n        sources.append({\n            \"source\": doc.metadata[\"source\"],\n            \"similarity\": 1 - score,  # Convert distance to similarity\n            \"preview\": doc.page_content[:100] + \"...\"\n        })\n    \n    context = \"\\n\\n---\\n\\n\".join(context_parts)\n    \n    # Step 3: Create the RAG prompt\n    prompt = f\"\"\"You are a helpful AI assistant. Answer the question based ONLY on the provided context.\nIf the context doesn't contain enough information to answer the question, say so.\nAlways cite which source document your information comes from.\n\nCONTEXT:\n{context}\n\nQUESTION: {question}\n\nANSWER (be concise and cite sources):\"\"\"\n    \n    # Step 4: Generate answer with LLM\n    if verbose:\n        print(f\"üí≠ Generating answer with {model}...\")\n    \n    generation_start = time.time()\n    response = ollama.chat(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    generation_time = time.time() - generation_start\n    \n    answer = response[\"message\"][\"content\"]\n    total_time = time.time() - start_time\n    \n    if verbose:\n        print(f\"   Generated in {generation_time:.1f}s\")\n        print(f\"   Total time: {total_time:.1f}s\")\n    \n    return {\n        \"question\": question,\n        \"answer\": answer,\n        \"sources\": sources,\n        \"timing\": {\n            \"retrieval_ms\": retrieval_time * 1000,\n            \"generation_s\": generation_time,\n            \"total_s\": total_time\n        }\n    }"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Test Our RAG System!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Question 1: DGX Spark specifications\n",
    "question1 = \"What is the memory capacity of the DGX Spark and why is it special?\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"‚ùì Question: {question1}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "result1 = rag_query(question1, vectorstore, model=LLM_MODEL)\n",
    "\n",
    "print(f\"\\nüí¨ Answer:\")\n",
    "print(result1[\"answer\"])\n",
    "\n",
    "print(f\"\\nüìö Sources Used:\")\n",
    "for src in result1[\"sources\"][:3]:  # Show top 3 sources\n",
    "    print(f\"   - {src['source']} (similarity: {src['similarity']:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Question 2: Technical concept\n",
    "question2 = \"How does LoRA reduce memory requirements for fine-tuning?\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"‚ùì Question: {question2}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "result2 = rag_query(question2, vectorstore, model=LLM_MODEL)\n",
    "\n",
    "print(f\"\\nüí¨ Answer:\")\n",
    "print(result2[\"answer\"])\n",
    "\n",
    "print(f\"\\nüìö Sources Used:\")\n",
    "for src in result2[\"sources\"][:3]:\n",
    "    print(f\"   - {src['source']} (similarity: {src['similarity']:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Question 3: Question that should NOT be in our documents\n",
    "question3 = \"What is the recipe for chocolate chip cookies?\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"‚ùì Question: {question3}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "result3 = rag_query(question3, vectorstore, model=LLM_MODEL)\n",
    "\n",
    "print(f\"\\nüí¨ Answer:\")\n",
    "print(result3[\"answer\"])\n",
    "\n",
    "print(f\"\\nüìö Sources Retrieved (not relevant):\")\n",
    "for src in result3[\"sources\"][:3]:\n",
    "    print(f\"   - {src['source']} (similarity: {src['similarity']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "Our RAG pipeline:\n",
    "1. **Embedded** the question using the same model as our documents\n",
    "2. **Retrieved** the 5 most similar chunks from ChromaDB\n",
    "3. **Constructed** a prompt with the question and retrieved context\n",
    "4. **Generated** an answer using the LLM, grounded in the retrieved documents\n",
    "\n",
    "Notice:\n",
    "- Questions about our docs get accurate, cited answers\n",
    "- Questions NOT in our docs (like cookies) correctly report that the info isn't available!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Interactive RAG Demo\n",
    "\n",
    "Let's create an interactive demo where you can ask your own questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def interactive_rag(vectorstore: Chroma, model: str = \"qwen3:8b\"):\n    \"\"\"\n    Interactive RAG session - ask questions about your documents!\n    Type 'quit' to exit.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 70)\n    print(\"ü§ñ RAG Knowledge Assistant\")\n    print(\"=\" * 70)\n    print(\"Ask questions about the loaded documents!\")\n    print(\"Type 'quit' to exit.\\n\")\n    \n    while True:\n        question = input(\"\\n‚ùì Your question: \").strip()\n        \n        if question.lower() in ['quit', 'exit', 'q']:\n            print(\"\\nüëã Goodbye!\")\n            break\n        \n        if not question:\n            print(\"Please enter a question.\")\n            continue\n        \n        print()\n        result = rag_query(question, vectorstore, model=model, verbose=True)\n        \n        print(f\"\\nüí¨ Answer:\")\n        print(\"-\" * 50)\n        print(result[\"answer\"])\n        print(\"-\" * 50)\n        \n        print(f\"\\nüìö Sources:\")\n        for src in result[\"sources\"][:3]:\n            print(f\"   - {src['source']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run interactive mode\n",
    "# interactive_rag(vectorstore, model=LLM_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself: Test Questions\n",
    "\n",
    "Try asking these questions about our documents:\n",
    "\n",
    "1. \"What are the different quantization methods and their trade-offs?\"\n",
    "2. \"Explain how the attention mechanism works in Transformers\"\n",
    "3. \"What index types are available in FAISS?\"\n",
    "4. \"How do I choose between ChromaDB, FAISS, and Qdrant?\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Chunks Too Small\n",
    "```python\n",
    "# ‚ùå Wrong: Tiny chunks lose context\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "\n",
    "# ‚úÖ Right: Balanced chunk size with overlap\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "```\n",
    "**Why:** Tiny chunks might only contain \"The model uses...\" without explaining WHAT it uses.\n",
    "\n",
    "### Mistake 2: No Overlap Between Chunks\n",
    "```python\n",
    "# ‚ùå Wrong: No overlap\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=0)\n",
    "\n",
    "# ‚úÖ Right: 10-20% overlap\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "```\n",
    "**Why:** Without overlap, a key sentence split between chunks is never fully retrievable.\n",
    "\n",
    "### Mistake 3: Not Normalizing Embeddings\n",
    "```python\n",
    "# ‚ùå Wrong: Embeddings not normalized\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"...\")\n",
    "\n",
    "# ‚úÖ Right: Normalize for cosine similarity\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"...\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "```\n",
    "**Why:** Cosine similarity requires normalized vectors to work correctly.\n",
    "\n",
    "### Mistake 4: Not Handling Missing Information\n",
    "```python\n",
    "# ‚ùå Wrong: No guidance for missing info\n",
    "prompt = f\"Answer this: {question}\\nContext: {context}\"\n",
    "\n",
    "# ‚úÖ Right: Tell LLM to acknowledge limitations\n",
    "prompt = f\"\"\"Answer based ONLY on the context.\n",
    "If the context doesn't have the answer, say 'I don't have information about that.'\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\"\"\"\n",
    "```\n",
    "**Why:** Without explicit instruction, the LLM may hallucinate answers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Checkpoint\n",
    "\n",
    "Congratulations! You've built a complete RAG system from scratch!\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ What RAG is and why it's essential for practical LLM applications\n",
    "- ‚úÖ How to load and preprocess documents\n",
    "- ‚úÖ How chunking works and why overlap matters\n",
    "- ‚úÖ How embeddings capture semantic meaning\n",
    "- ‚úÖ How vector databases enable fast similarity search\n",
    "- ‚úÖ How to combine retrieval with generation for grounded answers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "### Challenge 1: Add More Documents\n",
    "Add your own documents (PDF, Word, or text) to the knowledge base.\n",
    "\n",
    "### Challenge 2: Implement Conversation History\n",
    "Modify `rag_query` to maintain conversation context for follow-up questions.\n",
    "\n",
    "### Challenge 3: Add Source Citations\n",
    "Modify the prompt to include specific page/chunk references in the answer.\n",
    "\n",
    "### Challenge 4: Measure Quality\n",
    "Create 10 Q&A pairs with ground-truth answers and measure how well your RAG system performs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Further Reading\n",
    "\n",
    "- [RAG Paper](https://arxiv.org/abs/2005.11401) - The original RAG research paper\n",
    "- [LangChain RAG Guide](https://python.langchain.com/docs/use_cases/question_answering/)\n",
    "- [ChromaDB Documentation](https://docs.trychroma.com/)\n",
    "- [BGE Embedding Models](https://huggingface.co/BAAI/bge-large-en-v1.5)\n",
    "- [Ollama Documentation](https://ollama.ai/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleanup\n",
    "\n",
    "Free up GPU memory and clean up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del embedding_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")\n",
    "print(f\"\\nüí° Note: The vector database persists at {CHROMA_PATH}\")\n",
    "print(\"   You can reload it later without re-embedding!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next lab, we'll explore different **chunking strategies** to improve retrieval quality!\n",
    "\n",
    "‚û°Ô∏è Continue to [Lab 3.5.2: Chunking Strategies](./lab-3.5.2-chunking.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}