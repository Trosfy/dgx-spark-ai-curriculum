{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.5.3: Vector Database Comparison\n",
    "\n",
    "**Module:** 3.5 - RAG Systems & Vector Databases  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "- [ ] Implement the same RAG pipeline with ChromaDB, FAISS, and Qdrant\n",
    "- [ ] Benchmark indexing time, query latency, and memory usage\n",
    "- [ ] Understand GPU acceleration with FAISS on DGX Spark\n",
    "- [ ] Implement metadata filtering with each database\n",
    "- [ ] Know when to choose each vector database\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Lab 3.5.1-3.5.2\n",
    "- DGX Spark with GPU for FAISS acceleration\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The Decision:** You've built your RAG prototype with ChromaDB. Now your startup is scaling - you need to handle 10 million documents and 1000 queries/second. Which vector database should you use in production?\n",
    "\n",
    "**Trade-offs:**\n",
    "- **ChromaDB**: Easy to use, but slower at scale\n",
    "- **FAISS**: Blazing fast with GPU, but no built-in filtering\n",
    "- **Qdrant**: Production features, but more complex setup\n",
    "\n",
    "**The Goal:** Make an informed decision based on YOUR requirements.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßí ELI5: Vector Databases\n",
    "\n",
    "> **Imagine three different librarians helping you find books:**\n",
    ">\n",
    "> **ChromaDB Librarian** üìö: Friendly and helpful. Knows where everything is, can handle special requests (\"only books from 2023\"). Works well for small libraries.\n",
    ">\n",
    "> **FAISS Librarian** ‚ö°: SUPER fast because they memorized the entire library layout. But if you want special requests, you have to do that yourself. Best for HUGE libraries.\n",
    ">\n",
    "> **Qdrant Librarian** üèóÔ∏è: Professional grade. Handles special requests, works with a team (distributed), keeps detailed records. Perfect for a large organization.\n",
    ">\n",
    "> On your DGX Spark, the FAISS librarian gets a jetpack (GPU) and becomes even faster!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install all vector databases\n# Note: On DGX Spark (ARM64), use faiss-cpu. GPU acceleration via faiss-gpu\n# requires building from source or using the NGC container with pre-built binaries.\n!pip install -q \\\n    chromadb==0.5.23 \\\n    faiss-cpu==1.9.0 \\\n    qdrant-client==1.12.1 \\\n    langchain langchain-community langchain-huggingface \\\n    sentence-transformers\n\nprint(\"‚úÖ Vector databases installed!\")\nprint(\"‚ÑπÔ∏è  Note: Using faiss-cpu. For GPU-accelerated FAISS, use NGC containers.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "# Vector databases\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import faiss\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import (\n",
    "    VectorParams, Distance, PointStruct,\n",
    "    Filter, FieldCondition, MatchValue\n",
    ")\n",
    "\n",
    "# LangChain wrappers\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare documents\n",
    "DOCS_PATH = Path(\"../data/sample_documents\")\n",
    "\n",
    "documents = []\n",
    "for file_path in sorted(DOCS_PATH.glob(\"*.md\")):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    documents.append(Document(\n",
    "        page_content=content,\n",
    "        metadata={\n",
    "            \"source\": file_path.name,\n",
    "            \"category\": \"technical\",\n",
    "            \"file_size\": file_path.stat().st_size\n",
    "        }\n",
    "    ))\n",
    "\n",
    "print(f\"üìö Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk documents\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(documents)\n",
    "print(f\"‚úÇÔ∏è Created {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "print(\"üîÑ Loading embedding model...\")\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True, \"batch_size\": 32}\n",
    ")\n",
    "\n",
    "# Pre-compute embeddings for fair comparison\n",
    "print(\"üìä Pre-computing embeddings...\")\n",
    "start = time.time()\n",
    "chunk_texts = [c.page_content for c in chunks]\n",
    "chunk_embeddings = embedding_model.embed_documents(chunk_texts)\n",
    "embedding_time = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ Computed {len(chunk_embeddings)} embeddings in {embedding_time:.2f}s\")\n",
    "print(f\"   Embedding dimension: {len(chunk_embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 2: ChromaDB Implementation\n\nChromaDB is Python-native and easy to use. Let's benchmark it.\n\n### Utility Functions\n\nBefore benchmarking, we need some helper functions:\n\n**psutil** is a Python library for system monitoring:\n- `psutil.Process().memory_info().rss` - Returns the Resident Set Size (physical memory used by the process) in bytes\n- Useful for measuring memory consumption of vector databases"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass BenchmarkResult:\n    \"\"\"Benchmark results for a vector database.\"\"\"\n    db_name: str\n    index_time_s: float\n    avg_query_time_ms: float\n    min_query_time_ms: float\n    max_query_time_ms: float\n    memory_mb: float\n    supports_filtering: bool\n    supports_gpu: bool\n\n\ndef get_memory_usage() -> float:\n    \"\"\"\n    Get current process memory usage in MB.\n    \n    Uses psutil.Process().memory_info().rss which returns:\n    - RSS (Resident Set Size): Physical memory currently used by the process\n    - Measured in bytes, so we divide by 1024¬≤ to get megabytes\n    \"\"\"\n    process = psutil.Process()\n    return process.memory_info().rss / 1024 / 1024\n\n\ntest_queries = [\n    \"What is the memory capacity of DGX Spark?\",\n    \"How does LoRA reduce training requirements?\",\n    \"Explain the attention mechanism\",\n    \"What is GPTQ quantization?\",\n    \"How does hybrid search work in RAG?\"\n]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_chromadb(\n",
    "    chunks: List[Document],\n",
    "    embeddings: List[List[float]],\n",
    "    test_queries: List[str]\n",
    ") -> BenchmarkResult:\n",
    "    \"\"\"\n",
    "    Benchmark ChromaDB performance.\n",
    "    \"\"\"\n",
    "    db_path = \"./benchmark_chroma\"\n",
    "    if Path(db_path).exists():\n",
    "        shutil.rmtree(db_path)\n",
    "    \n",
    "    memory_before = get_memory_usage()\n",
    "    \n",
    "    # Time indexing\n",
    "    start = time.time()\n",
    "    \n",
    "    client = chromadb.PersistentClient(path=db_path)\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=\"documents\",\n",
    "        metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "    \n",
    "    # Add documents with embeddings\n",
    "    collection.add(\n",
    "        ids=[f\"doc_{i}\" for i in range(len(chunks))],\n",
    "        embeddings=embeddings,\n",
    "        documents=[c.page_content for c in chunks],\n",
    "        metadatas=[c.metadata for c in chunks]\n",
    "    )\n",
    "    \n",
    "    index_time = time.time() - start\n",
    "    memory_after = get_memory_usage()\n",
    "    \n",
    "    # Time queries\n",
    "    query_times = []\n",
    "    for query in test_queries:\n",
    "        query_emb = embedding_model.embed_query(query)\n",
    "        \n",
    "        start = time.time()\n",
    "        results = collection.query(\n",
    "            query_embeddings=[query_emb],\n",
    "            n_results=5\n",
    "        )\n",
    "        query_times.append((time.time() - start) * 1000)\n",
    "    \n",
    "    # Test filtering\n",
    "    query_emb = embedding_model.embed_query(test_queries[0])\n",
    "    filtered = collection.query(\n",
    "        query_embeddings=[query_emb],\n",
    "        n_results=5,\n",
    "        where={\"category\": \"technical\"}\n",
    "    )\n",
    "    supports_filtering = len(filtered['ids'][0]) > 0\n",
    "    \n",
    "    # Cleanup\n",
    "    del client, collection\n",
    "    shutil.rmtree(db_path)\n",
    "    \n",
    "    return BenchmarkResult(\n",
    "        db_name=\"ChromaDB\",\n",
    "        index_time_s=index_time,\n",
    "        avg_query_time_ms=np.mean(query_times),\n",
    "        min_query_time_ms=min(query_times),\n",
    "        max_query_time_ms=max(query_times),\n",
    "        memory_mb=memory_after - memory_before,\n",
    "        supports_filtering=supports_filtering,\n",
    "        supports_gpu=False\n",
    "    )\n",
    "\n",
    "print(\"üîµ Benchmarking ChromaDB...\")\n",
    "chroma_result = benchmark_chromadb(chunks, chunk_embeddings, test_queries)\n",
    "print(f\"   Index time: {chroma_result.index_time_s:.2f}s\")\n",
    "print(f\"   Avg query: {chroma_result.avg_query_time_ms:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 3: FAISS Implementation (GPU Accelerated)\n\nFAISS (Facebook AI Similarity Search) is a library for efficient similarity search.\n\n### Key FAISS Concepts and Functions\n\n| Function | Purpose |\n|----------|---------|\n| `faiss.IndexFlatIP(dim)` | Creates a flat (brute-force) index using Inner Product similarity. Best for small datasets. |\n| `faiss.IndexFlatL2(dim)` | Creates a flat index using L2 (Euclidean) distance. |\n| `faiss.IndexIVFFlat(quantizer, dim, nlist)` | Creates an IVF (Inverted File) index that clusters vectors for faster approximate search. |\n| `faiss.StandardGpuResources()` | Allocates GPU memory for FAISS operations. |\n| `faiss.index_cpu_to_gpu(res, gpu_id, index)` | Moves a CPU index to GPU for acceleration. |\n| `index.add(vectors)` | Adds vectors to the index. |\n| `index.search(query, k)` | Finds k nearest neighbors. Returns (distances, indices). |\n\n**Inner Product (IP) vs L2:** For normalized vectors, IP is equivalent to cosine similarity. We use IP because our embeddings are normalized."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def benchmark_faiss(\n    chunks: List[Document],\n    embeddings: List[List[float]],\n    test_queries: List[str],\n    use_gpu: bool = True\n) -> BenchmarkResult:\n    \"\"\"\n    Benchmark FAISS performance with optional GPU acceleration.\n    \n    FAISS Index Types Used:\n    - IndexFlatIP: Exact search using inner product (cosine similarity for normalized vectors)\n      Syntax: faiss.IndexFlatIP(dimension) where dimension = embedding size (e.g., 1024)\n    \n    GPU Acceleration (requires faiss-gpu):\n    - faiss.StandardGpuResources(): Allocates GPU memory pool for FAISS\n    - faiss.index_cpu_to_gpu(resources, gpu_id, cpu_index): Moves index to GPU\n    \n    Note: GPU acceleration requires faiss-gpu which must be built from source\n    on ARM64 or used via NGC containers. With faiss-cpu, GPU is not available.\n    \"\"\"\n    memory_before = get_memory_usage()\n    \n    # Convert to numpy - FAISS requires float32 numpy arrays\n    embeddings_np = np.array(embeddings).astype('float32')\n    dimension = embeddings_np.shape[1]\n    \n    # Check if GPU functions are available in faiss\n    has_gpu_support = hasattr(faiss, 'StandardGpuResources')\n    \n    # Time indexing\n    start = time.time()\n    \n    # Create index - IndexFlatIP for inner product (cosine similarity with normalized vectors)\n    index = faiss.IndexFlatIP(dimension)\n    \n    gpu_used = False\n    if use_gpu and torch.cuda.is_available() and has_gpu_support:\n        try:\n            # Move to GPU (only available with faiss-gpu)\n            # StandardGpuResources manages GPU memory allocation\n            res = faiss.StandardGpuResources()\n            # index_cpu_to_gpu(resources, gpu_device_id, cpu_index)\n            index = faiss.index_cpu_to_gpu(res, 0, index)\n            gpu_used = True\n        except Exception as e:\n            print(f\"   ‚ö†Ô∏è GPU acceleration not available: {e}\")\n    \n    # Add vectors to the index\n    index.add(embeddings_np)\n    \n    index_time = time.time() - start\n    memory_after = get_memory_usage()\n    \n    # Time queries\n    query_times = []\n    for query in test_queries:\n        query_emb = embedding_model.embed_query(query)\n        query_np = np.array([query_emb]).astype('float32')\n        \n        start = time.time()\n        # search returns (distances, indices) - distances shape: (n_queries, k)\n        distances, indices = index.search(query_np, k=5)\n        query_times.append((time.time() - start) * 1000)\n    \n    # FAISS doesn't have built-in filtering\n    supports_filtering = False\n    \n    return BenchmarkResult(\n        db_name=f\"FAISS ({'GPU' if gpu_used else 'CPU'})\",\n        index_time_s=index_time,\n        avg_query_time_ms=np.mean(query_times),\n        min_query_time_ms=min(query_times),\n        max_query_time_ms=max(query_times),\n        memory_mb=memory_after - memory_before,\n        supports_filtering=supports_filtering,\n        supports_gpu=gpu_used\n    )\n\nprint(\"üü¢ Benchmarking FAISS (attempting GPU)...\")\nfaiss_gpu_result = benchmark_faiss(chunks, chunk_embeddings, test_queries, use_gpu=True)\nprint(f\"   Mode: {faiss_gpu_result.db_name}\")\nprint(f\"   Index time: {faiss_gpu_result.index_time_s:.3f}s\")\nprint(f\"   Avg query: {faiss_gpu_result.avg_query_time_ms:.3f}ms\")\n\nprint(\"\\nüü° Benchmarking FAISS (CPU only)...\")\nfaiss_cpu_result = benchmark_faiss(chunks, chunk_embeddings, test_queries, use_gpu=False)\nprint(f\"   Index time: {faiss_cpu_result.index_time_s:.3f}s\")\nprint(f\"   Avg query: {faiss_cpu_result.avg_query_time_ms:.3f}ms\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üîç GPU Acceleration Deep Dive\n\nLet's see the GPU advantage more clearly with multiple index types.\n\n**FAISS Index Types:**\n- **Flat (Exact)**: Compares query to ALL vectors. Accurate but slow for large datasets.\n- **IVF (Approximate)**: Clusters vectors into `nlist` groups. At query time, only searches `nprobe` clusters.\n  - `nlist`: Number of clusters (more = better recall, slower indexing)\n  - `nprobe`: Clusters to search at query time (more = better recall, slower query)\n  - Requires training with `index.train(vectors)` before adding vectors"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def benchmark_faiss_index_types(\n    embeddings: List[List[float]],\n    test_queries: List[str]\n) -> Dict[str, float]:\n    \"\"\"\n    Compare different FAISS index types.\n    \n    Index Types Demonstrated:\n    1. IndexFlatIP - Exact brute-force search (slow but accurate)\n    2. IndexIVFFlat - Approximate search using clustering (faster for large datasets)\n       - Created with: faiss.IndexIVFFlat(quantizer, dimension, nlist)\n       - quantizer: Index used for clustering (usually IndexFlatIP)\n       - nlist: Number of clusters to create\n       - Must call index.train(vectors) before adding vectors\n       - Set index.nprobe to control how many clusters to search\n    \"\"\"\n    embeddings_np = np.array(embeddings).astype('float32')\n    dimension = embeddings_np.shape[1]\n    n_vectors = len(embeddings)\n    \n    results = {}\n    has_gpu_support = hasattr(faiss, 'StandardGpuResources')\n    \n    # 1. Flat Index (exact search)\n    print(\"   Testing Flat index (exact)...\")\n    index_flat = faiss.IndexFlatIP(dimension)\n    \n    # Try GPU if available\n    if torch.cuda.is_available() and has_gpu_support:\n        try:\n            res = faiss.StandardGpuResources()\n            index_flat = faiss.index_cpu_to_gpu(res, 0, index_flat)\n            print(\"      (Using GPU)\")\n        except Exception:\n            print(\"      (Using CPU - GPU not available)\")\n    else:\n        print(\"      (Using CPU)\")\n    \n    index_flat.add(embeddings_np)\n    \n    times = []\n    for query in test_queries:\n        query_np = np.array([embedding_model.embed_query(query)]).astype('float32')\n        start = time.time()\n        index_flat.search(query_np, 5)\n        times.append((time.time() - start) * 1000)\n    results[\"Flat (Exact)\"] = np.mean(times)\n    \n    # 2. IVF Index (approximate, faster)\n    print(\"   Testing IVF index (approximate)...\")\n    # nlist = number of clusters. Rule of thumb: sqrt(n_vectors) to 4*sqrt(n_vectors)\n    nlist = min(50, n_vectors // 10)\n    \n    # Create quantizer (used to assign vectors to clusters)\n    quantizer = faiss.IndexFlatIP(dimension)\n    \n    # Create IVF index: IndexIVFFlat(quantizer, dimension, nlist)\n    index_ivf = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n    \n    # IVF requires training to learn cluster centroids\n    index_ivf.train(embeddings_np)\n    index_ivf.add(embeddings_np)\n    \n    # nprobe = number of clusters to search (higher = more accurate but slower)\n    index_ivf.nprobe = 5\n    \n    times = []\n    for query in test_queries:\n        query_np = np.array([embedding_model.embed_query(query)]).astype('float32')\n        start = time.time()\n        index_ivf.search(query_np, 5)\n        times.append((time.time() - start) * 1000)\n    results[\"IVF (Approximate)\"] = np.mean(times)\n    \n    return results\n\nprint(\"üìä FAISS Index Type Comparison:\")\nfaiss_index_results = benchmark_faiss_index_types(chunk_embeddings, test_queries)\nfor name, time_ms in faiss_index_results.items():\n    print(f\"   {name}: {time_ms:.3f}ms\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 4: Qdrant Implementation\n\nQdrant is a production-ready vector database with excellent filtering support.\n\n### Key Qdrant Concepts and Functions\n\n| Class/Function | Purpose |\n|----------------|---------|\n| `QdrantClient(\":memory:\")` | Creates an in-memory Qdrant instance (or use URL for remote server) |\n| `VectorParams(size, distance)` | Defines vector configuration: dimension size and distance metric |\n| `Distance.COSINE` | Cosine similarity metric (also: EUCLID, DOT) |\n| `PointStruct(id, vector, payload)` | A single vector with its ID and metadata (payload) |\n| `client.create_collection(name, vectors_config)` | Creates a new collection to store vectors |\n| `client.upsert(collection, points)` | Adds or updates points in the collection |\n| `client.search(collection, query_vector, limit)` | Finds nearest neighbors |\n| `Filter(must, should)` | Combines multiple filter conditions (AND/OR logic) |\n| `FieldCondition(key, match)` | Filters on a specific metadata field |\n| `MatchValue(value)` | Matches exact value in a field |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def benchmark_qdrant(\n    chunks: List[Document],\n    embeddings: List[List[float]],\n    test_queries: List[str]\n) -> BenchmarkResult:\n    \"\"\"\n    Benchmark Qdrant performance (in-memory mode).\n    \n    Qdrant API Overview:\n    - QdrantClient: Main client for interacting with Qdrant\n      - \":memory:\" creates an in-memory instance (no persistence)\n      - Can also connect to a Qdrant server with QdrantClient(url=\"http://localhost:6333\")\n    \n    - create_collection: Creates a named collection with vector configuration\n      - VectorParams(size=dimension, distance=Distance.COSINE)\n    \n    - upsert: Adds points (vectors + metadata) to the collection\n      - Each point is a PointStruct(id, vector, payload)\n      - payload is a dict with metadata (like source, category, etc.)\n    \n    - search: Finds nearest neighbors with optional filtering\n      - query_filter uses Filter with FieldCondition for metadata filtering\n    \"\"\"\n    memory_before = get_memory_usage()\n    dimension = len(embeddings[0])\n    \n    # Time indexing\n    start = time.time()\n    \n    # Create in-memory client - use \":memory:\" for testing, URL for production\n    client = QdrantClient(\":memory:\")\n    \n    # Create collection with vector configuration\n    # VectorParams defines: size (dimension) and distance metric\n    client.create_collection(\n        collection_name=\"documents\",\n        vectors_config=VectorParams(\n            size=dimension,\n            distance=Distance.COSINE  # Also: Distance.EUCLID, Distance.DOT\n        )\n    )\n    \n    # Create points - each point has: id, vector, and payload (metadata)\n    points = [\n        PointStruct(\n            id=i,\n            vector=embeddings[i],\n            payload={\n                \"content\": chunks[i].page_content,\n                **chunks[i].metadata  # Include all document metadata\n            }\n        )\n        for i in range(len(chunks))\n    ]\n    \n    # Upsert (insert or update) points into the collection\n    client.upsert(\n        collection_name=\"documents\",\n        points=points\n    )\n    \n    index_time = time.time() - start\n    memory_after = get_memory_usage()\n    \n    # Time queries\n    query_times = []\n    for query in test_queries:\n        query_emb = embedding_model.embed_query(query)\n        \n        start = time.time()\n        results = client.search(\n            collection_name=\"documents\",\n            query_vector=query_emb,\n            limit=5\n        )\n        query_times.append((time.time() - start) * 1000)\n    \n    # Test filtering - Qdrant's killer feature!\n    # Filter uses: must (AND), should (OR) with FieldCondition\n    query_emb = embedding_model.embed_query(test_queries[0])\n    filtered = client.search(\n        collection_name=\"documents\",\n        query_vector=query_emb,\n        query_filter=Filter(\n            must=[FieldCondition(key=\"category\", match=MatchValue(value=\"technical\"))]\n        ),\n        limit=5\n    )\n    supports_filtering = len(filtered) > 0\n    \n    return BenchmarkResult(\n        db_name=\"Qdrant\",\n        index_time_s=index_time,\n        avg_query_time_ms=np.mean(query_times),\n        min_query_time_ms=min(query_times),\n        max_query_time_ms=max(query_times),\n        memory_mb=memory_after - memory_before,\n        supports_filtering=supports_filtering,\n        supports_gpu=False\n    )\n\nprint(\"üü£ Benchmarking Qdrant...\")\nqdrant_result = benchmark_qdrant(chunks, chunk_embeddings, test_queries)\nprint(f\"   Index time: {qdrant_result.index_time_s:.2f}s\")\nprint(f\"   Avg query: {qdrant_result.avg_query_time_ms:.2f}ms\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Comprehensive Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "all_results = [chroma_result, faiss_gpu_result, faiss_cpu_result, qdrant_result]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"üìä VECTOR DATABASE BENCHMARK RESULTS\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Database':<20} {'Index(s)':<10} {'Query(ms)':<12} {'Memory(MB)':<12} {'Filtering':<12} {'GPU':<8}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for r in sorted(all_results, key=lambda x: x.avg_query_time_ms):\n",
    "    filter_str = \"‚úÖ\" if r.supports_filtering else \"‚ùå\"\n",
    "    gpu_str = \"‚úÖ\" if r.supports_gpu else \"‚ùå\"\n",
    "    print(f\"{r.db_name:<20} {r.index_time_s:<10.3f} {r.avg_query_time_ms:<12.3f} \"\n",
    "          f\"{r.memory_mb:<12.1f} {filter_str:<12} {gpu_str:<8}\")\n",
    "\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate speedups\n",
    "baseline = chroma_result.avg_query_time_ms\n",
    "\n",
    "print(\"\\n‚ö° Speed Comparison (vs ChromaDB):\")\n",
    "for r in all_results:\n",
    "    if r.db_name != \"ChromaDB\":\n",
    "        speedup = baseline / r.avg_query_time_ms\n",
    "        print(f\"   {r.db_name}: {speedup:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Feature Comparison\n",
    "\n",
    "### Filtering Capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate filtering capabilities\n",
    "print(\"üîç Filtering Capabilities Comparison\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ChromaDB filtering example\n",
    "print(\"\\nüîµ ChromaDB Filtering:\")\n",
    "print(\"\"\"```python\n",
    "collection.query(\n",
    "    query_embeddings=[query_emb],\n",
    "    n_results=5,\n",
    "    where={\"category\": \"technical\"},  # Exact match\n",
    "    where_document={\"$contains\": \"GPU\"}  # Document contains\n",
    ")\n",
    "```\"\"\")\n",
    "\n",
    "# FAISS filtering (manual)\n",
    "print(\"\\nüü¢ FAISS Filtering (Manual Post-Processing):\")\n",
    "print(\"\"\"```python\n",
    "# FAISS doesn't have built-in filtering\n",
    "# You must filter results after retrieval\n",
    "distances, indices = index.search(query_np, k=50)  # Get more results\n",
    "filtered = [i for i in indices[0] if documents[i].metadata['category'] == 'technical'][:5]\n",
    "```\"\"\")\n",
    "\n",
    "# Qdrant filtering\n",
    "print(\"\\nüü£ Qdrant Filtering (Advanced):\")\n",
    "print(\"\"\"```python\n",
    "client.search(\n",
    "    collection_name=\"documents\",\n",
    "    query_vector=query_emb,\n",
    "    query_filter=Filter(\n",
    "        must=[\n",
    "            FieldCondition(key=\"category\", match=MatchValue(value=\"technical\")),\n",
    "            FieldCondition(key=\"file_size\", range=Range(gte=1000, lte=10000))\n",
    "        ],\n",
    "        should=[\n",
    "            FieldCondition(key=\"source\", match=MatchValue(value=\"guide.md\"))\n",
    "        ]\n",
    "    ),\n",
    "    limit=5\n",
    ")\n",
    "```\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Decision Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "üìã VECTOR DATABASE SELECTION GUIDE\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "üîµ Choose ChromaDB if:\n",
    "   ‚úÖ You're prototyping or learning\n",
    "   ‚úÖ You have < 1M vectors\n",
    "   ‚úÖ You need simple filtering\n",
    "   ‚úÖ You want Python-native simplicity\n",
    "   ‚ùå Not for: High-performance production, GPU acceleration\n",
    "\n",
    "üü¢ Choose FAISS if:\n",
    "   ‚úÖ Performance is critical (DGX Spark GPU acceleration!)\n",
    "   ‚úÖ You have millions of vectors\n",
    "   ‚úÖ You don't need complex filtering\n",
    "   ‚úÖ You want the fastest possible search\n",
    "   ‚ùå Not for: Built-in filtering, persistence (need to manage yourself)\n",
    "\n",
    "üü£ Choose Qdrant if:\n",
    "   ‚úÖ You need production-ready features\n",
    "   ‚úÖ Complex filtering is required\n",
    "   ‚úÖ You want distributed deployment\n",
    "   ‚úÖ You need a managed cloud option\n",
    "   ‚ùå Not for: Simplest use cases, GPU acceleration\n",
    "\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "üöÄ DGX Spark Recommendation:\n",
    "   Development: ChromaDB (simplicity)\n",
    "   Performance: FAISS with GPU (blazing fast!)\n",
    "   Production: Qdrant (features) or FAISS (speed)\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Using GPU with FAISS\n",
    "```python\n",
    "# ‚ùå Wrong: CPU-only FAISS on DGX Spark (wasted GPU!)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "# ‚úÖ Right: GPU-accelerated FAISS\n",
    "res = faiss.StandardGpuResources()\n",
    "index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "```\n",
    "\n",
    "### Mistake 2: Using Flat Index for Large Datasets\n",
    "```python\n",
    "# ‚ùå Wrong: Flat index with millions of vectors (slow!)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "# ‚úÖ Right: IVF index for large scale\n",
    "quantizer = faiss.IndexFlatIP(dimension)\n",
    "index = faiss.IndexIVFFlat(quantizer, dimension, nlist=1000)\n",
    "index.train(embeddings)\n",
    "```\n",
    "\n",
    "### Mistake 3: Not Persisting Vector Stores\n",
    "```python\n",
    "# ‚ùå Wrong: In-memory only, lost on restart\n",
    "client = chromadb.Client()\n",
    "\n",
    "# ‚úÖ Right: Persistent storage\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ‚úã Try It Yourself\n\n### Exercise 1: Scale Test\nDuplicate the chunks 10x and re-run the benchmarks. How do the results change?\n\n### Exercise 2: FAISS IVF Tuning\nExperiment with different `nlist` and `nprobe` values for FAISS IVF.\n\n**IVF Tuning Guide:**\n- `nlist` (training): More clusters = better recall but slower indexing. Try: 50, 100, 200\n- `nprobe` (query): More probes = better recall but slower queries. Try: 5, 10, 20\n\n### Exercise 3: Qdrant Quantization\nEnable scalar quantization in Qdrant and measure memory savings.\n\n<details>\n<summary>üí° Hint for Exercise 3</summary>\n\n**Scalar Quantization** reduces memory by converting float32 vectors to int8:\n- Reduces memory by ~4x (32 bits ‚Üí 8 bits per dimension)\n- Slight accuracy loss but usually acceptable\n- `always_ram=True` keeps quantized vectors in RAM for fast access\n\n```python\nfrom qdrant_client.models import ScalarQuantization, ScalarQuantizationConfig\n\n# ScalarQuantization: Converts vectors from float32 to int8\n# - type=\"int8\": Use 8-bit integers (also available: \"int4\" for even more compression)\n# - always_ram=True: Keep quantized vectors in memory for speed\n\nclient.update_collection(\n    collection_name=\"documents\",\n    quantization_config=ScalarQuantization(\n        scalar=ScalarQuantizationConfig(\n            type=\"int8\",        # Quantization precision\n            always_ram=True     # Keep in RAM for fast access\n        )\n    )\n)\n\n# After quantization, memory usage drops significantly\n# Trade-off: ~1-2% recall loss for ~4x memory savings\n```\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How to implement RAG with ChromaDB, FAISS, and Qdrant\n",
    "- ‚úÖ How to leverage GPU acceleration with FAISS on DGX Spark\n",
    "- ‚úÖ The trade-offs between different vector databases\n",
    "- ‚úÖ When to choose each database for your use case\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del embedding_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Remove temp directories\n",
    "for p in Path(\".\").glob(\"benchmark_*\"):\n",
    "    if p.is_dir():\n",
    "        shutil.rmtree(p)\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next lab, we'll implement **hybrid search** combining dense embeddings with sparse BM25 retrieval!\n",
    "\n",
    "‚û°Ô∏è Continue to [Lab 3.5.4: Hybrid Search](./lab-3.5.4-hybrid-search.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}