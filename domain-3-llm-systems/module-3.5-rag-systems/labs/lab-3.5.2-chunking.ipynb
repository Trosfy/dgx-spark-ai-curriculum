{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.5.2: Chunking Strategies Comparison\n",
    "\n",
    "**Module:** 3.5 - RAG Systems & Vector Databases  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "- [ ] Understand why chunking strategy matters for RAG quality\n",
    "- [ ] Implement fixed-size chunking with various sizes\n",
    "- [ ] Implement semantic chunking by section/paragraph\n",
    "- [ ] Implement sentence-based chunking\n",
    "- [ ] Evaluate and compare strategies using retrieval quality metrics\n",
    "- [ ] Know when to use each strategy\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Lab 3.5.1 (Basic RAG Pipeline)\n",
    "- Knowledge of: Basic RAG concepts, embeddings\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The Problem:** You've built a RAG system but users complain that it sometimes retrieves irrelevant chunks or misses important information. The culprit? Poor chunking strategy.\n",
    "\n",
    "**The Impact:**\n",
    "- Too small chunks: \"The model uses\" - uses WHAT? Context is missing!\n",
    "- Too large chunks: Returns pages of text when only one sentence was needed\n",
    "- Wrong boundaries: Splits a concept mid-explanation\n",
    "\n",
    "**The Solution:** Test different chunking strategies and measure which works best for YOUR data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßí ELI5: Why Does Chunking Matter?\n",
    "\n",
    "> **Imagine you're cutting a pizza.**\n",
    ">\n",
    "> - Cut it into tiny pieces (like dice) ‚Üí Hard to pick up, toppings fall off. That's **too small chunks** - you lose context.\n",
    "> - Don't cut it at all ‚Üí Can't fit a whole pizza in your mouth! That's **too large chunks** - too much irrelevant info.\n",
    "> - Cut through the middle of a topping ‚Üí Messy and wasteful. That's **bad boundaries** - splitting concepts mid-thought.\n",
    ">\n",
    "> The perfect pizza slice: just right size, cut between the toppings, easy to eat. That's what good chunking achieves for documents!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q langchain langchain-community langchain-huggingface chromadb sentence-transformers nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    SentenceTransformersTokenTextSplitter\n",
    ")\n",
    "from langchain.schema import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our sample documents\n",
    "DOCS_PATH = Path(\"../data/sample_documents\")\n",
    "\n",
    "documents = []\n",
    "for file_path in sorted(DOCS_PATH.glob(\"*.md\")):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    documents.append(Document(\n",
    "        page_content=content,\n",
    "        metadata={\"source\": file_path.name}\n",
    "    ))\n",
    "    print(f\"üìÑ Loaded: {file_path.name} ({len(content):,} chars)\")\n",
    "\n",
    "print(f\"\\nüìö Total: {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Chunking Strategies\n",
    "\n",
    "We'll implement and compare four different chunking strategies:\n",
    "\n",
    "| Strategy | Description | Best For |\n",
    "|----------|-------------|----------|\n",
    "| Fixed-Size (Small) | 256 chars, 25 overlap | Precise retrieval |\n",
    "| Fixed-Size (Medium) | 512 chars, 50 overlap | Balanced |\n",
    "| Fixed-Size (Large) | 1024 chars, 100 overlap | Rich context |\n",
    "| Semantic (Headers) | Split by markdown headers | Structured docs |\n",
    "| Sentence-Based | Group sentences | Natural boundaries |\n",
    "\n",
    "### 2.1 Fixed-Size Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fixed_size_chunks(\n",
    "    documents: List[Document],\n",
    "    chunk_size: int = 512,\n",
    "    chunk_overlap: int = 50\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents into fixed-size chunks with overlap.\n",
    "    Uses recursive splitting to try to maintain semantic boundaries.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    return splitter.split_documents(documents)\n",
    "\n",
    "# Create chunks with different sizes\n",
    "print(\"üìè Creating fixed-size chunks...\")\n",
    "\n",
    "chunks_256 = create_fixed_size_chunks(documents, chunk_size=256, chunk_overlap=25)\n",
    "chunks_512 = create_fixed_size_chunks(documents, chunk_size=512, chunk_overlap=50)\n",
    "chunks_1024 = create_fixed_size_chunks(documents, chunk_size=1024, chunk_overlap=100)\n",
    "\n",
    "print(f\"\\nüìä Chunk Counts:\")\n",
    "print(f\"   256 chars:  {len(chunks_256):4d} chunks (avg: {np.mean([len(c.page_content) for c in chunks_256]):.0f} chars)\")\n",
    "print(f\"   512 chars:  {len(chunks_512):4d} chunks (avg: {np.mean([len(c.page_content) for c in chunks_512]):.0f} chars)\")\n",
    "print(f\"   1024 chars: {len(chunks_1024):4d} chunks (avg: {np.mean([len(c.page_content) for c in chunks_1024]):.0f} chars)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Semantic Chunking (By Headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_semantic_chunks(\n",
    "    documents: List[Document],\n",
    "    max_chunk_size: int = 1500\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents by markdown headers, preserving semantic structure.\n",
    "    Falls back to fixed-size if chunks are too large.\n",
    "    \"\"\"\n",
    "    # Define header hierarchy\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"header_1\"),\n",
    "        (\"##\", \"header_2\"),\n",
    "        (\"###\", \"header_3\"),\n",
    "    ]\n",
    "    \n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on,\n",
    "        strip_headers=False  # Keep headers in content for context\n",
    "    )\n",
    "    \n",
    "    # Secondary splitter for oversized chunks\n",
    "    size_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=max_chunk_size,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    \n",
    "    all_chunks = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # First split by headers\n",
    "        header_chunks = markdown_splitter.split_text(doc.page_content)\n",
    "        \n",
    "        for chunk in header_chunks:\n",
    "            # Reconstruct header context\n",
    "            header_context = \"\"\n",
    "            for key in ['header_1', 'header_2', 'header_3']:\n",
    "                if key in chunk.metadata:\n",
    "                    level = int(key[-1])\n",
    "                    header_context += \"#\" * level + \" \" + chunk.metadata[key] + \"\\n\"\n",
    "            \n",
    "            content = header_context + chunk.page_content\n",
    "            \n",
    "            # If chunk is too large, split further\n",
    "            if len(content) > max_chunk_size:\n",
    "                sub_chunks = size_splitter.split_text(content)\n",
    "                for i, sub in enumerate(sub_chunks):\n",
    "                    all_chunks.append(Document(\n",
    "                        page_content=sub,\n",
    "                        metadata={\n",
    "                            \"source\": doc.metadata[\"source\"],\n",
    "                            **chunk.metadata,\n",
    "                            \"sub_chunk\": i\n",
    "                        }\n",
    "                    ))\n",
    "            else:\n",
    "                all_chunks.append(Document(\n",
    "                    page_content=content,\n",
    "                    metadata={\n",
    "                        \"source\": doc.metadata[\"source\"],\n",
    "                        **chunk.metadata\n",
    "                    }\n",
    "                ))\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "print(\"üìë Creating semantic chunks (by headers)...\")\n",
    "chunks_semantic = create_semantic_chunks(documents)\n",
    "\n",
    "print(f\"   Created {len(chunks_semantic)} chunks\")\n",
    "print(f\"   Avg size: {np.mean([len(c.page_content) for c in chunks_semantic]):.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Sentence-Based Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_chunks(\n",
    "    documents: List[Document],\n",
    "    sentences_per_chunk: int = 5,\n",
    "    sentence_overlap: int = 1\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents by sentences, grouping N sentences per chunk.\n",
    "    Maintains natural language boundaries.\n",
    "    \"\"\"\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    \n",
    "    all_chunks = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Tokenize into sentences\n",
    "        sentences = sent_tokenize(doc.page_content)\n",
    "        \n",
    "        # Group sentences with overlap\n",
    "        i = 0\n",
    "        chunk_idx = 0\n",
    "        while i < len(sentences):\n",
    "            # Get chunk sentences\n",
    "            end = min(i + sentences_per_chunk, len(sentences))\n",
    "            chunk_sentences = sentences[i:end]\n",
    "            chunk_text = \" \".join(chunk_sentences)\n",
    "            \n",
    "            all_chunks.append(Document(\n",
    "                page_content=chunk_text,\n",
    "                metadata={\n",
    "                    \"source\": doc.metadata[\"source\"],\n",
    "                    \"chunk_idx\": chunk_idx,\n",
    "                    \"sentence_count\": len(chunk_sentences)\n",
    "                }\n",
    "            ))\n",
    "            \n",
    "            # Move forward with overlap\n",
    "            i += sentences_per_chunk - sentence_overlap\n",
    "            chunk_idx += 1\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "print(\"üìù Creating sentence-based chunks...\")\n",
    "chunks_sentence = create_sentence_chunks(documents, sentences_per_chunk=5, sentence_overlap=1)\n",
    "\n",
    "print(f\"   Created {len(chunks_sentence)} chunks\")\n",
    "print(f\"   Avg size: {np.mean([len(c.page_content) for c in chunks_sentence]):.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Chunk Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = {\n",
    "    \"Fixed-256\": chunks_256,\n",
    "    \"Fixed-512\": chunks_512,\n",
    "    \"Fixed-1024\": chunks_1024,\n",
    "    \"Semantic\": chunks_semantic,\n",
    "    \"Sentence\": chunks_sentence\n",
    "}\n",
    "\n",
    "print(\"üìä Chunking Strategy Comparison:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Strategy':<15} {'Count':<10} {'Avg Size':<12} {'Min':<8} {'Max':<8}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, chunks in strategies.items():\n",
    "    sizes = [len(c.page_content) for c in chunks]\n",
    "    print(f\"{name:<15} {len(chunks):<10} {np.mean(sizes):<12.0f} {min(sizes):<8} {max(sizes):<8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at how each strategy chunks the same content\n",
    "sample_doc = documents[0]  # DGX Spark guide\n",
    "\n",
    "print(f\"üìÑ Sample Document: {sample_doc.metadata['source']}\")\n",
    "print(f\"   Total length: {len(sample_doc.page_content):,} characters\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Show first few chunks from each strategy for this document\n",
    "for name, all_chunks in strategies.items():\n",
    "    # Filter chunks from this document\n",
    "    doc_chunks = [c for c in all_chunks if c.metadata.get('source') == sample_doc.metadata['source']]\n",
    "    \n",
    "    print(f\"\\nüîπ {name}: {len(doc_chunks)} chunks\")\n",
    "    \n",
    "    if doc_chunks:\n",
    "        # Show first chunk\n",
    "        first_chunk = doc_chunks[0].page_content[:200]\n",
    "        print(f\"   First chunk preview: '{first_chunk}...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Retrieval Quality Evaluation\n",
    "\n",
    "Now let's actually test which chunking strategy retrieves the best results!\n",
    "\n",
    "### Create Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Q&A pairs with expected source documents\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"question\": \"How much unified memory does DGX Spark have?\",\n",
    "        \"expected_source\": \"dgx_spark_technical_guide.md\",\n",
    "        \"expected_keywords\": [\"128GB\", \"unified memory\", \"LPDDR5X\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the attention mechanism in transformers?\",\n",
    "        \"expected_source\": \"transformer_architecture_explained.md\",\n",
    "        \"expected_keywords\": [\"attention\", \"query\", \"key\", \"value\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does LoRA reduce training memory requirements?\",\n",
    "        \"expected_source\": \"lora_finetuning_guide.md\",\n",
    "        \"expected_keywords\": [\"low-rank\", \"decomposition\", \"parameters\", \"trainable\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is GPTQ quantization?\",\n",
    "        \"expected_source\": \"quantization_methods.md\",\n",
    "        \"expected_keywords\": [\"GPTQ\", \"quantization\", \"4-bit\", \"weight\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are the advantages of RAG over fine-tuning?\",\n",
    "        \"expected_source\": \"rag_architecture_patterns.md\",\n",
    "        \"expected_keywords\": [\"RAG\", \"retrieval\", \"dynamic\", \"grounded\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does FAISS compare to ChromaDB?\",\n",
    "        \"expected_source\": \"vector_database_comparison.md\",\n",
    "        \"expected_keywords\": [\"FAISS\", \"ChromaDB\", \"GPU\", \"performance\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are Tensor Cores used for?\",\n",
    "        \"expected_source\": \"dgx_spark_technical_guide.md\",\n",
    "        \"expected_keywords\": [\"tensor core\", \"AI\", \"compute\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do positional encodings work?\",\n",
    "        \"expected_source\": \"transformer_architecture_explained.md\",\n",
    "        \"expected_keywords\": [\"positional\", \"encoding\", \"sinusoidal\", \"position\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is QLoRA and how does it differ from LoRA?\",\n",
    "        \"expected_source\": \"lora_finetuning_guide.md\",\n",
    "        \"expected_keywords\": [\"QLoRA\", \"4-bit\", \"quantized\", \"NF4\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is hybrid search in RAG?\",\n",
    "        \"expected_source\": \"rag_architecture_patterns.md\",\n",
    "        \"expected_keywords\": [\"hybrid\", \"dense\", \"sparse\", \"BM25\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"üìã Created {len(eval_dataset)} evaluation questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Loading embedding model...\")\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True, \"batch_size\": 32}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Embedding model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Each Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvalResult:\n",
    "    \"\"\"Results from evaluating a chunking strategy.\"\"\"\n",
    "    strategy_name: str\n",
    "    source_recall_at_1: float  # Correct source in top 1\n",
    "    source_recall_at_3: float  # Correct source in top 3\n",
    "    source_recall_at_5: float  # Correct source in top 5\n",
    "    keyword_coverage: float    # % of expected keywords found\n",
    "    avg_retrieval_time_ms: float\n",
    "    chunk_count: int\n",
    "\n",
    "\n",
    "def evaluate_chunking_strategy(\n",
    "    strategy_name: str,\n",
    "    chunks: List[Document],\n",
    "    embedding_model: HuggingFaceEmbeddings,\n",
    "    eval_dataset: List[Dict],\n",
    "    k: int = 5\n",
    ") -> EvalResult:\n",
    "    \"\"\"\n",
    "    Evaluate a chunking strategy on the evaluation dataset.\n",
    "    \"\"\"\n",
    "    import shutil\n",
    "    \n",
    "    # Create a temporary vector store\n",
    "    db_path = f\"./temp_chroma_{strategy_name.replace('-', '_')}\"\n",
    "    if Path(db_path).exists():\n",
    "        shutil.rmtree(db_path)\n",
    "    \n",
    "    # Build vector store\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=db_path\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    source_correct_at_1 = 0\n",
    "    source_correct_at_3 = 0\n",
    "    source_correct_at_5 = 0\n",
    "    keyword_scores = []\n",
    "    retrieval_times = []\n",
    "    \n",
    "    for item in eval_dataset:\n",
    "        question = item[\"question\"]\n",
    "        expected_source = item[\"expected_source\"]\n",
    "        expected_keywords = item[\"expected_keywords\"]\n",
    "        \n",
    "        # Time the retrieval\n",
    "        start_time = time.time()\n",
    "        results = vectorstore.similarity_search(question, k=k)\n",
    "        retrieval_times.append((time.time() - start_time) * 1000)\n",
    "        \n",
    "        # Check source recall\n",
    "        retrieved_sources = [r.metadata.get('source') for r in results]\n",
    "        \n",
    "        if retrieved_sources and retrieved_sources[0] == expected_source:\n",
    "            source_correct_at_1 += 1\n",
    "        if expected_source in retrieved_sources[:3]:\n",
    "            source_correct_at_3 += 1\n",
    "        if expected_source in retrieved_sources:\n",
    "            source_correct_at_5 += 1\n",
    "        \n",
    "        # Check keyword coverage\n",
    "        all_content = \" \".join([r.page_content.lower() for r in results])\n",
    "        keywords_found = sum(1 for kw in expected_keywords if kw.lower() in all_content)\n",
    "        keyword_scores.append(keywords_found / len(expected_keywords))\n",
    "    \n",
    "    # Cleanup\n",
    "    shutil.rmtree(db_path)\n",
    "    \n",
    "    n = len(eval_dataset)\n",
    "    return EvalResult(\n",
    "        strategy_name=strategy_name,\n",
    "        source_recall_at_1=source_correct_at_1 / n,\n",
    "        source_recall_at_3=source_correct_at_3 / n,\n",
    "        source_recall_at_5=source_correct_at_5 / n,\n",
    "        keyword_coverage=np.mean(keyword_scores),\n",
    "        avg_retrieval_time_ms=np.mean(retrieval_times),\n",
    "        chunk_count=len(chunks)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all strategies\n",
    "print(\"üî¨ Evaluating chunking strategies...\")\n",
    "print(\"   This may take a few minutes...\\n\")\n",
    "\n",
    "results = []\n",
    "for name, chunks in strategies.items():\n",
    "    print(f\"   Evaluating {name}...\")\n",
    "    result = evaluate_chunking_strategy(\n",
    "        strategy_name=name,\n",
    "        chunks=chunks,\n",
    "        embedding_model=embedding_model,\n",
    "        eval_dataset=eval_dataset\n",
    "    )\n",
    "    results.append(result)\n",
    "    print(f\"   ‚úÖ {name}: Recall@5 = {result.source_recall_at_5:.0%}\")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä EVALUATION RESULTS\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Strategy':<15} {'Chunks':<8} {'R@1':<8} {'R@3':<8} {'R@5':<8} {'Keywords':<10} {'Time(ms)':<10}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for r in sorted(results, key=lambda x: x.source_recall_at_5, reverse=True):\n",
    "    print(f\"{r.strategy_name:<15} {r.chunk_count:<8} {r.source_recall_at_1:<8.0%} \"\n",
    "          f\"{r.source_recall_at_3:<8.0%} {r.source_recall_at_5:<8.0%} \"\n",
    "          f\"{r.keyword_coverage:<10.0%} {r.avg_retrieval_time_ms:<10.1f}\")\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"\\nüìù Legend:\")\n",
    "print(\"   R@K = Source Recall at K (correct document in top K results)\")\n",
    "print(\"   Keywords = Percentage of expected keywords found in retrieved chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the winner\n",
    "best_result = max(results, key=lambda r: r.source_recall_at_5 + r.keyword_coverage)\n",
    "\n",
    "print(f\"\\nüèÜ Best Strategy: {best_result.strategy_name}\")\n",
    "print(f\"   - Recall@5: {best_result.source_recall_at_5:.0%}\")\n",
    "print(f\"   - Keyword Coverage: {best_result.keyword_coverage:.0%}\")\n",
    "print(f\"   - Chunk Count: {best_result.chunk_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Analysis and Recommendations\n",
    "\n",
    "### When to Use Each Strategy\n",
    "\n",
    "| Strategy | Best For | Avoid When |\n",
    "|----------|----------|------------|\n",
    "| **Fixed-256** | Very specific queries, code snippets | Long-form explanations needed |\n",
    "| **Fixed-512** | General purpose, balanced | Highly structured documents |\n",
    "| **Fixed-1024** | Complex topics, rich context | Precise retrieval required |\n",
    "| **Semantic** | Structured docs (manuals, docs) | Unstructured text |\n",
    "| **Sentence** | Natural language, Q&A | Technical/code-heavy docs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine where each strategy succeeds and fails\n",
    "print(\"üîç Detailed Analysis: Looking at specific examples\\n\")\n",
    "\n",
    "# Test a specific question with all strategies\n",
    "test_q = eval_dataset[0]  # \"How much unified memory does DGX Spark have?\"\n",
    "print(f\"Question: {test_q['question']}\")\n",
    "print(f\"Expected Source: {test_q['expected_source']}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, chunks in strategies.items():\n",
    "    # Quick search (build temp vectorstore)\n",
    "    import shutil\n",
    "    db_path = f\"./temp_analysis_{name.replace('-', '_')}\"\n",
    "    if Path(db_path).exists():\n",
    "        shutil.rmtree(db_path)\n",
    "    \n",
    "    vs = Chroma.from_documents(chunks, embedding_model, persist_directory=db_path)\n",
    "    top_result = vs.similarity_search(test_q['question'], k=1)[0]\n",
    "    \n",
    "    is_correct = top_result.metadata.get('source') == test_q['expected_source']\n",
    "    icon = \"‚úÖ\" if is_correct else \"‚ùå\"\n",
    "    \n",
    "    print(f\"\\n{icon} {name}:\")\n",
    "    print(f\"   Retrieved: {top_result.metadata.get('source')}\")\n",
    "    print(f\"   Content: {top_result.page_content[:100]}...\")\n",
    "    \n",
    "    shutil.rmtree(db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: No Overlap Between Chunks\n",
    "```python\n",
    "# ‚ùå Wrong: Key information at boundaries is lost\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=0)\n",
    "\n",
    "# ‚úÖ Right: 10-20% overlap preserves boundary information\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "```\n",
    "\n",
    "### Mistake 2: Using the Same Strategy for All Document Types\n",
    "```python\n",
    "# ‚ùå Wrong: One size fits all\n",
    "all_chunks = splitter.split_documents(all_documents)\n",
    "\n",
    "# ‚úÖ Right: Different strategies for different content\n",
    "code_chunks = fixed_size_splitter.split_documents(code_docs)\n",
    "doc_chunks = semantic_splitter.split_documents(documentation)\n",
    "```\n",
    "\n",
    "### Mistake 3: Not Preserving Metadata\n",
    "```python\n",
    "# ‚ùå Wrong: Source information is lost\n",
    "chunks = [chunk.page_content for chunk in split_result]\n",
    "\n",
    "# ‚úÖ Right: Keep metadata for citation\n",
    "chunks = splitter.split_documents(documents)  # Preserves metadata\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Custom Chunk Size\n",
    "Try `chunk_size=384` with `overlap=64`. How does it compare?\n",
    "\n",
    "### Exercise 2: Paragraph-Based Chunking\n",
    "Implement a chunking strategy that splits only on `\\n\\n` (paragraph breaks).\n",
    "\n",
    "### Exercise 3: Add Your Own Documents\n",
    "Add 2-3 new documents and see if the best strategy changes.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint for Exercise 2</summary>\n",
    "\n",
    "```python\n",
    "paragraph_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=2000,  # Max size\n",
    "    chunk_overlap=0\n",
    ")\n",
    "```\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ Why chunking strategy matters for RAG quality\n",
    "- ‚úÖ How to implement fixed-size, semantic, and sentence-based chunking\n",
    "- ‚úÖ How to evaluate chunking strategies with retrieval metrics\n",
    "- ‚úÖ When to use different chunking strategies\n",
    "\n",
    "**Key Insight:** There's no universally \"best\" chunking strategy. The optimal choice depends on your documents and queries. Always evaluate on your specific use case!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del embedding_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Remove any temp databases\n",
    "import shutil\n",
    "for p in Path(\".\").glob(\"temp_*\"):\n",
    "    if p.is_dir():\n",
    "        shutil.rmtree(p)\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next lab, we'll compare different **vector databases** (ChromaDB, FAISS, Qdrant) to find the best one for your use case!\n",
    "\n",
    "‚û°Ô∏è Continue to [Lab 3.5.3: Vector Database Comparison](./lab-3.5.3-vector-dbs.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
