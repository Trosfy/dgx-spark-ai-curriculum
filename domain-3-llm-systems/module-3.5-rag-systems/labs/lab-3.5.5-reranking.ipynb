{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.5.5: Reranking Pipeline\n",
    "\n",
    "**Module:** 3.5 - RAG Systems & Vector Databases  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê (Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "- [ ] Understand why reranking improves retrieval quality\n",
    "- [ ] Implement two-stage retrieval with cross-encoders\n",
    "- [ ] Load and use BGE-reranker on DGX Spark GPU\n",
    "- [ ] Benchmark quality improvement vs latency cost\n",
    "- [ ] Find the optimal first-stage K for your use case\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Labs 3.5.1-3.5.4\n",
    "- Understanding of: Bi-encoders vs Cross-encoders\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The Problem:** Even with hybrid search, your top 5 results aren't always the best 5. The embedding model might rank a tangentially related chunk above the perfect match.\n",
    "\n",
    "**The Solution:** Two-stage retrieval! First, quickly get 50 candidates. Then, use a more powerful model to rerank and select the final 5.\n",
    "\n",
    "**Industry Usage:** Google Search, Bing, and enterprise search all use two-stage retrieval. Cohere's Rerank API is a commercial offering of exactly this.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßí ELI5: Bi-Encoders vs Cross-Encoders\n",
    "\n",
    "> **Bi-Encoder (Fast, Less Accurate):**\n",
    "> Imagine you and your friend each read a different book, then try to guess if your books are similar by describing them in a few sentences. Fast, but you might miss subtle connections.\n",
    ">\n",
    "> **Cross-Encoder (Slow, More Accurate):**\n",
    "> Now imagine both of you read BOTH books together, discussing as you go. \"Oh, this part connects to that part!\" Much slower, but you catch every connection.\n",
    ">\n",
    "> **Two-Stage Retrieval:**\n",
    "> - Stage 1 (Bi-Encoder): Quickly filter 1000 books down to 50 candidates\n",
    "> - Stage 2 (Cross-Encoder): Carefully compare those 50 to pick the best 5\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q \\\n",
    "    langchain langchain-community langchain-huggingface \\\n",
    "    chromadb sentence-transformers \\\n",
    "    rank_bm25\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and chunk documents\n",
    "DOCS_PATH = Path(\"../data/sample_documents\")\n",
    "\n",
    "documents = []\n",
    "for file_path in sorted(DOCS_PATH.glob(\"*.md\")):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    documents.append(Document(\n",
    "        page_content=content,\n",
    "        metadata={\"source\": file_path.name}\n",
    "    ))\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(f\"üìö Loaded {len(documents)} documents ‚Üí {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 2: Understanding Cross-Encoders\n\n### Bi-Encoder vs Cross-Encoder Architecture\n\n```\nBI-ENCODER:                          CROSS-ENCODER:\n                                     \nQuery ‚îÄ‚îÄ‚ñ∫ Encoder ‚îÄ‚îÄ‚ñ∫ Query Vec      Query + Doc ‚îÄ‚îÄ‚ñ∫ Encoder ‚îÄ‚îÄ‚ñ∫ Score\n                      ‚Üì cosine\nDoc ‚îÄ‚îÄ‚ñ∫ Encoder ‚îÄ‚îÄ‚ñ∫ Doc Vec\n\n- Encode separately                  - Encode together\n- Compare vectors                    - Direct relevance score\n- Fast (can pre-compute docs)        - Slow (must run for each pair)\n- Used for: Initial retrieval        - Used for: Reranking\n```\n\n### Key Classes from sentence-transformers\n\n| Class | Purpose |\n|-------|---------|\n| `SentenceTransformer(model)` | Bi-encoder: Encodes text into vectors separately |\n| `CrossEncoder(model)` | Cross-encoder: Takes query+doc pairs, outputs relevance scores |\n| `.encode(texts)` | SentenceTransformer method to get embeddings |\n| `.predict(pairs)` | CrossEncoder method to get relevance scores for query-doc pairs |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load models\n# ============\n# HuggingFaceEmbeddings is a LangChain wrapper around SentenceTransformer\n# It provides a consistent interface for embedding models\n\nprint(\"üîÑ Loading embedding model (bi-encoder)...\")\nbi_encoder = HuggingFaceEmbeddings(\n    model_name=\"BAAI/bge-large-en-v1.5\",\n    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n    encode_kwargs={\"normalize_embeddings\": True}\n)\nprint(\"‚úÖ Bi-encoder loaded!\")\n\n# CrossEncoder from sentence-transformers\n# ========================================\n# CrossEncoder takes a pair of texts [query, document] and outputs a relevance score\n# Unlike bi-encoders, it cannot pre-compute document embeddings\n# \n# Usage:\n#   scores = cross_encoder.predict([[query1, doc1], [query1, doc2], ...])\n#   Returns: array of relevance scores (higher = more relevant)\n\nprint(\"\\nüîÑ Loading reranker model (cross-encoder)...\")\ncross_encoder = CrossEncoder(\n    \"BAAI/bge-reranker-large\",  # Specialized model for reranking\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n)\nprint(\"‚úÖ Cross-encoder loaded!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the difference\n",
    "query = \"How much memory does DGX Spark have?\"\n",
    "doc1 = \"The DGX Spark has 128GB of unified LPDDR5X memory.\"\n",
    "doc2 = \"Memory management is important for large models.\"\n",
    "\n",
    "print(f\"üìù Query: '{query}'\")\n",
    "print(f\"üìÑ Doc 1: '{doc1}'\")\n",
    "print(f\"üìÑ Doc 2: '{doc2}'\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Bi-encoder: embed separately, compute similarity\n",
    "query_emb = bi_encoder.embed_query(query)\n",
    "doc1_emb = bi_encoder.embed_query(doc1)\n",
    "doc2_emb = bi_encoder.embed_query(doc2)\n",
    "\n",
    "bi_score1 = np.dot(query_emb, doc1_emb)\n",
    "bi_score2 = np.dot(query_emb, doc2_emb)\n",
    "\n",
    "print(f\"\\nüîµ Bi-Encoder Scores (cosine similarity):\")\n",
    "print(f\"   Doc 1: {bi_score1:.4f}\")\n",
    "print(f\"   Doc 2: {bi_score2:.4f}\")\n",
    "print(f\"   Bi-encoder prefers: Doc {'1' if bi_score1 > bi_score2 else '2'}\")\n",
    "\n",
    "# Cross-encoder: process together, get relevance score\n",
    "cross_scores = cross_encoder.predict([\n",
    "    [query, doc1],\n",
    "    [query, doc2]\n",
    "])\n",
    "\n",
    "print(f\"\\nüü¢ Cross-Encoder Scores (relevance):\")\n",
    "print(f\"   Doc 1: {cross_scores[0]:.4f}\")\n",
    "print(f\"   Doc 2: {cross_scores[1]:.4f}\")\n",
    "print(f\"   Cross-encoder prefers: Doc {'1' if cross_scores[0] > cross_scores[1] else '2'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "Both encoders correctly identified Doc 1 as more relevant, but notice the cross-encoder has a much larger gap between scores. The cross-encoder can:\n",
    "\n",
    "1. See the query and document together\n",
    "2. Understand exact word matches (\"DGX Spark\", \"128GB\", \"memory\")\n",
    "3. Evaluate semantic relevance more precisely\n",
    "\n",
    "The trade-off? Cross-encoders are ~10-100x slower because they can't pre-compute document embeddings.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Building the Two-Stage Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoStageRetriever:\n",
    "    \"\"\"\n",
    "    Two-stage retrieval with bi-encoder + cross-encoder reranking.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        documents: List[Document],\n",
    "        bi_encoder: HuggingFaceEmbeddings,\n",
    "        cross_encoder: CrossEncoder\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize with documents and models.\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        self.bi_encoder = bi_encoder\n",
    "        self.cross_encoder = cross_encoder\n",
    "        \n",
    "        # Pre-compute document embeddings\n",
    "        print(\"   Computing document embeddings...\")\n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        self.embeddings = np.array(bi_encoder.embed_documents(texts))\n",
    "        print(f\"   ‚úÖ Embedded {len(documents)} documents\")\n",
    "        \n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        k: int = 5,\n",
    "        first_stage_k: int = 50\n",
    "    ) -> List[Tuple[Document, float, Dict]]:\n",
    "        \"\"\"\n",
    "        Two-stage retrieval.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            k: Final number of results\n",
    "            first_stage_k: Candidates from first stage\n",
    "            \n",
    "        Returns:\n",
    "            List of (document, score, metadata) tuples\n",
    "        \"\"\"\n",
    "        # Stage 1: Fast bi-encoder retrieval\n",
    "        stage1_start = time.time()\n",
    "        \n",
    "        query_emb = np.array(self.bi_encoder.embed_query(query))\n",
    "        similarities = np.dot(self.embeddings, query_emb)\n",
    "        top_indices = np.argsort(similarities)[-first_stage_k:][::-1]\n",
    "        \n",
    "        candidates = [(self.documents[i], similarities[i]) for i in top_indices]\n",
    "        stage1_time = time.time() - stage1_start\n",
    "        \n",
    "        # Stage 2: Cross-encoder reranking\n",
    "        stage2_start = time.time()\n",
    "        \n",
    "        pairs = [[query, doc.page_content] for doc, _ in candidates]\n",
    "        rerank_scores = self.cross_encoder.predict(pairs)\n",
    "        \n",
    "        # Sort by rerank score\n",
    "        reranked = sorted(\n",
    "            zip(candidates, rerank_scores),\n",
    "            key=lambda x: -x[1]\n",
    "        )\n",
    "        \n",
    "        stage2_time = time.time() - stage2_start\n",
    "        \n",
    "        # Build results with metadata\n",
    "        results = []\n",
    "        for i, ((doc, bi_score), rerank_score) in enumerate(reranked[:k]):\n",
    "            results.append((\n",
    "                doc,\n",
    "                rerank_score,\n",
    "                {\n",
    "                    \"bi_encoder_score\": float(bi_score),\n",
    "                    \"bi_encoder_rank\": top_indices.tolist().index(\n",
    "                        self.documents.index(doc)\n",
    "                    ) + 1 if doc in self.documents else -1,\n",
    "                    \"rerank_score\": float(rerank_score),\n",
    "                    \"final_rank\": i + 1,\n",
    "                    \"stage1_time_ms\": stage1_time * 1000,\n",
    "                    \"stage2_time_ms\": stage2_time * 1000\n",
    "                }\n",
    "            ))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def search_without_reranking(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        Single-stage retrieval (bi-encoder only) for comparison.\n",
    "        \"\"\"\n",
    "        query_emb = np.array(self.bi_encoder.embed_query(query))\n",
    "        similarities = np.dot(self.embeddings, query_emb)\n",
    "        top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "        \n",
    "        return [(self.documents[i], similarities[i]) for i in top_indices]\n",
    "\n",
    "\n",
    "# Build the two-stage retriever\n",
    "print(\"üîÑ Building two-stage retriever...\")\n",
    "two_stage = TwoStageRetriever(chunks, bi_encoder, cross_encoder)\n",
    "print(\"‚úÖ Two-stage retriever ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the two-stage retriever\n",
    "query = \"What is the attention mechanism in transformers?\"\n",
    "\n",
    "print(f\"üîç Query: '{query}'\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Compare with and without reranking\n",
    "without_rerank = two_stage.search_without_reranking(query, k=5)\n",
    "with_rerank = two_stage.search(query, k=5, first_stage_k=50)\n",
    "\n",
    "print(\"\\nüîµ WITHOUT Reranking (Bi-encoder only):\")\n",
    "for i, (doc, score) in enumerate(without_rerank):\n",
    "    print(f\"   {i+1}. [{score:.4f}] {doc.metadata['source']}\")\n",
    "    print(f\"      {doc.page_content[:80]}...\")\n",
    "\n",
    "print(\"\\nüü¢ WITH Reranking (Two-stage):\")\n",
    "for doc, score, meta in with_rerank:\n",
    "    bi_rank = meta['bi_encoder_rank']\n",
    "    final_rank = meta['final_rank']\n",
    "    movement = bi_rank - final_rank\n",
    "    arrow = \"‚Üë\" if movement > 0 else (\"‚Üì\" if movement < 0 else \"=\")\n",
    "    \n",
    "    print(f\"   {final_rank}. [{score:.4f}] {doc.metadata['source']} (was #{bi_rank} {arrow})\")\n",
    "    print(f\"      {doc.page_content[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Benchmarking Quality Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation dataset\n",
    "eval_dataset = [\n",
    "    {\"question\": \"What is the memory capacity of DGX Spark?\", \"expected_source\": \"dgx_spark_technical_guide.md\"},\n",
    "    {\"question\": \"How do Tensor Cores work?\", \"expected_source\": \"dgx_spark_technical_guide.md\"},\n",
    "    {\"question\": \"Explain self-attention in transformers\", \"expected_source\": \"transformer_architecture_explained.md\"},\n",
    "    {\"question\": \"What is positional encoding?\", \"expected_source\": \"transformer_architecture_explained.md\"},\n",
    "    {\"question\": \"How does LoRA reduce memory?\", \"expected_source\": \"lora_finetuning_guide.md\"},\n",
    "    {\"question\": \"What is QLoRA?\", \"expected_source\": \"lora_finetuning_guide.md\"},\n",
    "    {\"question\": \"How does GPTQ quantization work?\", \"expected_source\": \"quantization_methods.md\"},\n",
    "    {\"question\": \"What is GGUF format?\", \"expected_source\": \"quantization_methods.md\"},\n",
    "    {\"question\": \"What are the benefits of RAG?\", \"expected_source\": \"rag_architecture_patterns.md\"},\n",
    "    {\"question\": \"How to choose a vector database?\", \"expected_source\": \"vector_database_comparison.md\"},\n",
    "]\n",
    "\n",
    "print(f\"üìã Evaluation dataset: {len(eval_dataset)} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retriever(\n",
    "    retriever: TwoStageRetriever,\n",
    "    eval_dataset: List[Dict],\n",
    "    use_reranking: bool = True,\n",
    "    first_stage_k: int = 50,\n",
    "    k: int = 5\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate retriever on the evaluation dataset.\n",
    "    \"\"\"\n",
    "    correct_at_1 = 0\n",
    "    correct_at_3 = 0\n",
    "    correct_at_5 = 0\n",
    "    total_time_ms = 0\n",
    "    \n",
    "    for item in eval_dataset:\n",
    "        question = item[\"question\"]\n",
    "        expected = item[\"expected_source\"]\n",
    "        \n",
    "        start = time.time()\n",
    "        if use_reranking:\n",
    "            results = retriever.search(question, k=k, first_stage_k=first_stage_k)\n",
    "            sources = [r[0].metadata.get('source') for r in results]\n",
    "        else:\n",
    "            results = retriever.search_without_reranking(question, k=k)\n",
    "            sources = [r[0].metadata.get('source') for r in results]\n",
    "        total_time_ms += (time.time() - start) * 1000\n",
    "        \n",
    "        if sources and sources[0] == expected:\n",
    "            correct_at_1 += 1\n",
    "        if expected in sources[:3]:\n",
    "            correct_at_3 += 1\n",
    "        if expected in sources[:5]:\n",
    "            correct_at_5 += 1\n",
    "    \n",
    "    n = len(eval_dataset)\n",
    "    return {\n",
    "        \"recall@1\": correct_at_1 / n,\n",
    "        \"recall@3\": correct_at_3 / n,\n",
    "        \"recall@5\": correct_at_5 / n,\n",
    "        \"avg_latency_ms\": total_time_ms / n\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate both methods\n",
    "print(\"üìä Evaluating retrieval methods...\")\n",
    "\n",
    "no_rerank_metrics = evaluate_retriever(two_stage, eval_dataset, use_reranking=False)\n",
    "rerank_metrics = evaluate_retriever(two_stage, eval_dataset, use_reranking=True)\n",
    "\n",
    "print(f\"\\n{'Method':<25} {'R@1':<10} {'R@5':<10} {'Latency':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Bi-encoder Only':<25} {no_rerank_metrics['recall@1']:<10.0%} \"\n",
    "      f\"{no_rerank_metrics['recall@5']:<10.0%} {no_rerank_metrics['avg_latency_ms']:<15.1f}ms\")\n",
    "print(f\"{'Two-Stage (Reranking)':<25} {rerank_metrics['recall@1']:<10.0%} \"\n",
    "      f\"{rerank_metrics['recall@5']:<10.0%} {rerank_metrics['avg_latency_ms']:<15.1f}ms\")\n",
    "\n",
    "# Calculate improvement\n",
    "r1_improvement = (rerank_metrics['recall@1'] - no_rerank_metrics['recall@1']) / no_rerank_metrics['recall@1'] * 100 if no_rerank_metrics['recall@1'] > 0 else 0\n",
    "latency_increase = rerank_metrics['avg_latency_ms'] / no_rerank_metrics['avg_latency_ms']\n",
    "\n",
    "print(f\"\\nüéØ Improvement: R@1 +{r1_improvement:.1f}%\")\n",
    "print(f\"‚è±Ô∏è Latency: {latency_increase:.1f}x slower (still fast on GPU!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Finding Optimal First-Stage K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different first-stage K values\n",
    "print(\"üî¨ Finding optimal first-stage K...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "k_values = [10, 20, 30, 50, 75, 100]\n",
    "results = []\n",
    "\n",
    "for first_stage_k in k_values:\n",
    "    metrics = evaluate_retriever(\n",
    "        two_stage, eval_dataset, \n",
    "        use_reranking=True, \n",
    "        first_stage_k=first_stage_k\n",
    "    )\n",
    "    results.append((first_stage_k, metrics))\n",
    "    print(f\"   K={first_stage_k:3d}: R@1={metrics['recall@1']:.0%}, \"\n",
    "          f\"R@5={metrics['recall@5']:.0%}, Latency={metrics['avg_latency_ms']:.1f}ms\")\n",
    "\n",
    "# Find best K (optimize for R@1 with reasonable latency)\n",
    "best_k, best_metrics = max(results, key=lambda x: x[1]['recall@1'])\n",
    "print(f\"\\nüèÜ Best K: {best_k} (R@1: {best_metrics['recall@1']:.0%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trade-off\n",
    "print(\"\\nüìä Quality vs Latency Trade-off:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for first_stage_k, metrics in results:\n",
    "    r1 = metrics['recall@1']\n",
    "    latency = metrics['avg_latency_ms']\n",
    "    \n",
    "    # Visual bar\n",
    "    quality_bar = \"‚ñà\" * int(r1 * 20)\n",
    "    latency_bar = \"‚ñë\" * int(latency / 10)\n",
    "    \n",
    "    print(f\"K={first_stage_k:3d} | Quality: {quality_bar:<20} {r1:.0%}\")\n",
    "    print(f\"       | Latency: {latency_bar:<20} {latency:.0f}ms\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 6: Production Reranking Pipeline\n\nFor production use, we use `SentenceTransformer` directly (instead of the LangChain wrapper) for better control over batching and performance.\n\n### SentenceTransformer vs HuggingFaceEmbeddings\n\n| Feature | SentenceTransformer | HuggingFaceEmbeddings |\n|---------|--------------------|-----------------------|\n| Library | sentence-transformers | LangChain wrapper |\n| Batching | Built-in batch_size parameter | Via encode_kwargs |\n| Return type | numpy array | Python list |\n| Use case | Direct control, production | LangChain integration |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class ProductionReranker:\n    \"\"\"\n    Production-ready reranking pipeline with optimizations.\n    \n    Uses sentence-transformers directly for better performance:\n    - SentenceTransformer: For fast bi-encoder embeddings with batching\n    - CrossEncoder: For accurate reranking with batch processing\n    \"\"\"\n    \n    def __init__(\n        self,\n        documents: List[Document],\n        bi_encoder_model: str = \"BAAI/bge-large-en-v1.5\",\n        reranker_model: str = \"BAAI/bge-reranker-large\",\n        device: str = \"cuda\"\n    ):\n        self.documents = documents\n        self.device = device if torch.cuda.is_available() else \"cpu\"\n        \n        # Load models using sentence-transformers directly\n        # SentenceTransformer: Bi-encoder for fast initial retrieval\n        # - .encode(texts, batch_size, normalize_embeddings) ‚Üí numpy array\n        print(f\"   Loading bi-encoder...\")\n        self.bi_encoder = SentenceTransformer(bi_encoder_model, device=self.device)\n        \n        # CrossEncoder: For accurate pairwise relevance scoring\n        # - .predict(pairs, batch_size) ‚Üí numpy array of scores\n        print(f\"   Loading reranker...\")\n        self.reranker = CrossEncoder(reranker_model, device=self.device)\n        \n        # Pre-compute embeddings with batching for efficiency\n        # normalize_embeddings=True enables cosine similarity via dot product\n        print(f\"   Computing embeddings...\")\n        texts = [doc.page_content for doc in documents]\n        self.embeddings = self.bi_encoder.encode(\n            texts,\n            batch_size=32,              # Process 32 docs at a time\n            show_progress_bar=False,\n            normalize_embeddings=True    # Enable dot product = cosine similarity\n        )\n        \n        print(f\"   ‚úÖ Ready!\")\n    \n    def search(\n        self,\n        query: str,\n        k: int = 5,\n        first_stage_k: int = 50,\n        score_threshold: float = None\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Production search with optional score threshold.\n        \n        Args:\n            query: Search query\n            k: Final number of results to return\n            first_stage_k: Candidates to retrieve before reranking\n            score_threshold: Minimum reranker score to include\n        \"\"\"\n        # Stage 1: Bi-encoder retrieval (fast)\n        # encode single query, get embedding vector\n        query_emb = self.bi_encoder.encode(\n            query,\n            normalize_embeddings=True\n        )\n        \n        # Dot product with all document embeddings (= cosine sim for normalized vectors)\n        similarities = np.dot(self.embeddings, query_emb)\n        top_indices = np.argsort(similarities)[-first_stage_k:][::-1]\n        \n        # Stage 2: Reranking with CrossEncoder (accurate)\n        # predict() takes list of [query, doc] pairs\n        pairs = [[query, self.documents[i].page_content] for i in top_indices]\n        rerank_scores = self.reranker.predict(\n            pairs,\n            batch_size=32,\n            show_progress_bar=False\n        )\n        \n        # Build results sorted by reranker score\n        results = []\n        sorted_results = sorted(\n            zip(top_indices, rerank_scores),\n            key=lambda x: -x[1]\n        )\n        \n        for idx, score in sorted_results[:k]:\n            # Optional score threshold filtering\n            if score_threshold and score < score_threshold:\n                continue\n                \n            results.append({\n                \"content\": self.documents[idx].page_content,\n                \"metadata\": self.documents[idx].metadata,\n                \"score\": float(score),\n                \"bi_encoder_score\": float(similarities[idx])\n            })\n        \n        return results\n\n\n# Build production reranker\nprint(\"üîÑ Building production reranker...\")\nprod_reranker = ProductionReranker(chunks)\nprint(\"‚úÖ Production reranker ready!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test production reranker\n",
    "query = \"How do I fine-tune a large language model efficiently?\"\n",
    "\n",
    "print(f\"üîç Query: '{query}'\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Time the query\n",
    "start = time.time()\n",
    "results = prod_reranker.search(query, k=5, first_stage_k=50)\n",
    "latency = (time.time() - start) * 1000\n",
    "\n",
    "print(f\"‚è±Ô∏è Total latency: {latency:.1f}ms\\n\")\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"üîπ Result {i+1}:\")\n",
    "    print(f\"   Source: {result['metadata']['source']}\")\n",
    "    print(f\"   Score: {result['score']:.4f} (bi-encoder: {result['bi_encoder_score']:.4f})\")\n",
    "    print(f\"   Content: {result['content'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: First-Stage K Too Small\n",
    "```python\n",
    "# ‚ùå Wrong: Only get 5 candidates, then rerank\n",
    "results = retriever.search(query, k=5, first_stage_k=5)\n",
    "\n",
    "# ‚úÖ Right: Get more candidates for reranking\n",
    "results = retriever.search(query, k=5, first_stage_k=50)\n",
    "```\n",
    "**Why:** If the best document isn't in the top 5 from the bi-encoder, reranking can't find it!\n",
    "\n",
    "### Mistake 2: Not Batching Reranker Calls\n",
    "```python\n",
    "# ‚ùå Wrong: One by one (slow!)\n",
    "for pair in pairs:\n",
    "    score = reranker.predict([pair])[0]\n",
    "\n",
    "# ‚úÖ Right: Batch processing\n",
    "scores = reranker.predict(pairs, batch_size=32)\n",
    "```\n",
    "\n",
    "### Mistake 3: Using Reranker for Initial Retrieval\n",
    "```python\n",
    "# ‚ùå Wrong: Rerank all documents (impossible for large corpora)\n",
    "pairs = [[query, doc] for doc in all_documents]  # 1M documents?\n",
    "scores = reranker.predict(pairs)  # Hours of compute!\n",
    "\n",
    "# ‚úÖ Right: Bi-encoder first, then rerank candidates\n",
    "candidates = bi_encoder.search(query, k=100)  # Fast!\n",
    "reranked = reranker.predict([[query, c] for c in candidates])  # Reasonable\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Different Reranker Models\n",
    "Try `BAAI/bge-reranker-base` instead of `-large`. Compare quality and speed.\n",
    "\n",
    "### Exercise 2: Score Threshold\n",
    "Add a minimum score threshold to filter low-confidence results.\n",
    "\n",
    "### Exercise 3: Combine with Hybrid Search\n",
    "Use hybrid search (Lab 3.5.4) as the first stage instead of pure bi-encoder.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint for Exercise 3</summary>\n",
    "\n",
    "```python\n",
    "class HybridTwoStageRetriever:\n",
    "    def __init__(self, hybrid_retriever, cross_encoder):\n",
    "        self.hybrid = hybrid_retriever\n",
    "        self.reranker = cross_encoder\n",
    "    \n",
    "    def search(self, query, k=5, first_stage_k=50):\n",
    "        # Stage 1: Hybrid search\n",
    "        candidates = self.hybrid.search(query, k=first_stage_k)\n",
    "        \n",
    "        # Stage 2: Rerank\n",
    "        pairs = [[query, c[0].page_content] for c in candidates]\n",
    "        scores = self.reranker.predict(pairs)\n",
    "        \n",
    "        # Return top-k by rerank score\n",
    "        ...\n",
    "```\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ The difference between bi-encoders and cross-encoders\n",
    "- ‚úÖ How to implement two-stage retrieval with reranking\n",
    "- ‚úÖ How to benchmark quality improvement vs latency\n",
    "- ‚úÖ How to find the optimal first-stage K\n",
    "\n",
    "**Key Insight:** Reranking can significantly improve retrieval quality with manageable latency cost, especially on GPU-equipped systems like DGX Spark!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del bi_encoder, cross_encoder, two_stage, prod_reranker\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next lab, we'll learn how to **evaluate RAG systems** with RAGAS!\n",
    "\n",
    "‚û°Ô∏è Continue to [Lab 3.5.6: RAGAS Evaluation](./lab-3.5.6-evaluation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}