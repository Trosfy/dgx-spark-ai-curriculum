{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.5.7: Production RAG System\n",
    "\n",
    "**Module:** 3.5 - RAG Systems & Vector Databases  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Expert)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "- [ ] Build a production-ready RAG pipeline with all best practices\n",
    "- [ ] Implement error handling, retries, and graceful degradation\n",
    "- [ ] Add caching for improved performance\n",
    "- [ ] Implement logging and monitoring\n",
    "- [ ] Handle edge cases (empty results, long queries)\n",
    "- [ ] Benchmark throughput and latency\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: All previous labs in Module 3.5\n",
    "- Understanding of: RAG, chunking, hybrid search, reranking, evaluation\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The Situation:** Your RAG prototype impressed stakeholders. Now they want it in production serving real users. Demo code won't cut it - you need reliability, monitoring, and performance.\n",
    "\n",
    "**Production Requirements:**\n",
    "- 99.9% uptime\n",
    "- < 3s response time P95\n",
    "- Graceful handling of failures\n",
    "- Observable metrics and logging\n",
    "- Scalable to 100+ concurrent users\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßí ELI5: Production vs Demo\n",
    "\n",
    "> **Demo Code**: Like cooking dinner for yourself - if something goes wrong, you just start over.\n",
    ">\n",
    "> **Production Code**: Like running a restaurant - you need:\n",
    "> - Backup ingredients (error handling)\n",
    "> - Health inspections (monitoring)\n",
    "> - Multiple chefs (scalability)\n",
    "> - Recipe records (logging)\n",
    "> - Pre-prepped ingredients (caching)\n",
    "> - \"We're out of fish\" plan (graceful degradation)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q \\\n",
    "    langchain langchain-community langchain-huggingface \\\n",
    "    chromadb sentence-transformers \\\n",
    "    rank_bm25 \\\n",
    "    ollama \\\n",
    "    cachetools \\\n",
    "    tenacity \\\n",
    "    structlog\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from datetime import datetime\n",
    "from functools import wraps\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError\n",
    "import threading\n",
    "\n",
    "import numpy as np\n",
    "from cachetools import TTLCache, LRUCache\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "import structlog\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "from rank_bm25 import BM25Okapi\n",
    "import ollama\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Production RAG Architecture\n",
    "\n",
    "### System Design\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                        PRODUCTION RAG SYSTEM                            ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ\n",
    "‚îÇ  ‚îÇ Request ‚îÇ ‚îÄ‚îÄ‚ñ∫ ‚îÇ Validate ‚îÇ ‚îÄ‚îÄ‚ñ∫ ‚îÇ  Cache  ‚îÇ ‚îÄ‚îÄ‚ñ∫ ‚îÇ Retrieve ‚îÇ        ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ  Check  ‚îÇ     ‚îÇ + Rerank ‚îÇ        ‚îÇ\n",
    "‚îÇ                                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ\n",
    "‚îÇ                                        ‚îÇ               ‚îÇ               ‚îÇ\n",
    "‚îÇ                                   [Cache Hit]    [Cache Miss]          ‚îÇ\n",
    "‚îÇ                                        ‚ñº               ‚ñº               ‚îÇ\n",
    "‚îÇ                                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ\n",
    "‚îÇ                                   ‚îÇ Return  ‚îÇ ‚óÑ‚îÄ‚îÄ ‚îÇ Generate ‚îÇ        ‚îÇ\n",
    "‚îÇ                                   ‚îÇ Cached  ‚îÇ     ‚îÇ + Cache  ‚îÇ        ‚îÇ\n",
    "‚îÇ                                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ\n",
    "‚îÇ                                                                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
    "‚îÇ  ‚îÇ                    MONITORING & LOGGING                         ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚Ä¢ Request metrics  ‚Ä¢ Latency tracking  ‚Ä¢ Error rates           ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Core Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure structured logging\n",
    "structlog.configure(\n",
    "    processors=[\n",
    "        structlog.stdlib.filter_by_level,\n",
    "        structlog.stdlib.add_logger_name,\n",
    "        structlog.stdlib.add_log_level,\n",
    "        structlog.processors.TimeStamper(fmt=\"iso\"),\n",
    "        structlog.processors.JSONRenderer()\n",
    "    ],\n",
    "    wrapper_class=structlog.stdlib.BoundLogger,\n",
    "    context_class=dict,\n",
    "    logger_factory=structlog.stdlib.LoggerFactory(),\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = structlog.get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RAGConfig:\n",
    "    \"\"\"Configuration for Production RAG System.\"\"\"\n",
    "    # Model settings\n",
    "    embedding_model: str = \"BAAI/bge-large-en-v1.5\"\n",
    "    reranker_model: str = \"BAAI/bge-reranker-large\"\n",
    "    llm_model: str = \"llama3.1:8b\"\n",
    "    \n",
    "    # Retrieval settings\n",
    "    chunk_size: int = 512\n",
    "    chunk_overlap: int = 50\n",
    "    first_stage_k: int = 50\n",
    "    final_k: int = 5\n",
    "    use_reranking: bool = True\n",
    "    use_hybrid_search: bool = True\n",
    "    hybrid_alpha: float = 0.5\n",
    "    \n",
    "    # Cache settings\n",
    "    cache_ttl_seconds: int = 3600  # 1 hour\n",
    "    cache_max_size: int = 1000\n",
    "    \n",
    "    # Timeouts\n",
    "    retrieval_timeout_s: float = 5.0\n",
    "    generation_timeout_s: float = 30.0\n",
    "    \n",
    "    # Quality thresholds\n",
    "    min_similarity_score: float = 0.3\n",
    "    max_query_length: int = 1000\n",
    "    \n",
    "    # Retry settings\n",
    "    max_retries: int = 3\n",
    "    retry_delay_s: float = 1.0\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RAGResponse:\n",
    "    \"\"\"Structured response from RAG system.\"\"\"\n",
    "    query: str\n",
    "    answer: str\n",
    "    sources: List[Dict[str, Any]]\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    @property\n",
    "    def success(self) -> bool:\n",
    "        return bool(self.answer and not self.answer.startswith(\"Error\"))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RAGMetrics:\n",
    "    \"\"\"Metrics for monitoring.\"\"\"\n",
    "    total_requests: int = 0\n",
    "    successful_requests: int = 0\n",
    "    failed_requests: int = 0\n",
    "    cache_hits: int = 0\n",
    "    cache_misses: int = 0\n",
    "    total_latency_ms: float = 0\n",
    "    \n",
    "    @property\n",
    "    def success_rate(self) -> float:\n",
    "        if self.total_requests == 0:\n",
    "            return 0.0\n",
    "        return self.successful_requests / self.total_requests\n",
    "    \n",
    "    @property\n",
    "    def cache_hit_rate(self) -> float:\n",
    "        total = self.cache_hits + self.cache_misses\n",
    "        if total == 0:\n",
    "            return 0.0\n",
    "        return self.cache_hits / total\n",
    "    \n",
    "    @property\n",
    "    def avg_latency_ms(self) -> float:\n",
    "        if self.total_requests == 0:\n",
    "            return 0.0\n",
    "        return self.total_latency_ms / self.total_requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Production RAG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionRAG:\n",
    "    \"\"\"\n",
    "    Production-ready RAG system with:\n",
    "    - Hybrid search (dense + sparse)\n",
    "    - Cross-encoder reranking\n",
    "    - Caching\n",
    "    - Error handling with retries\n",
    "    - Logging and monitoring\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig = None):\n",
    "        self.config = config or RAGConfig()\n",
    "        self.logger = structlog.get_logger().bind(component=\"ProductionRAG\")\n",
    "        self.metrics = RAGMetrics()\n",
    "        self._lock = threading.Lock()\n",
    "        \n",
    "        # Caches\n",
    "        self.query_cache = TTLCache(\n",
    "            maxsize=self.config.cache_max_size,\n",
    "            ttl=self.config.cache_ttl_seconds\n",
    "        )\n",
    "        self.embedding_cache = LRUCache(maxsize=10000)\n",
    "        \n",
    "        # Models (loaded lazily)\n",
    "        self._embedding_model = None\n",
    "        self._reranker = None\n",
    "        self._vectorstore = None\n",
    "        self._bm25 = None\n",
    "        self._documents = None\n",
    "        \n",
    "        self.logger.info(\"ProductionRAG initialized\", config=asdict(self.config))\n",
    "    \n",
    "    def load_documents(self, documents: List[Document]):\n",
    "        \"\"\"\n",
    "        Load and index documents.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Loading documents\", count=len(documents))\n",
    "        start = time.time()\n",
    "        \n",
    "        # Chunk documents\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.config.chunk_size,\n",
    "            chunk_overlap=self.config.chunk_overlap\n",
    "        )\n",
    "        self._documents = splitter.split_documents(documents)\n",
    "        \n",
    "        # Load embedding model\n",
    "        self._embedding_model = HuggingFaceEmbeddings(\n",
    "            model_name=self.config.embedding_model,\n",
    "            model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "            encode_kwargs={\"normalize_embeddings\": True, \"batch_size\": 32}\n",
    "        )\n",
    "        \n",
    "        # Build vector store\n",
    "        import shutil\n",
    "        db_path = \"./production_chroma_db\"\n",
    "        if Path(db_path).exists():\n",
    "            shutil.rmtree(db_path)\n",
    "        \n",
    "        self._vectorstore = Chroma.from_documents(\n",
    "            documents=self._documents,\n",
    "            embedding=self._embedding_model,\n",
    "            persist_directory=db_path\n",
    "        )\n",
    "        \n",
    "        # Build BM25 index for hybrid search\n",
    "        if self.config.use_hybrid_search:\n",
    "            tokenized = [doc.page_content.lower().split() for doc in self._documents]\n",
    "            self._bm25 = BM25Okapi(tokenized)\n",
    "        \n",
    "        # Load reranker\n",
    "        if self.config.use_reranking:\n",
    "            self._reranker = CrossEncoder(\n",
    "                self.config.reranker_model,\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            )\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        self.logger.info(\n",
    "            \"Documents loaded\",\n",
    "            chunks=len(self._documents),\n",
    "            elapsed_s=elapsed\n",
    "        )\n",
    "    \n",
    "    def _get_cache_key(self, query: str) -> str:\n",
    "        \"\"\"Generate cache key for query.\"\"\"\n",
    "        return hashlib.md5(query.lower().strip().encode()).hexdigest()\n",
    "    \n",
    "    def _validate_query(self, query: str) -> Tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Validate query before processing.\n",
    "        Returns (is_valid, error_message).\n",
    "        \"\"\"\n",
    "        if not query or not query.strip():\n",
    "            return False, \"Query cannot be empty\"\n",
    "        \n",
    "        if len(query) > self.config.max_query_length:\n",
    "            return False, f\"Query too long (max {self.config.max_query_length} chars)\"\n",
    "        \n",
    "        return True, \"\"\n",
    "    \n",
    "    def _dense_retrieve(self, query: str, k: int) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"Dense retrieval using embeddings.\"\"\"\n",
    "        results = self._vectorstore.similarity_search_with_score(query, k=k)\n",
    "        return [(doc, 1 - score) for doc, score in results]  # Convert distance to similarity\n",
    "    \n",
    "    def _sparse_retrieve(self, query: str, k: int) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"Sparse retrieval using BM25.\"\"\"\n",
    "        tokenized_query = query.lower().split()\n",
    "        scores = self._bm25.get_scores(tokenized_query)\n",
    "        top_indices = np.argsort(scores)[-k:][::-1]\n",
    "        return [(self._documents[i], scores[i]) for i in top_indices if scores[i] > 0]\n",
    "    \n",
    "    def _hybrid_retrieve(self, query: str, k: int) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"Hybrid retrieval combining dense and sparse.\"\"\"\n",
    "        dense_results = self._dense_retrieve(query, k)\n",
    "        sparse_results = self._sparse_retrieve(query, k)\n",
    "        \n",
    "        # RRF fusion\n",
    "        rrf_k = 60\n",
    "        doc_scores = {}\n",
    "        \n",
    "        for rank, (doc, _) in enumerate(dense_results):\n",
    "            doc_id = id(doc)\n",
    "            doc_scores[doc_id] = doc_scores.get(doc_id, {\"doc\": doc, \"score\": 0})\n",
    "            doc_scores[doc_id][\"score\"] += self.config.hybrid_alpha / (rrf_k + rank + 1)\n",
    "        \n",
    "        for rank, (doc, _) in enumerate(sparse_results):\n",
    "            doc_id = id(doc)\n",
    "            doc_scores[doc_id] = doc_scores.get(doc_id, {\"doc\": doc, \"score\": 0})\n",
    "            doc_scores[doc_id][\"score\"] += (1 - self.config.hybrid_alpha) / (rrf_k + rank + 1)\n",
    "        \n",
    "        sorted_results = sorted(doc_scores.values(), key=lambda x: -x[\"score\"])\n",
    "        return [(r[\"doc\"], r[\"score\"]) for r in sorted_results[:k]]\n",
    "    \n",
    "    def _rerank(self, query: str, candidates: List[Tuple[Document, float]]) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"Rerank candidates using cross-encoder.\"\"\"\n",
    "        if not candidates:\n",
    "            return []\n",
    "        \n",
    "        pairs = [[query, doc.page_content] for doc, _ in candidates]\n",
    "        scores = self._reranker.predict(pairs, batch_size=32, show_progress_bar=False)\n",
    "        \n",
    "        reranked = sorted(\n",
    "            zip([doc for doc, _ in candidates], scores),\n",
    "            key=lambda x: -x[1]\n",
    "        )\n",
    "        \n",
    "        return reranked\n",
    "    \n",
    "    @retry(\n",
    "        stop=stop_after_attempt(3),\n",
    "        wait=wait_exponential(multiplier=1, min=1, max=10),\n",
    "        retry=retry_if_exception_type(Exception)\n",
    "    )\n",
    "    def _generate_answer(self, query: str, contexts: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer using LLM with retry logic.\n",
    "        \"\"\"\n",
    "        if not contexts:\n",
    "            return \"I couldn't find relevant information to answer your question.\"\n",
    "        \n",
    "        context_str = \"\\n\\n---\\n\\n\".join(contexts[:self.config.final_k])\n",
    "        \n",
    "        prompt = f\"\"\"You are a helpful AI assistant. Answer the question based ONLY on the provided context.\n",
    "If the context doesn't contain enough information, acknowledge this.\n",
    "Be concise and accurate.\n",
    "\n",
    "CONTEXT:\n",
    "{context_str}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "        \n",
    "        response = ollama.chat(\n",
    "            model=self.config.llm_model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            options={\"temperature\": 0.1}\n",
    "        )\n",
    "        \n",
    "        return response[\"message\"][\"content\"]\n",
    "    \n",
    "    def query(self, query: str) -> RAGResponse:\n",
    "        \"\"\"\n",
    "        Main query method with full production handling.\n",
    "        \"\"\"\n",
    "        request_id = hashlib.md5(f\"{query}{time.time()}\".encode()).hexdigest()[:8]\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.logger.info(\"Query received\", request_id=request_id, query=query[:100])\n",
    "        \n",
    "        with self._lock:\n",
    "            self.metrics.total_requests += 1\n",
    "        \n",
    "        # Validate query\n",
    "        is_valid, error_msg = self._validate_query(query)\n",
    "        if not is_valid:\n",
    "            self.logger.warning(\"Invalid query\", request_id=request_id, error=error_msg)\n",
    "            with self._lock:\n",
    "                self.metrics.failed_requests += 1\n",
    "            return RAGResponse(\n",
    "                query=query,\n",
    "                answer=f\"Error: {error_msg}\",\n",
    "                sources=[],\n",
    "                metadata={\"request_id\": request_id, \"error\": error_msg}\n",
    "            )\n",
    "        \n",
    "        # Check cache\n",
    "        cache_key = self._get_cache_key(query)\n",
    "        if cache_key in self.query_cache:\n",
    "            cached = self.query_cache[cache_key]\n",
    "            with self._lock:\n",
    "                self.metrics.cache_hits += 1\n",
    "            self.logger.info(\"Cache hit\", request_id=request_id)\n",
    "            cached.metadata[\"cached\"] = True\n",
    "            return cached\n",
    "        \n",
    "        with self._lock:\n",
    "            self.metrics.cache_misses += 1\n",
    "        \n",
    "        try:\n",
    "            # Retrieve\n",
    "            if self.config.use_hybrid_search:\n",
    "                candidates = self._hybrid_retrieve(query, self.config.first_stage_k)\n",
    "            else:\n",
    "                candidates = self._dense_retrieve(query, self.config.first_stage_k)\n",
    "            \n",
    "            # Filter low-quality results\n",
    "            candidates = [(doc, score) for doc, score in candidates \n",
    "                         if score >= self.config.min_similarity_score]\n",
    "            \n",
    "            # Rerank\n",
    "            if self.config.use_reranking and candidates:\n",
    "                candidates = self._rerank(query, candidates)\n",
    "            \n",
    "            # Extract contexts and sources\n",
    "            contexts = [doc.page_content for doc, _ in candidates[:self.config.final_k]]\n",
    "            sources = [\n",
    "                {\n",
    "                    \"source\": doc.metadata.get(\"source\", \"unknown\"),\n",
    "                    \"score\": float(score),\n",
    "                    \"preview\": doc.page_content[:100] + \"...\"\n",
    "                }\n",
    "                for doc, score in candidates[:self.config.final_k]\n",
    "            ]\n",
    "            \n",
    "            # Generate answer\n",
    "            answer = self._generate_answer(query, contexts)\n",
    "            \n",
    "            # Build response\n",
    "            elapsed_ms = (time.time() - start_time) * 1000\n",
    "            response = RAGResponse(\n",
    "                query=query,\n",
    "                answer=answer,\n",
    "                sources=sources,\n",
    "                metadata={\n",
    "                    \"request_id\": request_id,\n",
    "                    \"latency_ms\": elapsed_ms,\n",
    "                    \"cached\": False,\n",
    "                    \"candidates_count\": len(candidates)\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Cache response\n",
    "            self.query_cache[cache_key] = response\n",
    "            \n",
    "            with self._lock:\n",
    "                self.metrics.successful_requests += 1\n",
    "                self.metrics.total_latency_ms += elapsed_ms\n",
    "            \n",
    "            self.logger.info(\n",
    "                \"Query completed\",\n",
    "                request_id=request_id,\n",
    "                latency_ms=elapsed_ms,\n",
    "                sources_count=len(sources)\n",
    "            )\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(\n",
    "                \"Query failed\",\n",
    "                request_id=request_id,\n",
    "                error=str(e)\n",
    "            )\n",
    "            with self._lock:\n",
    "                self.metrics.failed_requests += 1\n",
    "            \n",
    "            return RAGResponse(\n",
    "                query=query,\n",
    "                answer=f\"Error: An unexpected error occurred. Please try again.\",\n",
    "                sources=[],\n",
    "                metadata={\"request_id\": request_id, \"error\": str(e)}\n",
    "            )\n",
    "    \n",
    "    def get_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get current metrics.\"\"\"\n",
    "        return {\n",
    "            \"total_requests\": self.metrics.total_requests,\n",
    "            \"successful_requests\": self.metrics.successful_requests,\n",
    "            \"failed_requests\": self.metrics.failed_requests,\n",
    "            \"success_rate\": self.metrics.success_rate,\n",
    "            \"cache_hits\": self.metrics.cache_hits,\n",
    "            \"cache_hit_rate\": self.metrics.cache_hit_rate,\n",
    "            \"avg_latency_ms\": self.metrics.avg_latency_ms,\n",
    "            \"cache_size\": len(self.query_cache)\n",
    "        }\n",
    "    \n",
    "    def health_check(self) -> Dict[str, Any]:\n",
    "        \"\"\"Health check endpoint.\"\"\"\n",
    "        checks = {\n",
    "            \"vectorstore\": self._vectorstore is not None,\n",
    "            \"embedding_model\": self._embedding_model is not None,\n",
    "            \"reranker\": self._reranker is not None or not self.config.use_reranking,\n",
    "            \"documents_loaded\": self._documents is not None and len(self._documents) > 0,\n",
    "        }\n",
    "        \n",
    "        # Test LLM connection\n",
    "        try:\n",
    "            ollama.chat(\n",
    "                model=self.config.llm_model,\n",
    "                messages=[{\"role\": \"user\", \"content\": \"test\"}],\n",
    "                options={\"num_predict\": 1}\n",
    "            )\n",
    "            checks[\"llm\"] = True\n",
    "        except:\n",
    "            checks[\"llm\"] = False\n",
    "        \n",
    "        return {\n",
    "            \"healthy\": all(checks.values()),\n",
    "            \"checks\": checks,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Building and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "DOCS_PATH = Path(\"../data/sample_documents\")\n",
    "\n",
    "documents = []\n",
    "for file_path in sorted(DOCS_PATH.glob(\"*.md\")):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    documents.append(Document(\n",
    "        page_content=content,\n",
    "        metadata={\"source\": file_path.name}\n",
    "    ))\n",
    "\n",
    "print(f\"üìö Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Production RAG\n",
    "print(\"üîÑ Initializing Production RAG System...\")\n",
    "\n",
    "config = RAGConfig(\n",
    "    use_hybrid_search=True,\n",
    "    use_reranking=True,\n",
    "    first_stage_k=50,\n",
    "    final_k=5,\n",
    "    cache_ttl_seconds=3600\n",
    ")\n",
    "\n",
    "rag = ProductionRAG(config)\n",
    "rag.load_documents(documents)\n",
    "\n",
    "print(\"\\n‚úÖ Production RAG System ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Health check\n",
    "health = rag.health_check()\n",
    "\n",
    "print(\"\\nüè• Health Check:\")\n",
    "print(f\"   Overall: {'‚úÖ Healthy' if health['healthy'] else '‚ùå Unhealthy'}\")\n",
    "for check, status in health['checks'].items():\n",
    "    print(f\"   {check}: {'‚úÖ' if status else '‚ùå'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is the memory capacity of DGX Spark?\",\n",
    "    \"How does LoRA work?\",\n",
    "    \"What are the benefits of hybrid search?\",\n",
    "]\n",
    "\n",
    "print(\"\\nüß™ Testing queries...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n‚ùì Query: {query}\")\n",
    "    response = rag.query(query)\n",
    "    \n",
    "    print(f\"\\nüí¨ Answer: {response.answer[:300]}...\")\n",
    "    print(f\"\\nüìö Sources: {[s['source'] for s in response.sources]}\")\n",
    "    print(f\"‚è±Ô∏è Latency: {response.metadata.get('latency_ms', 0):.1f}ms\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cache\n",
    "print(\"\\nüîÑ Testing cache...\")\n",
    "\n",
    "# First query (cache miss)\n",
    "response1 = rag.query(\"What is GPTQ?\")\n",
    "print(f\"First query: cached={response1.metadata.get('cached', False)}, latency={response1.metadata.get('latency_ms', 0):.1f}ms\")\n",
    "\n",
    "# Second query (cache hit)\n",
    "response2 = rag.query(\"What is GPTQ?\")\n",
    "print(f\"Second query: cached={response2.metadata.get('cached', False)}\")\n",
    "\n",
    "metrics = rag.get_metrics()\n",
    "print(f\"\\nCache hit rate: {metrics['cache_hit_rate']:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Throughput Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark throughput\n",
    "benchmark_queries = [\n",
    "    \"What is DGX Spark?\",\n",
    "    \"Explain attention mechanism\",\n",
    "    \"How does LoRA reduce memory?\",\n",
    "    \"What is GPTQ quantization?\",\n",
    "    \"Benefits of RAG?\",\n",
    "    \"Compare ChromaDB and FAISS\",\n",
    "    \"What are Tensor Cores?\",\n",
    "    \"Explain positional encoding\",\n",
    "    \"What is QLoRA?\",\n",
    "    \"How does hybrid search work?\",\n",
    "]\n",
    "\n",
    "print(\"\\n‚ö° Throughput Benchmark\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sequential benchmark\n",
    "start = time.time()\n",
    "latencies = []\n",
    "\n",
    "for query in benchmark_queries:\n",
    "    q_start = time.time()\n",
    "    response = rag.query(query)\n",
    "    latencies.append((time.time() - q_start) * 1000)\n",
    "\n",
    "total_time = time.time() - start\n",
    "\n",
    "print(f\"\\nüìä Sequential Results ({len(benchmark_queries)} queries):\")\n",
    "print(f\"   Total time: {total_time:.2f}s\")\n",
    "print(f\"   Throughput: {len(benchmark_queries) / total_time:.2f} queries/sec\")\n",
    "print(f\"   Avg latency: {np.mean(latencies):.0f}ms\")\n",
    "print(f\"   P50 latency: {np.percentile(latencies, 50):.0f}ms\")\n",
    "print(f\"   P95 latency: {np.percentile(latencies, 95):.0f}ms\")\n",
    "print(f\"   P99 latency: {np.percentile(latencies, 99):.0f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get final metrics\n",
    "final_metrics = rag.get_metrics()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä FINAL METRICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìà Request Statistics:\")\n",
    "print(f\"   Total requests: {final_metrics['total_requests']}\")\n",
    "print(f\"   Successful: {final_metrics['successful_requests']}\")\n",
    "print(f\"   Failed: {final_metrics['failed_requests']}\")\n",
    "print(f\"   Success rate: {final_metrics['success_rate']:.1%}\")\n",
    "\n",
    "print(f\"\\nüíæ Cache Statistics:\")\n",
    "print(f\"   Cache hits: {final_metrics['cache_hits']}\")\n",
    "print(f\"   Cache hit rate: {final_metrics['cache_hit_rate']:.1%}\")\n",
    "print(f\"   Cache size: {final_metrics['cache_size']} entries\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è Latency Statistics:\")\n",
    "print(f\"   Average latency: {final_metrics['avg_latency_ms']:.0f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Edge Case Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test edge cases\n",
    "print(\"\\nüß™ Edge Case Testing\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "edge_cases = [\n",
    "    (\"\", \"Empty query\"),\n",
    "    (\"   \", \"Whitespace only\"),\n",
    "    (\"a\" * 2000, \"Query too long\"),\n",
    "    (\"What is the recipe for chocolate cake?\", \"Out of domain\"),\n",
    "    (\"LPDDR5X\", \"Single word query\"),\n",
    "]\n",
    "\n",
    "for query, description in edge_cases:\n",
    "    print(f\"\\nüîπ Testing: {description}\")\n",
    "    response = rag.query(query)\n",
    "    \n",
    "    if response.success:\n",
    "        print(f\"   ‚úÖ Success: {response.answer[:100]}...\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è Handled: {response.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: No Error Handling\n",
    "```python\n",
    "# ‚ùå Wrong: Unhandled exceptions crash the service\n",
    "def query(self, query):\n",
    "    results = self.vectorstore.search(query)  # Can throw!\n",
    "    return self.llm.generate(results)\n",
    "\n",
    "# ‚úÖ Right: Catch and handle gracefully\n",
    "def query(self, query):\n",
    "    try:\n",
    "        results = self.vectorstore.search(query)\n",
    "        return self.llm.generate(results)\n",
    "    except Exception as e:\n",
    "        logger.error(\"Query failed\", error=str(e))\n",
    "        return RAGResponse(error=\"An error occurred\")\n",
    "```\n",
    "\n",
    "### Mistake 2: No Caching\n",
    "```python\n",
    "# ‚ùå Wrong: Compute everything every time\n",
    "def query(self, query):\n",
    "    return expensive_computation(query)\n",
    "\n",
    "# ‚úÖ Right: Cache results\n",
    "def query(self, query):\n",
    "    cache_key = hash(query)\n",
    "    if cache_key in self.cache:\n",
    "        return self.cache[cache_key]\n",
    "    result = expensive_computation(query)\n",
    "    self.cache[cache_key] = result\n",
    "    return result\n",
    "```\n",
    "\n",
    "### Mistake 3: No Monitoring\n",
    "```python\n",
    "# ‚ùå Wrong: No visibility into system health\n",
    "def query(self, query):\n",
    "    return process(query)\n",
    "\n",
    "# ‚úÖ Right: Track metrics\n",
    "def query(self, query):\n",
    "    start = time.time()\n",
    "    result = process(query)\n",
    "    self.metrics.record(latency=time.time() - start)\n",
    "    return result\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Checkpoint\n",
    "\n",
    "You've built a production-ready RAG system with:\n",
    "- ‚úÖ Hybrid search (dense + sparse)\n",
    "- ‚úÖ Cross-encoder reranking\n",
    "- ‚úÖ Query caching with TTL\n",
    "- ‚úÖ Error handling with retries\n",
    "- ‚úÖ Structured logging\n",
    "- ‚úÖ Metrics and monitoring\n",
    "- ‚úÖ Health check endpoint\n",
    "- ‚úÖ Edge case handling\n",
    "\n",
    "**Congratulations on completing Module 3.5!** üéä\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "import shutil\n",
    "\n",
    "del rag\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if Path(\"./production_chroma_db\").exists():\n",
    "    shutil.rmtree(\"./production_chroma_db\")\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Congratulations on completing Module 3.5: RAG Systems!\n",
    "\n",
    "You've mastered:\n",
    "1. Building RAG pipelines from scratch\n",
    "2. Chunking strategies and their trade-offs\n",
    "3. Vector databases (ChromaDB, FAISS, Qdrant)\n",
    "4. Hybrid search (dense + sparse)\n",
    "5. Reranking with cross-encoders\n",
    "6. Evaluation with RAGAS metrics\n",
    "7. Production-ready implementation\n",
    "\n",
    "‚û°Ô∏è Continue to [Module 3.6: AI Agents & Agentic Systems](../../module-3.6-ai-agents/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
