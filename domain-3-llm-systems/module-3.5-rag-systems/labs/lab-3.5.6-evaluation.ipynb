{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.5.6: RAGAS Evaluation Framework\n",
    "\n",
    "**Module:** 3.5 - RAG Systems & Vector Databases  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê (Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "- [ ] Understand the key metrics for evaluating RAG systems\n",
    "- [ ] Create evaluation datasets with ground truth\n",
    "- [ ] Use RAGAS to measure faithfulness, relevancy, and precision\n",
    "- [ ] Build custom evaluation metrics\n",
    "- [ ] Set quality thresholds for production deployment\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Labs 3.5.1-3.5.5\n",
    "- A working RAG pipeline to evaluate\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The Problem:** Your RAG system is deployed, but how do you know it's actually working well? Users complain sometimes, but you can't manually check every response.\n",
    "\n",
    "**The Solution:** Systematic evaluation with metrics like RAGAS. Just like unit tests for code, these metrics catch regressions before users do.\n",
    "\n",
    "**Industry Standard:** Companies like Arize, LangSmith, and Weights & Biases all build evaluation tooling around these concepts.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßí ELI5: RAG Evaluation Metrics\n",
    "\n",
    "> **Imagine grading a student's open-book exam:**\n",
    ">\n",
    "> **Faithfulness**: Did they only use information from the book? (No making stuff up!)\n",
    ">\n",
    "> **Answer Relevancy**: Did they actually answer the question? (Not just copy random paragraphs)\n",
    ">\n",
    "> **Context Precision**: Did they find the RIGHT pages to look at? (Found relevant sections)\n",
    ">\n",
    "> **Context Recall**: Did they find ALL the relevant pages? (Didn't miss important info)\n",
    ">\n",
    "> A good student (and a good RAG) scores high on all four!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q \\\n",
    "    ragas==0.1.21 \\\n",
    "    langchain langchain-community langchain-huggingface \\\n",
    "    chromadb sentence-transformers \\\n",
    "    datasets \\\n",
    "    ollama\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "import numpy as np\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "import ollama\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents and build RAG pipeline\n",
    "DOCS_PATH = Path(\"../data/sample_documents\")\n",
    "\n",
    "documents = []\n",
    "for file_path in sorted(DOCS_PATH.glob(\"*.md\")):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    documents.append(Document(\n",
    "        page_content=content,\n",
    "        metadata={\"source\": file_path.name}\n",
    "    ))\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(f\"üìö Loaded {len(documents)} documents ‚Üí {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the RAG pipeline\n",
    "print(\"üîÑ Building RAG pipeline...\")\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "import shutil\n",
    "CHROMA_PATH = \"./eval_chroma_db\"\n",
    "if Path(CHROMA_PATH).exists():\n",
    "    shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=CHROMA_PATH\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG pipeline ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG query function\n",
    "LLM_MODEL = \"qwen3:8b\"\n",
    "\n",
    "def rag_query(question: str, k: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Execute RAG query and return structured result for evaluation.\n",
    "    \"\"\"\n",
    "    # Retrieve\n",
    "    results = vectorstore.similarity_search(question, k=k)\n",
    "    contexts = [doc.page_content for doc in results]\n",
    "    \n",
    "    # Generate\n",
    "    context_str = \"\\n\\n---\\n\\n\".join(contexts)\n",
    "    prompt = f\"\"\"Answer the question based ONLY on the provided context.\n",
    "If the context doesn't contain the answer, say \"I don't have enough information.\"\n",
    "\n",
    "Context:\n",
    "{context_str}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=LLM_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"contexts\": contexts,\n",
    "        \"answer\": response[\"message\"][\"content\"]\n",
    "    }\n",
    "\n",
    "# Test\n",
    "test_result = rag_query(\"What is DGX Spark's memory capacity?\")\n",
    "print(f\"‚úÖ RAG query working!\")\n",
    "print(f\"   Answer: {test_result['answer'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 2: Understanding RAGAS Metrics\n\n### The Four Key Metrics\n\n| Metric | What It Measures | Input Required |\n|--------|-----------------|----------------|\n| **Faithfulness** | Is the answer grounded in context? | Question, Context, Answer |\n| **Answer Relevancy** | Does the answer address the question? | Question, Answer |\n| **Context Precision** | Are retrieved docs relevant? | Question, Context, Ground Truth |\n| **Context Recall** | Are all needed docs retrieved? | Context, Ground Truth |\n\n### Python Dataclasses\n\nWe'll use Python's `@dataclass` decorator to create structured data containers:\n\n```python\nfrom dataclasses import dataclass, asdict\n\n@dataclass\nclass Example:\n    name: str\n    value: int = 0  # Default value\n    \n# Usage:\nex = Example(name=\"test\", value=42)\nprint(ex.name)  # \"test\"\nprint(asdict(ex))  # {\"name\": \"test\", \"value\": 42}\n```\n\nKey functions:\n- `@dataclass`: Decorator that auto-generates `__init__`, `__repr__`, etc.\n- `asdict(obj)`: Converts a dataclass instance to a dictionary"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Since RAGAS requires an LLM for evaluation, we'll implement custom metrics\n# that can work with local LLMs via Ollama\n\nfrom dataclasses import dataclass, asdict\n\n@dataclass\nclass EvaluationSample:\n    \"\"\"\n    A single sample for RAG evaluation.\n    \n    @dataclass automatically generates:\n    - __init__(question, ground_truth, contexts=None, answer=None)\n    - __repr__ for pretty printing\n    - __eq__ for comparison\n    \"\"\"\n    question: str\n    ground_truth: str\n    contexts: List[str] = None\n    answer: str = None\n\n    \n@dataclass \nclass EvaluationResult:\n    \"\"\"\n    Evaluation results for a single sample.\n    \n    Properties can be defined in dataclasses using @property decorator.\n    \"\"\"\n    question: str\n    faithfulness: float\n    answer_relevancy: float\n    context_precision: float\n    context_recall: float\n    \n    @property\n    def average(self) -> float:\n        \"\"\"Compute average of all metrics.\"\"\"\n        return (self.faithfulness + self.answer_relevancy + \n                self.context_precision + self.context_recall) / 4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Creating an Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation dataset with ground truth answers\n",
    "evaluation_dataset = [\n",
    "    EvaluationSample(\n",
    "        question=\"What is the memory capacity of DGX Spark?\",\n",
    "        ground_truth=\"DGX Spark has 128GB of unified LPDDR5X memory that is shared between CPU and GPU.\"\n",
    "    ),\n",
    "    EvaluationSample(\n",
    "        question=\"How many CUDA cores does DGX Spark have?\",\n",
    "        ground_truth=\"DGX Spark has 6,144 CUDA cores.\"\n",
    "    ),\n",
    "    EvaluationSample(\n",
    "        question=\"What is the attention mechanism in transformers?\",\n",
    "        ground_truth=\"The attention mechanism allows each position in a sequence to attend to all other positions, computing Query, Key, and Value vectors to capture relationships.\"\n",
    "    ),\n",
    "    EvaluationSample(\n",
    "        question=\"How does LoRA reduce memory requirements?\",\n",
    "        ground_truth=\"LoRA freezes pretrained weights and injects trainable low-rank decomposition matrices, training only 0.1-1% of parameters.\"\n",
    "    ),\n",
    "    EvaluationSample(\n",
    "        question=\"What is QLoRA?\",\n",
    "        ground_truth=\"QLoRA combines LoRA with 4-bit quantization (NF4), keeping the base model quantized while training LoRA adapters in FP16/BF16.\"\n",
    "    ),\n",
    "    EvaluationSample(\n",
    "        question=\"What is GPTQ quantization?\",\n",
    "        ground_truth=\"GPTQ is a one-shot weight quantization method using approximate second-order information to find optimal quantized values.\"\n",
    "    ),\n",
    "    EvaluationSample(\n",
    "        question=\"What are the advantages of RAG over fine-tuning?\",\n",
    "        ground_truth=\"RAG provides dynamic knowledge updates, grounded responses with source citations, scalable knowledge without retraining, and domain expertise.\"\n",
    "    ),\n",
    "    EvaluationSample(\n",
    "        question=\"What is hybrid search in RAG?\",\n",
    "        ground_truth=\"Hybrid search combines dense retrieval (embeddings) with sparse retrieval (BM25/keywords) using fusion methods like RRF.\"\n",
    "    ),\n",
    "    EvaluationSample(\n",
    "        question=\"How does ChromaDB compare to FAISS?\",\n",
    "        ground_truth=\"ChromaDB is Python-native and easy to use, while FAISS offers GPU acceleration and better performance at scale but lacks built-in filtering.\"\n",
    "    ),\n",
    "    EvaluationSample(\n",
    "        question=\"What is positional encoding in transformers?\",\n",
    "        ground_truth=\"Positional encoding adds position information to tokens since transformers process positions in parallel, using sinusoidal functions or learned embeddings.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"üìã Created {len(evaluation_dataset)} evaluation samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RAG on all evaluation samples\n",
    "print(\"üîÑ Running RAG on evaluation samples...\")\n",
    "\n",
    "for i, sample in enumerate(evaluation_dataset):\n",
    "    result = rag_query(sample.question)\n",
    "    sample.contexts = result[\"contexts\"]\n",
    "    sample.answer = result[\"answer\"]\n",
    "    print(f\"   [{i+1}/{len(evaluation_dataset)}] {sample.question[:40]}...\")\n",
    "\n",
    "print(\"‚úÖ All samples processed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Implementing Custom Evaluation Metrics\n",
    "\n",
    "We'll implement simplified versions of RAGAS metrics that work with local LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEvaluator:\n",
    "    \"\"\"\n",
    "    Custom RAG evaluator using local LLM.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_model: str = \"qwen3:8b\"):\n",
    "        self.llm_model = llm_model\n",
    "        \n",
    "    def _llm_judge(self, prompt: str) -> str:\n",
    "        \"\"\"Get LLM judgment.\"\"\"\n",
    "        response = ollama.chat(\n",
    "            model=self.llm_model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response[\"message\"][\"content\"].strip()\n",
    "    \n",
    "    def evaluate_faithfulness(self, sample: EvaluationSample) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate if the answer is grounded in the provided context.\n",
    "        Returns score from 0 to 1.\n",
    "        \"\"\"\n",
    "        context_str = \"\\n\".join(sample.contexts[:3])  # Use top 3 contexts\n",
    "        \n",
    "        prompt = f\"\"\"You are an expert evaluator. Determine if the answer is faithfully grounded in the provided context.\n",
    "\n",
    "CONTEXT:\n",
    "{context_str}\n",
    "\n",
    "ANSWER:\n",
    "{sample.answer}\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "- Score 1.0: All claims in the answer are supported by the context\n",
    "- Score 0.5: Some claims are supported, some are not\n",
    "- Score 0.0: The answer contains claims not found in context (hallucination)\n",
    "\n",
    "Respond with ONLY a number: 0.0, 0.5, or 1.0\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self._llm_judge(prompt)\n",
    "            # Extract number from response\n",
    "            for val in [\"1.0\", \"0.5\", \"0.0\", \"1\", \"0\"]:\n",
    "                if val in response:\n",
    "                    return float(val)\n",
    "            return 0.5  # Default if parsing fails\n",
    "        except:\n",
    "            return 0.5\n",
    "    \n",
    "    def evaluate_answer_relevancy(self, sample: EvaluationSample) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate if the answer addresses the question.\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"You are an expert evaluator. Determine if the answer directly addresses the question.\n",
    "\n",
    "QUESTION:\n",
    "{sample.question}\n",
    "\n",
    "ANSWER:\n",
    "{sample.answer}\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "- Score 1.0: The answer directly and completely addresses the question\n",
    "- Score 0.5: The answer partially addresses the question or is incomplete\n",
    "- Score 0.0: The answer does not address the question at all\n",
    "\n",
    "Respond with ONLY a number: 0.0, 0.5, or 1.0\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self._llm_judge(prompt)\n",
    "            for val in [\"1.0\", \"0.5\", \"0.0\", \"1\", \"0\"]:\n",
    "                if val in response:\n",
    "                    return float(val)\n",
    "            return 0.5\n",
    "        except:\n",
    "            return 0.5\n",
    "    \n",
    "    def evaluate_context_precision(self, sample: EvaluationSample) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate if retrieved contexts are relevant to the question.\n",
    "        \"\"\"\n",
    "        relevant_count = 0\n",
    "        \n",
    "        for context in sample.contexts[:5]:  # Check top 5\n",
    "            prompt = f\"\"\"Is this context relevant to answering the question?\n",
    "\n",
    "QUESTION: {sample.question}\n",
    "\n",
    "CONTEXT: {context[:500]}\n",
    "\n",
    "Respond with ONLY: YES or NO\"\"\"\n",
    "            \n",
    "            try:\n",
    "                response = self._llm_judge(prompt).upper()\n",
    "                if \"YES\" in response:\n",
    "                    relevant_count += 1\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return relevant_count / min(5, len(sample.contexts))\n",
    "    \n",
    "    def evaluate_context_recall(self, sample: EvaluationSample) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate if the key information from ground truth is in the contexts.\n",
    "        \"\"\"\n",
    "        context_str = \"\\n\".join(sample.contexts)\n",
    "        \n",
    "        prompt = f\"\"\"Does the provided context contain the information needed to produce this ground truth answer?\n",
    "\n",
    "GROUND TRUTH ANSWER:\n",
    "{sample.ground_truth}\n",
    "\n",
    "RETRIEVED CONTEXT:\n",
    "{context_str[:2000]}\n",
    "\n",
    "EVALUATION:\n",
    "- Score 1.0: All key information from ground truth is present in context\n",
    "- Score 0.5: Some key information is present\n",
    "- Score 0.0: Little to no relevant information in context\n",
    "\n",
    "Respond with ONLY a number: 0.0, 0.5, or 1.0\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self._llm_judge(prompt)\n",
    "            for val in [\"1.0\", \"0.5\", \"0.0\", \"1\", \"0\"]:\n",
    "                if val in response:\n",
    "                    return float(val)\n",
    "            return 0.5\n",
    "        except:\n",
    "            return 0.5\n",
    "    \n",
    "    def evaluate(self, sample: EvaluationSample) -> EvaluationResult:\n",
    "        \"\"\"\n",
    "        Run all evaluations on a sample.\n",
    "        \"\"\"\n",
    "        return EvaluationResult(\n",
    "            question=sample.question,\n",
    "            faithfulness=self.evaluate_faithfulness(sample),\n",
    "            answer_relevancy=self.evaluate_answer_relevancy(sample),\n",
    "            context_precision=self.evaluate_context_precision(sample),\n",
    "            context_recall=self.evaluate_context_recall(sample)\n",
    "        )\n",
    "\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = RAGEvaluator(llm_model=LLM_MODEL)\n",
    "print(\"‚úÖ Evaluator ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Running Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on all samples\n",
    "print(\"üî¨ Running evaluation...\")\n",
    "print(\"   (This may take a few minutes)\\n\")\n",
    "\n",
    "results = []\n",
    "for i, sample in enumerate(evaluation_dataset):\n",
    "    print(f\"   [{i+1}/{len(evaluation_dataset)}] Evaluating: {sample.question[:40]}...\")\n",
    "    result = evaluator.evaluate(sample)\n",
    "    results.append(result)\n",
    "    print(f\"       Faithfulness: {result.faithfulness:.2f}, Relevancy: {result.answer_relevancy:.2f}, \"\n",
    "          f\"Precision: {result.context_precision:.2f}, Recall: {result.context_recall:.2f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results\n",
    "def calculate_aggregate_metrics(results: List[EvaluationResult]) -> Dict[str, float]:\n",
    "    \"\"\"Calculate aggregate metrics across all samples.\"\"\"\n",
    "    faithfulness = np.mean([r.faithfulness for r in results])\n",
    "    answer_relevancy = np.mean([r.answer_relevancy for r in results])\n",
    "    context_precision = np.mean([r.context_precision for r in results])\n",
    "    context_recall = np.mean([r.context_recall for r in results])\n",
    "    \n",
    "    return {\n",
    "        \"faithfulness\": faithfulness,\n",
    "        \"answer_relevancy\": answer_relevancy,\n",
    "        \"context_precision\": context_precision,\n",
    "        \"context_recall\": context_recall,\n",
    "        \"average\": (faithfulness + answer_relevancy + context_precision + context_recall) / 4\n",
    "    }\n",
    "\n",
    "metrics = calculate_aggregate_metrics(results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Metric':<25} {'Score':<10} {'Status'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for metric, score in metrics.items():\n",
    "    if metric == \"average\":\n",
    "        continue\n",
    "    status = \"‚úÖ Good\" if score >= 0.7 else (\"‚ö†Ô∏è Needs Work\" if score >= 0.5 else \"‚ùå Poor\")\n",
    "    bar = \"‚ñà\" * int(score * 20) + \"‚ñë\" * (20 - int(score * 20))\n",
    "    print(f\"{metric:<25} {bar} {score:.2f} {status}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'OVERALL AVERAGE':<25} {metrics['average']:.2f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find problematic samples\n",
    "print(\"\\nüîç Detailed Sample Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, (sample, result) in enumerate(zip(evaluation_dataset, results)):\n",
    "    avg = result.average\n",
    "    status = \"‚úÖ\" if avg >= 0.75 else (\"‚ö†Ô∏è\" if avg >= 0.5 else \"‚ùå\")\n",
    "    \n",
    "    print(f\"\\n{status} Sample {i+1}: {sample.question[:50]}...\")\n",
    "    print(f\"   Faithfulness: {result.faithfulness:.2f} | \"\n",
    "          f\"Relevancy: {result.answer_relevancy:.2f} | \"\n",
    "          f\"Precision: {result.context_precision:.2f} | \"\n",
    "          f\"Recall: {result.context_recall:.2f}\")\n",
    "    print(f\"   Average: {avg:.2f}\")\n",
    "    \n",
    "    # Show answer preview for low scores\n",
    "    if avg < 0.5:\n",
    "        print(f\"   Answer: {sample.answer[:100]}...\")\n",
    "        print(f\"   Ground Truth: {sample.ground_truth[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify failure patterns\n",
    "print(\"\\nüìà Failure Pattern Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "low_faithfulness = [r for r in results if r.faithfulness < 0.5]\n",
    "low_relevancy = [r for r in results if r.answer_relevancy < 0.5]\n",
    "low_precision = [r for r in results if r.context_precision < 0.5]\n",
    "low_recall = [r for r in results if r.context_recall < 0.5]\n",
    "\n",
    "print(f\"\\nüî∏ Low Faithfulness ({len(low_faithfulness)}/{len(results)}): Hallucination issues\")\n",
    "print(f\"üî∏ Low Answer Relevancy ({len(low_relevancy)}/{len(results)}): Not answering question\")\n",
    "print(f\"üî∏ Low Context Precision ({len(low_precision)}/{len(results)}): Retrieving irrelevant docs\")\n",
    "print(f\"üî∏ Low Context Recall ({len(low_recall)}/{len(results)}): Missing relevant docs\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\nüí° Recommendations:\")\n",
    "if len(low_faithfulness) > len(results) * 0.3:\n",
    "    print(\"   - Improve prompt to emphasize using only context\")\n",
    "    print(\"   - Consider adding 'if unsure, say so' instruction\")\n",
    "if len(low_precision) > len(results) * 0.3:\n",
    "    print(\"   - Try hybrid search (dense + sparse)\")\n",
    "    print(\"   - Add reranking stage\")\n",
    "if len(low_recall) > len(results) * 0.3:\n",
    "    print(\"   - Increase chunk overlap\")\n",
    "    print(\"   - Try smaller chunk sizes for more granular retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Setting Quality Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quality thresholds for production\n",
    "@dataclass\n",
    "class QualityThresholds:\n",
    "    \"\"\"Quality thresholds for production deployment.\"\"\"\n",
    "    min_faithfulness: float = 0.7\n",
    "    min_answer_relevancy: float = 0.7\n",
    "    min_context_precision: float = 0.6\n",
    "    min_context_recall: float = 0.6\n",
    "    min_average: float = 0.65\n",
    "\n",
    "\n",
    "def check_production_ready(\n",
    "    metrics: Dict[str, float],\n",
    "    thresholds: QualityThresholds = QualityThresholds()\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Check if the system meets production quality thresholds.\n",
    "    \"\"\"\n",
    "    checks = {\n",
    "        \"faithfulness\": metrics[\"faithfulness\"] >= thresholds.min_faithfulness,\n",
    "        \"answer_relevancy\": metrics[\"answer_relevancy\"] >= thresholds.min_answer_relevancy,\n",
    "        \"context_precision\": metrics[\"context_precision\"] >= thresholds.min_context_precision,\n",
    "        \"context_recall\": metrics[\"context_recall\"] >= thresholds.min_context_recall,\n",
    "        \"average\": metrics[\"average\"] >= thresholds.min_average,\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"passed\": all(checks.values()),\n",
    "        \"checks\": checks,\n",
    "        \"metrics\": metrics,\n",
    "        \"thresholds\": asdict(thresholds)\n",
    "    }\n",
    "\n",
    "\n",
    "# Check production readiness\n",
    "thresholds = QualityThresholds()\n",
    "prod_check = check_production_ready(metrics, thresholds)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üöÄ PRODUCTION READINESS CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for metric, passed in prod_check[\"checks\"].items():\n",
    "    icon = \"‚úÖ\" if passed else \"‚ùå\"\n",
    "    actual = metrics.get(metric, 0)\n",
    "    threshold = getattr(thresholds, f\"min_{metric}\", 0)\n",
    "    print(f\"   {icon} {metric}: {actual:.2f} (threshold: {threshold:.2f})\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "if prod_check[\"passed\"]:\n",
    "    print(\"üéâ SYSTEM IS PRODUCTION READY!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è SYSTEM NEEDS IMPROVEMENT BEFORE PRODUCTION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation report\n",
    "report = {\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"num_samples\": len(evaluation_dataset),\n",
    "    \"metrics\": metrics,\n",
    "    \"production_ready\": prod_check[\"passed\"],\n",
    "    \"thresholds\": asdict(thresholds),\n",
    "    \"sample_results\": [\n",
    "        {\n",
    "            \"question\": r.question,\n",
    "            \"faithfulness\": r.faithfulness,\n",
    "            \"answer_relevancy\": r.answer_relevancy,\n",
    "            \"context_precision\": r.context_precision,\n",
    "            \"context_recall\": r.context_recall,\n",
    "            \"average\": r.average\n",
    "        }\n",
    "        for r in results\n",
    "    ]\n",
    "}\n",
    "\n",
    "report_path = Path(\"./evaluation_report.json\")\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"\\nüìÑ Evaluation report saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Evaluating Without Ground Truth\n",
    "```python\n",
    "# ‚ùå Wrong: No ground truth, can't measure recall\n",
    "eval_samples = [{\"question\": \"...\"}]  # Missing ground_truth\n",
    "\n",
    "# ‚úÖ Right: Include ground truth for meaningful evaluation\n",
    "eval_samples = [\n",
    "    {\"question\": \"...\", \"ground_truth\": \"Expected answer...\"}\n",
    "]\n",
    "```\n",
    "\n",
    "### Mistake 2: Too Few Evaluation Samples\n",
    "```python\n",
    "# ‚ùå Wrong: 5 samples isn't statistically meaningful\n",
    "eval_dataset = samples[:5]\n",
    "\n",
    "# ‚úÖ Right: At least 50 samples for reliable metrics\n",
    "eval_dataset = samples[:50]\n",
    "```\n",
    "\n",
    "### Mistake 3: Not Testing Edge Cases\n",
    "```python\n",
    "# ‚ùå Wrong: Only testing happy-path questions\n",
    "questions = [\"What is X?\", \"How does Y work?\"]\n",
    "\n",
    "# ‚úÖ Right: Include edge cases\n",
    "questions = [\n",
    "    \"What is X?\",                          # Normal\n",
    "    \"What is the recipe for pizza?\",       # Out of domain\n",
    "    \"Compare X and Y and Z in detail\",     # Complex\n",
    "    \"X?\",                                   # Minimal query\n",
    "]\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Add More Samples\n",
    "Expand the evaluation dataset to 30+ samples covering all documents.\n",
    "\n",
    "### Exercise 2: Edge Case Testing\n",
    "Add samples for: out-of-domain questions, ambiguous questions, and multi-hop reasoning.\n",
    "\n",
    "### Exercise 3: A/B Comparison\n",
    "Compare your current RAG with one using different chunking or retrieval settings.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint for Exercise 3</summary>\n",
    "\n",
    "```python\n",
    "# Build two RAG variants\n",
    "rag_v1 = build_rag(chunk_size=256)\n",
    "rag_v2 = build_rag(chunk_size=1024)\n",
    "\n",
    "# Evaluate both\n",
    "results_v1 = evaluate_all(rag_v1, eval_dataset)\n",
    "results_v2 = evaluate_all(rag_v2, eval_dataset)\n",
    "\n",
    "# Compare\n",
    "print(f\"V1 Average: {calculate_average(results_v1):.2f}\")\n",
    "print(f\"V2 Average: {calculate_average(results_v2):.2f}\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ The four key RAGAS metrics: Faithfulness, Relevancy, Precision, Recall\n",
    "- ‚úÖ How to create evaluation datasets with ground truth\n",
    "- ‚úÖ How to implement custom evaluation metrics\n",
    "- ‚úÖ How to set and check production quality thresholds\n",
    "\n",
    "**Key Insight:** Systematic evaluation is what separates demo projects from production systems. Always measure before deploying!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del embedding_model, vectorstore\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if Path(CHROMA_PATH).exists():\n",
    "    shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the final lab, we'll build a **production-ready RAG system** with all the best practices!\n",
    "\n",
    "‚û°Ô∏è Continue to [Lab 3.5.7: Production RAG System](./lab-3.5.7-production-rag.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}