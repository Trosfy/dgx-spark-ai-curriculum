{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.5.1 Solutions: Building Your First RAG Pipeline\n",
    "\n",
    "Complete solutions to all exercises with explanations and alternative approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Load and Chunk Documents\n",
    "\n",
    "**Task**: Load all sample documents and create optimized chunks with metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_documents(data_dir: str = \"../data/sample_documents\") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load all markdown documents from the data directory.\n",
    "    \n",
    "    Key decisions:\n",
    "    - Using pathlib for cross-platform compatibility\n",
    "    - Extracting title from first heading\n",
    "    - Adding comprehensive metadata for filtering\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    for file_path in data_path.glob(\"*.md\"):\n",
    "        content = file_path.read_text(encoding='utf-8')\n",
    "        \n",
    "        # Extract title from first heading\n",
    "        lines = content.split('\\n')\n",
    "        title = file_path.stem\n",
    "        for line in lines:\n",
    "            if line.startswith('# '):\n",
    "                title = line[2:].strip()\n",
    "                break\n",
    "        \n",
    "        # Create document with rich metadata\n",
    "        doc = Document(\n",
    "            page_content=content,\n",
    "            metadata={\n",
    "                \"source\": str(file_path),\n",
    "                \"filename\": file_path.name,\n",
    "                \"title\": title,\n",
    "                \"file_type\": \"markdown\",\n",
    "                \"char_count\": len(content),\n",
    "                \"word_count\": len(content.split())\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Load documents\n",
    "documents = load_all_documents()\n",
    "print(f\"Loaded {len(documents)} documents:\")\n",
    "for doc in documents:\n",
    "    print(f\"  - {doc.metadata['title']} ({doc.metadata['word_count']} words)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_with_overlap(documents: List[Document], chunk_size: int = 500, overlap: int = 100) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Chunk documents with optimal settings for RAG.\n",
    "    \n",
    "    Key decisions:\n",
    "    - 500 chars captures ~1-2 paragraphs (semantic units)\n",
    "    - 100 char overlap preserves context at boundaries\n",
    "    - Markdown separators respect document structure\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        separators=[\"\\n## \", \"\\n### \", \"\\n\\n\", \"\\n\", \". \", \" \"],\n",
    "        length_function=len\n",
    "    )\n",
    "    \n",
    "    all_chunks = []\n",
    "    for doc in documents:\n",
    "        chunks = splitter.split_documents([doc])\n",
    "        \n",
    "        # Enrich chunk metadata\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk.metadata['chunk_index'] = i\n",
    "            chunk.metadata['total_chunks'] = len(chunks)\n",
    "            chunk.metadata['chunk_size'] = len(chunk.page_content)\n",
    "        \n",
    "        all_chunks.extend(chunks)\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "# Create chunks\n",
    "chunks = chunk_with_overlap(documents)\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "print(f\"Average chunk size: {np.mean([c.metadata['chunk_size'] for c in chunks]):.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Approach: Semantic Chunking\n",
    "\n",
    "For more advanced use cases, you can chunk based on semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_chunk(text: str, embedding_model, threshold: float = 0.7) -> List[str]:\n",
    "    \"\"\"\n",
    "    Alternative: Semantic-aware chunking based on embedding similarity.\n",
    "    \n",
    "    This approach groups sentences that are semantically related,\n",
    "    creating more coherent chunks.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Split into sentences\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    if len(sentences) < 2:\n",
    "        return [text]\n",
    "    \n",
    "    # Get embeddings for all sentences\n",
    "    embeddings = embedding_model.embed_documents(sentences)\n",
    "    embeddings = np.array(embeddings)\n",
    "    \n",
    "    # Group similar sentences\n",
    "    chunks = []\n",
    "    current_chunk = [sentences[0]]\n",
    "    current_emb = embeddings[0]\n",
    "    \n",
    "    for i in range(1, len(sentences)):\n",
    "        # Compute similarity with current chunk centroid\n",
    "        similarity = np.dot(current_emb, embeddings[i]) / (\n",
    "            np.linalg.norm(current_emb) * np.linalg.norm(embeddings[i])\n",
    "        )\n",
    "        \n",
    "        if similarity > threshold:\n",
    "            current_chunk.append(sentences[i])\n",
    "            # Update centroid\n",
    "            current_emb = np.mean([embeddings[j] for j in range(len(current_chunk))], axis=0)\n",
    "        else:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentences[i]]\n",
    "            current_emb = embeddings[i]\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "print(\"Semantic chunking function defined (use with embedding model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Create Vector Store with GPU Acceleration\n",
    "\n",
    "**Task**: Create an optimized vector store using DGX Spark's GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_vectorstore(\n",
    "    chunks: List[Document],\n",
    "    model_name: str = \"BAAI/bge-large-en-v1.5\",\n",
    "    persist_dir: str = \"./chroma_db\"\n",
    ") -> Chroma:\n",
    "    \"\"\"\n",
    "    Create GPU-accelerated vector store.\n",
    "    \n",
    "    Key optimizations:\n",
    "    - Uses CUDA for embedding computation\n",
    "    - L2 normalization for cosine similarity\n",
    "    - Batch processing for efficiency\n",
    "    \"\"\"\n",
    "    import shutil\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Clean previous database\n",
    "    if Path(persist_dir).exists():\n",
    "        shutil.rmtree(persist_dir)\n",
    "    \n",
    "    # Load embedding model with GPU\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs={\"device\": device},\n",
    "        encode_kwargs={\n",
    "            \"normalize_embeddings\": True,  # L2 normalize for cosine sim\n",
    "            \"batch_size\": 64  # Larger batches for GPU\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Create vector store\n",
    "    import time\n",
    "    start = time.time()\n",
    "    \n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=persist_dir,\n",
    "        collection_name=\"rag_documents\"\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    print(f\"Created vector store with {len(chunks)} chunks in {elapsed:.2f}s\")\n",
    "    print(f\"Throughput: {len(chunks)/elapsed:.1f} chunks/second\")\n",
    "    \n",
    "    return vectorstore, embedding_model\n",
    "\n",
    "# Create vector store\n",
    "vectorstore, embedding_model = create_optimized_vectorstore(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 Solution: Implement Complete RAG Pipeline\n",
    "\n",
    "**Task**: Build an end-to-end RAG system with proper generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompleteRAGPipeline:\n",
    "    \"\"\"\n",
    "    Production-ready RAG pipeline with all components.\n",
    "    \n",
    "    Features:\n",
    "    - Configurable retrieval (k, threshold)\n",
    "    - Source tracking for citations\n",
    "    - Fallback handling\n",
    "    - Context window management\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vectorstore: Chroma,\n",
    "        llm_model: str = \"llama3.1:8b\",\n",
    "        k: int = 5,\n",
    "        score_threshold: float = 0.3\n",
    "    ):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm_model = llm_model\n",
    "        self.k = k\n",
    "        self.score_threshold = score_threshold\n",
    "        self.context_window = 4096  # Tokens available for context\n",
    "    \n",
    "    def retrieve(self, query: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve relevant documents with scores.\"\"\"\n",
    "        results = self.vectorstore.similarity_search_with_score(query, k=self.k)\n",
    "        \n",
    "        retrieved = []\n",
    "        for doc, score in results:\n",
    "            # Convert distance to similarity (Chroma uses L2 distance)\n",
    "            similarity = 1 / (1 + score)\n",
    "            \n",
    "            if similarity >= self.score_threshold:\n",
    "                retrieved.append({\n",
    "                    \"content\": doc.page_content,\n",
    "                    \"metadata\": doc.metadata,\n",
    "                    \"score\": similarity\n",
    "                })\n",
    "        \n",
    "        return retrieved\n",
    "    \n",
    "    def build_context(self, retrieved: List[Dict]) -> str:\n",
    "        \"\"\"Build context string with source tracking.\"\"\"\n",
    "        context_parts = []\n",
    "        total_chars = 0\n",
    "        max_chars = self.context_window * 4  # Rough char estimate\n",
    "        \n",
    "        for i, doc in enumerate(retrieved):\n",
    "            source = doc['metadata'].get('title', doc['metadata'].get('filename', 'Unknown'))\n",
    "            content = f\"[Source {i+1}: {source}]\\n{doc['content']}\"\n",
    "            \n",
    "            if total_chars + len(content) > max_chars:\n",
    "                break\n",
    "            \n",
    "            context_parts.append(content)\n",
    "            total_chars += len(content)\n",
    "        \n",
    "        return \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    def generate(self, query: str, context: str) -> str:\n",
    "        \"\"\"Generate answer using LLM.\"\"\"\n",
    "        prompt = f\"\"\"You are a helpful AI assistant. Answer the question based ONLY on the provided context.\n",
    "If the context doesn't contain enough information, say \"I don't have enough information to answer this.\"\n",
    "Cite sources using [Source N] format.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            import ollama\n",
    "            response = ollama.chat(\n",
    "                model=self.llm_model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response[\"message\"][\"content\"]\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {e}\"\n",
    "    \n",
    "    def query(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Complete RAG query with full results.\"\"\"\n",
    "        # Retrieve\n",
    "        retrieved = self.retrieve(question)\n",
    "        \n",
    "        if not retrieved:\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"answer\": \"I couldn't find any relevant information in the knowledge base.\",\n",
    "                \"sources\": [],\n",
    "                \"num_sources\": 0\n",
    "            }\n",
    "        \n",
    "        # Build context\n",
    "        context = self.build_context(retrieved)\n",
    "        \n",
    "        # Generate\n",
    "        answer = self.generate(question, context)\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"sources\": [{\n",
    "                \"title\": r['metadata'].get('title', 'Unknown'),\n",
    "                \"score\": r['score'],\n",
    "                \"preview\": r['content'][:200] + \"...\"\n",
    "            } for r in retrieved],\n",
    "            \"num_sources\": len(retrieved)\n",
    "        }\n",
    "\n",
    "# Create RAG pipeline\n",
    "rag = CompleteRAGPipeline(vectorstore)\n",
    "print(\"RAG pipeline created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the complete pipeline\n",
    "test_questions = [\n",
    "    \"What is the memory capacity of DGX Spark?\",\n",
    "    \"How does LoRA reduce the number of trainable parameters?\",\n",
    "    \"What are the key components of a transformer?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Q: {question}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    result = rag.query(question)\n",
    "    print(f\"\\nAnswer: {result['answer'][:500]}...\")\n",
    "    print(f\"\\nSources used: {result['num_sources']}\")\n",
    "    for i, src in enumerate(result['sources'][:3]):\n",
    "        print(f\"  [{i+1}] {src['title']} (score: {src['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 Solution: Evaluate Retrieval Quality\n",
    "\n",
    "**Task**: Implement metrics to evaluate RAG performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(\n",
    "    vectorstore: Chroma,\n",
    "    test_queries: List[Dict[str, Any]]\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate retrieval quality with standard metrics.\n",
    "    \n",
    "    Metrics:\n",
    "    - Recall@k: Fraction of relevant docs retrieved\n",
    "    - MRR: Mean Reciprocal Rank (position of first relevant)\n",
    "    - Precision@k: Fraction of retrieved docs that are relevant\n",
    "    \"\"\"\n",
    "    recalls = []\n",
    "    mrrs = []\n",
    "    precisions = []\n",
    "    \n",
    "    for query_data in test_queries:\n",
    "        query = query_data[\"query\"]\n",
    "        expected_sources = set(query_data.get(\"expected_sources\", []))\n",
    "        k = query_data.get(\"k\", 5)\n",
    "        \n",
    "        # Retrieve\n",
    "        results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "        retrieved_sources = [doc.metadata.get(\"filename\", \"\") for doc, _ in results]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if expected_sources:\n",
    "            # Recall@k\n",
    "            found = set(retrieved_sources) & expected_sources\n",
    "            recall = len(found) / len(expected_sources)\n",
    "            recalls.append(recall)\n",
    "            \n",
    "            # Precision@k\n",
    "            precision = len(found) / len(retrieved_sources) if retrieved_sources else 0\n",
    "            precisions.append(precision)\n",
    "            \n",
    "            # MRR\n",
    "            mrr = 0\n",
    "            for i, source in enumerate(retrieved_sources):\n",
    "                if source in expected_sources:\n",
    "                    mrr = 1 / (i + 1)\n",
    "                    break\n",
    "            mrrs.append(mrr)\n",
    "    \n",
    "    return {\n",
    "        \"recall@k\": np.mean(recalls) if recalls else 0,\n",
    "        \"precision@k\": np.mean(precisions) if precisions else 0,\n",
    "        \"mrr\": np.mean(mrrs) if mrrs else 0\n",
    "    }\n",
    "\n",
    "# Test evaluation\n",
    "test_queries = [\n",
    "    {\n",
    "        \"query\": \"DGX Spark memory specifications\",\n",
    "        \"expected_sources\": [\"dgx_spark_technical_guide.md\"],\n",
    "        \"k\": 5\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"LoRA fine-tuning technique\",\n",
    "        \"expected_sources\": [\"lora_finetuning_guide.md\"],\n",
    "        \"k\": 5\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"transformer attention mechanism\",\n",
    "        \"expected_sources\": [\"transformer_architecture_explained.md\"],\n",
    "        \"k\": 5\n",
    "    }\n",
    "]\n",
    "\n",
    "metrics = evaluate_retrieval(vectorstore, test_queries)\n",
    "print(\"Retrieval Evaluation Results:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"  {metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison: CPU vs GPU\n",
    "\n",
    "Benchmark showing the benefit of GPU acceleration on DGX Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_embedding(texts: List[str], device: str) -> Dict[str, float]:\n",
    "    \"\"\"Benchmark embedding performance on specified device.\"\"\"\n",
    "    model = HuggingFaceEmbeddings(\n",
    "        model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "        model_kwargs={\"device\": device},\n",
    "        encode_kwargs={\"normalize_embeddings\": True, \"batch_size\": 32}\n",
    "    )\n",
    "    \n",
    "    # Warmup\n",
    "    _ = model.embed_documents(texts[:10])\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.time()\n",
    "    embeddings = model.embed_documents(texts)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    return {\n",
    "        \"device\": device,\n",
    "        \"num_texts\": len(texts),\n",
    "        \"total_time\": elapsed,\n",
    "        \"throughput\": len(texts) / elapsed\n",
    "    }\n",
    "\n",
    "# Prepare benchmark texts\n",
    "benchmark_texts = [chunk.page_content for chunk in chunks]\n",
    "\n",
    "print(f\"Benchmarking with {len(benchmark_texts)} text chunks...\")\n",
    "\n",
    "# GPU benchmark\n",
    "if torch.cuda.is_available():\n",
    "    gpu_results = benchmark_embedding(benchmark_texts, \"cuda\")\n",
    "    print(f\"\\nGPU Results:\")\n",
    "    print(f\"  Time: {gpu_results['total_time']:.2f}s\")\n",
    "    print(f\"  Throughput: {gpu_results['throughput']:.1f} texts/sec\")\n",
    "\n",
    "# CPU benchmark (on subset for speed)\n",
    "cpu_texts = benchmark_texts[:50]\n",
    "cpu_results = benchmark_embedding(cpu_texts, \"cpu\")\n",
    "print(f\"\\nCPU Results (subset of {len(cpu_texts)} texts):\")\n",
    "print(f\"  Time: {cpu_results['total_time']:.2f}s\")\n",
    "print(f\"  Throughput: {cpu_results['throughput']:.1f} texts/sec\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    speedup = gpu_results['throughput'] / cpu_results['throughput']\n",
    "    print(f\"\\nGPU Speedup: {speedup:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Document Loading**: Always add comprehensive metadata for filtering and debugging\n",
    "\n",
    "2. **Chunking**: 500 chars with 100 overlap works well for general documents. Use semantic chunking for better coherence.\n",
    "\n",
    "3. **GPU Acceleration**: DGX Spark's GPU provides 5-10x speedup for embedding computation\n",
    "\n",
    "4. **Evaluation**: Always measure Recall@k and MRR to track retrieval quality\n",
    "\n",
    "5. **Production**: Include source tracking, score thresholds, and proper error handling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
