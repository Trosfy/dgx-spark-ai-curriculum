{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.5.5 Solutions: Cross-Encoder Reranking\n",
    "\n",
    "Complete solutions for two-stage retrieval with cross-encoder reranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "def load_and_chunk():\n",
    "    documents = []\n",
    "    for file_path in Path(\"../data/sample_documents\").glob(\"*.md\"):\n",
    "        content = file_path.read_text(encoding='utf-8')\n",
    "        documents.append(Document(\n",
    "            page_content=content,\n",
    "            metadata={\"source\": file_path.name}\n",
    "        ))\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    return splitter.split_documents(documents)\n",
    "\n",
    "chunks = load_and_chunk()\n",
    "print(f\"Loaded {len(chunks)} chunks\")\n",
    "\n",
    "# Load embedding model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    model_kwargs={\"device\": device},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Understand Bi-Encoder vs Cross-Encoder\n",
    "\n",
    "**Task**: Implement and compare both architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiEncoderScorer:\n",
    "    \"\"\"\n",
    "    Bi-encoder: Encodes query and document separately.\n",
    "    \n",
    "    Architecture:\n",
    "        Query -> Encoder -> query_emb\n",
    "        Document -> Encoder -> doc_emb\n",
    "        Score = cosine_similarity(query_emb, doc_emb)\n",
    "    \n",
    "    Pros: Fast (can pre-compute doc embeddings)\n",
    "    Cons: Limited interaction between query and document\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: HuggingFaceEmbeddings):\n",
    "        self.embedding_model = embedding_model\n",
    "    \n",
    "    def score(self, query: str, document: str) -> float:\n",
    "        \"\"\"Score a query-document pair.\"\"\"\n",
    "        query_emb = np.array(self.embedding_model.embed_query(query))\n",
    "        doc_emb = np.array(self.embedding_model.embed_documents([document])[0])\n",
    "        \n",
    "        # Cosine similarity\n",
    "        return float(np.dot(query_emb, doc_emb) / (\n",
    "            np.linalg.norm(query_emb) * np.linalg.norm(doc_emb) + 1e-8\n",
    "        ))\n",
    "    \n",
    "    def score_batch(self, query: str, documents: List[str]) -> List[float]:\n",
    "        \"\"\"Score multiple documents (efficient with batch encoding).\"\"\"\n",
    "        query_emb = np.array(self.embedding_model.embed_query(query))\n",
    "        doc_embs = np.array(self.embedding_model.embed_documents(documents))\n",
    "        \n",
    "        # Batch cosine similarity\n",
    "        similarities = np.dot(doc_embs, query_emb) / (\n",
    "            np.linalg.norm(doc_embs, axis=1) * np.linalg.norm(query_emb) + 1e-8\n",
    "        )\n",
    "        return similarities.tolist()\n",
    "\n",
    "\n",
    "class CrossEncoderScorer:\n",
    "    \"\"\"\n",
    "    Cross-encoder: Encodes query and document together.\n",
    "    \n",
    "    Architecture:\n",
    "        [CLS] query [SEP] document [SEP] -> Encoder -> Score\n",
    "    \n",
    "    Pros: Full attention between query and document, higher accuracy\n",
    "    Cons: Slow (can't pre-compute, O(n) for n documents)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"BAAI/bge-reranker-large\",\n",
    "        device: str = None\n",
    "    ):\n",
    "        from sentence_transformers import CrossEncoder\n",
    "        \n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        self.model = CrossEncoder(model_name, device=device)\n",
    "        self.device = device\n",
    "        print(f\"Cross-encoder loaded on {device}\")\n",
    "    \n",
    "    def score(self, query: str, document: str) -> float:\n",
    "        \"\"\"Score a single query-document pair.\"\"\"\n",
    "        return float(self.model.predict([[query, document]])[0])\n",
    "    \n",
    "    def score_batch(self, query: str, documents: List[str]) -> List[float]:\n",
    "        \"\"\"Score multiple documents.\"\"\"\n",
    "        pairs = [[query, doc] for doc in documents]\n",
    "        scores = self.model.predict(pairs, batch_size=32, show_progress_bar=False)\n",
    "        return scores.tolist()\n",
    "\n",
    "# Create scorers\n",
    "bi_encoder = BiEncoderScorer(embedding_model)\n",
    "cross_encoder = CrossEncoderScorer()\n",
    "\n",
    "print(\"\\nBoth scorers created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare scoring behavior\n",
    "test_query = \"What is the memory capacity of DGX Spark?\"\n",
    "\n",
    "test_documents = [\n",
    "    \"DGX Spark features 128GB of unified memory shared between CPU and GPU.\",\n",
    "    \"The memory system uses LPDDR5X technology for high bandwidth.\",\n",
    "    \"GPU memory is important for training large models.\",\n",
    "    \"The weather today is sunny and warm.\"\n",
    "]\n",
    "\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "print(\"Scoring comparison:\")\n",
    "print(f\"{'Document':<60} {'Bi-Encoder':<12} {'Cross-Encoder':<12}\")\n",
    "print(\"-\"*84)\n",
    "\n",
    "bi_scores = bi_encoder.score_batch(test_query, test_documents)\n",
    "cross_scores = cross_encoder.score_batch(test_query, test_documents)\n",
    "\n",
    "for doc, bi_score, cross_score in zip(test_documents, bi_scores, cross_scores):\n",
    "    doc_short = doc[:55] + \"...\" if len(doc) > 55 else doc\n",
    "    print(f\"{doc_short:<60} {bi_score:<12.4f} {cross_score:<12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Implement Two-Stage Retrieval\n",
    "\n",
    "**Task**: Build complete two-stage pipeline with reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoStageRetriever:\n",
    "    \"\"\"\n",
    "    Two-stage retrieval: Bi-encoder for recall, Cross-encoder for precision.\n",
    "    \n",
    "    Stage 1 (Bi-encoder):\n",
    "    - Fast approximate search\n",
    "    - High recall, moderate precision\n",
    "    - Returns top-N candidates (typically 50-100)\n",
    "    \n",
    "    Stage 2 (Cross-encoder):\n",
    "    - Accurate reranking\n",
    "    - High precision\n",
    "    - Reranks to top-K (typically 5-10)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        documents: List[Document],\n",
    "        embedding_model: HuggingFaceEmbeddings,\n",
    "        reranker_model: str = \"BAAI/bge-reranker-large\"\n",
    "    ):\n",
    "        self.documents = documents\n",
    "        self.embedding_model = embedding_model\n",
    "        \n",
    "        # Pre-compute document embeddings for Stage 1\n",
    "        print(\"Pre-computing document embeddings...\")\n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        self.embeddings = np.array(embedding_model.embed_documents(texts))\n",
    "        print(f\"Embeddings shape: {self.embeddings.shape}\")\n",
    "        \n",
    "        # Initialize cross-encoder for Stage 2\n",
    "        print(\"Loading cross-encoder...\")\n",
    "        from sentence_transformers import CrossEncoder\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.reranker = CrossEncoder(reranker_model, device=device)\n",
    "        print(\"Two-stage retriever ready!\")\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        k: int = 5,\n",
    "        first_stage_k: int = 50\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Two-stage search.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            k: Final number of results\n",
    "            first_stage_k: Candidates from Stage 1\n",
    "        \n",
    "        Returns:\n",
    "            List of result dicts with scores and timing\n",
    "        \"\"\"\n",
    "        # Stage 1: Bi-encoder retrieval\n",
    "        stage1_start = time.time()\n",
    "        query_emb = np.array(self.embedding_model.embed_query(query))\n",
    "        similarities = np.dot(self.embeddings, query_emb)\n",
    "        top_indices = np.argsort(similarities)[-first_stage_k:][::-1]\n",
    "        stage1_time = (time.time() - stage1_start) * 1000\n",
    "        \n",
    "        # Get candidates\n",
    "        candidates = [(self.documents[i], similarities[i], i) for i in top_indices]\n",
    "        \n",
    "        # Stage 2: Cross-encoder reranking\n",
    "        stage2_start = time.time()\n",
    "        pairs = [[query, doc.page_content] for doc, _, _ in candidates]\n",
    "        rerank_scores = self.reranker.predict(pairs, batch_size=32, show_progress_bar=False)\n",
    "        stage2_time = (time.time() - stage2_start) * 1000\n",
    "        \n",
    "        # Combine and sort\n",
    "        reranked = sorted(\n",
    "            zip(candidates, rerank_scores),\n",
    "            key=lambda x: -x[1]  # Sort by rerank score descending\n",
    "        )[:k]\n",
    "        \n",
    "        # Build results\n",
    "        results = []\n",
    "        for i, ((doc, bi_score, orig_idx), rerank_score) in enumerate(reranked):\n",
    "            # Find original bi-encoder rank\n",
    "            bi_rank = list(top_indices).index(orig_idx) + 1\n",
    "            \n",
    "            results.append({\n",
    "                \"document\": doc,\n",
    "                \"content\": doc.page_content,\n",
    "                \"metadata\": doc.metadata,\n",
    "                \"final_rank\": i + 1,\n",
    "                \"rerank_score\": float(rerank_score),\n",
    "                \"bi_encoder_score\": float(bi_score),\n",
    "                \"bi_encoder_rank\": bi_rank,\n",
    "                \"rank_change\": bi_rank - (i + 1),\n",
    "                \"stage1_time_ms\": stage1_time,\n",
    "                \"stage2_time_ms\": stage2_time\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def search_without_reranking(self, query: str, k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Single-stage search (bi-encoder only) for comparison.\"\"\"\n",
    "        start = time.time()\n",
    "        query_emb = np.array(self.embedding_model.embed_query(query))\n",
    "        similarities = np.dot(self.embeddings, query_emb)\n",
    "        top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "        elapsed = (time.time() - start) * 1000\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                \"document\": self.documents[i],\n",
    "                \"content\": self.documents[i].page_content,\n",
    "                \"metadata\": self.documents[i].metadata,\n",
    "                \"rank\": rank + 1,\n",
    "                \"score\": float(similarities[i]),\n",
    "                \"time_ms\": elapsed\n",
    "            }\n",
    "            for rank, i in enumerate(top_indices)\n",
    "        ]\n",
    "\n",
    "# Create two-stage retriever\n",
    "retriever = TwoStageRetriever(chunks, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 Solution: Analyze Reranking Impact\n",
    "\n",
    "**Task**: Measure how reranking changes rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_reranking_impact(\n",
    "    retriever: TwoStageRetriever,\n",
    "    test_queries: List[str],\n",
    "    k: int = 5,\n",
    "    first_stage_k: int = 50\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze how reranking changes document rankings.\n",
    "    \n",
    "    Metrics:\n",
    "    - Average rank change\n",
    "    - Percentage of docs that moved up\n",
    "    - Top-1 stability\n",
    "    \"\"\"\n",
    "    \n",
    "    all_rank_changes = []\n",
    "    top1_changed = 0\n",
    "    moved_up = 0\n",
    "    moved_down = 0\n",
    "    total_docs = 0\n",
    "    \n",
    "    latencies = {\"stage1\": [], \"stage2\": []}\n",
    "    \n",
    "    for query in test_queries:\n",
    "        results = retriever.search(query, k=k, first_stage_k=first_stage_k)\n",
    "        \n",
    "        for r in results:\n",
    "            rank_change = r[\"rank_change\"]\n",
    "            all_rank_changes.append(abs(rank_change))\n",
    "            \n",
    "            if rank_change > 0:\n",
    "                moved_up += 1\n",
    "            elif rank_change < 0:\n",
    "                moved_down += 1\n",
    "            \n",
    "            total_docs += 1\n",
    "        \n",
    "        # Check if top-1 changed\n",
    "        if results[0][\"bi_encoder_rank\"] != 1:\n",
    "            top1_changed += 1\n",
    "        \n",
    "        latencies[\"stage1\"].append(results[0][\"stage1_time_ms\"])\n",
    "        latencies[\"stage2\"].append(results[0][\"stage2_time_ms\"])\n",
    "    \n",
    "    return {\n",
    "        \"avg_rank_change\": np.mean(all_rank_changes),\n",
    "        \"max_rank_change\": np.max(all_rank_changes),\n",
    "        \"pct_moved_up\": 100 * moved_up / total_docs,\n",
    "        \"pct_moved_down\": 100 * moved_down / total_docs,\n",
    "        \"pct_unchanged\": 100 * (total_docs - moved_up - moved_down) / total_docs,\n",
    "        \"top1_change_rate\": 100 * top1_changed / len(test_queries),\n",
    "        \"avg_stage1_ms\": np.mean(latencies[\"stage1\"]),\n",
    "        \"avg_stage2_ms\": np.mean(latencies[\"stage2\"])\n",
    "    }\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is the memory capacity of DGX Spark?\",\n",
    "    \"How does LoRA reduce trainable parameters?\",\n",
    "    \"Explain transformer attention mechanism\",\n",
    "    \"What quantization methods are available?\",\n",
    "    \"Compare vector databases for RAG\",\n",
    "    \"How to fine-tune a large language model efficiently?\",\n",
    "    \"What is the difference between GPTQ and AWQ?\",\n",
    "    \"Explain retrieval augmented generation\"\n",
    "]\n",
    "\n",
    "# Analyze impact\n",
    "impact = analyze_reranking_impact(retriever, test_queries)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RERANKING IMPACT ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Average rank change:     {impact['avg_rank_change']:.2f} positions\")\n",
    "print(f\"Maximum rank change:     {impact['max_rank_change']:.0f} positions\")\n",
    "print(f\"Documents moved up:      {impact['pct_moved_up']:.1f}%\")\n",
    "print(f\"Documents moved down:    {impact['pct_moved_down']:.1f}%\")\n",
    "print(f\"Documents unchanged:     {impact['pct_unchanged']:.1f}%\")\n",
    "print(f\"Top-1 change rate:       {impact['top1_change_rate']:.1f}%\")\n",
    "print(f\"\\nLatency breakdown:\")\n",
    "print(f\"  Stage 1 (bi-encoder):  {impact['avg_stage1_ms']:.2f}ms\")\n",
    "print(f\"  Stage 2 (reranking):   {impact['avg_stage2_ms']:.2f}ms\")\n",
    "print(f\"  Total:                 {impact['avg_stage1_ms'] + impact['avg_stage2_ms']:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 Solution: Compare With and Without Reranking\n",
    "\n",
    "**Task**: Evaluate quality improvement from reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval_quality(\n",
    "    retriever: TwoStageRetriever,\n",
    "    test_queries: List[Dict[str, Any]],\n",
    "    k: int = 5\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Compare retrieval quality with and without reranking.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {\n",
    "        \"without_reranking\": {\"recalls\": [], \"mrrs\": []},\n",
    "        \"with_reranking\": {\"recalls\": [], \"mrrs\": []}\n",
    "    }\n",
    "    \n",
    "    for query_data in test_queries:\n",
    "        query = query_data[\"query\"]\n",
    "        expected = query_data.get(\"expected_source\", \"\")\n",
    "        \n",
    "        # Without reranking\n",
    "        simple_results = retriever.search_without_reranking(query, k=k)\n",
    "        \n",
    "        # With reranking\n",
    "        reranked_results = retriever.search(query, k=k)\n",
    "        \n",
    "        # Calculate metrics for each\n",
    "        for method, search_results in [\n",
    "            (\"without_reranking\", simple_results),\n",
    "            (\"with_reranking\", reranked_results)\n",
    "        ]:\n",
    "            found = False\n",
    "            mrr = 0\n",
    "            \n",
    "            for i, r in enumerate(search_results):\n",
    "                source = r[\"metadata\"].get(\"source\", \"\")\n",
    "                if expected in source:\n",
    "                    found = True\n",
    "                    mrr = 1 / (i + 1)\n",
    "                    break\n",
    "            \n",
    "            results[method][\"recalls\"].append(1.0 if found else 0.0)\n",
    "            results[method][\"mrrs\"].append(mrr)\n",
    "    \n",
    "    # Aggregate\n",
    "    summary = {}\n",
    "    for method, metrics in results.items():\n",
    "        summary[method] = {\n",
    "            \"recall@k\": np.mean(metrics[\"recalls\"]),\n",
    "            \"mrr\": np.mean(metrics[\"mrrs\"])\n",
    "        }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Labeled test queries\n",
    "labeled_queries = [\n",
    "    {\"query\": \"DGX Spark memory capacity\", \"expected_source\": \"dgx_spark\"},\n",
    "    {\"query\": \"LoRA parameter reduction\", \"expected_source\": \"lora\"},\n",
    "    {\"query\": \"transformer attention\", \"expected_source\": \"transformer\"},\n",
    "    {\"query\": \"quantization methods comparison\", \"expected_source\": \"quantization\"},\n",
    "    {\"query\": \"vector database for retrieval\", \"expected_source\": \"vector_database\"},\n",
    "    {\"query\": \"128GB unified memory GPU\", \"expected_source\": \"dgx_spark\"},\n",
    "    {\"query\": \"efficient fine-tuning with low rank\", \"expected_source\": \"lora\"},\n",
    "    {\"query\": \"NVFP4 4-bit quantization\", \"expected_source\": \"quantization\"}\n",
    "]\n",
    "\n",
    "quality = evaluate_retrieval_quality(retriever, labeled_queries)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUALITY COMPARISON: WITH vs WITHOUT RERANKING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<20} {'Without Reranking':<20} {'With Reranking':<20} {'Improvement':<15}\")\n",
    "print(\"-\"*75)\n",
    "\n",
    "for metric in [\"recall@k\", \"mrr\"]:\n",
    "    without = quality[\"without_reranking\"][metric]\n",
    "    with_rr = quality[\"with_reranking\"][metric]\n",
    "    improvement = (with_rr - without) / (without + 1e-8) * 100\n",
    "    \n",
    "    print(f\"{metric:<20} {without:<20.3f} {with_rr:<20.3f} {improvement:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 Solution: Optimize First-Stage K\n",
    "\n",
    "**Task**: Find optimal candidate pool size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_first_stage_k(\n",
    "    retriever: TwoStageRetriever,\n",
    "    test_queries: List[Dict],\n",
    "    k_values: List[int] = [10, 20, 50, 100],\n",
    "    final_k: int = 5\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Find optimal first_stage_k balancing quality and latency.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for first_k in k_values:\n",
    "        recalls = []\n",
    "        mrrs = []\n",
    "        latencies = []\n",
    "        \n",
    "        for query_data in test_queries:\n",
    "            query = query_data[\"query\"]\n",
    "            expected = query_data.get(\"expected_source\", \"\")\n",
    "            \n",
    "            search_results = retriever.search(query, k=final_k, first_stage_k=first_k)\n",
    "            \n",
    "            # Check recall\n",
    "            found = False\n",
    "            mrr = 0\n",
    "            for i, r in enumerate(search_results):\n",
    "                if expected in r[\"metadata\"].get(\"source\", \"\"):\n",
    "                    found = True\n",
    "                    mrr = 1 / (i + 1)\n",
    "                    break\n",
    "            \n",
    "            recalls.append(1.0 if found else 0.0)\n",
    "            mrrs.append(mrr)\n",
    "            latencies.append(search_results[0][\"stage1_time_ms\"] + search_results[0][\"stage2_time_ms\"])\n",
    "        \n",
    "        results.append({\n",
    "            \"first_stage_k\": first_k,\n",
    "            \"recall@k\": np.mean(recalls),\n",
    "            \"mrr\": np.mean(mrrs),\n",
    "            \"latency_ms\": np.mean(latencies)\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run optimization\n",
    "k_results = optimize_first_stage_k(\n",
    "    retriever,\n",
    "    labeled_queries,\n",
    "    k_values=[10, 20, 30, 50, 75, 100]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FIRST-STAGE K OPTIMIZATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'First-Stage K':<15} {'Recall@5':<12} {'MRR':<12} {'Latency (ms)':<15}\")\n",
    "print(\"-\"*54)\n",
    "\n",
    "best_quality = max(k_results, key=lambda x: x['mrr'])\n",
    "best_latency = min(k_results, key=lambda x: x['latency_ms'])\n",
    "\n",
    "for r in k_results:\n",
    "    markers = []\n",
    "    if r == best_quality:\n",
    "        markers.append(\"BEST QUALITY\")\n",
    "    if r == best_latency:\n",
    "        markers.append(\"FASTEST\")\n",
    "    marker_str = \" <-- \" + \", \".join(markers) if markers else \"\"\n",
    "    \n",
    "    print(f\"{r['first_stage_k']:<15} {r['recall@k']:<12.3f} {r['mrr']:<12.3f} {r['latency_ms']:<15.2f}{marker_str}\")\n",
    "\n",
    "print(f\"\\nRecommendation: Use first_stage_k={best_quality['first_stage_k']} for best quality\")\n",
    "print(f\"Alternative: Use first_stage_k={best_latency['first_stage_k']} for lowest latency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"\"\"\n",
    "RERANKING BEST PRACTICES\n",
    "========================\n",
    "\n",
    "1. ARCHITECTURE CHOICE\n",
    "   - Bi-encoder: Fast, good for initial retrieval\n",
    "   - Cross-encoder: Accurate, use for reranking top candidates\n",
    "   - Combine both for optimal quality/latency tradeoff\n",
    "\n",
    "2. FIRST-STAGE K SELECTION\n",
    "   - Too low (< 20): May miss relevant documents\n",
    "   - Too high (> 100): Increases reranking latency\n",
    "   - Sweet spot: 30-50 for most use cases\n",
    "\n",
    "3. MODEL SELECTION\n",
    "   - BGE-reranker-large: Best quality, ~300ms/query\n",
    "   - BGE-reranker-base: Good balance, ~100ms/query\n",
    "   - MiniLM-based: Fastest, slight quality drop\n",
    "\n",
    "4. DGX SPARK OPTIMIZATION\n",
    "   - Use GPU for both bi-encoder and cross-encoder\n",
    "   - Batch reranking requests for throughput\n",
    "   - Consider caching embeddings in GPU memory\n",
    "\n",
    "5. WHEN TO USE RERANKING\n",
    "   - High-stakes queries (customer support, legal)\n",
    "   - Complex semantic queries\n",
    "   - When latency budget allows (~300ms extra)\n",
    "   \n",
    "6. WHEN TO SKIP RERANKING\n",
    "   - Simple keyword queries\n",
    "   - Real-time applications with strict latency\n",
    "   - When bi-encoder quality is sufficient\n",
    "\"\"\"\n",
    "\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
