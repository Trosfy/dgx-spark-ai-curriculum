{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.5.7 Solutions: Production RAG System\n",
    "\n",
    "Complete solutions for building a production-ready RAG system with caching, monitoring, and error handling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Callable\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "from collections import OrderedDict\n",
    "import threading\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"ProductionRAG\")\n",
    "\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Implement Caching Layer\n",
    "\n",
    "**Task**: Build a caching system for embeddings and query results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRUCache:\n",
    "    \"\"\"\n",
    "    Thread-safe LRU cache with TTL support.\n",
    "    \n",
    "    Features:\n",
    "    - Least Recently Used eviction\n",
    "    - Time-to-live expiration\n",
    "    - Thread-safe operations\n",
    "    - Memory size tracking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int = 1000, ttl_seconds: int = 3600):\n",
    "        self.max_size = max_size\n",
    "        self.ttl_seconds = ttl_seconds\n",
    "        self.cache: OrderedDict = OrderedDict()\n",
    "        self.timestamps: Dict[str, float] = {}\n",
    "        self.lock = threading.RLock()\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "    \n",
    "    def _hash_key(self, key: Any) -> str:\n",
    "        \"\"\"Create a hash key for any input.\"\"\"\n",
    "        if isinstance(key, str):\n",
    "            return hashlib.md5(key.encode()).hexdigest()\n",
    "        return hashlib.md5(json.dumps(key, sort_keys=True).encode()).hexdigest()\n",
    "    \n",
    "    def _is_expired(self, key: str) -> bool:\n",
    "        \"\"\"Check if a cache entry has expired.\"\"\"\n",
    "        if key not in self.timestamps:\n",
    "            return True\n",
    "        return time.time() - self.timestamps[key] > self.ttl_seconds\n",
    "    \n",
    "    def get(self, key: Any) -> Optional[Any]:\n",
    "        \"\"\"Get a value from the cache.\"\"\"\n",
    "        hashed_key = self._hash_key(key)\n",
    "        \n",
    "        with self.lock:\n",
    "            if hashed_key in self.cache:\n",
    "                if self._is_expired(hashed_key):\n",
    "                    # Remove expired entry\n",
    "                    del self.cache[hashed_key]\n",
    "                    del self.timestamps[hashed_key]\n",
    "                    self.misses += 1\n",
    "                    return None\n",
    "                \n",
    "                # Move to end (most recently used)\n",
    "                self.cache.move_to_end(hashed_key)\n",
    "                self.hits += 1\n",
    "                return self.cache[hashed_key]\n",
    "            \n",
    "            self.misses += 1\n",
    "            return None\n",
    "    \n",
    "    def set(self, key: Any, value: Any) -> None:\n",
    "        \"\"\"Set a value in the cache.\"\"\"\n",
    "        hashed_key = self._hash_key(key)\n",
    "        \n",
    "        with self.lock:\n",
    "            if hashed_key in self.cache:\n",
    "                # Update existing\n",
    "                self.cache.move_to_end(hashed_key)\n",
    "            else:\n",
    "                # Evict if necessary\n",
    "                while len(self.cache) >= self.max_size:\n",
    "                    oldest = next(iter(self.cache))\n",
    "                    del self.cache[oldest]\n",
    "                    del self.timestamps[oldest]\n",
    "            \n",
    "            self.cache[hashed_key] = value\n",
    "            self.timestamps[hashed_key] = time.time()\n",
    "    \n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the cache.\"\"\"\n",
    "        with self.lock:\n",
    "            self.cache.clear()\n",
    "            self.timestamps.clear()\n",
    "    \n",
    "    @property\n",
    "    def stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get cache statistics.\"\"\"\n",
    "        total = self.hits + self.misses\n",
    "        return {\n",
    "            \"size\": len(self.cache),\n",
    "            \"max_size\": self.max_size,\n",
    "            \"hits\": self.hits,\n",
    "            \"misses\": self.misses,\n",
    "            \"hit_rate\": self.hits / total if total > 0 else 0\n",
    "        }\n",
    "\n",
    "\n",
    "class EmbeddingCache:\n",
    "    \"\"\"\n",
    "    Specialized cache for embeddings with GPU memory awareness.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        max_embeddings: int = 10000,\n",
    "        store_on_gpu: bool = False\n",
    "    ):\n",
    "        self.cache = LRUCache(max_size=max_embeddings, ttl_seconds=86400)  # 24h TTL\n",
    "        self.store_on_gpu = store_on_gpu and torch.cuda.is_available()\n",
    "    \n",
    "    def get_embedding(self, text: str) -> Optional[np.ndarray]:\n",
    "        \"\"\"Get cached embedding.\"\"\"\n",
    "        result = self.cache.get(text)\n",
    "        if result is not None and self.store_on_gpu:\n",
    "            return result.cpu().numpy()\n",
    "        return result\n",
    "    \n",
    "    def set_embedding(self, text: str, embedding: np.ndarray) -> None:\n",
    "        \"\"\"Cache an embedding.\"\"\"\n",
    "        if self.store_on_gpu:\n",
    "            embedding = torch.tensor(embedding, device=\"cuda\")\n",
    "        self.cache.set(text, embedding)\n",
    "    \n",
    "    def get_or_compute(\n",
    "        self,\n",
    "        text: str,\n",
    "        compute_fn: Callable[[str], np.ndarray]\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Get from cache or compute and cache.\"\"\"\n",
    "        cached = self.get_embedding(text)\n",
    "        if cached is not None:\n",
    "            return cached\n",
    "        \n",
    "        embedding = compute_fn(text)\n",
    "        self.set_embedding(text, embedding)\n",
    "        return embedding\n",
    "\n",
    "print(\"Caching classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Implement Error Handling\n",
    "\n",
    "**Task**: Build robust error handling with retries and fallbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from typing import TypeVar, Callable\n",
    "import traceback\n",
    "\n",
    "T = TypeVar('T')\n",
    "\n",
    "@dataclass\n",
    "class RetryConfig:\n",
    "    \"\"\"Configuration for retry behavior.\"\"\"\n",
    "    max_retries: int = 3\n",
    "    base_delay: float = 1.0  # seconds\n",
    "    max_delay: float = 30.0\n",
    "    exponential_backoff: bool = True\n",
    "    retry_exceptions: tuple = (Exception,)\n",
    "\n",
    "\n",
    "def with_retry(config: RetryConfig = RetryConfig()):\n",
    "    \"\"\"\n",
    "    Decorator for automatic retry with exponential backoff.\n",
    "    \"\"\"\n",
    "    def decorator(func: Callable[..., T]) -> Callable[..., T]:\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs) -> T:\n",
    "            last_exception = None\n",
    "            \n",
    "            for attempt in range(config.max_retries + 1):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except config.retry_exceptions as e:\n",
    "                    last_exception = e\n",
    "                    \n",
    "                    if attempt < config.max_retries:\n",
    "                        # Calculate delay\n",
    "                        if config.exponential_backoff:\n",
    "                            delay = min(\n",
    "                                config.base_delay * (2 ** attempt),\n",
    "                                config.max_delay\n",
    "                            )\n",
    "                        else:\n",
    "                            delay = config.base_delay\n",
    "                        \n",
    "                        logger.warning(\n",
    "                            f\"Attempt {attempt + 1} failed: {e}. \"\n",
    "                            f\"Retrying in {delay:.1f}s...\"\n",
    "                        )\n",
    "                        time.sleep(delay)\n",
    "            \n",
    "            # All retries exhausted\n",
    "            raise last_exception\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "\n",
    "class CircuitBreaker:\n",
    "    \"\"\"\n",
    "    Circuit breaker pattern for failing gracefully.\n",
    "    \n",
    "    States:\n",
    "    - CLOSED: Normal operation\n",
    "    - OPEN: Failing fast, not calling service\n",
    "    - HALF_OPEN: Testing if service recovered\n",
    "    \"\"\"\n",
    "    \n",
    "    CLOSED = \"closed\"\n",
    "    OPEN = \"open\"\n",
    "    HALF_OPEN = \"half_open\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        failure_threshold: int = 5,\n",
    "        reset_timeout: float = 60.0\n",
    "    ):\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.reset_timeout = reset_timeout\n",
    "        self.state = self.CLOSED\n",
    "        self.failures = 0\n",
    "        self.last_failure_time = 0\n",
    "        self.lock = threading.Lock()\n",
    "    \n",
    "    def can_execute(self) -> bool:\n",
    "        \"\"\"Check if we should attempt execution.\"\"\"\n",
    "        with self.lock:\n",
    "            if self.state == self.CLOSED:\n",
    "                return True\n",
    "            \n",
    "            if self.state == self.OPEN:\n",
    "                # Check if timeout has passed\n",
    "                if time.time() - self.last_failure_time > self.reset_timeout:\n",
    "                    self.state = self.HALF_OPEN\n",
    "                    return True\n",
    "                return False\n",
    "            \n",
    "            # HALF_OPEN - allow one attempt\n",
    "            return True\n",
    "    \n",
    "    def record_success(self):\n",
    "        \"\"\"Record a successful execution.\"\"\"\n",
    "        with self.lock:\n",
    "            self.failures = 0\n",
    "            self.state = self.CLOSED\n",
    "    \n",
    "    def record_failure(self):\n",
    "        \"\"\"Record a failed execution.\"\"\"\n",
    "        with self.lock:\n",
    "            self.failures += 1\n",
    "            self.last_failure_time = time.time()\n",
    "            \n",
    "            if self.failures >= self.failure_threshold:\n",
    "                self.state = self.OPEN\n",
    "                logger.warning(f\"Circuit breaker opened after {self.failures} failures\")\n",
    "    \n",
    "    def execute(self, func: Callable[..., T], *args, **kwargs) -> T:\n",
    "        \"\"\"Execute with circuit breaker protection.\"\"\"\n",
    "        if not self.can_execute():\n",
    "            raise Exception(\"Circuit breaker is open\")\n",
    "        \n",
    "        try:\n",
    "            result = func(*args, **kwargs)\n",
    "            self.record_success()\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            self.record_failure()\n",
    "            raise\n",
    "\n",
    "print(\"Error handling classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 Solution: Implement Monitoring\n",
    "\n",
    "**Task**: Build monitoring for latency, throughput, and quality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class QueryMetrics:\n",
    "    \"\"\"Metrics for a single query.\"\"\"\n",
    "    query_id: str\n",
    "    timestamp: datetime\n",
    "    query: str\n",
    "    latency_ms: float\n",
    "    retrieval_latency_ms: float\n",
    "    generation_latency_ms: float\n",
    "    num_contexts: int\n",
    "    top_score: float\n",
    "    cache_hit: bool\n",
    "    success: bool\n",
    "    error: Optional[str] = None\n",
    "\n",
    "\n",
    "class RAGMonitor:\n",
    "    \"\"\"\n",
    "    Comprehensive monitoring for RAG systems.\n",
    "    \n",
    "    Tracks:\n",
    "    - Latency distribution (p50, p95, p99)\n",
    "    - Throughput (QPS)\n",
    "    - Error rates\n",
    "    - Cache performance\n",
    "    - Retrieval quality indicators\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, window_size: int = 1000):\n",
    "        self.window_size = window_size\n",
    "        self.metrics: List[QueryMetrics] = []\n",
    "        self.lock = threading.Lock()\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def record(self, metrics: QueryMetrics):\n",
    "        \"\"\"Record query metrics.\"\"\"\n",
    "        with self.lock:\n",
    "            self.metrics.append(metrics)\n",
    "            \n",
    "            # Keep only recent metrics\n",
    "            if len(self.metrics) > self.window_size:\n",
    "                self.metrics = self.metrics[-self.window_size:]\n",
    "    \n",
    "    def get_latency_stats(self) -> Dict[str, float]:\n",
    "        \"\"\"Get latency statistics.\"\"\"\n",
    "        with self.lock:\n",
    "            if not self.metrics:\n",
    "                return {}\n",
    "            \n",
    "            latencies = [m.latency_ms for m in self.metrics if m.success]\n",
    "            if not latencies:\n",
    "                return {}\n",
    "            \n",
    "            return {\n",
    "                \"mean\": np.mean(latencies),\n",
    "                \"p50\": np.percentile(latencies, 50),\n",
    "                \"p95\": np.percentile(latencies, 95),\n",
    "                \"p99\": np.percentile(latencies, 99),\n",
    "                \"min\": np.min(latencies),\n",
    "                \"max\": np.max(latencies)\n",
    "            }\n",
    "    \n",
    "    def get_throughput(self) -> float:\n",
    "        \"\"\"Get queries per second.\"\"\"\n",
    "        with self.lock:\n",
    "            if not self.metrics:\n",
    "                return 0\n",
    "            \n",
    "            elapsed = time.time() - self.start_time\n",
    "            return len(self.metrics) / elapsed if elapsed > 0 else 0\n",
    "    \n",
    "    def get_error_rate(self) -> float:\n",
    "        \"\"\"Get error rate.\"\"\"\n",
    "        with self.lock:\n",
    "            if not self.metrics:\n",
    "                return 0\n",
    "            \n",
    "            errors = sum(1 for m in self.metrics if not m.success)\n",
    "            return errors / len(self.metrics)\n",
    "    \n",
    "    def get_cache_stats(self) -> Dict[str, float]:\n",
    "        \"\"\"Get cache performance.\"\"\"\n",
    "        with self.lock:\n",
    "            if not self.metrics:\n",
    "                return {}\n",
    "            \n",
    "            hits = sum(1 for m in self.metrics if m.cache_hit)\n",
    "            return {\n",
    "                \"hit_rate\": hits / len(self.metrics),\n",
    "                \"hits\": hits,\n",
    "                \"misses\": len(self.metrics) - hits\n",
    "            }\n",
    "    \n",
    "    def get_retrieval_stats(self) -> Dict[str, float]:\n",
    "        \"\"\"Get retrieval quality indicators.\"\"\"\n",
    "        with self.lock:\n",
    "            if not self.metrics:\n",
    "                return {}\n",
    "            \n",
    "            successful = [m for m in self.metrics if m.success]\n",
    "            if not successful:\n",
    "                return {}\n",
    "            \n",
    "            return {\n",
    "                \"avg_contexts\": np.mean([m.num_contexts for m in successful]),\n",
    "                \"avg_top_score\": np.mean([m.top_score for m in successful]),\n",
    "                \"avg_retrieval_ms\": np.mean([m.retrieval_latency_ms for m in successful])\n",
    "            }\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get full monitoring summary.\"\"\"\n",
    "        return {\n",
    "            \"total_queries\": len(self.metrics),\n",
    "            \"qps\": self.get_throughput(),\n",
    "            \"error_rate\": self.get_error_rate(),\n",
    "            \"latency\": self.get_latency_stats(),\n",
    "            \"cache\": self.get_cache_stats(),\n",
    "            \"retrieval\": self.get_retrieval_stats()\n",
    "        }\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print formatted summary.\"\"\"\n",
    "        summary = self.get_summary()\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(\"RAG SYSTEM MONITORING SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total Queries: {summary['total_queries']}\")\n",
    "        print(f\"Throughput: {summary['qps']:.2f} QPS\")\n",
    "        print(f\"Error Rate: {summary['error_rate']*100:.2f}%\")\n",
    "        \n",
    "        if summary['latency']:\n",
    "            print(f\"\\nLatency:\")\n",
    "            print(f\"  Mean: {summary['latency']['mean']:.2f}ms\")\n",
    "            print(f\"  P50:  {summary['latency']['p50']:.2f}ms\")\n",
    "            print(f\"  P95:  {summary['latency']['p95']:.2f}ms\")\n",
    "            print(f\"  P99:  {summary['latency']['p99']:.2f}ms\")\n",
    "        \n",
    "        if summary['cache']:\n",
    "            print(f\"\\nCache:\")\n",
    "            print(f\"  Hit Rate: {summary['cache']['hit_rate']*100:.2f}%\")\n",
    "        \n",
    "        if summary['retrieval']:\n",
    "            print(f\"\\nRetrieval:\")\n",
    "            print(f\"  Avg Contexts: {summary['retrieval']['avg_contexts']:.1f}\")\n",
    "            print(f\"  Avg Top Score: {summary['retrieval']['avg_top_score']:.3f}\")\n",
    "\n",
    "print(\"RAGMonitor class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 Solution: Build Production RAG System\n",
    "\n",
    "**Task**: Combine all components into a production-ready system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import uuid\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RAGConfig:\n",
    "    \"\"\"Configuration for production RAG system.\"\"\"\n",
    "    # Retrieval\n",
    "    k: int = 5\n",
    "    score_threshold: float = 0.3\n",
    "    \n",
    "    # Caching\n",
    "    enable_cache: bool = True\n",
    "    cache_size: int = 10000\n",
    "    cache_ttl: int = 3600\n",
    "    \n",
    "    # Error handling\n",
    "    max_retries: int = 3\n",
    "    circuit_breaker_threshold: int = 5\n",
    "    \n",
    "    # Models\n",
    "    embedding_model: str = \"BAAI/bge-large-en-v1.5\"\n",
    "    llm_model: str = \"llama3.1:8b\"\n",
    "    \n",
    "    # Monitoring\n",
    "    enable_monitoring: bool = True\n",
    "\n",
    "\n",
    "class ProductionRAG:\n",
    "    \"\"\"\n",
    "    Production-ready RAG system.\n",
    "    \n",
    "    Features:\n",
    "    - LRU caching for queries and embeddings\n",
    "    - Automatic retries with exponential backoff\n",
    "    - Circuit breaker for failing gracefully\n",
    "    - Comprehensive monitoring\n",
    "    - GPU acceleration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig = RAGConfig()):\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize components\n",
    "        self._init_caching()\n",
    "        self._init_error_handling()\n",
    "        self._init_monitoring()\n",
    "        self._init_models()\n",
    "        \n",
    "        self.vectorstore = None\n",
    "        logger.info(\"ProductionRAG initialized\")\n",
    "    \n",
    "    def _init_caching(self):\n",
    "        \"\"\"Initialize caching layers.\"\"\"\n",
    "        if self.config.enable_cache:\n",
    "            self.query_cache = LRUCache(\n",
    "                max_size=self.config.cache_size,\n",
    "                ttl_seconds=self.config.cache_ttl\n",
    "            )\n",
    "            self.embedding_cache = EmbeddingCache(\n",
    "                max_embeddings=self.config.cache_size\n",
    "            )\n",
    "        else:\n",
    "            self.query_cache = None\n",
    "            self.embedding_cache = None\n",
    "    \n",
    "    def _init_error_handling(self):\n",
    "        \"\"\"Initialize error handling.\"\"\"\n",
    "        self.llm_circuit_breaker = CircuitBreaker(\n",
    "            failure_threshold=self.config.circuit_breaker_threshold\n",
    "        )\n",
    "        self.retry_config = RetryConfig(max_retries=self.config.max_retries)\n",
    "    \n",
    "    def _init_monitoring(self):\n",
    "        \"\"\"Initialize monitoring.\"\"\"\n",
    "        if self.config.enable_monitoring:\n",
    "            self.monitor = RAGMonitor()\n",
    "        else:\n",
    "            self.monitor = None\n",
    "    \n",
    "    def _init_models(self):\n",
    "        \"\"\"Initialize embedding model.\"\"\"\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.embedding_model = HuggingFaceEmbeddings(\n",
    "            model_name=self.config.embedding_model,\n",
    "            model_kwargs={\"device\": device},\n",
    "            encode_kwargs={\"normalize_embeddings\": True, \"batch_size\": 64}\n",
    "        )\n",
    "        logger.info(f\"Embedding model loaded on {device}\")\n",
    "    \n",
    "    def index_documents(self, documents_path: str):\n",
    "        \"\"\"Index documents into vector store.\"\"\"\n",
    "        # Load documents\n",
    "        documents = []\n",
    "        for file_path in Path(documents_path).glob(\"*.md\"):\n",
    "            content = file_path.read_text(encoding='utf-8')\n",
    "            documents.append(Document(\n",
    "                page_content=content,\n",
    "                metadata={\"source\": file_path.name}\n",
    "            ))\n",
    "        \n",
    "        # Chunk\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "        chunks = splitter.split_documents(documents)\n",
    "        \n",
    "        # Create vector store\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=self.embedding_model,\n",
    "            persist_directory=\"./production_chroma_db\"\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Indexed {len(chunks)} chunks from {len(documents)} documents\")\n",
    "    \n",
    "    def _retrieve(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Retrieve with caching.\"\"\"\n",
    "        # Check cache\n",
    "        if self.query_cache:\n",
    "            cached = self.query_cache.get(f\"retrieve:{query}\")\n",
    "            if cached is not None:\n",
    "                return cached, True\n",
    "        \n",
    "        # Perform retrieval\n",
    "        results = self.vectorstore.similarity_search_with_score(\n",
    "            query, k=self.config.k\n",
    "        )\n",
    "        \n",
    "        formatted = [\n",
    "            {\n",
    "                \"content\": doc.page_content,\n",
    "                \"metadata\": doc.metadata,\n",
    "                \"score\": 1 / (1 + score)\n",
    "            }\n",
    "            for doc, score in results\n",
    "        ]\n",
    "        \n",
    "        # Cache results\n",
    "        if self.query_cache:\n",
    "            self.query_cache.set(f\"retrieve:{query}\", formatted)\n",
    "        \n",
    "        return formatted, False\n",
    "    \n",
    "    @with_retry(RetryConfig(max_retries=2))\n",
    "    def _generate(self, query: str, contexts: List[str]) -> str:\n",
    "        \"\"\"Generate with retries and circuit breaker.\"\"\"\n",
    "        def _call_llm():\n",
    "            import ollama\n",
    "            \n",
    "            context_str = \"\\n\\n\".join(contexts[:5])\n",
    "            prompt = f\"\"\"Answer based ONLY on the context. If unsure, say so.\n",
    "\n",
    "Context:\n",
    "{context_str}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "            \n",
    "            response = ollama.chat(\n",
    "                model=self.config.llm_model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response[\"message\"][\"content\"]\n",
    "        \n",
    "        return self.llm_circuit_breaker.execute(_call_llm)\n",
    "    \n",
    "    def query(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Execute RAG query with full production features.\"\"\"\n",
    "        query_id = str(uuid.uuid4())[:8]\n",
    "        start_time = time.time()\n",
    "        cache_hit = False\n",
    "        error = None\n",
    "        \n",
    "        try:\n",
    "            # Retrieval\n",
    "            retrieval_start = time.time()\n",
    "            retrieved, cache_hit = self._retrieve(question)\n",
    "            retrieval_time = (time.time() - retrieval_start) * 1000\n",
    "            \n",
    "            contexts = [r[\"content\"] for r in retrieved]\n",
    "            \n",
    "            # Generation\n",
    "            generation_start = time.time()\n",
    "            answer = self._generate(question, contexts)\n",
    "            generation_time = (time.time() - generation_start) * 1000\n",
    "            \n",
    "            total_time = (time.time() - start_time) * 1000\n",
    "            \n",
    "            result = {\n",
    "                \"query_id\": query_id,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"contexts\": contexts,\n",
    "                \"scores\": [r[\"score\"] for r in retrieved],\n",
    "                \"latency_ms\": total_time,\n",
    "                \"retrieval_ms\": retrieval_time,\n",
    "                \"generation_ms\": generation_time,\n",
    "                \"cache_hit\": cache_hit\n",
    "            }\n",
    "            \n",
    "            # Record metrics\n",
    "            if self.monitor:\n",
    "                self.monitor.record(QueryMetrics(\n",
    "                    query_id=query_id,\n",
    "                    timestamp=datetime.now(),\n",
    "                    query=question,\n",
    "                    latency_ms=total_time,\n",
    "                    retrieval_latency_ms=retrieval_time,\n",
    "                    generation_latency_ms=generation_time,\n",
    "                    num_contexts=len(contexts),\n",
    "                    top_score=retrieved[0][\"score\"] if retrieved else 0,\n",
    "                    cache_hit=cache_hit,\n",
    "                    success=True\n",
    "                ))\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            error = str(e)\n",
    "            logger.error(f\"Query failed: {error}\")\n",
    "            \n",
    "            # Record failure\n",
    "            if self.monitor:\n",
    "                self.monitor.record(QueryMetrics(\n",
    "                    query_id=query_id,\n",
    "                    timestamp=datetime.now(),\n",
    "                    query=question,\n",
    "                    latency_ms=(time.time() - start_time) * 1000,\n",
    "                    retrieval_latency_ms=0,\n",
    "                    generation_latency_ms=0,\n",
    "                    num_contexts=0,\n",
    "                    top_score=0,\n",
    "                    cache_hit=cache_hit,\n",
    "                    success=False,\n",
    "                    error=error\n",
    "                ))\n",
    "            \n",
    "            return {\n",
    "                \"query_id\": query_id,\n",
    "                \"question\": question,\n",
    "                \"answer\": \"I'm sorry, I encountered an error processing your request.\",\n",
    "                \"error\": error\n",
    "            }\n",
    "\n",
    "print(\"ProductionRAG class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize production RAG\n",
    "config = RAGConfig(\n",
    "    k=5,\n",
    "    enable_cache=True,\n",
    "    cache_size=1000,\n",
    "    enable_monitoring=True\n",
    ")\n",
    "\n",
    "rag = ProductionRAG(config)\n",
    "rag.index_documents(\"../data/sample_documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is the memory capacity of DGX Spark?\",\n",
    "    \"How does LoRA work?\",\n",
    "    \"Explain transformer attention\",\n",
    "    \"What is the memory capacity of DGX Spark?\",  # Repeat for cache\n",
    "    \"What quantization methods are available?\"\n",
    "]\n",
    "\n",
    "print(\"Running test queries...\\n\")\n",
    "for query in test_queries:\n",
    "    result = rag.query(query)\n",
    "    print(f\"Q: {query}\")\n",
    "    print(f\"A: {result['answer'][:200]}...\")\n",
    "    print(f\"Latency: {result.get('latency_ms', 0):.0f}ms, Cache: {result.get('cache_hit', False)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print monitoring summary\n",
    "if rag.monitor:\n",
    "    rag.monitor.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist = \"\"\"\n",
    "PRODUCTION RAG DEPLOYMENT CHECKLIST\n",
    "====================================\n",
    "\n",
    "PERFORMANCE\n",
    "-----------\n",
    "[x] GPU acceleration for embeddings\n",
    "[x] Query result caching (LRU with TTL)\n",
    "[x] Embedding caching\n",
    "[x] Batch processing support\n",
    "[ ] Async/concurrent query handling\n",
    "[ ] Connection pooling for vector store\n",
    "\n",
    "RELIABILITY\n",
    "-----------\n",
    "[x] Automatic retries with backoff\n",
    "[x] Circuit breaker pattern\n",
    "[x] Graceful error handling\n",
    "[ ] Health check endpoints\n",
    "[ ] Graceful shutdown\n",
    "\n",
    "MONITORING\n",
    "----------\n",
    "[x] Latency tracking (p50, p95, p99)\n",
    "[x] Throughput monitoring\n",
    "[x] Error rate tracking\n",
    "[x] Cache hit rates\n",
    "[ ] Export to Prometheus/Grafana\n",
    "[ ] Alerting rules\n",
    "\n",
    "SECURITY\n",
    "--------\n",
    "[ ] Input validation and sanitization\n",
    "[ ] Rate limiting\n",
    "[ ] Authentication/authorization\n",
    "[ ] Audit logging\n",
    "\n",
    "QUALITY\n",
    "-------\n",
    "[ ] Automated evaluation pipeline\n",
    "[ ] A/B testing framework\n",
    "[ ] User feedback collection\n",
    "[ ] Continuous monitoring of RAGAS metrics\n",
    "\n",
    "DEPLOYMENT\n",
    "----------\n",
    "[ ] Containerization (Docker)\n",
    "[ ] Kubernetes deployment configs\n",
    "[ ] Load balancer configuration\n",
    "[ ] Auto-scaling rules\n",
    "\"\"\"\n",
    "\n",
    "print(checklist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
