{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.5.6 Solutions: RAG Evaluation with RAGAS\n",
    "\n",
    "Complete solutions for comprehensive RAG evaluation using RAGAS metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Implement RAGAS Metrics\n",
    "\n",
    "**Task**: Implement the four core RAGAS metrics from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvaluationSample:\n",
    "    \"\"\"A single sample for RAG evaluation.\"\"\"\n",
    "    question: str\n",
    "    ground_truth: str  # Expected answer\n",
    "    contexts: List[str] = field(default_factory=list)  # Retrieved contexts\n",
    "    answer: str = \"\"  # Generated answer\n",
    "    expected_source: Optional[str] = None  # For retrieval evaluation\n",
    "\n",
    "\n",
    "@dataclass \n",
    "class EvaluationResult:\n",
    "    \"\"\"Evaluation results for a sample.\"\"\"\n",
    "    question: str\n",
    "    faithfulness: float  # Is answer grounded in context?\n",
    "    answer_relevancy: float  # Does answer address question?\n",
    "    context_precision: float  # Are retrieved contexts relevant?\n",
    "    context_recall: float  # Do contexts contain needed info?\n",
    "    \n",
    "    @property\n",
    "    def average(self) -> float:\n",
    "        \"\"\"Overall score.\"\"\"\n",
    "        return (self.faithfulness + self.answer_relevancy + \n",
    "                self.context_precision + self.context_recall) / 4\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"question\": self.question,\n",
    "            \"faithfulness\": self.faithfulness,\n",
    "            \"answer_relevancy\": self.answer_relevancy,\n",
    "            \"context_precision\": self.context_precision,\n",
    "            \"context_recall\": self.context_recall,\n",
    "            \"average\": self.average\n",
    "        }\n",
    "\n",
    "\n",
    "class RAGASEvaluator:\n",
    "    \"\"\"\n",
    "    RAGAS-style evaluator using LLM-as-judge.\n",
    "    \n",
    "    Four core metrics:\n",
    "    1. Faithfulness: Is the answer grounded in the context?\n",
    "    2. Answer Relevancy: Does the answer address the question?\n",
    "    3. Context Precision: Are the retrieved contexts relevant?\n",
    "    4. Context Recall: Do contexts contain info for ground truth?\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_model: str = \"llama3.1:8b\"):\n",
    "        \"\"\"\n",
    "        Initialize evaluator with an LLM.\n",
    "        \n",
    "        Args:\n",
    "            llm_model: Ollama model name for evaluation\n",
    "        \"\"\"\n",
    "        self.llm_model = llm_model\n",
    "        self._verify_llm()\n",
    "    \n",
    "    def _verify_llm(self):\n",
    "        \"\"\"Verify LLM is available.\"\"\"\n",
    "        try:\n",
    "            import ollama\n",
    "            ollama.chat(model=self.llm_model, messages=[{\"role\": \"user\", \"content\": \"Hi\"}])\n",
    "            print(f\"LLM verified: {self.llm_model}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: LLM not available ({e}). Using fallback scoring.\")\n",
    "    \n",
    "    def _llm_judge(self, prompt: str) -> str:\n",
    "        \"\"\"Get LLM judgment.\"\"\"\n",
    "        try:\n",
    "            import ollama\n",
    "            response = ollama.chat(\n",
    "                model=self.llm_model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                options={\"temperature\": 0}  # Deterministic\n",
    "            )\n",
    "            return response[\"message\"][\"content\"].strip()\n",
    "        except Exception as e:\n",
    "            return \"0.5\"  # Fallback\n",
    "    \n",
    "    def _parse_score(self, response: str) -> float:\n",
    "        \"\"\"Parse numeric score from LLM response.\"\"\"\n",
    "        # Try to find a score in the response\n",
    "        import re\n",
    "        \n",
    "        # Look for decimal numbers\n",
    "        numbers = re.findall(r'\\b(0\\.\\d+|1\\.0|0|1)\\b', response)\n",
    "        if numbers:\n",
    "            return float(numbers[0])\n",
    "        \n",
    "        # Look for keywords\n",
    "        response_lower = response.lower()\n",
    "        if \"yes\" in response_lower or \"true\" in response_lower:\n",
    "            return 1.0\n",
    "        if \"no\" in response_lower or \"false\" in response_lower:\n",
    "            return 0.0\n",
    "        if \"partial\" in response_lower:\n",
    "            return 0.5\n",
    "        \n",
    "        return 0.5  # Default\n",
    "    \n",
    "    def evaluate_faithfulness(self, sample: EvaluationSample) -> float:\n",
    "        \"\"\"\n",
    "        Faithfulness: Are all claims in the answer supported by context?\n",
    "        \n",
    "        High faithfulness = No hallucinations\n",
    "        Low faithfulness = Answer contains unsupported claims\n",
    "        \"\"\"\n",
    "        if not sample.contexts or not sample.answer:\n",
    "            return 0.0\n",
    "        \n",
    "        context_str = \"\\n---\\n\".join(sample.contexts[:5])\n",
    "        \n",
    "        prompt = f\"\"\"Evaluate if the answer is faithfully grounded in the context.\n",
    "\n",
    "CONTEXT:\n",
    "{context_str}\n",
    "\n",
    "ANSWER:\n",
    "{sample.answer}\n",
    "\n",
    "Instructions:\n",
    "1. Identify each factual claim in the answer\n",
    "2. Check if each claim is supported by the context\n",
    "3. Score based on percentage of supported claims\n",
    "\n",
    "Score:\n",
    "- 1.0: All claims are supported by context\n",
    "- 0.5: Some claims are supported, some are not\n",
    "- 0.0: Most claims are not supported (hallucination)\n",
    "\n",
    "Respond with ONLY a number: 0.0, 0.5, or 1.0\"\"\"\n",
    "        \n",
    "        return self._parse_score(self._llm_judge(prompt))\n",
    "    \n",
    "    def evaluate_answer_relevancy(self, sample: EvaluationSample) -> float:\n",
    "        \"\"\"\n",
    "        Answer Relevancy: Does the answer address what was asked?\n",
    "        \n",
    "        High relevancy = Direct, complete answer\n",
    "        Low relevancy = Off-topic or incomplete\n",
    "        \"\"\"\n",
    "        if not sample.answer:\n",
    "            return 0.0\n",
    "        \n",
    "        prompt = f\"\"\"Evaluate if the answer directly addresses the question.\n",
    "\n",
    "QUESTION: {sample.question}\n",
    "\n",
    "ANSWER: {sample.answer}\n",
    "\n",
    "Instructions:\n",
    "1. Does the answer attempt to address the question?\n",
    "2. Is the answer complete or partial?\n",
    "3. Is there irrelevant information?\n",
    "\n",
    "Score:\n",
    "- 1.0: Answer fully and directly addresses the question\n",
    "- 0.5: Answer partially addresses the question\n",
    "- 0.0: Answer is off-topic or doesn't address the question\n",
    "\n",
    "Respond with ONLY a number: 0.0, 0.5, or 1.0\"\"\"\n",
    "        \n",
    "        return self._parse_score(self._llm_judge(prompt))\n",
    "    \n",
    "    def evaluate_context_precision(self, sample: EvaluationSample) -> float:\n",
    "        \"\"\"\n",
    "        Context Precision: What fraction of retrieved contexts are relevant?\n",
    "        \n",
    "        High precision = All contexts are useful\n",
    "        Low precision = Many irrelevant contexts retrieved\n",
    "        \"\"\"\n",
    "        if not sample.contexts:\n",
    "            return 0.0\n",
    "        \n",
    "        relevant_count = 0\n",
    "        total_count = min(5, len(sample.contexts))  # Evaluate top 5\n",
    "        \n",
    "        for context in sample.contexts[:total_count]:\n",
    "            prompt = f\"\"\"Is this context relevant to answering the question?\n",
    "\n",
    "QUESTION: {sample.question}\n",
    "\n",
    "CONTEXT: {context[:500]}\n",
    "\n",
    "Respond with ONLY: YES or NO\"\"\"\n",
    "            \n",
    "            response = self._llm_judge(prompt).upper()\n",
    "            if \"YES\" in response:\n",
    "                relevant_count += 1\n",
    "        \n",
    "        return relevant_count / total_count\n",
    "    \n",
    "    def evaluate_context_recall(self, sample: EvaluationSample) -> float:\n",
    "        \"\"\"\n",
    "        Context Recall: Do contexts contain info needed for ground truth?\n",
    "        \n",
    "        High recall = All needed info is in contexts\n",
    "        Low recall = Missing important information\n",
    "        \"\"\"\n",
    "        if not sample.contexts or not sample.ground_truth:\n",
    "            return 0.0\n",
    "        \n",
    "        context_str = \"\\n---\\n\".join(sample.contexts[:5])\n",
    "        \n",
    "        prompt = f\"\"\"Do the contexts contain enough information to produce the ground truth answer?\n",
    "\n",
    "GROUND TRUTH ANSWER: {sample.ground_truth}\n",
    "\n",
    "CONTEXTS:\n",
    "{context_str[:2000]}\n",
    "\n",
    "Instructions:\n",
    "1. Identify key facts in the ground truth\n",
    "2. Check if each fact can be found in the contexts\n",
    "3. Score based on coverage\n",
    "\n",
    "Score:\n",
    "- 1.0: All information in ground truth is present in contexts\n",
    "- 0.5: Some information is present, some is missing\n",
    "- 0.0: Critical information is missing from contexts\n",
    "\n",
    "Respond with ONLY a number: 0.0, 0.5, or 1.0\"\"\"\n",
    "        \n",
    "        return self._parse_score(self._llm_judge(prompt))\n",
    "    \n",
    "    def evaluate(self, sample: EvaluationSample) -> EvaluationResult:\n",
    "        \"\"\"Run all evaluations on a sample.\"\"\"\n",
    "        return EvaluationResult(\n",
    "            question=sample.question,\n",
    "            faithfulness=self.evaluate_faithfulness(sample),\n",
    "            answer_relevancy=self.evaluate_answer_relevancy(sample),\n",
    "            context_precision=self.evaluate_context_precision(sample),\n",
    "            context_recall=self.evaluate_context_recall(sample)\n",
    "        )\n",
    "    \n",
    "    def evaluate_batch(\n",
    "        self,\n",
    "        samples: List[EvaluationSample],\n",
    "        verbose: bool = True\n",
    "    ) -> List[EvaluationResult]:\n",
    "        \"\"\"Evaluate multiple samples.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, sample in enumerate(samples):\n",
    "            if verbose:\n",
    "                print(f\"Evaluating {i+1}/{len(samples)}: {sample.question[:50]}...\")\n",
    "            \n",
    "            result = self.evaluate(sample)\n",
    "            results.append(result)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Scores: F={result.faithfulness:.2f}, AR={result.answer_relevancy:.2f}, \"\n",
    "                      f\"CP={result.context_precision:.2f}, CR={result.context_recall:.2f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"RAGASEvaluator class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Create Test Dataset\n",
    "\n",
    "**Task**: Build a golden dataset for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_dataset() -> List[EvaluationSample]:\n",
    "    \"\"\"\n",
    "    Create a golden dataset for RAG evaluation.\n",
    "    \n",
    "    Each sample includes:\n",
    "    - Question\n",
    "    - Ground truth answer\n",
    "    - Expected source (for retrieval evaluation)\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = [\n",
    "        EvaluationSample(\n",
    "            question=\"What is the memory capacity of DGX Spark?\",\n",
    "            ground_truth=\"DGX Spark has 128GB of unified memory shared between CPU and GPU.\",\n",
    "            expected_source=\"dgx_spark\"\n",
    "        ),\n",
    "        EvaluationSample(\n",
    "            question=\"How does LoRA reduce the number of trainable parameters?\",\n",
    "            ground_truth=\"LoRA adds low-rank matrices A and B alongside frozen original weights. Only these small matrices are trained, reducing parameters by 10-100x.\",\n",
    "            expected_source=\"lora\"\n",
    "        ),\n",
    "        EvaluationSample(\n",
    "            question=\"What is the key innovation of the transformer architecture?\",\n",
    "            ground_truth=\"The self-attention mechanism that allows each token to attend to all other tokens in the sequence, capturing long-range dependencies.\",\n",
    "            expected_source=\"transformer\"\n",
    "        ),\n",
    "        EvaluationSample(\n",
    "            question=\"What is NVFP4 quantization?\",\n",
    "            ground_truth=\"NVFP4 is NVIDIA's 4-bit floating point format that maintains more precision than integer quantization while reducing model size.\",\n",
    "            expected_source=\"quantization\"\n",
    "        ),\n",
    "        EvaluationSample(\n",
    "            question=\"What are the main components of a RAG system?\",\n",
    "            ground_truth=\"A RAG system has three main components: a retriever (finds relevant documents), a knowledge base (stores documents as vectors), and a generator (LLM that produces answers).\",\n",
    "            expected_source=\"rag\"\n",
    "        ),\n",
    "        EvaluationSample(\n",
    "            question=\"How does ChromaDB differ from FAISS?\",\n",
    "            ground_truth=\"ChromaDB provides built-in persistence, filtering, and a simpler API. FAISS offers superior performance especially with GPU acceleration but lacks built-in persistence.\",\n",
    "            expected_source=\"vector_database\"\n",
    "        ),\n",
    "        EvaluationSample(\n",
    "            question=\"What is the recommended rank (r) for LoRA fine-tuning?\",\n",
    "            ground_truth=\"Common values are 8-64. Lower ranks (8-16) for simple tasks, higher ranks (32-64) for complex domain adaptation.\",\n",
    "            expected_source=\"lora\"\n",
    "        ),\n",
    "        EvaluationSample(\n",
    "            question=\"What is the purpose of positional encoding in transformers?\",\n",
    "            ground_truth=\"Positional encoding adds information about token positions since self-attention is position-agnostic. It allows the model to understand sequence order.\",\n",
    "            expected_source=\"transformer\"\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "evaluation_dataset = create_evaluation_dataset()\n",
    "print(f\"Created evaluation dataset with {len(evaluation_dataset)} samples\")\n",
    "\n",
    "# Show samples\n",
    "for i, sample in enumerate(evaluation_dataset[:3]):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  Q: {sample.question}\")\n",
    "    print(f\"  A: {sample.ground_truth[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 Solution: Build End-to-End Evaluation Pipeline\n",
    "\n",
    "**Task**: Combine RAG system with evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "class EvaluatableRAG:\n",
    "    \"\"\"\n",
    "    RAG system designed for easy evaluation.\n",
    "    \n",
    "    Provides access to intermediate outputs:\n",
    "    - Retrieved contexts\n",
    "    - Retrieval scores\n",
    "    - Generated answer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        documents_path: str = \"../data/sample_documents\",\n",
    "        embedding_model_name: str = \"BAAI/bge-large-en-v1.5\",\n",
    "        llm_model: str = \"llama3.1:8b\"\n",
    "    ):\n",
    "        self.llm_model = llm_model\n",
    "        \n",
    "        # Load and chunk documents\n",
    "        documents = self._load_documents(documents_path)\n",
    "        chunks = self._chunk_documents(documents)\n",
    "        \n",
    "        # Create vector store\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.embedding_model = HuggingFaceEmbeddings(\n",
    "            model_name=embedding_model_name,\n",
    "            model_kwargs={\"device\": device}\n",
    "        )\n",
    "        \n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=self.embedding_model,\n",
    "            persist_directory=\"./eval_chroma_db\"\n",
    "        )\n",
    "        \n",
    "        print(f\"RAG system initialized with {len(chunks)} chunks\")\n",
    "    \n",
    "    def _load_documents(self, path: str) -> List[Document]:\n",
    "        documents = []\n",
    "        for file_path in Path(path).glob(\"*.md\"):\n",
    "            content = file_path.read_text(encoding='utf-8')\n",
    "            documents.append(Document(\n",
    "                page_content=content,\n",
    "                metadata={\"source\": file_path.name}\n",
    "            ))\n",
    "        return documents\n",
    "    \n",
    "    def _chunk_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "        return splitter.split_documents(documents)\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Retrieve documents with scores.\"\"\"\n",
    "        results = self.vectorstore.similarity_search_with_score(query, k=k)\n",
    "        return [\n",
    "            {\n",
    "                \"content\": doc.page_content,\n",
    "                \"metadata\": doc.metadata,\n",
    "                \"score\": 1 / (1 + score)  # Convert distance to similarity\n",
    "            }\n",
    "            for doc, score in results\n",
    "        ]\n",
    "    \n",
    "    def generate(self, query: str, contexts: List[str]) -> str:\n",
    "        \"\"\"Generate answer from contexts.\"\"\"\n",
    "        context_str = \"\\n\\n\".join(contexts[:5])\n",
    "        \n",
    "        prompt = f\"\"\"Answer the question based ONLY on the provided context.\n",
    "If the context doesn't contain the answer, say \"I don't have enough information.\"\n",
    "\n",
    "Context:\n",
    "{context_str}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            import ollama\n",
    "            response = ollama.chat(\n",
    "                model=self.llm_model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response[\"message\"][\"content\"]\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {e}\"\n",
    "    \n",
    "    def query(self, question: str, k: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Complete RAG query with all intermediate outputs.\"\"\"\n",
    "        # Retrieve\n",
    "        retrieved = self.retrieve(question, k=k)\n",
    "        contexts = [r[\"content\"] for r in retrieved]\n",
    "        \n",
    "        # Generate\n",
    "        answer = self.generate(question, contexts)\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"contexts\": contexts,\n",
    "            \"answer\": answer,\n",
    "            \"retrieval_scores\": [r[\"score\"] for r in retrieved],\n",
    "            \"sources\": [r[\"metadata\"].get(\"source\", \"\") for r in retrieved]\n",
    "        }\n",
    "\n",
    "# Create RAG system\n",
    "rag = EvaluatableRAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(\n",
    "    rag: EvaluatableRAG,\n",
    "    evaluator: RAGASEvaluator,\n",
    "    dataset: List[EvaluationSample]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run complete evaluation pipeline.\n",
    "    \"\"\"\n",
    "    # Enrich samples with RAG outputs\n",
    "    enriched_samples = []\n",
    "    \n",
    "    print(\"Running RAG pipeline on evaluation samples...\")\n",
    "    for sample in dataset:\n",
    "        result = rag.query(sample.question)\n",
    "        \n",
    "        enriched = EvaluationSample(\n",
    "            question=sample.question,\n",
    "            ground_truth=sample.ground_truth,\n",
    "            contexts=result[\"contexts\"],\n",
    "            answer=result[\"answer\"],\n",
    "            expected_source=sample.expected_source\n",
    "        )\n",
    "        enriched_samples.append(enriched)\n",
    "    \n",
    "    # Run evaluation\n",
    "    print(\"\\nRunning RAGAS evaluation...\")\n",
    "    results = evaluator.evaluate_batch(enriched_samples, verbose=True)\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    metrics = {\n",
    "        \"faithfulness\": np.mean([r.faithfulness for r in results]),\n",
    "        \"answer_relevancy\": np.mean([r.answer_relevancy for r in results]),\n",
    "        \"context_precision\": np.mean([r.context_precision for r in results]),\n",
    "        \"context_recall\": np.mean([r.context_recall for r in results]),\n",
    "        \"average\": np.mean([r.average for r in results])\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"metrics\": metrics,\n",
    "        \"results\": results,\n",
    "        \"samples\": enriched_samples\n",
    "    }\n",
    "\n",
    "# Run evaluation\n",
    "evaluator = RAGASEvaluator()\n",
    "eval_output = run_evaluation(rag, evaluator, evaluation_dataset[:4])  # Subset for demo\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "for metric, value in eval_output[\"metrics\"].items():\n",
    "    print(f\"{metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 Solution: Analyze Results and Identify Issues\n",
    "\n",
    "**Task**: Deep-dive into evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_evaluation_results(\n",
    "    results: List[EvaluationResult],\n",
    "    samples: List[EvaluationSample]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze evaluation results to identify issues.\n",
    "    \"\"\"\n",
    "    analysis = {\n",
    "        \"low_faithfulness\": [],  # Hallucination issues\n",
    "        \"low_relevancy\": [],  # Answer quality issues\n",
    "        \"low_precision\": [],  # Retrieval noise\n",
    "        \"low_recall\": [],  # Missing information\n",
    "        \"recommendations\": []\n",
    "    }\n",
    "    \n",
    "    threshold = 0.7  # Below this is \"low\"\n",
    "    \n",
    "    for result, sample in zip(results, samples):\n",
    "        if result.faithfulness < threshold:\n",
    "            analysis[\"low_faithfulness\"].append({\n",
    "                \"question\": sample.question,\n",
    "                \"score\": result.faithfulness,\n",
    "                \"answer_preview\": sample.answer[:100]\n",
    "            })\n",
    "        \n",
    "        if result.answer_relevancy < threshold:\n",
    "            analysis[\"low_relevancy\"].append({\n",
    "                \"question\": sample.question,\n",
    "                \"score\": result.answer_relevancy,\n",
    "                \"answer_preview\": sample.answer[:100]\n",
    "            })\n",
    "        \n",
    "        if result.context_precision < threshold:\n",
    "            analysis[\"low_precision\"].append({\n",
    "                \"question\": sample.question,\n",
    "                \"score\": result.context_precision\n",
    "            })\n",
    "        \n",
    "        if result.context_recall < threshold:\n",
    "            analysis[\"low_recall\"].append({\n",
    "                \"question\": sample.question,\n",
    "                \"score\": result.context_recall\n",
    "            })\n",
    "    \n",
    "    # Generate recommendations\n",
    "    if len(analysis[\"low_faithfulness\"]) > len(results) * 0.3:\n",
    "        analysis[\"recommendations\"].append(\n",
    "            \"High hallucination rate. Consider: stricter prompts, better context, smaller temperature.\"\n",
    "        )\n",
    "    \n",
    "    if len(analysis[\"low_precision\"]) > len(results) * 0.3:\n",
    "        analysis[\"recommendations\"].append(\n",
    "            \"Low retrieval precision. Consider: better chunking, reranking, or hybrid search.\"\n",
    "        )\n",
    "    \n",
    "    if len(analysis[\"low_recall\"]) > len(results) * 0.3:\n",
    "        analysis[\"recommendations\"].append(\n",
    "            \"Low context recall. Consider: increasing k, better embeddings, or query expansion.\"\n",
    "        )\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Analyze results\n",
    "analysis = analyze_evaluation_results(\n",
    "    eval_output[\"results\"],\n",
    "    eval_output[\"samples\"]\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nIssues Found:\")\n",
    "print(f\"  Low faithfulness: {len(analysis['low_faithfulness'])} samples\")\n",
    "print(f\"  Low relevancy: {len(analysis['low_relevancy'])} samples\")\n",
    "print(f\"  Low precision: {len(analysis['low_precision'])} samples\")\n",
    "print(f\"  Low recall: {len(analysis['low_recall'])} samples\")\n",
    "\n",
    "if analysis[\"recommendations\"]:\n",
    "    print(f\"\\nRecommendations:\")\n",
    "    for rec in analysis[\"recommendations\"]:\n",
    "        print(f\"  - {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = \"\"\"\n",
    "RAGAS QUALITY THRESHOLDS\n",
    "========================\n",
    "\n",
    "Production-Ready (all metrics >= threshold):\n",
    "-------------------------------------------\n",
    "  Faithfulness:       >= 0.85  (minimize hallucinations)\n",
    "  Answer Relevancy:   >= 0.80  (useful answers)\n",
    "  Context Precision:  >= 0.75  (efficient retrieval)\n",
    "  Context Recall:     >= 0.80  (complete information)\n",
    "\n",
    "Acceptable (development/testing):\n",
    "---------------------------------\n",
    "  Faithfulness:       >= 0.70\n",
    "  Answer Relevancy:   >= 0.65\n",
    "  Context Precision:  >= 0.60\n",
    "  Context Recall:     >= 0.65\n",
    "\n",
    "Actions by Metric:\n",
    "------------------\n",
    "Low Faithfulness:\n",
    "  - Add \"only use provided context\" to prompts\n",
    "  - Reduce temperature\n",
    "  - Add citation requirements\n",
    "\n",
    "Low Answer Relevancy:\n",
    "  - Improve prompt clarity\n",
    "  - Add examples to prompt\n",
    "  - Consider query rewriting\n",
    "\n",
    "Low Context Precision:\n",
    "  - Add reranking stage\n",
    "  - Improve chunking strategy\n",
    "  - Use hybrid search\n",
    "\n",
    "Low Context Recall:\n",
    "  - Increase retrieval k\n",
    "  - Use query expansion\n",
    "  - Improve embedding model\n",
    "\"\"\"\n",
    "\n",
    "print(thresholds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
