{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.5.4 Solutions: Hybrid Search Implementation\n",
    "\n",
    "Complete solutions for dense + sparse hybrid retrieval with RRF fusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "def load_and_chunk():\n",
    "    documents = []\n",
    "    for file_path in Path(\"../data/sample_documents\").glob(\"*.md\"):\n",
    "        content = file_path.read_text(encoding='utf-8')\n",
    "        documents.append(Document(\n",
    "            page_content=content,\n",
    "            metadata={\"source\": file_path.name, \"title\": file_path.stem}\n",
    "        ))\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    return splitter.split_documents(documents)\n",
    "\n",
    "chunks = load_and_chunk()\n",
    "print(f\"Loaded {len(chunks)} chunks\")\n",
    "\n",
    "# Load embedding model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    model_kwargs={\"device\": device},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Implement BM25 Retriever\n",
    "\n",
    "**Task**: Build a complete BM25 sparse retriever with proper tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Retriever:\n",
    "    \"\"\"\n",
    "    BM25 sparse retrieval with configurable tokenization.\n",
    "    \n",
    "    BM25 scores documents based on term frequency (TF) and \n",
    "    inverse document frequency (IDF), with saturation for TF.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        documents: List[Document],\n",
    "        k1: float = 1.5,  # TF saturation\n",
    "        b: float = 0.75,  # Length normalization\n",
    "        remove_stopwords: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize BM25 with documents.\n",
    "        \n",
    "        Args:\n",
    "            k1: Controls term frequency saturation (1.2-2.0 typical)\n",
    "            b: Controls length normalization (0.75 typical)\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        \n",
    "        # Setup tokenization\n",
    "        self._setup_tokenizer()\n",
    "        \n",
    "        # Index documents\n",
    "        self._build_index()\n",
    "    \n",
    "    def _setup_tokenizer(self):\n",
    "        \"\"\"Setup NLTK tokenizer and stopwords.\"\"\"\n",
    "        import nltk\n",
    "        try:\n",
    "            from nltk.tokenize import word_tokenize\n",
    "            from nltk.corpus import stopwords\n",
    "            self.stopwords = set(stopwords.words('english'))\n",
    "        except LookupError:\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            from nltk.tokenize import word_tokenize\n",
    "            from nltk.corpus import stopwords\n",
    "            self.stopwords = set(stopwords.words('english'))\n",
    "        \n",
    "        self.tokenize = word_tokenize\n",
    "    \n",
    "    def _preprocess(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize and optionally remove stopwords.\"\"\"\n",
    "        tokens = self.tokenize(text.lower())\n",
    "        tokens = [t for t in tokens if t.isalnum()]\n",
    "        if self.remove_stopwords:\n",
    "            tokens = [t for t in tokens if t not in self.stopwords]\n",
    "        return tokens\n",
    "    \n",
    "    def _build_index(self):\n",
    "        \"\"\"Build BM25 index.\"\"\"\n",
    "        from rank_bm25 import BM25Okapi\n",
    "        \n",
    "        # Tokenize all documents\n",
    "        self.tokenized_docs = [\n",
    "            self._preprocess(doc.page_content) \n",
    "            for doc in self.documents\n",
    "        ]\n",
    "        \n",
    "        # Create BM25 index\n",
    "        self.bm25 = BM25Okapi(\n",
    "            self.tokenized_docs,\n",
    "            k1=self.k1,\n",
    "            b=self.b\n",
    "        )\n",
    "        \n",
    "        # Compute average document length\n",
    "        self.avg_doc_len = np.mean([len(d) for d in self.tokenized_docs])\n",
    "        print(f\"BM25 index built: {len(self.documents)} docs, avg length: {self.avg_doc_len:.0f} tokens\")\n",
    "    \n",
    "    def search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"Search for documents matching the query.\"\"\"\n",
    "        tokenized_query = self._preprocess(query)\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        # Get top-k indices\n",
    "        top_indices = np.argsort(scores)[-k:][::-1]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            if scores[idx] > 0:  # Only include if there's a match\n",
    "                results.append((self.documents[idx], float(scores[idx])))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_all_scores(self, query: str) -> np.ndarray:\n",
    "        \"\"\"Get BM25 scores for all documents.\"\"\"\n",
    "        tokenized_query = self._preprocess(query)\n",
    "        return self.bm25.get_scores(tokenized_query)\n",
    "\n",
    "# Create BM25 retriever\n",
    "bm25_retriever = BM25Retriever(chunks)\n",
    "\n",
    "# Test\n",
    "test_results = bm25_retriever.search(\"memory capacity specifications\", k=3)\n",
    "print(\"\\nTest search results:\")\n",
    "for doc, score in test_results:\n",
    "    print(f\"  [{score:.3f}] {doc.page_content[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Implement Dense Retriever\n",
    "\n",
    "**Task**: Build GPU-accelerated dense retriever with embedding caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseRetriever:\n",
    "    \"\"\"\n",
    "    Dense retrieval using pre-computed embeddings.\n",
    "    \n",
    "    Optimized for DGX Spark with:\n",
    "    - GPU-accelerated embedding computation\n",
    "    - Batch processing for efficiency\n",
    "    - Optional embedding caching\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        documents: List[Document],\n",
    "        embedding_model: HuggingFaceEmbeddings,\n",
    "        cache_embeddings: bool = True\n",
    "    ):\n",
    "        self.documents = documents\n",
    "        self.embedding_model = embedding_model\n",
    "        \n",
    "        # Pre-compute embeddings\n",
    "        print(\"Computing document embeddings...\")\n",
    "        start = time.time()\n",
    "        \n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        self.embeddings = np.array(embedding_model.embed_documents(texts))\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        print(f\"Computed {len(documents)} embeddings in {elapsed:.2f}s\")\n",
    "        print(f\"Embedding shape: {self.embeddings.shape}\")\n",
    "    \n",
    "    def search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"Search for semantically similar documents.\"\"\"\n",
    "        # Get query embedding\n",
    "        query_emb = np.array(self.embedding_model.embed_query(query))\n",
    "        \n",
    "        # Compute similarities (dot product for normalized embeddings)\n",
    "        similarities = np.dot(self.embeddings, query_emb)\n",
    "        \n",
    "        # Get top-k\n",
    "        top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "        \n",
    "        return [(self.documents[i], float(similarities[i])) for i in top_indices]\n",
    "    \n",
    "    def get_all_scores(self, query: str) -> np.ndarray:\n",
    "        \"\"\"Get similarity scores for all documents.\"\"\"\n",
    "        query_emb = np.array(self.embedding_model.embed_query(query))\n",
    "        return np.dot(self.embeddings, query_emb)\n",
    "\n",
    "# Create dense retriever\n",
    "dense_retriever = DenseRetriever(chunks, embedding_model)\n",
    "\n",
    "# Test\n",
    "test_results = dense_retriever.search(\"memory capacity specifications\", k=3)\n",
    "print(\"\\nTest search results:\")\n",
    "for doc, score in test_results:\n",
    "    print(f\"  [{score:.3f}] {doc.page_content[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 Solution: Implement Hybrid Retriever with RRF\n",
    "\n",
    "**Task**: Combine dense and sparse with Reciprocal Rank Fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRetriever:\n",
    "    \"\"\"\n",
    "    Hybrid retrieval combining dense and sparse methods.\n",
    "    \n",
    "    Fusion methods:\n",
    "    - RRF (Reciprocal Rank Fusion): Robust, parameter-free\n",
    "    - Linear: Weighted combination of normalized scores\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dense_retriever: DenseRetriever,\n",
    "        sparse_retriever: BM25Retriever,\n",
    "        alpha: float = 0.5,\n",
    "        fusion_method: str = \"rrf\",\n",
    "        rrf_k: int = 60\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize hybrid retriever.\n",
    "        \n",
    "        Args:\n",
    "            alpha: Weight for dense retriever (1-alpha for sparse)\n",
    "            fusion_method: \"rrf\" or \"linear\"\n",
    "            rrf_k: RRF constant (higher = more equal weighting)\n",
    "        \"\"\"\n",
    "        self.dense = dense_retriever\n",
    "        self.sparse = sparse_retriever\n",
    "        self.alpha = alpha\n",
    "        self.fusion_method = fusion_method\n",
    "        self.rrf_k = rrf_k\n",
    "        self.documents = dense_retriever.documents\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        k: int = 5,\n",
    "        first_stage_k: int = 50\n",
    "    ) -> List[Tuple[Document, float, Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Hybrid search with fusion.\n",
    "        \n",
    "        Returns:\n",
    "            List of (Document, score, metadata) tuples\n",
    "            Metadata includes source scores and ranks\n",
    "        \"\"\"\n",
    "        if self.fusion_method == \"rrf\":\n",
    "            return self._rrf_search(query, k, first_stage_k)\n",
    "        else:\n",
    "            return self._linear_search(query, k)\n",
    "    \n",
    "    def _rrf_search(\n",
    "        self,\n",
    "        query: str,\n",
    "        k: int,\n",
    "        first_stage_k: int\n",
    "    ) -> List[Tuple[Document, float, Dict]]:\n",
    "        \"\"\"\n",
    "        Reciprocal Rank Fusion.\n",
    "        \n",
    "        RRF Score = sum(1 / (k + rank)) for each ranking\n",
    "        \n",
    "        Benefits:\n",
    "        - No need to normalize scores\n",
    "        - Robust to outliers\n",
    "        - Works well with different scoring scales\n",
    "        \"\"\"\n",
    "        # Get results from both retrievers\n",
    "        dense_results = self.dense.search(query, k=first_stage_k)\n",
    "        sparse_results = self.sparse.search(query, k=first_stage_k)\n",
    "        \n",
    "        # Build document ID mapping\n",
    "        doc_to_idx = {id(doc): i for i, doc in enumerate(self.documents)}\n",
    "        \n",
    "        # Compute RRF scores\n",
    "        rrf_scores = {}\n",
    "        \n",
    "        for rank, (doc, score) in enumerate(dense_results):\n",
    "            doc_idx = doc_to_idx.get(id(doc), id(doc))\n",
    "            if doc_idx not in rrf_scores:\n",
    "                rrf_scores[doc_idx] = {\n",
    "                    \"doc\": doc,\n",
    "                    \"rrf_score\": 0,\n",
    "                    \"dense_rank\": None,\n",
    "                    \"dense_score\": None,\n",
    "                    \"sparse_rank\": None,\n",
    "                    \"sparse_score\": None\n",
    "                }\n",
    "            rrf_scores[doc_idx][\"rrf_score\"] += self.alpha / (self.rrf_k + rank + 1)\n",
    "            rrf_scores[doc_idx][\"dense_rank\"] = rank + 1\n",
    "            rrf_scores[doc_idx][\"dense_score\"] = score\n",
    "        \n",
    "        for rank, (doc, score) in enumerate(sparse_results):\n",
    "            doc_idx = doc_to_idx.get(id(doc), id(doc))\n",
    "            if doc_idx not in rrf_scores:\n",
    "                rrf_scores[doc_idx] = {\n",
    "                    \"doc\": doc,\n",
    "                    \"rrf_score\": 0,\n",
    "                    \"dense_rank\": None,\n",
    "                    \"dense_score\": None,\n",
    "                    \"sparse_rank\": None,\n",
    "                    \"sparse_score\": None\n",
    "                }\n",
    "            rrf_scores[doc_idx][\"rrf_score\"] += (1 - self.alpha) / (self.rrf_k + rank + 1)\n",
    "            rrf_scores[doc_idx][\"sparse_rank\"] = rank + 1\n",
    "            rrf_scores[doc_idx][\"sparse_score\"] = score\n",
    "        \n",
    "        # Sort by RRF score\n",
    "        sorted_results = sorted(\n",
    "            rrf_scores.values(),\n",
    "            key=lambda x: -x[\"rrf_score\"]\n",
    "        )\n",
    "        \n",
    "        # Return top-k with metadata\n",
    "        return [\n",
    "            (\n",
    "                r[\"doc\"],\n",
    "                r[\"rrf_score\"],\n",
    "                {\n",
    "                    \"dense_rank\": r[\"dense_rank\"],\n",
    "                    \"dense_score\": r[\"dense_score\"],\n",
    "                    \"sparse_rank\": r[\"sparse_rank\"],\n",
    "                    \"sparse_score\": r[\"sparse_score\"]\n",
    "                }\n",
    "            )\n",
    "            for r in sorted_results[:k]\n",
    "        ]\n",
    "    \n",
    "    def _linear_search(\n",
    "        self,\n",
    "        query: str,\n",
    "        k: int\n",
    "    ) -> List[Tuple[Document, float, Dict]]:\n",
    "        \"\"\"\n",
    "        Linear combination of normalized scores.\n",
    "        \n",
    "        hybrid_score = alpha * norm(dense) + (1-alpha) * norm(sparse)\n",
    "        \"\"\"\n",
    "        # Get all scores\n",
    "        dense_scores = self.dense.get_all_scores(query)\n",
    "        sparse_scores = self.sparse.get_all_scores(query)\n",
    "        \n",
    "        # Min-max normalize\n",
    "        def normalize(scores):\n",
    "            min_s, max_s = scores.min(), scores.max()\n",
    "            if max_s - min_s < 1e-8:\n",
    "                return np.zeros_like(scores)\n",
    "            return (scores - min_s) / (max_s - min_s)\n",
    "        \n",
    "        dense_norm = normalize(dense_scores)\n",
    "        sparse_norm = normalize(sparse_scores)\n",
    "        \n",
    "        # Combine\n",
    "        hybrid_scores = self.alpha * dense_norm + (1 - self.alpha) * sparse_norm\n",
    "        \n",
    "        # Get top-k\n",
    "        top_indices = np.argsort(hybrid_scores)[-k:][::-1]\n",
    "        \n",
    "        return [\n",
    "            (\n",
    "                self.documents[i],\n",
    "                float(hybrid_scores[i]),\n",
    "                {\n",
    "                    \"dense_score\": float(dense_scores[i]),\n",
    "                    \"sparse_score\": float(sparse_scores[i]),\n",
    "                    \"dense_norm\": float(dense_norm[i]),\n",
    "                    \"sparse_norm\": float(sparse_norm[i])\n",
    "                }\n",
    "            )\n",
    "            for i in top_indices\n",
    "        ]\n",
    "\n",
    "# Create hybrid retriever\n",
    "hybrid_retriever = HybridRetriever(\n",
    "    dense_retriever,\n",
    "    bm25_retriever,\n",
    "    alpha=0.5,\n",
    "    fusion_method=\"rrf\"\n",
    ")\n",
    "\n",
    "print(\"Hybrid retriever created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 Solution: Compare Retrieval Methods\n",
    "\n",
    "**Task**: Benchmark dense vs sparse vs hybrid retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_retrieval_methods(\n",
    "    test_queries: List[Dict[str, Any]],\n",
    "    dense: DenseRetriever,\n",
    "    sparse: BM25Retriever,\n",
    "    hybrid: HybridRetriever,\n",
    "    k: int = 5\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Compare retrieval methods on test queries.\n",
    "    \n",
    "    Metrics:\n",
    "    - Recall@k: Did we find the expected source?\n",
    "    - MRR: Mean Reciprocal Rank\n",
    "    - Latency: Query time\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"Dense\": {\"recalls\": [], \"mrrs\": [], \"latencies\": []},\n",
    "        \"Sparse (BM25)\": {\"recalls\": [], \"mrrs\": [], \"latencies\": []},\n",
    "        \"Hybrid (RRF)\": {\"recalls\": [], \"mrrs\": [], \"latencies\": []}\n",
    "    }\n",
    "    \n",
    "    for query_data in test_queries:\n",
    "        query = query_data[\"query\"]\n",
    "        expected = query_data.get(\"expected_source\", \"\")\n",
    "        \n",
    "        # Dense retrieval\n",
    "        start = time.time()\n",
    "        dense_results = dense.search(query, k=k)\n",
    "        results[\"Dense\"][\"latencies\"].append(time.time() - start)\n",
    "        \n",
    "        # Sparse retrieval\n",
    "        start = time.time()\n",
    "        sparse_results = sparse.search(query, k=k)\n",
    "        results[\"Sparse (BM25)\"][\"latencies\"].append(time.time() - start)\n",
    "        \n",
    "        # Hybrid retrieval\n",
    "        start = time.time()\n",
    "        hybrid_results = hybrid.search(query, k=k)\n",
    "        results[\"Hybrid (RRF)\"][\"latencies\"].append(time.time() - start)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        for method_name, method_results in [\n",
    "            (\"Dense\", dense_results),\n",
    "            (\"Sparse (BM25)\", sparse_results),\n",
    "            (\"Hybrid (RRF)\", [(r[0], r[1]) for r in hybrid_results])\n",
    "        ]:\n",
    "            # Check recall\n",
    "            found = False\n",
    "            mrr = 0\n",
    "            for i, (doc, _) in enumerate(method_results):\n",
    "                source = doc.metadata.get(\"source\", \"\")\n",
    "                if expected in source:\n",
    "                    found = True\n",
    "                    mrr = 1 / (i + 1)\n",
    "                    break\n",
    "            \n",
    "            results[method_name][\"recalls\"].append(1.0 if found else 0.0)\n",
    "            results[method_name][\"mrrs\"].append(mrr)\n",
    "    \n",
    "    # Aggregate\n",
    "    summary = {}\n",
    "    for method, metrics in results.items():\n",
    "        summary[method] = {\n",
    "            \"recall@k\": np.mean(metrics[\"recalls\"]),\n",
    "            \"mrr\": np.mean(metrics[\"mrrs\"]),\n",
    "            \"latency_ms\": np.mean(metrics[\"latencies\"]) * 1000\n",
    "        }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Test queries with expected sources\n",
    "test_queries = [\n",
    "    {\"query\": \"What is the memory capacity of DGX Spark?\", \"expected_source\": \"dgx_spark\"},\n",
    "    {\"query\": \"How does LoRA reduce trainable parameters?\", \"expected_source\": \"lora\"},\n",
    "    {\"query\": \"Explain the attention mechanism\", \"expected_source\": \"transformer\"},\n",
    "    {\"query\": \"What quantization methods exist?\", \"expected_source\": \"quantization\"},\n",
    "    {\"query\": \"vector database comparison\", \"expected_source\": \"vector_database\"},\n",
    "    {\"query\": \"128GB unified memory\", \"expected_source\": \"dgx_spark\"},  # Keyword-heavy\n",
    "    {\"query\": \"efficient model training techniques\", \"expected_source\": \"lora\"},  # Semantic\n",
    "    {\"query\": \"NVFP4 vs AWQ quantization\", \"expected_source\": \"quantization\"}  # Mixed\n",
    "]\n",
    "\n",
    "# Run comparison\n",
    "comparison = compare_retrieval_methods(\n",
    "    test_queries,\n",
    "    dense_retriever,\n",
    "    bm25_retriever,\n",
    "    hybrid_retriever\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*70)\n",
    "print(\"RETRIEVAL METHOD COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Method':<20} {'Recall@5':<12} {'MRR':<12} {'Latency (ms)':<12}\")\n",
    "print(\"-\"*56)\n",
    "\n",
    "for method, metrics in comparison.items():\n",
    "    print(f\"{method:<20} {metrics['recall@k']:<12.3f} {metrics['mrr']:<12.3f} {metrics['latency_ms']:<12.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 Solution: Optimize Alpha Parameter\n",
    "\n",
    "**Task**: Find optimal balance between dense and sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_alpha(\n",
    "    test_queries: List[Dict],\n",
    "    dense: DenseRetriever,\n",
    "    sparse: BM25Retriever,\n",
    "    alpha_values: List[float] = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    ") -> Dict[float, float]:\n",
    "    \"\"\"Find optimal alpha for hybrid search.\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for alpha in alpha_values:\n",
    "        hybrid = HybridRetriever(dense, sparse, alpha=alpha, fusion_method=\"rrf\")\n",
    "        \n",
    "        recalls = []\n",
    "        for query_data in test_queries:\n",
    "            query = query_data[\"query\"]\n",
    "            expected = query_data.get(\"expected_source\", \"\")\n",
    "            \n",
    "            search_results = hybrid.search(query, k=5)\n",
    "            \n",
    "            found = any(\n",
    "                expected in doc.metadata.get(\"source\", \"\")\n",
    "                for doc, _, _ in search_results\n",
    "            )\n",
    "            recalls.append(1.0 if found else 0.0)\n",
    "        \n",
    "        results[alpha] = np.mean(recalls)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run optimization\n",
    "alpha_results = optimize_alpha(\n",
    "    test_queries,\n",
    "    dense_retriever,\n",
    "    bm25_retriever,\n",
    "    alpha_values=[0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ALPHA OPTIMIZATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Alpha':<10} {'Dense Weight':<15} {'Sparse Weight':<15} {'Recall@5':<10}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "best_alpha = max(alpha_results, key=alpha_results.get)\n",
    "for alpha, recall in sorted(alpha_results.items()):\n",
    "    marker = \" <-- BEST\" if alpha == best_alpha else \"\"\n",
    "    print(f\"{alpha:<10.2f} {alpha:<15.2f} {1-alpha:<15.2f} {recall:<10.3f}{marker}\")\n",
    "\n",
    "print(f\"\\nOptimal alpha: {best_alpha} (Recall@5: {alpha_results[best_alpha]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insights = \"\"\"\n",
    "HYBRID SEARCH INSIGHTS\n",
    "======================\n",
    "\n",
    "1. WHEN TO USE EACH METHOD\n",
    "   - Dense: Semantic similarity, paraphrase matching\n",
    "   - Sparse (BM25): Exact keyword matching, technical terms\n",
    "   - Hybrid: General purpose, unknown query distribution\n",
    "\n",
    "2. RRF vs LINEAR FUSION\n",
    "   - RRF: More robust, doesn't need score calibration\n",
    "   - Linear: Faster, but requires careful normalization\n",
    "   - Recommendation: Use RRF for production\n",
    "\n",
    "3. ALPHA TUNING GUIDELINES\n",
    "   - alpha=0.5: Good default for balanced queries\n",
    "   - alpha>0.5: Prioritize semantic understanding\n",
    "   - alpha<0.5: Prioritize keyword matching\n",
    "   - Tune based on your query distribution!\n",
    "\n",
    "4. DGX SPARK OPTIMIZATION\n",
    "   - Pre-compute dense embeddings on GPU\n",
    "   - BM25 runs efficiently on CPU\n",
    "   - Batch queries for better GPU utilization\n",
    "\n",
    "5. PRODUCTION CONSIDERATIONS\n",
    "   - Cache both embedding and BM25 results\n",
    "   - Use first_stage_k to control candidate pool\n",
    "   - Monitor which method contributes more to wins\n",
    "\"\"\"\n",
    "\n",
    "print(insights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
