{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.5.3 Solutions: Vector Database Comparison\n",
    "\n",
    "Complete solutions with GPU benchmarks and production recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and chunk documents\n",
    "def prepare_data():\n",
    "    documents = []\n",
    "    for file_path in Path(\"../data/sample_documents\").glob(\"*.md\"):\n",
    "        content = file_path.read_text(encoding='utf-8')\n",
    "        documents.append(Document(\n",
    "            page_content=content,\n",
    "            metadata={\"source\": file_path.name}\n",
    "        ))\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    return chunks\n",
    "\n",
    "chunks = prepare_data()\n",
    "print(f\"Prepared {len(chunks)} chunks for testing\")\n",
    "\n",
    "# Load embedding model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    model_kwargs={\"device\": device},\n",
    "    encode_kwargs={\"normalize_embeddings\": True, \"batch_size\": 64}\n",
    ")\n",
    "print(f\"Embedding model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Implement Vector Store Wrappers\n",
    "\n",
    "**Task**: Create unified interface for ChromaDB, FAISS, and Qdrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStoreWrapper:\n",
    "    \"\"\"Unified interface for vector store operations.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.store = None\n",
    "        self.build_time = 0\n",
    "    \n",
    "    def build(self, chunks: List[Document], embedding_model) -> float:\n",
    "        \"\"\"Build the vector store. Returns build time in seconds.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"Search for similar documents. Returns (doc, score) tuples.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class ChromaWrapper(VectorStoreWrapper):\n",
    "    \"\"\"ChromaDB wrapper.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"ChromaDB\")\n",
    "        self.persist_dir = None\n",
    "    \n",
    "    def build(self, chunks: List[Document], embedding_model) -> float:\n",
    "        self.persist_dir = tempfile.mkdtemp()\n",
    "        \n",
    "        start = time.time()\n",
    "        self.store = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=embedding_model,\n",
    "            persist_directory=self.persist_dir\n",
    "        )\n",
    "        self.build_time = time.time() - start\n",
    "        return self.build_time\n",
    "    \n",
    "    def search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:\n",
    "        return self.store.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    def cleanup(self):\n",
    "        if self.persist_dir and Path(self.persist_dir).exists():\n",
    "            shutil.rmtree(self.persist_dir)\n",
    "\n",
    "\n",
    "class FAISSWrapper(VectorStoreWrapper):\n",
    "    \"\"\"FAISS wrapper with GPU support.\"\"\"\n",
    "    \n",
    "    def __init__(self, use_gpu: bool = True):\n",
    "        super().__init__(f\"FAISS-{'GPU' if use_gpu else 'CPU'}\")\n",
    "        self.use_gpu = use_gpu\n",
    "    \n",
    "    def build(self, chunks: List[Document], embedding_model) -> float:\n",
    "        try:\n",
    "            from langchain_community.vectorstores import FAISS\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Install faiss: pip install faiss-gpu\")\n",
    "        \n",
    "        start = time.time()\n",
    "        self.store = FAISS.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=embedding_model\n",
    "        )\n",
    "        \n",
    "        # Move to GPU if requested\n",
    "        if self.use_gpu and torch.cuda.is_available():\n",
    "            try:\n",
    "                import faiss\n",
    "                # Get the index\n",
    "                cpu_index = self.store.index\n",
    "                # Create GPU resource\n",
    "                res = faiss.StandardGpuResources()\n",
    "                # Move index to GPU\n",
    "                gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)\n",
    "                self.store.index = gpu_index\n",
    "                print(\"  FAISS index moved to GPU\")\n",
    "            except Exception as e:\n",
    "                print(f\"  GPU transfer failed, using CPU: {e}\")\n",
    "        \n",
    "        self.build_time = time.time() - start\n",
    "        return self.build_time\n",
    "    \n",
    "    def search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:\n",
    "        results = self.store.similarity_search_with_score(query, k=k)\n",
    "        return results\n",
    "\n",
    "\n",
    "class QdrantWrapper(VectorStoreWrapper):\n",
    "    \"\"\"Qdrant wrapper.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"Qdrant\")\n",
    "        self.collection_name = \"benchmark_collection\"\n",
    "    \n",
    "    def build(self, chunks: List[Document], embedding_model) -> float:\n",
    "        try:\n",
    "            from langchain_qdrant import Qdrant\n",
    "            from qdrant_client import QdrantClient\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Install qdrant: pip install langchain-qdrant qdrant-client\")\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        # Use in-memory storage for benchmarking\n",
    "        self.store = Qdrant.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=embedding_model,\n",
    "            location=\":memory:\",\n",
    "            collection_name=self.collection_name\n",
    "        )\n",
    "        \n",
    "        self.build_time = time.time() - start\n",
    "        return self.build_time\n",
    "    \n",
    "    def search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:\n",
    "        results = self.store.similarity_search_with_score(query, k=k)\n",
    "        return results\n",
    "\n",
    "print(\"Vector store wrappers defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Comprehensive Benchmark\n",
    "\n",
    "**Task**: Compare build time, query latency, and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_vector_store(\n",
    "    wrapper: VectorStoreWrapper,\n",
    "    chunks: List[Document],\n",
    "    embedding_model,\n",
    "    test_queries: List[str],\n",
    "    num_iterations: int = 10\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Comprehensive benchmark for a vector store.\n",
    "    \n",
    "    Metrics:\n",
    "    - Build time\n",
    "    - Query latency (mean, p50, p95, p99)\n",
    "    - Memory usage\n",
    "    \"\"\"\n",
    "    import psutil\n",
    "    import gc\n",
    "    \n",
    "    # Measure memory before\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gpu_mem_before = torch.cuda.memory_allocated()\n",
    "    cpu_mem_before = psutil.Process().memory_info().rss\n",
    "    \n",
    "    # Build\n",
    "    print(f\"  Building {wrapper.name}...\")\n",
    "    build_time = wrapper.build(chunks, embedding_model)\n",
    "    \n",
    "    # Measure memory after build\n",
    "    cpu_mem_after = psutil.Process().memory_info().rss\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_mem_after = torch.cuda.memory_allocated()\n",
    "        gpu_mem_used = (gpu_mem_after - gpu_mem_before) / (1024**2)\n",
    "    else:\n",
    "        gpu_mem_used = 0\n",
    "    \n",
    "    cpu_mem_used = (cpu_mem_after - cpu_mem_before) / (1024**2)\n",
    "    \n",
    "    # Query latency benchmark\n",
    "    print(f\"  Benchmarking queries...\")\n",
    "    latencies = []\n",
    "    \n",
    "    # Warmup\n",
    "    for query in test_queries[:3]:\n",
    "        _ = wrapper.search(query, k=5)\n",
    "    \n",
    "    # Actual benchmark\n",
    "    for _ in range(num_iterations):\n",
    "        for query in test_queries:\n",
    "            start = time.time()\n",
    "            _ = wrapper.search(query, k=5)\n",
    "            latencies.append(time.time() - start)\n",
    "    \n",
    "    latencies = np.array(latencies) * 1000  # Convert to ms\n",
    "    \n",
    "    # Cleanup\n",
    "    wrapper.cleanup()\n",
    "    \n",
    "    return {\n",
    "        \"name\": wrapper.name,\n",
    "        \"build_time_s\": build_time,\n",
    "        \"query_latency_mean_ms\": np.mean(latencies),\n",
    "        \"query_latency_p50_ms\": np.percentile(latencies, 50),\n",
    "        \"query_latency_p95_ms\": np.percentile(latencies, 95),\n",
    "        \"query_latency_p99_ms\": np.percentile(latencies, 99),\n",
    "        \"cpu_memory_mb\": cpu_mem_used,\n",
    "        \"gpu_memory_mb\": gpu_mem_used,\n",
    "        \"throughput_qps\": 1000 / np.mean(latencies)  # Queries per second\n",
    "    }\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is the memory capacity of DGX Spark?\",\n",
    "    \"How does LoRA reduce trainable parameters?\",\n",
    "    \"Explain transformer attention\",\n",
    "    \"What quantization methods are available?\",\n",
    "    \"How do vector databases work?\",\n",
    "    \"What is NVFP4 quantization?\",\n",
    "    \"Explain RAG architecture\",\n",
    "    \"What is the difference between ChromaDB and FAISS?\"\n",
    "]\n",
    "\n",
    "# Run benchmarks\n",
    "print(\"Running comprehensive benchmarks...\\n\")\n",
    "\n",
    "wrappers = [\n",
    "    ChromaWrapper(),\n",
    "    FAISSWrapper(use_gpu=True),\n",
    "    FAISSWrapper(use_gpu=False),\n",
    "]\n",
    "\n",
    "# Try to add Qdrant if available\n",
    "try:\n",
    "    from langchain_qdrant import Qdrant\n",
    "    wrappers.append(QdrantWrapper())\n",
    "except ImportError:\n",
    "    print(\"Qdrant not installed, skipping...\")\n",
    "\n",
    "results = []\n",
    "for wrapper in wrappers:\n",
    "    try:\n",
    "        result = benchmark_vector_store(wrapper, chunks, embedding_model, test_queries)\n",
    "        results.append(result)\n",
    "        print(f\"  {wrapper.name}: {result['query_latency_mean_ms']:.2f}ms avg latency\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {wrapper.name} failed: {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"=\"*100)\n",
    "print(\"VECTOR DATABASE BENCHMARK RESULTS\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Database':<15} {'Build (s)':<12} {'Latency (ms)':<15} {'P95 (ms)':<12} {'QPS':<10} {'Memory (MB)':<12}\")\n",
    "print(\"-\"*76)\n",
    "\n",
    "for r in sorted(results, key=lambda x: x['query_latency_mean_ms']):\n",
    "    print(f\"{r['name']:<15} {r['build_time_s']:<12.2f} {r['query_latency_mean_ms']:<15.2f} {r['query_latency_p95_ms']:<12.2f} {r['throughput_qps']:<10.0f} {r['cpu_memory_mb']:<12.1f}\")\n",
    "\n",
    "# Find best in each category\n",
    "if results:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CATEGORY WINNERS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    fastest_build = min(results, key=lambda x: x['build_time_s'])\n",
    "    lowest_latency = min(results, key=lambda x: x['query_latency_mean_ms'])\n",
    "    highest_qps = max(results, key=lambda x: x['throughput_qps'])\n",
    "    lowest_memory = min(results, key=lambda x: x['cpu_memory_mb'])\n",
    "    \n",
    "    print(f\"Fastest Build:    {fastest_build['name']} ({fastest_build['build_time_s']:.2f}s)\")\n",
    "    print(f\"Lowest Latency:   {lowest_latency['name']} ({lowest_latency['query_latency_mean_ms']:.2f}ms)\")\n",
    "    print(f\"Highest QPS:      {highest_qps['name']} ({highest_qps['throughput_qps']:.0f} qps)\")\n",
    "    print(f\"Lowest Memory:    {lowest_memory['name']} ({lowest_memory['cpu_memory_mb']:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 Solution: Scaling Analysis\n",
    "\n",
    "**Task**: Test how each database scales with document count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_analysis(\n",
    "    base_chunks: List[Document],\n",
    "    embedding_model,\n",
    "    test_queries: List[str],\n",
    "    scale_factors: List[int] = [1, 2, 5, 10]\n",
    ") -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Analyze how databases scale with increasing data.\n",
    "    \"\"\"\n",
    "    scaling_results = {}\n",
    "    \n",
    "    for wrapper_class in [ChromaWrapper, lambda: FAISSWrapper(use_gpu=True)]:\n",
    "        wrapper = wrapper_class()\n",
    "        scaling_results[wrapper.name] = []\n",
    "        \n",
    "        for factor in scale_factors:\n",
    "            # Create scaled dataset\n",
    "            scaled_chunks = base_chunks * factor\n",
    "            \n",
    "            # Add unique IDs to prevent deduplication\n",
    "            for i, chunk in enumerate(scaled_chunks):\n",
    "                chunk.metadata = {**chunk.metadata, \"unique_id\": i}\n",
    "            \n",
    "            print(f\"  {wrapper.name} at {len(scaled_chunks)} chunks...\")\n",
    "            \n",
    "            try:\n",
    "                result = benchmark_vector_store(\n",
    "                    wrapper_class(),\n",
    "                    scaled_chunks,\n",
    "                    embedding_model,\n",
    "                    test_queries,\n",
    "                    num_iterations=3\n",
    "                )\n",
    "                result['num_chunks'] = len(scaled_chunks)\n",
    "                scaling_results[wrapper.name].append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"    Failed: {e}\")\n",
    "    \n",
    "    return scaling_results\n",
    "\n",
    "print(\"Running scaling analysis...\\n\")\n",
    "scaling = scaling_analysis(chunks, embedding_model, test_queries[:3], scale_factors=[1, 2, 3])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SCALING ANALYSIS RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for db_name, results in scaling.items():\n",
    "    print(f\"\\n{db_name}:\")\n",
    "    print(f\"{'Chunks':<10} {'Build (s)':<12} {'Latency (ms)':<15} {'QPS':<10}\")\n",
    "    print(\"-\"*47)\n",
    "    for r in results:\n",
    "        print(f\"{r['num_chunks']:<10} {r['build_time_s']:<12.2f} {r['query_latency_mean_ms']:<15.2f} {r['throughput_qps']:<10.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = \"\"\"\n",
    "VECTOR DATABASE SELECTION GUIDE\n",
    "================================\n",
    "\n",
    "CHROMADB\n",
    "--------\n",
    "Best for: Development, prototyping, small-medium datasets\n",
    "Strengths:\n",
    "  - Easy setup, no external dependencies\n",
    "  - Built-in persistence\n",
    "  - Good filtering support\n",
    "Weaknesses:\n",
    "  - Slower at scale (>100k vectors)\n",
    "  - No native GPU support\n",
    "Use when: Prototyping, datasets < 100k vectors\n",
    "\n",
    "FAISS (GPU)\n",
    "-----------\n",
    "Best for: High-performance production, large datasets\n",
    "Strengths:\n",
    "  - Extremely fast search (GPU accelerated)\n",
    "  - Excellent scaling characteristics\n",
    "  - Multiple index types (IVF, HNSW, PQ)\n",
    "Weaknesses:\n",
    "  - Requires GPU for best performance\n",
    "  - No built-in persistence (need to save/load)\n",
    "  - Limited filtering\n",
    "Use when: Performance critical, GPU available, > 1M vectors\n",
    "\n",
    "QDRANT\n",
    "------\n",
    "Best for: Production with complex filtering needs\n",
    "Strengths:\n",
    "  - Excellent filtering performance\n",
    "  - Good horizontal scaling\n",
    "  - Rich query language\n",
    "Weaknesses:\n",
    "  - Slightly higher latency than FAISS\n",
    "  - Requires running server for persistence\n",
    "Use when: Need complex filters, production deployment\n",
    "\n",
    "DGX SPARK RECOMMENDATIONS\n",
    "=========================\n",
    "- Use FAISS-GPU for maximum throughput\n",
    "- ChromaDB for development/debugging\n",
    "- Pre-compute embeddings with batch_size=64\n",
    "- Monitor GPU memory with torch.cuda.memory_allocated()\n",
    "\"\"\"\n",
    "\n",
    "print(recommendations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
