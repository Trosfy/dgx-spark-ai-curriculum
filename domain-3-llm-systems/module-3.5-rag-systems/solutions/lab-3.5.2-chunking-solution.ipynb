{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.5.2 Solutions: Chunking Strategies for RAG\n",
    "\n",
    "Complete solutions with performance analysis and recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Callable\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    "    TokenTextSplitter\n",
    ")\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample documents\n",
    "def load_documents():\n",
    "    documents = []\n",
    "    data_path = Path(\"../data/sample_documents\")\n",
    "    \n",
    "    for file_path in data_path.glob(\"*.md\"):\n",
    "        content = file_path.read_text(encoding='utf-8')\n",
    "        doc = Document(\n",
    "            page_content=content,\n",
    "            metadata={\"source\": str(file_path), \"filename\": file_path.name}\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "documents = load_documents()\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "\n",
    "# Combine all text for analysis\n",
    "full_text = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "print(f\"Total text length: {len(full_text):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Implement All Chunking Strategies\n",
    "\n",
    "**Task**: Implement fixed-size, recursive, sentence-based, and semantic chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkingStrategies:\n",
    "    \"\"\"Collection of chunking strategies with consistent interface.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def fixed_size(documents: List[Document], chunk_size: int = 500, overlap: int = 0) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Fixed-size chunking - simple but may break mid-sentence.\n",
    "        \n",
    "        Best for: Uniform processing, simple requirements\n",
    "        Weakness: Ignores semantic boundaries\n",
    "        \"\"\"\n",
    "        splitter = CharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=overlap,\n",
    "            separator=\"\"\n",
    "        )\n",
    "        return splitter.split_documents(documents)\n",
    "    \n",
    "    @staticmethod\n",
    "    def recursive(documents: List[Document], chunk_size: int = 500, overlap: int = 100) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Recursive chunking - respects document structure.\n",
    "        \n",
    "        Best for: Structured documents (Markdown, code)\n",
    "        Strength: Preserves semantic units\n",
    "        \"\"\"\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=overlap,\n",
    "            separators=[\"\\n## \", \"\\n### \", \"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    "        )\n",
    "        return splitter.split_documents(documents)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sentence_based(documents: List[Document], sentences_per_chunk: int = 5) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Sentence-based chunking - natural language units.\n",
    "        \n",
    "        Best for: Prose text, Q&A content\n",
    "        Strength: Complete thoughts preserved\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        chunks = []\n",
    "        for doc in documents:\n",
    "            # Split into sentences\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', doc.page_content)\n",
    "            \n",
    "            # Group sentences\n",
    "            for i in range(0, len(sentences), sentences_per_chunk):\n",
    "                chunk_sentences = sentences[i:i + sentences_per_chunk]\n",
    "                chunk_text = ' '.join(chunk_sentences)\n",
    "                \n",
    "                if chunk_text.strip():\n",
    "                    chunks.append(Document(\n",
    "                        page_content=chunk_text,\n",
    "                        metadata={\n",
    "                            **doc.metadata,\n",
    "                            \"chunk_type\": \"sentence\",\n",
    "                            \"sentences\": len(chunk_sentences)\n",
    "                        }\n",
    "                    ))\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def semantic(\n",
    "        documents: List[Document],\n",
    "        embedding_model: HuggingFaceEmbeddings,\n",
    "        similarity_threshold: float = 0.75,\n",
    "        max_chunk_size: int = 1000\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Semantic chunking - groups by meaning.\n",
    "        \n",
    "        Best for: Complex documents, varied content\n",
    "        Strength: Highest semantic coherence\n",
    "        Weakness: Computationally expensive\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        all_chunks = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            # Split into sentences\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', doc.page_content)\n",
    "            sentences = [s.strip() for s in sentences if s.strip()]\n",
    "            \n",
    "            if len(sentences) < 2:\n",
    "                all_chunks.append(doc)\n",
    "                continue\n",
    "            \n",
    "            # Get sentence embeddings\n",
    "            embeddings = np.array(embedding_model.embed_documents(sentences))\n",
    "            \n",
    "            # Group similar consecutive sentences\n",
    "            current_chunk = [sentences[0]]\n",
    "            current_emb = embeddings[0]\n",
    "            \n",
    "            for i in range(1, len(sentences)):\n",
    "                # Compute similarity\n",
    "                similarity = np.dot(current_emb, embeddings[i]) / (\n",
    "                    np.linalg.norm(current_emb) * np.linalg.norm(embeddings[i]) + 1e-8\n",
    "                )\n",
    "                \n",
    "                chunk_text = ' '.join(current_chunk)\n",
    "                \n",
    "                # Start new chunk if dissimilar or too large\n",
    "                if similarity < similarity_threshold or len(chunk_text) > max_chunk_size:\n",
    "                    all_chunks.append(Document(\n",
    "                        page_content=chunk_text,\n",
    "                        metadata={\n",
    "                            **doc.metadata,\n",
    "                            \"chunk_type\": \"semantic\",\n",
    "                            \"sentences\": len(current_chunk)\n",
    "                        }\n",
    "                    ))\n",
    "                    current_chunk = [sentences[i]]\n",
    "                    current_emb = embeddings[i]\n",
    "                else:\n",
    "                    current_chunk.append(sentences[i])\n",
    "                    # Update centroid\n",
    "                    chunk_indices = list(range(i - len(current_chunk) + 1, i + 1))\n",
    "                    current_emb = np.mean(embeddings[chunk_indices], axis=0)\n",
    "            \n",
    "            # Don't forget last chunk\n",
    "            if current_chunk:\n",
    "                all_chunks.append(Document(\n",
    "                    page_content=' '.join(current_chunk),\n",
    "                    metadata={\n",
    "                        **doc.metadata,\n",
    "                        \"chunk_type\": \"semantic\",\n",
    "                        \"sentences\": len(current_chunk)\n",
    "                    }\n",
    "                ))\n",
    "        \n",
    "        return all_chunks\n",
    "\n",
    "print(\"ChunkingStrategies class defined with 4 strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Compare Chunk Statistics\n",
    "\n",
    "**Task**: Analyze chunk size distribution for each strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_chunks(chunks: List[Document], strategy_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"Compute comprehensive chunk statistics.\"\"\"\n",
    "    sizes = [len(c.page_content) for c in chunks]\n",
    "    \n",
    "    return {\n",
    "        \"strategy\": strategy_name,\n",
    "        \"num_chunks\": len(chunks),\n",
    "        \"mean_size\": np.mean(sizes),\n",
    "        \"std_size\": np.std(sizes),\n",
    "        \"min_size\": np.min(sizes),\n",
    "        \"max_size\": np.max(sizes),\n",
    "        \"median_size\": np.median(sizes),\n",
    "        \"size_variance_coef\": np.std(sizes) / np.mean(sizes)  # Lower = more uniform\n",
    "    }\n",
    "\n",
    "# Create chunks with each strategy\n",
    "fixed_chunks = ChunkingStrategies.fixed_size(documents, chunk_size=500)\n",
    "recursive_chunks = ChunkingStrategies.recursive(documents, chunk_size=500, overlap=100)\n",
    "sentence_chunks = ChunkingStrategies.sentence_based(documents, sentences_per_chunk=5)\n",
    "\n",
    "# Load embedding model for semantic chunking\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    model_kwargs={\"device\": device}\n",
    ")\n",
    "\n",
    "print(\"Computing semantic chunks (this may take a moment)...\")\n",
    "semantic_chunks = ChunkingStrategies.semantic(documents, embedding_model)\n",
    "\n",
    "# Analyze all strategies\n",
    "strategies = [\n",
    "    (fixed_chunks, \"Fixed-Size\"),\n",
    "    (recursive_chunks, \"Recursive\"),\n",
    "    (sentence_chunks, \"Sentence-Based\"),\n",
    "    (semantic_chunks, \"Semantic\")\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHUNK STATISTICS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "for chunks, name in strategies:\n",
    "    stats = analyze_chunks(chunks, name)\n",
    "    results.append(stats)\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Chunks: {stats['num_chunks']}\")\n",
    "    print(f\"  Mean size: {stats['mean_size']:.0f} chars\")\n",
    "    print(f\"  Std dev: {stats['std_size']:.0f} chars\")\n",
    "    print(f\"  Range: {stats['min_size']}-{stats['max_size']} chars\")\n",
    "    print(f\"  Variance coef: {stats['size_variance_coef']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 Solution: Benchmark Retrieval Quality\n",
    "\n",
    "**Task**: Compare retrieval performance across chunking strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_retrieval(\n",
    "    chunks: List[Document],\n",
    "    embedding_model: HuggingFaceEmbeddings,\n",
    "    test_queries: List[Dict[str, Any]],\n",
    "    k: int = 5\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Benchmark retrieval quality for a chunking strategy.\n",
    "    \n",
    "    Metrics:\n",
    "    - Recall@k: Did we find the expected source?\n",
    "    - Avg similarity: How relevant are results?\n",
    "    - Latency: How fast is retrieval?\n",
    "    \"\"\"\n",
    "    import shutil\n",
    "    import tempfile\n",
    "    \n",
    "    # Create temporary vector store\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=embedding_model,\n",
    "            persist_directory=tmpdir\n",
    "        )\n",
    "        \n",
    "        recalls = []\n",
    "        similarities = []\n",
    "        latencies = []\n",
    "        \n",
    "        for query_data in test_queries:\n",
    "            query = query_data[\"query\"]\n",
    "            expected = query_data.get(\"expected_source\", \"\")\n",
    "            \n",
    "            # Time the retrieval\n",
    "            start = time.time()\n",
    "            results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "            latency = time.time() - start\n",
    "            latencies.append(latency)\n",
    "            \n",
    "            # Check if expected source was found\n",
    "            found = False\n",
    "            for doc, score in results:\n",
    "                if expected in doc.metadata.get(\"source\", \"\"):\n",
    "                    found = True\n",
    "                    break\n",
    "            recalls.append(1.0 if found else 0.0)\n",
    "            \n",
    "            # Average similarity (convert distance to similarity)\n",
    "            avg_sim = np.mean([1 / (1 + score) for _, score in results])\n",
    "            similarities.append(avg_sim)\n",
    "    \n",
    "    return {\n",
    "        \"recall@k\": np.mean(recalls),\n",
    "        \"avg_similarity\": np.mean(similarities),\n",
    "        \"avg_latency_ms\": np.mean(latencies) * 1000\n",
    "    }\n",
    "\n",
    "# Define test queries\n",
    "test_queries = [\n",
    "    {\"query\": \"What is the memory capacity of DGX Spark?\", \"expected_source\": \"dgx_spark\"},\n",
    "    {\"query\": \"How does LoRA reduce trainable parameters?\", \"expected_source\": \"lora\"},\n",
    "    {\"query\": \"Explain the attention mechanism in transformers\", \"expected_source\": \"transformer\"},\n",
    "    {\"query\": \"What quantization methods are available?\", \"expected_source\": \"quantization\"},\n",
    "    {\"query\": \"How do vector databases work?\", \"expected_source\": \"vector_database\"}\n",
    "]\n",
    "\n",
    "print(\"Benchmarking retrieval quality for each strategy...\\n\")\n",
    "\n",
    "benchmark_results = []\n",
    "for chunks, name in strategies:\n",
    "    print(f\"Testing {name}...\")\n",
    "    metrics = benchmark_retrieval(chunks, embedding_model, test_queries)\n",
    "    metrics[\"strategy\"] = name\n",
    "    benchmark_results.append(metrics)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RETRIEVAL QUALITY COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Strategy':<20} {'Recall@5':<12} {'Avg Similarity':<16} {'Latency (ms)':<12}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for result in benchmark_results:\n",
    "    print(f\"{result['strategy']:<20} {result['recall@k']:<12.3f} {result['avg_similarity']:<16.3f} {result['avg_latency_ms']:<12.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 Solution: Optimal Parameters\n",
    "\n",
    "**Task**: Find optimal chunk size and overlap for the recursive strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_parameters(\n",
    "    documents: List[Document],\n",
    "    embedding_model: HuggingFaceEmbeddings,\n",
    "    test_queries: List[Dict[str, Any]],\n",
    "    chunk_sizes: List[int] = [256, 512, 768, 1024],\n",
    "    overlaps: List[int] = [0, 50, 100, 150]\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Grid search for optimal chunking parameters.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    total = len(chunk_sizes) * len(overlaps)\n",
    "    current = 0\n",
    "    \n",
    "    for chunk_size in chunk_sizes:\n",
    "        for overlap in overlaps:\n",
    "            current += 1\n",
    "            print(f\"Testing {current}/{total}: size={chunk_size}, overlap={overlap}\")\n",
    "            \n",
    "            # Skip invalid combinations\n",
    "            if overlap >= chunk_size:\n",
    "                continue\n",
    "            \n",
    "            # Create chunks\n",
    "            chunks = ChunkingStrategies.recursive(documents, chunk_size, overlap)\n",
    "            \n",
    "            # Benchmark\n",
    "            metrics = benchmark_retrieval(chunks, embedding_model, test_queries)\n",
    "            \n",
    "            results.append({\n",
    "                \"chunk_size\": chunk_size,\n",
    "                \"overlap\": overlap,\n",
    "                \"num_chunks\": len(chunks),\n",
    "                **metrics\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run grid search\n",
    "print(\"Running grid search for optimal parameters...\\n\")\n",
    "grid_results = grid_search_parameters(\n",
    "    documents,\n",
    "    embedding_model,\n",
    "    test_queries,\n",
    "    chunk_sizes=[256, 512, 768],\n",
    "    overlaps=[0, 50, 100]\n",
    ")\n",
    "\n",
    "# Find best configuration\n",
    "best = max(grid_results, key=lambda x: x['recall@k'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PARAMETER OPTIMIZATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Size':<8} {'Overlap':<10} {'Chunks':<10} {'Recall@5':<12} {'Similarity':<12}\")\n",
    "print(\"-\"*52)\n",
    "\n",
    "for r in sorted(grid_results, key=lambda x: -x['recall@k']):\n",
    "    marker = \" <-- BEST\" if r == best else \"\"\n",
    "    print(f\"{r['chunk_size']:<8} {r['overlap']:<10} {r['num_chunks']:<10} {r['recall@k']:<12.3f} {r['avg_similarity']:<12.3f}{marker}\")\n",
    "\n",
    "print(f\"\\nOptimal configuration:\")\n",
    "print(f\"  Chunk size: {best['chunk_size']}\")\n",
    "print(f\"  Overlap: {best['overlap']}\")\n",
    "print(f\"  Recall@5: {best['recall@k']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations Summary\n",
    "\n",
    "Based on the experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = \"\"\"\n",
    "CHUNKING STRATEGY RECOMMENDATIONS\n",
    "==================================\n",
    "\n",
    "1. FIXED-SIZE CHUNKING\n",
    "   - Use for: Simple documents, uniform processing needs\n",
    "   - Avoid for: Technical documentation, code\n",
    "   - Parameters: 500 chars, no overlap\n",
    "\n",
    "2. RECURSIVE CHUNKING (RECOMMENDED DEFAULT)\n",
    "   - Use for: Most use cases, structured documents\n",
    "   - Parameters: 512 chars, 100 overlap\n",
    "   - Separators: Prioritize section headers, then paragraphs\n",
    "\n",
    "3. SENTENCE-BASED CHUNKING\n",
    "   - Use for: Q&A datasets, conversational content\n",
    "   - Parameters: 5-7 sentences per chunk\n",
    "   - Best for: High answer relevancy requirements\n",
    "\n",
    "4. SEMANTIC CHUNKING\n",
    "   - Use for: Complex documents with varied topics\n",
    "   - Parameters: 0.75 similarity threshold\n",
    "   - Trade-off: Best quality, highest compute cost\n",
    "\n",
    "DGX SPARK OPTIMIZATION\n",
    "======================\n",
    "- Use GPU for semantic chunking (5x faster)\n",
    "- Batch size 64 for embedding computation\n",
    "- Pre-compute embeddings for large document sets\n",
    "\"\"\"\n",
    "\n",
    "print(recommendations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
