{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.4.4: R1 vs Standard Model Comparison - SOLUTIONS\n",
    "\n",
    "This notebook contains complete solutions to all exercises from Lab 3.4.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "# Model configuration\n",
    "models = ollama.list()\n",
    "model_names = [m['name'] for m in models.get('models', [])]\n",
    "\n",
    "# Find models\n",
    "R1_MODEL = None\n",
    "STANDARD_MODEL = None\n",
    "\n",
    "for name in model_names:\n",
    "    if 'r1' in name.lower() and not R1_MODEL:\n",
    "        R1_MODEL = name\n",
    "    elif any(x in name.lower() for x in ['llama', 'qwen', 'mistral']) and 'r1' not in name.lower() and not STANDARD_MODEL:\n",
    "        STANDARD_MODEL = name\n",
    "\n",
    "if not R1_MODEL:\n",
    "    R1_MODEL = model_names[0] if model_names else \"deepseek-r1:7b\"\n",
    "if not STANDARD_MODEL:\n",
    "    STANDARD_MODEL = model_names[0] if model_names else \"qwen3:8b\"\n",
    "\n",
    "print(f\"R1 Model: {R1_MODEL}\")\n",
    "print(f\"Standard Model: {STANDARD_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Evaluation Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvalResult:\n",
    "    \"\"\"Result from evaluating a single problem.\"\"\"\n",
    "    question: str\n",
    "    expected: str\n",
    "    predicted: str\n",
    "    correct: bool\n",
    "    response_time: float\n",
    "    response_tokens: int\n",
    "    thinking_tokens: int\n",
    "    category: str\n",
    "    full_response: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelEvaluation:\n",
    "    \"\"\"Complete evaluation of a model.\"\"\"\n",
    "    model_name: str\n",
    "    results: List[EvalResult] = field(default_factory=list)\n",
    "    \n",
    "    @property\n",
    "    def accuracy(self) -> float:\n",
    "        if not self.results:\n",
    "            return 0.0\n",
    "        return sum(1 for r in self.results if r.correct) / len(self.results)\n",
    "    \n",
    "    @property\n",
    "    def avg_time(self) -> float:\n",
    "        if not self.results:\n",
    "            return 0.0\n",
    "        return sum(r.response_time for r in self.results) / len(self.results)\n",
    "    \n",
    "    @property\n",
    "    def total_tokens(self) -> int:\n",
    "        return sum(r.response_tokens for r in self.results)\n",
    "    \n",
    "    @property\n",
    "    def total_thinking_tokens(self) -> int:\n",
    "        return sum(r.thinking_tokens for r in self.results)\n",
    "    \n",
    "    def accuracy_by_category(self) -> Dict[str, float]:\n",
    "        by_cat = defaultdict(list)\n",
    "        for r in self.results:\n",
    "            by_cat[r.category].append(r.correct)\n",
    "        return {cat: sum(correct) / len(correct) for cat, correct in by_cat.items()}\n",
    "    \n",
    "    def time_by_category(self) -> Dict[str, float]:\n",
    "        by_cat = defaultdict(list)\n",
    "        for r in self.results:\n",
    "            by_cat[r.category].append(r.response_time)\n",
    "        return {cat: sum(times) / len(times) for cat, times in by_cat.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Answer Extraction and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(response: str, expected_type: str = \"number\") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Solution: Extract answer from response based on expected type.\n",
    "    \"\"\"\n",
    "    # Remove thinking tokens\n",
    "    response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL)\n",
    "    response = response.strip()\n",
    "    \n",
    "    if expected_type == \"number\":\n",
    "        patterns = [\n",
    "            r\"[Tt]he (?:final )?answer is[:\\s]+\\$?([\\d,]+(?:\\.\\d+)?)\",\n",
    "            r\"[Aa]nswer[:\\s]+\\$?([\\d,]+(?:\\.\\d+)?)\",\n",
    "            r\"=\\s*\\$?([\\d,]+(?:\\.\\d+)?)\\s*(?:$|\\.|\\n)\",\n",
    "            r\"(?:result|total|sum)[:\\s]+\\$?([\\d,]+(?:\\.\\d+)?)\",\n",
    "        ]\n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, response)\n",
    "            if matches:\n",
    "                num_str = matches[-1].replace(',', '')\n",
    "                try:\n",
    "                    num = float(num_str)\n",
    "                    return str(int(num)) if num == int(num) else str(num)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        # Fallback: last number\n",
    "        numbers = re.findall(r'-?[\\d,]+(?:\\.\\d+)?', response)\n",
    "        if numbers:\n",
    "            num_str = numbers[-1].replace(',', '')\n",
    "            try:\n",
    "                num = float(num_str)\n",
    "                return str(int(num)) if num == int(num) else str(num)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    elif expected_type == \"yes_no\":\n",
    "        response_lower = response.lower()\n",
    "        if 'yes' in response_lower:\n",
    "            return 'yes'\n",
    "        elif 'no' in response_lower:\n",
    "            return 'no'\n",
    "    \n",
    "    elif expected_type == \"multiple_choice\":\n",
    "        match = re.search(r'\\b([A-D])\\b', response)\n",
    "        if match:\n",
    "            return match.group(1).upper()\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def count_thinking_tokens(response: str) -> int:\n",
    "    \"\"\"Count tokens in <think> blocks.\"\"\"\n",
    "    matches = re.findall(r'<think>(.*?)</think>', response, re.DOTALL)\n",
    "    thinking_text = ' '.join(matches)\n",
    "    return len(thinking_text) // 4\n",
    "\n",
    "\n",
    "def compare_answers(predicted: Optional[str], expected, tolerance: float = 0.01) -> bool:\n",
    "    \"\"\"Compare predicted to expected answer.\"\"\"\n",
    "    if predicted is None:\n",
    "        return False\n",
    "    \n",
    "    # Try numeric comparison\n",
    "    try:\n",
    "        pred_num = float(str(predicted).replace(',', ''))\n",
    "        exp_num = float(str(expected).replace(',', ''))\n",
    "        if exp_num == 0:\n",
    "            return abs(pred_num) < tolerance\n",
    "        return abs(pred_num - exp_num) / abs(exp_num) < tolerance\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # String comparison\n",
    "    return str(predicted).lower().strip() == str(expected).lower().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Complete Model Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: str,\n",
    "    problems: Dict[str, List],\n",
    "    n_per_category: int = 5,\n",
    "    use_cot: bool = True,\n",
    "    verbose: bool = True,\n",
    ") -> ModelEvaluation:\n",
    "    \"\"\"\n",
    "    Solution: Comprehensive model evaluation.\n",
    "    \"\"\"\n",
    "    evaluation = ModelEvaluation(model_name=model)\n",
    "    \n",
    "    for category, probs in problems.items():\n",
    "        if category == 'code':  # Skip code (needs execution to evaluate)\n",
    "            continue\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Category: {category.upper()}\")\n",
    "            print('='*50)\n",
    "        \n",
    "        for i, prob in enumerate(probs[:n_per_category]):\n",
    "            question = prob.get('question', '')\n",
    "            expected = prob.get('answer', prob.get('numerical_answer', ''))\n",
    "            \n",
    "            # Determine expected type\n",
    "            if str(expected).lower() in ['yes', 'no']:\n",
    "                exp_type = 'yes_no'\n",
    "            elif str(expected).upper() in ['A', 'B', 'C', 'D']:\n",
    "                exp_type = 'multiple_choice'\n",
    "            else:\n",
    "                exp_type = 'number'\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\nProblem {i+1}: {question[:60]}...\")\n",
    "            \n",
    "            # Build prompt\n",
    "            if use_cot:\n",
    "                prompt = f\"{question}\\n\\nLet's think step by step:\"\n",
    "            else:\n",
    "                prompt = question\n",
    "            \n",
    "            # Get response\n",
    "            start_time = time.time()\n",
    "            response = ollama.chat(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                options={\"temperature\": 0.0, \"num_predict\": 1024}\n",
    "            )\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            response_text = response['message']['content']\n",
    "            \n",
    "            # Extract and compare\n",
    "            predicted = extract_answer(response_text, exp_type)\n",
    "            correct = compare_answers(predicted, expected)\n",
    "            thinking_tokens = count_thinking_tokens(response_text)\n",
    "            response_tokens = len(response_text) // 4\n",
    "            \n",
    "            result = EvalResult(\n",
    "                question=question,\n",
    "                expected=str(expected),\n",
    "                predicted=predicted or \"N/A\",\n",
    "                correct=correct,\n",
    "                response_time=elapsed,\n",
    "                response_tokens=response_tokens,\n",
    "                thinking_tokens=thinking_tokens,\n",
    "                category=category,\n",
    "                full_response=response_text,\n",
    "            )\n",
    "            evaluation.results.append(result)\n",
    "            \n",
    "            if verbose:\n",
    "                status = \"CORRECT\" if correct else \"WRONG\"\n",
    "                print(f\"  Expected: {expected}, Predicted: {predicted} [{status}]\")\n",
    "                print(f\"  Time: {elapsed:.1f}s, Tokens: {response_tokens}\")\n",
    "    \n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Detailed Comparison Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_detailed_comparison(\n",
    "    r1_eval: ModelEvaluation,\n",
    "    std_eval: ModelEvaluation\n",
    "):\n",
    "    \"\"\"\n",
    "    Solution: Generate detailed comparison report.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"COMPREHENSIVE MODEL COMPARISON REPORT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Overall metrics\n",
    "    print(f\"\\n{'Metric':<30} {'R1 Model':<18} {'Standard Model':<18}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Model Name':<30} {r1_eval.model_name[:16]:<18} {std_eval.model_name[:16]:<18}\")\n",
    "    print(f\"{'Overall Accuracy':<30} {r1_eval.accuracy:<18.1%} {std_eval.accuracy:<18.1%}\")\n",
    "    print(f\"{'Avg Response Time':<30} {r1_eval.avg_time:<18.1f}s {std_eval.avg_time:<18.1f}s\")\n",
    "    print(f\"{'Total Tokens':<30} {r1_eval.total_tokens:<18} {std_eval.total_tokens:<18}\")\n",
    "    print(f\"{'Thinking Tokens':<30} {r1_eval.total_thinking_tokens:<18} {std_eval.total_thinking_tokens:<18}\")\n",
    "    \n",
    "    # Accuracy by category\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"ACCURACY BY CATEGORY\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    r1_by_cat = r1_eval.accuracy_by_category()\n",
    "    std_by_cat = std_eval.accuracy_by_category()\n",
    "    all_cats = set(r1_by_cat.keys()) | set(std_by_cat.keys())\n",
    "    \n",
    "    print(f\"{'Category':<20} {'R1':<15} {'Standard':<15} {'Diff':<15}\")\n",
    "    for cat in sorted(all_cats):\n",
    "        r1_acc = r1_by_cat.get(cat, 0)\n",
    "        std_acc = std_by_cat.get(cat, 0)\n",
    "        diff = r1_acc - std_acc\n",
    "        sign = \"+\" if diff > 0 else \"\"\n",
    "        print(f\"{cat:<20} {r1_acc:<15.1%} {std_acc:<15.1%} {sign}{diff:<15.1%}\")\n",
    "    \n",
    "    # Latency by category\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"LATENCY BY CATEGORY\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    r1_time = r1_eval.time_by_category()\n",
    "    std_time = std_eval.time_by_category()\n",
    "    \n",
    "    print(f\"{'Category':<20} {'R1':<15} {'Standard':<15} {'Ratio':<15}\")\n",
    "    for cat in sorted(all_cats):\n",
    "        r1_t = r1_time.get(cat, 0)\n",
    "        std_t = std_time.get(cat, 0)\n",
    "        ratio = r1_t / std_t if std_t > 0 else 0\n",
    "        print(f\"{cat:<20} {r1_t:<15.2f}s {std_t:<15.2f}s {ratio:<15.1f}x\")\n",
    "    \n",
    "    # Cost-benefit analysis\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"COST-BENEFIT ANALYSIS\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    acc_improvement = r1_eval.accuracy - std_eval.accuracy\n",
    "    time_overhead = r1_eval.avg_time / std_eval.avg_time if std_eval.avg_time > 0 else 0\n",
    "    token_overhead = r1_eval.total_tokens / std_eval.total_tokens if std_eval.total_tokens > 0 else 0\n",
    "    \n",
    "    print(f\"Accuracy improvement:   {acc_improvement:+.1%}\")\n",
    "    print(f\"Time overhead:          {time_overhead:.1f}x\")\n",
    "    print(f\"Token overhead:         {token_overhead:.1f}x\")\n",
    "    \n",
    "    # Efficiency metrics\n",
    "    r1_correct = sum(1 for r in r1_eval.results if r.correct)\n",
    "    std_correct = sum(1 for r in std_eval.results if r.correct)\n",
    "    extra_correct = r1_correct - std_correct\n",
    "    extra_time = sum(r.response_time for r in r1_eval.results) - sum(r.response_time for r in std_eval.results)\n",
    "    \n",
    "    print(f\"\\nExtra correct answers:  {extra_correct}\")\n",
    "    if extra_correct > 0:\n",
    "        print(f\"Time cost per extra:    {extra_time / extra_correct:.1f}s\")\n",
    "    \n",
    "    # Recommendation\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"RECOMMENDATION\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    if acc_improvement > 0.15:\n",
    "        print(f\"R1 shows strong improvement (+{acc_improvement:.0%}).\")\n",
    "        print(\"RECOMMENDED for: complex reasoning, math, coding tasks\")\n",
    "    elif acc_improvement > 0.05:\n",
    "        print(f\"R1 shows moderate improvement (+{acc_improvement:.0%}).\")\n",
    "        print(\"RECOMMENDED for: complex queries; use standard for simple ones\")\n",
    "    elif acc_improvement > 0:\n",
    "        print(f\"R1 shows slight improvement (+{acc_improvement:.0%}).\")\n",
    "        print(\"Consider cost vs benefit; routing may help\")\n",
    "    else:\n",
    "        print(\"Standard model matches or beats R1 on this benchmark.\")\n",
    "        print(\"Review test cases; R1 may still help on harder problems\")\n",
    "    \n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(\n",
    "    r1_eval: ModelEvaluation,\n",
    "    std_eval: ModelEvaluation,\n",
    "    show_examples: int = 3\n",
    "):\n",
    "    \"\"\"\n",
    "    Solution: Detailed error analysis comparing models.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ERROR ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    r1_results = {r.question: r for r in r1_eval.results}\n",
    "    std_results = {r.question: r for r in std_eval.results}\n",
    "    \n",
    "    # Categorize problems\n",
    "    both_correct = []\n",
    "    both_wrong = []\n",
    "    r1_only = []\n",
    "    std_only = []\n",
    "    \n",
    "    for q in r1_results:\n",
    "        if q in std_results:\n",
    "            r1_c = r1_results[q].correct\n",
    "            std_c = std_results[q].correct\n",
    "            \n",
    "            if r1_c and std_c:\n",
    "                both_correct.append(q)\n",
    "            elif not r1_c and not std_c:\n",
    "                both_wrong.append(q)\n",
    "            elif r1_c:\n",
    "                r1_only.append(q)\n",
    "            else:\n",
    "                std_only.append(q)\n",
    "    \n",
    "    total = len(r1_results)\n",
    "    \n",
    "    print(f\"\\nAgreement Analysis ({total} problems):\")\n",
    "    print(f\"  Both correct:      {len(both_correct):3} ({len(both_correct)/total:.0%})\")\n",
    "    print(f\"  Both wrong:        {len(both_wrong):3} ({len(both_wrong)/total:.0%})\")\n",
    "    print(f\"  R1 only correct:   {len(r1_only):3} ({len(r1_only)/total:.0%})\")\n",
    "    print(f\"  Std only correct:  {len(std_only):3} ({len(std_only)/total:.0%})\")\n",
    "    \n",
    "    # Show examples\n",
    "    if r1_only:\n",
    "        print(\"\\n\" + \"-\" * 50)\n",
    "        print(\"R1 SUCCEEDED, Standard FAILED:\")\n",
    "        print(\"-\" * 50)\n",
    "        for q in r1_only[:show_examples]:\n",
    "            r1_r = r1_results[q]\n",
    "            std_r = std_results[q]\n",
    "            print(f\"\\nQ: {q[:70]}...\")\n",
    "            print(f\"  Expected: {r1_r.expected}\")\n",
    "            print(f\"  R1: {r1_r.predicted} (CORRECT)\")\n",
    "            print(f\"  Standard: {std_r.predicted} (WRONG)\")\n",
    "    \n",
    "    if std_only:\n",
    "        print(\"\\n\" + \"-\" * 50)\n",
    "        print(\"Standard SUCCEEDED, R1 FAILED:\")\n",
    "        print(\"-\" * 50)\n",
    "        for q in std_only[:show_examples]:\n",
    "            r1_r = r1_results[q]\n",
    "            std_r = std_results[q]\n",
    "            print(f\"\\nQ: {q[:70]}...\")\n",
    "            print(f\"  Expected: {r1_r.expected}\")\n",
    "            print(f\"  R1: {r1_r.predicted} (WRONG)\")\n",
    "            print(f\"  Standard: {std_r.predicted} (CORRECT)\")\n",
    "    \n",
    "    if both_wrong:\n",
    "        print(\"\\n\" + \"-\" * 50)\n",
    "        print(\"BOTH FAILED (hardest problems):\")\n",
    "        print(\"-\" * 50)\n",
    "        for q in both_wrong[:show_examples]:\n",
    "            r1_r = r1_results[q]\n",
    "            print(f\"\\nQ: {q[:70]}...\")\n",
    "            print(f\"  Expected: {r1_r.expected}\")\n",
    "            print(f\"  R1: {r1_results[q].predicted}\")\n",
    "            print(f\"  Std: {std_results[q].predicted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Run Full Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample problems for testing\n",
    "test_problems = {\n",
    "    \"math\": [\n",
    "        {\"question\": \"What is 17 * 23?\", \"answer\": 391},\n",
    "        {\"question\": \"What is 15% of 240?\", \"answer\": 36},\n",
    "        {\"question\": \"If 3x + 7 = 22, what is x?\", \"answer\": 5},\n",
    "    ],\n",
    "    \"reasoning\": [\n",
    "        {\"question\": \"A bat and ball cost $1.10. The bat costs $1 more than the ball. How much does the ball cost in cents?\", \"answer\": 5},\n",
    "        {\"question\": \"Tom is taller than Jim. Jim is taller than Mary. Is Tom taller than Mary? Answer yes or no.\", \"answer\": \"yes\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Run evaluations (uncomment to execute)\n",
    "# print(\"Evaluating R1 Model...\")\n",
    "# r1_eval = evaluate_model(R1_MODEL, test_problems, n_per_category=3)\n",
    "# \n",
    "# print(\"\\nEvaluating Standard Model...\")\n",
    "# std_eval = evaluate_model(STANDARD_MODEL, test_problems, n_per_category=3)\n",
    "# \n",
    "# print_detailed_comparison(r1_eval, std_eval)\n",
    "# analyze_errors(r1_eval, std_eval)\n",
    "\n",
    "print(\"Uncomment the evaluation code to run the comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Fair comparison**: Match model sizes (7B vs 8B, 70B vs 70B)\n",
    "2. **Use CoT for both**: Give standard model the same prompting advantage\n",
    "3. **Measure multiple metrics**: Accuracy, latency, tokens\n",
    "4. **Analyze by category**: R1 may excel at some types more than others\n",
    "5. **Error analysis**: Understand where each model fails"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
