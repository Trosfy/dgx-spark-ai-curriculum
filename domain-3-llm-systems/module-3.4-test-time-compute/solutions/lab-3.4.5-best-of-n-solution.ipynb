{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.4.5: Best-of-N with Reward Model - SOLUTIONS\n",
    "\n",
    "This notebook contains complete solutions to all exercises from Lab 3.4.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "LLM_MODEL = \"qwen3:8b\"\n",
    "\n",
    "# Check for transformers\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "    HAS_TRANSFORMERS = True\n",
    "    print(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")\n",
    "except ImportError:\n",
    "    HAS_TRANSFORMERS = False\n",
    "    print(\"Transformers not available - using heuristic reward model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: ScoredResponse Dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScoredResponse:\n",
    "    \"\"\"A response with its reward score and metadata.\"\"\"\n",
    "    response: str\n",
    "    score: float\n",
    "    generation_time: float\n",
    "    token_count: int = 0\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ScoredResponse(score={self.score:.3f}, len={len(self.response)}, time={self.generation_time:.2f}s)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Enhanced Heuristic Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedHeuristicRewardModel:\n",
    "    \"\"\"\n",
    "    Solution: A more sophisticated heuristic reward model.\n",
    "    \n",
    "    Scores based on:\n",
    "    - Response length (appropriate range)\n",
    "    - Reasoning markers\n",
    "    - Answer clarity\n",
    "    - Repetition penalty\n",
    "    - Confidence language\n",
    "    - Structure (lists, steps)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"enhanced_heuristic\"\n",
    "    \n",
    "    def score(self, prompt: str, response: str) -> float:\n",
    "        \"\"\"Score a response. Returns value in [0, 1].\"\"\"\n",
    "        score = 0.5  # Neutral start\n",
    "        response_lower = response.lower()\n",
    "        \n",
    "        # 1. Length scoring\n",
    "        length = len(response)\n",
    "        if 100 < length < 800:\n",
    "            score += 0.15\n",
    "        elif 50 < length < 1200:\n",
    "            score += 0.08\n",
    "        elif length < 30:\n",
    "            score -= 0.2  # Too short\n",
    "        elif length > 2000:\n",
    "            score -= 0.1  # Too verbose\n",
    "        \n",
    "        # 2. Reasoning markers\n",
    "        reasoning_markers = [\n",
    "            'because', 'therefore', 'thus', 'hence',\n",
    "            'first', 'second', 'then', 'finally',\n",
    "            'step', 'reason', 'explanation',\n",
    "        ]\n",
    "        marker_count = sum(1 for m in reasoning_markers if m in response_lower)\n",
    "        score += min(marker_count * 0.03, 0.15)\n",
    "        \n",
    "        # 3. Clear answer indicators\n",
    "        answer_markers = ['the answer is', 'answer:', 'result:', 'solution:', '=']\n",
    "        if any(m in response_lower for m in answer_markers):\n",
    "            score += 0.1\n",
    "        \n",
    "        # 4. Structure bonus (lists, numbered steps)\n",
    "        if any(f'{i}.' in response or f'{i})' in response for i in range(1, 6)):\n",
    "            score += 0.08\n",
    "        if '- ' in response or '* ' in response:\n",
    "            score += 0.05\n",
    "        \n",
    "        # 5. Repetition penalty\n",
    "        words = response_lower.split()\n",
    "        if words:\n",
    "            unique_ratio = len(set(words)) / len(words)\n",
    "            if unique_ratio < 0.4:\n",
    "                score -= 0.25\n",
    "            elif unique_ratio < 0.6:\n",
    "                score -= 0.1\n",
    "        \n",
    "        # 6. Uncertainty penalty\n",
    "        uncertain_markers = [\"i'm not sure\", \"i don't know\", \"might be\", \"could be\", \"perhaps\"]\n",
    "        if any(m in response_lower for m in uncertain_markers):\n",
    "            score -= 0.1\n",
    "        \n",
    "        # 7. Confidence bonus (but not overconfidence)\n",
    "        confident_markers = ['clearly', 'definitely', 'certainly']\n",
    "        if any(m in response_lower for m in confident_markers):\n",
    "            score += 0.05\n",
    "        \n",
    "        # 8. Relevance to prompt (basic keyword overlap)\n",
    "        prompt_words = set(prompt.lower().split())\n",
    "        response_words = set(response_lower.split())\n",
    "        overlap = len(prompt_words & response_words) / max(len(prompt_words), 1)\n",
    "        score += min(overlap * 0.1, 0.1)\n",
    "        \n",
    "        return max(0.0, min(1.0, score))\n",
    "\n",
    "\n",
    "# Test the enhanced model\n",
    "reward_model = EnhancedHeuristicRewardModel()\n",
    "\n",
    "test_prompt = \"Explain what machine learning is.\"\n",
    "test_responses = [\n",
    "    \"ML is AI.\",  # Too short\n",
    "    \"Machine learning is a subset of artificial intelligence that enables systems to learn from data. First, data is collected. Then, the model learns patterns. Finally, it makes predictions.\",\n",
    "    \"I'm not sure, but I think machine learning might be something to do with computers learning stuff maybe?\",\n",
    "    \"Machine learning is a type of AI. Machine learning is AI. AI uses machine learning. Learning machines use AI.\",  # Repetitive\n",
    "]\n",
    "\n",
    "print(\"Testing Enhanced Reward Model:\")\n",
    "print(f\"Prompt: {test_prompt}\\n\")\n",
    "for i, resp in enumerate(test_responses):\n",
    "    score = reward_model.score(test_prompt, resp)\n",
    "    print(f\"Response {i+1} (score: {score:.3f}): {resp[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Best-of-N Sampler Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestOfNSampler:\n",
    "    \"\"\"\n",
    "    Solution: Complete Best-of-N sampling implementation.\n",
    "    \n",
    "    Features:\n",
    "    - Configurable N and temperature\n",
    "    - Detailed statistics tracking\n",
    "    - Score distribution analysis\n",
    "    - Performance metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_model: str = LLM_MODEL,\n",
    "        reward_model = None,\n",
    "        default_n: int = 5,\n",
    "        default_temperature: float = 0.7,\n",
    "    ):\n",
    "        self.llm_model = llm_model\n",
    "        self.reward_model = reward_model or EnhancedHeuristicRewardModel()\n",
    "        self.default_n = default_n\n",
    "        self.default_temperature = default_temperature\n",
    "        \n",
    "        # Statistics\n",
    "        self.total_samples = 0\n",
    "        self.total_queries = 0\n",
    "        self.score_history = []\n",
    "    \n",
    "    def generate_candidates(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        n: int = None,\n",
    "        temperature: float = None,\n",
    "        max_tokens: int = 512,\n",
    "    ) -> List[ScoredResponse]:\n",
    "        \"\"\"Generate N candidate responses.\"\"\"\n",
    "        n = n or self.default_n\n",
    "        temperature = temperature or self.default_temperature\n",
    "        \n",
    "        candidates = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            response = ollama.chat(\n",
    "                model=self.llm_model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                options={\"temperature\": temperature, \"num_predict\": max_tokens}\n",
    "            )\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            response_text = response['message']['content']\n",
    "            \n",
    "            candidates.append(ScoredResponse(\n",
    "                response=response_text,\n",
    "                score=0.0,\n",
    "                generation_time=elapsed,\n",
    "                token_count=len(response_text) // 4,\n",
    "            ))\n",
    "            \n",
    "            self.total_samples += 1\n",
    "        \n",
    "        return candidates\n",
    "    \n",
    "    def score_candidates(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        candidates: List[ScoredResponse]\n",
    "    ) -> List[ScoredResponse]:\n",
    "        \"\"\"Score all candidates with the reward model.\"\"\"\n",
    "        for candidate in candidates:\n",
    "            candidate.score = self.reward_model.score(prompt, candidate.response)\n",
    "            self.score_history.append(candidate.score)\n",
    "        \n",
    "        return candidates\n",
    "    \n",
    "    def sample(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        n: int = None,\n",
    "        temperature: float = None,\n",
    "        max_tokens: int = 512,\n",
    "        verbose: bool = False,\n",
    "    ) -> Tuple[ScoredResponse, List[ScoredResponse]]:\n",
    "        \"\"\"\n",
    "        Run Best-of-N sampling.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (best_response, all_candidates)\n",
    "        \"\"\"\n",
    "        n = n or self.default_n\n",
    "        self.total_queries += 1\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Best-of-{n} Sampling\")\n",
    "            print(f\"Prompt: {prompt[:50]}...\")\n",
    "        \n",
    "        # Generate\n",
    "        if verbose:\n",
    "            print(f\"\\nGenerating {n} candidates...\")\n",
    "        candidates = self.generate_candidates(prompt, n, temperature, max_tokens)\n",
    "        \n",
    "        # Score\n",
    "        if verbose:\n",
    "            print(\"Scoring candidates...\")\n",
    "        candidates = self.score_candidates(prompt, candidates)\n",
    "        \n",
    "        # Find best\n",
    "        best = max(candidates, key=lambda x: x.score)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nScore distribution:\")\n",
    "            for i, c in enumerate(sorted(candidates, key=lambda x: x.score, reverse=True)):\n",
    "                marker = \" <-- BEST\" if c.score == best.score else \"\"\n",
    "                print(f\"  {i+1}. Score: {c.score:.3f}{marker}\")\n",
    "        \n",
    "        return best, candidates\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"Get sampling statistics.\"\"\"\n",
    "        if not self.score_history:\n",
    "            return {'message': 'No samples yet'}\n",
    "        \n",
    "        return {\n",
    "            'total_queries': self.total_queries,\n",
    "            'total_samples': self.total_samples,\n",
    "            'avg_n': self.total_samples / max(self.total_queries, 1),\n",
    "            'avg_score': sum(self.score_history) / len(self.score_history),\n",
    "            'min_score': min(self.score_history),\n",
    "            'max_score': max(self.score_history),\n",
    "            'score_std': self._std(self.score_history),\n",
    "        }\n",
    "    \n",
    "    def _std(self, values: List[float]) -> float:\n",
    "        \"\"\"Calculate standard deviation.\"\"\"\n",
    "        if len(values) < 2:\n",
    "            return 0.0\n",
    "        mean = sum(values) / len(values)\n",
    "        variance = sum((x - mean) ** 2 for x in values) / len(values)\n",
    "        return variance ** 0.5\n",
    "\n",
    "\n",
    "# Test the sampler\n",
    "sampler = BestOfNSampler(default_n=5)\n",
    "\n",
    "test_prompt = \"What are three benefits of regular exercise?\"\n",
    "best, all_candidates = sampler.sample(test_prompt, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BEST RESPONSE:\")\n",
    "print(\"=\"*50)\n",
    "print(best.response[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Greedy vs Best-of-N Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_greedy_vs_bon(\n",
    "    prompts: List[str],\n",
    "    sampler: BestOfNSampler,\n",
    "    n: int = 5,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Solution: Compare greedy decoding vs Best-of-N.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'greedy': {'scores': [], 'times': []},\n",
    "        'bon': {'scores': [], 'times': []},\n",
    "        'improvements': [],\n",
    "    }\n",
    "    \n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print(f\"\\nPrompt {i+1}/{len(prompts)}: {prompt[:40]}...\")\n",
    "        \n",
    "        # Greedy (temperature=0, n=1)\n",
    "        greedy_start = time.time()\n",
    "        response = ollama.chat(\n",
    "            model=sampler.llm_model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            options={\"temperature\": 0.0, \"num_predict\": 512}\n",
    "        )\n",
    "        greedy_time = time.time() - greedy_start\n",
    "        greedy_score = sampler.reward_model.score(prompt, response['message']['content'])\n",
    "        \n",
    "        results['greedy']['scores'].append(greedy_score)\n",
    "        results['greedy']['times'].append(greedy_time)\n",
    "        \n",
    "        # Best-of-N\n",
    "        bon_start = time.time()\n",
    "        best, _ = sampler.sample(prompt, n=n)\n",
    "        bon_time = time.time() - bon_start\n",
    "        \n",
    "        results['bon']['scores'].append(best.score)\n",
    "        results['bon']['times'].append(bon_time)\n",
    "        results['improvements'].append(best.score - greedy_score)\n",
    "        \n",
    "        print(f\"  Greedy: {greedy_score:.3f} ({greedy_time:.1f}s)\")\n",
    "        print(f\"  BoN-{n}: {best.score:.3f} ({bon_time:.1f}s)\")\n",
    "        print(f\"  Improvement: {best.score - greedy_score:+.3f}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    avg_greedy = sum(results['greedy']['scores']) / len(prompts)\n",
    "    avg_bon = sum(results['bon']['scores']) / len(prompts)\n",
    "    avg_improvement = sum(results['improvements']) / len(prompts)\n",
    "    \n",
    "    print(f\"Average Greedy Score: {avg_greedy:.3f}\")\n",
    "    print(f\"Average BoN Score:    {avg_bon:.3f}\")\n",
    "    print(f\"Average Improvement:  {avg_improvement:+.3f}\")\n",
    "    \n",
    "    bon_wins = sum(1 for imp in results['improvements'] if imp > 0)\n",
    "    print(f\"\\nBoN outperformed: {bon_wins}/{len(prompts)} ({bon_wins/len(prompts):.0%})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"What is the difference between a list and a tuple in Python?\",\n",
    "    \"Explain how photosynthesis works.\",\n",
    "    \"Describe the process of making coffee.\",\n",
    "]\n",
    "\n",
    "# Run comparison (uncomment to execute)\n",
    "# comparison = compare_greedy_vs_bon(test_prompts, sampler, n=5)\n",
    "print(\"Uncomment the comparison call to run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Experiment with Different N Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_with_n(\n",
    "    prompt: str,\n",
    "    sampler: BestOfNSampler,\n",
    "    n_values: List[int] = [1, 3, 5, 10, 15],\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Solution: Experiment with different N values.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(f\"Prompt: {prompt[:50]}...\\n\")\n",
    "    \n",
    "    for n in n_values:\n",
    "        start = time.time()\n",
    "        \n",
    "        if n == 1:\n",
    "            # Greedy\n",
    "            response = ollama.chat(\n",
    "                model=sampler.llm_model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                options={\"temperature\": 0.0, \"num_predict\": 512}\n",
    "            )\n",
    "            score = sampler.reward_model.score(prompt, response['message']['content'])\n",
    "            all_scores = [score]\n",
    "        else:\n",
    "            best, candidates = sampler.sample(prompt, n=n)\n",
    "            score = best.score\n",
    "            all_scores = [c.score for c in candidates]\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        results[n] = {\n",
    "            'best_score': score,\n",
    "            'all_scores': all_scores,\n",
    "            'mean_score': sum(all_scores) / len(all_scores),\n",
    "            'time': elapsed,\n",
    "            'time_per_sample': elapsed / n,\n",
    "        }\n",
    "        \n",
    "        print(f\"N={n:2}: best={score:.3f}, mean={results[n]['mean_score']:.3f}, time={elapsed:.1f}s\")\n",
    "    \n",
    "    # Analysis\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DIMINISHING RETURNS ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    baseline = results[1]['best_score']\n",
    "    for n in n_values[1:]:\n",
    "        improvement = results[n]['best_score'] - baseline\n",
    "        efficiency = improvement / results[n]['time'] if results[n]['time'] > 0 else 0\n",
    "        print(f\"N={n:2}: +{improvement:.3f} improvement, efficiency={efficiency:.4f}/s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test (uncomment to run)\n",
    "# experiment_results = experiment_with_n(\n",
    "#     \"Explain the concept of recursion in programming.\",\n",
    "#     sampler,\n",
    "#     n_values=[1, 3, 5, 10]\n",
    "# )\n",
    "print(\"Uncomment the experiment call to run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Temperature > 0** is essential for diverse candidates\n",
    "2. **N=3-5** usually provides good balance\n",
    "3. **Diminishing returns** after N~10\n",
    "4. **Reward model quality** directly impacts selection quality\n",
    "5. **Time/cost tradeoff**: 5x samples â‰ˆ 5x cost, but often <5x improvement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
