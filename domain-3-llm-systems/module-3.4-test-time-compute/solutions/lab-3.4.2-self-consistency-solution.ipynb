{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.4.2: Self-Consistency Implementation - SOLUTIONS\n",
    "\n",
    "This notebook contains complete solutions to all exercises from Lab 3.4.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "MODEL = \"llama3.1:8b\"  # Adjust as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Advanced Answer Extraction\n",
    "\n",
    "Complete solution for extracting answers from diverse formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedAnswerExtractor:\n",
    "    \"\"\"\n",
    "    Solution: Robust answer extraction for multiple formats.\n",
    "    \n",
    "    Handles:\n",
    "    - Numerical answers (with units, currency, fractions)\n",
    "    - Multiple choice (A, B, C, D)\n",
    "    - Yes/No/True/False\n",
    "    - Named entities\n",
    "    - Lists of items\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.patterns = {\n",
    "            'numerical': [\n",
    "                r\"[Tt]he (?:final )?answer is[:\\s]+\\$?([\\d,]+(?:\\.\\d+)?)\",\n",
    "                r\"[Aa]nswer[:\\s]+\\$?([\\d,]+(?:\\.\\d+)?)\",\n",
    "                r\"=\\s*\\$?([\\d,]+(?:\\.\\d+)?)\\s*(?:$|\\.|\\n)\",\n",
    "                r\"(?:result|total|sum)[:\\s]+\\$?([\\d,]+(?:\\.\\d+)?)\",\n",
    "            ],\n",
    "            'multiple_choice': [\n",
    "                r\"[Tt]he (?:correct )?answer is[:\\s]+([A-D])\",\n",
    "                r\"[Aa]nswer[:\\s]+([A-D])\\b\",\n",
    "                r\"\\b([A-D])\\)\\s+is correct\",\n",
    "                r\"Option\\s+([A-D])\",\n",
    "            ],\n",
    "            'boolean': [\n",
    "                r\"[Tt]he answer is[:\\s]+(yes|no|true|false)\",\n",
    "                r\"\\b(yes|no|true|false)[.,]?\\s*$\",\n",
    "            ],\n",
    "            'fraction': [\n",
    "                r\"([\\d]+)/([\\d]+)\",\n",
    "                r\"([\\d]+)\\s+out of\\s+([\\d]+)\",\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def extract(self, response: str, answer_type: str = 'auto') -> Any:\n",
    "        \"\"\"Extract answer based on type.\"\"\"\n",
    "        response_lower = response.lower()\n",
    "        \n",
    "        if answer_type == 'auto':\n",
    "            answer_type = self._detect_type(response)\n",
    "        \n",
    "        if answer_type == 'numerical':\n",
    "            return self._extract_numerical(response)\n",
    "        elif answer_type == 'multiple_choice':\n",
    "            return self._extract_choice(response)\n",
    "        elif answer_type == 'boolean':\n",
    "            return self._extract_boolean(response_lower)\n",
    "        elif answer_type == 'fraction':\n",
    "            return self._extract_fraction(response)\n",
    "        else:\n",
    "            return self._extract_numerical(response)  # Default\n",
    "    \n",
    "    def _detect_type(self, response: str) -> str:\n",
    "        \"\"\"Auto-detect answer type.\"\"\"\n",
    "        response_lower = response.lower()\n",
    "        \n",
    "        # Check for multiple choice indicators\n",
    "        if re.search(r'\\b[A-D]\\)', response) or 'option' in response_lower:\n",
    "            return 'multiple_choice'\n",
    "        \n",
    "        # Check for boolean\n",
    "        if re.search(r'\\b(yes|no|true|false)\\b', response_lower):\n",
    "            return 'boolean'\n",
    "        \n",
    "        # Check for fractions\n",
    "        if re.search(r'\\d+/\\d+', response) or 'out of' in response_lower:\n",
    "            return 'fraction'\n",
    "        \n",
    "        # Default to numerical\n",
    "        return 'numerical'\n",
    "    \n",
    "    def _extract_numerical(self, response: str) -> Optional[float]:\n",
    "        \"\"\"Extract numerical answer.\"\"\"\n",
    "        for pattern in self.patterns['numerical']:\n",
    "            matches = re.findall(pattern, response)\n",
    "            if matches:\n",
    "                try:\n",
    "                    return float(matches[-1].replace(',', ''))\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        # Fallback: find any number\n",
    "        numbers = re.findall(r'-?[\\d,]+(?:\\.\\d+)?', response)\n",
    "        if numbers:\n",
    "            try:\n",
    "                return float(numbers[-1].replace(',', ''))\n",
    "            except:\n",
    "                pass\n",
    "        return None\n",
    "    \n",
    "    def _extract_choice(self, response: str) -> Optional[str]:\n",
    "        \"\"\"Extract multiple choice answer.\"\"\"\n",
    "        for pattern in self.patterns['multiple_choice']:\n",
    "            match = re.search(pattern, response, re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(1).upper()\n",
    "        return None\n",
    "    \n",
    "    def _extract_boolean(self, response: str) -> Optional[bool]:\n",
    "        \"\"\"Extract boolean answer.\"\"\"\n",
    "        for pattern in self.patterns['boolean']:\n",
    "            match = re.search(pattern, response, re.IGNORECASE)\n",
    "            if match:\n",
    "                answer = match.group(1).lower()\n",
    "                return answer in ('yes', 'true')\n",
    "        return None\n",
    "    \n",
    "    def _extract_fraction(self, response: str) -> Optional[float]:\n",
    "        \"\"\"Extract fraction and convert to decimal.\"\"\"\n",
    "        for pattern in self.patterns['fraction']:\n",
    "            match = re.search(pattern, response)\n",
    "            if match:\n",
    "                try:\n",
    "                    num = float(match.group(1))\n",
    "                    den = float(match.group(2))\n",
    "                    return num / den if den != 0 else None\n",
    "                except:\n",
    "                    continue\n",
    "        return None\n",
    "\n",
    "\n",
    "# Test the extractor\n",
    "extractor = AdvancedAnswerExtractor()\n",
    "\n",
    "test_cases = [\n",
    "    (\"After calculation, the answer is $150.50\", 'numerical'),\n",
    "    (\"Based on my analysis, option B is correct.\", 'multiple_choice'),\n",
    "    (\"Is the sky blue? Yes.\", 'boolean'),\n",
    "    (\"The probability is 3/4 or 75%.\", 'fraction'),\n",
    "]\n",
    "\n",
    "print(\"Testing Advanced Answer Extractor:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for response, expected_type in test_cases:\n",
    "    result = extractor.extract(response, 'auto')\n",
    "    detected = extractor._detect_type(response)\n",
    "    print(f\"Response: {response[:40]}...\")\n",
    "    print(f\"  Detected type: {detected}\")\n",
    "    print(f\"  Extracted: {result}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Weighted Voting Self-Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SampledResponse:\n",
    "    \"\"\"Container for a sampled response with metadata.\"\"\"\n",
    "    response: str\n",
    "    answer: Any\n",
    "    confidence: float\n",
    "    reasoning_steps: int\n",
    "    latency: float\n",
    "\n",
    "\n",
    "class WeightedSelfConsistency:\n",
    "    \"\"\"\n",
    "    Solution: Self-consistency with weighted voting based on response quality.\n",
    "    \n",
    "    Weights can be based on:\n",
    "    - Response length (longer = more thorough)\n",
    "    - Reasoning steps (more steps = more detailed)\n",
    "    - Confidence markers (explicit confidence statements)\n",
    "    - Token probability (if available)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = MODEL, n_samples: int = 5):\n",
    "        self.model = model\n",
    "        self.n_samples = n_samples\n",
    "        self.extractor = AdvancedAnswerExtractor()\n",
    "    \n",
    "    def sample_with_cot(self, question: str, temperature: float = 0.7) -> SampledResponse:\n",
    "        \"\"\"Generate one CoT response with metadata.\"\"\"\n",
    "        prompt = f\"{question}\\n\\nLet's think step by step:\"\n",
    "        \n",
    "        start = time.time()\n",
    "        response = ollama.chat(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            options={\"temperature\": temperature, \"num_predict\": 1024}\n",
    "        )\n",
    "        latency = time.time() - start\n",
    "        \n",
    "        text = response['message']['content']\n",
    "        answer = self.extractor.extract(text)\n",
    "        confidence = self._estimate_confidence(text)\n",
    "        steps = self._count_reasoning_steps(text)\n",
    "        \n",
    "        return SampledResponse(\n",
    "            response=text,\n",
    "            answer=answer,\n",
    "            confidence=confidence,\n",
    "            reasoning_steps=steps,\n",
    "            latency=latency\n",
    "        )\n",
    "    \n",
    "    def _estimate_confidence(self, response: str) -> float:\n",
    "        \"\"\"Estimate confidence from response text.\"\"\"\n",
    "        confidence = 0.5  # Base confidence\n",
    "        \n",
    "        # Boost for explicit confidence markers\n",
    "        high_conf_markers = ['definitely', 'certainly', 'clearly', 'obviously', 'therefore']\n",
    "        low_conf_markers = ['maybe', 'perhaps', 'might', 'not sure', 'uncertain']\n",
    "        \n",
    "        response_lower = response.lower()\n",
    "        \n",
    "        for marker in high_conf_markers:\n",
    "            if marker in response_lower:\n",
    "                confidence += 0.1\n",
    "        \n",
    "        for marker in low_conf_markers:\n",
    "            if marker in response_lower:\n",
    "                confidence -= 0.1\n",
    "        \n",
    "        # Boost for verification steps\n",
    "        if 'let me verify' in response_lower or 'double check' in response_lower:\n",
    "            confidence += 0.15\n",
    "        \n",
    "        return max(0.1, min(1.0, confidence))\n",
    "    \n",
    "    def _count_reasoning_steps(self, response: str) -> int:\n",
    "        \"\"\"Count explicit reasoning steps.\"\"\"\n",
    "        patterns = [\n",
    "            r'^\\d+[.)\\s]',  # Numbered steps\n",
    "            r'^[-*]\\s',     # Bullet points\n",
    "            r'^Step\\s+\\d+',  # \"Step N\"\n",
    "            r'^First,|^Second,|^Third,|^Finally,',  # Ordinal markers\n",
    "        ]\n",
    "        \n",
    "        count = 0\n",
    "        for line in response.split('\\n'):\n",
    "            for pattern in patterns:\n",
    "                if re.match(pattern, line.strip(), re.IGNORECASE):\n",
    "                    count += 1\n",
    "                    break\n",
    "        \n",
    "        return max(1, count)\n",
    "    \n",
    "    def weighted_vote(self, samples: List[SampledResponse], weight_type: str = 'confidence') -> Tuple[Any, float]:\n",
    "        \"\"\"Perform weighted voting on samples.\"\"\"\n",
    "        valid_samples = [s for s in samples if s.answer is not None]\n",
    "        \n",
    "        if not valid_samples:\n",
    "            return None, 0.0\n",
    "        \n",
    "        # Calculate weights based on type\n",
    "        weights = {}\n",
    "        for s in valid_samples:\n",
    "            if weight_type == 'confidence':\n",
    "                w = s.confidence\n",
    "            elif weight_type == 'steps':\n",
    "                w = s.reasoning_steps / 10.0  # Normalize\n",
    "            elif weight_type == 'combined':\n",
    "                w = s.confidence * (1 + s.reasoning_steps / 10.0)\n",
    "            else:\n",
    "                w = 1.0  # Uniform\n",
    "            \n",
    "            # Round numerical answers for grouping\n",
    "            if isinstance(s.answer, float):\n",
    "                key = round(s.answer, 2)\n",
    "            else:\n",
    "                key = s.answer\n",
    "            \n",
    "            weights[key] = weights.get(key, 0) + w\n",
    "        \n",
    "        # Find answer with highest weight\n",
    "        best_answer = max(weights.keys(), key=lambda k: weights[k])\n",
    "        total_weight = sum(weights.values())\n",
    "        confidence = weights[best_answer] / total_weight if total_weight > 0 else 0\n",
    "        \n",
    "        return best_answer, confidence\n",
    "    \n",
    "    def run(self, question: str, temperature: float = 0.7, weight_type: str = 'combined') -> Dict:\n",
    "        \"\"\"Run weighted self-consistency.\"\"\"\n",
    "        print(f\"Generating {self.n_samples} samples...\")\n",
    "        \n",
    "        samples = []\n",
    "        for i in range(self.n_samples):\n",
    "            sample = self.sample_with_cot(question, temperature)\n",
    "            samples.append(sample)\n",
    "            print(f\"  Sample {i+1}: answer={sample.answer}, conf={sample.confidence:.2f}, steps={sample.reasoning_steps}\")\n",
    "        \n",
    "        # Vote with different weighting schemes\n",
    "        results = {}\n",
    "        for wt in ['uniform', 'confidence', 'steps', 'combined']:\n",
    "            answer, conf = self.weighted_vote(samples, wt)\n",
    "            results[wt] = {'answer': answer, 'confidence': conf}\n",
    "        \n",
    "        # Use requested weight type for final answer\n",
    "        final_answer, final_conf = results[weight_type]['answer'], results[weight_type]['confidence']\n",
    "        \n",
    "        return {\n",
    "            'answer': final_answer,\n",
    "            'confidence': final_conf,\n",
    "            'samples': samples,\n",
    "            'all_results': results\n",
    "        }\n",
    "\n",
    "\n",
    "# Test weighted self-consistency\n",
    "wsc = WeightedSelfConsistency(n_samples=5)\n",
    "\n",
    "question = \"A store has 150 apples. They sell 30% in the morning and 25% of the remaining in the afternoon. How many apples are left?\"\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "result = wsc.run(question)\n",
    "\n",
    "print(f\"\\nFinal Answer: {result['answer']}\")\n",
    "print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "print(f\"\\nResults by weighting scheme:\")\n",
    "for scheme, data in result['all_results'].items():\n",
    "    print(f\"  {scheme}: {data['answer']} (conf={data['confidence']:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Adaptive Self-Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveSelfConsistency:\n",
    "    \"\"\"\n",
    "    Solution: Dynamically adjust number of samples based on agreement.\n",
    "    \n",
    "    Strategy:\n",
    "    - Start with min_samples\n",
    "    - If agreement is high (>threshold), stop early\n",
    "    - If agreement is low, continue sampling up to max_samples\n",
    "    - Saves compute on easy problems, invests more on hard ones\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = MODEL,\n",
    "        min_samples: int = 3,\n",
    "        max_samples: int = 10,\n",
    "        agreement_threshold: float = 0.8,\n",
    "        check_interval: int = 2\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.min_samples = min_samples\n",
    "        self.max_samples = max_samples\n",
    "        self.agreement_threshold = agreement_threshold\n",
    "        self.check_interval = check_interval\n",
    "        self.extractor = AdvancedAnswerExtractor()\n",
    "    \n",
    "    def _get_agreement(self, answers: List[Any]) -> Tuple[Any, float]:\n",
    "        \"\"\"Calculate agreement rate for current answers.\"\"\"\n",
    "        if not answers:\n",
    "            return None, 0.0\n",
    "        \n",
    "        # Normalize numerical answers\n",
    "        normalized = []\n",
    "        for a in answers:\n",
    "            if a is None:\n",
    "                continue\n",
    "            if isinstance(a, float):\n",
    "                normalized.append(round(a, 2))\n",
    "            else:\n",
    "                normalized.append(a)\n",
    "        \n",
    "        if not normalized:\n",
    "            return None, 0.0\n",
    "        \n",
    "        counts = Counter(normalized)\n",
    "        most_common, count = counts.most_common(1)[0]\n",
    "        agreement = count / len(normalized)\n",
    "        \n",
    "        return most_common, agreement\n",
    "    \n",
    "    def sample_one(self, question: str, temperature: float = 0.7) -> Tuple[str, Any]:\n",
    "        \"\"\"Generate one sample.\"\"\"\n",
    "        prompt = f\"{question}\\n\\nLet's think step by step:\"\n",
    "        \n",
    "        response = ollama.chat(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            options={\"temperature\": temperature, \"num_predict\": 1024}\n",
    "        )\n",
    "        \n",
    "        text = response['message']['content']\n",
    "        answer = self.extractor.extract(text)\n",
    "        return text, answer\n",
    "    \n",
    "    def run(self, question: str, temperature: float = 0.7) -> Dict:\n",
    "        \"\"\"Run adaptive self-consistency.\"\"\"\n",
    "        responses = []\n",
    "        answers = []\n",
    "        \n",
    "        print(f\"Adaptive Self-Consistency (min={self.min_samples}, max={self.max_samples}, threshold={self.agreement_threshold})\")\n",
    "        \n",
    "        for i in range(self.max_samples):\n",
    "            text, answer = self.sample_one(question, temperature)\n",
    "            responses.append(text)\n",
    "            answers.append(answer)\n",
    "            \n",
    "            print(f\"  Sample {i+1}: {answer}\")\n",
    "            \n",
    "            # Check agreement after minimum samples\n",
    "            if (i + 1) >= self.min_samples and (i + 1) % self.check_interval == 0:\n",
    "                best_answer, agreement = self._get_agreement(answers)\n",
    "                print(f\"    -> Agreement check: {agreement:.1%}\")\n",
    "                \n",
    "                if agreement >= self.agreement_threshold:\n",
    "                    print(f\"  âœ“ Early stopping at {i+1} samples (agreement={agreement:.1%})\")\n",
    "                    break\n",
    "        \n",
    "        final_answer, final_agreement = self._get_agreement(answers)\n",
    "        \n",
    "        return {\n",
    "            'answer': final_answer,\n",
    "            'agreement': final_agreement,\n",
    "            'num_samples': len(answers),\n",
    "            'all_answers': answers,\n",
    "            'early_stopped': len(answers) < self.max_samples\n",
    "        }\n",
    "\n",
    "\n",
    "# Test adaptive self-consistency\n",
    "asc = AdaptiveSelfConsistency(\n",
    "    min_samples=3,\n",
    "    max_samples=8,\n",
    "    agreement_threshold=0.75\n",
    ")\n",
    "\n",
    "# Easy question (should stop early)\n",
    "easy_q = \"What is 25% of 200?\"\n",
    "print(f\"\\nEasy question: {easy_q}\")\n",
    "result = asc.run(easy_q)\n",
    "print(f\"Result: {result['answer']} (samples used: {result['num_samples']}, early_stop: {result['early_stopped']})\")\n",
    "\n",
    "# Harder question (may need more samples)\n",
    "hard_q = \"A train travels at 60mph for 2.5 hours, then 45mph for 1.5 hours. What's the average speed for the whole journey?\"\n",
    "print(f\"\\nHarder question: {hard_q}\")\n",
    "result = asc.run(hard_q)\n",
    "print(f\"Result: {result['answer']} (samples used: {result['num_samples']}, early_stop: {result['early_stopped']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Self-Consistency Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_sc_evaluation(\n",
    "    problems: List[Dict],\n",
    "    n_problems: int = 10,\n",
    "    samples_to_test: List[int] = [1, 3, 5, 7]\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Solution: Comprehensive evaluation of self-consistency.\n",
    "    \n",
    "    Tests:\n",
    "    - Different numbers of samples\n",
    "    - Accuracy vs compute tradeoff\n",
    "    - Agreement as confidence proxy\n",
    "    \"\"\"\n",
    "    extractor = AdvancedAnswerExtractor()\n",
    "    \n",
    "    results = {n: {'correct': 0, 'total': 0, 'avg_agreement': 0, 'latency': 0} for n in samples_to_test}\n",
    "    \n",
    "    for i, prob in enumerate(problems[:n_problems]):\n",
    "        question = prob['question']\n",
    "        expected = prob.get('numerical_answer', prob.get('answer'))\n",
    "        \n",
    "        print(f\"\\nProblem {i+1}: {question[:50]}...\")\n",
    "        \n",
    "        # Generate max samples once\n",
    "        max_n = max(samples_to_test)\n",
    "        all_samples = []\n",
    "        \n",
    "        start = time.time()\n",
    "        for _ in range(max_n):\n",
    "            prompt = f\"{question}\\n\\nLet's think step by step:\"\n",
    "            response = ollama.chat(\n",
    "                model=MODEL,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                options={\"temperature\": 0.7, \"num_predict\": 512}\n",
    "            )\n",
    "            answer = extractor.extract(response['message']['content'])\n",
    "            all_samples.append(answer)\n",
    "        total_time = time.time() - start\n",
    "        \n",
    "        # Evaluate for each sample count\n",
    "        for n in samples_to_test:\n",
    "            subset = all_samples[:n]\n",
    "            valid = [a for a in subset if a is not None]\n",
    "            \n",
    "            if valid:\n",
    "                counts = Counter([round(a, 2) if isinstance(a, float) else a for a in valid])\n",
    "                predicted, count = counts.most_common(1)[0]\n",
    "                agreement = count / len(valid)\n",
    "            else:\n",
    "                predicted = None\n",
    "                agreement = 0\n",
    "            \n",
    "            try:\n",
    "                correct = abs(float(predicted or 0) - float(expected)) < 0.01 * abs(float(expected))\n",
    "            except:\n",
    "                correct = False\n",
    "            \n",
    "            results[n]['total'] += 1\n",
    "            results[n]['correct'] += int(correct)\n",
    "            results[n]['avg_agreement'] += agreement\n",
    "            results[n]['latency'] += (total_time * n / max_n)  # Proportional\n",
    "            \n",
    "            print(f\"  n={n}: pred={predicted}, correct={correct}, agreement={agreement:.1%}\")\n",
    "    \n",
    "    # Calculate summaries\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SELF-CONSISTENCY EVALUATION RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Samples':<10} {'Accuracy':<12} {'Avg Agreement':<15} {'Avg Latency':<12}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for n in samples_to_test:\n",
    "        data = results[n]\n",
    "        accuracy = data['correct'] / data['total'] if data['total'] > 0 else 0\n",
    "        avg_agree = data['avg_agreement'] / data['total'] if data['total'] > 0 else 0\n",
    "        avg_lat = data['latency'] / data['total'] if data['total'] > 0 else 0\n",
    "        \n",
    "        print(f\"n={n:<7} {accuracy:<12.1%} {avg_agree:<15.1%} {avg_lat:<12.2f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Load test problems\n",
    "try:\n",
    "    with open(\"../data/gsm8k_sample.json\") as f:\n",
    "        problems = json.load(f)\n",
    "    \n",
    "    # Run evaluation (uncomment to execute)\n",
    "    # comprehensive_sc_evaluation(problems, n_problems=5, samples_to_test=[1, 3, 5])\n",
    "    print(\"Problems loaded. Uncomment the evaluation call to run.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Sample problems not found. Skipping evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Temperature matters**: Use temperature > 0 (e.g., 0.7) for diverse reasoning paths\n",
    "2. **More samples = higher accuracy** but with diminishing returns (usually 5-10 is sufficient)\n",
    "3. **Weighted voting** can improve over simple majority voting\n",
    "4. **Adaptive sampling** saves compute on easy problems\n",
    "5. **Agreement rate** is a good proxy for confidence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
