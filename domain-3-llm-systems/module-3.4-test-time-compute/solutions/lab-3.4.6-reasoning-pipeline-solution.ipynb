{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.4.6: Reasoning Pipeline - SOLUTIONS\n",
    "\n",
    "This notebook contains complete solutions to all exercises from Lab 3.4.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import re\n",
    "from typing import Dict, List, Optional, Tuple, Callable\n",
    "from dataclasses import dataclass, field\n",
    "from collections import OrderedDict\n",
    "from enum import Enum\n",
    "\n",
    "# Model configuration\n",
    "models = ollama.list()\n",
    "model_names = [m['name'] for m in models.get('models', [])]\n",
    "\n",
    "# Find models\n",
    "FAST_MODEL = None\n",
    "REASONING_MODEL = None\n",
    "\n",
    "for name in model_names:\n",
    "    if any(x in name.lower() for x in ['7b', '8b', '3b']) and 'r1' not in name.lower() and not FAST_MODEL:\n",
    "        FAST_MODEL = name\n",
    "    if 'r1' in name.lower() and not REASONING_MODEL:\n",
    "        REASONING_MODEL = name\n",
    "\n",
    "FAST_MODEL = FAST_MODEL or (model_names[0] if model_names else \"llama3.1:8b\")\n",
    "REASONING_MODEL = REASONING_MODEL or FAST_MODEL\n",
    "\n",
    "print(f\"Fast Model: {FAST_MODEL}\")\n",
    "print(f\"Reasoning Model: {REASONING_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Query Complexity Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryComplexity(Enum):\n",
    "    \"\"\"Query complexity levels.\"\"\"\n",
    "    SIMPLE = \"simple\"\n",
    "    MODERATE = \"moderate\"\n",
    "    COMPLEX = \"complex\"\n",
    "\n",
    "\n",
    "class EnhancedComplexityClassifier:\n",
    "    \"\"\"\n",
    "    Solution: Enhanced complexity classifier with configurable weights.\n",
    "    \n",
    "    Features:\n",
    "    - Configurable keyword weights\n",
    "    - Pattern-based detection\n",
    "    - Length and structure analysis\n",
    "    - Detailed logging\n",
    "    \"\"\"\n",
    "    \n",
    "    # Weighted keywords\n",
    "    COMPLEX_KEYWORDS = {\n",
    "        'solve': 3, 'calculate': 2, 'prove': 4, 'derive': 4,\n",
    "        'analyze': 2, 'step by step': 3, 'explain how': 2,\n",
    "        'why does': 2, 'compare and contrast': 3, 'what if': 2,\n",
    "        'optimize': 3, 'debug': 2, 'implement': 3, 'algorithm': 3,\n",
    "        'mathematical': 3, 'equation': 2, 'probability': 3,\n",
    "        'logic puzzle': 4, 'proof': 4, 'theorem': 4,\n",
    "    }\n",
    "    \n",
    "    SIMPLE_KEYWORDS = {\n",
    "        'what is': -2, 'who is': -2, 'when did': -2,\n",
    "        'where is': -2, 'define': -1, 'capital of': -3,\n",
    "        'how many': -1, 'true or false': -2, 'yes or no': -2,\n",
    "        'list': -1, 'name': -1,\n",
    "    }\n",
    "    \n",
    "    MATH_PATTERNS = [\n",
    "        (r'\\d+\\s*[+\\-*/]\\s*\\d+', 2),  # Arithmetic\n",
    "        (r'\\d+%\\s*of', 2),  # Percentage\n",
    "        (r'equation|formula', 3),\n",
    "        (r'\\$\\d+', 1),  # Money\n",
    "        (r'\\d+\\s*(mph|km|miles|meters)', 2),  # Word problems\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, simple_threshold: int = 0, complex_threshold: int = 4):\n",
    "        self.simple_threshold = simple_threshold\n",
    "        self.complex_threshold = complex_threshold\n",
    "        self.classifications = []\n",
    "        self.detailed_logs = []\n",
    "    \n",
    "    def classify(self, query: str, log_details: bool = False) -> QueryComplexity:\n",
    "        \"\"\"Classify query complexity with optional detailed logging.\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        score = 0\n",
    "        details = {'query': query[:50], 'factors': []}\n",
    "        \n",
    "        # Check complex keywords\n",
    "        for keyword, weight in self.COMPLEX_KEYWORDS.items():\n",
    "            if keyword in query_lower:\n",
    "                score += weight\n",
    "                details['factors'].append(f\"+{weight} ('{keyword}')\")\n",
    "        \n",
    "        # Check simple keywords\n",
    "        for keyword, weight in self.SIMPLE_KEYWORDS.items():\n",
    "            if keyword in query_lower:\n",
    "                score += weight\n",
    "                details['factors'].append(f\"{weight} ('{keyword}')\")\n",
    "        \n",
    "        # Check math patterns\n",
    "        for pattern, weight in self.MATH_PATTERNS:\n",
    "            if re.search(pattern, query_lower):\n",
    "                score += weight\n",
    "                details['factors'].append(f\"+{weight} (pattern: {pattern[:20]})\")\n",
    "        \n",
    "        # Length factor\n",
    "        if len(query) > 200:\n",
    "            score += 1\n",
    "            details['factors'].append(\"+1 (long query)\")\n",
    "        if len(query) > 400:\n",
    "            score += 1\n",
    "            details['factors'].append(\"+1 (very long query)\")\n",
    "        \n",
    "        # Multiple sentences\n",
    "        sentence_count = len(re.split(r'[.!?]', query))\n",
    "        if sentence_count > 3:\n",
    "            score += 1\n",
    "            details['factors'].append(f\"+1 ({sentence_count} sentences)\")\n",
    "        \n",
    "        # Question marks (multiple questions = more complex)\n",
    "        q_count = query.count('?')\n",
    "        if q_count > 1:\n",
    "            score += q_count - 1\n",
    "            details['factors'].append(f\"+{q_count-1} (multiple questions)\")\n",
    "        \n",
    "        # Classify\n",
    "        details['score'] = score\n",
    "        \n",
    "        if score <= self.simple_threshold:\n",
    "            result = QueryComplexity.SIMPLE\n",
    "        elif score >= self.complex_threshold:\n",
    "            result = QueryComplexity.COMPLEX\n",
    "        else:\n",
    "            result = QueryComplexity.MODERATE\n",
    "        \n",
    "        details['result'] = result.value\n",
    "        self.classifications.append((query[:50], result))\n",
    "        \n",
    "        if log_details:\n",
    "            self.detailed_logs.append(details)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get classification statistics.\"\"\"\n",
    "        counts = {c: 0 for c in QueryComplexity}\n",
    "        for _, complexity in self.classifications:\n",
    "            counts[complexity] += 1\n",
    "        \n",
    "        return {\n",
    "            'total': len(self.classifications),\n",
    "            'by_complexity': {c.value: count for c, count in counts.items()},\n",
    "        }\n",
    "    \n",
    "    def print_detailed_log(self, n: int = 5):\n",
    "        \"\"\"Print detailed classification logs.\"\"\"\n",
    "        for log in self.detailed_logs[-n:]:\n",
    "            print(f\"\\nQuery: {log['query']}...\")\n",
    "            print(f\"  Score: {log['score']} -> {log['result']}\")\n",
    "            print(f\"  Factors: {', '.join(log['factors'][:5])}\")\n",
    "\n",
    "\n",
    "# Test enhanced classifier\n",
    "classifier = EnhancedComplexityClassifier()\n",
    "\n",
    "test_queries = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain how neural networks learn.\",\n",
    "    \"Solve step by step: A train leaves at 9am at 60mph...\",\n",
    "    \"Is Python interpreted?\",\n",
    "]\n",
    "\n",
    "print(\"Enhanced Classifier Test:\")\n",
    "for q in test_queries:\n",
    "    result = classifier.classify(q, log_details=True)\n",
    "    print(f\"  [{result.value:8}] {q[:50]}...\")\n",
    "\n",
    "print(\"\\nDetailed Logs:\")\n",
    "classifier.print_detailed_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: LRU Cache with TTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRUCacheWithTTL:\n",
    "    \"\"\"\n",
    "    Solution: LRU Cache with Time-To-Live support.\n",
    "    \n",
    "    Features:\n",
    "    - LRU eviction when full\n",
    "    - TTL-based expiration\n",
    "    - Detailed statistics\n",
    "    - Optional semantic key hashing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int = 100, ttl_seconds: float = 3600):\n",
    "        self.max_size = max_size\n",
    "        self.ttl_seconds = ttl_seconds\n",
    "        self.cache = OrderedDict()\n",
    "        self.timestamps = {}\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "        self.expirations = 0\n",
    "    \n",
    "    def _make_key(self, query: str, model: str) -> str:\n",
    "        \"\"\"Create cache key.\"\"\"\n",
    "        combined = f\"{model}:{query}\"\n",
    "        return hashlib.md5(combined.encode()).hexdigest()\n",
    "    \n",
    "    def _is_expired(self, key: str) -> bool:\n",
    "        \"\"\"Check if entry has expired.\"\"\"\n",
    "        if key not in self.timestamps:\n",
    "            return True\n",
    "        age = time.time() - self.timestamps[key]\n",
    "        return age > self.ttl_seconds\n",
    "    \n",
    "    def get(self, query: str, model: str) -> Optional[str]:\n",
    "        \"\"\"Get cached response if available and not expired.\"\"\"\n",
    "        key = self._make_key(query, model)\n",
    "        \n",
    "        if key not in self.cache:\n",
    "            self.misses += 1\n",
    "            return None\n",
    "        \n",
    "        if self._is_expired(key):\n",
    "            # Remove expired entry\n",
    "            del self.cache[key]\n",
    "            del self.timestamps[key]\n",
    "            self.expirations += 1\n",
    "            self.misses += 1\n",
    "            return None\n",
    "        \n",
    "        # Move to end (most recently used)\n",
    "        self.cache.move_to_end(key)\n",
    "        self.hits += 1\n",
    "        return self.cache[key]\n",
    "    \n",
    "    def set(self, query: str, model: str, response: str):\n",
    "        \"\"\"Cache a response.\"\"\"\n",
    "        key = self._make_key(query, model)\n",
    "        \n",
    "        if key in self.cache:\n",
    "            self.cache.move_to_end(key)\n",
    "        else:\n",
    "            if len(self.cache) >= self.max_size:\n",
    "                # Evict oldest\n",
    "                oldest_key = next(iter(self.cache))\n",
    "                del self.cache[oldest_key]\n",
    "                if oldest_key in self.timestamps:\n",
    "                    del self.timestamps[oldest_key]\n",
    "        \n",
    "        self.cache[key] = response\n",
    "        self.timestamps[key] = time.time()\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear the cache.\"\"\"\n",
    "        self.cache.clear()\n",
    "        self.timestamps.clear()\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "        self.expirations = 0\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get cache statistics.\"\"\"\n",
    "        total = self.hits + self.misses\n",
    "        return {\n",
    "            'size': len(self.cache),\n",
    "            'max_size': self.max_size,\n",
    "            'ttl_seconds': self.ttl_seconds,\n",
    "            'hits': self.hits,\n",
    "            'misses': self.misses,\n",
    "            'expirations': self.expirations,\n",
    "            'hit_rate': self.hits / total if total > 0 else 0,\n",
    "        }\n",
    "\n",
    "\n",
    "# Test cache\n",
    "cache = LRUCacheWithTTL(max_size=10, ttl_seconds=60)\n",
    "cache.set(\"test query\", \"model\", \"test response\")\n",
    "print(f\"Cache hit: {cache.get('test query', 'model')[:20]}...\")\n",
    "print(f\"Cache miss: {cache.get('unknown', 'model')}\")\n",
    "print(f\"Stats: {cache.get_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Complete Reasoning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PipelineResponse:\n",
    "    \"\"\"Response from the reasoning pipeline.\"\"\"\n",
    "    response: str\n",
    "    model_used: str\n",
    "    complexity: QueryComplexity\n",
    "    from_cache: bool\n",
    "    latency: float\n",
    "    thinking_tokens: int = 0\n",
    "\n",
    "\n",
    "class ProductionReasoningPipeline:\n",
    "    \"\"\"\n",
    "    Solution: Production-ready adaptive reasoning pipeline.\n",
    "    \n",
    "    Features:\n",
    "    - Intelligent complexity-based routing\n",
    "    - TTL-enabled response caching\n",
    "    - Fallback handling\n",
    "    - Comprehensive metrics\n",
    "    - Configurable thresholds\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        fast_model: str = FAST_MODEL,\n",
    "        reasoning_model: str = REASONING_MODEL,\n",
    "        cache_size: int = 100,\n",
    "        cache_ttl: float = 3600,\n",
    "        timeout: float = 30.0,\n",
    "    ):\n",
    "        self.fast_model = fast_model\n",
    "        self.reasoning_model = reasoning_model\n",
    "        self.timeout = timeout\n",
    "        \n",
    "        self.classifier = EnhancedComplexityClassifier()\n",
    "        self.cache = LRUCacheWithTTL(max_size=cache_size, ttl_seconds=cache_ttl)\n",
    "        \n",
    "        # Metrics\n",
    "        self.total_queries = 0\n",
    "        self.fast_calls = 0\n",
    "        self.reasoning_calls = 0\n",
    "        self.cache_hits = 0\n",
    "        self.fallbacks = 0\n",
    "        self.total_latency = 0.0\n",
    "        self.errors = []\n",
    "    \n",
    "    def query(\n",
    "        self,\n",
    "        query: str,\n",
    "        use_cache: bool = True,\n",
    "        force_model: str = None,\n",
    "        verbose: bool = False,\n",
    "    ) -> PipelineResponse:\n",
    "        \"\"\"Process a query through the pipeline.\"\"\"\n",
    "        self.total_queries += 1\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Classify\n",
    "        complexity = self.classifier.classify(query)\n",
    "        if verbose:\n",
    "            print(f\"Complexity: {complexity.value}\")\n",
    "        \n",
    "        # Select model\n",
    "        if force_model:\n",
    "            model = force_model\n",
    "        elif complexity == QueryComplexity.COMPLEX:\n",
    "            model = self.reasoning_model\n",
    "        else:\n",
    "            model = self.fast_model\n",
    "        \n",
    "        use_cot = complexity in [QueryComplexity.MODERATE, QueryComplexity.COMPLEX]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Model: {model}, CoT: {use_cot}\")\n",
    "        \n",
    "        # Check cache\n",
    "        if use_cache:\n",
    "            cached = self.cache.get(query, model)\n",
    "            if cached:\n",
    "                self.cache_hits += 1\n",
    "                latency = time.time() - start_time\n",
    "                if verbose:\n",
    "                    print(\"Cache HIT\")\n",
    "                return PipelineResponse(\n",
    "                    response=cached,\n",
    "                    model_used=model,\n",
    "                    complexity=complexity,\n",
    "                    from_cache=True,\n",
    "                    latency=latency,\n",
    "                )\n",
    "        \n",
    "        # Build prompt\n",
    "        prompt = f\"{query}\\n\\nLet's think step by step:\" if use_cot else query\n",
    "        \n",
    "        # Generate with fallback\n",
    "        try:\n",
    "            response = ollama.chat(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                options={\"temperature\": 0.0, \"num_predict\": 1024}\n",
    "            )\n",
    "            response_text = response['message']['content']\n",
    "            \n",
    "            if model == self.reasoning_model:\n",
    "                self.reasoning_calls += 1\n",
    "            else:\n",
    "                self.fast_calls += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Fallback to fast model\n",
    "            self.fallbacks += 1\n",
    "            self.errors.append(str(e))\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Error with {model}, falling back...\")\n",
    "            \n",
    "            response = ollama.chat(\n",
    "                model=self.fast_model,\n",
    "                messages=[{\"role\": \"user\", \"content\": query}],\n",
    "                options={\"temperature\": 0.0, \"num_predict\": 512}\n",
    "            )\n",
    "            response_text = response['message']['content']\n",
    "            model = self.fast_model\n",
    "            self.fast_calls += 1\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        self.total_latency += latency\n",
    "        \n",
    "        # Count thinking tokens\n",
    "        thinking_matches = re.findall(r'<think>.*?</think>', response_text, re.DOTALL)\n",
    "        thinking_tokens = sum(len(m) // 4 for m in thinking_matches)\n",
    "        \n",
    "        # Cache response\n",
    "        if use_cache:\n",
    "            self.cache.set(query, model, response_text)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Latency: {latency:.2f}s\")\n",
    "        \n",
    "        return PipelineResponse(\n",
    "            response=response_text,\n",
    "            model_used=model,\n",
    "            complexity=complexity,\n",
    "            from_cache=False,\n",
    "            latency=latency,\n",
    "            thinking_tokens=thinking_tokens,\n",
    "        )\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get comprehensive pipeline statistics.\"\"\"\n",
    "        return {\n",
    "            'total_queries': self.total_queries,\n",
    "            'fast_calls': self.fast_calls,\n",
    "            'reasoning_calls': self.reasoning_calls,\n",
    "            'cache_hits': self.cache_hits,\n",
    "            'cache_hit_rate': self.cache_hits / max(self.total_queries, 1),\n",
    "            'fallbacks': self.fallbacks,\n",
    "            'total_latency': self.total_latency,\n",
    "            'avg_latency': self.total_latency / max(self.total_queries - self.cache_hits, 1),\n",
    "            'routing_efficiency': self.fast_calls / max(self.fast_calls + self.reasoning_calls, 1),\n",
    "            'cache_stats': self.cache.get_stats(),\n",
    "            'classifier_stats': self.classifier.get_stats(),\n",
    "        }\n",
    "    \n",
    "    def print_dashboard(self):\n",
    "        \"\"\"Print a dashboard of pipeline statistics.\"\"\"\n",
    "        stats = self.get_stats()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"REASONING PIPELINE DASHBOARD\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\nQuery Statistics:\")\n",
    "        print(f\"  Total queries:      {stats['total_queries']}\")\n",
    "        print(f\"  Fast model calls:   {stats['fast_calls']}\")\n",
    "        print(f\"  Reasoning calls:    {stats['reasoning_calls']}\")\n",
    "        print(f\"  Cache hits:         {stats['cache_hits']} ({stats['cache_hit_rate']:.1%})\")\n",
    "        print(f\"  Fallbacks:          {stats['fallbacks']}\")\n",
    "        \n",
    "        print(f\"\\nPerformance:\")\n",
    "        print(f\"  Total latency:      {stats['total_latency']:.1f}s\")\n",
    "        print(f\"  Avg latency:        {stats['avg_latency']:.2f}s\")\n",
    "        print(f\"  Routing efficiency: {stats['routing_efficiency']:.1%}\")\n",
    "        \n",
    "        class_stats = stats['classifier_stats']['by_complexity']\n",
    "        print(f\"\\nComplexity Distribution:\")\n",
    "        for comp, count in class_stats.items():\n",
    "            print(f\"  {comp}: {count}\")\n",
    "        \n",
    "        print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Test the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "pipeline = ProductionReasoningPipeline(\n",
    "    fast_model=FAST_MODEL,\n",
    "    reasoning_model=REASONING_MODEL,\n",
    "    cache_size=50,\n",
    "    cache_ttl=300,\n",
    ")\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is the capital of Japan?\",  # Simple\n",
    "    \"Explain machine learning.\",  # Moderate\n",
    "    \"Solve step by step: 3x + 7 = 22\",  # Complex\n",
    "    \"What is the capital of Japan?\",  # Cache hit\n",
    "]\n",
    "\n",
    "print(\"Testing Pipeline:\\n\")\n",
    "for q in test_queries:\n",
    "    print(f\"Q: {q}\")\n",
    "    result = pipeline.query(q, verbose=True)\n",
    "    print(f\"  Response: {result.response[:80]}...\")\n",
    "    print()\n",
    "\n",
    "pipeline.print_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Complexity classification** routes queries to appropriate models\n",
    "2. **Caching with TTL** prevents stale data while saving compute\n",
    "3. **Fallback handling** ensures reliability\n",
    "4. **Metrics tracking** enables optimization\n",
    "5. **DGX Spark** can run both models simultaneously (128GB memory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
