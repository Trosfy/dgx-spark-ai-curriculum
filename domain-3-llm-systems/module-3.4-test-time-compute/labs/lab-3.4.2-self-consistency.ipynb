{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.4.2: Self-Consistency Implementation\n",
    "\n",
    "**Module:** 3.4 - Test-Time Compute & Reasoning  \n",
    "**Time:** 1.5 hours  \n",
    "**Difficulty:** ⭐⭐⭐ (Intermediate-Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- [ ] Understand why multiple reasoning paths improve accuracy\n",
    "- [ ] Implement self-consistency with majority voting\n",
    "- [ ] Experiment with different sampling parameters (N, temperature)\n",
    "- [ ] Measure the accuracy improvement over single-sample CoT\n",
    "- [ ] Understand the cost-quality tradeoff of self-consistency\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Lab 3.4.1 (Chain-of-Thought Workshop)\n",
    "- Ollama with a model loaded\n",
    "- Understanding of CoT prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "In 2022, Google researchers discovered that generating multiple reasoning paths and taking a **majority vote** consistently outperformed single-sample Chain-of-Thought. This technique, called **Self-Consistency**, became a key ingredient in achieving state-of-the-art results on reasoning benchmarks.\n",
    "\n",
    "**Why does this matter?**\n",
    "- OpenAI's o1 uses similar multi-path reasoning internally\n",
    "- High-stakes AI decisions (medical, legal) benefit from consensus\n",
    "- It's a simple way to trade compute for accuracy\n",
    "\n",
    "**Industry Applications:**\n",
    "- **Medical Diagnosis:** Multiple diagnostic pathways, consensus on likely condition\n",
    "- **Code Generation:** Generate multiple solutions, pick the most common pattern\n",
    "- **Financial Modeling:** Multiple calculation approaches, validate consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: Self-Consistency\n",
    "\n",
    "> **Imagine you're lost in a forest...**\n",
    ">\n",
    "> You ask 5 different hikers for directions to the lake.\n",
    "> - 3 hikers say \"Go North\"\n",
    "> - 1 hiker says \"Go East\" \n",
    "> - 1 hiker says \"Go West\"\n",
    ">\n",
    "> You'd probably go North, right? The majority usually knows best!\n",
    ">\n",
    "> **That's exactly what Self-Consistency does with AI:**\n",
    "> 1. Ask the model the same question multiple times\n",
    "> 2. Use temperature > 0 so each answer takes a slightly different path\n",
    "> 3. Take a vote on the final answer\n",
    ">\n",
    "> Even if some paths make mistakes, the majority often converges on the right answer!\n",
    "\n",
    "```\n",
    "Single CoT:     Problem ─> Path1 ─> Answer (hope it's right!)\n",
    "\n",
    "Self-Consistency:\n",
    "                Problem ─> Path1 ─> Answer A ┐\n",
    "                Problem ─> Path2 ─> Answer A ├─> VOTE ─> Answer A wins!\n",
    "                Problem ─> Path3 ─> Answer A │\n",
    "                Problem ─> Path4 ─> Answer B │\n",
    "                Problem ─> Path5 ─> Answer A ┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import ollama\n",
    "\n",
    "# Configuration\n",
    "MODEL = \"qwen3:8b\"  # Change to your available model\n",
    "\n",
    "# For DGX Spark with 128GB unified memory:\n",
    "# MODEL = \"qwen3:32b\"  # Better reasoning, ~45GB\n",
    "\n",
    "print(f\"Using model: {MODEL}\")\n",
    "\n",
    "# Test connection\n",
    "response = ollama.chat(\n",
    "    model=MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say 'ready'\"}],\n",
    "    options={\"num_predict\": 10}\n",
    ")\n",
    "print(f\"Model response: {response['message']['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test problems\n",
    "data_path = Path(\"../data/gsm8k_sample.json\")\n",
    "if data_path.exists():\n",
    "    with open(data_path) as f:\n",
    "        problems = json.load(f)\n",
    "else:\n",
    "    # Fallback sample\n",
    "    problems = [\n",
    "        {\n",
    "            \"question\": \"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\",\n",
    "            \"numerical_answer\": 18\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take?\",\n",
    "            \"numerical_answer\": 3\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Josh decides to try flipping a house. He buys a house for $80,000 and then puts in $50,000 in repairs. This increased the value of the house by 150%. How much profit did he make?\",\n",
    "            \"numerical_answer\": 70000\n",
    "        },\n",
    "    ]\n",
    "\n",
    "print(f\"Loaded {len(problems)} problems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>---\n\n## Part 2: Building the Self-Consistency Implementation\n\nLet's implement self-consistency step by step.\n\n### Key Python Tool: Counter for Majority Voting\n\nThe `Counter` class from Python's `collections` module is perfect for counting votes:\n\n```python\nfrom collections import Counter\n\n# Count occurrences in a list\nanswers = [\"42\", \"42\", \"41\", \"42\", \"43\"]\ncounter = Counter(answers)\n# Counter({'42': 3, '41': 1, '43': 1})\n\n# Get the most common item(s)\nmost_common = counter.most_common(1)  # Returns [('42', 3)]\nwinner, count = most_common[0]        # winner='42', count=3\n```\n\nThis makes implementing majority voting simple and efficient!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(response: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract the final answer from a CoT response.\n",
    "    \n",
    "    Returns the answer as a string for voting (handles both numbers and text).\n",
    "    \"\"\"\n",
    "    # Patterns to look for\n",
    "    patterns = [\n",
    "        r\"[Tt]he (?:final )?answer is[:\\s]+\\$?([\\d,]+(?:\\.\\d+)?)\",\n",
    "        r\"[Aa]nswer[:\\s]+\\$?([\\d,]+(?:\\.\\d+)?)\",\n",
    "        r\"=\\s*\\$?([\\d,]+(?:\\.\\d+)?)\\s*(?:$|\\.|\\n|dollars|miles|hours)\",\n",
    "        r\"\\$\\s*([\\d,]+(?:\\.\\d+)?)\",\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, response)\n",
    "        if matches:\n",
    "            # Return the last match (usually the final answer)\n",
    "            answer = matches[-1].replace(',', '')\n",
    "            # Normalize: convert to float then back to string\n",
    "            try:\n",
    "                num = float(answer)\n",
    "                # Return integer if whole number\n",
    "                if num == int(num):\n",
    "                    return str(int(num))\n",
    "                return str(num)\n",
    "            except:\n",
    "                return answer\n",
    "    \n",
    "    # Fallback: last number in response\n",
    "    all_numbers = re.findall(r'-?[\\d,]+(?:\\.\\d+)?', response)\n",
    "    if all_numbers:\n",
    "        answer = all_numbers[-1].replace(',', '')\n",
    "        try:\n",
    "            num = float(answer)\n",
    "            if num == int(num):\n",
    "                return str(int(num))\n",
    "            return str(num)\n",
    "        except:\n",
    "            return answer\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "# Test extraction\n",
    "test_cases = [\n",
    "    \"So the total is 42. The answer is 42.\",\n",
    "    \"After calculation, we get $18 per day.\",\n",
    "    \"The profit = 200000 - 80000 - 50000 = 70000\",\n",
    "]\n",
    "\n",
    "print(\"Testing answer extraction:\")\n",
    "for tc in test_cases:\n",
    "    ans = extract_answer(tc)\n",
    "    print(f\"  '{tc[:40]}...' -> {ans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_consistency(\n",
    "    question: str,\n",
    "    n_samples: int = 5,\n",
    "    temperature: float = 0.7,\n",
    "    model: str = MODEL,\n",
    "    verbose: bool = True,\n",
    ") -> Tuple[Optional[str], float, List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Implement self-consistency: generate multiple reasoning paths and vote.\n",
    "    \n",
    "    Args:\n",
    "        question: The problem to solve\n",
    "        n_samples: Number of reasoning paths to generate\n",
    "        temperature: Sampling temperature (>0 for diversity)\n",
    "        model: Model to use\n",
    "        verbose: Whether to print progress\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (best_answer, confidence, all_answers, all_responses)\n",
    "    \"\"\"\n",
    "    prompt = f\"{question}\\n\\nLet's think step by step:\"\n",
    "    \n",
    "    answers = []\n",
    "    responses = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        if verbose:\n",
    "            print(f\"  Generating path {i+1}/{n_samples}...\", end=\" \", flush=True)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            options={\"temperature\": temperature, \"num_predict\": 512}\n",
    "        )\n",
    "        \n",
    "        response_text = response['message']['content']\n",
    "        responses.append(response_text)\n",
    "        \n",
    "        # Extract answer\n",
    "        answer = extract_answer(response_text)\n",
    "        answers.append(answer)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        if verbose:\n",
    "            print(f\"Answer: {answer} ({elapsed:.1f}s)\")\n",
    "    \n",
    "    # Filter out None values for voting\n",
    "    valid_answers = [a for a in answers if a is not None]\n",
    "    \n",
    "    if not valid_answers:\n",
    "        return None, 0.0, answers, responses\n",
    "    \n",
    "    # Majority vote\n",
    "    counter = Counter(valid_answers)\n",
    "    best_answer, count = counter.most_common(1)[0]\n",
    "    confidence = count / len(valid_answers)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n  Vote distribution: {dict(counter)}\")\n",
    "        print(f\"  Winner: {best_answer} (confidence: {confidence:.0%})\")\n",
    "    \n",
    "    return best_answer, confidence, answers, responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test self-consistency on one problem\n",
    "test_problem = problems[0]\n",
    "\n",
    "print(f\"Problem: {test_problem['question'][:80]}...\")\n",
    "print(f\"Expected answer: {test_problem['numerical_answer']}\")\n",
    "print(\"\\nRunning self-consistency with N=5:\\n\")\n",
    "\n",
    "best, confidence, all_answers, responses = self_consistency(\n",
    "    test_problem['question'],\n",
    "    n_samples=5,\n",
    "    temperature=0.7,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal answer: {best}\")\n",
    "print(f\"Correct: {str(test_problem['numerical_answer']) == best}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "We generated 5 different reasoning paths with temperature=0.7. Notice:\n",
    "- Each path might take slightly different approaches\n",
    "- Sometimes paths lead to different answers\n",
    "- The majority vote helps filter out occasional errors\n",
    "\n",
    "**Confidence** tells us how strongly the answers agreed. High confidence (80%+) is a good sign!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Comparing Single-Sample vs Self-Consistency\n",
    "\n",
    "Let's systematically compare the accuracy of single-sample CoT vs self-consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_cot(problems: List[Dict], n_problems: int = 5) -> Dict:\n",
    "    \"\"\"Evaluate single-sample CoT (temperature=0).\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, prob in enumerate(problems[:n_problems]):\n",
    "        print(f\"Problem {i+1}/{n_problems}...\", end=\" \", flush=True)\n",
    "        \n",
    "        prompt = f\"{prob['question']}\\n\\nLet's think step by step:\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = ollama.chat(\n",
    "            model=MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            options={\"temperature\": 0.0, \"num_predict\": 512}\n",
    "        )\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        response_text = response['message']['content']\n",
    "        predicted = extract_answer(response_text)\n",
    "        expected = str(prob['numerical_answer'])\n",
    "        correct = predicted == expected\n",
    "        \n",
    "        results.append({\n",
    "            'predicted': predicted,\n",
    "            'expected': expected,\n",
    "            'correct': correct,\n",
    "            'time': elapsed,\n",
    "        })\n",
    "        \n",
    "        status = \"CORRECT\" if correct else \"WRONG\"\n",
    "        print(f\"{status} (pred={predicted}, exp={expected})\")\n",
    "    \n",
    "    accuracy = sum(1 for r in results if r['correct']) / len(results)\n",
    "    avg_time = sum(r['time'] for r in results) / len(results)\n",
    "    \n",
    "    return {\n",
    "        'method': 'single_cot',\n",
    "        'accuracy': accuracy,\n",
    "        'avg_time': avg_time,\n",
    "        'results': results,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_self_consistency(\n",
    "    problems: List[Dict],\n",
    "    n_problems: int = 5,\n",
    "    n_samples: int = 5,\n",
    "    temperature: float = 0.7,\n",
    ") -> Dict:\n",
    "    \"\"\"Evaluate self-consistency.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, prob in enumerate(problems[:n_problems]):\n",
    "        print(f\"\\nProblem {i+1}/{n_problems}:\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        best_answer, confidence, all_answers, _ = self_consistency(\n",
    "            prob['question'],\n",
    "            n_samples=n_samples,\n",
    "            temperature=temperature,\n",
    "            verbose=True,\n",
    "        )\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        expected = str(prob['numerical_answer'])\n",
    "        correct = best_answer == expected\n",
    "        \n",
    "        results.append({\n",
    "            'predicted': best_answer,\n",
    "            'expected': expected,\n",
    "            'correct': correct,\n",
    "            'confidence': confidence,\n",
    "            'all_answers': all_answers,\n",
    "            'time': elapsed,\n",
    "        })\n",
    "        \n",
    "        status = \"CORRECT\" if correct else \"WRONG\"\n",
    "        print(f\"  Result: {status} (pred={best_answer}, exp={expected})\")\n",
    "    \n",
    "    accuracy = sum(1 for r in results if r['correct']) / len(results)\n",
    "    avg_time = sum(r['time'] for r in results) / len(results)\n",
    "    avg_conf = sum(r['confidence'] for r in results) / len(results)\n",
    "    \n",
    "    return {\n",
    "        'method': 'self_consistency',\n",
    "        'n_samples': n_samples,\n",
    "        'temperature': temperature,\n",
    "        'accuracy': accuracy,\n",
    "        'avg_time': avg_time,\n",
    "        'avg_confidence': avg_conf,\n",
    "        'results': results,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comparison\n",
    "N_PROBLEMS = 5  # Use 5 problems for quick testing\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUATING SINGLE-SAMPLE COT (temperature=0)\")\n",
    "print(\"=\"*60)\n",
    "single_results = evaluate_single_cot(problems, n_problems=N_PROBLEMS)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING SELF-CONSISTENCY (N=5, temperature=0.7)\")\n",
    "print(\"=\"*60)\n",
    "sc_results = evaluate_self_consistency(\n",
    "    problems,\n",
    "    n_problems=N_PROBLEMS,\n",
    "    n_samples=5,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n{'Method':<25} {'Accuracy':<12} {'Avg Time':<12}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Single-sample CoT':<25} {single_results['accuracy']:<12.1%} {single_results['avg_time']:<12.1f}s\")\n",
    "print(f\"{'Self-Consistency (N=5)':<25} {sc_results['accuracy']:<12.1%} {sc_results['avg_time']:<12.1f}s\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "improvement = sc_results['accuracy'] - single_results['accuracy']\n",
    "time_ratio = sc_results['avg_time'] / single_results['avg_time']\n",
    "\n",
    "print(f\"\\nAccuracy improvement: {improvement:+.1%}\")\n",
    "print(f\"Time increase: {time_ratio:.1f}x\")\n",
    "print(f\"Average confidence: {sc_results['avg_confidence']:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Experimenting with Parameters\n",
    "\n",
    "Two key parameters affect self-consistency:\n",
    "1. **N (number of samples):** More samples = more diverse paths = better voting\n",
    "2. **Temperature:** Higher = more diversity, but also more randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different N values\n",
    "n_values = [3, 5, 10]\n",
    "n_test_problems = 3  # Quick test with 3 problems\n",
    "\n",
    "results_by_n = {}\n",
    "\n",
    "for n in n_values:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing N={n}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    result = evaluate_self_consistency(\n",
    "        problems,\n",
    "        n_problems=n_test_problems,\n",
    "        n_samples=n,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    results_by_n[n] = result\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EFFECT OF N ON ACCURACY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'N':<10} {'Accuracy':<12} {'Avg Time':<12} {'Avg Confidence':<15}\")\n",
    "print(\"-\" * 50)\n",
    "for n, result in results_by_n.items():\n",
    "    print(f\"{n:<10} {result['accuracy']:<12.1%} {result['avg_time']:<12.1f}s {result['avg_confidence']:<15.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different temperatures\n",
    "temperatures = [0.3, 0.7, 1.0]\n",
    "\n",
    "results_by_temp = {}\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing temperature={temp}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    result = evaluate_self_consistency(\n",
    "        problems,\n",
    "        n_problems=n_test_problems,\n",
    "        n_samples=5,\n",
    "        temperature=temp\n",
    "    )\n",
    "    results_by_temp[temp] = result\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EFFECT OF TEMPERATURE ON ACCURACY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Temp':<10} {'Accuracy':<12} {'Avg Confidence':<15} {'Answer Diversity':<15}\")\n",
    "print(\"-\" * 55)\n",
    "for temp, result in results_by_temp.items():\n",
    "    # Calculate answer diversity (unique answers / total)\n",
    "    all_answers = [a for r in result['results'] for a in r['all_answers'] if a is not None]\n",
    "    diversity = len(set(all_answers)) / max(len(all_answers), 1)\n",
    "    print(f\"{temp:<10} {result['accuracy']:<12.1%} {result['avg_confidence']:<15.0%} {diversity:<15.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights on Parameter Tuning\n",
    "\n",
    "**Number of Samples (N):**\n",
    "- More samples generally helps, but with diminishing returns\n",
    "- N=5 is a good default balance\n",
    "- N=10+ for high-stakes decisions\n",
    "\n",
    "**Temperature:**\n",
    "- Too low (0.3): Answers are too similar, voting doesn't help much\n",
    "- Too high (1.0+): Answers become too random, more errors\n",
    "- Sweet spot: 0.5-0.8 for most tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Confidence as a Quality Signal\n",
    "\n",
    "One benefit of self-consistency: **confidence tells us when to trust the answer!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_confidence(results: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze the relationship between confidence and correctness.\n",
    "    \n",
    "    Higher confidence should correlate with higher accuracy!\n",
    "    \"\"\"\n",
    "    high_conf = [r for r in results['results'] if r['confidence'] >= 0.8]\n",
    "    low_conf = [r for r in results['results'] if r['confidence'] < 0.8]\n",
    "    \n",
    "    high_acc = sum(1 for r in high_conf if r['correct']) / max(len(high_conf), 1)\n",
    "    low_acc = sum(1 for r in low_conf if r['correct']) / max(len(low_conf), 1)\n",
    "    \n",
    "    return {\n",
    "        'high_confidence': {\n",
    "            'count': len(high_conf),\n",
    "            'accuracy': high_acc,\n",
    "        },\n",
    "        'low_confidence': {\n",
    "            'count': len(low_conf),\n",
    "            'accuracy': low_acc,\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "# Analyze our results\n",
    "conf_analysis = analyze_confidence(sc_results)\n",
    "\n",
    "print(\"Confidence vs Accuracy Analysis:\")\n",
    "print(f\"\\n  High confidence (>=80%):\")\n",
    "print(f\"    Count: {conf_analysis['high_confidence']['count']}\")\n",
    "print(f\"    Accuracy: {conf_analysis['high_confidence']['accuracy']:.0%}\")\n",
    "print(f\"\\n  Low confidence (<80%):\")\n",
    "print(f\"    Count: {conf_analysis['low_confidence']['count']}\")\n",
    "print(f\"    Accuracy: {conf_analysis['low_confidence']['accuracy']:.0%}\")\n",
    "\n",
    "print(\"\\n Key insight: When confidence is high, you can trust the answer more!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Cost-Quality Tradeoff\n",
    "\n",
    "Self-consistency trades compute for accuracy. Let's quantify this tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost_quality_tradeoff(single_results: Dict, sc_results: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate the cost (tokens, time) vs quality (accuracy) tradeoff.\n",
    "    \"\"\"\n",
    "    single_time = single_results['avg_time']\n",
    "    sc_time = sc_results['avg_time']\n",
    "    \n",
    "    single_acc = single_results['accuracy']\n",
    "    sc_acc = sc_results['accuracy']\n",
    "    \n",
    "    # Estimate tokens (rough: 1 token ~ 4 chars, avg 300 chars per response)\n",
    "    single_tokens = 75  # Rough estimate per response\n",
    "    sc_tokens = single_tokens * sc_results['n_samples']\n",
    "    \n",
    "    return {\n",
    "        'time_multiplier': sc_time / single_time,\n",
    "        'token_multiplier': sc_tokens / single_tokens,\n",
    "        'accuracy_gain': sc_acc - single_acc,\n",
    "        'accuracy_gain_per_token': (sc_acc - single_acc) / (sc_tokens - single_tokens) if sc_tokens > single_tokens else 0,\n",
    "    }\n",
    "\n",
    "\n",
    "tradeoff = calculate_cost_quality_tradeoff(single_results, sc_results)\n",
    "\n",
    "print(\"Cost-Quality Tradeoff Analysis:\")\n",
    "print(f\"\\n  Time multiplier: {tradeoff['time_multiplier']:.1f}x\")\n",
    "print(f\"  Token multiplier: {tradeoff['token_multiplier']:.0f}x\")\n",
    "print(f\"  Accuracy gain: {tradeoff['accuracy_gain']:+.1%}\")\n",
    "\n",
    "print(\"\\n Key question: Is the accuracy gain worth the extra compute?\")\n",
    "print(\"  - For high-stakes decisions: Usually YES\")\n",
    "print(\"  - For cost-sensitive applications: Consider smaller N\")\n",
    "print(\"  - For latency-critical: Maybe single CoT is better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Using Temperature = 0\n",
    "\n",
    "```python\n",
    "# Wrong: Temperature = 0 means all samples are identical!\n",
    "self_consistency(question, n_samples=5, temperature=0.0)\n",
    "# Result: All 5 samples give the same answer - voting is useless\n",
    "\n",
    "# Right: Use temperature > 0 for diversity\n",
    "self_consistency(question, n_samples=5, temperature=0.7)\n",
    "# Result: Different reasoning paths lead to meaningful voting\n",
    "```\n",
    "\n",
    "### Mistake 2: Not Normalizing Answers Before Voting\n",
    "\n",
    "```python\n",
    "# Wrong: Treating \"42\" and \"42.0\" and \"$42\" as different answers\n",
    "answers = [\"42\", \"42.0\", \"$42\", \"42\", \"42\"]\n",
    "Counter(answers)  # {'42': 2, '42.0': 1, '$42': 1, '42': 2}\n",
    "\n",
    "# Right: Normalize before voting\n",
    "answers = [normalize(a) for a in answers]  # [\"42\", \"42\", \"42\", \"42\", \"42\"]\n",
    "Counter(answers)  # {'42': 5} - Unanimous!\n",
    "```\n",
    "\n",
    "### Mistake 3: Too Few Samples\n",
    "\n",
    "```python\n",
    "# Risky: N=3 with split votes\n",
    "answers = [\"A\", \"A\", \"B\"]  # 2-1 split, not very confident\n",
    "\n",
    "# Better: N=5 for clearer majority\n",
    "answers = [\"A\", \"A\", \"A\", \"B\", \"B\"]  # 3-2, more confident\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ✅ **What self-consistency is:** Multiple paths + majority voting\n",
    "- ✅ **How to implement it:** Generate with temperature > 0, extract answers, vote\n",
    "- ✅ **Key parameters:** N (samples) and temperature\n",
    "- ✅ **Confidence as a signal:** High confidence = more trustworthy\n",
    "- ✅ **The tradeoff:** More compute → better accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge: Adaptive Self-Consistency\n",
    "\n",
    "Can we be smarter about when to use more samples? Implement \"early stopping\" - if the first 3 samples all agree, we probably don't need 10!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_self_consistency(\n",
    "    question: str,\n",
    "    min_samples: int = 3,\n",
    "    max_samples: int = 10,\n",
    "    confidence_threshold: float = 0.8,\n",
    "    temperature: float = 0.7,\n",
    "    model: str = MODEL,\n",
    ") -> Tuple[Optional[str], float, List[str], int]:\n",
    "    \"\"\"\n",
    "    Adaptive self-consistency: stop early if confident.\n",
    "    \n",
    "    Returns:\n",
    "        (best_answer, confidence, all_answers, n_samples_used)\n",
    "    \"\"\"\n",
    "    prompt = f\"{question}\\n\\nLet's think step by step:\"\n",
    "    answers = []\n",
    "    \n",
    "    for i in range(max_samples):\n",
    "        # Generate sample\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            options={\"temperature\": temperature, \"num_predict\": 512}\n",
    "        )\n",
    "        \n",
    "        answer = extract_answer(response['message']['content'])\n",
    "        answers.append(answer)\n",
    "        \n",
    "        # Check if we can stop early (after min_samples)\n",
    "        if i + 1 >= min_samples:\n",
    "            valid_answers = [a for a in answers if a is not None]\n",
    "            if valid_answers:\n",
    "                counter = Counter(valid_answers)\n",
    "                best_answer, count = counter.most_common(1)[0]\n",
    "                confidence = count / len(valid_answers)\n",
    "                \n",
    "                if confidence >= confidence_threshold:\n",
    "                    print(f\"  Early stopping at {i+1} samples (confidence: {confidence:.0%})\")\n",
    "                    return best_answer, confidence, answers, i + 1\n",
    "    \n",
    "    # Used all samples\n",
    "    valid_answers = [a for a in answers if a is not None]\n",
    "    if not valid_answers:\n",
    "        return None, 0.0, answers, max_samples\n",
    "    \n",
    "    counter = Counter(valid_answers)\n",
    "    best_answer, count = counter.most_common(1)[0]\n",
    "    confidence = count / len(valid_answers)\n",
    "    \n",
    "    print(f\"  Used all {max_samples} samples (confidence: {confidence:.0%})\")\n",
    "    return best_answer, confidence, answers, max_samples\n",
    "\n",
    "\n",
    "# Test adaptive self-consistency\n",
    "print(\"Testing adaptive self-consistency:\")\n",
    "for i, prob in enumerate(problems[:3]):\n",
    "    print(f\"\\nProblem {i+1}: {prob['question'][:50]}...\")\n",
    "    answer, conf, all_ans, n_used = adaptive_self_consistency(\n",
    "        prob['question'],\n",
    "        min_samples=3,\n",
    "        max_samples=8,\n",
    "        confidence_threshold=0.8\n",
    "    )\n",
    "    print(f\"  Answer: {answer} (expected: {prob['numerical_answer']})\")\n",
    "    print(f\"  Samples used: {n_used}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>---\n\n## Further Reading\n\n- [Self-Consistency Paper](https://arxiv.org/abs/2203.11171) - The original Google research\n- [Universal Self-Consistency](https://arxiv.org/abs/2311.17311) - Works without answer extraction\n- [Complexity-Based Prompting](https://arxiv.org/abs/2210.01717) - Route by complexity\n\n---\n\n## Cleanup\n\nWe save our results and use `gc.collect()` to free up memory."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our results\n",
    "summary = {\n",
    "    'model': MODEL,\n",
    "    'single_cot_accuracy': single_results['accuracy'],\n",
    "    'self_consistency_accuracy': sc_results['accuracy'],\n",
    "    'improvement': sc_results['accuracy'] - single_results['accuracy'],\n",
    "    'n_samples': sc_results['n_samples'],\n",
    "    'avg_confidence': sc_results['avg_confidence'],\n",
    "}\n",
    "\n",
    "print(\"Summary saved:\")\n",
    "print(json.dumps(summary, indent=2))\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"\\nMemory cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Excellent work! You've mastered self-consistency, a key test-time compute strategy.\n",
    "\n",
    "In the next lab, you'll explore **DeepSeek-R1** - a state-of-the-art reasoning model that \"thinks out loud\" with explicit `<think>` tokens. This is self-consistency built into the model itself!\n",
    "\n",
    "**Continue to:** [Lab 3.4.3: DeepSeek-R1 Exploration](./lab-3.4.3-deepseek-r1.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}