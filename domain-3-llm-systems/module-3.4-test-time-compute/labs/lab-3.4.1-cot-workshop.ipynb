{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.4.1: Chain-of-Thought Workshop\n",
    "\n",
    "**Module:** 3.4 - Test-Time Compute & Reasoning  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐ (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- [ ] Understand why Chain-of-Thought (CoT) prompting dramatically improves reasoning\n",
    "- [ ] Implement zero-shot CoT with \"Let's think step by step\"\n",
    "- [ ] Create effective few-shot CoT examples\n",
    "- [ ] Measure accuracy improvements on math problems\n",
    "- [ ] Know when CoT helps (and when it doesn't)\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Module 3.3 (Deployment & Inference)\n",
    "- Ollama installed with a model loaded (e.g., `qwen3:8b`)\n",
    "- Basic Python and prompting experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "Before ChatGPT could solve math problems reliably, researchers discovered something remarkable: just asking the model to **\"think step by step\"** could dramatically improve accuracy. Google's 2022 paper showed Chain-of-Thought prompting boosted GSM8K math accuracy from ~18% to over 50% on PaLM-540B!\n",
    "\n",
    "Today, every major AI system uses reasoning strategies internally. When you ask Claude or GPT-4 a complex question, they're often \"thinking\" through the problem before responding. Understanding CoT is foundational to understanding how modern AI reasons.\n",
    "\n",
    "**Industry Applications:**\n",
    "- **Customer Support:** Breaking down complex troubleshooting into steps\n",
    "- **Code Review:** Analyzing code changes systematically\n",
    "- **Medical Diagnosis:** Walking through differential diagnosis\n",
    "- **Financial Analysis:** Step-by-step valuation reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: Chain-of-Thought Prompting\n",
    "\n",
    "> **Imagine you're taking a math test...**\n",
    ">\n",
    "> Your teacher says: \"Just write the final answer.\"\n",
    "> You might rush, make mistakes, and get it wrong.\n",
    ">\n",
    "> But then your teacher says: \"Show your work! Write each step.\"\n",
    "> Now you slow down, think through each part, and catch errors.\n",
    ">\n",
    "> **That's exactly what we're doing with AI!**\n",
    ">\n",
    "> When we say \"Let's think step by step,\" we're asking the AI to\n",
    "> \"show its work\" - and just like students, AI performs much better\n",
    "> when it reasons through problems explicitly.\n",
    ">\n",
    "> **Why does this work?**\n",
    "> - Each step constrains the next (harder to make wild jumps)\n",
    "> - The model \"sees\" its own reasoning (can catch errors)\n",
    "> - Complex problems become manageable chunks\n",
    "\n",
    "```\n",
    "Without CoT:  Problem ──────────────────> Answer  (one big leap, easy to fall!)\n",
    "\n",
    "With CoT:     Problem ─> Step1 ─> Step2 ─> Step3 ─> Answer  (small safe steps)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup and Environment Check\n",
    "\n",
    "Let's verify our environment and set up the tools we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages (if not already installed)\n# !pip install ollama\n\nimport json\nimport time\nimport re\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Tuple\n\n# Check if ollama is available\ntry:\n    import ollama\n    print(\"Ollama library loaded successfully!\")\nexcept ImportError:\n    print(\"Please install ollama: pip install ollama\")\n    raise\n\n# Check available models\ntry:\n    models = ollama.list()\n    print(f\"\\nAvailable models:\")\n    for model in models.get('models', []):\n        print(f\"  - {model.get('name', 'unknown')}\")\nexcept Exception as e:\n    print(f\"Note: Could not list models. Make sure 'ollama serve' is running.\")\n    print(f\"Error: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL = \"qwen3:8b\"  # Change this to your available model\n",
    "\n",
    "# For DGX Spark, you can use larger models:\n",
    "# MODEL = \"qwen3:32b\"  # ~45GB with Q4 quantization\n",
    "# MODEL = \"qwen2.5:72b\"   # ~45GB with Q4 quantization\n",
    "\n",
    "print(f\"Using model: {MODEL}\")\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    response = ollama.chat(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say 'ready' if you can hear me.\"}],\n",
    "        options={\"num_predict\": 10}\n",
    "    )\n",
    "    print(f\"Model response: {response['message']['content']}\")\n",
    "    print(\"Connection successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to model: {e}\")\n",
    "    print(\"Make sure Ollama is running and the model is downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The Problem with Direct Answering\n",
    "\n",
    "Let's first see what happens when we ask the model to answer directly, without any reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A math problem that requires multiple steps\n",
    "problem = \"\"\"\n",
    "A store sells apples for $2 each and oranges for $3 each.\n",
    "If I buy 5 apples and some oranges and spend $25 total, \n",
    "how many oranges did I buy?\n",
    "\"\"\"\n",
    "\n",
    "# Direct answer prompt (no reasoning requested)\n",
    "direct_prompt = f\"{problem}\\n\\nAnswer with just the number:\"\n",
    "\n",
    "print(\"Problem:\")\n",
    "print(problem)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Asking for direct answer...\\n\")\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": direct_prompt}],\n",
    "    options={\"temperature\": 0.0, \"num_predict\": 50}\n",
    ")\n",
    "\n",
    "print(f\"Model's answer: {response['message']['content']}\")\n",
    "print(f\"\\nCorrect answer: 5 oranges\")\n",
    "print(\"(5 apples * $2 = $10, leaving $15 for oranges. $15 / $3 = 5 oranges)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "The model may or may not get this right. Even if it does, we don't know *how* it arrived at the answer. When models try to jump directly to answers:\n",
    "\n",
    "- They're more likely to make arithmetic errors\n",
    "- They might miss steps in the reasoning\n",
    "- We can't verify their thinking\n",
    "\n",
    "Let's try with Chain-of-Thought prompting next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Zero-Shot Chain-of-Thought\n",
    "\n",
    "Zero-shot CoT is beautifully simple: just add **\"Let's think step by step\"** to your prompt. No examples needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_cot(question: str, model: str = MODEL) -> str:\n",
    "    \"\"\"\n",
    "    Apply zero-shot Chain-of-Thought prompting.\n",
    "    \n",
    "    Just adds \"Let's think step by step\" - that's it!\n",
    "    \"\"\"\n",
    "    prompt = f\"{question}\\n\\nLet's think step by step:\"\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        options={\"temperature\": 0.0, \"num_predict\": 512}\n",
    "    )\n",
    "    \n",
    "    return response['message']['content']\n",
    "\n",
    "\n",
    "# Test on our problem\n",
    "print(\"Problem:\")\n",
    "print(problem)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Using Chain-of-Thought...\\n\")\n",
    "\n",
    "cot_response = zero_shot_cot(problem)\n",
    "print(cot_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "Notice how the model:\n",
    "1. Broke down the problem into manageable pieces\n",
    "2. Showed its arithmetic at each step\n",
    "3. Built up to the final answer logically\n",
    "\n",
    "This is the magic of CoT - the model *discovers* the right approach by reasoning through it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Let's Test on Multiple Problems\n",
    "\n",
    "Now let's systematically compare direct answering vs. CoT on a set of problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "<cell_type>markdown</cell_type>---\n\n## Key Python Tools for This Lab\n\nBefore we evaluate our methods, let's understand some key Python tools we'll use:\n\n### Regular Expressions (`re` module)\nRegular expressions help us find patterns in text - essential for extracting answers from model responses.\n\n```python\nimport re\n\n# re.findall(pattern, text) - finds ALL matches of a pattern\nnumbers = re.findall(r'\\d+', \"I have 5 apples and 3 oranges\")\n# Returns: ['5', '3']\n\n# Common patterns:\n# \\d+     - one or more digits\n# \\$?     - optional dollar sign\n# [\\d,]+  - digits with optional commas (like \"1,000\")\n```\n\n### Path and JSON for Data Loading\n```python\nfrom pathlib import Path\nimport json\n\n# Path provides clean file path handling\ndata_path = Path(\"../data/file.json\")\nif data_path.exists():\n    with open(data_path) as f:\n        data = json.load(f)  # Parse JSON file into Python dict/list\n```\n\n### Time Measurement\n```python\nimport time\n\nstart = time.time()\n# ... do something ...\nelapsed = time.time() - start  # Time in seconds\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(response: str) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Extract the numerical answer from a model response.\n",
    "    \n",
    "    Looks for patterns like:\n",
    "    - \"The answer is 42\"\n",
    "    - \"= 42\"\n",
    "    - \"$42\"\n",
    "    \"\"\"\n",
    "    # Try various patterns\n",
    "    patterns = [\n",
    "        r\"[Tt]he (?:final )?answer is[:\\s]+\\$?([\\d,]+(?:\\.\\d+)?)\",\n",
    "        r\"[Aa]nswer[:\\s]+\\$?([\\d,]+(?:\\.\\d+)?)\",\n",
    "        r\"=\\s*\\$?([\\d,]+(?:\\.\\d+)?)\\s*(?:$|\\.|\\n)\",\n",
    "        r\"\\$\\s*([\\d,]+(?:\\.\\d+)?)\",\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, response)\n",
    "        if matches:\n",
    "            # Take the last match (usually the final answer)\n",
    "            num_str = matches[-1].replace(',', '')\n",
    "            try:\n",
    "                return float(num_str)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    # Fallback: find last number in response\n",
    "    all_numbers = re.findall(r'-?[\\d,]+(?:\\.\\d+)?', response)\n",
    "    if all_numbers:\n",
    "        num_str = all_numbers[-1].replace(',', '')\n",
    "        try:\n",
    "            return float(num_str)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def is_correct(predicted: Optional[float], expected: float, tolerance: float = 0.01) -> bool:\n",
    "    \"\"\"Check if predicted answer matches expected (with tolerance).\"\"\"\n",
    "    if predicted is None:\n",
    "        return False\n",
    "    if expected == 0:\n",
    "        return abs(predicted) < tolerance\n",
    "    return abs(predicted - expected) / abs(expected) < tolerance\n",
    "\n",
    "\n",
    "# Test our extraction\n",
    "test_responses = [\n",
    "    \"Let's calculate... 5 + 3 = 8. The answer is 8.\",\n",
    "    \"After all the calculations, we get $42.\",\n",
    "    \"So the total would be 156 meters.\",\n",
    "]\n",
    "\n",
    "print(\"Testing answer extraction:\")\n",
    "for resp in test_responses:\n",
    "    answer = extract_answer(resp)\n",
    "    print(f\"  '{resp[:40]}...' -> {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_method(\n",
    "    problems: List[Dict],\n",
    "    method: str = \"cot\",\n",
    "    n_problems: int = 5,\n",
    "    verbose: bool = True\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate a prompting method on a set of problems.\n",
    "    \n",
    "    Args:\n",
    "        problems: List of problem dicts with 'question' and 'numerical_answer'\n",
    "        method: \"direct\" or \"cot\"\n",
    "        n_problems: Number of problems to evaluate\n",
    "        verbose: Whether to print progress\n",
    "    \n",
    "    Returns:\n",
    "        Dict with accuracy and results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, problem in enumerate(problems[:n_problems]):\n",
    "        question = problem['question']\n",
    "        expected = problem['numerical_answer']\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nProblem {i+1}/{n_problems}: {question[:50]}...\")\n",
    "        \n",
    "        # Build prompt based on method\n",
    "        if method == \"direct\":\n",
    "            prompt = f\"{question}\\n\\nGive only the numerical answer:\"\n",
    "        else:  # cot\n",
    "            prompt = f\"{question}\\n\\nLet's think step by step:\"\n",
    "        \n",
    "        # Get response\n",
    "        start_time = time.time()\n",
    "        response = ollama.chat(\n",
    "            model=MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            options={\"temperature\": 0.0, \"num_predict\": 512}\n",
    "        )\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        response_text = response['message']['content']\n",
    "        predicted = extract_answer(response_text)\n",
    "        correct = is_correct(predicted, expected)\n",
    "        \n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'expected': expected,\n",
    "            'predicted': predicted,\n",
    "            'correct': correct,\n",
    "            'response': response_text,\n",
    "            'time': elapsed,\n",
    "        })\n",
    "        \n",
    "        if verbose:\n",
    "            status = \"CORRECT\" if correct else \"WRONG\"\n",
    "            print(f\"  Expected: {expected}, Predicted: {predicted} [{status}]\")\n",
    "    \n",
    "    accuracy = sum(1 for r in results if r['correct']) / len(results)\n",
    "    avg_time = sum(r['time'] for r in results) / len(results)\n",
    "    \n",
    "    return {\n",
    "        'method': method,\n",
    "        'accuracy': accuracy,\n",
    "        'correct': sum(1 for r in results if r['correct']),\n",
    "        'total': len(results),\n",
    "        'avg_time': avg_time,\n",
    "        'results': results,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare methods!\n",
    "n_test = min(5, len(problems))  # Use 5 problems for quick testing\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUATING DIRECT ANSWERING\")\n",
    "print(\"=\"*60)\n",
    "direct_results = evaluate_method(problems, method=\"direct\", n_problems=n_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING CHAIN-OF-THOUGHT\")\n",
    "print(\"=\"*60)\n",
    "cot_results = evaluate_method(problems, method=\"cot\", n_problems=n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Method':<20} {'Accuracy':<15} {'Correct/Total':<15} {'Avg Time':<10}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Direct Answering':<20} {direct_results['accuracy']:<15.1%} {direct_results['correct']}/{direct_results['total']:<12} {direct_results['avg_time']:<10.1f}s\")\n",
    "print(f\"{'Chain-of-Thought':<20} {cot_results['accuracy']:<15.1%} {cot_results['correct']}/{cot_results['total']:<12} {cot_results['avg_time']:<10.1f}s\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "improvement = cot_results['accuracy'] - direct_results['accuracy']\n",
    "print(f\"\\nAccuracy improvement with CoT: {improvement:+.1%}\")\n",
    "\n",
    "if improvement > 0:\n",
    "    print(\"Chain-of-Thought prompting improved accuracy!\")\n",
    "elif improvement < 0:\n",
    "    print(\"Interesting - direct answering performed better on this sample.\")\n",
    "    print(\"(This can happen with small samples or simple problems)\")\n",
    "else:\n",
    "    print(\"Both methods performed equally on this sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Few-Shot Chain-of-Thought\n",
    "\n",
    "We can make CoT even more powerful by showing the model *examples* of good reasoning. This is called **few-shot CoT**.\n",
    "\n",
    "> **ELI5:** It's like showing a student a worked example before asking them to solve a new problem. They learn the \"style\" of reasoning from the example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create high-quality few-shot examples\n",
    "FEW_SHOT_EXAMPLES = [\n",
    "    {\n",
    "        \"question\": \"If there are 3 cars in the parking lot and 2 more arrive, how many cars are there?\",\n",
    "        \"reasoning\": \"\"\"Let me solve this step by step:\n",
    "1. Start with 3 cars in the parking lot\n",
    "2. 2 more cars arrive\n",
    "3. Total cars = 3 + 2 = 5 cars\n",
    "\n",
    "The answer is 5.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Tom has 8 apples. He gives 3 to his friend and buys 5 more. How many apples does Tom have now?\",\n",
    "        \"reasoning\": \"\"\"Let me solve this step by step:\n",
    "1. Tom starts with 8 apples\n",
    "2. He gives away 3 apples: 8 - 3 = 5 apples\n",
    "3. He buys 5 more apples: 5 + 5 = 10 apples\n",
    "\n",
    "The answer is 10.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"A store sells pens for $2 each. If I have $15, how many pens can I buy and how much money will I have left?\",\n",
    "        \"reasoning\": \"\"\"Let me solve this step by step:\n",
    "1. Each pen costs $2\n",
    "2. I have $15 to spend\n",
    "3. Number of pens I can buy = $15 / $2 = 7 pens (with remainder)\n",
    "4. Cost of 7 pens = 7 * $2 = $14\n",
    "5. Money left = $15 - $14 = $1\n",
    "\n",
    "The answer is 7 pens with $1 left.\"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def few_shot_cot(question: str, examples: List[Dict] = FEW_SHOT_EXAMPLES, model: str = MODEL) -> str:\n",
    "    \"\"\"\n",
    "    Apply few-shot Chain-of-Thought prompting.\n",
    "    \n",
    "    Shows examples of good reasoning before the actual question.\n",
    "    \"\"\"\n",
    "    # Build the prompt with examples\n",
    "    prompt_parts = []\n",
    "    \n",
    "    for ex in examples:\n",
    "        prompt_parts.append(f\"Q: {ex['question']}\")\n",
    "        prompt_parts.append(f\"A: {ex['reasoning']}\")\n",
    "        prompt_parts.append(\"\")  # Empty line between examples\n",
    "    \n",
    "    # Add the actual question\n",
    "    prompt_parts.append(f\"Q: {question}\")\n",
    "    prompt_parts.append(\"A: Let me solve this step by step:\")\n",
    "    \n",
    "    prompt = \"\\n\".join(prompt_parts)\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        options={\"temperature\": 0.0, \"num_predict\": 512}\n",
    "    )\n",
    "    \n",
    "    return response['message']['content']\n",
    "\n",
    "\n",
    "# Test few-shot CoT\n",
    "test_question = \"A baker has 24 cupcakes. She sells 8 in the morning and 10 in the afternoon. How many cupcakes does she have left?\"\n",
    "\n",
    "print(\"Question:\", test_question)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Few-Shot CoT Response:\\n\")\n",
    "print(few_shot_cot(test_question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try It Yourself: Create Your Own Examples\n",
    "\n",
    "The quality of few-shot examples matters! Try creating examples that match your use case.\n",
    "\n",
    "<details>\n",
    "<summary>Hint: Tips for good examples</summary>\n",
    "\n",
    "1. **Be consistent in format** - Use the same step numbering and phrasing\n",
    "2. **Show intermediate calculations** - Don't skip mental math\n",
    "3. **End with clear answer** - \"The answer is X\" is easy to extract\n",
    "4. **Match problem complexity** - Your examples should be similar difficulty\n",
    "5. **Use 2-4 examples** - More isn't always better (context limits!)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your own few-shot examples for a specific domain\n",
    "# For example: percentage calculations, time/distance, or unit conversions\n",
    "\n",
    "MY_EXAMPLES = [\n",
    "    {\n",
    "        \"question\": \"Your question here\",\n",
    "        \"reasoning\": \"\"\"Your step-by-step reasoning here...\n",
    "1. First step\n",
    "2. Second step\n",
    "3. Third step\n",
    "\n",
    "The answer is X.\"\"\"\n",
    "    },\n",
    "    # Add more examples...\n",
    "]\n",
    "\n",
    "# Test your examples\n",
    "# test_response = few_shot_cot(\"Your test question\", examples=MY_EXAMPLES)\n",
    "# print(test_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: When Does CoT Help (and When Doesn't It)?\n",
    "\n",
    "CoT isn't magic - it helps most on certain types of problems. Let's explore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problems where CoT helps most\n",
    "cot_helps_problems = [\n",
    "    # Multi-step arithmetic\n",
    "    \"If a shirt costs $45 and is on sale for 20% off, how much do I save?\",\n",
    "    \n",
    "    # Word problems requiring interpretation\n",
    "    \"A train leaves at 9am traveling at 60mph. Another train leaves at 10am traveling at 80mph. When does the second train catch up?\",\n",
    "    \n",
    "    # Problems with multiple constraints\n",
    "    \"I need to buy at least 100 items. Boxes come in packs of 12. What's the minimum number of boxes I need?\",\n",
    "    \n",
    "    # Sequential reasoning\n",
    "    \"If Monday is 2 days after yesterday, what day is today?\",\n",
    "]\n",
    "\n",
    "# Problems where CoT might not help as much\n",
    "cot_doesnt_help_problems = [\n",
    "    # Simple factual recall\n",
    "    \"What is the capital of France?\",\n",
    "    \n",
    "    # Direct calculations\n",
    "    \"What is 7 + 5?\",\n",
    "    \n",
    "    # Pattern recognition\n",
    "    \"Complete the pattern: 2, 4, 6, 8, _\",\n",
    "    \n",
    "    # Simple lookups\n",
    "    \"How many days are in February in a leap year?\",\n",
    "]\n",
    "\n",
    "\n",
    "def compare_on_problem(question: str) -> Dict:\n",
    "    \"\"\"Compare direct vs CoT on a single problem.\"\"\"\n",
    "    # Direct\n",
    "    direct_response = ollama.chat(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"{question}\\n\\nAnswer:\"}],\n",
    "        options={\"temperature\": 0.0, \"num_predict\": 100}\n",
    "    )['message']['content']\n",
    "    \n",
    "    # CoT\n",
    "    cot_response = zero_shot_cot(question)\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'direct': direct_response,\n",
    "        'cot': cot_response,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Testing problems where CoT typically HELPS:\\n\")\n",
    "for q in cot_helps_problems[:2]:  # Test first 2\n",
    "    result = compare_on_problem(q)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"Direct: {result['direct'][:100]}...\")\n",
    "    print(f\"CoT: {result['cot'][:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight: When to Use CoT\n",
    "\n",
    "| Use CoT When... | Skip CoT When... |\n",
    "|----------------|------------------|\n",
    "| Multi-step reasoning required | Simple factual questions |\n",
    "| Math word problems | Direct calculations |\n",
    "| Logical puzzles | Pattern completion |\n",
    "| Cause-and-effect chains | Yes/no questions |\n",
    "| Constraint satisfaction | Simple lookups |\n",
    "\n",
    "**Rule of thumb:** If a human would need to \"think it through,\" use CoT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Extracting the Final Answer\n",
    "\n",
    "```python\n",
    "# Wrong: Just using the full response\n",
    "response = model.generate(\"What is 5+3? Let's think step by step:\")\n",
    "answer = response  # This includes all the reasoning!\n",
    "\n",
    "# Right: Extract just the answer\n",
    "response = model.generate(\"What is 5+3? Let's think step by step:\")\n",
    "answer = extract_answer(response)  # Just the number: 8\n",
    "```\n",
    "\n",
    "### Mistake 2: Using Temperature Too High\n",
    "\n",
    "```python\n",
    "# Wrong: High temperature for reasoning\n",
    "response = ollama.chat(model, messages, options={\"temperature\": 1.0})\n",
    "# Risk: Model may give creative but wrong reasoning\n",
    "\n",
    "# Right: Low temperature for consistent reasoning\n",
    "response = ollama.chat(model, messages, options={\"temperature\": 0.0})\n",
    "# Deterministic, reproducible reasoning\n",
    "```\n",
    "\n",
    "### Mistake 3: Too Many Few-Shot Examples\n",
    "\n",
    "```python\n",
    "# Wrong: 10 examples (uses up context window)\n",
    "examples = [ex1, ex2, ex3, ex4, ex5, ex6, ex7, ex8, ex9, ex10]\n",
    "\n",
    "# Right: 2-4 high-quality examples\n",
    "examples = [ex1, ex2, ex3]  # Quality over quantity!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ✅ **What CoT is:** Prompting models to \"show their work\"\n",
    "- ✅ **Zero-shot CoT:** Just add \"Let's think step by step\"\n",
    "- ✅ **Few-shot CoT:** Provide examples of good reasoning\n",
    "- ✅ **When to use it:** Multi-step reasoning, math, logic puzzles\n",
    "- ✅ **How to measure:** Extract answers and compare accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge: Build a CoT Prompt Template Library\n",
    "\n",
    "Create a library of CoT prompt templates for different domains. This is what production AI systems actually use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoTPromptLibrary:\n",
    "    \"\"\"\n",
    "    A library of Chain-of-Thought prompt templates for different domains.\n",
    "    \n",
    "    Usage:\n",
    "        library = CoTPromptLibrary()\n",
    "        prompt = library.get_prompt(\"math\", \"What is 15% of 80?\")\n",
    "    \"\"\"\n",
    "    \n",
    "    TEMPLATES = {\n",
    "        \"math\": {\n",
    "            \"system\": \"You are a math tutor. Always show your work step by step.\",\n",
    "            \"trigger\": \"Let me solve this step by step:\",\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"q\": \"What is 25% of 80?\",\n",
    "                    \"a\": \"\"\"Let me solve this step by step:\n",
    "1. 25% means 25 per 100, or 25/100 = 0.25\n",
    "2. To find 25% of 80, multiply: 80 * 0.25\n",
    "3. 80 * 0.25 = 20\n",
    "\n",
    "The answer is 20.\"\"\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"code_debug\": {\n",
    "            \"system\": \"You are a debugging expert. Analyze code systematically.\",\n",
    "            \"trigger\": \"Let me analyze this code step by step:\",\n",
    "            \"examples\": []\n",
    "        },\n",
    "        \"logic\": {\n",
    "            \"system\": \"You are a logic expert. Use formal reasoning.\",\n",
    "            \"trigger\": \"Let me reason through this logically:\",\n",
    "            \"examples\": []\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.templates = self.TEMPLATES.copy()\n",
    "    \n",
    "    def get_prompt(self, domain: str, question: str, use_examples: bool = True) -> str:\n",
    "        \"\"\"Get a CoT prompt for a given domain and question.\"\"\"\n",
    "        if domain not in self.templates:\n",
    "            raise ValueError(f\"Unknown domain: {domain}. Available: {list(self.templates.keys())}\")\n",
    "        \n",
    "        template = self.templates[domain]\n",
    "        \n",
    "        parts = []\n",
    "        \n",
    "        # Add examples if available and requested\n",
    "        if use_examples and template.get('examples'):\n",
    "            for ex in template['examples']:\n",
    "                parts.append(f\"Q: {ex['q']}\")\n",
    "                parts.append(f\"A: {ex['a']}\")\n",
    "                parts.append(\"\")\n",
    "        \n",
    "        # Add the actual question\n",
    "        parts.append(f\"Q: {question}\")\n",
    "        parts.append(f\"A: {template['trigger']}\")\n",
    "        \n",
    "        return \"\\n\".join(parts)\n",
    "    \n",
    "    def add_domain(self, domain: str, system: str, trigger: str, examples: List[Dict] = None):\n",
    "        \"\"\"Add a new domain to the library.\"\"\"\n",
    "        self.templates[domain] = {\n",
    "            \"system\": system,\n",
    "            \"trigger\": trigger,\n",
    "            \"examples\": examples or []\n",
    "        }\n",
    "\n",
    "\n",
    "# Test the library\n",
    "library = CoTPromptLibrary()\n",
    "\n",
    "# Math question\n",
    "math_prompt = library.get_prompt(\"math\", \"What is 15% of 240?\")\n",
    "print(\"Math Prompt:\")\n",
    "print(math_prompt)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Get response\n",
    "response = ollama.chat(\n",
    "    model=MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": math_prompt}],\n",
    "    options={\"temperature\": 0.0}\n",
    ")\n",
    "print(\"Response:\")\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>---\n\n## Further Reading\n\n- [Chain-of-Thought Prompting Paper](https://arxiv.org/abs/2201.11903) - The original Google research\n- [Zero-Shot CoT Paper](https://arxiv.org/abs/2205.11916) - \"Let's think step by step\"\n- [Self-Consistency Paper](https://arxiv.org/abs/2203.11171) - Next lab's topic!\n- [Prompt Engineering Guide](https://www.promptingguide.ai/techniques/cot) - Practical tips\n\n---\n\n## Cleanup\n\nWe use `gc.collect()` (garbage collection) to free up memory that's no longer being used. This is especially helpful when working with large language models."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for later comparison\n",
    "results_summary = {\n",
    "    'model': MODEL,\n",
    "    'direct_accuracy': direct_results['accuracy'],\n",
    "    'cot_accuracy': cot_results['accuracy'],\n",
    "    'improvement': cot_results['accuracy'] - direct_results['accuracy'],\n",
    "    'n_problems': n_test,\n",
    "}\n",
    "\n",
    "print(\"Results saved for comparison with other methods!\")\n",
    "print(json.dumps(results_summary, indent=2))\n",
    "\n",
    "# Clean up GPU memory if using GPU-based model\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"\\nMemory cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Congratulations on completing the Chain-of-Thought Workshop!\n",
    "\n",
    "In the next lab, you'll learn **Self-Consistency** - a powerful technique that generates multiple reasoning paths and votes on the best answer. This often beats CoT alone!\n",
    "\n",
    "**Continue to:** [Lab 3.4.2: Self-Consistency Implementation](./lab-3.4.2-self-consistency.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}