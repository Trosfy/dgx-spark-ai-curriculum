{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.4.3: DeepSeek-R1 Exploration\n",
    "\n",
    "**Module:** 3.4 - Test-Time Compute & Reasoning  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐⭐ (Intermediate-Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- [ ] Understand how DeepSeek-R1 differs from standard LLMs\n",
    "- [ ] Run R1 models on DGX Spark via Ollama\n",
    "- [ ] Parse and analyze `<think>` reasoning tokens\n",
    "- [ ] Observe how R1 approaches different problem types\n",
    "- [ ] Understand the GRPO training approach\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Labs 3.4.1 and 3.4.2\n",
    "- Ollama installed and running\n",
    "- DeepSeek-R1 model downloaded (see setup below)\n",
    "- DGX Spark or system with sufficient memory (~45GB for 70B Q4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "In January 2024, DeepSeek released R1 - an open-source reasoning model that rivals OpenAI's o1. The key innovation: **R1 was trained with reinforcement learning (GRPO) to explicitly reason before answering**, producing visible \"thinking\" tokens.\n",
    "\n",
    "**Why R1 Matters:**\n",
    "- First open-source model to match o1-level reasoning\n",
    "- Distilled versions (1.5B to 70B) run on consumer hardware\n",
    "- The `<think>` tokens let us see *how* the model reasons\n",
    "- Beats GPT-4 on many math and coding benchmarks\n",
    "\n",
    "**Industry Impact:**\n",
    "- Open-source alternative to expensive API calls\n",
    "- Interpretable reasoning for high-stakes applications\n",
    "- Foundation for custom reasoning agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: DeepSeek-R1 and Reasoning Models\n",
    "\n",
    "> **Imagine two students taking a math test...**\n",
    ">\n",
    "> **Student A (Standard LLM):** Reads the problem, immediately writes an answer.\n",
    "> Sometimes right, sometimes wrong. You can't see their thinking.\n",
    ">\n",
    "> **Student B (R1):** Reads the problem, then *writes out their thinking*\n",
    "> on scratch paper before giving the final answer.\n",
    "> \"Hmm, let me break this down... First I need to... Wait, that doesn't work...\n",
    "> Let me try another approach...\"\n",
    ">\n",
    "> **The magic:** Student B was *trained with a reward* for getting the right\n",
    "> answer, so they learned that careful thinking leads to better grades!\n",
    ">\n",
    "> **In AI terms:**\n",
    "> - `<think>` tokens = scratch paper (visible reasoning)\n",
    "> - GRPO training = rewarding correct final answers\n",
    "> - The model learned: more thinking = better answers\n",
    "\n",
    "```\n",
    "Standard LLM:    Question ──────────────────> Answer\n",
    "                                               \n",
    "DeepSeek-R1:     Question ─> <think>          </think> ─> Answer\n",
    "                             │                │\n",
    "                             │ \"Let me see...\"│\n",
    "                             │ \"This means...\"│\n",
    "                             │ \"So therefore..\"│\n",
    "                             └────────────────┘\n",
    "                             (Visible reasoning!)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup and Model Download\n",
    "\n",
    "First, let's ensure we have DeepSeek-R1 available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport time\nimport re\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Tuple\nfrom dataclasses import dataclass\n\nimport ollama\n\nprint(\"Checking Ollama models...\")\n\ntry:\n    models = ollama.list()\n    model_names = [m.get('name', 'unknown') for m in models.get('models', [])]\n    print(f\"\\nAvailable models:\")\n    for name in model_names:\n        marker = \" <-- R1\" if 'r1' in name.lower() else \"\"\n        print(f\"  - {name}{marker}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n    print(\"Make sure 'ollama serve' is running.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection based on available memory\n",
    "# DGX Spark (128GB) can run the 70B distilled version easily!\n",
    "\n",
    "# Check if R1 is available, otherwise provide instructions\n",
    "R1_MODELS = {\n",
    "    'small': 'deepseek-r1:7b',      # ~7GB, fast for testing\n",
    "    'medium': 'deepseek-r1:32b',    # ~20GB, good balance\n",
    "    'large': 'deepseek-r1:70b',     # ~45GB, best quality\n",
    "}\n",
    "\n",
    "# Detect which R1 model is available\n",
    "available_r1 = None\n",
    "for size, name in R1_MODELS.items():\n",
    "    if any(name in m for m in model_names):\n",
    "        available_r1 = name\n",
    "        print(f\"Found R1 model: {name} ({size})\")\n",
    "        break\n",
    "\n",
    "if not available_r1:\n",
    "    print(\"\\nNo DeepSeek-R1 model found!\")\n",
    "    print(\"\\nTo download, run one of these commands in your terminal:\")\n",
    "    print(\"\\n  # For testing (7B, ~7GB):\")\n",
    "    print(\"  ollama pull deepseek-r1:7b\")\n",
    "    print(\"\\n  # For DGX Spark (70B, ~45GB) - RECOMMENDED:\")\n",
    "    print(\"  ollama pull deepseek-r1:70b\")\n",
    "    print(\"\\nThen re-run this cell.\")\n",
    "else:\n",
    "    MODEL = available_r1\n",
    "    print(f\"\\nUsing model: {MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If no R1 available, you can simulate with a standard model for learning\n",
    "# Set MODEL manually if needed\n",
    "\n",
    "if 'MODEL' not in dir() or MODEL is None:\n",
    "    # Fallback: use any available model for demonstration\n",
    "    MODEL = model_names[0] if model_names else \"qwen3:8b\"\n",
    "    print(f\"Using fallback model: {MODEL}\")\n",
    "    print(\"Note: This won't show <think> tokens, but code will still run.\")\n",
    "\n",
    "# Test the model\n",
    "print(f\"\\nTesting {MODEL}...\")\n",
    "response = ollama.chat(\n",
    "    model=MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is 2+2? Be brief.\"}],\n",
    "    options={\"num_predict\": 50}\n",
    ")\n",
    "print(f\"Response: {response['message']['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>---\n\n## Part 2: Understanding R1's Thinking Process\n\nR1 wraps its reasoning in `<think>...</think>` tags. Let's see it in action!\n\n### Key Python Tools: Dataclasses and Advanced Regex\n\n**Dataclasses** provide a clean way to create data containers:\n\n```python\nfrom dataclasses import dataclass\n\n@dataclass\nclass Person:\n    name: str\n    age: int\n\n# Creates a class with __init__, __repr__, etc. automatically\nperson = Person(name=\"Alice\", age=30)\nprint(person)  # Person(name='Alice', age=30)\n```\n\n**Advanced Regex** for parsing multi-line text:\n\n```python\nimport re\n\n# re.DOTALL flag: makes '.' match newlines too\ntext = \"<think>\\nLine 1\\nLine 2\\n</think>\"\nmatch = re.findall(r'<think>(.*?)</think>', text, re.DOTALL)\n# Without DOTALL: [] (no match because . doesn't match \\n)\n# With DOTALL: ['\\nLine 1\\nLine 2\\n']\n\n# re.sub() replaces patterns (like find-and-replace)\ncleaned = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n# Result: '' (thinking block removed)\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ThinkingResult:\n",
    "    \"\"\"Parsed result from R1-style response.\"\"\"\n",
    "    thinking: str       # Content within <think>...</think>\n",
    "    answer: str         # Final answer after thinking\n",
    "    thinking_tokens: int  # Rough token count for thinking\n",
    "    answer_tokens: int    # Rough token count for answer\n",
    "    raw_response: str     # Full original response\n",
    "\n",
    "\n",
    "def parse_r1_response(response: str) -> ThinkingResult:\n",
    "    \"\"\"\n",
    "    Parse DeepSeek-R1 style thinking tokens from a response.\n",
    "    \n",
    "    R1 wraps its reasoning in <think>...</think> tags.\n",
    "    \"\"\"\n",
    "    # Pattern for <think>...</think> blocks\n",
    "    think_pattern = r'<think>(.*?)</think>'\n",
    "    \n",
    "    # Find all thinking blocks\n",
    "    thinking_matches = re.findall(think_pattern, response, re.DOTALL)\n",
    "    thinking_content = \"\\n\".join(thinking_matches)\n",
    "    \n",
    "    # Remove thinking blocks from response to get the answer\n",
    "    answer_content = re.sub(think_pattern, '', response, flags=re.DOTALL)\n",
    "    answer_content = answer_content.strip()\n",
    "    \n",
    "    # Rough token count (1 token ~ 4 characters on average)\n",
    "    thinking_tokens = len(thinking_content) // 4\n",
    "    answer_tokens = len(answer_content) // 4\n",
    "    \n",
    "    return ThinkingResult(\n",
    "        thinking=thinking_content.strip(),\n",
    "        answer=answer_content,\n",
    "        thinking_tokens=thinking_tokens,\n",
    "        answer_tokens=answer_tokens,\n",
    "        raw_response=response,\n",
    "    )\n",
    "\n",
    "\n",
    "# Test with a sample R1-style response\n",
    "sample_response = '''<think>\n",
    "Let me work through this step by step.\n",
    "\n",
    "The problem asks for 17 * 23.\n",
    "\n",
    "I can break this down:\n",
    "17 * 23 = 17 * (20 + 3)\n",
    "       = 17 * 20 + 17 * 3\n",
    "       = 340 + 51\n",
    "       = 391\n",
    "\n",
    "Let me verify: 391 / 17 = 23. Yes, that's correct.\n",
    "</think>\n",
    "\n",
    "The answer is **391**.'''\n",
    "\n",
    "parsed = parse_r1_response(sample_response)\n",
    "\n",
    "print(\"Parsed R1 Response:\")\n",
    "print(f\"\\nThinking ({parsed.thinking_tokens} tokens):\")\n",
    "print(f\"  {parsed.thinking[:200]}...\")\n",
    "print(f\"\\nFinal Answer ({parsed.answer_tokens} tokens):\")\n",
    "print(f\"  {parsed.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: R1 on Math Problems\n",
    "\n",
    "Let's see how R1 reasons through math problems. Watch the `<think>` tokens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_r1(\n",
    "    question: str,\n",
    "    model: str = MODEL,\n",
    "    max_tokens: int = 2048,\n",
    "    temperature: float = 0.0,\n",
    "    show_thinking: bool = True,\n",
    ") -> ThinkingResult:\n",
    "    \"\"\"\n",
    "    Query R1 and parse the response.\n",
    "    \"\"\"\n",
    "    print(f\"Querying {model}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": question}],\n",
    "        options={\"temperature\": temperature, \"num_predict\": max_tokens}\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    response_text = response['message']['content']\n",
    "    \n",
    "    result = parse_r1_response(response_text)\n",
    "    \n",
    "    print(f\"Response time: {elapsed:.1f}s\")\n",
    "    print(f\"Thinking tokens: ~{result.thinking_tokens}\")\n",
    "    print(f\"Answer tokens: ~{result.answer_tokens}\")\n",
    "    \n",
    "    if show_thinking and result.thinking:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"THINKING PROCESS:\")\n",
    "        print(\"=\"*50)\n",
    "        print(result.thinking)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL ANSWER:\")\n",
    "    print(\"=\"*50)\n",
    "    print(result.answer)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a multi-step math problem\n",
    "math_problem = \"\"\"\n",
    "A store sells apples for $2 each and oranges for $3 each.\n",
    "If I buy 5 apples and some oranges and spend $25 total,\n",
    "how many oranges did I buy?\n",
    "\"\"\"\n",
    "\n",
    "result = query_r1(math_problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A harder problem - watch R1 reason!\n",
    "hard_math = \"\"\"\n",
    "A train leaves Station A at 9:00 AM traveling at 60 mph toward Station B.\n",
    "Another train leaves Station B at 10:00 AM traveling at 80 mph toward Station A.\n",
    "The stations are 280 miles apart.\n",
    "At what time do the trains meet?\n",
    "\"\"\"\n",
    "\n",
    "result = query_r1(hard_math)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing R1's Thinking Patterns\n",
    "\n",
    "Notice how R1 typically:\n",
    "1. **Restates the problem** - Ensures understanding\n",
    "2. **Identifies unknowns** - \"Let x = ...\"\n",
    "3. **Sets up equations** - Translates words to math\n",
    "4. **Shows calculations** - Step by step arithmetic\n",
    "5. **Verifies the answer** - Checks if it makes sense\n",
    "\n",
    "This is much more reliable than models that jump straight to answers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: R1 on Coding Problems\n",
    "\n",
    "R1 also excels at coding. Let's see how it reasons about code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coding_problem = \"\"\"\n",
    "Write a Python function that finds the longest palindromic substring \n",
    "in a given string. For example, for \"babad\", the answer could be \"bab\" or \"aba\".\n",
    "\n",
    "Explain your approach before coding.\n",
    "\"\"\"\n",
    "\n",
    "result = query_r1(coding_problem, max_tokens=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug a piece of code\n",
    "debug_problem = \"\"\"\n",
    "The following Python code is supposed to find the two numbers in a list\n",
    "that add up to a target. But it has bugs. Find and fix them:\n",
    "\n",
    "```python\n",
    "def two_sum(nums, target):\n",
    "    for i in range(len(nums)):\n",
    "        for j in range(len(nums)):\n",
    "            if nums[i] + nums[j] = target:\n",
    "                return [i, j]\n",
    "    return None\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "result = query_r1(debug_problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: R1 on Logic and Reasoning\n",
    "\n",
    "Where R1 really shines: complex logical reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic_problem = \"\"\"\n",
    "Five friends (Alice, Bob, Carol, David, Eve) are sitting in a row at a movie theater.\n",
    "- Alice doesn't sit next to Bob.\n",
    "- Carol sits exactly in the middle.\n",
    "- David sits to the right of Alice.\n",
    "- Eve sits at one of the ends.\n",
    "\n",
    "What is one valid seating arrangement from left to right?\n",
    "\"\"\"\n",
    "\n",
    "result = query_r1(logic_problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tricky reasoning problem\n",
    "trick_problem = \"\"\"\n",
    "A bat and a ball cost $1.10 in total.\n",
    "The bat costs $1.00 more than the ball.\n",
    "How much does the ball cost?\n",
    "\n",
    "Think carefully before answering.\n",
    "\"\"\"\n",
    "\n",
    "result = query_r1(trick_problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Classic Trick Question\n",
    "\n",
    "The bat and ball problem is famous for tripping up humans (and AIs)!\n",
    "\n",
    "- **Wrong intuition:** Ball = $0.10 (because $1.10 - $1.00 = $0.10)\n",
    "- **Correct answer:** Ball = $0.05\n",
    "  - If ball = $0.05, bat = $1.05 ($1.00 more)\n",
    "  - Total: $0.05 + $1.05 = $1.10 ✓\n",
    "\n",
    "Watch how R1 catches the mistake in its `<think>` process!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Analyzing Thinking Overhead\n",
    "\n",
    "R1 uses more tokens due to thinking. Is it worth it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_thinking_overhead(results: List[ThinkingResult]) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze the thinking token overhead across multiple responses.\n",
    "    \"\"\"\n",
    "    thinking_tokens = [r.thinking_tokens for r in results]\n",
    "    answer_tokens = [r.answer_tokens for r in results]\n",
    "    \n",
    "    total_thinking = sum(thinking_tokens)\n",
    "    total_answer = sum(answer_tokens)\n",
    "    total = total_thinking + total_answer\n",
    "    \n",
    "    return {\n",
    "        'total_thinking_tokens': total_thinking,\n",
    "        'total_answer_tokens': total_answer,\n",
    "        'total_tokens': total,\n",
    "        'thinking_percentage': total_thinking / total * 100 if total > 0 else 0,\n",
    "        'overhead_ratio': total_thinking / total_answer if total_answer > 0 else 0,\n",
    "        'avg_thinking_per_response': total_thinking / len(results) if results else 0,\n",
    "        'avg_answer_per_response': total_answer / len(results) if results else 0,\n",
    "    }\n",
    "\n",
    "\n",
    "# Test on multiple problems\n",
    "test_problems = [\n",
    "    \"What is 17 * 23?\",\n",
    "    \"If I have 5 apples and eat 2, how many are left?\",\n",
    "    \"A rectangle has length 12 and width 7. What is its area?\",\n",
    "]\n",
    "\n",
    "results = []\n",
    "print(\"Running test problems...\\n\")\n",
    "for i, prob in enumerate(test_problems):\n",
    "    print(f\"Problem {i+1}: {prob[:40]}...\")\n",
    "    result = query_r1(prob, show_thinking=False)\n",
    "    results.append(result)\n",
    "    print()\n",
    "\n",
    "# Analyze overhead\n",
    "overhead = analyze_thinking_overhead(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"THINKING OVERHEAD ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total thinking tokens: {overhead['total_thinking_tokens']}\")\n",
    "print(f\"Total answer tokens: {overhead['total_answer_tokens']}\")\n",
    "print(f\"Thinking percentage: {overhead['thinking_percentage']:.1f}%\")\n",
    "print(f\"Overhead ratio: {overhead['overhead_ratio']:.1f}x\")\n",
    "print(f\"\\nAvg thinking per response: {overhead['avg_thinking_per_response']:.0f} tokens\")\n",
    "print(f\"Avg answer per response: {overhead['avg_answer_per_response']:.0f} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Understanding GRPO Training\n",
    "\n",
    "What makes R1 special? The **GRPO (Group Relative Policy Optimization)** training approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How R1 Was Trained\n",
    "\n",
    "```\n",
    "Traditional LLM Training:\n",
    "  \"Learn to predict the next token from human text\"\n",
    "  → Good at mimicking, not always at reasoning\n",
    "\n",
    "R1's GRPO Training:\n",
    "  1. Start with a base model (DeepSeek-V3)\n",
    "  2. Give it reasoning problems\n",
    "  3. Let it generate multiple solutions with thinking\n",
    "  4. Score solutions by correctness of final answer\n",
    "  5. Reward the model for solutions that led to correct answers\n",
    "  6. The model learns: \"careful thinking → correct answers → reward\"\n",
    "```\n",
    "\n",
    "**Key Insight:** R1 discovered that \"showing its work\" leads to better outcomes!\n",
    "\n",
    "### Distilled Models\n",
    "\n",
    "DeepSeek also released \"distilled\" versions:\n",
    "- R1-distill-Qwen-1.5B, 7B, 14B, 32B\n",
    "- R1-distill-Llama-8B, 70B\n",
    "\n",
    "These are smaller models trained to mimic R1's reasoning patterns.\n",
    "\n",
    "| Model | Size | Memory (Q4) | Reasoning Quality |\n",
    "|-------|------|-------------|-------------------|\n",
    "| R1-distill-1.5B | 1.5B | ~1GB | Basic |\n",
    "| R1-distill-7B | 7B | ~5GB | Good |\n",
    "| R1-distill-32B | 32B | ~20GB | Very Good |\n",
    "| R1-distill-70B | 70B | ~45GB | Excellent |\n",
    "| R1 (full) | 671B | ~350GB | Best |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: DGX Spark Optimization\n",
    "\n",
    "On DGX Spark's 128GB unified memory, you can run the 70B distilled version comfortably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current memory usage (if CUDA available)\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        mem_used = torch.cuda.memory_allocated() / 1e9\n",
    "        mem_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"GPU Memory: {mem_used:.1f}GB / {mem_total:.1f}GB\")\n",
    "        print(f\"\\nDGX Spark Tip: With 128GB unified memory,\")\n",
    "        print(f\"you can run R1-70B (~45GB) + a reward model (~16GB)\")\n",
    "        print(f\"simultaneously for Best-of-N sampling!\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch not available - can't check GPU memory\")\n",
    "    print(\"\\nDGX Spark has 128GB unified CPU+GPU memory.\")\n",
    "    print(\"R1-70B uses ~45GB, leaving plenty of room.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison: Different R1 sizes on DGX Spark\n",
    "dgx_spark_performance = \"\"\"\n",
    "Expected Performance on DGX Spark (Blackwell GB10):\n",
    "\n",
    "| Model           | Memory  | Speed (tok/s) | Quality    |\n",
    "|-----------------|---------|---------------|------------|\n",
    "| R1-distill-7B   | ~7GB    | ~50-60        | Good       |\n",
    "| R1-distill-32B  | ~20GB   | ~30-40        | Very Good  |\n",
    "| R1-distill-70B  | ~45GB   | ~15-25        | Excellent  |\n",
    "\n",
    "Tips:\n",
    "- Use Q4_K_M quantization for best speed/quality balance\n",
    "- Enable mmap for faster model loading\n",
    "- Clear cache between large model switches:\n",
    "  sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'\n",
    "\"\"\"\n",
    "print(dgx_spark_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Ignoring the Thinking Tokens\n",
    "\n",
    "```python\n",
    "# Wrong: Just taking the full response as the answer\n",
    "answer = response['message']['content']  # Includes <think> blocks!\n",
    "\n",
    "# Right: Parse out the thinking and extract the answer\n",
    "result = parse_r1_response(response['message']['content'])\n",
    "answer = result.answer  # Clean answer only\n",
    "```\n",
    "\n",
    "### Mistake 2: Setting Max Tokens Too Low\n",
    "\n",
    "```python\n",
    "# Wrong: R1 needs room to think!\n",
    "response = ollama.chat(model=\"deepseek-r1:70b\", messages=...,\n",
    "                       options={\"num_predict\": 256})  # Thinking gets cut off!\n",
    "\n",
    "# Right: Allow enough tokens for thinking + answer\n",
    "response = ollama.chat(model=\"deepseek-r1:70b\", messages=...,\n",
    "                       options={\"num_predict\": 2048})  # Room to reason\n",
    "```\n",
    "\n",
    "### Mistake 3: Using High Temperature for Reasoning\n",
    "\n",
    "```python\n",
    "# Wrong: High temperature makes reasoning inconsistent\n",
    "response = ollama.chat(..., options={\"temperature\": 1.0})\n",
    "\n",
    "# Right: Use low temperature for reliable reasoning\n",
    "response = ollama.chat(..., options={\"temperature\": 0.0})  # Deterministic\n",
    "# Or slight temperature for minor variation\n",
    "response = ollama.chat(..., options={\"temperature\": 0.3})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ✅ **What R1 is:** A reasoning model trained with GRPO to \"think out loud\"\n",
    "- ✅ **How to parse responses:** Extract `<think>` tokens and final answers\n",
    "- ✅ **R1's strengths:** Math, coding, logic puzzles\n",
    "- ✅ **The overhead tradeoff:** More tokens but more accurate\n",
    "- ✅ **DGX Spark optimization:** Can run 70B comfortably"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge: Build a \"Thinking Visualizer\"\n",
    "\n",
    "Create a tool that nicely formats R1's thinking process for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_thinking(result: ThinkingResult, max_chars: int = 500) -> str:\n",
    "    \"\"\"\n",
    "    Create a nice visualization of R1's thinking process.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    output.append(\"\\n\" + \"#\" * 60)\n",
    "    output.append(\"#  R1 THINKING PROCESS\")\n",
    "    output.append(\"#\" * 60)\n",
    "    \n",
    "    if result.thinking:\n",
    "        output.append(\"\\n[THINKING] (~{} tokens)\".format(result.thinking_tokens))\n",
    "        output.append(\"-\" * 40)\n",
    "        \n",
    "        # Split thinking into \"steps\"\n",
    "        lines = result.thinking.split('\\n')\n",
    "        for line in lines[:20]:  # Limit lines shown\n",
    "            if line.strip():\n",
    "                if any(line.strip().startswith(x) for x in ['1.', '2.', '3.', 'Step', 'First', 'Then', 'Finally']):\n",
    "                    output.append(f\"   {line.strip()}\")\n",
    "                else:\n",
    "                    output.append(f\"   {line.strip()[:80]}\")\n",
    "        \n",
    "        if len(lines) > 20:\n",
    "            output.append(f\"   ... ({len(lines) - 20} more lines)\")\n",
    "    else:\n",
    "        output.append(\"\\n[NO THINKING TOKENS]\")\n",
    "        output.append(\"(Model may not be R1, or problem was simple)\")\n",
    "    \n",
    "    output.append(\"\\n[FINAL ANSWER] (~{} tokens)\".format(result.answer_tokens))\n",
    "    output.append(\"-\" * 40)\n",
    "    answer_preview = result.answer[:max_chars]\n",
    "    if len(result.answer) > max_chars:\n",
    "        answer_preview += \"...\"\n",
    "    output.append(answer_preview)\n",
    "    \n",
    "    output.append(\"\\n\" + \"#\" * 60)\n",
    "    overhead = result.thinking_tokens / max(result.answer_tokens, 1)\n",
    "    output.append(f\"Thinking overhead: {overhead:.1f}x answer length\")\n",
    "    output.append(\"#\" * 60)\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "\n",
    "# Test visualization\n",
    "if results:  # Use results from earlier\n",
    "    print(visualize_thinking(results[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [DeepSeek-R1 Paper](https://arxiv.org/abs/2401.02954) - The research paper\n",
    "- [DeepSeek-R1 GitHub](https://github.com/deepseek-ai/DeepSeek-R1) - Code and models\n",
    "- [GRPO Explained](https://arxiv.org/abs/2402.03300) - Training methodology\n",
    "- [R1 vs o1 Comparison](https://www.deepseek.com/research) - Benchmark results\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Summary\nprint(\"Lab 3.4.3 Complete!\")\nprint(f\"\\nModel used: {MODEL}\")\nprint(f\"Problems tested: {len(results) if 'results' in dir() else 0}\")\n\n# Cleanup memory\nimport gc\n\n# Clear GPU memory if torch was used\ntry:\n    import torch\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\nexcept ImportError:\n    pass\n\ngc.collect()\nprint(\"\\nMemory cleaned up.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you understand how R1 reasons, let's put it to the test!\n",
    "\n",
    "In the next lab, you'll **compare R1 vs. standard models** quantitatively to see exactly how much the thinking helps.\n",
    "\n",
    "**Continue to:** [Lab 3.4.4: R1 vs Standard Model Comparison](./lab-3.4.4-r1-comparison.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}