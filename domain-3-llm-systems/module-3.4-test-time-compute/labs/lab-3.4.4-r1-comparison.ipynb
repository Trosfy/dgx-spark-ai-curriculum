{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.4.4: R1 vs Standard Model Comparison\n",
    "\n",
    "**Module:** 3.4 - Test-Time Compute & Reasoning  \n",
    "**Time:** 1.5 hours  \n",
    "**Difficulty:** ⭐⭐⭐ (Intermediate-Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- [ ] Quantify the accuracy advantage of reasoning models\n",
    "- [ ] Understand the token/time overhead of thinking\n",
    "- [ ] Know when reasoning models are worth the extra cost\n",
    "- [ ] Compare models across different problem categories\n",
    "- [ ] Make data-driven decisions about model selection\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Lab 3.4.3 (DeepSeek-R1 Exploration)\n",
    "- Both DeepSeek-R1 and a standard model (e.g., Llama 3.1) available in Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "When deploying AI in production, you face a key question: **Should I use a reasoning model or a standard model?**\n",
    "\n",
    "The answer depends on:\n",
    "- Task complexity (is reasoning needed?)\n",
    "- Accuracy requirements (can I tolerate errors?)\n",
    "- Latency constraints (do I need fast responses?)\n",
    "- Cost budget (more tokens = more money)\n",
    "\n",
    "This lab gives you the data to make informed decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: Why Compare?\n",
    "\n",
    "> **Imagine hiring for a job...**\n",
    ">\n",
    "> **Candidate A (Standard Model):** Fast worker, answers quickly.\n",
    "> Sometimes right, sometimes wrong. Cheap to hire.\n",
    ">\n",
    "> **Candidate B (R1):** Careful worker, thinks before answering.\n",
    "> Usually right, but takes longer. More expensive.\n",
    ">\n",
    "> **The question:** For THIS job, which is the better hire?\n",
    "> - Simple data entry? Candidate A is fine.\n",
    "> - Complex analysis where errors are costly? Candidate B is worth it.\n",
    ">\n",
    "> **This lab:** We'll run both candidates through a test and measure exactly how they perform!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "\n",
    "import ollama\n",
    "\n",
    "# List available models\n",
    "models = ollama.list()\n",
    "model_names = [m['name'] for m in models.get('models', [])]\n",
    "\n",
    "print(\"Available models:\")\n",
    "for name in model_names:\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure models for comparison\n",
    "# Adjust these based on your available models\n",
    "\n",
    "# Reasoning model (R1)\n",
    "R1_MODEL = None\n",
    "for name in model_names:\n",
    "    if 'r1' in name.lower() or 'deepseek-r1' in name.lower():\n",
    "        R1_MODEL = name\n",
    "        break\n",
    "\n",
    "# Standard model (Llama, Qwen, etc.)\n",
    "STANDARD_MODEL = None\n",
    "for name in model_names:\n",
    "    if any(x in name.lower() for x in ['llama', 'qwen', 'mistral', 'gemma']):\n",
    "        if 'r1' not in name.lower():  # Exclude R1 distilled versions\n",
    "            STANDARD_MODEL = name\n",
    "            break\n",
    "\n",
    "print(f\"\\nReasoningmodel (R1): {R1_MODEL or 'Not found'}\")\n",
    "print(f\"Standard model: {STANDARD_MODEL or 'Not found'}\")\n",
    "\n",
    "if not R1_MODEL:\n",
    "    print(\"\\nTo download R1: ollama pull deepseek-r1:7b\")\n",
    "if not STANDARD_MODEL:\n",
    "    print(\"To download standard model: ollama pull llama3.1:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual override if auto-detection failed\n",
    "# Uncomment and set these if needed:\n",
    "\n",
    "# R1_MODEL = \"deepseek-r1:7b\"\n",
    "# STANDARD_MODEL = \"llama3.1:8b\"\n",
    "\n",
    "# For fair comparison, try to match parameter counts:\n",
    "# - deepseek-r1:7b vs llama3.1:8b\n",
    "# - deepseek-r1:32b vs qwen2.5:32b\n",
    "# - deepseek-r1:70b vs llama3.1:70b\n",
    "\n",
    "print(f\"Using R1: {R1_MODEL}\")\n",
    "print(f\"Using Standard: {STANDARD_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test problems\n",
    "data_path = Path(\"../data/test_problems.json\")\n",
    "if data_path.exists():\n",
    "    with open(data_path) as f:\n",
    "        all_problems = json.load(f)\n",
    "    print(f\"Loaded problems:\")\n",
    "    print(f\"  Math: {len(all_problems['math'])}\")\n",
    "    print(f\"  Code: {len(all_problems['code'])}\")\n",
    "    print(f\"  Reasoning: {len(all_problems['reasoning'])}\")\n",
    "else:\n",
    "    # Fallback\n",
    "    all_problems = {\n",
    "        \"math\": [\n",
    "            {\"question\": \"What is 17 * 23?\", \"answer\": 391},\n",
    "            {\"question\": \"What is 15% of 240?\", \"answer\": 36},\n",
    "            {\"question\": \"If 3x + 7 = 22, what is x?\", \"answer\": 5},\n",
    "        ],\n",
    "        \"reasoning\": [\n",
    "            {\"question\": \"A bat and ball cost $1.10. The bat costs $1 more than the ball. How much does the ball cost in cents?\", \"answer\": 5},\n",
    "            {\"question\": \"Tom is taller than Jim. Jim is taller than Mary. Is Tom taller than Mary? Answer yes or no.\", \"answer\": \"yes\"},\n",
    "        ],\n",
    "        \"code\": [\n",
    "            {\"question\": \"Write a Python function is_prime(n) that returns True if n is prime.\", \"test_cases\": [{\"input\": 17, \"expected\": True}]},\n",
    "        ]\n",
    "    }\n",
    "    print(\"Using fallback problems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvalResult:\n",
    "    \"\"\"Result from evaluating a single problem.\"\"\"\n",
    "    question: str\n",
    "    expected: str\n",
    "    predicted: str\n",
    "    correct: bool\n",
    "    response_time: float\n",
    "    response_tokens: int\n",
    "    thinking_tokens: int\n",
    "    category: str\n",
    "    full_response: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelEvaluation:\n",
    "    \"\"\"Complete evaluation of a model.\"\"\"\n",
    "    model_name: str\n",
    "    results: List[EvalResult] = field(default_factory=list)\n",
    "    \n",
    "    @property\n",
    "    def accuracy(self) -> float:\n",
    "        if not self.results:\n",
    "            return 0.0\n",
    "        return sum(1 for r in self.results if r.correct) / len(self.results)\n",
    "    \n",
    "    @property\n",
    "    def avg_time(self) -> float:\n",
    "        if not self.results:\n",
    "            return 0.0\n",
    "        return sum(r.response_time for r in self.results) / len(self.results)\n",
    "    \n",
    "    @property\n",
    "    def total_tokens(self) -> int:\n",
    "        return sum(r.response_tokens for r in self.results)\n",
    "    \n",
    "    @property\n",
    "    def total_thinking_tokens(self) -> int:\n",
    "        return sum(r.thinking_tokens for r in self.results)\n",
    "    \n",
    "    def accuracy_by_category(self) -> Dict[str, float]:\n",
    "        by_cat = defaultdict(list)\n",
    "        for r in self.results:\n",
    "            by_cat[r.category].append(r.correct)\n",
    "        return {cat: sum(correct) / len(correct) for cat, correct in by_cat.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(response: str, expected_type: str = \"number\") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract answer from response based on expected type.\n",
    "    \"\"\"\n",
    "    # Remove thinking tokens if present\n",
    "    response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL)\n",
    "    response = response.strip()\n",
    "    \n",
    "    if expected_type == \"number\":\n",
    "        # Look for patterns like \"The answer is X\"\n",
    "        patterns = [\n",
    "            r\"[Tt]he (?:final )?answer is[:\\s]+\\$?([\\d,]+(?:\\.\\d+)?)\",\n",
    "            r\"[Aa]nswer[:\\s]+\\$?([\\d,]+(?:\\.\\d+)?)\",\n",
    "            r\"=\\s*\\$?([\\d,]+(?:\\.\\d+)?)\\s*(?:$|\\.|\\n)\",\n",
    "        ]\n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, response)\n",
    "            if matches:\n",
    "                num_str = matches[-1].replace(',', '')\n",
    "                try:\n",
    "                    num = float(num_str)\n",
    "                    if num == int(num):\n",
    "                        return str(int(num))\n",
    "                    return str(num)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        # Fallback: last number\n",
    "        numbers = re.findall(r'-?[\\d,]+(?:\\.\\d+)?', response)\n",
    "        if numbers:\n",
    "            num_str = numbers[-1].replace(',', '')\n",
    "            try:\n",
    "                num = float(num_str)\n",
    "                if num == int(num):\n",
    "                    return str(int(num))\n",
    "                return str(num)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    elif expected_type == \"yes_no\":\n",
    "        response_lower = response.lower()\n",
    "        if 'yes' in response_lower:\n",
    "            return 'yes'\n",
    "        elif 'no' in response_lower:\n",
    "            return 'no'\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def count_thinking_tokens(response: str) -> int:\n",
    "    \"\"\"Count tokens in <think> blocks.\"\"\"\n",
    "    matches = re.findall(r'<think>(.*?)</think>', response, re.DOTALL)\n",
    "    thinking_text = ' '.join(matches)\n",
    "    return len(thinking_text) // 4  # Rough estimate\n",
    "\n",
    "\n",
    "def compare_answers(predicted: Optional[str], expected, tolerance: float = 0.01) -> bool:\n",
    "    \"\"\"Compare predicted to expected answer.\"\"\"\n",
    "    if predicted is None:\n",
    "        return False\n",
    "    \n",
    "    # Try numeric comparison\n",
    "    try:\n",
    "        pred_num = float(str(predicted).replace(',', ''))\n",
    "        exp_num = float(str(expected).replace(',', ''))\n",
    "        if exp_num == 0:\n",
    "            return abs(pred_num) < tolerance\n",
    "        return abs(pred_num - exp_num) / abs(exp_num) < tolerance\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # String comparison\n",
    "    return str(predicted).lower().strip() == str(expected).lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: str,\n",
    "    problems: Dict[str, List],\n",
    "    n_per_category: int = 5,\n",
    "    use_cot: bool = True,\n",
    "    verbose: bool = True,\n",
    ") -> ModelEvaluation:\n",
    "    \"\"\"\n",
    "    Evaluate a model on problems from each category.\n",
    "    \"\"\"\n",
    "    evaluation = ModelEvaluation(model_name=model)\n",
    "    \n",
    "    for category, probs in problems.items():\n",
    "        if category == 'code':  # Skip code for now (harder to evaluate)\n",
    "            continue\n",
    "            \n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Category: {category.upper()}\")\n",
    "            print('='*50)\n",
    "        \n",
    "        for i, prob in enumerate(probs[:n_per_category]):\n",
    "            question = prob.get('question', '')\n",
    "            expected = prob.get('answer', prob.get('numerical_answer', ''))\n",
    "            \n",
    "            # Determine expected type\n",
    "            if str(expected).lower() in ['yes', 'no']:\n",
    "                exp_type = 'yes_no'\n",
    "            else:\n",
    "                exp_type = 'number'\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\nProblem {i+1}: {question[:60]}...\")\n",
    "            \n",
    "            # Build prompt\n",
    "            if use_cot:\n",
    "                prompt = f\"{question}\\n\\nLet's think step by step:\"\n",
    "            else:\n",
    "                prompt = question\n",
    "            \n",
    "            # Get response\n",
    "            start_time = time.time()\n",
    "            response = ollama.chat(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                options={\"temperature\": 0.0, \"num_predict\": 1024}\n",
    "            )\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            response_text = response['message']['content']\n",
    "            \n",
    "            # Extract and compare\n",
    "            predicted = extract_answer(response_text, exp_type)\n",
    "            correct = compare_answers(predicted, expected)\n",
    "            thinking_tokens = count_thinking_tokens(response_text)\n",
    "            response_tokens = len(response_text) // 4\n",
    "            \n",
    "            result = EvalResult(\n",
    "                question=question,\n",
    "                expected=str(expected),\n",
    "                predicted=predicted or \"N/A\",\n",
    "                correct=correct,\n",
    "                response_time=elapsed,\n",
    "                response_tokens=response_tokens,\n",
    "                thinking_tokens=thinking_tokens,\n",
    "                category=category,\n",
    "                full_response=response_text,\n",
    "            )\n",
    "            evaluation.results.append(result)\n",
    "            \n",
    "            if verbose:\n",
    "                status = \"CORRECT\" if correct else \"WRONG\"\n",
    "                print(f\"  Expected: {expected}, Predicted: {predicted} [{status}]\")\n",
    "                print(f\"  Time: {elapsed:.1f}s, Tokens: {response_tokens} (thinking: {thinking_tokens})\")\n",
    "    \n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Run the Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of problems per category to test\n",
    "N_PROBLEMS = 5  # Increase for more thorough evaluation\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"EVALUATING: {R1_MODEL} (Reasoning Model)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if R1_MODEL:\n",
    "    r1_eval = evaluate_model(\n",
    "        R1_MODEL,\n",
    "        all_problems,\n",
    "        n_per_category=N_PROBLEMS,\n",
    "        use_cot=True,\n",
    "        verbose=True\n",
    "    )\n",
    "else:\n",
    "    print(\"R1 model not available. Skipping.\")\n",
    "    r1_eval = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"EVALUATING: {STANDARD_MODEL} (Standard Model)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if STANDARD_MODEL:\n",
    "    standard_eval = evaluate_model(\n",
    "        STANDARD_MODEL,\n",
    "        all_problems,\n",
    "        n_per_category=N_PROBLEMS,\n",
    "        use_cot=True,\n",
    "        verbose=True\n",
    "    )\n",
    "else:\n",
    "    print(\"Standard model not available. Skipping.\")\n",
    "    standard_eval = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_comparison_report(r1_eval: ModelEvaluation, std_eval: ModelEvaluation):\n",
    "    \"\"\"Print a detailed comparison report.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL COMPARISON REPORT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Overall comparison\n",
    "    print(f\"\\n{'Metric':<30} {'R1 Model':<18} {'Standard Model':<18}\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'Model Name':<30} {r1_eval.model_name[:16]:<18} {std_eval.model_name[:16]:<18}\")\n",
    "    print(f\"{'Overall Accuracy':<30} {r1_eval.accuracy:<18.1%} {std_eval.accuracy:<18.1%}\")\n",
    "    print(f\"{'Avg Response Time':<30} {r1_eval.avg_time:<18.1f}s {std_eval.avg_time:<18.1f}s\")\n",
    "    print(f\"{'Total Tokens':<30} {r1_eval.total_tokens:<18} {std_eval.total_tokens:<18}\")\n",
    "    print(f\"{'Thinking Tokens':<30} {r1_eval.total_thinking_tokens:<18} {std_eval.total_thinking_tokens:<18}\")\n",
    "    \n",
    "    # By category\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"ACCURACY BY CATEGORY\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    r1_by_cat = r1_eval.accuracy_by_category()\n",
    "    std_by_cat = std_eval.accuracy_by_category()\n",
    "    \n",
    "    all_cats = set(r1_by_cat.keys()) | set(std_by_cat.keys())\n",
    "    \n",
    "    print(f\"{'Category':<20} {'R1':<15} {'Standard':<15} {'Difference':<15}\")\n",
    "    for cat in sorted(all_cats):\n",
    "        r1_acc = r1_by_cat.get(cat, 0)\n",
    "        std_acc = std_by_cat.get(cat, 0)\n",
    "        diff = r1_acc - std_acc\n",
    "        sign = \"+\" if diff > 0 else \"\"\n",
    "        print(f\"{cat:<20} {r1_acc:<15.1%} {std_acc:<15.1%} {sign}{diff:<15.1%}\")\n",
    "    \n",
    "    # Cost analysis\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"COST-BENEFIT ANALYSIS\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    acc_improvement = r1_eval.accuracy - std_eval.accuracy\n",
    "    time_overhead = r1_eval.avg_time / std_eval.avg_time if std_eval.avg_time > 0 else 0\n",
    "    token_overhead = r1_eval.total_tokens / std_eval.total_tokens if std_eval.total_tokens > 0 else 0\n",
    "    \n",
    "    print(f\"Accuracy improvement: {acc_improvement:+.1%}\")\n",
    "    print(f\"Time overhead: {time_overhead:.1f}x\")\n",
    "    print(f\"Token overhead: {token_overhead:.1f}x\")\n",
    "    \n",
    "    # Recommendation\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"RECOMMENDATION\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    if acc_improvement > 0.1:\n",
    "        print(f\"R1 shows significant accuracy improvement (+{acc_improvement:.0%}).\")\n",
    "        print(\"Use R1 for:\")\n",
    "        print(\"  - Complex reasoning tasks\")\n",
    "        print(\"  - High-stakes decisions\")\n",
    "        print(\"  - When accuracy matters more than speed\")\n",
    "    elif acc_improvement > 0:\n",
    "        print(f\"R1 shows modest improvement (+{acc_improvement:.0%}).\")\n",
    "        print(\"Consider R1 for complex tasks, standard model for simple ones.\")\n",
    "    else:\n",
    "        print(\"Standard model matches or beats R1 on this benchmark.\")\n",
    "        print(\"The overhead may not be worth it for these tasks.\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "# Print report\n",
    "if r1_eval and standard_eval:\n",
    "    print_comparison_report(r1_eval, standard_eval)\n",
    "else:\n",
    "    print(\"Cannot generate comparison - one or both models not evaluated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Detailed Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(r1_eval: ModelEvaluation, std_eval: ModelEvaluation):\n",
    "    \"\"\"Analyze where each model made errors.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ERROR ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Find problems where models disagreed\n",
    "    r1_results = {r.question: r for r in r1_eval.results}\n",
    "    std_results = {r.question: r for r in std_eval.results}\n",
    "    \n",
    "    both_correct = []\n",
    "    both_wrong = []\n",
    "    r1_only_correct = []\n",
    "    std_only_correct = []\n",
    "    \n",
    "    for q in r1_results:\n",
    "        if q in std_results:\n",
    "            r1_correct = r1_results[q].correct\n",
    "            std_correct = std_results[q].correct\n",
    "            \n",
    "            if r1_correct and std_correct:\n",
    "                both_correct.append(q)\n",
    "            elif not r1_correct and not std_correct:\n",
    "                both_wrong.append(q)\n",
    "            elif r1_correct and not std_correct:\n",
    "                r1_only_correct.append(q)\n",
    "            else:\n",
    "                std_only_correct.append(q)\n",
    "    \n",
    "    total = len(r1_results)\n",
    "    \n",
    "    print(f\"\\nProblem Agreement Analysis ({total} problems):\")\n",
    "    print(f\"  Both correct: {len(both_correct)} ({len(both_correct)/total:.0%})\")\n",
    "    print(f\"  Both wrong: {len(both_wrong)} ({len(both_wrong)/total:.0%})\")\n",
    "    print(f\"  R1 only correct: {len(r1_only_correct)} ({len(r1_only_correct)/total:.0%})\")\n",
    "    print(f\"  Standard only correct: {len(std_only_correct)} ({len(std_only_correct)/total:.0%})\")\n",
    "    \n",
    "    # Show examples where R1 succeeded\n",
    "    if r1_only_correct:\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "        print(\"Examples where R1 SUCCEEDED but Standard FAILED:\")\n",
    "        print(\"-\"*50)\n",
    "        for q in r1_only_correct[:3]:\n",
    "            r1_r = r1_results[q]\n",
    "            std_r = std_results[q]\n",
    "            print(f\"\\nQ: {q[:80]}...\")\n",
    "            print(f\"  Expected: {r1_r.expected}\")\n",
    "            print(f\"  R1 predicted: {r1_r.predicted} (CORRECT)\")\n",
    "            print(f\"  Standard predicted: {std_r.predicted} (WRONG)\")\n",
    "    \n",
    "    # Show examples where Standard succeeded\n",
    "    if std_only_correct:\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "        print(\"Examples where Standard SUCCEEDED but R1 FAILED:\")\n",
    "        print(\"-\"*50)\n",
    "        for q in std_only_correct[:3]:\n",
    "            r1_r = r1_results[q]\n",
    "            std_r = std_results[q]\n",
    "            print(f\"\\nQ: {q[:80]}...\")\n",
    "            print(f\"  Expected: {r1_r.expected}\")\n",
    "            print(f\"  R1 predicted: {r1_r.predicted} (WRONG)\")\n",
    "            print(f\"  Standard predicted: {std_r.predicted} (CORRECT)\")\n",
    "\n",
    "\n",
    "if r1_eval and standard_eval:\n",
    "    analyze_errors(r1_eval, standard_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Token Economy Analysis\n",
    "\n",
    "How much does the extra thinking cost in terms of tokens (which translates to API costs and latency)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_token_economy(r1_eval: ModelEvaluation, std_eval: ModelEvaluation):\n",
    "    \"\"\"Analyze the token cost vs accuracy benefit.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TOKEN ECONOMY ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Total tokens\n",
    "    r1_tokens = r1_eval.total_tokens\n",
    "    std_tokens = std_eval.total_tokens\n",
    "    thinking_tokens = r1_eval.total_thinking_tokens\n",
    "    \n",
    "    # Accuracy\n",
    "    r1_acc = r1_eval.accuracy\n",
    "    std_acc = std_eval.accuracy\n",
    "    \n",
    "    # Calculations\n",
    "    extra_tokens = r1_tokens - std_tokens\n",
    "    extra_correct = sum(1 for r in r1_eval.results if r.correct) - sum(1 for r in std_eval.results if r.correct)\n",
    "    \n",
    "    print(f\"\\nToken Usage:\")\n",
    "    print(f\"  R1 total tokens: {r1_tokens:,}\")\n",
    "    print(f\"  Standard total tokens: {std_tokens:,}\")\n",
    "    print(f\"  Extra tokens for R1: {extra_tokens:,} (+{extra_tokens/std_tokens*100:.0f}%)\")\n",
    "    print(f\"  Thinking tokens (R1): {thinking_tokens:,} ({thinking_tokens/r1_tokens*100:.0f}% of R1 output)\")\n",
    "    \n",
    "    print(f\"\\nAccuracy Impact:\")\n",
    "    print(f\"  R1 correct: {sum(1 for r in r1_eval.results if r.correct)}/{len(r1_eval.results)}\")\n",
    "    print(f\"  Standard correct: {sum(1 for r in std_eval.results if r.correct)}/{len(std_eval.results)}\")\n",
    "    print(f\"  Extra correct with R1: {extra_correct}\")\n",
    "    \n",
    "    if extra_correct > 0 and extra_tokens > 0:\n",
    "        cost_per_extra_correct = extra_tokens / extra_correct\n",
    "        print(f\"\\n  Token cost per extra correct answer: {cost_per_extra_correct:.0f} tokens\")\n",
    "    \n",
    "    # Cost estimation (rough, based on typical API pricing)\n",
    "    # Assuming ~$0.001 per 1K tokens (varies by model/provider)\n",
    "    print(\"\\n  Estimated API cost difference (at $0.001/1K tokens):\")\n",
    "    r1_cost = r1_tokens / 1000 * 0.001\n",
    "    std_cost = std_tokens / 1000 * 0.001\n",
    "    print(f\"    R1: ${r1_cost:.4f}\")\n",
    "    print(f\"    Standard: ${std_cost:.4f}\")\n",
    "    print(f\"    Difference: ${r1_cost - std_cost:.4f}\")\n",
    "\n",
    "\n",
    "if r1_eval and standard_eval:\n",
    "    analyze_token_economy(r1_eval, standard_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Decision Framework\n",
    "\n",
    "Based on our analysis, when should you use a reasoning model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_framework = \"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════╗\n",
    "║             WHEN TO USE A REASONING MODEL (R1)                       ║\n",
    "╠══════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                      ║\n",
    "║  USE R1 WHEN:                                                        ║\n",
    "║  ─────────────────────────────────────────────────────────────────   ║\n",
    "║  ✓ Task requires multi-step reasoning                                ║\n",
    "║  ✓ Accuracy is more important than speed                             ║\n",
    "║  ✓ You need interpretable reasoning (for auditing/debugging)         ║\n",
    "║  ✓ High-stakes decisions (medical, legal, financial)                 ║\n",
    "║  ✓ Math, logic, or coding problems                                   ║\n",
    "║                                                                      ║\n",
    "║  USE STANDARD MODEL WHEN:                                            ║\n",
    "║  ─────────────────────────────────────────────────────────────────   ║\n",
    "║  ✓ Simple factual questions                                          ║\n",
    "║  ✓ Speed is critical (real-time applications)                        ║\n",
    "║  ✓ Cost-sensitive (high-volume, low-margin)                          ║\n",
    "║  ✓ Creative writing or open-ended generation                         ║\n",
    "║  ✓ Simple classification or extraction tasks                         ║\n",
    "║                                                                      ║\n",
    "║  HYBRID APPROACH (Best of Both Worlds):                              ║\n",
    "║  ─────────────────────────────────────────────────────────────────   ║\n",
    "║  1. Use classifier to detect query complexity                        ║\n",
    "║  2. Route simple queries → Standard model (fast, cheap)              ║\n",
    "║  3. Route complex queries → R1 (accurate, thorough)                  ║\n",
    "║  4. Cache repeated reasoning patterns                                ║\n",
    "║                                                                      ║\n",
    "╚══════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\"\n",
    "\n",
    "print(decision_framework)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Comparing Models of Very Different Sizes\n",
    "\n",
    "```python\n",
    "# Wrong: Unfair comparison\n",
    "compare(\"deepseek-r1:70b\", \"llama3.1:8b\")  # 70B vs 8B!\n",
    "\n",
    "# Right: Similar parameter counts\n",
    "compare(\"deepseek-r1:7b\", \"llama3.1:8b\")   # ~7B vs 8B\n",
    "compare(\"deepseek-r1:70b\", \"llama3.1:70b\") # 70B vs 70B\n",
    "```\n",
    "\n",
    "### Mistake 2: Not Using CoT with Standard Model\n",
    "\n",
    "```python\n",
    "# Wrong: Comparing R1 (with built-in CoT) vs Standard (no CoT)\n",
    "r1_response = query(r1_model, question)  # Has <think> tokens\n",
    "std_response = query(std_model, question)  # Direct answer\n",
    "\n",
    "# Right: Give standard model CoT too\n",
    "std_response = query(std_model, question + \"\\nLet's think step by step:\")\n",
    "```\n",
    "\n",
    "### Mistake 3: Small Sample Size\n",
    "\n",
    "```python\n",
    "# Wrong: Too few problems\n",
    "evaluate(model, problems[:3])  # Only 3 problems!\n",
    "\n",
    "# Right: Enough for statistical significance\n",
    "evaluate(model, problems[:30])  # At least 30+ per category\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ✅ How to fairly compare reasoning vs standard models\n",
    "- ✅ How to measure accuracy, speed, and token usage\n",
    "- ✅ How to analyze where each model succeeds/fails\n",
    "- ✅ The token economy of reasoning models\n",
    "- ✅ When to choose reasoning vs standard models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results\n",
    "if r1_eval and standard_eval:\n",
    "    comparison_summary = {\n",
    "        'r1_model': r1_eval.model_name,\n",
    "        'standard_model': standard_eval.model_name,\n",
    "        'r1_accuracy': r1_eval.accuracy,\n",
    "        'standard_accuracy': standard_eval.accuracy,\n",
    "        'accuracy_difference': r1_eval.accuracy - standard_eval.accuracy,\n",
    "        'r1_avg_time': r1_eval.avg_time,\n",
    "        'standard_avg_time': standard_eval.avg_time,\n",
    "        'r1_total_tokens': r1_eval.total_tokens,\n",
    "        'standard_total_tokens': standard_eval.total_tokens,\n",
    "        'r1_thinking_tokens': r1_eval.total_thinking_tokens,\n",
    "        'r1_accuracy_by_category': r1_eval.accuracy_by_category(),\n",
    "        'standard_accuracy_by_category': standard_eval.accuracy_by_category(),\n",
    "    }\n",
    "    \n",
    "    print(\"Comparison Summary:\")\n",
    "    print(json.dumps(comparison_summary, indent=2, default=str))\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"\\nMemory cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Great work! You now have data-driven insights on reasoning models.\n",
    "\n",
    "In the next lab, you'll learn **Best-of-N sampling with reward models** - another powerful way to improve output quality at inference time.\n",
    "\n",
    "**Continue to:** [Lab 3.4.5: Best-of-N with Reward Model](./lab-3.4.5-best-of-n.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
