{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.4.6: Reasoning Pipeline\n",
    "\n",
    "**Module:** 3.4 - Test-Time Compute & Reasoning  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐⭐⭐ (Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- [ ] Build an intelligent query router that detects complexity\n",
    "- [ ] Route simple queries to fast models, complex ones to reasoning models\n",
    "- [ ] Implement caching for repeated reasoning patterns\n",
    "- [ ] Measure overall latency and quality improvements\n",
    "- [ ] Deploy a production-ready adaptive reasoning system\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Labs 3.4.1-3.4.5\n",
    "- Multiple models available in Ollama (fast + reasoning)\n",
    "- Understanding of CoT, self-consistency, and reward models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "In production, you can't use a reasoning model for every query - it's too slow and expensive. The solution: **intelligent routing**.\n",
    "\n",
    "**The Problem:**\n",
    "- Simple queries (\"What's the capital of France?\") don't need R1\n",
    "- Complex queries (\"Solve this optimization problem\") do\n",
    "- Using R1 for everything: slow, expensive\n",
    "- Using fast model for everything: inaccurate on hard problems\n",
    "\n",
    "**The Solution:** Build a pipeline that:\n",
    "1. Classifies query complexity\n",
    "2. Routes to appropriate model\n",
    "3. Caches results for efficiency\n",
    "\n",
    "**Industry Examples:**\n",
    "- **ChatGPT:** Routes between GPT-3.5 and GPT-4 based on complexity\n",
    "- **Claude:** Uses different reasoning depths internally\n",
    "- **Cursor:** Routes coding queries to specialized models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: Adaptive Reasoning Pipeline\n",
    "\n",
    "> **Imagine a hospital emergency room...**\n",
    ">\n",
    "> Not every patient needs to see the top specialist!\n",
    ">\n",
    "> **Triage Nurse (Router):** Quickly assesses each patient\n",
    "> - Minor scrape? → Nurse can handle it (fast model)\n",
    "> - Complex symptoms? → See the specialist (reasoning model)\n",
    ">\n",
    "> **Our AI Pipeline:**\n",
    "> - Simple question? → Fast 8B model (milliseconds)\n",
    "> - Complex reasoning? → R1 70B model (seconds, but accurate)\n",
    ">\n",
    "> **Bonus:** If someone asks the same question twice,\n",
    "> we just look up the previous answer (cache)!\n",
    "\n",
    "```\n",
    "Query ─> [Complexity Classifier] ─┬─> Simple ─> Fast Model (8B) ─┐\n",
    "                                  │                              ├─> Response\n",
    "                                  └─> Complex ─> R1 Model (70B) ─┘\n",
    "                                        ↑\n",
    "                                    [Cache: Skip if seen before]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport time\nimport hashlib\nimport re\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Tuple, Callable\nfrom dataclasses import dataclass, field\nfrom collections import OrderedDict\nfrom enum import Enum\n\nimport ollama\n\n# List available models\nmodels = ollama.list()\nmodel_names = [m.get('name', 'unknown') for m in models.get('models', [])]\n\nprint(\"Available models:\")\nfor name in model_names:\n    print(f\"  - {name}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure models\n",
    "# Fast model: smaller, quicker responses\n",
    "FAST_MODEL = None\n",
    "for name in model_names:\n",
    "    if any(x in name.lower() for x in ['7b', '8b', '3b']) and 'r1' not in name.lower():\n",
    "        FAST_MODEL = name\n",
    "        break\n",
    "\n",
    "# Reasoning model: larger, more capable\n",
    "REASONING_MODEL = None\n",
    "for name in model_names:\n",
    "    if 'r1' in name.lower():\n",
    "        REASONING_MODEL = name\n",
    "        break\n",
    "\n",
    "# Fallback: use largest available model as reasoning\n",
    "if not REASONING_MODEL:\n",
    "    for name in model_names:\n",
    "        if any(x in name.lower() for x in ['70b', '32b', '14b']):\n",
    "            REASONING_MODEL = name\n",
    "            break\n",
    "\n",
    "# If still not found, use same model for both\n",
    "if not FAST_MODEL:\n",
    "    FAST_MODEL = model_names[0] if model_names else \"qwen3:8b\"\n",
    "if not REASONING_MODEL:\n",
    "    REASONING_MODEL = FAST_MODEL\n",
    "\n",
    "print(f\"Fast model: {FAST_MODEL}\")\n",
    "print(f\"Reasoning model: {REASONING_MODEL}\")\n",
    "\n",
    "# Note if using same model\n",
    "if FAST_MODEL == REASONING_MODEL:\n",
    "    print(\"\\nNote: Using same model for both. The pipeline will still demonstrate routing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Query Complexity Classifier\n",
    "\n",
    "The first component: detect whether a query needs reasoning or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryComplexity(Enum):\n",
    "    \"\"\"Query complexity levels.\"\"\"\n",
    "    SIMPLE = \"simple\"      # Factual, direct answer\n",
    "    MODERATE = \"moderate\"  # Some reasoning needed\n",
    "    COMPLEX = \"complex\"    # Multi-step reasoning required\n",
    "\n",
    "\n",
    "class ComplexityClassifier:\n",
    "    \"\"\"\n",
    "    Classify query complexity using heuristics and patterns.\n",
    "    \n",
    "    This uses rule-based classification. For production, you could\n",
    "    use a small trained classifier or an LLM call.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keywords indicating complexity\n",
    "    COMPLEX_KEYWORDS = [\n",
    "        'solve', 'calculate', 'prove', 'derive', 'analyze',\n",
    "        'step by step', 'explain how', 'why does', 'compare and contrast',\n",
    "        'what if', 'optimize', 'debug', 'implement', 'algorithm',\n",
    "        'mathematical', 'equation', 'probability', 'logic puzzle',\n",
    "    ]\n",
    "    \n",
    "    SIMPLE_KEYWORDS = [\n",
    "        'what is', 'who is', 'when did', 'where is', 'define',\n",
    "        'capital of', 'how many', 'true or false', 'yes or no',\n",
    "    ]\n",
    "    \n",
    "    # Patterns indicating math problems\n",
    "    MATH_PATTERNS = [\n",
    "        r'\\d+\\s*[+\\-*/]\\s*\\d+',  # Basic arithmetic\n",
    "        r'\\d+%\\s*of',            # Percentage\n",
    "        r'equation|formula',      # Math terms\n",
    "        r'\\$\\d+',                # Money calculations\n",
    "    ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.call_count = 0\n",
    "        self.classifications = []\n",
    "    \n",
    "    def classify(self, query: str) -> QueryComplexity:\n",
    "        \"\"\"\n",
    "        Classify a query's complexity.\n",
    "        \n",
    "        Returns:\n",
    "            QueryComplexity enum value\n",
    "        \"\"\"\n",
    "        self.call_count += 1\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Score based on indicators\n",
    "        complexity_score = 0\n",
    "        \n",
    "        # Check complex keywords\n",
    "        for keyword in self.COMPLEX_KEYWORDS:\n",
    "            if keyword in query_lower:\n",
    "                complexity_score += 2\n",
    "        \n",
    "        # Check simple keywords (reduce score)\n",
    "        for keyword in self.SIMPLE_KEYWORDS:\n",
    "            if keyword in query_lower:\n",
    "                complexity_score -= 1\n",
    "        \n",
    "        # Check math patterns\n",
    "        for pattern in self.MATH_PATTERNS:\n",
    "            if re.search(pattern, query_lower):\n",
    "                complexity_score += 2\n",
    "        \n",
    "        # Length heuristic (longer = more complex)\n",
    "        if len(query) > 200:\n",
    "            complexity_score += 1\n",
    "        if len(query) > 500:\n",
    "            complexity_score += 1\n",
    "        \n",
    "        # Multiple sentences often mean complex\n",
    "        sentence_count = len(re.split(r'[.!?]', query))\n",
    "        if sentence_count > 3:\n",
    "            complexity_score += 1\n",
    "        \n",
    "        # Classify based on score\n",
    "        if complexity_score <= 0:\n",
    "            result = QueryComplexity.SIMPLE\n",
    "        elif complexity_score <= 3:\n",
    "            result = QueryComplexity.MODERATE\n",
    "        else:\n",
    "            result = QueryComplexity.COMPLEX\n",
    "        \n",
    "        self.classifications.append((query[:50], result))\n",
    "        return result\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get classification statistics.\"\"\"\n",
    "        counts = {c: 0 for c in QueryComplexity}\n",
    "        for _, complexity in self.classifications:\n",
    "            counts[complexity] += 1\n",
    "        return {\n",
    "            'total': self.call_count,\n",
    "            'by_complexity': {c.value: count for c, count in counts.items()},\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the classifier\n",
    "classifier = ComplexityClassifier()\n",
    "\n",
    "test_queries = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain how photosynthesis works.\",\n",
    "    \"Solve this equation step by step: 3x + 7 = 22\",\n",
    "    \"Who wrote Romeo and Juliet?\",\n",
    "    \"A train leaves at 9am traveling at 60mph. Another train leaves at 10am at 80mph. The stations are 280 miles apart. When do they meet?\",\n",
    "    \"Is Python a programming language?\",\n",
    "    \"Implement a function to find the longest palindromic substring in O(n) time complexity.\",\n",
    "]\n",
    "\n",
    "print(\"Query Complexity Classification:\")\n",
    "print(\"=\"*70)\n",
    "for query in test_queries:\n",
    "    complexity = classifier.classify(query)\n",
    "    print(f\"[{complexity.value.upper():8}] {query[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Response Cache\n",
    "\n",
    "Cache reasoning results to avoid recomputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRUCache:\n",
    "    \"\"\"\n",
    "    Least Recently Used (LRU) cache for responses.\n",
    "    \n",
    "    Caches query-response pairs. When full, evicts least recently used entries.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int = 100):\n",
    "        \"\"\"\n",
    "        Initialize cache.\n",
    "        \n",
    "        Args:\n",
    "            max_size: Maximum number of entries to cache\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.cache = OrderedDict()  # Maintains insertion order\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "    \n",
    "    def _make_key(self, query: str, model: str) -> str:\n",
    "        \"\"\"Create a cache key from query and model.\"\"\"\n",
    "        combined = f\"{model}:{query}\"\n",
    "        return hashlib.md5(combined.encode()).hexdigest()\n",
    "    \n",
    "    def get(self, query: str, model: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Get cached response if available.\n",
    "        \n",
    "        Returns:\n",
    "            Cached response or None\n",
    "        \"\"\"\n",
    "        key = self._make_key(query, model)\n",
    "        \n",
    "        if key in self.cache:\n",
    "            # Move to end (most recently used)\n",
    "            self.cache.move_to_end(key)\n",
    "            self.hits += 1\n",
    "            return self.cache[key]\n",
    "        \n",
    "        self.misses += 1\n",
    "        return None\n",
    "    \n",
    "    def set(self, query: str, model: str, response: str):\n",
    "        \"\"\"\n",
    "        Cache a response.\n",
    "        \"\"\"\n",
    "        key = self._make_key(query, model)\n",
    "        \n",
    "        # If already exists, update and move to end\n",
    "        if key in self.cache:\n",
    "            self.cache.move_to_end(key)\n",
    "            self.cache[key] = response\n",
    "            return\n",
    "        \n",
    "        # If full, evict oldest\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            self.cache.popitem(last=False)  # Remove oldest\n",
    "        \n",
    "        self.cache[key] = response\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear the cache.\"\"\"\n",
    "        self.cache.clear()\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get cache statistics.\"\"\"\n",
    "        total = self.hits + self.misses\n",
    "        hit_rate = self.hits / total if total > 0 else 0\n",
    "        return {\n",
    "            'size': len(self.cache),\n",
    "            'max_size': self.max_size,\n",
    "            'hits': self.hits,\n",
    "            'misses': self.misses,\n",
    "            'hit_rate': hit_rate,\n",
    "        }\n",
    "\n",
    "\n",
    "# Test cache\n",
    "cache = LRUCache(max_size=10)\n",
    "\n",
    "# Add some entries\n",
    "cache.set(\"What is 2+2?\", \"fast_model\", \"The answer is 4.\")\n",
    "cache.set(\"Explain gravity.\", \"reasoning_model\", \"Gravity is...\")\n",
    "\n",
    "# Test retrieval\n",
    "print(\"Cache test:\")\n",
    "print(f\"  Hit: {cache.get('What is 2+2?', 'fast_model')[:30]}...\")\n",
    "print(f\"  Miss: {cache.get('Unknown query', 'fast_model')}\")\n",
    "print(f\"  Stats: {cache.get_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Building the Reasoning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PipelineResponse:\n",
    "    \"\"\"Response from the reasoning pipeline.\"\"\"\n",
    "    response: str\n",
    "    model_used: str\n",
    "    complexity: QueryComplexity\n",
    "    from_cache: bool\n",
    "    latency: float\n",
    "    thinking_tokens: int = 0\n",
    "    \n",
    "    def __repr__(self):\n",
    "        cache_str = \"[CACHED]\" if self.from_cache else \"\"\n",
    "        return f\"PipelineResponse({self.complexity.value}, {self.model_used}, {self.latency:.2f}s) {cache_str}\"\n",
    "\n",
    "\n",
    "class ReasoningPipeline:\n",
    "    \"\"\"\n",
    "    Adaptive reasoning pipeline that routes queries intelligently.\n",
    "    \n",
    "    Features:\n",
    "    - Complexity-based routing\n",
    "    - Response caching\n",
    "    - Performance tracking\n",
    "    \n",
    "    Optimized for DGX Spark: can run fast model (8B) and reasoning\n",
    "    model (70B) both in memory thanks to 128GB unified memory.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        fast_model: str = FAST_MODEL,\n",
    "        reasoning_model: str = REASONING_MODEL,\n",
    "        cache_size: int = 100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline.\n",
    "        \n",
    "        Args:\n",
    "            fast_model: Model for simple queries\n",
    "            reasoning_model: Model for complex queries\n",
    "            cache_size: Maximum cache entries\n",
    "        \"\"\"\n",
    "        self.fast_model = fast_model\n",
    "        self.reasoning_model = reasoning_model\n",
    "        \n",
    "        self.classifier = ComplexityClassifier()\n",
    "        self.cache = LRUCache(max_size=cache_size)\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.total_queries = 0\n",
    "        self.fast_model_calls = 0\n",
    "        self.reasoning_model_calls = 0\n",
    "        self.total_latency = 0.0\n",
    "        self.saved_latency = 0.0  # From cache hits\n",
    "    \n",
    "    def query(\n",
    "        self,\n",
    "        query: str,\n",
    "        use_cache: bool = True,\n",
    "        force_reasoning: bool = False,\n",
    "        verbose: bool = False,\n",
    "    ) -> PipelineResponse:\n",
    "        \"\"\"\n",
    "        Process a query through the pipeline.\n",
    "        \n",
    "        Args:\n",
    "            query: The user query\n",
    "            use_cache: Whether to use caching\n",
    "            force_reasoning: Force use of reasoning model\n",
    "            verbose: Print detailed info\n",
    "        \n",
    "        Returns:\n",
    "            PipelineResponse with result and metadata\n",
    "        \"\"\"\n",
    "        self.total_queries += 1\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Step 1: Classify complexity\n",
    "        complexity = self.classifier.classify(query)\n",
    "        if verbose:\n",
    "            print(f\"Complexity: {complexity.value}\")\n",
    "        \n",
    "        # Step 2: Choose model\n",
    "        if force_reasoning or complexity == QueryComplexity.COMPLEX:\n",
    "            model = self.reasoning_model\n",
    "            use_cot = True\n",
    "        elif complexity == QueryComplexity.MODERATE:\n",
    "            # Use fast model with CoT for moderate\n",
    "            model = self.fast_model\n",
    "            use_cot = True\n",
    "        else:\n",
    "            model = self.fast_model\n",
    "            use_cot = False\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Model selected: {model}\")\n",
    "        \n",
    "        # Step 3: Check cache\n",
    "        if use_cache:\n",
    "            cached = self.cache.get(query, model)\n",
    "            if cached:\n",
    "                latency = time.time() - start_time\n",
    "                self.saved_latency += 2.0  # Estimate of saved time\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"Cache hit!\")\n",
    "                \n",
    "                return PipelineResponse(\n",
    "                    response=cached,\n",
    "                    model_used=model,\n",
    "                    complexity=complexity,\n",
    "                    from_cache=True,\n",
    "                    latency=latency,\n",
    "                )\n",
    "        \n",
    "        # Step 4: Generate response\n",
    "        if model == self.reasoning_model:\n",
    "            self.reasoning_model_calls += 1\n",
    "        else:\n",
    "            self.fast_model_calls += 1\n",
    "        \n",
    "        # Build prompt\n",
    "        if use_cot:\n",
    "            prompt = f\"{query}\\n\\nLet's think step by step:\"\n",
    "        else:\n",
    "            prompt = query\n",
    "        \n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            options={\"temperature\": 0.0, \"num_predict\": 1024}\n",
    "        )\n",
    "        \n",
    "        response_text = response['message']['content']\n",
    "        latency = time.time() - start_time\n",
    "        self.total_latency += latency\n",
    "        \n",
    "        # Count thinking tokens (for R1)\n",
    "        thinking_tokens = len(re.findall(r'<think>.*?</think>', response_text, re.DOTALL))\n",
    "        \n",
    "        # Step 5: Cache the response\n",
    "        if use_cache:\n",
    "            self.cache.set(query, model, response_text)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Response generated in {latency:.2f}s\")\n",
    "        \n",
    "        return PipelineResponse(\n",
    "            response=response_text,\n",
    "            model_used=model,\n",
    "            complexity=complexity,\n",
    "            from_cache=False,\n",
    "            latency=latency,\n",
    "            thinking_tokens=thinking_tokens,\n",
    "        )\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get pipeline statistics.\"\"\"\n",
    "        return {\n",
    "            'total_queries': self.total_queries,\n",
    "            'fast_model_calls': self.fast_model_calls,\n",
    "            'reasoning_model_calls': self.reasoning_model_calls,\n",
    "            'total_latency': self.total_latency,\n",
    "            'avg_latency': self.total_latency / max(self.total_queries, 1),\n",
    "            'saved_latency': self.saved_latency,\n",
    "            'cache_stats': self.cache.get_stats(),\n",
    "            'classifier_stats': self.classifier.get_stats(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline\n",
    "pipeline = ReasoningPipeline(\n",
    "    fast_model=FAST_MODEL,\n",
    "    reasoning_model=REASONING_MODEL,\n",
    "    cache_size=50,\n",
    ")\n",
    "\n",
    "print(f\"Pipeline created:\")\n",
    "print(f\"  Fast model: {pipeline.fast_model}\")\n",
    "print(f\"  Reasoning model: {pipeline.reasoning_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Testing the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries of varying complexity\n",
    "test_queries = [\n",
    "    # Simple (should use fast model)\n",
    "    \"What is the capital of Japan?\",\n",
    "    \"Is Python a programming language?\",\n",
    "    \n",
    "    # Moderate (fast model with CoT)\n",
    "    \"Explain what machine learning is in simple terms.\",\n",
    "    \"What are the main differences between Python and JavaScript?\",\n",
    "    \n",
    "    # Complex (should use reasoning model)\n",
    "    \"Solve step by step: If a train leaves at 9am traveling at 60mph, and another train leaves at 10am traveling at 80mph, when do they meet if they started 280 miles apart?\",\n",
    "    \"A bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost? Think carefully.\",\n",
    "]\n",
    "\n",
    "print(\"Testing Pipeline\\n\" + \"=\"*70)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query[:60]}...\")\n",
    "    \n",
    "    result = pipeline.query(query, verbose=True)\n",
    "    \n",
    "    print(f\"  Model: {result.model_used}\")\n",
    "    print(f\"  Complexity: {result.complexity.value}\")\n",
    "    print(f\"  Latency: {result.latency:.2f}s\")\n",
    "    print(f\"  Response: {result.response[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test caching by repeating a query\n",
    "print(\"\\nTesting Cache:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "query = \"What is the capital of Japan?\"\n",
    "\n",
    "# First call (cache miss)\n",
    "result1 = pipeline.query(query)\n",
    "print(f\"First call: {result1.latency:.3f}s (from_cache: {result1.from_cache})\")\n",
    "\n",
    "# Second call (cache hit)\n",
    "result2 = pipeline.query(query)\n",
    "print(f\"Second call: {result2.latency:.3f}s (from_cache: {result2.from_cache})\")\n",
    "\n",
    "speedup = result1.latency / result2.latency if result2.latency > 0 else 0\n",
    "print(f\"Speedup: {speedup:.0f}x faster with cache!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(\n",
    "    pipeline: ReasoningPipeline,\n",
    "    queries: List[str],\n",
    "    repetitions: int = 2,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Run a benchmark on the pipeline.\n",
    "    \n",
    "    Runs each query multiple times to test caching.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'queries': len(queries),\n",
    "        'repetitions': repetitions,\n",
    "        'total_calls': len(queries) * repetitions,\n",
    "        'latencies': [],\n",
    "        'by_complexity': {},\n",
    "        'cache_hits': 0,\n",
    "        'cache_misses': 0,\n",
    "    }\n",
    "    \n",
    "    for rep in range(repetitions):\n",
    "        for query in queries:\n",
    "            result = pipeline.query(query)\n",
    "            \n",
    "            results['latencies'].append(result.latency)\n",
    "            \n",
    "            if result.from_cache:\n",
    "                results['cache_hits'] += 1\n",
    "            else:\n",
    "                results['cache_misses'] += 1\n",
    "            \n",
    "            # Track by complexity\n",
    "            comp = result.complexity.value\n",
    "            if comp not in results['by_complexity']:\n",
    "                results['by_complexity'][comp] = {'count': 0, 'total_latency': 0}\n",
    "            results['by_complexity'][comp]['count'] += 1\n",
    "            results['by_complexity'][comp]['total_latency'] += result.latency\n",
    "    \n",
    "    # Calculate averages\n",
    "    results['avg_latency'] = sum(results['latencies']) / len(results['latencies'])\n",
    "    results['cache_hit_rate'] = results['cache_hits'] / results['total_calls']\n",
    "    \n",
    "    for comp in results['by_complexity']:\n",
    "        data = results['by_complexity'][comp]\n",
    "        data['avg_latency'] = data['total_latency'] / data['count']\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fresh pipeline for benchmarking\n",
    "benchmark_pipeline = ReasoningPipeline(\n",
    "    fast_model=FAST_MODEL,\n",
    "    reasoning_model=REASONING_MODEL,\n",
    ")\n",
    "\n",
    "# Mixed complexity queries\n",
    "benchmark_queries = [\n",
    "    # Simple\n",
    "    \"What year did World War II end?\",\n",
    "    \"Who painted the Mona Lisa?\",\n",
    "    \n",
    "    # Moderate\n",
    "    \"Explain what an API is.\",\n",
    "    \"What's the difference between HTTP and HTTPS?\",\n",
    "    \n",
    "    # Complex\n",
    "    \"Calculate step by step: A car travels 60 miles in 1.5 hours. What is its average speed in km/h? (1 mile = 1.6 km)\",\n",
    "]\n",
    "\n",
    "print(\"Running benchmark...\")\n",
    "benchmark_results = run_benchmark(benchmark_pipeline, benchmark_queries, repetitions=2)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BENCHMARK RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTotal queries: {benchmark_results['total_calls']}\")\n",
    "print(f\"Average latency: {benchmark_results['avg_latency']:.2f}s\")\n",
    "print(f\"Cache hit rate: {benchmark_results['cache_hit_rate']:.0%}\")\n",
    "\n",
    "print(\"\\nLatency by Complexity:\")\n",
    "for comp, data in benchmark_results['by_complexity'].items():\n",
    "    print(f\"  {comp}: {data['avg_latency']:.2f}s avg ({data['count']} calls)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: Pipeline vs Always-Reasoning\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE vs ALWAYS-REASONING COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get pipeline stats\n",
    "stats = benchmark_pipeline.get_stats()\n",
    "\n",
    "# Estimate what always-reasoning would cost\n",
    "# Assume reasoning model is ~3x slower on average\n",
    "fast_calls = stats['fast_model_calls']\n",
    "reasoning_calls = stats['reasoning_model_calls']\n",
    "avg_fast_latency = 1.0  # Estimate: 1s for fast model\n",
    "avg_reasoning_latency = 3.0  # Estimate: 3s for reasoning model\n",
    "\n",
    "pipeline_latency = stats['total_latency']\n",
    "always_reasoning_latency = (fast_calls + reasoning_calls) * avg_reasoning_latency\n",
    "\n",
    "print(f\"\\n{'Metric':<30} {'Pipeline':<15} {'Always-Reasoning':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Fast model calls':<30} {fast_calls:<15} {0:<15}\")\n",
    "print(f\"{'Reasoning model calls':<30} {reasoning_calls:<15} {fast_calls + reasoning_calls:<15}\")\n",
    "print(f\"{'Est. total latency':<30} {pipeline_latency:<15.1f}s {always_reasoning_latency:<15.1f}s\")\n",
    "\n",
    "savings = always_reasoning_latency - pipeline_latency\n",
    "savings_pct = savings / always_reasoning_latency * 100 if always_reasoning_latency > 0 else 0\n",
    "\n",
    "print(f\"\\nEstimated savings: {savings:.1f}s ({savings_pct:.0f}%)\")\n",
    "print(\"\\nNote: Actual savings depend on your specific query mix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Production Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_tips = \"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════╗\n",
    "║             PRODUCTION DEPLOYMENT TIPS                               ║\n",
    "╠══════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                      ║\n",
    "║  1. CLASSIFIER IMPROVEMENTS:                                         ║\n",
    "║     - Train a small ML classifier on labeled examples                ║\n",
    "║     - Use an LLM to classify (costs 1 extra call, but more accurate) ║\n",
    "║     - A/B test different routing thresholds                          ║\n",
    "║                                                                      ║\n",
    "║  2. CACHING STRATEGIES:                                              ║\n",
    "║     - Use Redis/Memcached for distributed caching                    ║\n",
    "║     - Implement semantic similarity for cache lookup                 ║\n",
    "║     - Set TTL (time-to-live) for stale data                         ║\n",
    "║     - Cache by embedding similarity, not exact match                 ║\n",
    "║                                                                      ║\n",
    "║  3. MONITORING:                                                      ║\n",
    "║     - Track latency percentiles (p50, p95, p99)                      ║\n",
    "║     - Monitor model accuracy by complexity level                     ║\n",
    "║     - Alert on cache hit rate drops                                  ║\n",
    "║     - Log routing decisions for analysis                             ║\n",
    "║                                                                      ║\n",
    "║  4. FALLBACK STRATEGIES:                                             ║\n",
    "║     - If reasoning model is slow, timeout and use fast model         ║\n",
    "║     - Implement circuit breakers for model failures                  ║\n",
    "║     - Have a \"confident answer\" threshold before responding          ║\n",
    "║                                                                      ║\n",
    "║  5. DGX SPARK OPTIMIZATION:                                          ║\n",
    "║     - Keep both models in memory (128GB is enough!)                  ║\n",
    "║     - Use NVFP4 for reasoning model if available                     ║\n",
    "║     - Batch similar queries together                                 ║\n",
    "║     - Use tensor parallelism for 70B+ models                         ║\n",
    "║                                                                      ║\n",
    "╚══════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\"\n",
    "\n",
    "print(production_tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Updating the Cache for Updated Models\n",
    "\n",
    "```python\n",
    "# Wrong: Cache persists old responses after model update\n",
    "pipeline.update_model(\"new-model-v2\")\n",
    "# Old cached responses from v1 are still returned!\n",
    "\n",
    "# Right: Clear cache when changing models\n",
    "pipeline.update_model(\"new-model-v2\")\n",
    "pipeline.cache.clear()\n",
    "```\n",
    "\n",
    "### Mistake 2: Over-Routing to Reasoning Model\n",
    "\n",
    "```python\n",
    "# Wrong: Classifier is too aggressive\n",
    "if any_math_word_present(query):  # \"one\", \"two\" trigger reasoning\n",
    "    use_reasoning_model()\n",
    "\n",
    "# Right: More nuanced classification\n",
    "if requires_calculation(query) and multi_step(query):\n",
    "    use_reasoning_model()\n",
    "```\n",
    "\n",
    "### Mistake 3: Not Handling Edge Cases\n",
    "\n",
    "```python\n",
    "# Wrong: No fallback\n",
    "response = reasoning_model.generate(query, timeout=30)\n",
    "# If timeout, user gets nothing!\n",
    "\n",
    "# Right: Graceful fallback\n",
    "try:\n",
    "    response = reasoning_model.generate(query, timeout=30)\n",
    "except TimeoutError:\n",
    "    response = fast_model.generate(query)  # Fallback\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ✅ How to classify query complexity\n",
    "- ✅ How to route to appropriate models\n",
    "- ✅ How to implement response caching\n",
    "- ✅ How to measure pipeline performance\n",
    "- ✅ Production deployment considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*70)\n",
    "print(\"MODULE 3.4 COMPLETE: TEST-TIME COMPUTE & REASONING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "You've mastered:\n",
    "\n",
    "1. Chain-of-Thought Prompting\n",
    "   - Zero-shot and few-shot CoT\n",
    "   - When and why it improves accuracy\n",
    "\n",
    "2. Self-Consistency\n",
    "   - Multiple reasoning paths + majority voting\n",
    "   - Temperature and N tuning\n",
    "\n",
    "3. Reasoning Models (DeepSeek-R1)\n",
    "   - <think> tokens and GRPO training\n",
    "   - Running 70B models on DGX Spark\n",
    "\n",
    "4. Model Comparison\n",
    "   - Quantifying reasoning advantages\n",
    "   - Token economy analysis\n",
    "\n",
    "5. Reward Models & Best-of-N\n",
    "   - Scoring responses for quality\n",
    "   - Generate N, pick best strategy\n",
    "\n",
    "6. Adaptive Reasoning Pipeline\n",
    "   - Complexity classification\n",
    "   - Intelligent routing\n",
    "   - Response caching\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(\"Next: Module 3.5 - RAG Systems & Vector Databases\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"\\nMemory cleaned up. Great work!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}