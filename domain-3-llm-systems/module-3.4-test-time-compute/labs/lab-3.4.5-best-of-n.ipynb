{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.4.5: Best-of-N with Reward Model\n",
    "\n",
    "**Module:** 3.4 - Test-Time Compute & Reasoning  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐⭐⭐ (Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- [ ] Understand what reward models are and how they score responses\n",
    "- [ ] Implement Best-of-N sampling (generate N, pick best)\n",
    "- [ ] Load and use real reward models (ArmoRM, Skywork-Reward)\n",
    "- [ ] Measure quality improvement vs greedy decoding\n",
    "- [ ] Understand the cost-quality tradeoff\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Labs 3.4.1-3.4.4\n",
    "- Ollama running with an LLM\n",
    "- PyTorch and Transformers installed\n",
    "- DGX Spark (128GB) for running LLM + reward model together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "**The Problem:** LLMs can generate many valid responses to a prompt, but some are better than others. How do we automatically select the best one?\n",
    "\n",
    "**The Solution:** Use a **Reward Model** - a separate model trained to score how \"good\" a response is, based on human preferences.\n",
    "\n",
    "**Best-of-N (BoN) Sampling:**\n",
    "1. Generate N different responses (with temperature > 0)\n",
    "2. Score each with the reward model\n",
    "3. Return the highest-scoring response\n",
    "\n",
    "**Industry Applications:**\n",
    "- **ChatGPT/Claude:** Uses reward models to rank responses before showing you\n",
    "- **Code Generation:** Pick the most likely correct solution\n",
    "- **Content Moderation:** Score safety/helpfulness\n",
    "- **Customer Service:** Choose most helpful response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: Best-of-N with Reward Models\n",
    "\n",
    "> **Imagine you're a restaurant owner...**\n",
    ">\n",
    "> You have a chef (the LLM) who can make 5 versions of a dish.\n",
    "> Each version is slightly different.\n",
    ">\n",
    "> You also have a food critic (the reward model) who tastes each dish\n",
    "> and gives it a score from 1-10.\n",
    ">\n",
    "> **Best-of-N:** The chef makes 5 dishes, the critic scores them all,\n",
    "> and you serve the highest-scoring one to your customer!\n",
    ">\n",
    "> **The magic:** The critic has tasted millions of dishes and learned\n",
    "> what humans prefer. Even if the chef sometimes makes a mediocre dish,\n",
    "> the critic helps ensure customers get the best one.\n",
    "\n",
    "```\n",
    "Greedy:     Prompt ──> Generate 1 ──> Serve\n",
    "                       (hope it's good!)\n",
    "\n",
    "Best-of-N:  Prompt ──> Generate N ──> Score Each ──> Serve Best\n",
    "                       [A, B, C, D, E]   [7, 9, 6, 8, 5]   [B wins!]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import ollama\n",
    "\n",
    "# Check for transformers (needed for reward models)\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    HAS_TRANSFORMERS = True\n",
    "except ImportError as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "    print(\"Install with: pip install torch transformers\")\n",
    "    HAS_TRANSFORMERS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure models\n",
    "LLM_MODEL = \"qwen3:8b\"  # Your LLM for generation\n",
    "\n",
    "# Reward model options (from HuggingFace)\n",
    "REWARD_MODELS = {\n",
    "    'armo': 'RLHFlow/ArmoRM-Llama3-8B-v0.1',  # ~16GB, general purpose\n",
    "    'skywork': 'Skywork/Skywork-Reward-Llama-3.1-8B-v0.2',  # ~16GB\n",
    "    'internlm': 'internlm/internlm2-7b-reward',  # ~14GB\n",
    "}\n",
    "\n",
    "# For DGX Spark: You can run LLM (70B, ~45GB) + Reward (8B, ~16GB) together!\n",
    "print(f\"LLM for generation: {LLM_MODEL}\")\n",
    "print(f\"\\nAvailable reward models:\")\n",
    "for name, model_id in REWARD_MODELS.items():\n",
    "    print(f\"  {name}: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Building a Simple Reward Model Wrapper\n",
    "\n",
    "First, let's create a simple scoring function before loading a real reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScoredResponse:\n",
    "    \"\"\"A response with its reward score.\"\"\"\n",
    "    response: str\n",
    "    score: float\n",
    "    generation_time: float\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ScoredResponse(score={self.score:.3f}, len={len(self.response)})\"\n",
    "\n",
    "\n",
    "class SimpleRewardModel:\n",
    "    \"\"\"\n",
    "    A simple heuristic reward model for demonstration.\n",
    "    \n",
    "    This is NOT a real reward model - it uses simple heuristics.\n",
    "    Replace with a real model for production use.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"simple_heuristic\"\n",
    "    \n",
    "    def score(self, prompt: str, response: str) -> float:\n",
    "        \"\"\"\n",
    "        Score a response using simple heuristics.\n",
    "        \n",
    "        Higher scores are better. Range: [0, 1]\n",
    "        \"\"\"\n",
    "        score = 0.5  # Start at neutral\n",
    "        \n",
    "        # Reward: Appropriate length (not too short, not too long)\n",
    "        length = len(response)\n",
    "        if 50 < length < 500:\n",
    "            score += 0.1\n",
    "        elif length < 20:\n",
    "            score -= 0.2\n",
    "        \n",
    "        # Reward: Contains reasoning markers\n",
    "        reasoning_markers = ['because', 'therefore', 'first', 'then', 'finally', 'step']\n",
    "        for marker in reasoning_markers:\n",
    "            if marker in response.lower():\n",
    "                score += 0.05\n",
    "        \n",
    "        # Reward: Has a clear answer\n",
    "        if any(x in response.lower() for x in ['the answer is', 'answer:', '=']):\n",
    "            score += 0.1\n",
    "        \n",
    "        # Penalty: Repetition\n",
    "        words = response.lower().split()\n",
    "        unique_ratio = len(set(words)) / max(len(words), 1)\n",
    "        if unique_ratio < 0.5:\n",
    "            score -= 0.2\n",
    "        \n",
    "        # Penalty: Uncertainty markers\n",
    "        if any(x in response.lower() for x in [\"i'm not sure\", \"i don't know\", \"might be\"]):\n",
    "            score -= 0.1\n",
    "        \n",
    "        return max(0.0, min(1.0, score))  # Clamp to [0, 1]\n",
    "\n",
    "\n",
    "# Test simple reward model\n",
    "simple_rm = SimpleRewardModel()\n",
    "\n",
    "test_responses = [\n",
    "    \"42\",  # Too short\n",
    "    \"The answer is 42 because we need to calculate the sum first, then multiply.\",  # Good\n",
    "    \"I'm not sure but it might be around 42 or so.\",  # Uncertain\n",
    "    \"First, let's break this down step by step. The answer is 42.\",  # Great\n",
    "]\n",
    "\n",
    "print(\"Testing simple reward model:\")\n",
    "for resp in test_responses:\n",
    "    score = simple_rm.score(\"What is 6*7?\", resp)\n",
    "    print(f\"  Score {score:.2f}: {resp[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Loading a Real Reward Model\n",
    "\n",
    "Now let's load a real reward model from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceRewardModel:\n",
    "    \"\"\"\n",
    "    Wrapper for HuggingFace reward models.\n",
    "    \n",
    "    Supports models like ArmoRM and Skywork-Reward.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, device: str = \"auto\"):\n",
    "        \"\"\"\n",
    "        Load a reward model from HuggingFace.\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model ID\n",
    "            device: Device to use (\"auto\", \"cuda\", \"cpu\")\n",
    "        \"\"\"\n",
    "        print(f\"Loading reward model: {model_name}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.name = model_name\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Load model\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=device,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        self.model.eval()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Loaded in {elapsed:.1f}s\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            mem_used = torch.cuda.memory_allocated() / 1e9\n",
    "            print(f\"GPU memory used: {mem_used:.1f} GB\")\n",
    "    \n",
    "    def score(self, prompt: str, response: str, max_length: int = 2048) -> float:\n",
    "        \"\"\"\n",
    "        Score a response given a prompt.\n",
    "        \n",
    "        Returns a float score (higher is better).\n",
    "        \"\"\"\n",
    "        # Format as conversation\n",
    "        conversation = f\"User: {prompt}\\n\\nAssistant: {response}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            conversation,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=True,\n",
    "        )\n",
    "        \n",
    "        # Move to model's device\n",
    "        device = next(self.model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Get score\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            \n",
    "            if hasattr(outputs, 'logits'):\n",
    "                score = outputs.logits.squeeze().item()\n",
    "            elif hasattr(outputs, 'score'):\n",
    "                score = outputs.score.squeeze().item()\n",
    "            else:\n",
    "                # Fallback\n",
    "                score = outputs.last_hidden_state[:, -1, :].mean().item()\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def score_batch(self, prompt: str, responses: List[str], batch_size: int = 4) -> List[float]:\n",
    "        \"\"\"\n",
    "        Score multiple responses efficiently.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        \n",
    "        for i in range(0, len(responses), batch_size):\n",
    "            batch = responses[i:i + batch_size]\n",
    "            \n",
    "            # Format conversations\n",
    "            conversations = [\n",
    "                f\"User: {prompt}\\n\\nAssistant: {resp}\"\n",
    "                for resp in batch\n",
    "            ]\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                conversations,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=2048,\n",
    "                padding=True,\n",
    "            )\n",
    "            \n",
    "            device = next(self.model.parameters()).device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                batch_scores = outputs.logits.squeeze(-1).tolist()\n",
    "                \n",
    "                if isinstance(batch_scores, (int, float)):\n",
    "                    batch_scores = [batch_scores]\n",
    "                \n",
    "                scores.extend(batch_scores)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real reward model if transformers is available\n",
    "if HAS_TRANSFORMERS:\n",
    "    print(\"Loading ArmoRM reward model...\")\n",
    "    print(\"(This may take a minute on first run - model needs to download)\\n\")\n",
    "    \n",
    "    try:\n",
    "        reward_model = HuggingFaceRewardModel(REWARD_MODELS['armo'])\n",
    "        USE_REAL_RM = True\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load real reward model: {e}\")\n",
    "        print(\"Using simple heuristic reward model instead.\")\n",
    "        reward_model = SimpleRewardModel()\n",
    "        USE_REAL_RM = False\n",
    "else:\n",
    "    print(\"Using simple heuristic reward model (transformers not available)\")\n",
    "    reward_model = SimpleRewardModel()\n",
    "    USE_REAL_RM = False\n",
    "\n",
    "print(f\"\\nReward model: {reward_model.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the reward model\n",
    "test_prompt = \"Explain what machine learning is.\"\n",
    "test_responses = [\n",
    "    \"Machine learning is a type of AI.\",  # Too short\n",
    "    \"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing algorithms that can access data and use it to learn for themselves.\",  # Good\n",
    "    \"I don't know much about machine learning, but I think it's something to do with computers maybe?\",  # Uncertain\n",
    "]\n",
    "\n",
    "print(f\"Testing {reward_model.name}:\")\n",
    "print(f\"Prompt: {test_prompt}\\n\")\n",
    "\n",
    "for i, resp in enumerate(test_responses):\n",
    "    score = reward_model.score(test_prompt, resp)\n",
    "    print(f\"Response {i+1} (score: {score:.3f}):\")\n",
    "    print(f\"  {resp[:80]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Implementing Best-of-N Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates(\n",
    "    prompt: str,\n",
    "    n: int = 5,\n",
    "    model: str = LLM_MODEL,\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: int = 512,\n",
    "    verbose: bool = True,\n",
    ") -> List[ScoredResponse]:\n",
    "    \"\"\"\n",
    "    Generate N candidate responses.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The input prompt\n",
    "        n: Number of candidates to generate\n",
    "        model: LLM model to use\n",
    "        temperature: Sampling temperature (>0 for diversity)\n",
    "        max_tokens: Maximum tokens per response\n",
    "        verbose: Whether to print progress\n",
    "    \n",
    "    Returns:\n",
    "        List of ScoredResponse objects (scores not yet filled)\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        if verbose:\n",
    "            print(f\"  Generating candidate {i+1}/{n}...\", end=\" \", flush=True)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            options={\"temperature\": temperature, \"num_predict\": max_tokens}\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        response_text = response['message']['content']\n",
    "        \n",
    "        candidates.append(ScoredResponse(\n",
    "            response=response_text,\n",
    "            score=0.0,  # Will be filled by reward model\n",
    "            generation_time=elapsed,\n",
    "        ))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"({elapsed:.1f}s, {len(response_text)} chars)\")\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "\n",
    "def best_of_n(\n",
    "    prompt: str,\n",
    "    reward_model,\n",
    "    n: int = 5,\n",
    "    llm_model: str = LLM_MODEL,\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: int = 512,\n",
    "    verbose: bool = True,\n",
    ") -> Tuple[ScoredResponse, List[ScoredResponse]]:\n",
    "    \"\"\"\n",
    "    Best-of-N sampling: generate N candidates and return the best.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input prompt\n",
    "        reward_model: Reward model for scoring\n",
    "        n: Number of candidates\n",
    "        llm_model: LLM for generation\n",
    "        temperature: Sampling temperature\n",
    "        max_tokens: Max tokens per response\n",
    "        verbose: Whether to print progress\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (best_response, all_candidates)\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\nBest-of-{n} Sampling\")\n",
    "        print(f\"Prompt: {prompt[:60]}...\")\n",
    "        print()\n",
    "    \n",
    "    # Generate candidates\n",
    "    if verbose:\n",
    "        print(\"Generating candidates:\")\n",
    "    candidates = generate_candidates(\n",
    "        prompt, n, llm_model, temperature, max_tokens, verbose\n",
    "    )\n",
    "    \n",
    "    # Score candidates\n",
    "    if verbose:\n",
    "        print(\"\\nScoring candidates:\")\n",
    "    \n",
    "    for i, candidate in enumerate(candidates):\n",
    "        score = reward_model.score(prompt, candidate.response)\n",
    "        candidate.score = score\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Candidate {i+1}: score = {score:.3f}\")\n",
    "    \n",
    "    # Find best\n",
    "    best = max(candidates, key=lambda x: x.score)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nBest candidate: score = {best.score:.3f}\")\n",
    "        print(f\"Score range: [{min(c.score for c in candidates):.3f}, {max(c.score for c in candidates):.3f}]\")\n",
    "    \n",
    "    return best, candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Best-of-N\n",
    "test_prompt = \"What are three benefits of regular exercise? Explain each briefly.\"\n",
    "\n",
    "best, all_candidates = best_of_n(\n",
    "    test_prompt,\n",
    "    reward_model,\n",
    "    n=5,\n",
    "    temperature=0.7,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST RESPONSE:\")\n",
    "print(\"=\"*60)\n",
    "print(best.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Comparing Best-of-N vs Greedy Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(prompt: str, model: str = LLM_MODEL, max_tokens: int = 512) -> ScoredResponse:\n",
    "    \"\"\"\n",
    "    Generate a single response with temperature=0 (greedy decoding).\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        options={\"temperature\": 0.0, \"num_predict\": max_tokens}\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    return ScoredResponse(\n",
    "        response=response['message']['content'],\n",
    "        score=0.0,  # Will be filled\n",
    "        generation_time=elapsed,\n",
    "    )\n",
    "\n",
    "\n",
    "def compare_greedy_vs_bon(\n",
    "    prompts: List[str],\n",
    "    reward_model,\n",
    "    n: int = 5,\n",
    "    verbose: bool = True,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Compare greedy decoding vs Best-of-N on a set of prompts.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'greedy_scores': [],\n",
    "        'bon_scores': [],\n",
    "        'greedy_times': [],\n",
    "        'bon_times': [],\n",
    "        'improvements': [],\n",
    "    }\n",
    "    \n",
    "    for i, prompt in enumerate(prompts):\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Prompt {i+1}/{len(prompts)}: {prompt[:50]}...\")\n",
    "            print('='*60)\n",
    "        \n",
    "        # Greedy decoding\n",
    "        if verbose:\n",
    "            print(\"\\n[Greedy Decoding]\")\n",
    "        greedy_start = time.time()\n",
    "        greedy_response = greedy_decode(prompt)\n",
    "        greedy_response.score = reward_model.score(prompt, greedy_response.response)\n",
    "        greedy_time = time.time() - greedy_start\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Score: {greedy_response.score:.3f}\")\n",
    "            print(f\"  Time: {greedy_time:.1f}s\")\n",
    "        \n",
    "        # Best-of-N\n",
    "        if verbose:\n",
    "            print(f\"\\n[Best-of-{n}]\")\n",
    "        bon_start = time.time()\n",
    "        best, _ = best_of_n(\n",
    "            prompt, reward_model, n=n,\n",
    "            temperature=0.7, verbose=False\n",
    "        )\n",
    "        bon_time = time.time() - bon_start\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Score: {best.score:.3f}\")\n",
    "            print(f\"  Time: {bon_time:.1f}s\")\n",
    "        \n",
    "        # Record results\n",
    "        results['greedy_scores'].append(greedy_response.score)\n",
    "        results['bon_scores'].append(best.score)\n",
    "        results['greedy_times'].append(greedy_time)\n",
    "        results['bon_times'].append(bon_time)\n",
    "        results['improvements'].append(best.score - greedy_response.score)\n",
    "        \n",
    "        if verbose:\n",
    "            improvement = best.score - greedy_response.score\n",
    "            print(f\"\\n  Improvement: {improvement:+.3f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"What is the difference between a list and a tuple in Python?\",\n",
    "    \"Explain how photosynthesis works in simple terms.\",\n",
    "    \"What are the main causes of climate change?\",\n",
    "    \"Describe the process of making coffee.\",\n",
    "]\n",
    "\n",
    "# Run comparison\n",
    "comparison_results = compare_greedy_vs_bon(\n",
    "    test_prompts,\n",
    "    reward_model,\n",
    "    n=5,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "avg_greedy_score = sum(comparison_results['greedy_scores']) / len(comparison_results['greedy_scores'])\n",
    "avg_bon_score = sum(comparison_results['bon_scores']) / len(comparison_results['bon_scores'])\n",
    "avg_improvement = sum(comparison_results['improvements']) / len(comparison_results['improvements'])\n",
    "\n",
    "avg_greedy_time = sum(comparison_results['greedy_times']) / len(comparison_results['greedy_times'])\n",
    "avg_bon_time = sum(comparison_results['bon_times']) / len(comparison_results['bon_times'])\n",
    "\n",
    "print(f\"\\n{'Metric':<25} {'Greedy':<15} {'Best-of-5':<15}\")\n",
    "print(\"-\"*55)\n",
    "print(f\"{'Average Score':<25} {avg_greedy_score:<15.3f} {avg_bon_score:<15.3f}\")\n",
    "print(f\"{'Average Time':<25} {avg_greedy_time:<15.1f}s {avg_bon_time:<15.1f}s\")\n",
    "print(\"-\"*55)\n",
    "print(f\"{'Average Improvement':<25} {avg_improvement:+.3f}\")\n",
    "print(f\"{'Time Overhead':<25} {avg_bon_time/avg_greedy_time:.1f}x\")\n",
    "\n",
    "# How often did BoN win?\n",
    "bon_wins = sum(1 for imp in comparison_results['improvements'] if imp > 0)\n",
    "print(f\"\\nBoN outperformed Greedy: {bon_wins}/{len(comparison_results['improvements'])} times ({bon_wins/len(comparison_results['improvements']):.0%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Experimenting with N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_with_n(\n",
    "    prompt: str,\n",
    "    reward_model,\n",
    "    n_values: List[int] = [1, 3, 5, 10],\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Experiment with different values of N.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for n in n_values:\n",
    "        print(f\"Testing N={n}...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        if n == 1:\n",
    "            # Greedy\n",
    "            response = greedy_decode(prompt)\n",
    "            response.score = reward_model.score(prompt, response.response)\n",
    "            best_score = response.score\n",
    "            all_scores = [best_score]\n",
    "        else:\n",
    "            # Best-of-N\n",
    "            best, candidates = best_of_n(\n",
    "                prompt, reward_model, n=n,\n",
    "                verbose=False\n",
    "            )\n",
    "            best_score = best.score\n",
    "            all_scores = [c.score for c in candidates]\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        results[n] = {\n",
    "            'best_score': best_score,\n",
    "            'all_scores': all_scores,\n",
    "            'time': elapsed,\n",
    "        }\n",
    "        \n",
    "        print(f\"  Best score: {best_score:.3f}, Time: {elapsed:.1f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test with different N\n",
    "test_prompt = \"Explain the concept of recursion in programming with an example.\"\n",
    "\n",
    "print(f\"Prompt: {test_prompt}\\n\")\n",
    "n_experiment = experiment_with_n(\n",
    "    test_prompt,\n",
    "    reward_model,\n",
    "    n_values=[1, 3, 5, 10]\n",
    ")\n",
    "\n",
    "# Visualize results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EFFECT OF N ON QUALITY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\n{'N':<10} {'Best Score':<15} {'Time':<15}\")\n",
    "print(\"-\"*40)\n",
    "for n, data in n_experiment.items():\n",
    "    print(f\"{n:<10} {data['best_score']:<15.3f} {data['time']:<15.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight: Diminishing Returns\n",
    "\n",
    "As N increases:\n",
    "- Quality improves (higher best score)\n",
    "- But with diminishing returns\n",
    "- Time/cost increases linearly\n",
    "\n",
    "**Recommendation:**\n",
    "- N=3-5 for most applications (good balance)\n",
    "- N=10+ for high-stakes decisions\n",
    "- N=1 (greedy) when speed matters most"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Using Temperature = 0\n",
    "\n",
    "```python\n",
    "# Wrong: All candidates are identical!\n",
    "best_of_n(prompt, rm, n=5, temperature=0.0)\n",
    "\n",
    "# Right: Use temperature > 0 for diversity\n",
    "best_of_n(prompt, rm, n=5, temperature=0.7)\n",
    "```\n",
    "\n",
    "### Mistake 2: Not Matching Reward Model to Use Case\n",
    "\n",
    "```python\n",
    "# Wrong: Using a general RM for code generation\n",
    "reward_model = load(\"general-chat-rm\")  # Optimized for chat\n",
    "score_code_response(reward_model, code)  # May not score code well\n",
    "\n",
    "# Right: Use domain-specific RM when available\n",
    "reward_model = load(\"code-quality-rm\")  # Optimized for code\n",
    "```\n",
    "\n",
    "### Mistake 3: Running Out of Memory\n",
    "\n",
    "```python\n",
    "# Wrong: Trying to run huge LLM + huge RM\n",
    "llm = load(\"llama-70b\")  # 45GB\n",
    "rm = load(\"reward-70b\")  # 45GB\n",
    "# Total: 90GB - may not fit!\n",
    "\n",
    "# Right: Use smaller RM or quantize\n",
    "llm = load(\"llama-70b\")  # 45GB\n",
    "rm = load(\"armo-8b\")     # 16GB\n",
    "# Total: 61GB - fits on DGX Spark!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ✅ What reward models are and how they score responses\n",
    "- ✅ How to implement Best-of-N sampling\n",
    "- ✅ How to load real reward models from HuggingFace\n",
    "- ✅ The quality improvement over greedy decoding\n",
    "- ✅ The tradeoff between N and quality/cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"Lab 3.4.5 Complete!\")\n",
    "print(f\"\\nReward model used: {reward_model.name}\")\n",
    "print(f\"Average BoN improvement: {avg_improvement:+.3f}\")\n",
    "\n",
    "# Cleanup\n",
    "import gc\n",
    "\n",
    "# Delete reward model to free memory\n",
    "if HAS_TRANSFORMERS and USE_REAL_RM:\n",
    "    del reward_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()\n",
    "print(\"\\nMemory cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Excellent! You've mastered Best-of-N sampling with reward models.\n",
    "\n",
    "In the final lab of this module, you'll build an **Adaptive Reasoning Pipeline** that combines everything:\n",
    "- Routes simple queries to fast models\n",
    "- Routes complex queries to reasoning models\n",
    "- Uses caching for efficiency\n",
    "\n",
    "**Continue to:** [Lab 3.4.6: Reasoning Pipeline](./lab-3.4.6-reasoning-pipeline.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
