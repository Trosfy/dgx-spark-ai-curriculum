{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2.8: TensorRT-LLM Engine\n",
    "\n",
    "## Production-Ready LLM Deployment with NVIDIA TensorRT-LLM\n",
    "\n",
    "**Duration:** 2 hours\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Understand TensorRT-LLM architecture** and why it's essential for production\n",
    "2. **Convert models to TensorRT-LLM engines** with quantization\n",
    "3. **Optimize for DGX Spark Blackwell** using native FP8/FP4 support\n",
    "4. **Configure batching and KV-cache** for maximum throughput\n",
    "5. **Benchmark and compare** TensorRT-LLM vs vanilla transformers\n",
    "\n",
    "---\n",
    "\n",
    "### Why TensorRT-LLM?\n",
    "\n",
    "```\n",
    "Professor SPARK says:\n",
    "\n",
    "\"TensorRT-LLM is like having a Formula 1 pit crew for your AI model.\n",
    "\n",
    "Vanilla PyTorch is a regular car - gets you there, but not optimized.\n",
    "TensorRT-LLM rebuilds your engine with:\n",
    "- Fused operations (fewer pit stops)\n",
    "- Optimized memory (bigger fuel tank)\n",
    "- Native quantization (lighter weight)\n",
    "- Paged KV-cache (smart fuel management)\n",
    "\n",
    "Result? 2-5x faster inference. On Blackwell? Even more with FP8!\"\n",
    "```\n",
    "\n",
    "### TensorRT-LLM Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                    TensorRT-LLM Pipeline                │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│                                                         │\n",
    "│  ┌──────────────┐    ┌──────────────┐    ┌──────────┐  │\n",
    "│  │   HuggingFace │ -> │   Convert    │ -> │  Engine  │  │\n",
    "│  │    Model      │    │   to TRT-LLM │    │  (.engine)│  │\n",
    "│  └──────────────┘    └──────────────┘    └──────────┘  │\n",
    "│                              │                          │\n",
    "│                              v                          │\n",
    "│  ┌────────────────────────────────────────────────────┐│\n",
    "│  │              Optimizations Applied:                ││\n",
    "│  │  • Kernel fusion (attention, MLP, LayerNorm)       ││\n",
    "│  │  • Quantization (INT8, FP8, INT4, FP4)            ││\n",
    "│  │  • Flash Attention 2                               ││\n",
    "│  │  • Paged KV-cache                                  ││\n",
    "│  │  • In-flight batching                              ││\n",
    "│  │  • Tensor parallelism                              ││\n",
    "│  └────────────────────────────────────────────────────┘│\n",
    "│                              │                          │\n",
    "│                              v                          │\n",
    "│  ┌──────────────────────────────────────────────────┐  │\n",
    "│  │        Runtime: Triton Inference Server           │  │\n",
    "│  │        or direct Python API                       │  │\n",
    "│  └──────────────────────────────────────────────────┘  │\n",
    "│                                                         │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Environment Setup\n",
    "\n",
    "### 1.1 Install TensorRT-LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorRT-LLM installation\n",
    "# Note: TensorRT-LLM requires specific CUDA and driver versions\n",
    "# On DGX Spark, it's pre-installed in the AI container\n",
    "\n",
    "# If not installed, use:\n",
    "# !pip install tensorrt-llm\n",
    "\n",
    "# Or pull the official container:\n",
    "# docker pull nvcr.io/nvidia/tensorrt-llm:latest\n",
    "\n",
    "print(\"Checking TensorRT-LLM installation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# Try importing TensorRT-LLM\n",
    "try:\n",
    "    import tensorrt_llm\n",
    "    from tensorrt_llm import LLM, SamplingParams\n",
    "    from tensorrt_llm.quantization import QuantMode\n",
    "    TRT_LLM_AVAILABLE = True\n",
    "    print(f\"TensorRT-LLM version: {tensorrt_llm.__version__}\")\n",
    "except ImportError:\n",
    "    TRT_LLM_AVAILABLE = False\n",
    "    print(\"TensorRT-LLM not available - running in simulation mode\")\n",
    "    print(\"Install with: pip install tensorrt-llm\")\n",
    "\n",
    "# Local utilities\n",
    "sys.path.append('..')\n",
    "from scripts import (\n",
    "    get_gpu_memory,\n",
    "    clear_memory,\n",
    "    MemoryTracker,\n",
    "    print_dgx_spark_status,\n",
    "    benchmark_inference\n",
    ")\n",
    "\n",
    "# Plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"\\nEnvironment setup complete!\")\n",
    "print_dgx_spark_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TRTLLMConfig:\n",
    "    \"\"\"Configuration for TensorRT-LLM engine building.\"\"\"\n",
    "    \n",
    "    # Model settings\n",
    "    model_name: str = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "    \n",
    "    # Engine settings\n",
    "    engine_dir: str = \"../data/trt_engines\"\n",
    "    \n",
    "    # Quantization (for Blackwell)\n",
    "    quantization: str = \"fp8\"  # Options: none, int8, int4, fp8, fp4\n",
    "    \n",
    "    # Batch settings\n",
    "    max_batch_size: int = 8\n",
    "    max_input_len: int = 2048\n",
    "    max_output_len: int = 512\n",
    "    max_beam_width: int = 1\n",
    "    \n",
    "    # KV-cache settings\n",
    "    kv_cache_type: str = \"paged\"  # paged or contiguous\n",
    "    max_num_tokens: int = 8192\n",
    "    \n",
    "    # Hardware settings (DGX Spark)\n",
    "    tp_size: int = 1  # Tensor parallelism (1 for single GPU)\n",
    "    pp_size: int = 1  # Pipeline parallelism\n",
    "    \n",
    "    # Builder settings\n",
    "    builder_opt_level: int = 3  # 0-5, higher = more optimization\n",
    "    use_fused_mlp: bool = True\n",
    "    use_flash_attention: bool = True\n",
    "    \n",
    "    # Benchmark settings\n",
    "    warmup_runs: int = 5\n",
    "    benchmark_runs: int = 50\n",
    "\n",
    "\n",
    "# Create configuration\n",
    "config = TRTLLMConfig()\n",
    "\n",
    "# Ensure directories exist\n",
    "Path(config.engine_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"TensorRT-LLM Configuration:\")\n",
    "print(f\"  Model: {config.model_name}\")\n",
    "print(f\"  Quantization: {config.quantization}\")\n",
    "print(f\"  Max batch size: {config.max_batch_size}\")\n",
    "print(f\"  Max input length: {config.max_input_len}\")\n",
    "print(f\"  Engine directory: {config.engine_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Understanding TensorRT-LLM Optimizations\n",
    "\n",
    "### 2.1 Key Optimizations Explained\n",
    "\n",
    "```\n",
    "Professor SPARK's Optimization Guide:\n",
    "\n",
    "1. KERNEL FUSION\n",
    "   Before: LayerNorm -> Linear -> GELU -> Linear (4 kernel launches)\n",
    "   After:  FusedMLP (1 kernel launch)\n",
    "   Benefit: Less memory traffic, fewer synchronization points\n",
    "\n",
    "2. QUANTIZATION\n",
    "   FP8 on Blackwell: 2x throughput with minimal quality loss\n",
    "   INT4: 4x memory reduction for larger batch sizes\n",
    "\n",
    "3. FLASH ATTENTION\n",
    "   Standard: O(n²) memory for attention\n",
    "   Flash:    O(n) memory, tiled computation\n",
    "   Benefit:  Handle longer sequences, faster\n",
    "\n",
    "4. PAGED KV-CACHE\n",
    "   Like virtual memory for your model\n",
    "   Only allocates cache pages as needed\n",
    "   Benefit: More requests in parallel, less memory waste\n",
    "\n",
    "5. IN-FLIGHT BATCHING\n",
    "   Requests join/leave batch dynamically\n",
    "   No waiting for slowest request\n",
    "   Benefit: Higher GPU utilization\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of optimization impact\n",
    "optimizations = {\n",
    "    'Baseline (PyTorch)': 1.0,\n",
    "    '+ Kernel Fusion': 1.4,\n",
    "    '+ Flash Attention': 1.8,\n",
    "    '+ FP8 Quantization': 2.5,\n",
    "    '+ Paged KV-Cache': 3.0,\n",
    "    '+ In-flight Batching': 4.0,\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "names = list(optimizations.keys())\n",
    "speedups = list(optimizations.values())\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(names)))\n",
    "\n",
    "bars = ax.barh(names, speedups, color=colors, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "# Add value labels\n",
    "for bar, speedup in zip(bars, speedups):\n",
    "    ax.text(bar.get_width() + 0.05, bar.get_y() + bar.get_height()/2,\n",
    "            f'{speedup:.1f}x', va='center', fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Speedup Factor', fontsize=12)\n",
    "ax.set_title('Cumulative TensorRT-LLM Optimizations on DGX Spark', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(0, 5)\n",
    "\n",
    "# Add vertical line at 1x\n",
    "ax.axvline(x=1.0, color='red', linestyle='--', alpha=0.5, label='Baseline')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(config.engine_dir) / 'optimization_impact.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Quantization Modes in TensorRT-LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorRT-LLM Quantization Options\n",
    "quant_modes = {\n",
    "    'none': {\n",
    "        'description': 'FP16/BF16 weights and activations',\n",
    "        'memory_ratio': 1.0,\n",
    "        'speedup': 1.0,\n",
    "        'quality_loss': 'None',\n",
    "        'blackwell_optimized': False\n",
    "    },\n",
    "    'int8_weight_only': {\n",
    "        'description': 'INT8 weights, FP16 compute',\n",
    "        'memory_ratio': 0.5,\n",
    "        'speedup': 1.5,\n",
    "        'quality_loss': 'Minimal',\n",
    "        'blackwell_optimized': False\n",
    "    },\n",
    "    'int8_sq': {\n",
    "        'description': 'SmoothQuant INT8 weights + activations',\n",
    "        'memory_ratio': 0.5,\n",
    "        'speedup': 2.0,\n",
    "        'quality_loss': 'Low',\n",
    "        'blackwell_optimized': True\n",
    "    },\n",
    "    'fp8': {\n",
    "        'description': 'FP8 weights and activations (E4M3)',\n",
    "        'memory_ratio': 0.5,\n",
    "        'speedup': 2.5,\n",
    "        'quality_loss': 'Very Low',\n",
    "        'blackwell_optimized': True\n",
    "    },\n",
    "    'int4_awq': {\n",
    "        'description': 'AWQ 4-bit weight quantization',\n",
    "        'memory_ratio': 0.25,\n",
    "        'speedup': 2.0,\n",
    "        'quality_loss': 'Low-Medium',\n",
    "        'blackwell_optimized': False\n",
    "    },\n",
    "    'int4_gptq': {\n",
    "        'description': 'GPTQ 4-bit weight quantization',\n",
    "        'memory_ratio': 0.25,\n",
    "        'speedup': 2.0,\n",
    "        'quality_loss': 'Low-Medium',\n",
    "        'blackwell_optimized': False\n",
    "    },\n",
    "    'fp4': {\n",
    "        'description': 'NVFP4 weights (Blackwell native)',\n",
    "        'memory_ratio': 0.25,\n",
    "        'speedup': 3.5,\n",
    "        'quality_loss': 'Low',\n",
    "        'blackwell_optimized': True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display as table\n",
    "df_quant = pd.DataFrame(quant_modes).T\n",
    "df_quant.index.name = 'Mode'\n",
    "\n",
    "print(\"TensorRT-LLM Quantization Modes:\")\n",
    "print(\"=\"*80)\n",
    "print(df_quant.to_string())\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nRecommendation for DGX Spark Blackwell: Use 'fp8' or 'fp4' for best performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Building TensorRT-LLM Engine\n",
    "\n",
    "### 3.1 Engine Builder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRTLLMEngineBuilder:\n",
    "    \"\"\"\n",
    "    Helper class for building TensorRT-LLM engines.\n",
    "    \n",
    "    Supports various quantization modes optimized for DGX Spark Blackwell.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: TRTLLMConfig):\n",
    "        self.config = config\n",
    "        self.engine_path = None\n",
    "    \n",
    "    def get_quant_config(self) -> dict:\n",
    "        \"\"\"\n",
    "        Get quantization configuration based on mode.\n",
    "        \"\"\"\n",
    "        quant_configs = {\n",
    "            'none': {},\n",
    "            'int8': {\n",
    "                'quant_mode': 'int8_weight_only'\n",
    "            },\n",
    "            'int8_sq': {\n",
    "                'quant_mode': 'int8_sq',\n",
    "                'per_token': True,\n",
    "                'per_channel': True\n",
    "            },\n",
    "            'fp8': {\n",
    "                'quant_mode': 'fp8',\n",
    "                'fp8_kv_cache': True\n",
    "            },\n",
    "            'int4_awq': {\n",
    "                'quant_mode': 'int4_awq',\n",
    "                'group_size': 128\n",
    "            },\n",
    "            'int4_gptq': {\n",
    "                'quant_mode': 'int4_gptq',\n",
    "                'group_size': 128\n",
    "            },\n",
    "            'fp4': {\n",
    "                'quant_mode': 'fp4',\n",
    "                'fp4_kv_cache': True\n",
    "            }\n",
    "        }\n",
    "        return quant_configs.get(self.config.quantization, {})\n",
    "    \n",
    "    def build_engine(self) -> str:\n",
    "        \"\"\"\n",
    "        Build TensorRT-LLM engine from HuggingFace model.\n",
    "        \n",
    "        Returns:\n",
    "            Path to built engine\n",
    "        \"\"\"\n",
    "        if not TRT_LLM_AVAILABLE:\n",
    "            print(\"TensorRT-LLM not available - returning simulated path\")\n",
    "            return f\"{self.config.engine_dir}/simulated_engine\"\n",
    "        \n",
    "        print(f\"\\nBuilding TensorRT-LLM engine...\")\n",
    "        print(f\"  Model: {self.config.model_name}\")\n",
    "        print(f\"  Quantization: {self.config.quantization}\")\n",
    "        \n",
    "        # Create engine name\n",
    "        model_short = self.config.model_name.split('/')[-1]\n",
    "        engine_name = f\"{model_short}_{self.config.quantization}_tp{self.config.tp_size}\"\n",
    "        engine_path = Path(self.config.engine_dir) / engine_name\n",
    "        \n",
    "        # Build using TensorRT-LLM LLM class\n",
    "        try:\n",
    "            with MemoryTracker() as tracker:\n",
    "                # Get quantization config\n",
    "                quant_config = self.get_quant_config()\n",
    "                \n",
    "                # Build engine (simplified API)\n",
    "                llm = LLM(\n",
    "                    model=self.config.model_name,\n",
    "                    tensor_parallel_size=self.config.tp_size,\n",
    "                    **quant_config\n",
    "                )\n",
    "                \n",
    "                # Save engine\n",
    "                llm.save(str(engine_path))\n",
    "            \n",
    "            print(f\"\\nEngine built successfully!\")\n",
    "            print(f\"  Path: {engine_path}\")\n",
    "            print(f\"  Build time: {tracker.elapsed_time:.1f}s\")\n",
    "            print(f\"  Peak memory: {tracker.peak_memory_gb:.2f} GB\")\n",
    "            \n",
    "            self.engine_path = str(engine_path)\n",
    "            return self.engine_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Engine build failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def build_engine_cli(self) -> str:\n",
    "        \"\"\"\n",
    "        Build engine using CLI tools (alternative method).\n",
    "        \n",
    "        Useful for more control over the build process.\n",
    "        \"\"\"\n",
    "        print(\"\\nBuilding engine using CLI tools...\")\n",
    "        \n",
    "        model_short = self.config.model_name.split('/')[-1]\n",
    "        checkpoint_dir = Path(self.config.engine_dir) / f\"{model_short}_checkpoint\"\n",
    "        engine_dir = Path(self.config.engine_dir) / f\"{model_short}_{self.config.quantization}_engine\"\n",
    "        \n",
    "        # Step 1: Convert checkpoint\n",
    "        convert_cmd = f\"\"\"\n",
    "python -m tensorrt_llm.commands.convert_checkpoint \\\n",
    "    --model_dir {self.config.model_name} \\\n",
    "    --output_dir {checkpoint_dir} \\\n",
    "    --dtype float16 \\\n",
    "    --tp_size {self.config.tp_size}\n",
    "\"\"\"\n",
    "        \n",
    "        # Add quantization flags\n",
    "        if self.config.quantization == 'fp8':\n",
    "            convert_cmd += \" --use_fp8\"\n",
    "        elif self.config.quantization == 'int8':\n",
    "            convert_cmd += \" --int8_kv_cache\"\n",
    "        elif self.config.quantization == 'int4_awq':\n",
    "            convert_cmd += \" --use_weight_only --weight_only_precision int4_awq\"\n",
    "        \n",
    "        print(f\"Convert command:\\n{convert_cmd}\")\n",
    "        \n",
    "        # Step 2: Build engine\n",
    "        build_cmd = f\"\"\"\n",
    "trtllm-build \\\n",
    "    --checkpoint_dir {checkpoint_dir} \\\n",
    "    --output_dir {engine_dir} \\\n",
    "    --max_batch_size {self.config.max_batch_size} \\\n",
    "    --max_input_len {self.config.max_input_len} \\\n",
    "    --max_seq_len {self.config.max_input_len + self.config.max_output_len} \\\n",
    "    --gemm_plugin float16 \\\n",
    "    --gpt_attention_plugin float16 \\\n",
    "    --paged_kv_cache enable \\\n",
    "    --remove_input_padding enable\n",
    "\"\"\"\n",
    "        \n",
    "        print(f\"\\nBuild command:\\n{build_cmd}\")\n",
    "        \n",
    "        # Return path (actual execution would be done in terminal)\n",
    "        self.engine_path = str(engine_dir)\n",
    "        return self.engine_path\n",
    "\n",
    "\n",
    "# Create builder\n",
    "builder = TRTLLMEngineBuilder(config)\n",
    "print(\"\\nEngine builder ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Build Engine (Demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate CLI commands for building engine\n",
    "print(\"TensorRT-LLM Engine Build Commands:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "engine_path = builder.build_engine_cli()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nTo build the engine, run these commands in your terminal.\")\n",
    "print(\"The build process can take 10-30 minutes depending on model size.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Build with Different Quantization Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_build_commands(model_name: str, output_base: str) -> dict:\n",
    "    \"\"\"\n",
    "    Generate build commands for all quantization modes.\n",
    "    \"\"\"\n",
    "    modes = ['fp16', 'int8', 'fp8', 'int4_awq', 'fp4']\n",
    "    commands = {}\n",
    "    \n",
    "    for mode in modes:\n",
    "        checkpoint_dir = f\"{output_base}/{mode}_checkpoint\"\n",
    "        engine_dir = f\"{output_base}/{mode}_engine\"\n",
    "        \n",
    "        # Convert checkpoint command\n",
    "        convert_cmd = f\"python -m tensorrt_llm.commands.convert_checkpoint --model_dir {model_name} --output_dir {checkpoint_dir} --dtype float16\"\n",
    "        \n",
    "        if mode == 'fp8':\n",
    "            convert_cmd += \" --use_fp8\"\n",
    "        elif mode == 'int8':\n",
    "            convert_cmd += \" --int8_kv_cache\"\n",
    "        elif mode == 'int4_awq':\n",
    "            convert_cmd += \" --use_weight_only --weight_only_precision int4_awq\"\n",
    "        elif mode == 'fp4':\n",
    "            convert_cmd += \" --use_fp4\"  # Blackwell native\n",
    "        \n",
    "        # Build engine command\n",
    "        build_cmd = f\"trtllm-build --checkpoint_dir {checkpoint_dir} --output_dir {engine_dir} --max_batch_size 8 --max_input_len 2048 --max_seq_len 2560 --gemm_plugin float16 --gpt_attention_plugin float16 --paged_kv_cache enable\"\n",
    "        \n",
    "        commands[mode] = {\n",
    "            'convert': convert_cmd,\n",
    "            'build': build_cmd\n",
    "        }\n",
    "    \n",
    "    return commands\n",
    "\n",
    "# Generate commands for all modes\n",
    "all_commands = generate_build_commands(config.model_name, config.engine_dir)\n",
    "\n",
    "print(\"Build Commands for All Quantization Modes:\")\n",
    "print(\"=\"*60)\n",
    "for mode, cmds in all_commands.items():\n",
    "    print(f\"\\n### {mode.upper()} ###\")\n",
    "    print(f\"1. Convert: {cmds['convert'][:80]}...\")\n",
    "    print(f\"2. Build: {cmds['build'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Running Inference with TensorRT-LLM\n",
    "\n",
    "### 4.1 TensorRT-LLM Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRTLLMInference:\n",
    "    \"\"\"\n",
    "    TensorRT-LLM inference wrapper.\n",
    "    \n",
    "    Provides easy-to-use interface for running inference.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, engine_dir: str):\n",
    "        self.engine_dir = engine_dir\n",
    "        self.llm = None\n",
    "        self.loaded = False\n",
    "    \n",
    "    def load(self):\n",
    "        \"\"\"Load the TensorRT-LLM engine.\"\"\"\n",
    "        if not TRT_LLM_AVAILABLE:\n",
    "            print(\"TensorRT-LLM not available - using simulation mode\")\n",
    "            self.loaded = False\n",
    "            return\n",
    "        \n",
    "        print(f\"Loading engine from {self.engine_dir}...\")\n",
    "        \n",
    "        try:\n",
    "            self.llm = LLM(model=self.engine_dir)\n",
    "            self.loaded = True\n",
    "            print(\"Engine loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load engine: {e}\")\n",
    "            self.loaded = False\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        max_tokens: int = 100,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.9\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate text from prompts.\n",
    "        \n",
    "        Args:\n",
    "            prompts: List of input prompts\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            temperature: Sampling temperature\n",
    "            top_p: Nucleus sampling parameter\n",
    "            \n",
    "        Returns:\n",
    "            List of generated texts\n",
    "        \"\"\"\n",
    "        if not self.loaded:\n",
    "            # Simulation mode\n",
    "            return [f\"[Simulated output for: {p[:50]}...]\" for p in prompts]\n",
    "        \n",
    "        sampling_params = SamplingParams(\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate(prompts, sampling_params)\n",
    "        \n",
    "        return [output.outputs[0].text for output in outputs]\n",
    "    \n",
    "    def benchmark(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        max_tokens: int = 100,\n",
    "        num_runs: int = 50,\n",
    "        warmup_runs: int = 5,\n",
    "        batch_sizes: List[int] = [1, 2, 4, 8]\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Benchmark inference performance.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with benchmark results\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            prompts = [prompt] * batch_size\n",
    "            \n",
    "            # Warmup\n",
    "            for _ in range(warmup_runs):\n",
    "                self.generate(prompts, max_tokens=max_tokens)\n",
    "            \n",
    "            # Benchmark\n",
    "            latencies = []\n",
    "            for _ in range(num_runs):\n",
    "                start = time.perf_counter()\n",
    "                outputs = self.generate(prompts, max_tokens=max_tokens)\n",
    "                latency = (time.perf_counter() - start) * 1000  # ms\n",
    "                latencies.append(latency)\n",
    "            \n",
    "            mean_latency = np.mean(latencies)\n",
    "            std_latency = np.std(latencies)\n",
    "            tokens_per_sec = (batch_size * max_tokens) / (mean_latency / 1000)\n",
    "            \n",
    "            results[batch_size] = {\n",
    "                'mean_latency_ms': mean_latency,\n",
    "                'std_latency_ms': std_latency,\n",
    "                'tokens_per_second': tokens_per_sec,\n",
    "                'throughput_requests_per_sec': batch_size / (mean_latency / 1000)\n",
    "            }\n",
    "            \n",
    "            print(f\"Batch {batch_size}: {mean_latency:.1f}ms, {tokens_per_sec:.0f} tok/s\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# Note: Actual loading requires built engine\n",
    "print(\"TensorRT-LLM inference class ready!\")\n",
    "print(\"(Engine loading requires pre-built engine files)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Compare with Vanilla Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_inference_methods(\n",
    "    model_name: str,\n",
    "    prompt: str = \"The future of artificial intelligence is\",\n",
    "    max_tokens: int = 50,\n",
    "    num_runs: int = 20\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare vanilla transformers vs TensorRT-LLM performance.\n",
    "    \n",
    "    Note: For demonstration, uses simulated TRT-LLM results.\n",
    "    \"\"\"\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Vanilla Transformers FP16\n",
    "    print(\"Benchmarking vanilla Transformers (FP16)...\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        # Warmup\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        for _ in range(3):\n",
    "            model.generate(**inputs, max_new_tokens=max_tokens, do_sample=False)\n",
    "        \n",
    "        # Benchmark\n",
    "        latencies = []\n",
    "        for _ in tqdm(range(num_runs), desc=\"Transformers\"):\n",
    "            start = time.perf_counter()\n",
    "            outputs = model.generate(**inputs, max_new_tokens=max_tokens, do_sample=False)\n",
    "            latency = (time.perf_counter() - start) * 1000\n",
    "            latencies.append(latency)\n",
    "        \n",
    "        mean_lat = np.mean(latencies)\n",
    "        tps = max_tokens / (mean_lat / 1000)\n",
    "        \n",
    "        results.append({\n",
    "            'Method': 'Transformers FP16',\n",
    "            'Latency (ms)': mean_lat,\n",
    "            'Tokens/sec': tps,\n",
    "            'Memory (GB)': get_gpu_memory().get('used_gb', 0),\n",
    "            'Speedup': 1.0\n",
    "        })\n",
    "        \n",
    "        baseline_lat = mean_lat\n",
    "        \n",
    "        del model\n",
    "        clear_memory()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Transformers benchmark failed: {e}\")\n",
    "        baseline_lat = 100  # Fallback\n",
    "    \n",
    "    # Simulated TensorRT-LLM results (based on typical improvements)\n",
    "    trt_configs = [\n",
    "        ('TRT-LLM FP16', 1.0, 0.5, 0.85),\n",
    "        ('TRT-LLM INT8', 0.75, 0.6, 0.55),\n",
    "        ('TRT-LLM FP8', 0.55, 0.7, 0.50),\n",
    "        ('TRT-LLM INT4-AWQ', 0.65, 0.55, 0.35),\n",
    "        ('TRT-LLM FP4 (Blackwell)', 0.40, 0.85, 0.30),\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nSimulated TensorRT-LLM results (typical improvements):\")\n",
    "    for name, lat_factor, tps_factor, mem_factor in trt_configs:\n",
    "        # Apply typical TRT-LLM improvement factors\n",
    "        lat = baseline_lat * lat_factor\n",
    "        tps = (max_tokens / (lat / 1000)) * tps_factor * 2  # TRT-LLM typically 2x+ faster\n",
    "        mem = results[0]['Memory (GB)'] * mem_factor if results else 3.0 * mem_factor\n",
    "        \n",
    "        results.append({\n",
    "            'Method': name,\n",
    "            'Latency (ms)': lat,\n",
    "            'Tokens/sec': tps,\n",
    "            'Memory (GB)': mem,\n",
    "            'Speedup': baseline_lat / lat if results else 1.0 / lat_factor\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Run comparison\n",
    "print(\"Running inference method comparison...\")\n",
    "comparison_df = compare_inference_methods(config.model_name)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Inference Method Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Visualize Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inference_comparison(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Visualize inference performance comparison.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Throughput comparison\n",
    "    ax1 = axes[0]\n",
    "    colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(df)))\n",
    "    bars = ax1.bar(df['Method'], df['Tokens/sec'], color=colors, edgecolor='black')\n",
    "    ax1.set_xlabel('Method', fontsize=10)\n",
    "    ax1.set_ylabel('Tokens/second', fontsize=10)\n",
    "    ax1.set_title('Throughput Comparison', fontsize=12, fontweight='bold')\n",
    "    ax1.tick_params(axis='x', rotation=45, labelsize=8)\n",
    "    \n",
    "    for bar, val in zip(bars, df['Tokens/sec']):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "                 f'{val:.0f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # Latency comparison\n",
    "    ax2 = axes[1]\n",
    "    bars = ax2.bar(df['Method'], df['Latency (ms)'], color=colors, edgecolor='black')\n",
    "    ax2.set_xlabel('Method', fontsize=10)\n",
    "    ax2.set_ylabel('Latency (ms)', fontsize=10)\n",
    "    ax2.set_title('Latency Comparison', fontsize=12, fontweight='bold')\n",
    "    ax2.tick_params(axis='x', rotation=45, labelsize=8)\n",
    "    \n",
    "    # Memory comparison\n",
    "    ax3 = axes[2]\n",
    "    bars = ax3.bar(df['Method'], df['Memory (GB)'], color=colors, edgecolor='black')\n",
    "    ax3.set_xlabel('Method', fontsize=10)\n",
    "    ax3.set_ylabel('Memory (GB)', fontsize=10)\n",
    "    ax3.set_title('Memory Usage', fontsize=12, fontweight='bold')\n",
    "    ax3.tick_params(axis='x', rotation=45, labelsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(config.engine_dir) / 'inference_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize comparison\n",
    "plot_inference_comparison(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Production Deployment Configuration\n",
    "\n",
    "### 5.1 Triton Inference Server Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_triton_config(\n",
    "    model_name: str,\n",
    "    max_batch_size: int = 8,\n",
    "    instance_count: int = 1\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate Triton Inference Server model configuration.\n",
    "    \"\"\"\n",
    "    config = f\"\"\"\n",
    "# Triton Model Configuration for {model_name}\n",
    "# Generated for DGX Spark with TensorRT-LLM\n",
    "\n",
    "name: \"{model_name.replace('/', '_')}\"\n",
    "backend: \"tensorrtllm\"\n",
    "max_batch_size: {max_batch_size}\n",
    "\n",
    "model_transaction_policy {{\n",
    "  decoupled: True\n",
    "}}\n",
    "\n",
    "input [\n",
    "  {{\n",
    "    name: \"text_input\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ -1 ]\n",
    "  }},\n",
    "  {{\n",
    "    name: \"max_tokens\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 1 ]\n",
    "  }},\n",
    "  {{\n",
    "    name: \"temperature\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]\n",
    "    optional: true\n",
    "  }},\n",
    "  {{\n",
    "    name: \"top_p\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]\n",
    "    optional: true\n",
    "  }}\n",
    "]\n",
    "\n",
    "output [\n",
    "  {{\n",
    "    name: \"text_output\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ -1 ]\n",
    "  }}\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "  {{\n",
    "    count: {instance_count}\n",
    "    kind: KIND_GPU\n",
    "  }}\n",
    "]\n",
    "\n",
    "parameters: {{\n",
    "  key: \"gpt_model_type\"\n",
    "  value: {{\n",
    "    string_value: \"inflight_fused_batching\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "parameters: {{\n",
    "  key: \"kv_cache_type\"\n",
    "  value: {{\n",
    "    string_value: \"paged\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "parameters: {{\n",
    "  key: \"max_tokens_in_paged_kv_cache\"\n",
    "  value: {{\n",
    "    string_value: \"16384\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "dynamic_batching {{\n",
    "  preferred_batch_size: [1, 2, 4, 8]\n",
    "  max_queue_delay_microseconds: 100\n",
    "}}\n",
    "\"\"\"\n",
    "    return config\n",
    "\n",
    "\n",
    "# Generate config\n",
    "triton_config = generate_triton_config(config.model_name)\n",
    "\n",
    "# Save config\n",
    "config_path = Path(config.engine_dir) / \"triton_config.pbtxt\"\n",
    "with open(config_path, 'w') as f:\n",
    "    f.write(triton_config)\n",
    "\n",
    "print(f\"Triton configuration saved to: {config_path}\")\n",
    "print(\"\\n\" + triton_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Docker Compose for Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_compose = \"\"\"\n",
    "# Docker Compose for TensorRT-LLM Production Deployment\n",
    "# Optimized for DGX Spark\n",
    "\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  triton-trtllm:\n",
    "    image: nvcr.io/nvidia/tritonserver:24.11-trtllm-python-py3\n",
    "    container_name: triton-llm\n",
    "    ports:\n",
    "      - \"8000:8000\"  # HTTP\n",
    "      - \"8001:8001\"  # gRPC\n",
    "      - \"8002:8002\"  # Metrics\n",
    "    volumes:\n",
    "      - ./model_repository:/models\n",
    "      - ./engines:/engines\n",
    "    environment:\n",
    "      - CUDA_VISIBLE_DEVICES=0\n",
    "    command: |\n",
    "      tritonserver \n",
    "        --model-repository=/models \n",
    "        --http-port=8000 \n",
    "        --grpc-port=8001 \n",
    "        --metrics-port=8002\n",
    "        --log-verbose=1\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              count: 1\n",
    "              capabilities: [gpu]\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/v2/health/ready\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "    restart: unless-stopped\n",
    "\n",
    "  prometheus:\n",
    "    image: prom/prometheus:latest\n",
    "    container_name: prometheus\n",
    "    ports:\n",
    "      - \"9090:9090\"\n",
    "    volumes:\n",
    "      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n",
    "    depends_on:\n",
    "      - triton-trtllm\n",
    "\n",
    "  grafana:\n",
    "    image: grafana/grafana:latest\n",
    "    container_name: grafana\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "    volumes:\n",
    "      - grafana-data:/var/lib/grafana\n",
    "    depends_on:\n",
    "      - prometheus\n",
    "\n",
    "volumes:\n",
    "  grafana-data:\n",
    "\"\"\"\n",
    "\n",
    "# Save docker-compose\n",
    "compose_path = Path(config.engine_dir) / \"docker-compose.yml\"\n",
    "with open(compose_path, 'w') as f:\n",
    "    f.write(docker_compose)\n",
    "\n",
    "print(f\"Docker Compose configuration saved to: {compose_path}\")\n",
    "print(\"\\nTo start the production server:\")\n",
    "print(f\"  cd {config.engine_dir}\")\n",
    "print(\"  docker-compose up -d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Performance Optimization Tips\n",
    "\n",
    "### 6.1 DGX Spark Blackwell Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_tips = \"\"\"\n",
    "============================================================\n",
    "TensorRT-LLM Optimization Tips for DGX Spark Blackwell\n",
    "============================================================\n",
    "\n",
    "1. QUANTIZATION SELECTION\n",
    "   -----------------------\n",
    "   • Use FP8 for best quality/speed balance (native Blackwell support)\n",
    "   • Use FP4 for maximum throughput (2-3x faster than FP16)\n",
    "   • Avoid INT4 if quality is critical - use FP4 instead\n",
    "\n",
    "2. BATCHING CONFIGURATION\n",
    "   ----------------------\n",
    "   • Enable inflight batching for variable-length requests\n",
    "   • Set max_batch_size based on expected load (8-16 typical)\n",
    "   • Use paged KV-cache to maximize concurrent requests\n",
    "\n",
    "3. MEMORY OPTIMIZATION\n",
    "   -------------------\n",
    "   • DGX Spark has 128GB unified memory - use it!\n",
    "   • Set max_num_tokens based on expected sequence lengths\n",
    "   • Enable FP8/FP4 KV-cache for 2x memory savings\n",
    "\n",
    "4. KERNEL OPTIMIZATION\n",
    "   -------------------\n",
    "   • Always enable Flash Attention 2\n",
    "   • Enable fused MLP operations\n",
    "   • Use GEMM plugin with appropriate dtype\n",
    "\n",
    "5. BUILD OPTIMIZATION\n",
    "   ------------------\n",
    "   • Use builder_optimization_level=5 for production\n",
    "   • Enable remove_input_padding for variable lengths\n",
    "   • Profile with different batch sizes to find optimal\n",
    "\n",
    "6. BLACKWELL-SPECIFIC\n",
    "   ------------------\n",
    "   • Native FP4 support = 3-4x throughput vs FP16\n",
    "   • FP8 compute = minimal quality loss, 2x speedup\n",
    "   • Unified memory eliminates GPU memory limits\n",
    "\n",
    "EXPECTED PERFORMANCE ON DGX SPARK:\n",
    "==================================\n",
    "\n",
    "| Model Size | FP16 | FP8 | FP4 |\n",
    "|------------|------|-----|-----|\n",
    "| 3B params  | 100  | 200 | 350 | tokens/sec (batch=1)\n",
    "| 8B params  | 40   | 80  | 150 | tokens/sec (batch=1)\n",
    "| 70B params | 15   | 30  | 60  | tokens/sec (batch=1)\n",
    "\n",
    "* With batching, throughput scales nearly linearly up to memory limits\n",
    "\"\"\"\n",
    "\n",
    "print(optimization_tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Troubleshooting Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "troubleshooting_guide = \"\"\"\n",
    "============================================================\n",
    "TensorRT-LLM Troubleshooting Guide\n",
    "============================================================\n",
    "\n",
    "ISSUE: \"Out of memory\" during engine build\n",
    "SOLUTION:\n",
    "  - Reduce max_batch_size\n",
    "  - Reduce max_input_len + max_output_len\n",
    "  - Use weight-only quantization first (INT8/INT4)\n",
    "  - Close other GPU processes\n",
    "\n",
    "ISSUE: Engine build fails with quantization\n",
    "SOLUTION:\n",
    "  - Ensure calibration data is valid\n",
    "  - Check model architecture is supported\n",
    "  - Try simpler quantization (INT8 before FP8)\n",
    "  - Update TensorRT-LLM to latest version\n",
    "\n",
    "ISSUE: Slow inference speed\n",
    "SOLUTION:\n",
    "  - Enable all plugins (GEMM, attention, etc.)\n",
    "  - Check GPU utilization with nvidia-smi\n",
    "  - Profile with nsys/ncu for bottlenecks\n",
    "  - Verify quantization is applied correctly\n",
    "\n",
    "ISSUE: Quality degradation after quantization\n",
    "SOLUTION:\n",
    "  - Use larger calibration dataset (1000+ samples)\n",
    "  - Try SmoothQuant for INT8\n",
    "  - Use FP8 instead of INT4\n",
    "  - Increase group size for weight quantization\n",
    "\n",
    "ISSUE: Triton server crashes\n",
    "SOLUTION:\n",
    "  - Check model configuration (pbtxt)\n",
    "  - Verify engine path is correct\n",
    "  - Check CUDA/TensorRT version compatibility\n",
    "  - Review Triton logs: docker logs triton-llm\n",
    "\n",
    "COMMON COMMANDS:\n",
    "================\n",
    "\n",
    "# Check GPU status\n",
    "nvidia-smi\n",
    "\n",
    "# Check TensorRT-LLM version\n",
    "python -c \"import tensorrt_llm; print(tensorrt_llm.__version__)\"\n",
    "\n",
    "# Profile engine build\n",
    "nsys profile -o trtllm_build trtllm-build ...\n",
    "\n",
    "# Test Triton health\n",
    "curl localhost:8000/v2/health/ready\n",
    "\n",
    "# Get model metrics\n",
    "curl localhost:8002/metrics | grep trtllm\n",
    "\"\"\"\n",
    "\n",
    "print(troubleshooting_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Expected Results Summary\n",
    "\n",
    "### 7.1 Benchmark Results on DGX Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected TensorRT-LLM results on DGX Spark\n",
    "expected_results = pd.DataFrame([\n",
    "    {\n",
    "        'Configuration': 'Llama-3.2-3B FP16',\n",
    "        'Batch=1 (tok/s)': 95,\n",
    "        'Batch=4 (tok/s)': 350,\n",
    "        'Batch=8 (tok/s)': 650,\n",
    "        'Memory (GB)': 6.0,\n",
    "        'Quality': '100%'\n",
    "    },\n",
    "    {\n",
    "        'Configuration': 'Llama-3.2-3B FP8',\n",
    "        'Batch=1 (tok/s)': 180,\n",
    "        'Batch=4 (tok/s)': 650,\n",
    "        'Batch=8 (tok/s)': 1150,\n",
    "        'Memory (GB)': 3.2,\n",
    "        'Quality': '99.5%'\n",
    "    },\n",
    "    {\n",
    "        'Configuration': 'Llama-3.2-3B FP4',\n",
    "        'Batch=1 (tok/s)': 280,\n",
    "        'Batch=4 (tok/s)': 1000,\n",
    "        'Batch=8 (tok/s)': 1800,\n",
    "        'Memory (GB)': 1.8,\n",
    "        'Quality': '98%'\n",
    "    },\n",
    "    {\n",
    "        'Configuration': 'Llama-3.2-8B FP8',\n",
    "        'Batch=1 (tok/s)': 85,\n",
    "        'Batch=4 (tok/s)': 320,\n",
    "        'Batch=8 (tok/s)': 580,\n",
    "        'Memory (GB)': 8.5,\n",
    "        'Quality': '99.3%'\n",
    "    },\n",
    "    {\n",
    "        'Configuration': 'Llama-3.1-70B FP4',\n",
    "        'Batch=1 (tok/s)': 45,\n",
    "        'Batch=4 (tok/s)': 160,\n",
    "        'Batch=8 (tok/s)': 280,\n",
    "        'Memory (GB)': 42,\n",
    "        'Quality': '97%'\n",
    "    },\n",
    "])\n",
    "\n",
    "print(\"Expected TensorRT-LLM Results on DGX Spark Blackwell:\")\n",
    "print(\"=\"*75)\n",
    "print(expected_results.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*75)\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"  • FP8 provides 2x throughput with <1% quality loss\")\n",
    "print(\"  • FP4 provides 3x throughput - ideal for high-traffic applications\")\n",
    "print(\"  • 70B model fits in memory with FP4 quantization\")\n",
    "print(\"  • Batch processing increases throughput near-linearly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "```\n",
    "Professor SPARK's Summary:\n",
    "\n",
    "\"TensorRT-LLM is THE production solution for LLM deployment on NVIDIA hardware.\n",
    "\n",
    "Key benefits:\n",
    "1. 2-5x faster than vanilla PyTorch/Transformers\n",
    "2. Native FP8/FP4 support on Blackwell = maximum performance\n",
    "3. Paged KV-cache = handle more concurrent requests\n",
    "4. In-flight batching = maximize GPU utilization\n",
    "5. Triton integration = production-ready serving\n",
    "\n",
    "For DGX Spark:\n",
    "- Use FP8 for best quality/speed balance\n",
    "- Use FP4 for maximum throughput\n",
    "- 128GB unified memory enables large models without swapping\n",
    "- Single GPU can serve production traffic efficiently\n",
    "\n",
    "Remember: The engine build takes time (10-30 min), but runtime is FAST!\"\n",
    "```\n",
    "\n",
    "### Module 3.2 Complete!\n",
    "\n",
    "You have now learned:\n",
    "\n",
    "1. **Data types** (FP32 → FP16 → BF16 → INT8 → INT4 → FP8 → FP4)\n",
    "2. **NVFP4** - Blackwell's native 4-bit format with micro-block scaling\n",
    "3. **FP8 training and inference** - E4M3 vs E5M2 with Transformer Engine\n",
    "4. **GPTQ** - Hessian-based post-training quantization\n",
    "5. **AWQ** - Activation-aware weight quantization\n",
    "6. **GGUF** - llama.cpp format for CPU/mixed inference\n",
    "7. **Quality benchmarks** - Perplexity + MMLU evaluation\n",
    "8. **TensorRT-LLM** - Production deployment with optimized engines\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Module 3.3: Model Deployment with Triton and FastAPI\n",
    "- Apply quantization to your own models\n",
    "- Build production inference pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Build Custom Engine\n",
    "\n",
    "Build a TensorRT-LLM engine with your own configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Create custom engine configuration\n",
    "\n",
    "# TODO: Modify these parameters for your use case\n",
    "custom_config = TRTLLMConfig(\n",
    "    model_name=\"your-model-name\",\n",
    "    quantization=\"fp8\",  # Try: none, int8, fp8, int4_awq, fp4\n",
    "    max_batch_size=16,\n",
    "    max_input_len=4096,\n",
    "    max_output_len=1024\n",
    ")\n",
    "\n",
    "# Generate build commands\n",
    "# custom_builder = TRTLLMEngineBuilder(custom_config)\n",
    "# custom_builder.build_engine_cli()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Benchmark Different Batch Sizes\n",
    "\n",
    "Find the optimal batch size for your workload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Batch size optimization\n",
    "\n",
    "# TODO: Run benchmarks with different batch sizes\n",
    "batch_sizes = [1, 2, 4, 8, 16, 32]\n",
    "\n",
    "# For each batch size, measure:\n",
    "# - Throughput (tokens/second)\n",
    "# - Latency (per-request)\n",
    "# - Memory usage\n",
    "\n",
    "# Find the sweet spot for your latency requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Deploy to Triton\n",
    "\n",
    "Create a complete Triton deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Triton deployment\n",
    "\n",
    "# TODO:\n",
    "# 1. Build your TensorRT-LLM engine\n",
    "# 2. Create model repository structure:\n",
    "#    model_repository/\n",
    "#      your_model/\n",
    "#        1/\n",
    "#          model.plan  (or engine files)\n",
    "#        config.pbtxt\n",
    "\n",
    "# 3. Start Triton server:\n",
    "#    docker run --gpus all -p 8000:8000 -v ./model_repository:/models \\\n",
    "#      nvcr.io/nvidia/tritonserver:24.11-trtllm-python-py3 \\\n",
    "#      tritonserver --model-repository=/models\n",
    "\n",
    "# 4. Test with curl:\n",
    "#    curl -X POST localhost:8000/v2/models/your_model/infer \\\n",
    "#      -H 'Content-Type: application/json' \\\n",
    "#      -d '{\"inputs\": [...]}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
