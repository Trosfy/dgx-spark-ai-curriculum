{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2.5: AWQ Quantization\n",
    "\n",
    "**Module:** 3.2 - Model Quantization & Optimization  \n",
    "**Time:** 1.5 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand AWQ's activation-aware approach\n",
    "- [ ] Apply AWQ quantization to a model\n",
    "- [ ] Compare AWQ vs GPTQ performance and quality\n",
    "- [ ] Choose the right quantization method for your use case\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Lab 3.2.4 (GPTQ Quantization)\n",
    "- Library: `autoawq` (`pip install autoawq`)\n",
    "- Hardware: CUDA GPU (DGX Spark recommended)\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**AWQ (Activation-aware Weight Quantization)** was published in 2023 and quickly became a GPTQ alternative:\n",
    "\n",
    "| Aspect | GPTQ | AWQ |\n",
    "|--------|------|-----|\n",
    "| Core idea | Hessian-based error correction | Protect salient weights |\n",
    "| Speed | Slower quantization | Faster quantization |\n",
    "| Quality | Excellent | Often slightly better |\n",
    "| Inference | Fast | Fast (comparable) |\n",
    "\n",
    "**Key Insight:** Not all weights are equally important! AWQ identifies \"salient\" weights (those that matter most for accuracy) and protects them during quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: How AWQ Works\n",
    "\n",
    "> **Imagine you're compressing a photo album...**\n",
    ">\n",
    "> **GPTQ approach:** Compress everything, then go back and fix the most blurry parts.\n",
    ">\n",
    "> **AWQ approach:** First, identify which photos are your favorites (most viewed).  \n",
    "> Then, compress the boring photos aggressively but keep your favorites sharp!\n",
    ">\n",
    "> **In AI terms:**\n",
    "> 1. Run calibration data through the model\n",
    "> 2. Track which weights produce large activations (\"salient\" weights)\n",
    "> 3. Scale up salient weights before quantization (protecting them)\n",
    "> 4. Quantize all weights to 4-bit\n",
    "> 5. Scale down at inference to compensate\n",
    ">\n",
    "> The result: Important weights stay accurate, less important weights get compressed more!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"AWQ Quantization Lab\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Check for autoawq\n",
    "try:\n",
    "    from awq import AutoAWQForCausalLM\n",
    "    HAS_AWQ = True\n",
    "    print(f\"\\nautoawq: Available\")\n",
    "except ImportError:\n",
    "    HAS_AWQ = False\n",
    "    print(f\"\\nautoawq: Not installed\")\n",
    "    print(\"  Install with: pip install autoawq\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / 1e9\n",
    "    return 0\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Helpers loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding AWQ's Salient Weight Protection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the AWQ concept: protecting salient weights\n",
    "\n",
    "def simulate_awq_quantize(weights, activations, bits=4, protect_ratio=0.01):\n",
    "    \"\"\"\n",
    "    Simplified AWQ-style quantization.\n",
    "    \n",
    "    AWQ's key insight:\n",
    "    1. Some weights produce large activations consistently\n",
    "    2. These \"salient\" weights should be protected from quantization error\n",
    "    3. We scale them up before quantization, then scale down at inference\n",
    "    \n",
    "    Args:\n",
    "        weights: Weight matrix [out, in]\n",
    "        activations: Sample activations [batch, in]\n",
    "        bits: Quantization bits\n",
    "        protect_ratio: Fraction of weights to protect (top salient)\n",
    "    \"\"\"\n",
    "    # Step 1: Compute \"saliency\" - how much each weight contributes to activations\n",
    "    # In real AWQ, this is |weight| * activation_std\n",
    "    activation_importance = activations.abs().mean(dim=0)  # [in]\n",
    "    weight_importance = weights.abs()  # [out, in]\n",
    "    saliency = weight_importance * activation_importance  # [out, in]\n",
    "    \n",
    "    # Step 2: Find salient weights (top percentile)\n",
    "    threshold = torch.quantile(saliency.flatten(), 1 - protect_ratio)\n",
    "    is_salient = saliency >= threshold\n",
    "    \n",
    "    # Step 3: Compute per-channel scales\n",
    "    # Salient channels get scaled up to reduce relative quantization error\n",
    "    channel_saliency = saliency.mean(dim=0)  # [in]\n",
    "    scales = (channel_saliency / channel_saliency.mean()).clamp(min=0.5, max=2.0)\n",
    "    \n",
    "    # Step 4: Apply scales and quantize\n",
    "    scaled_weights = weights * scales  # Scale up salient channels\n",
    "    \n",
    "    # Quantize\n",
    "    qmax = 2 ** (bits - 1) - 1\n",
    "    max_val = scaled_weights.abs().max(dim=1, keepdim=True)[0]\n",
    "    quant_scale = max_val / qmax\n",
    "    quant_scale = torch.clamp(quant_scale, min=1e-10)\n",
    "    \n",
    "    quantized = torch.round(scaled_weights / quant_scale).clamp(-qmax-1, qmax)\n",
    "    dequantized = quantized * quant_scale\n",
    "    \n",
    "    # Undo scaling\n",
    "    final_weights = dequantized / scales\n",
    "    \n",
    "    return final_weights, is_salient, scales\n",
    "\n",
    "\n",
    "def simple_quantize(weights, bits=4):\n",
    "    \"\"\"Simple symmetric quantization without protection.\"\"\"\n",
    "    qmax = 2 ** (bits - 1) - 1\n",
    "    max_val = weights.abs().max(dim=1, keepdim=True)[0]\n",
    "    scale = max_val / qmax\n",
    "    scale = torch.clamp(scale, min=1e-10)\n",
    "    quantized = torch.round(weights / scale).clamp(-qmax-1, qmax)\n",
    "    return quantized * scale\n",
    "\n",
    "\n",
    "# Test with sample data\n",
    "torch.manual_seed(42)\n",
    "weights = torch.randn(256, 512)\n",
    "activations = torch.randn(100, 512)  # 100 samples\n",
    "\n",
    "# Compare simple vs AWQ\n",
    "simple_result = simple_quantize(weights, bits=4)\n",
    "awq_result, is_salient, scales = simulate_awq_quantize(weights, activations, bits=4)\n",
    "\n",
    "# Compute errors\n",
    "simple_error = (weights - simple_result).abs()\n",
    "awq_error = (weights - awq_result).abs()\n",
    "\n",
    "print(\"AWQ vs Simple Quantization\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Metric':<25} {'Simple':>12} {'AWQ':>12}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Mean error':<25} {simple_error.mean():.6f} {awq_error.mean():>12.6f}\")\n",
    "print(f\"{'Max error':<25} {simple_error.max():.6f} {awq_error.max():>12.6f}\")\n",
    "print(f\"{'RMSE':<25} {simple_error.pow(2).mean().sqrt():.6f} {awq_error.pow(2).mean().sqrt():>12.6f}\")\n",
    "\n",
    "# Error on salient weights specifically\n",
    "salient_simple_error = simple_error[is_salient].mean()\n",
    "salient_awq_error = awq_error[is_salient].mean()\n",
    "print(f\"{'Error on salient weights':<25} {salient_simple_error:.6f} {salient_awq_error:>12.6f}\")\n",
    "\n",
    "improvement = (salient_simple_error - salient_awq_error) / salient_simple_error * 100\n",
    "print(f\"\\nAWQ reduces salient weight error by {improvement:.1f}%!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize AWQ's protection of salient weights\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Saliency distribution\n",
    "ax = axes[0, 0]\n",
    "saliency = (weights.abs() * activations.abs().mean(dim=0)).flatten()\n",
    "ax.hist(saliency.numpy(), bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "threshold = torch.quantile(saliency, 0.99)\n",
    "ax.axvline(threshold.item(), color='red', linestyle='--', linewidth=2, label=f'Top 1% threshold')\n",
    "ax.set_xlabel('Saliency Score')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Weight Saliency Distribution')\n",
    "ax.legend()\n",
    "\n",
    "# Scaling factors\n",
    "ax = axes[0, 1]\n",
    "ax.bar(range(len(scales)), scales.numpy(), alpha=0.7, color='coral')\n",
    "ax.axhline(1.0, color='black', linestyle='--', linewidth=1)\n",
    "ax.set_xlabel('Channel Index')\n",
    "ax.set_ylabel('Scale Factor')\n",
    "ax.set_title('AWQ Per-Channel Scales (sample)')\n",
    "ax.set_xlim(0, 50)  # Show first 50 channels\n",
    "\n",
    "# Error comparison\n",
    "ax = axes[1, 0]\n",
    "ax.hist(simple_error.flatten().numpy(), bins=50, alpha=0.5, label='Simple', color='coral')\n",
    "ax.hist(awq_error.flatten().numpy(), bins=50, alpha=0.5, label='AWQ', color='steelblue')\n",
    "ax.set_xlabel('Absolute Error')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Error Distribution: Simple vs AWQ')\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Error on salient vs non-salient\n",
    "ax = axes[1, 1]\n",
    "categories = ['Non-Salient', 'Salient (Top 1%)']\n",
    "simple_errors = [\n",
    "    simple_error[~is_salient].mean().item(),\n",
    "    simple_error[is_salient].mean().item()\n",
    "]\n",
    "awq_errors = [\n",
    "    awq_error[~is_salient].mean().item(),\n",
    "    awq_error[is_salient].mean().item()\n",
    "]\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, simple_errors, width, label='Simple', color='coral')\n",
    "ax.bar(x + width/2, awq_errors, width, label='AWQ', color='steelblue')\n",
    "ax.set_ylabel('Mean Absolute Error')\n",
    "ax.set_title('Error by Weight Importance')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: AWQ significantly reduces error on the weights that matter most!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: AWQ Quantization with AutoAWQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Model selection\n",
    "MODEL_NAME = \"microsoft/phi-2\"  # Good for demo\n",
    "# MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"  # Production benchmark\n",
    "\n",
    "print(f\"Selected model: {MODEL_NAME}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer loaded. Vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWQ Quantization\n",
    "\n",
    "if HAS_AWQ:\n",
    "    print(\"\\nQuantizing with AWQ...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # AWQ configuration\n",
    "    awq_config = {\n",
    "        \"zero_point\": True,      # Use zero-point quantization\n",
    "        \"q_group_size\": 128,     # Group size (like GPTQ)\n",
    "        \"w_bit\": 4,              # 4-bit weights\n",
    "        \"version\": \"GEMM\",       # GEMM kernel (fastest)\n",
    "    }\n",
    "    \n",
    "    print(f\"Config: {awq_config}\")\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"\\nLoading model...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = AutoAWQForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"Model loaded in {time.time() - start_time:.1f}s\")\n",
    "    print(f\"GPU memory: {get_gpu_memory():.2f} GB\")\n",
    "    \n",
    "    # Quantize\n",
    "    print(f\"\\nRunning AWQ quantization...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.quantize(\n",
    "        tokenizer,\n",
    "        quant_config=awq_config,\n",
    "    )\n",
    "    \n",
    "    print(f\"Quantization complete in {time.time() - start_time:.1f}s\")\n",
    "    print(f\"GPU memory: {get_gpu_memory():.2f} GB\")\n",
    "    \n",
    "else:\n",
    "    print(\"autoawq not available. Showing expected workflow:\")\n",
    "    print(\"\"\"\n",
    "    from awq import AutoAWQForCausalLM\n",
    "    \n",
    "    model = AutoAWQForCausalLM.from_pretrained(MODEL_NAME)\n",
    "    model.quantize(tokenizer, quant_config={\"w_bit\": 4, \"q_group_size\": 128})\n",
    "    model.save_quantized(\"./my-model-awq-4bit\")\n",
    "    \"\"\")\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the AWQ model\n",
    "\n",
    "if model is not None:\n",
    "    test_prompt = \"The key to machine learning is\"\n",
    "    \n",
    "    print(f\"Testing AWQ model...\")\n",
    "    print(f\"Prompt: '{test_prompt}'\")\n",
    "    \n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"\\nGenerated: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: AWQ vs GPTQ Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWQ vs GPTQ comparison table\n",
    "\n",
    "print(\"AWQ vs GPTQ Comparison\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "Based on published benchmarks (Llama 2 7B):\n",
    "\n",
    "| Metric | FP16 | GPTQ-4bit | AWQ-4bit |\n",
    "|--------|------|-----------|----------|\n",
    "| Memory | 13.5 GB | 4.0 GB | 4.0 GB |\n",
    "| MMLU | 46.0% | 45.5% | 45.7% |\n",
    "| HellaSwag | 79.0% | 78.5% | 78.8% |\n",
    "| Winogrande | 74.0% | 73.5% | 73.8% |\n",
    "| Quant Time | - | ~45 min | ~15 min |\n",
    "| Inference Speed | 1.0x | 2.5x | 2.5x |\n",
    "\n",
    "Key differences:\n",
    "1. AWQ is ~3x faster to quantize\n",
    "2. AWQ often has slightly better quality (0.1-0.3%)\n",
    "3. Inference speed is comparable\n",
    "4. Memory usage is identical\n",
    "\n",
    "When to use each:\n",
    "- AWQ: Default choice for most use cases\n",
    "- GPTQ: When you need desc_act for specific models\n",
    "- Either: Both are production-ready!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pre-quantized AWQ models\n",
    "\n",
    "print(\"\\nLoading Pre-Quantized AWQ Models\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\"\"\n",
    "Many AWQ models are available on Hugging Face:\n",
    "\n",
    "TheBloke's AWQ models:\n",
    "- TheBloke/Llama-2-7B-AWQ\n",
    "- TheBloke/Llama-2-13B-AWQ  \n",
    "- TheBloke/Llama-2-70B-AWQ\n",
    "- TheBloke/Mistral-7B-AWQ\n",
    "- And many more!\n",
    "\n",
    "Loading a pre-quantized model:\n",
    "\"\"\")\n",
    "\n",
    "print(\"\"\"\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = AutoAWQForCausalLM.from_quantized(\n",
    "    \"TheBloke/Llama-2-7B-AWQ\",\n",
    "    fuse_layers=True,  # Fuse for faster inference\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Llama-2-7B-AWQ\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Compare AWQ and GPTQ Quality\n",
    "\n",
    "Load both AWQ and GPTQ versions of the same model and compare:\n",
    "1. Output quality on the same prompts\n",
    "2. Inference speed\n",
    "3. Memory usage\n",
    "\n",
    "### Exercise 2: Test AWQ Fused Layers\n",
    "\n",
    "Compare inference speed with and without layer fusion:\n",
    "```python\n",
    "model = AutoAWQForCausalLM.from_quantized(..., fuse_layers=True)\n",
    "model = AutoAWQForCausalLM.from_quantized(..., fuse_layers=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Your code here\n",
    "\n",
    "# TODO: Load AWQ and GPTQ versions of the same model\n",
    "# TODO: Run the same prompts through both\n",
    "# TODO: Compare quality and speed\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Using Fused Layers\n",
    "\n",
    "```python\n",
    "# Wrong: Loading without fusion (slower)\n",
    "model = AutoAWQForCausalLM.from_quantized(model_path)\n",
    "\n",
    "# Right: Enable layer fusion for speed\n",
    "model = AutoAWQForCausalLM.from_quantized(model_path, fuse_layers=True)\n",
    "```\n",
    "\n",
    "### Mistake 2: Wrong GEMM Version\n",
    "\n",
    "```python\n",
    "# Wrong: Using slower GEMV for batched inference\n",
    "config = {\"version\": \"GEMV\"}  # Only for batch_size=1\n",
    "\n",
    "# Right: Use GEMM for batched inference\n",
    "config = {\"version\": \"GEMM\"}  # Works for all batch sizes\n",
    "```\n",
    "\n",
    "### Mistake 3: Ignoring Hardware Compatibility\n",
    "\n",
    "```python\n",
    "# AWQ kernels require specific GPU architectures\n",
    "# Check compatibility before using\n",
    "\n",
    "# Supported: Ampere, Ada, Hopper, Blackwell (SM 80+)\n",
    "# Limited support: Turing (SM 75)\n",
    "# Not supported: Volta and earlier\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "\n",
    "- **AWQ concept**: Protect salient weights through activation-aware scaling\n",
    "- **AWQ vs GPTQ**: Similar quality, AWQ is faster to quantize\n",
    "- **Layer fusion**: Enable for faster inference\n",
    "- **Pre-quantized models**: TheBloke has AWQ versions of popular models\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [AWQ Paper](https://arxiv.org/abs/2306.00978)\n",
    "- [AutoAWQ GitHub](https://github.com/casper-hansen/AutoAWQ)\n",
    "- [AWQ vs GPTQ Comparison](https://huggingface.co/blog/4bit-transformers-bitsandbytes)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "if 'model' in dir() and model is not None:\n",
    "    del model\n",
    "\n",
    "clear_memory()\n",
    "print(\"Notebook complete! Ready for Lab 3.2.6: GGUF Conversion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, we'll explore **GGUF Conversion** - the format used by llama.cpp for CPU/GPU inference!\n",
    "\n",
    "‚û°Ô∏è Continue to: [Lab 3.2.6: GGUF Conversion](lab-3.2.6-gguf-conversion.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
