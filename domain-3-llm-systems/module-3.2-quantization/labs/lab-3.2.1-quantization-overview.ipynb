{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2.1: Quantization Overview\n",
    "\n",
    "**Module:** 3.2 - Model Quantization & Optimization  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand different numerical data types (FP32, FP16, BF16, INT8, INT4, FP4)\n",
    "- [ ] Explain the precision-memory-speed tradeoffs in quantization\n",
    "- [ ] Compare model sizes and inference speeds across precision levels\n",
    "- [ ] Measure perplexity to assess quality degradation\n",
    "- [ ] Appreciate DGX Spark's unique capabilities for quantized inference\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Module 10 (LLM Fine-tuning)\n",
    "- Knowledge of: PyTorch basics, transformers library\n",
    "- Hardware: DGX Spark with 128GB unified memory\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The Problem:** You've fine-tuned a powerful 70B parameter model, but deploying it costs a fortune!\n",
    "\n",
    "Consider the math:\n",
    "- **70B parameters √ó 2 bytes (FP16) = 140GB** just for weights\n",
    "- Plus activations, KV cache, framework overhead...\n",
    "- That's $25,000+ for a cloud GPU instance\n",
    "\n",
    "**The Solution:** Quantization reduces memory by 2-4√ó with minimal quality loss.\n",
    "\n",
    "| Company | Use Case | Quantization Win |\n",
    "|---------|----------|------------------|\n",
    "| Google | On-device Gemini Nano | 4-bit enables running on phones |\n",
    "| Meta | Llama deployment | INT8 halves serving costs |\n",
    "| Apple | CoreML models | 16‚Üí4 bit for Neural Engine |\n",
    "| **You** | DGX Spark | NVFP4 gives 3.5√ó compression! |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßí ELI5: What is Quantization?\n",
    "\n",
    "> **Imagine you're taking notes in class...**\n",
    ">\n",
    "> You could write down every single word the teacher says (FP32 - full precision).  \n",
    "> That's accurate, but your notebook fills up fast and your hand gets tired!\n",
    ">\n",
    "> Instead, you could:\n",
    "> - Write only the key points (FP16 - half precision)\n",
    "> - Use abbreviations like \"b/c\" for \"because\" (INT8 - 8-bit integers)\n",
    "> - Just draw simple diagrams (INT4 - 4-bit integers)\n",
    ">\n",
    "> Each shorthand:\n",
    "> - ‚úÖ Uses less notebook space\n",
    "> - ‚úÖ Lets you write faster\n",
    "> - ‚ö†Ô∏è Might lose some details\n",
    ">\n",
    "> **In AI terms:** Quantization is using fewer bits to store each number in a neural network.  \n",
    "> Just like your notes, smaller bits = smaller models = faster inference, but potentially less accurate.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Data Types\n",
    "\n",
    "Neural networks are just massive collections of numbers (weights). How we store those numbers matters!\n",
    "\n",
    "### The Number Line Analogy\n",
    "\n",
    "Imagine a ruler that can measure from -1000 to +1000:\n",
    "\n",
    "- **FP32 (32 bits)**: Like having 4 billion tick marks on your ruler. Incredibly precise!\n",
    "- **FP16 (16 bits)**: 65,000 tick marks. Still very precise for most purposes.\n",
    "- **BF16 (16 bits)**: Same range as FP32, but with FP16's precision. Best of both worlds!\n",
    "- **INT8 (8 bits)**: Only 256 tick marks. Some values get rounded to neighbors.\n",
    "- **INT4 (4 bits)**: Just 16 tick marks. Significant rounding, but surprisingly okay for many tasks!\n",
    "\n",
    "Let's visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# First, let's check our DGX Spark environment\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport gc\nimport time\nimport math\n\nprint(\"=\" * 60)\nprint(\"DGX Spark Environment Check\")\nprint(\"=\" * 60)\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n    print(f\"Compute Capability: {torch.cuda.get_device_capability()}\")\n\n# Check for BF16 support (Blackwell has native support)\nprint(f\"\\nBF16 supported: {torch.cuda.is_bf16_supported()}\")\n\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° **Note:** This module includes reusable utility scripts in the `scripts/` directory.\n",
    "> For your own projects, you can import them directly:\n",
    "> ```python\n",
    "> import sys\n",
    "> sys.path.insert(0, '../scripts')\n",
    "> from memory_utils import get_gpu_memory, clear_memory, MemoryTracker\n",
    "> from perplexity import calculate_perplexity\n",
    "> from quantization_utils import symmetric_quantize, dequantize\n",
    "> ```\n",
    "> For this tutorial, we define helper functions inline for clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualizing precision loss across data types\n\ndef show_float_representation(value, dtype_name):\n    \"\"\"Show how a number is represented in different data types.\"\"\"\n    if dtype_name == 'FP32':\n        tensor = torch.tensor([value], dtype=torch.float32)\n    elif dtype_name == 'FP16':\n        tensor = torch.tensor([value], dtype=torch.float16)\n    elif dtype_name == 'BF16':\n        tensor = torch.tensor([value], dtype=torch.bfloat16)\n    else:\n        tensor = torch.tensor([value], dtype=torch.float32)\n    \n    stored_value = tensor.item()\n    error = abs(value - stored_value)\n    \n    return stored_value, error\n\n# Let's see how the same number is stored differently\ntest_value = 3.141592653589793  # Pi\n\nprint(f\"Original value (Python float64): {test_value}\")\nprint(\"\\nHow different precisions store œÄ:\")\nprint(\"-\" * 50)\n\nfor dtype in ['FP32', 'FP16', 'BF16']:\n    stored, error = show_float_representation(test_value, dtype)\n    print(f\"{dtype:>6}: {stored:20.15f}  (error: {error:.2e})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory comparison for model weights\n",
    "# This is the key insight: fewer bits = smaller models\n",
    "\n",
    "model_sizes_billions = [1, 3, 7, 13, 34, 70]  # Common LLM sizes\n",
    "\n",
    "# Bytes per parameter for each data type\n",
    "bytes_per_param = {\n",
    "    'FP32': 4,\n",
    "    'FP16': 2,\n",
    "    'BF16': 2,\n",
    "    'INT8': 1,\n",
    "    'INT4': 0.5,\n",
    "    'FP4': 0.5,\n",
    "}\n",
    "\n",
    "print(\"Model Memory Requirements (GB) by Precision\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Model Size':<12}\", end=\"\")\n",
    "for dtype in bytes_per_param:\n",
    "    print(f\"{dtype:>10}\", end=\"\")\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "\n",
    "for params_b in model_sizes_billions:\n",
    "    print(f\"{params_b}B{'':<9}\", end=\"\")\n",
    "    for dtype, bpp in bytes_per_param.items():\n",
    "        size_gb = (params_b * 1e9 * bpp) / 1e9\n",
    "        print(f\"{size_gb:>10.1f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüí° Key Insight: DGX Spark has 128GB unified memory!\")\n",
    "print(\"   - FP16: Can fit up to ~64B parameters\")\n",
    "print(\"   - INT4: Can fit up to ~256B parameters!\")\n",
    "print(\"   - FP4 (Blackwell exclusive): Same as INT4, but better quality!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "We calculated how much memory different model sizes require at various precisions:\n",
    "\n",
    "1. **FP32 ‚Üí FP16**: Cuts memory in half with almost no quality loss. This is why FP16 is the standard for training.\n",
    "\n",
    "2. **FP16 ‚Üí INT8**: Another 50% reduction. Works great for inference.\n",
    "\n",
    "3. **INT8 ‚Üí INT4/FP4**: Another 50%! This is where DGX Spark shines with hardware FP4 support.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Quantization in Action\n",
    "\n",
    "Let's actually quantize some tensors and see what happens to the values.\n",
    "\n",
    "### Understanding Quantization Math\n",
    "\n",
    "At its core, quantization maps continuous floating-point values to discrete integers:\n",
    "\n",
    "```\n",
    "quantized_value = round((float_value - zero_point) / scale)\n",
    "dequantized_value = quantized_value * scale + zero_point\n",
    "```\n",
    "\n",
    "The `scale` and `zero_point` parameters determine how we map the float range to the integer range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple quantization demonstration\n",
    "\n",
    "def quantize_tensor(tensor, bits=8):\n",
    "    \"\"\"\n",
    "    Symmetric quantization of a tensor.\n",
    "    \n",
    "    Args:\n",
    "        tensor: Input float tensor\n",
    "        bits: Number of bits for quantization (4 or 8)\n",
    "    \n",
    "    Returns:\n",
    "        quantized: Integer tensor\n",
    "        scale: Scale factor for dequantization\n",
    "    \"\"\"\n",
    "    # Symmetric quantization: map [-max_val, max_val] to [-2^(bits-1), 2^(bits-1)-1]\n",
    "    max_val = tensor.abs().max()\n",
    "    qmax = 2 ** (bits - 1) - 1  # 127 for INT8, 7 for INT4\n",
    "    \n",
    "    scale = max_val / qmax\n",
    "    \n",
    "    # Quantize\n",
    "    quantized = torch.round(tensor / scale).to(torch.int8)\n",
    "    \n",
    "    return quantized, scale\n",
    "\n",
    "\n",
    "def dequantize_tensor(quantized, scale):\n",
    "    \"\"\"Convert quantized tensor back to float.\"\"\"\n",
    "    return quantized.float() * scale\n",
    "\n",
    "\n",
    "# Create a sample weight tensor (like from a neural network layer)\n",
    "torch.manual_seed(42)\n",
    "original_weights = torch.randn(4, 4)  # Small example for visibility\n",
    "\n",
    "print(\"Original Weights (FP32):\")\n",
    "print(original_weights)\n",
    "print(f\"\\nMemory: {original_weights.numel() * 4} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize to INT8\n",
    "quantized_int8, scale_int8 = quantize_tensor(original_weights, bits=8)\n",
    "\n",
    "print(\"INT8 Quantized Weights:\")\n",
    "print(quantized_int8)\n",
    "print(f\"Scale factor: {scale_int8:.6f}\")\n",
    "print(f\"\\nMemory: {quantized_int8.numel() * 1} bytes (4x smaller!)\")\n",
    "\n",
    "# Dequantize and check error\n",
    "dequantized_int8 = dequantize_tensor(quantized_int8, scale_int8)\n",
    "error_int8 = (original_weights - dequantized_int8).abs()\n",
    "\n",
    "print(f\"\\nReconstruction Error (mean): {error_int8.mean():.6f}\")\n",
    "print(f\"Reconstruction Error (max):  {error_int8.max():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare INT8 vs INT4\n",
    "quantized_int4, scale_int4 = quantize_tensor(original_weights, bits=4)\n",
    "\n",
    "print(\"INT4 Quantized Weights:\")\n",
    "print(quantized_int4)  # Note: values range from -7 to 7\n",
    "print(f\"Scale factor: {scale_int4:.6f}\")\n",
    "\n",
    "# Dequantize and check error\n",
    "dequantized_int4 = dequantize_tensor(quantized_int4, scale_int4)\n",
    "error_int4 = (original_weights - dequantized_int4).abs()\n",
    "\n",
    "print(f\"\\nReconstruction Error (mean): {error_int4.mean():.6f}\")\n",
    "print(f\"Reconstruction Error (max):  {error_int4.max():.6f}\")\n",
    "\n",
    "# Comparison\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Comparison: INT8 vs INT4\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Metric':<25} {'INT8':>12} {'INT4':>12}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Memory (bytes)':<25} {quantized_int8.numel():>12} {quantized_int8.numel() // 2:>12}\")\n",
    "print(f\"{'Mean Error':<25} {error_int8.mean():>12.6f} {error_int4.mean():>12.6f}\")\n",
    "print(f\"{'Max Error':<25} {error_int8.max():>12.6f} {error_int4.max():>12.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "1. **INT8 quantization** maps float values to 256 possible integers (-128 to 127), introducing small rounding errors.\n",
    "\n",
    "2. **INT4 quantization** uses only 16 possible values (-8 to 7), causing larger errors but using half the memory.\n",
    "\n",
    "3. **The key insight**: Even with 16√ó compression (FP32‚ÜíINT4), the fundamental \"shape\" of the weights is preserved!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Quantizing a Real Model\n",
    "\n",
    "Let's move from toy examples to real models. We'll compare a model at different precision levels.\n",
    "\n",
    "### Loading Models at Different Precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory monitoring utility for DGX Spark\n",
    "import subprocess\n",
    "import gc\n",
    "\n",
    "def get_gpu_memory():\n",
    "    \"\"\"Get current GPU memory usage in GB.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        return allocated, reserved\n",
    "    return 0, 0\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU memory cache.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Memory utilities loaded!\")\n",
    "allocated, reserved = get_gpu_memory()\n",
    "print(f\"Current GPU memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small model to demonstrate precision differences\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# We'll use a small model first - GPT-2 (124M parameters)\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "print(\"Loading GPT-2 at different precisions...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Clear any existing models\n",
    "clear_memory()\n",
    "\n",
    "# Load tokenizer (shared across all precision levels)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Now let's actually compare loading models at different precisions\nfrom transformers import AutoModelForCausalLM\n\n# We'll use a small model for demonstration\nmodel_name = \"gpt2\"  # ~500MB, good for quick demo\n\nprint(\"Loading models at different precisions...\")\nprint(\"=\" * 60)\n\n# Get baseline memory\ninitial_mem = get_gpu_memory()[0]\n\n# FP32 Model\nprint(\"\\n1. Loading FP32 model...\")\nmodel_fp32 = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float32,\n    device_map=\"cuda\"\n)\n\nfp32_mem = get_gpu_memory()[0] - initial_mem\nprint(f\"   FP32 Memory: {fp32_mem:.2f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# FP16 Model\nprint(\"\\n2. Loading FP16 model...\")\n\n# Defensive check: ensure fp32_mem is defined from previous cell\nif 'fp32_mem' not in dir() or fp32_mem is None or fp32_mem == 0:\n    # Fallback estimate based on GPT-2 parameter count\n    fp32_mem = 0.5  # ~500MB for GPT-2 in FP32\n\ndel model_fp32\nclear_memory()\ninitial_mem = get_gpu_memory()[0]\n\nmodel_fp16 = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map=\"cuda\"\n)\n\nfp16_mem = get_gpu_memory()[0] - initial_mem\nprint(f\"   FP16 Memory: {fp16_mem:.2f} GB\")\nprint(f\"   Savings vs FP32: {(1 - fp16_mem/fp32_mem) * 100:.1f}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# BF16 Model (preferred for Blackwell architecture)\nprint(\"\\n3. Loading BF16 model (recommended for DGX Spark)...\")\n\ndel model_fp16\nclear_memory()\ninitial_mem = get_gpu_memory()[0]\n\nmodel_bf16 = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,  # Native Blackwell support\n    device_map=\"cuda\"\n)\n\nbf16_mem = get_gpu_memory()[0] - initial_mem\nprint(f\"   BF16 Memory: {bf16_mem:.2f} GB\")\nprint(f\"   Savings vs FP32: {(1 - bf16_mem/fp32_mem) * 100:.1f}%\")\nprint(f\"   Note: BF16 has native tensor core support on Blackwell!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INT8 Model using bitsandbytes\n",
    "print(\"\\n4. Loading INT8 model (8-bit quantization)...\")\n",
    "del model_bf16\n",
    "clear_memory()\n",
    "initial_mem = get_gpu_memory()[0]\n",
    "\n",
    "try:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    \n",
    "    quantization_config_8bit = BitsAndBytesConfig(\n",
    "        load_in_8bit=True\n",
    "    )\n",
    "    \n",
    "    model_int8 = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config_8bit,\n",
    "        device_map=\"cuda\"\n",
    "    )\n",
    "    \n",
    "    int8_mem = get_gpu_memory()[0] - initial_mem\n",
    "    print(f\"   INT8 Memory: {int8_mem:.2f} GB\")\n",
    "    print(f\"   Savings vs FP32: {(1 - int8_mem/fp32_mem) * 100:.1f}%\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"   Note: bitsandbytes not installed. Run: pip install bitsandbytes\")\n",
    "    model_int8 = None\n",
    "    int8_mem = fp32_mem / 4  # Estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# INT4 Model using bitsandbytes\nprint(\"\\n5. Loading INT4 model (4-bit quantization)...\")\n\n# Safely clean up previous model if it exists\nif 'model_int8' in dir() and model_int8 is not None:\n    del model_int8\nclear_memory()\ninitial_mem = get_gpu_memory()[0]\n\ntry:\n    from transformers import BitsAndBytesConfig\n    \n    quantization_config_4bit = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.bfloat16,  # Use BF16 for compute on Blackwell\n        bnb_4bit_quant_type=\"nf4\"  # NormalFloat4 - optimized for weights\n    )\n    \n    model_int4 = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=quantization_config_4bit,\n        device_map=\"cuda\"\n    )\n    \n    int4_mem = get_gpu_memory()[0] - initial_mem\n    print(f\"   INT4 Memory: {int4_mem:.2f} GB\")\n    print(f\"   Savings vs FP32: {(1 - int4_mem/fp32_mem) * 100:.1f}%\")\n    \nexcept ImportError:\n    print(\"   Note: bitsandbytes not installed. Run: pip install bitsandbytes\")\n    model_int4 = None\n    int4_mem = fp32_mem / 8  # Estimated"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "# Guard against missing baseline\n",
    "if 'fp32_mem' not in dir() or fp32_mem is None or fp32_mem == 0:\n",
    "    print(\"‚ö†Ô∏è  FP32 baseline not available (previous cell may have failed).\")\n",
    "    print(\"   Using estimated values for comparison.\")\n",
    "    fp32_mem = 0.5  # Estimated for GPT-2\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Memory Comparison Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Precision':<15} {'Memory (GB)':>12} {'vs FP32':>12} {'Compression':>12}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'FP32':<15} {fp32_mem:>12.3f} {'baseline':>12} {'1.0x':>12}\")\n",
    "\n",
    "# Safe division helper to avoid ZeroDivisionError\n",
    "def safe_ratio(a, b, default=\"N/A\"):\n",
    "    return f\"{a/b:.1f}x\" if b > 0 else default\n",
    "\n",
    "def safe_percent(a, b, default=\"N/A\"):\n",
    "    return f\"{(1-a/b)*100:.0f}% less\" if b > 0 else default\n",
    "\n",
    "print(f\"{'FP16':<15} {fp16_mem:>12.3f} {safe_percent(fp16_mem, fp32_mem):>12} {safe_ratio(fp32_mem, fp16_mem):>12}\")\n",
    "print(f\"{'BF16':<15} {bf16_mem:>12.3f} {safe_percent(bf16_mem, fp32_mem):>12} {safe_ratio(fp32_mem, bf16_mem):>12}\")\n",
    "print(f\"{'INT8':<15} {int8_mem:>12.3f} {safe_percent(int8_mem, fp32_mem):>12} {safe_ratio(fp32_mem, int8_mem):>12}\")\n",
    "print(f\"{'INT4':<15} {int4_mem:>12.3f} {safe_percent(int4_mem, fp32_mem):>12} {safe_ratio(fp32_mem, int4_mem):>12}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Measuring Quality - Perplexity\n",
    "\n",
    "Memory savings are useless if the model becomes terrible! Let's measure quality using **perplexity**.\n",
    "\n",
    "### üßí ELI5: What is Perplexity?\n",
    "\n",
    "> **Imagine you're playing a guessing game...**\n",
    ">\n",
    "> I say \"The cat sat on the ___\". How many reasonable words could fill the blank?\n",
    "> - If you're confused and think it could be 100 different words ‚Üí high perplexity\n",
    "> - If you're confident it's \"mat\" or \"couch\" ‚Üí low perplexity\n",
    ">\n",
    "> **In AI terms:** Perplexity measures how \"surprised\" a model is by text.  \n",
    "> Lower perplexity = model predicts words better = higher quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity calculation function\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, texts, max_length=512):\n",
    "    \"\"\"\n",
    "    Calculate perplexity of a model on given texts.\n",
    "    \n",
    "    Perplexity = exp(average negative log-likelihood)\n",
    "    Lower is better!\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        texts: List of text strings\n",
    "        max_length: Maximum sequence length\n",
    "    \n",
    "    Returns:\n",
    "        float: The perplexity score\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Calculating perplexity\"):\n",
    "            # Tokenize\n",
    "            encodings = tokenizer(\n",
    "                text,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                max_length=max_length\n",
    "            )\n",
    "            \n",
    "            input_ids = encodings.input_ids.to(model.device)\n",
    "            \n",
    "            # Skip very short sequences\n",
    "            if input_ids.size(1) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Get model outputs\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            \n",
    "            # Accumulate loss\n",
    "            loss = outputs.loss.item()\n",
    "            num_tokens = input_ids.size(1) - 1  # Exclude first token\n",
    "            \n",
    "            total_loss += loss * num_tokens\n",
    "            total_tokens += num_tokens\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "print(\"Perplexity function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample evaluation texts\n",
    "# In practice, you'd use a proper benchmark like WikiText or C4\n",
    "\n",
    "sample_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog. This sentence contains every letter of the alphabet.\",\n",
    "    \"Machine learning is a subset of artificial intelligence that enables computers to learn from data.\",\n",
    "    \"The capital of France is Paris, which is known for the Eiffel Tower and its rich cultural heritage.\",\n",
    "    \"In the year 2024, large language models became increasingly sophisticated and widely deployed.\",\n",
    "    \"Neural networks consist of layers of interconnected nodes that process information in a hierarchical manner.\",\n",
    "    \"The Python programming language is widely used in data science and machine learning applications.\",\n",
    "    \"Quantization reduces the precision of neural network weights to decrease memory usage and increase speed.\",\n",
    "    \"The transformer architecture, introduced in 2017, revolutionized natural language processing.\",\n",
    "    \"Deep learning has enabled significant advances in computer vision, speech recognition, and translation.\",\n",
    "    \"The DGX Spark platform provides 128GB of unified memory for running large AI models efficiently.\"\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(sample_texts)} sample texts for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare perplexity across different precision levels\n",
    "# Note: For fair comparison, we'll load fresh models for each\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Test INT4 model (already loaded)\n",
    "if model_int4 is not None:\n",
    "    print(\"Testing INT4 model...\")\n",
    "    ppl_int4 = calculate_perplexity(model_int4, tokenizer, sample_texts)\n",
    "    results['INT4'] = ppl_int4\n",
    "    print(f\"INT4 Perplexity: {ppl_int4:.2f}\")\n",
    "\n",
    "# Clean up and load FP16 for comparison\n",
    "if model_int4 is not None:\n",
    "    del model_int4\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test FP16 model (baseline quality)\n",
    "print(\"Testing FP16 model...\")\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "\n",
    "ppl_fp16 = calculate_perplexity(model_fp16, tokenizer, sample_texts)\n",
    "results['FP16'] = ppl_fp16\n",
    "print(f\"FP16 Perplexity: {ppl_fp16:.2f}\")\n",
    "\n",
    "del model_fp16\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test INT8 model\nprint(\"Testing INT8 model...\")\ntry:\n    # Re-import to ensure availability even if earlier cells failed\n    from transformers import BitsAndBytesConfig\n    \n    model_int8 = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n        device_map=\"cuda\"\n    )\n    \n    ppl_int8 = calculate_perplexity(model_int8, tokenizer, sample_texts)\n    results['INT8'] = ppl_int8\n    print(f\"INT8 Perplexity: {ppl_int8:.2f}\")\n    \n    del model_int8\n    clear_memory()\nexcept Exception as e:\n    print(f\"INT8 test skipped: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of quality comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Quality Comparison: Perplexity by Precision\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Precision':<15} {'Perplexity':>15} {'vs FP16':>15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "baseline = results.get('FP16', 0)\n",
    "\n",
    "for precision in ['FP16', 'INT8', 'INT4']:\n",
    "    if precision in results:\n",
    "        ppl = results[precision]\n",
    "        if precision == 'FP16':\n",
    "            diff = \"baseline\"\n",
    "        else:\n",
    "            diff = f\"+{ppl - baseline:.2f}\"\n",
    "        print(f\"{precision:<15} {ppl:>15.2f} {diff:>15}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"   - Lower perplexity = Better quality\")\n",
    "print(\"   - Difference of <1.0 is typically acceptable\")\n",
    "print(\"   - Difference of <0.5 is excellent (production-ready)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Speed Comparison\n",
    "\n",
    "Quantization not only saves memory but can also speed up inference. Let's measure it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_inference(model, tokenizer, prompt, num_tokens=50, num_runs=5):\n",
    "    \"\"\"\n",
    "    Benchmark inference speed.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains tokens_per_second, latency_ms, and memory_gb\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(**inputs, max_new_tokens=10, do_sample=False)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for _ in range(num_runs):\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=num_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        end = time.perf_counter()\n",
    "        times.append(end - start)\n",
    "    \n",
    "    avg_time = sum(times) / len(times)\n",
    "    tokens_per_second = num_tokens / avg_time\n",
    "    \n",
    "    return {\n",
    "        'tokens_per_second': tokens_per_second,\n",
    "        'latency_ms': avg_time * 1000,\n",
    "        'memory_gb': get_gpu_memory()[0]\n",
    "    }\n",
    "\n",
    "print(\"Benchmark function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmarks\n",
    "benchmark_prompt = \"The future of artificial intelligence is\"\n",
    "benchmark_results = {}\n",
    "\n",
    "print(\"Running inference benchmarks...\")\n",
    "print(f\"Prompt: '{benchmark_prompt}'\")\n",
    "print(f\"Generating 50 tokens, 5 runs each\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark FP16\n",
    "print(\"Benchmarking FP16...\")\n",
    "clear_memory()\n",
    "\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "\n",
    "benchmark_results['FP16'] = benchmark_inference(model_fp16, tokenizer, benchmark_prompt)\n",
    "print(f\"  Tokens/sec: {benchmark_results['FP16']['tokens_per_second']:.1f}\")\n",
    "print(f\"  Memory: {benchmark_results['FP16']['memory_gb']:.2f} GB\")\n",
    "\n",
    "del model_fp16\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark INT8\n",
    "print(\"Benchmarking INT8...\")\n",
    "\n",
    "try:\n",
    "    model_int8 = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    "        device_map=\"cuda\"\n",
    "    )\n",
    "    \n",
    "    benchmark_results['INT8'] = benchmark_inference(model_int8, tokenizer, benchmark_prompt)\n",
    "    print(f\"  Tokens/sec: {benchmark_results['INT8']['tokens_per_second']:.1f}\")\n",
    "    print(f\"  Memory: {benchmark_results['INT8']['memory_gb']:.2f} GB\")\n",
    "    \n",
    "    del model_int8\n",
    "    clear_memory()\n",
    "except Exception as e:\n",
    "    print(f\"  Skipped: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark INT4\n",
    "print(\"Benchmarking INT4...\")\n",
    "\n",
    "try:\n",
    "    model_int4 = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        ),\n",
    "        device_map=\"cuda\"\n",
    "    )\n",
    "    \n",
    "    benchmark_results['INT4'] = benchmark_inference(model_int4, tokenizer, benchmark_prompt)\n",
    "    print(f\"  Tokens/sec: {benchmark_results['INT4']['tokens_per_second']:.1f}\")\n",
    "    print(f\"  Memory: {benchmark_results['INT4']['memory_gb']:.2f} GB\")\n",
    "    \n",
    "    del model_int4\n",
    "    clear_memory()\n",
    "except Exception as e:\n",
    "    print(f\"  Skipped: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL COMPARISON: Memory, Speed, and Quality\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Precision':<12} {'Memory (GB)':>12} {'Tokens/sec':>12} {'Perplexity':>12} {'Speedup':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "fp16_speed = benchmark_results.get('FP16', {}).get('tokens_per_second', 1)\n",
    "\n",
    "for precision in ['FP16', 'INT8', 'INT4']:\n",
    "    if precision in benchmark_results:\n",
    "        br = benchmark_results[precision]\n",
    "        ppl = results.get(precision, 'N/A')\n",
    "        if isinstance(ppl, float):\n",
    "            ppl = f\"{ppl:.2f}\"\n",
    "        speedup = br['tokens_per_second'] / fp16_speed\n",
    "        \n",
    "        print(f\"{precision:<12} {br['memory_gb']:>12.2f} {br['tokens_per_second']:>12.1f} {ppl:>12} {speedup:>9.2f}x\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüéâ You've just compared quantization techniques on real models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Quantization Math\n",
    "\n",
    "Implement asymmetric quantization (with zero-point) and compare it to symmetric quantization.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "Asymmetric quantization uses:\n",
    "```python\n",
    "scale = (max_val - min_val) / (qmax - qmin)\n",
    "zero_point = round(-min_val / scale)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement asymmetric quantization\n",
    "\n",
    "def asymmetric_quantize(tensor, bits=8):\n",
    "    \"\"\"\n",
    "    Asymmetric quantization with zero-point.\n",
    "    \n",
    "    Args:\n",
    "        tensor: Input float tensor\n",
    "        bits: Number of bits (default: 8)\n",
    "    \n",
    "    Returns:\n",
    "        quantized: Quantized tensor\n",
    "        scale: Scale factor\n",
    "        zero_point: Zero point offset\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "# test_tensor = torch.tensor([0.0, 0.5, 1.0, 1.5, 2.0])  # All positive values\n",
    "# q, s, zp = asymmetric_quantize(test_tensor)\n",
    "# print(f\"Quantized: {q}\")\n",
    "# print(f\"Scale: {s}, Zero point: {zp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Larger Model Comparison\n",
    "\n",
    "Try the same experiments with a larger model (e.g., `meta-llama/Llama-2-7b-hf`). \n",
    "How do the memory savings and quality compare?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "You'll need to:\n",
    "1. Log in to Hugging Face (`huggingface-cli login`)\n",
    "2. Accept the Llama 2 license agreement\n",
    "3. Use the same code patterns from above\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test with a larger model\n",
    "# large_model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Ignoring Calibration Data\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Using random data for calibration\n",
    "calibration_data = torch.randn(100, 512)\n",
    "\n",
    "# ‚úÖ Right: Use representative data from your domain\n",
    "calibration_data = load_samples_from_training_data()\n",
    "```\n",
    "\n",
    "**Why:** Quantization needs to see the actual value distributions in your weights and activations. Random data leads to poor scale factors.\n",
    "\n",
    "### Mistake 2: Quantizing Without Evaluation\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Just quantize and deploy\n",
    "model = quantize(model)\n",
    "deploy(model)\n",
    "\n",
    "# ‚úÖ Right: Always measure quality first\n",
    "model = quantize(model)\n",
    "ppl_original = evaluate(original_model)\n",
    "ppl_quantized = evaluate(model)\n",
    "if ppl_quantized - ppl_original < 0.5:  # Acceptable threshold\n",
    "    deploy(model)\n",
    "```\n",
    "\n",
    "**Why:** Some models/tasks are more sensitive to quantization than others.\n",
    "\n",
    "### Mistake 3: Wrong Compute Dtype\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Using FP32 compute for quantized weights\n",
    "config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float32  # Slow!\n",
    ")\n",
    "\n",
    "# ‚úÖ Right: Use BF16 on Blackwell for best performance\n",
    "config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # Native Blackwell support\n",
    ")\n",
    "```\n",
    "\n",
    "**Why:** The compute dtype affects inference speed. BF16 has native tensor core support on Blackwell.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "\n",
    "- ‚úÖ **Data types matter**: FP32‚ÜíFP16‚ÜíINT8‚ÜíINT4 each halve memory\n",
    "- ‚úÖ **Quantization = compression**: Map floats to fewer bits with controllable error\n",
    "- ‚úÖ **Perplexity measures quality**: Lower is better, <0.5 difference is excellent\n",
    "- ‚úÖ **Speed improves too**: Smaller data = faster memory access = faster inference\n",
    "- ‚úÖ **DGX Spark advantage**: 128GB lets you experiment with any model size\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Build a Quantization Dashboard**\n",
    "\n",
    "Create a function that takes a model and automatically:\n",
    "1. Quantizes it to FP16, INT8, and INT4\n",
    "2. Measures memory, speed, and perplexity for each\n",
    "3. Recommends the best precision based on user constraints (memory budget, quality threshold)\n",
    "\n",
    "```python\n",
    "def quantization_analysis(model_name, memory_budget_gb=8, max_ppl_increase=0.5):\n",
    "    \"\"\"\n",
    "    Analyze quantization options for a model.\n",
    "    \n",
    "    Returns recommendation based on constraints.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Further Reading\n",
    "\n",
    "- [A Survey of Quantization Methods for Efficient Neural Network Inference](https://arxiv.org/abs/2103.13630)\n",
    "- [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/abs/2208.07339)\n",
    "- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)\n",
    "- [NVIDIA Quantization Documentation](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#working-with-int8)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clear GPU memory for next notebook\nimport gc\nimport torch\n\n# Delete any remaining models safely\nfor var_name in list(globals().keys()):\n    if 'model' in var_name.lower():\n        try:\n            del globals()[var_name]\n        except (KeyError, NameError):\n            pass  # Variable already deleted or doesn't exist\n\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nprint(\"Memory cleared!\")\nif torch.cuda.is_available():\n    print(f\"GPU memory after cleanup: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, we'll dive deep into **GPTQ Quantization** - the most popular 4-bit quantization method for GPU inference!\n",
    "\n",
    "‚û°Ô∏è Continue to: [02-gptq-quantization.ipynb](02-gptq-quantization.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}