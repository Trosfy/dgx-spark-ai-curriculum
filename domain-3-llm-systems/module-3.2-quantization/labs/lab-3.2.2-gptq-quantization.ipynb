{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2.2: GPTQ Quantization\n",
    "\n",
    "**Module:** 3.2 - Model Quantization & Optimization  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand how GPTQ quantization works algorithmically\n",
    "- [ ] Quantize a model using AutoGPTQ\n",
    "- [ ] Experiment with different group sizes (32, 64, 128)\n",
    "- [ ] Evaluate the quality/speed tradeoffs\n",
    "- [ ] Save and load GPTQ quantized models\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Lab 3.2.1 (Quantization Overview)\n",
    "- Knowledge of: Basic quantization concepts, PyTorch\n",
    "- Hardware: DGX Spark with 128GB unified memory\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The Problem:** You want to run Llama 7B on a consumer GPU with 8GB VRAM.\n",
    "\n",
    "- **FP16**: 7B √ó 2 bytes = 14GB ‚Üí Doesn't fit!\n",
    "- **INT8**: 7B √ó 1 byte = 7GB ‚Üí Barely fits, no room for KV cache\n",
    "- **GPTQ 4-bit**: 7B √ó 0.5 bytes = 3.5GB ‚Üí Fits with room to spare!\n",
    "\n",
    "**Why GPTQ?**\n",
    "- Most widely adopted 4-bit quantization method\n",
    "- Thousands of pre-quantized models on Hugging Face\n",
    "- Optimized CUDA kernels for fast inference\n",
    "- Works great on DGX Spark!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßí ELI5: What is GPTQ?\n",
    "\n",
    "> **Imagine you're an artist making a mosaic...**\n",
    ">\n",
    "> You have a beautiful photograph to recreate, but you can only use 16 different colored tiles.\n",
    ">\n",
    "> **Naive approach:** Just pick the closest color for each pixel independently.\n",
    "> - Result: The mosaic looks grainy and loses detail.\n",
    ">\n",
    "> **GPTQ approach:** Start from one corner and work systematically:\n",
    "> 1. Pick the best tile for pixel 1\n",
    "> 2. **Adjust the remaining pixels** to compensate for any error\n",
    "> 3. Move to pixel 2, repeat\n",
    "> 4. The errors spread out and cancel, giving a better overall image!\n",
    ">\n",
    "> **In AI terms:** GPTQ quantizes weights one-by-one, updating remaining weights to compensate for quantization errors. This \"error compensation\" is what makes it so good!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding GPTQ Algorithm\n",
    "\n",
    "GPTQ is based on **Optimal Brain Quantization (OBQ)**, which uses second-order information (the Hessian matrix) to minimize quantization error.\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "When we quantize weight $w_i$, we introduce an error. Instead of just accepting this error, GPTQ asks:\n",
    "\n",
    "> \"How should I adjust the remaining unquantized weights to compensate?\"\n",
    "\n",
    "The answer uses the Hessian matrix: $H = X^T X$ where $X$ is the input activations.\n",
    "\n",
    "### Algorithm Overview\n",
    "\n",
    "```\n",
    "1. Collect calibration data (run forward passes)\n",
    "2. Compute Hessian matrix for each layer\n",
    "3. For each weight column:\n",
    "   a. Quantize the weight\n",
    "   b. Compute quantization error\n",
    "   c. Update remaining weights to compensate\n",
    "4. Save quantized model\n",
    "```\n",
    "\n",
    "### Group Size\n",
    "\n",
    "Instead of using one scale factor for an entire weight matrix, GPTQ uses **groups** of weights with shared scale factors.\n",
    "\n",
    "- **Group size 128:** Faster, less memory for scales, slightly lower quality\n",
    "- **Group size 64:** Balanced\n",
    "- **Group size 32:** Slower, more scales stored, higher quality\n",
    "\n",
    "Let's visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport time\n\nprint(\"=\" * 60)\nprint(\"DGX Spark Environment Check\")\nprint(\"=\" * 60)\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize group-wise quantization\n\ndef visualize_group_quantization(weights, group_sizes=[128, 64, 32]):\n    \"\"\"\n    Visualize how different group sizes affect quantization.\n    \"\"\"\n    fig, axes = plt.subplots(1, len(group_sizes) + 1, figsize=(15, 4))\n    \n    # Original weights\n    im = axes[0].imshow(weights.numpy(), cmap='RdBu', aspect='auto')\n    axes[0].set_title(f'Original\\n({weights.numel()*4/1024:.1f} KB)')\n    axes[0].set_xlabel('Columns')\n    axes[0].set_ylabel('Rows')\n    plt.colorbar(im, ax=axes[0], fraction=0.046)\n    \n    for idx, gs in enumerate(group_sizes):\n        # Reshape weights into groups\n        flat = weights.flatten()\n        num_groups = len(flat) // gs\n        grouped = flat[:num_groups * gs].reshape(-1, gs)\n        \n        # Quantize each group separately\n        scales = grouped.abs().max(dim=1, keepdim=True).values\n        scales = scales.clamp(min=1e-10)\n        quantized = torch.round(grouped / scales * 7).clamp(-8, 7)\n        dequantized = quantized * scales / 7\n        \n        # Reshape back\n        result = torch.zeros_like(weights)\n        result.flatten()[:num_groups * gs] = dequantized.flatten()\n        \n        # Calculate error and storage\n        error = (weights - result).pow(2).mean().sqrt()\n        storage = (weights.numel() * 0.5 + num_groups * 2) / 1024  # 4-bit + FP16 scales\n        \n        im = axes[idx + 1].imshow(result.numpy(), cmap='RdBu', aspect='auto')\n        axes[idx + 1].set_title(f'Group Size {gs}\\n({storage:.1f} KB, RMSE={error:.4f})')\n        axes[idx + 1].set_xlabel('Columns')\n        plt.colorbar(im, ax=axes[idx + 1], fraction=0.046)\n    \n    plt.tight_layout()\n    plt.savefig('group_quantization.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    plt.close(fig)  # Free memory from figure\n\n# Create sample weight matrix\ntorch.manual_seed(42)\nsample_weights = torch.randn(256, 256) * 0.5\n\nvisualize_group_quantization(sample_weights)\nprint(\"\\nüí° Notice: Smaller group sizes have lower RMSE but slightly more storage!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "We visualized how group size affects quantization:\n",
    "\n",
    "1. **Group size 128**: Fewer scale factors, more compression, slightly more error\n",
    "2. **Group size 64**: Balance between compression and accuracy\n",
    "3. **Group size 32**: More scale factors, less compression, lower error\n",
    "\n",
    "The difference in error is small because we're using the same 4-bit representation. The group size mainly affects how well we can capture weight distributions within each group.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Quantizing with AutoGPTQ\n",
    "\n",
    "Let's use the AutoGPTQ library to quantize a real model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install AutoGPTQ if needed\n",
    "# Note: On DGX Spark (ARM64), prefer using pre-installed NGC container packages\n",
    "\n",
    "try:\n",
    "    from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "    print(\"‚úÖ AutoGPTQ is available!\")\n",
    "except ImportError:\n",
    "    print(\"Installing AutoGPTQ...\")\n",
    "    print(\"‚ö†Ô∏è  On DGX Spark (ARM64), this may compile CUDA kernels from source.\")\n",
    "    print(\"   This can take 5-10 minutes. Please be patient...\")\n",
    "\n",
    "    import subprocess\n",
    "    result = subprocess.run(\n",
    "        [\"pip\", \"install\", \"auto-gptq\", \"--no-cache-dir\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "\n",
    "    if result.returncode != 0:\n",
    "        print(f\"‚ùå Installation failed!\")\n",
    "        print(\"Error output (last 1000 chars):\")\n",
    "        print(result.stderr[-1000:] if len(result.stderr) > 1000 else result.stderr)\n",
    "        print(\"\\nüí° Solution: Use an NGC container with AutoGPTQ pre-installed\")\n",
    "        print(\"   Or try: pip install auto-gptq --no-build-isolation\")\n",
    "        raise ImportError(\"AutoGPTQ installation failed - see error above\")\n",
    "\n",
    "    from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "    print(\"‚úÖ AutoGPTQ installed successfully!\")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare calibration data\n",
    "# GPTQ needs example inputs to calculate the Hessian matrix\n",
    "\n",
    "def get_calibration_data(tokenizer, num_samples=128, max_length=512):\n",
    "    \"\"\"\n",
    "    Generate calibration data for GPTQ quantization.\n",
    "    \n",
    "    In practice, use data similar to your inference workload!\n",
    "    \"\"\"\n",
    "    # Sample calibration texts (diverse topics)\n",
    "    calibration_texts = [\n",
    "        \"The field of machine learning has grown exponentially in recent years.\",\n",
    "        \"Artificial intelligence systems can now perform complex reasoning tasks.\",\n",
    "        \"Large language models are transforming how we interact with computers.\",\n",
    "        \"Neural networks consist of interconnected layers of artificial neurons.\",\n",
    "        \"Deep learning has enabled breakthroughs in computer vision and NLP.\",\n",
    "        \"The transformer architecture revolutionized sequence modeling in 2017.\",\n",
    "        \"Quantization reduces model size while maintaining performance.\",\n",
    "        \"GPU acceleration enables training of billion-parameter models.\",\n",
    "        \"Transfer learning allows models to leverage pre-trained knowledge.\",\n",
    "        \"Attention mechanisms help models focus on relevant information.\",\n",
    "        # Add more diverse samples for better calibration\n",
    "        \"In the year 1969, humans first landed on the moon.\",\n",
    "        \"The capital of France is Paris, known for the Eiffel Tower.\",\n",
    "        \"Python is a popular programming language for data science.\",\n",
    "        \"Climate change poses significant challenges to global ecosystems.\",\n",
    "        \"Quantum computing may revolutionize cryptography and drug discovery.\",\n",
    "        \"The stock market experienced significant volatility last quarter.\",\n",
    "        \"Healthy eating habits contribute to overall well-being.\",\n",
    "        \"Space exploration continues to expand our understanding of the universe.\",\n",
    "        \"Renewable energy sources are becoming increasingly cost-effective.\",\n",
    "        \"The history of mathematics spans thousands of years.\",\n",
    "    ]\n",
    "    \n",
    "    # Repeat and extend to get desired number of samples\n",
    "    extended_texts = (calibration_texts * ((num_samples // len(calibration_texts)) + 1))[:num_samples]\n",
    "    \n",
    "    # Tokenize\n",
    "    calibration_data = []\n",
    "    for text in extended_texts:\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=False\n",
    "        )\n",
    "        calibration_data.append(tokenized.input_ids[0].tolist())\n",
    "    \n",
    "    return calibration_data\n",
    "\n",
    "print(\"Calibration data generator defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Choose a model to quantize\n# For this tutorial, we use a smaller model. For production, use Llama 7B/13B/70B\n\nmodel_id = \"facebook/opt-350m\"  # Small model for quick demo\n# For larger models:\n# model_id = \"facebook/opt-1.3b\"\n# model_id = \"meta-llama/Llama-2-7b-hf\"  # Requires HF login\n\nprint(f\"Selected model: {model_id}\")\n\n# Load tokenizer with error handling for network issues\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    print(f\"‚úÖ Tokenizer loaded. Vocab size: {len(tokenizer)}\")\nexcept Exception as e:\n    print(f\"‚ùå Failed to load tokenizer: {e}\")\n    print(\"\\nPossible solutions:\")\n    print(\"  1. Check your internet connection\")\n    print(\"  2. Verify the model ID is correct\")\n    print(\"  3. For gated models (e.g., Llama), run: huggingface-cli login\")\n    raise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate calibration data\n",
    "print(\"Generating calibration data...\")\n",
    "calibration_data = get_calibration_data(tokenizer, num_samples=128)\n",
    "print(f\"Generated {len(calibration_data)} calibration samples\")\n",
    "print(f\"Sample lengths: {[len(s) for s in calibration_data[:5]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPTQ Configuration\n",
    "# We'll create configs for different group sizes to compare\n",
    "\n",
    "def create_gptq_config(bits=4, group_size=128, desc_act=True, sym=False):\n",
    "    \"\"\"\n",
    "    Create GPTQ quantization configuration.\n",
    "    \n",
    "    Args:\n",
    "        bits: Number of bits (4 is most common)\n",
    "        group_size: Weights per group (32, 64, 128)\n",
    "        desc_act: Use descending activation order (improves quality)\n",
    "        sym: Use symmetric quantization\n",
    "    \"\"\"\n",
    "    return BaseQuantizeConfig(\n",
    "        bits=bits,\n",
    "        group_size=group_size,\n",
    "        desc_act=desc_act,\n",
    "        sym=sym,\n",
    "        damp_percent=0.1,  # Damping for numerical stability\n",
    "    )\n",
    "\n",
    "# Create configs for comparison\n",
    "configs = {\n",
    "    'group_128': create_gptq_config(group_size=128),\n",
    "    'group_64': create_gptq_config(group_size=64),\n",
    "    'group_32': create_gptq_config(group_size=32),\n",
    "}\n",
    "\n",
    "print(\"GPTQ Configurations:\")\n",
    "for name, config in configs.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Bits: {config.bits}\")\n",
    "    print(f\"  Group size: {config.group_size}\")\n",
    "    print(f\"  Desc act: {config.desc_act}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize with group_size=128 first (fastest)\n",
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Quantizing with group_size=128...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load and quantize\n",
    "quantized_model_128 = AutoGPTQForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    configs['group_128']\n",
    ")\n",
    "\n",
    "# Perform quantization (this is where GPTQ runs)\n",
    "quantized_model_128.quantize(\n",
    "    calibration_data,\n",
    "    batch_size=4,  # Adjust based on your GPU memory\n",
    ")\n",
    "\n",
    "quant_time_128 = time.time() - start_time\n",
    "print(f\"\\nQuantization time: {quant_time_128:.1f} seconds\")\n",
    "\n",
    "# Memory usage\n",
    "mem_128 = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory used: {mem_128:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the quantized model\n",
    "import os\n",
    "\n",
    "save_dir_128 = \"./quantized_models/opt-350m-gptq-4bit-g128\"\n",
    "os.makedirs(save_dir_128, exist_ok=True)\n",
    "\n",
    "print(f\"Saving quantized model to {save_dir_128}...\")\n",
    "quantized_model_128.save_quantized(save_dir_128)\n",
    "tokenizer.save_pretrained(save_dir_128)\n",
    "\n",
    "# Check file sizes\n",
    "total_size = 0\n",
    "print(\"\\nSaved files:\")\n",
    "for f in os.listdir(save_dir_128):\n",
    "    size = os.path.getsize(os.path.join(save_dir_128, f))\n",
    "    total_size += size\n",
    "    print(f\"  {f}: {size/1e6:.2f} MB\")\n",
    "print(f\"\\nTotal: {total_size/1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up and quantize with other group sizes\n",
    "del quantized_model_128\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize with group_size=64\n",
    "print(\"=\"*60)\n",
    "print(\"Quantizing with group_size=64...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "quantized_model_64 = AutoGPTQForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    configs['group_64']\n",
    ")\n",
    "\n",
    "quantized_model_64.quantize(\n",
    "    calibration_data,\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "quant_time_64 = time.time() - start_time\n",
    "print(f\"\\nQuantization time: {quant_time_64:.1f} seconds\")\n",
    "\n",
    "save_dir_64 = \"./quantized_models/opt-350m-gptq-4bit-g64\"\n",
    "os.makedirs(save_dir_64, exist_ok=True)\n",
    "quantized_model_64.save_quantized(save_dir_64)\n",
    "tokenizer.save_pretrained(save_dir_64)\n",
    "\n",
    "del quantized_model_64\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize with group_size=32\n",
    "print(\"=\"*60)\n",
    "print(\"Quantizing with group_size=32...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "quantized_model_32 = AutoGPTQForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    configs['group_32']\n",
    ")\n",
    "\n",
    "quantized_model_32.quantize(\n",
    "    calibration_data,\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "quant_time_32 = time.time() - start_time\n",
    "print(f\"\\nQuantization time: {quant_time_32:.1f} seconds\")\n",
    "\n",
    "save_dir_32 = \"./quantized_models/opt-350m-gptq-4bit-g32\"\n",
    "os.makedirs(save_dir_32, exist_ok=True)\n",
    "quantized_model_32.save_quantized(save_dir_32)\n",
    "tokenizer.save_pretrained(save_dir_32)\n",
    "\n",
    "del quantized_model_32\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Comparing Quantized Models\n",
    "\n",
    "Let's compare the three group sizes on:\n",
    "1. Model size\n",
    "2. Inference speed\n",
    "3. Output quality (perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original FP16 model for baseline\n",
    "from transformers import AutoModelForCausalLM\n",
    "import math\n",
    "\n",
    "print(\"Loading FP16 baseline model...\")\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "\n",
    "# Calculate baseline model size\n",
    "param_count = sum(p.numel() for p in model_fp16.parameters())\n",
    "fp16_size_mb = param_count * 2 / 1e6\n",
    "print(f\"FP16 model size: {fp16_size_mb:.1f} MB ({param_count/1e6:.0f}M params)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity evaluation function\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, texts, max_length=256):\n",
    "    \"\"\"Calculate perplexity on a set of texts.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Evaluating\", leave=False):\n",
    "            encodings = tokenizer(\n",
    "                text, \n",
    "                return_tensors='pt', \n",
    "                truncation=True, \n",
    "                max_length=max_length\n",
    "            )\n",
    "            input_ids = encodings.input_ids.to(model.device)\n",
    "            \n",
    "            if input_ids.size(1) < 2:\n",
    "                continue\n",
    "            \n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss.item()\n",
    "            num_tokens = input_ids.size(1) - 1\n",
    "            \n",
    "            total_loss += loss * num_tokens\n",
    "            total_tokens += num_tokens\n",
    "    \n",
    "    return math.exp(total_loss / total_tokens)\n",
    "\n",
    "# Evaluation texts\n",
    "eval_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog in the garden.\",\n",
    "    \"Machine learning is transforming industries around the world.\",\n",
    "    \"Scientists have discovered a new species of deep-sea fish.\",\n",
    "    \"The history of ancient civilizations fascinates many scholars.\",\n",
    "    \"Technology continues to advance at an unprecedented rate.\",\n",
    "    \"Climate change affects ecosystems across the planet.\",\n",
    "    \"The stock market showed significant gains this quarter.\",\n",
    "    \"Music has the power to evoke strong emotions in listeners.\",\n",
    "    \"Space exploration opens new frontiers for humanity.\",\n",
    "    \"Healthy eating habits contribute to longevity and well-being.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate FP16 baseline\n",
    "print(\"\\nEvaluating FP16 baseline...\")\n",
    "ppl_fp16 = calculate_perplexity(model_fp16, tokenizer, eval_texts)\n",
    "print(f\"FP16 Perplexity: {ppl_fp16:.2f}\")\n",
    "\n",
    "# Clean up\n",
    "del model_fp16\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load and evaluate GPTQ models\nresults = {'FP16': {'perplexity': ppl_fp16, 'size_mb': fp16_size_mb}}\n\nfor name, save_dir in [\n    ('GPTQ-128', save_dir_128),\n    ('GPTQ-64', save_dir_64),\n    ('GPTQ-32', save_dir_32)\n]:\n    print(f\"\\nLoading {name}...\")\n    \n    # Check which format is available (safetensors or bin)\n    safetensors_exists = any(f.endswith('.safetensors') for f in os.listdir(save_dir))\n    \n    # Load quantized model with appropriate format\n    model = AutoGPTQForCausalLM.from_quantized(\n        save_dir,\n        device=\"cuda:0\",\n        use_safetensors=safetensors_exists\n    )\n    \n    # Get model size from saved files\n    size_mb = sum(\n        os.path.getsize(os.path.join(save_dir, f)) \n        for f in os.listdir(save_dir) \n        if f.endswith('.safetensors') or f.endswith('.bin')\n    ) / 1e6\n    \n    # Evaluate perplexity\n    ppl = calculate_perplexity(model, tokenizer, eval_texts)\n    \n    results[name] = {\n        'perplexity': ppl,\n        'size_mb': size_mb\n    }\n    \n    print(f\"  Size: {size_mb:.1f} MB\")\n    print(f\"  Perplexity: {ppl:.2f}\")\n    \n    del model\n    gc.collect()\n    torch.cuda.empty_cache()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GPTQ Quantization Comparison\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Model':<15} {'Size (MB)':>12} {'Perplexity':>12} {'PPL Delta':>12} {'Compression':>12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "baseline_ppl = results['FP16']['perplexity']\n",
    "baseline_size = results['FP16']['size_mb']\n",
    "\n",
    "for name, data in results.items():\n",
    "    ppl_delta = data['perplexity'] - baseline_ppl if name != 'FP16' else 0\n",
    "    compression = baseline_size / data['size_mb']\n",
    "    \n",
    "    delta_str = f\"+{ppl_delta:.2f}\" if ppl_delta > 0 else \"baseline\"\n",
    "    \n",
    "    print(f\"{name:<15} {data['size_mb']:>12.1f} {data['perplexity']:>12.2f} {delta_str:>12} {compression:>11.2f}x\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   - All GPTQ variants achieve ~4x compression\")\n",
    "print(\"   - Smaller group sizes have slightly better perplexity\")\n",
    "print(\"   - Quality degradation is minimal (<0.5 PPL typically acceptable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Inference Speed Comparison\n",
    "\n",
    "Let's benchmark generation speed for each variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_generation(model, tokenizer, prompt, num_tokens=50, num_runs=5):\n",
    "    \"\"\"\n",
    "    Benchmark text generation speed.\n",
    "    Returns tokens per second.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(**inputs, max_new_tokens=10, do_sample=False)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for _ in range(num_runs):\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=num_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        end = time.perf_counter()\n",
    "        times.append(end - start)\n",
    "    \n",
    "    avg_time = sum(times) / len(times)\n",
    "    tokens_per_second = num_tokens / avg_time\n",
    "    \n",
    "    return tokens_per_second, avg_time * 1000  # Return tok/s and latency_ms\n",
    "\n",
    "prompt = \"The future of artificial intelligence will\"\n",
    "print(f\"Benchmark prompt: '{prompt}'\")\n",
    "print(\"Generating 50 tokens, 5 runs each...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark FP16\n",
    "print(\"\\nBenchmarking FP16...\")\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "\n",
    "tok_s, latency = benchmark_generation(model_fp16, tokenizer, prompt)\n",
    "results['FP16']['tokens_per_sec'] = tok_s\n",
    "results['FP16']['latency_ms'] = latency\n",
    "print(f\"  {tok_s:.1f} tokens/sec, {latency:.0f} ms\")\n",
    "\n",
    "del model_fp16\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Benchmark GPTQ variants\nfor name, save_dir in [\n    ('GPTQ-128', save_dir_128),\n    ('GPTQ-64', save_dir_64),\n    ('GPTQ-32', save_dir_32)\n]:\n    print(f\"\\nBenchmarking {name}...\")\n    \n    # Check which format is available\n    safetensors_exists = any(f.endswith('.safetensors') for f in os.listdir(save_dir))\n    \n    model = AutoGPTQForCausalLM.from_quantized(\n        save_dir,\n        device=\"cuda:0\",\n        use_safetensors=safetensors_exists\n    )\n    \n    tok_s, latency = benchmark_generation(model, tokenizer, prompt)\n    results[name]['tokens_per_sec'] = tok_s\n    results[name]['latency_ms'] = latency\n    print(f\"  {tok_s:.1f} tokens/sec, {latency:.0f} ms\")\n    \n    del model\n    gc.collect()\n    torch.cuda.empty_cache()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive comparison\n",
    "print(\"\\n\" + \"=\"*85)\n",
    "print(\"COMPREHENSIVE GPTQ COMPARISON\")\n",
    "print(\"=\"*85)\n",
    "print(f\"{'Model':<12} {'Size (MB)':>10} {'PPL':>8} {'PPL Œî':>8} {'Tok/s':>10} {'Speedup':>10} {'Compress':>10}\")\n",
    "print(\"-\"*85)\n",
    "\n",
    "baseline_speed = results['FP16']['tokens_per_sec']\n",
    "\n",
    "for name, data in results.items():\n",
    "    ppl_delta = data['perplexity'] - baseline_ppl if name != 'FP16' else 0\n",
    "    speedup = data['tokens_per_sec'] / baseline_speed\n",
    "    compression = baseline_size / data['size_mb']\n",
    "    \n",
    "    delta_str = f\"+{ppl_delta:.2f}\" if ppl_delta > 0 else \"-\"\n",
    "    \n",
    "    print(f\"{name:<12} {data['size_mb']:>10.1f} {data['perplexity']:>8.2f} {delta_str:>8} {data['tokens_per_sec']:>10.1f} {speedup:>9.2f}x {compression:>9.2f}x\")\n",
    "\n",
    "print(\"=\"*85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize the comparison\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nmodels = list(results.keys())\ncolors = ['#2196F3', '#4CAF50', '#FF9800', '#F44336']\n\n# Size comparison\nsizes = [results[m]['size_mb'] for m in models]\naxes[0].bar(models, sizes, color=colors)\naxes[0].set_ylabel('Size (MB)')\naxes[0].set_title('Model Size')\nfor i, v in enumerate(sizes):\n    axes[0].text(i, v + 5, f'{v:.0f}', ha='center')\n\n# Perplexity comparison\nppls = [results[m]['perplexity'] for m in models]\naxes[1].bar(models, ppls, color=colors)\naxes[1].set_ylabel('Perplexity (lower is better)')\naxes[1].set_title('Quality (Perplexity)')\nfor i, v in enumerate(ppls):\n    axes[1].text(i, v + 0.5, f'{v:.1f}', ha='center')\n\n# Speed comparison\nspeeds = [results[m]['tokens_per_sec'] for m in models]\naxes[2].bar(models, speeds, color=colors)\naxes[2].set_ylabel('Tokens/second')\naxes[2].set_title('Inference Speed')\nfor i, v in enumerate(speeds):\n    axes[2].text(i, v + 2, f'{v:.0f}', ha='center')\n\nplt.tight_layout()\nplt.savefig('gptq_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()\nplt.close(fig)  # Free memory from figure"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Quantize a Larger Model\n",
    "\n",
    "Quantize OPT-1.3B or Llama-2-7B with different group sizes. How does model size affect the quality-compression tradeoff?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "Use the same code patterns, but with:\n",
    "```python\n",
    "model_id = \"facebook/opt-1.3b\"  # or \"meta-llama/Llama-2-7b-hf\"\n",
    "# You may need to reduce batch_size for larger models\n",
    "batch_size = 2\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Quantize a larger model\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Custom Calibration Data\n",
    "\n",
    "The quality of GPTQ quantization depends heavily on calibration data. Try using:\n",
    "1. Code snippets (for a coding assistant)\n",
    "2. Scientific papers (for a research assistant)\n",
    "3. Conversation logs (for a chatbot)\n",
    "\n",
    "Compare perplexity on domain-specific vs generic test data.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "Load domain-specific data from a text file:\n",
    "```python\n",
    "with open(\"code_samples.txt\") as f:\n",
    "    domain_texts = f.read().split(\"\\n\\n\")\n",
    "calibration_data = [tokenizer.encode(t) for t in domain_texts]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with domain-specific calibration data\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Insufficient Calibration Data\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Too few samples\n",
    "calibration_data = get_calibration_data(tokenizer, num_samples=10)\n",
    "\n",
    "# ‚úÖ Right: Use at least 128 samples\n",
    "calibration_data = get_calibration_data(tokenizer, num_samples=128)\n",
    "```\n",
    "\n",
    "**Why:** GPTQ needs enough samples to estimate the Hessian accurately.\n",
    "\n",
    "### Mistake 2: Wrong Model Loading for Inference\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Loading as regular model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./quantized_model\")\n",
    "\n",
    "# ‚úÖ Right: Use AutoGPTQ loader\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    \"./quantized_model\",\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Why:** GPTQ models have custom weight formats that require special loading.\n",
    "\n",
    "### Mistake 3: Ignoring desc_act\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: desc_act=False\n",
    "config = BaseQuantizeConfig(bits=4, group_size=128, desc_act=False)\n",
    "\n",
    "# ‚úÖ Right: desc_act=True for better quality\n",
    "config = BaseQuantizeConfig(bits=4, group_size=128, desc_act=True)\n",
    "```\n",
    "\n",
    "**Why:** Descending activation order processes most important weights first, improving quality.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "\n",
    "- ‚úÖ **GPTQ algorithm**: Uses Hessian-based error compensation for optimal quantization\n",
    "- ‚úÖ **Group sizes**: Smaller = better quality, larger = faster quantization\n",
    "- ‚úÖ **Calibration matters**: Representative data improves quantization quality\n",
    "- ‚úÖ **4x compression**: With minimal quality loss (<0.5 PPL typically)\n",
    "- ‚úÖ **Speedup bonus**: Quantized models often run faster!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Create a GPTQ Quantization Pipeline**\n",
    "\n",
    "Build an end-to-end function that:\n",
    "1. Takes a model name and quantization config\n",
    "2. Downloads and quantizes the model\n",
    "3. Evaluates perplexity and speed\n",
    "4. Saves with proper naming convention\n",
    "5. Uploads to Hugging Face Hub (optional)\n",
    "\n",
    "```python\n",
    "def quantize_and_publish(\n",
    "    model_id: str,\n",
    "    bits: int = 4,\n",
    "    group_size: int = 128,\n",
    "    upload_to_hub: bool = False\n",
    "):\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Further Reading\n",
    "\n",
    "- [GPTQ Paper: Accurate Post-Training Quantization](https://arxiv.org/abs/2210.17323)\n",
    "- [AutoGPTQ GitHub](https://github.com/PanQiWei/AutoGPTQ)\n",
    "- [Optimal Brain Compression](https://arxiv.org/abs/2208.11580) (OBC, GPTQ's predecessor)\n",
    "- [TheBloke's Quantized Models](https://huggingface.co/TheBloke) (Thousands of pre-quantized models!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up quantized models (optional - comment out to keep them)\n",
    "import shutil\n",
    "\n",
    "# Uncomment to delete quantized models:\n",
    "# shutil.rmtree(\"./quantized_models\", ignore_errors=True)\n",
    "\n",
    "# Clear GPU memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Cleanup complete!\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, we'll explore **AWQ (Activation-aware Weight Quantization)**, which improves on GPTQ by protecting \"salient\" weights!\n",
    "\n",
    "‚û°Ô∏è Continue to: [03-awq-quantization.ipynb](03-awq-quantization.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}