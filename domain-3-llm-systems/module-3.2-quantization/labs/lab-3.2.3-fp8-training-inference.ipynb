{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2.3: FP8 Training and Inference\n",
    "\n",
    "**Module:** 3.2 - Model Quantization & Optimization  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand FP8 E4M3 (inference) vs E5M2 (training) formats\n",
    "- [ ] Train a model using FP8 precision with Transformer Engine\n",
    "- [ ] Compare FP8 vs FP16/BF16 training speed and quality\n",
    "- [ ] Apply FP8 for inference optimization\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Lab 3.2.1 and 3.2.2\n",
    "- Hardware: DGX Spark (Blackwell for native FP8) or Hopper GPU\n",
    "- Software: NVIDIA Transformer Engine\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The Challenge:** Training large models is expensive and slow!\n",
    "- FP32 training: Maximum precision, but 4 bytes per parameter\n",
    "- FP16/BF16: 2√ó faster, but can have numerical stability issues\n",
    "\n",
    "**The Solution:** FP8 training gives you:\n",
    "- **2√ó compute throughput** vs FP16 on Tensor Cores\n",
    "- **2√ó memory efficiency** for activations\n",
    "- **Native Blackwell support** - no emulation overhead!\n",
    "- **<1% quality loss** with proper scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: Why Two FP8 Formats?\n",
    "\n",
    "> **Think of it like camera settings for different lighting...**\n",
    ">\n",
    "> **E4M3 (4 exponent, 3 mantissa)** - like your indoor camera mode:\n",
    "> - Higher precision (3 mantissa bits = more detail)\n",
    "> - Smaller range (¬±448)\n",
    "> - Best for: **weights and activations during inference**\n",
    ">\n",
    "> **E5M2 (5 exponent, 2 mantissa)** - like your outdoor camera mode:\n",
    "> - Lower precision (2 mantissa bits)\n",
    "> - Larger range (¬±57344) to handle bright/dark extremes\n",
    "> - Best for: **gradients during training** (can be very large or small)\n",
    ">\n",
    "> **In AI terms:**\n",
    "> - Inference uses E4M3 (precision matters more)\n",
    "> - Training gradients use E5M2 (range matters more)\n",
    "> - Blackwell tensor cores support BOTH natively!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding FP8 Formats\n",
    "\n",
    "Let's explore the two FP8 formats in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import gc\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FP8 Training Lab - Environment Check\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    cc = torch.cuda.get_device_capability()\n",
    "    print(f\"Compute Capability: {cc[0]}.{cc[1]}\")\n",
    "    \n",
    "    # Check FP8 support\n",
    "    if cc[0] >= 10:\n",
    "        print(\"\\nNative FP8 support: Yes (Blackwell)\")\n",
    "    elif cc[0] >= 9:\n",
    "        print(\"\\nNative FP8 support: Yes (Hopper)\")\n",
    "    else:\n",
    "        print(\"\\nNative FP8 support: No (emulation only)\")\n",
    "\n",
    "# Check for Transformer Engine\n",
    "try:\n",
    "    import transformer_engine.pytorch as te\n",
    "    from transformer_engine.common.recipe import Format, DelayedScaling\n",
    "    HAS_TE = True\n",
    "    print(f\"Transformer Engine: Available\")\n",
    "except ImportError:\n",
    "    HAS_TE = False\n",
    "    print(f\"Transformer Engine: Not installed\")\n",
    "    print(\"  Install with: pip install transformer-engine\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP8 format specifications\n",
    "# E4M3: 1 sign + 4 exponent + 3 mantissa = 8 bits\n",
    "# E5M2: 1 sign + 5 exponent + 2 mantissa = 8 bits\n",
    "\n",
    "class FP8Format:\n",
    "    def __init__(self, name, exp_bits, mant_bits):\n",
    "        self.name = name\n",
    "        self.exp_bits = exp_bits\n",
    "        self.mant_bits = mant_bits\n",
    "        self.bias = 2 ** (exp_bits - 1) - 1\n",
    "        \n",
    "        # Calculate range\n",
    "        max_exp = 2 ** exp_bits - 2  # Exclude special values\n",
    "        max_mant = (2 ** mant_bits - 1) / 2 ** mant_bits\n",
    "        self.max_value = (1 + max_mant) * 2 ** (max_exp - self.bias)\n",
    "        self.min_positive = 2 ** (1 - self.bias - mant_bits)  # Smallest subnormal\n",
    "\n",
    "E4M3 = FP8Format(\"E4M3\", 4, 3)\n",
    "E5M2 = FP8Format(\"E5M2\", 5, 2)\n",
    "\n",
    "# Comparison table\n",
    "print(\"FP8 Format Comparison\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Property':<25} {'E4M3 (Inference)':>20} {'E5M2 (Training)':>20}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Exponent bits':<25} {E4M3.exp_bits:>20} {E5M2.exp_bits:>20}\")\n",
    "print(f\"{'Mantissa bits':<25} {E4M3.mant_bits:>20} {E5M2.mant_bits:>20}\")\n",
    "print(f\"{'Exponent bias':<25} {E4M3.bias:>20} {E5M2.bias:>20}\")\n",
    "print(f\"{'Max value':<25} {E4M3.max_value:>20.1f} {E5M2.max_value:>20.1f}\")\n",
    "print(f\"{'Min positive':<25} {E4M3.min_positive:>20.2e} {E5M2.min_positive:>20.2e}\")\n",
    "print(f\"{'Dynamic range':<25} {np.log10(E4M3.max_value/E4M3.min_positive):>18.1f}x {np.log10(E5M2.max_value/E5M2.min_positive):>18.1f}x\")\n",
    "print(f\"{'Best for':<25} {'weights, activations':>20} {'gradients':>20}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize FP8 precision at different magnitudes\n",
    "\n",
    "def simulate_fp8(value, fp8_format):\n",
    "    \"\"\"Simulate FP8 quantization of a value.\"\"\"\n",
    "    # Clamp to range\n",
    "    clamped = np.clip(value, -fp8_format.max_value, fp8_format.max_value)\n",
    "    \n",
    "    # Simulate precision loss\n",
    "    # In real FP8, this is done by the hardware\n",
    "    multiplier = 2 ** fp8_format.mant_bits\n",
    "    quantized = np.round(clamped * multiplier) / multiplier\n",
    "    \n",
    "    return quantized\n",
    "\n",
    "\n",
    "# Test precision across magnitude range\n",
    "test_values = np.logspace(-3, 2, 100)  # 0.001 to 100\n",
    "\n",
    "e4m3_errors = []\n",
    "e5m2_errors = []\n",
    "\n",
    "for v in test_values:\n",
    "    e4m3_q = simulate_fp8(v, E4M3)\n",
    "    e5m2_q = simulate_fp8(v, E5M2)\n",
    "    \n",
    "    e4m3_errors.append(abs(v - e4m3_q) / v * 100)  # Relative error %\n",
    "    e5m2_errors.append(abs(v - e5m2_q) / v * 100)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.loglog(test_values, e4m3_errors, 'b-', label='E4M3', linewidth=2)\n",
    "ax.loglog(test_values, e5m2_errors, 'r-', label='E5M2', linewidth=2)\n",
    "ax.set_xlabel('Value Magnitude')\n",
    "ax.set_ylabel('Relative Error (%)')\n",
    "ax.set_title('FP8 Precision by Magnitude')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axvline(x=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.text(1.1, ax.get_ylim()[1]*0.5, 'Typical\\nweight\\nrange', fontsize=9)\n",
    "\n",
    "ax = axes[1]\n",
    "# Show representable values near 1.0\n",
    "e4m3_vals = [i/8 for i in range(1, 16)]  # 0.125 to 1.875 in steps of 0.125\n",
    "e5m2_vals = [i/4 for i in range(1, 8)]   # 0.25 to 1.75 in steps of 0.25\n",
    "\n",
    "ax.scatter(e4m3_vals, [1]*len(e4m3_vals), s=100, label='E4M3', marker='o')\n",
    "ax.scatter(e5m2_vals, [0.5]*len(e5m2_vals), s=100, label='E5M2', marker='s')\n",
    "ax.set_xlabel('Value')\n",
    "ax.set_yticks([0.5, 1])\n",
    "ax.set_yticklabels(['E5M2', 'E4M3'])\n",
    "ax.set_title('Representable Values Near 1.0')\n",
    "ax.set_xlim(0, 2)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: E4M3 has 8 values in [1,2), E5M2 has only 4.\")\n",
    "print(\"This is why E4M3 is preferred for inference (more precision).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: FP8 Training with Transformer Engine\n",
    "\n",
    "NVIDIA's Transformer Engine makes FP8 training seamless. Let's see how!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple model for demonstration\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    \"\"\"Simple MLP for demonstrating FP8 training.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=768, hidden_dim=3072, output_dim=768):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_bf16 = SimpleMLP().to(device).to(torch.bfloat16)\n",
    "\n",
    "param_count = sum(p.numel() for p in model_bf16.parameters())\n",
    "print(f\"Model parameters: {param_count / 1e6:.1f}M\")\n",
    "print(f\"Model dtype: {next(model_bf16.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If Transformer Engine is available, create an FP8 version\n",
    "\n",
    "if HAS_TE:\n",
    "    class SimpleMLP_FP8(nn.Module):\n",
    "        \"\"\"FP8-enabled MLP using Transformer Engine.\"\"\"\n",
    "        \n",
    "        def __init__(self, input_dim=768, hidden_dim=3072, output_dim=768):\n",
    "            super().__init__()\n",
    "            # Use TE's FP8-aware Linear layers\n",
    "            self.fc1 = te.Linear(input_dim, hidden_dim)\n",
    "            self.fc2 = te.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.fc1(x)\n",
    "            x = F.gelu(x)\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "    \n",
    "    # Create FP8 model\n",
    "    model_fp8 = SimpleMLP_FP8().to(device)\n",
    "    print(f\"FP8 model created with Transformer Engine!\")\n",
    "    print(f\"Parameters: {sum(p.numel() for p in model_fp8.parameters()) / 1e6:.1f}M\")\n",
    "else:\n",
    "    print(\"Transformer Engine not available. Simulating FP8 behavior.\")\n",
    "    model_fp8 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop comparison: BF16 vs FP8\n",
    "\n",
    "def train_step(model, optimizer, x, y, use_fp8=False):\n",
    "    \"\"\"Single training step.\"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if use_fp8 and HAS_TE:\n",
    "        # Use FP8 autocast context\n",
    "        fp8_recipe = DelayedScaling(\n",
    "            fp8_format=Format.HYBRID,  # E4M3 for forward, E5M2 for backward\n",
    "            amax_history_len=16,\n",
    "            amax_compute_algo=\"most_recent\",\n",
    "        )\n",
    "        \n",
    "        with te.fp8_autocast(enabled=True, fp8_recipe=fp8_recipe):\n",
    "            output = model(x)\n",
    "            loss = F.mse_loss(output, y)\n",
    "    else:\n",
    "        output = model(x)\n",
    "        loss = F.mse_loss(output, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def benchmark_training(model, num_steps=100, batch_size=32, use_fp8=False):\n",
    "    \"\"\"Benchmark training speed.\"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # Generate dummy data\n",
    "    x = torch.randn(batch_size, 768, device=device, dtype=torch.bfloat16)\n",
    "    y = torch.randn(batch_size, 768, device=device, dtype=torch.bfloat16)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        _ = train_step(model, optimizer, x, y, use_fp8=use_fp8)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    losses = []\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        loss = train_step(model, optimizer, x, y, use_fp8=use_fp8)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    elapsed = time.perf_counter() - start\n",
    "    \n",
    "    return {\n",
    "        'time_per_step_ms': elapsed / num_steps * 1000,\n",
    "        'steps_per_second': num_steps / elapsed,\n",
    "        'final_loss': losses[-1],\n",
    "        'avg_loss': sum(losses) / len(losses),\n",
    "    }\n",
    "\n",
    "\n",
    "# Benchmark BF16\n",
    "print(\"Benchmarking BF16 training...\")\n",
    "bf16_results = benchmark_training(model_bf16, num_steps=100, use_fp8=False)\n",
    "\n",
    "print(f\"\\nBF16 Results:\")\n",
    "print(f\"  Time per step: {bf16_results['time_per_step_ms']:.2f} ms\")\n",
    "print(f\"  Steps/second: {bf16_results['steps_per_second']:.1f}\")\n",
    "print(f\"  Final loss: {bf16_results['final_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark FP8 if available\n",
    "\n",
    "if model_fp8 is not None:\n",
    "    print(\"Benchmarking FP8 training...\")\n",
    "    fp8_results = benchmark_training(model_fp8, num_steps=100, use_fp8=True)\n",
    "    \n",
    "    print(f\"\\nFP8 Results:\")\n",
    "    print(f\"  Time per step: {fp8_results['time_per_step_ms']:.2f} ms\")\n",
    "    print(f\"  Steps/second: {fp8_results['steps_per_second']:.1f}\")\n",
    "    print(f\"  Final loss: {fp8_results['final_loss']:.4f}\")\n",
    "    \n",
    "    # Comparison\n",
    "    speedup = bf16_results['time_per_step_ms'] / fp8_results['time_per_step_ms']\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Speedup: {speedup:.2f}x faster with FP8!\")\n",
    "    print(f\"Loss difference: {abs(fp8_results['final_loss'] - bf16_results['final_loss']):.6f}\")\n",
    "    print(f\"{'='*50}\")\n",
    "else:\n",
    "    print(\"\\nFP8 benchmark skipped (Transformer Engine not available).\")\n",
    "    print(\"\\nExpected FP8 performance on Blackwell:\")\n",
    "    print(\"  - 1.5-2x speedup vs BF16\")\n",
    "    print(\"  - 2x reduced activation memory\")\n",
    "    print(\"  - <1% loss difference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: FP8 Inference Optimization\n",
    "\n",
    "FP8 also shines for inference with E4M3 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert model weights to FP8 for inference\n",
    "\n",
    "def simulate_fp8_inference(model, x, fp8_format=E4M3):\n",
    "    \"\"\"\n",
    "    Simulate FP8 inference by quantizing weights.\n",
    "    \n",
    "    In practice, this is done by TensorRT or the hardware.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Store original weights\n",
    "    original_weights = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Quantize each weight tensor\n",
    "        for name, param in model.named_parameters():\n",
    "            original_weights[name] = param.data.clone()\n",
    "            \n",
    "            # Simulate FP8 quantization\n",
    "            max_val = param.abs().max()\n",
    "            scale = max_val / fp8_format.max_value\n",
    "            scale = max(scale.item(), 1e-10)\n",
    "            \n",
    "            # Quantize and dequantize\n",
    "            quantized = torch.round(param / scale * (2 ** fp8_format.mant_bits)) / (2 ** fp8_format.mant_bits) * scale\n",
    "            param.data = quantized\n",
    "        \n",
    "        # Run inference\n",
    "        output = model(x)\n",
    "        \n",
    "        # Restore original weights\n",
    "        for name, param in model.named_parameters():\n",
    "            param.data = original_weights[name]\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "# Test FP8 vs BF16 inference\n",
    "test_input = torch.randn(32, 768, device=device, dtype=torch.bfloat16)\n",
    "\n",
    "model_bf16.eval()\n",
    "with torch.no_grad():\n",
    "    bf16_output = model_bf16(test_input)\n",
    "    fp8_output = simulate_fp8_inference(model_bf16, test_input)\n",
    "\n",
    "# Compare outputs\n",
    "diff = (bf16_output - fp8_output).abs()\n",
    "print(\"FP8 Inference Quality (Simulated)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Mean absolute difference: {diff.mean():.6f}\")\n",
    "print(f\"Max absolute difference: {diff.max():.6f}\")\n",
    "print(f\"Relative error: {(diff.mean() / bf16_output.abs().mean() * 100):.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark FP8 inference performance\n",
    "\n",
    "def benchmark_inference(model, num_runs=100, batch_size=32, input_dim=768):\n",
    "    \"\"\"Benchmark inference speed.\"\"\"\n",
    "    model.eval()\n",
    "    x = torch.randn(batch_size, input_dim, device=device, dtype=torch.bfloat16)\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = model(x)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            _ = model(x)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    elapsed = time.perf_counter() - start\n",
    "    \n",
    "    return {\n",
    "        'time_per_inference_ms': elapsed / num_runs * 1000,\n",
    "        'inferences_per_second': num_runs / elapsed,\n",
    "        'throughput_samples_per_second': batch_size * num_runs / elapsed,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Benchmarking inference...\")\n",
    "inf_results = benchmark_inference(model_bf16, num_runs=100)\n",
    "\n",
    "print(f\"\\nBF16 Inference Results:\")\n",
    "print(f\"  Time per inference: {inf_results['time_per_inference_ms']:.3f} ms\")\n",
    "print(f\"  Inferences/second: {inf_results['inferences_per_second']:.1f}\")\n",
    "print(f\"  Throughput: {inf_results['throughput_samples_per_second']:.0f} samples/s\")\n",
    "\n",
    "print(\"\\nExpected FP8 inference on Blackwell:\")\n",
    "print(f\"  Time per inference: ~{inf_results['time_per_inference_ms'] / 2:.3f} ms (2x faster)\")\n",
    "print(f\"  Memory: 50% reduction in activation memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Delayed Scaling for Numerical Stability\n",
    "\n",
    "FP8's limited range requires dynamic scaling. Let's understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate delayed scaling\n",
    "\n",
    "class DelayedScalingSimulator:\n",
    "    \"\"\"\n",
    "    Simulate the delayed scaling algorithm used in FP8 training.\n",
    "    \n",
    "    The idea:\n",
    "    1. Track the maximum absolute value (amax) over recent steps\n",
    "    2. Compute scale factor based on amax history\n",
    "    3. This allows FP8 to adapt to changing tensor magnitudes\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, history_len=16, fp8_format=E4M3):\n",
    "        self.history_len = history_len\n",
    "        self.fp8_format = fp8_format\n",
    "        self.amax_history = []\n",
    "    \n",
    "    def update_and_get_scale(self, tensor):\n",
    "        \"\"\"Update history and compute scale factor.\"\"\"\n",
    "        # Get current amax\n",
    "        current_amax = tensor.abs().max().item()\n",
    "        \n",
    "        # Update history\n",
    "        self.amax_history.append(current_amax)\n",
    "        if len(self.amax_history) > self.history_len:\n",
    "            self.amax_history.pop(0)\n",
    "        \n",
    "        # Compute scale from max of history\n",
    "        amax = max(self.amax_history)\n",
    "        scale = amax / self.fp8_format.max_value\n",
    "        \n",
    "        return max(scale, 1e-10)\n",
    "    \n",
    "    def quantize(self, tensor):\n",
    "        \"\"\"Quantize tensor using delayed scaling.\"\"\"\n",
    "        scale = self.update_and_get_scale(tensor)\n",
    "        \n",
    "        # Quantize\n",
    "        scaled = tensor / scale\n",
    "        clipped = torch.clamp(scaled, -self.fp8_format.max_value, self.fp8_format.max_value)\n",
    "        quantized = torch.round(clipped * (2 ** self.fp8_format.mant_bits)) / (2 ** self.fp8_format.mant_bits)\n",
    "        \n",
    "        # Dequantize\n",
    "        return quantized * scale, scale\n",
    "\n",
    "\n",
    "# Simulate training with varying tensor magnitudes\n",
    "scaler = DelayedScalingSimulator(history_len=8)\n",
    "\n",
    "# Simulate gradients that change magnitude over time\n",
    "print(\"Delayed Scaling Simulation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Step':<8} {'Tensor Max':>15} {'Scale':>15} {'Quantization Error':>20}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for step in range(16):\n",
    "    # Simulate gradients with varying magnitude\n",
    "    magnitude = 0.1 + step * 0.1  # Gradually increasing\n",
    "    tensor = torch.randn(1000) * magnitude\n",
    "    \n",
    "    quantized, scale = scaler.quantize(tensor)\n",
    "    error = (tensor - quantized).abs().mean().item()\n",
    "    \n",
    "    print(f\"{step:<8} {tensor.abs().max().item():>15.4f} {scale:>15.6f} {error:>20.6f}\")\n",
    "\n",
    "print(\"\\nNote: Scale adapts to track the increasing tensor magnitude!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Custom FP8 Recipe\n",
    "\n",
    "Experiment with different delayed scaling parameters:\n",
    "- `history_len`: How many steps to track (try 4, 8, 16, 32)\n",
    "- What happens with rapidly changing tensor magnitudes?\n",
    "\n",
    "### Exercise 2: Compare E4M3 vs E5M2\n",
    "\n",
    "Quantize the same gradients with both formats. Which has lower error?"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Helper function for quantizing tensors with specific FP8 formats\n# ================================================================\n#\n# For the exercises below, you'll need to quantize tensors with different\n# FP8 formats (E4M3 vs E5M2) and compare the results.\n\ndef quantize_tensor_fp8(tensor, fp8_format):\n    \"\"\"\n    Quantize a tensor using a specific FP8 format.\n    \n    Args:\n        tensor: PyTorch tensor to quantize\n        fp8_format: FP8Format object (E4M3 or E5M2)\n    \n    Returns:\n        tuple: (dequantized_tensor, quantization_error)\n    \"\"\"\n    # Compute scale to fit tensor in FP8 range\n    max_val = tensor.abs().max()\n    scale = max_val / fp8_format.max_value\n    scale = max(scale.item(), 1e-10)\n    \n    # Scale tensor to FP8 range\n    scaled = tensor / scale\n    \n    # Clip to FP8 representable range\n    clipped = torch.clamp(scaled, -fp8_format.max_value, fp8_format.max_value)\n    \n    # Simulate reduced mantissa precision (round to FP8 grid)\n    multiplier = 2 ** fp8_format.mant_bits\n    quantized = torch.round(clipped * multiplier) / multiplier\n    \n    # Scale back (dequantize)\n    dequantized = quantized * scale\n    \n    # Calculate error\n    error = (tensor - dequantized).abs()\n    \n    return dequantized, error\n\n\n# Demonstrate with a sample tensor\ntorch.manual_seed(42)\nsample_data = torch.randn(1000) * 5  # Some test data\n\n# Quantize with both formats\ndeq_e4m3, err_e4m3 = quantize_tensor_fp8(sample_data, E4M3)\ndeq_e5m2, err_e5m2 = quantize_tensor_fp8(sample_data, E5M2)\n\nprint(\"quantize_tensor_fp8() Demo\")\nprint(\"=\" * 50)\nprint(f\"Input tensor range: [{sample_data.min():.3f}, {sample_data.max():.3f}]\")\nprint(f\"\\nE4M3 (3 mantissa bits - more precision):\")\nprint(f\"  Mean error: {err_e4m3.mean():.6f}\")\nprint(f\"  Max error:  {err_e4m3.max():.6f}\")\nprint(f\"\\nE5M2 (2 mantissa bits - larger range):\")\nprint(f\"  Mean error: {err_e5m2.mean():.6f}\")\nprint(f\"  Max error:  {err_e5m2.max():.6f}\")\nprint(f\"\\nE4M3 error is {err_e5m2.mean()/err_e4m3.mean():.1f}x lower (better for typical values)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Experiment with history lengths\n",
    "\n",
    "# TODO: Create scalers with different history lengths\n",
    "# TODO: Simulate rapidly changing tensor magnitudes\n",
    "# TODO: Compare quantization errors\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Compare E4M3 vs E5M2 for gradients\n",
    "\n",
    "# Simulate gradients (can have large range)\n",
    "torch.manual_seed(42)\n",
    "gradients = torch.randn(10000) * 10  # Larger magnitude gradients\n",
    "\n",
    "# TODO: Quantize with E4M3\n",
    "# TODO: Quantize with E5M2\n",
    "# TODO: Compare errors\n",
    "# TODO: Which format is better for gradients? Why?\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Using Wrong FP8 Format\n",
    "\n",
    "```python\n",
    "# Wrong: Using E5M2 for inference weights\n",
    "weights = quantize_to_fp8(weights, format=\"E5M2\")  # Less precision!\n",
    "\n",
    "# Right: Use E4M3 for inference (more precision)\n",
    "weights = quantize_to_fp8(weights, format=\"E4M3\")\n",
    "```\n",
    "\n",
    "### Mistake 2: Forgetting to Scale Gradients\n",
    "\n",
    "```python\n",
    "# Wrong: Directly clipping gradients to FP8 range\n",
    "grad_fp8 = grad.clamp(-448, 448)  # May lose large gradients!\n",
    "\n",
    "# Right: Use delayed scaling\n",
    "scale = amax_history.max() / 448\n",
    "grad_fp8 = (grad / scale).clamp(-448, 448)\n",
    "```\n",
    "\n",
    "### Mistake 3: Not Using FP8 Autocast\n",
    "\n",
    "```python\n",
    "# Wrong: Manual FP8 conversion everywhere\n",
    "x_fp8 = x.to(torch.float8_e4m3fn)\n",
    "y = model(x_fp8).to(torch.bfloat16)\n",
    "\n",
    "# Right: Use Transformer Engine's autocast\n",
    "with te.fp8_autocast(enabled=True):\n",
    "    y = model(x)  # Automatic conversion!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "\n",
    "- **E4M3 vs E5M2**: Precision vs range trade-off\n",
    "- **FP8 training**: 2√ó speedup with Transformer Engine\n",
    "- **Delayed scaling**: How FP8 adapts to changing magnitudes\n",
    "- **Blackwell advantage**: Native FP8 tensor cores!\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [NVIDIA Transformer Engine](https://github.com/NVIDIA/TransformerEngine)\n",
    "- [FP8 Formats for Deep Learning](https://arxiv.org/abs/2209.05433)\n",
    "- [Mixed Precision Training](https://arxiv.org/abs/1710.03740)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del model_bf16\n",
    "if model_fp8 is not None:\n",
    "    del model_fp8\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Notebook complete! Ready for Lab 3.2.4: GPTQ Quantization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, we'll explore **GPTQ Quantization** - the most popular 4-bit quantization method for GPU inference!\n",
    "\n",
    "‚û°Ô∏è Continue to: [Lab 3.2.4: GPTQ Quantization](lab-3.2.4-gptq-quantization.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}