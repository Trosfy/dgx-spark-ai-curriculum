{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2.1: Data Type Exploration\n",
    "\n",
    "**Module:** 3.2 - Model Quantization & Optimization  \n",
    "**Time:** 1.5 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand different numerical data types (FP32, FP16, BF16, INT8, INT4, FP8, FP4)\n",
    "- [ ] Visualize precision loss at each quantization level\n",
    "- [ ] Calculate memory savings for various model sizes\n",
    "- [ ] Appreciate why DGX Spark's Blackwell architecture is special for quantization\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Module 3.1 (LLM Fine-tuning)\n",
    "- Knowledge of: PyTorch basics, neural network fundamentals\n",
    "- Hardware: DGX Spark with 128GB unified memory (ideal) or any CUDA GPU\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The Problem:** You've built an amazing 70B parameter model, but deploying it is incredibly expensive!\n",
    "\n",
    "Consider the math:\n",
    "- **70B parameters √ó 2 bytes (FP16) = 140GB** just for weights\n",
    "- Plus activations, KV cache, framework overhead...\n",
    "- That's $25,000+/month for a cloud GPU instance!\n",
    "\n",
    "**The Solution:** Quantization reduces memory by 2-4√ó with minimal quality loss.\n",
    "\n",
    "| Company | Use Case | Quantization Win |\n",
    "|---------|----------|------------------|\n",
    "| Google | On-device Gemini Nano | 4-bit enables running on phones |\n",
    "| Meta | Llama deployment | INT8 halves serving costs |\n",
    "| Apple | CoreML models | 16‚Üí4 bit for Neural Engine |\n",
    "| **You** | DGX Spark | NVFP4 gives 3.5√ó compression! |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What is Quantization?\n",
    "\n",
    "> **Imagine you're taking notes in class...**\n",
    ">\n",
    "> You could write down every single word the teacher says (FP32 - full precision).  \n",
    "> That's accurate, but your notebook fills up fast and your hand gets tired!\n",
    ">\n",
    "> Instead, you could:\n",
    "> - Write only the key points (FP16 - half precision)\n",
    "> - Use abbreviations like \"b/c\" for \"because\" (INT8 - 8-bit integers)\n",
    "> - Just draw simple diagrams (INT4 - 4-bit integers)\n",
    ">\n",
    "> Each shorthand:\n",
    "> - Uses less notebook space (memory)\n",
    "> - Lets you write faster (compute)\n",
    "> - Might lose some details (precision)\n",
    ">\n",
    "> **In AI terms:** Quantization is using fewer bits to store each number in a neural network.  \n",
    "> Just like your notes, smaller bits = smaller models = faster inference, but potentially less accurate.\n",
    "\n",
    "---\n",
    "\n",
    "## üßí ELI5: What's Special About FP4?\n",
    "\n",
    "> **Think of it like JPEG compression for photos...**\n",
    ">\n",
    "> Regular integer quantization (INT4) is like converting a color photo to just 16 shades of gray.  \n",
    "> It works, but important details can get lost.\n",
    ">\n",
    "> FP4 (floating-point 4-bit) is smarter! It's like JPEG - it keeps more detail where it matters  \n",
    "> and simplifies where you won't notice. The result looks almost like the original.\n",
    ">\n",
    "> **The catch?** FP4 needs special hardware to work fast.  \n",
    "> **The good news?** DGX Spark's Blackwell GPU has native FP4 support - you have that hardware!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding Data Types\n",
    "\n",
    "Neural networks are just massive collections of numbers (weights). How we store those numbers matters!\n",
    "\n",
    "### The Number Line Analogy\n",
    "\n",
    "Imagine a ruler that can measure from -1000 to +1000:\n",
    "\n",
    "| Type | Bits | Tick Marks | Precision | Use Case |\n",
    "|------|------|------------|-----------|----------|\n",
    "| FP32 | 32 | 4 billion | Highest | Training (legacy) |\n",
    "| FP16 | 16 | 65,536 | High | Training/Inference |\n",
    "| BF16 | 16 | 256 (wider range) | Medium | Training (preferred) |\n",
    "| FP8 | 8 | ~256 | Medium | Inference (Blackwell) |\n",
    "| INT8 | 8 | 256 | Medium | Inference |\n",
    "| INT4 | 4 | 16 | Low | Inference (quantized) |\n",
    "| FP4 | 4 | 16 (smarter) | Medium-Low | Inference (Blackwell!) |\n",
    "\n",
    "Let's explore each one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check our environment\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DGX Spark Environment Check\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"Total Memory: {props.total_memory / 1e9:.1f} GB\")\n",
    "    cc = torch.cuda.get_device_capability()\n",
    "    print(f\"Compute Capability: {cc[0]}.{cc[1]}\")\n",
    "    \n",
    "    # Check for special features\n",
    "    print(f\"\\nBF16 supported: {torch.cuda.is_bf16_supported()}\")\n",
    "    \n",
    "    if cc[0] >= 10:\n",
    "        print(\"FP8 native: Yes (Blackwell!)\")\n",
    "        print(\"FP4 native: Yes (Blackwell exclusive!)\")\n",
    "    elif cc[0] >= 9:\n",
    "        print(\"FP8 native: Yes (Hopper)\")\n",
    "        print(\"FP4 native: No (emulation only)\")\n",
    "    else:\n",
    "        print(\"FP8 native: No (emulation only)\")\n",
    "        print(\"FP4 native: No (emulation only)\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Floating-Point Representation\n",
    "\n",
    "A floating-point number is stored as: **(-1)^sign √ó mantissa √ó 2^exponent**\n",
    "\n",
    "```\n",
    "FP32: [1 sign][8 exponent][23 mantissa] = 32 bits\n",
    "FP16: [1 sign][5 exponent][10 mantissa] = 16 bits\n",
    "BF16: [1 sign][8 exponent][ 7 mantissa] = 16 bits (Google's format!)\n",
    "FP8:  [1 sign][4 exponent][ 3 mantissa] = 8 bits (E4M3) or\n",
    "      [1 sign][5 exponent][ 2 mantissa] = 8 bits (E5M2)\n",
    "FP4:  [1 sign][1 exponent][ 2 mantissa] = 4 bits (with scaling)\n",
    "```\n",
    "\n",
    "Let's visualize what each format can represent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the precision of different data types\n",
    "\n",
    "def analyze_dtype_precision(dtype_name, dtype):\n",
    "    \"\"\"Analyze precision of a PyTorch dtype.\"\"\"\n",
    "    # Create test value\n",
    "    pi = 3.141592653589793\n",
    "    tensor = torch.tensor([pi], dtype=dtype)\n",
    "    stored = tensor.item()\n",
    "    \n",
    "    # Calculate error\n",
    "    error = abs(pi - stored)\n",
    "    relative_error = error / pi * 100\n",
    "    \n",
    "    # Get info\n",
    "    info = torch.finfo(dtype) if dtype.is_floating_point else torch.iinfo(dtype)\n",
    "    \n",
    "    return {\n",
    "        'name': dtype_name,\n",
    "        'bits': tensor.element_size() * 8,\n",
    "        'stored_pi': stored,\n",
    "        'error': error,\n",
    "        'relative_error': relative_error,\n",
    "        'min': info.min if hasattr(info, 'min') else info.min,\n",
    "        'max': info.max,\n",
    "    }\n",
    "\n",
    "# Test common dtypes\n",
    "dtypes = [\n",
    "    ('FP64 (double)', torch.float64),\n",
    "    ('FP32 (float)', torch.float32),\n",
    "    ('FP16 (half)', torch.float16),\n",
    "    ('BF16 (bfloat)', torch.bfloat16),\n",
    "]\n",
    "\n",
    "print(\"How different precisions store œÄ = 3.141592653589793\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Type':<15} {'Bits':>6} {'Stored Value':>20} {'Error':>15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, dtype in dtypes:\n",
    "    result = analyze_dtype_precision(name, dtype)\n",
    "    print(f\"{result['name']:<15} {result['bits']:>6} {result['stored_pi']:>20.15f} {result['error']:>15.2e}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize precision loss with a distribution of random values\n",
    "\n",
    "def compare_precision(original, dtype):\n",
    "    \"\"\"Compare original FP32 tensor with converted version.\"\"\"\n",
    "    converted = original.to(dtype).to(torch.float32)  # Round trip\n",
    "    error = (original - converted).abs()\n",
    "    return error\n",
    "\n",
    "# Create random weights (like from a neural network)\n",
    "torch.manual_seed(42)\n",
    "original_weights = torch.randn(10000)  # 10k weights\n",
    "\n",
    "# Compare precision for each dtype\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('Precision Loss by Data Type (Error Distribution)', fontsize=14)\n",
    "\n",
    "dtypes_to_compare = [\n",
    "    ('FP16', torch.float16),\n",
    "    ('BF16', torch.bfloat16),\n",
    "]\n",
    "\n",
    "for idx, (name, dtype) in enumerate(dtypes_to_compare):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    errors = compare_precision(original_weights, dtype)\n",
    "    \n",
    "    ax.hist(errors.numpy(), bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    ax.axvline(errors.mean(), color='red', linestyle='--', label=f'Mean: {errors.mean():.2e}')\n",
    "    ax.set_xlabel('Absolute Error')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(f'{name} Precision Loss')\n",
    "    ax.legend()\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "# Add summary statistics\n",
    "ax = axes[1, 0]\n",
    "ax.clear()\n",
    "ax.axis('off')\n",
    "\n",
    "summary_text = \"Summary Statistics\\n\" + \"=\"*30 + \"\\n\\n\"\n",
    "for name, dtype in dtypes_to_compare:\n",
    "    errors = compare_precision(original_weights, dtype)\n",
    "    summary_text += f\"{name}:\\n\"\n",
    "    summary_text += f\"  Mean Error: {errors.mean():.2e}\\n\"\n",
    "    summary_text += f\"  Max Error:  {errors.max():.2e}\\n\"\n",
    "    summary_text += f\"  Std Dev:    {errors.std():.2e}\\n\\n\"\n",
    "\n",
    "ax.text(0.1, 0.9, summary_text, transform=ax.transAxes, fontsize=11,\n",
    "        verticalalignment='top', fontfamily='monospace')\n",
    "\n",
    "# Memory comparison\n",
    "ax = axes[1, 1]\n",
    "ax.clear()\n",
    "sizes = ['FP32', 'FP16/BF16', 'INT8/FP8', 'INT4/FP4']\n",
    "memory = [4, 2, 1, 0.5]\n",
    "colors = ['#ff6b6b', '#feca57', '#48dbfb', '#1dd1a1']\n",
    "\n",
    "bars = ax.bar(sizes, memory, color=colors, edgecolor='black')\n",
    "ax.set_ylabel('Bytes per Parameter')\n",
    "ax.set_title('Memory per Parameter')\n",
    "\n",
    "for bar, mem in zip(bars, memory):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "            f'{mem}B', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "We compared how accurately different data types can represent the same values:\n",
    "\n",
    "1. **FP16** has good precision but limited range (can overflow with large values)\n",
    "2. **BF16** has the same range as FP32 but less precision (great for training!)\n",
    "3. Both are **2√ó smaller** than FP32, which means **2√ó more model capacity**!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Integer Quantization (INT8 and INT4)\n",
    "\n",
    "Now let's explore integer quantization - the foundation of most model compression.\n",
    "\n",
    "### The Quantization Formula\n",
    "\n",
    "**Symmetric Quantization:**\n",
    "```\n",
    "quantized = round(float_value / scale)\n",
    "scale = max(|weights|) / 127  (for INT8)\n",
    "```\n",
    "\n",
    "**Asymmetric Quantization:**\n",
    "```\n",
    "quantized = round(float_value / scale) + zero_point\n",
    "scale = (max - min) / 255\n",
    "zero_point = round(-min / scale)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement and visualize INT8 and INT4 quantization\n",
    "\n",
    "def symmetric_quantize(tensor, bits):\n",
    "    \"\"\"Symmetric quantization to specified bit-width.\"\"\"\n",
    "    qmax = 2 ** (bits - 1) - 1\n",
    "    qmin = -2 ** (bits - 1)\n",
    "    \n",
    "    scale = tensor.abs().max() / qmax\n",
    "    scale = max(scale, 1e-10)  # Avoid division by zero\n",
    "    \n",
    "    quantized = torch.round(tensor / scale).clamp(qmin, qmax)\n",
    "    dequantized = quantized * scale\n",
    "    \n",
    "    return quantized, scale, dequantized\n",
    "\n",
    "\n",
    "# Test with sample weights\n",
    "torch.manual_seed(42)\n",
    "weights = torch.randn(16)  # 16 sample weights for visibility\n",
    "\n",
    "print(\"Original Weights (FP32):\")\n",
    "print(weights.numpy().round(4))\n",
    "print()\n",
    "\n",
    "# Quantize to different bit-widths\n",
    "for bits in [8, 4]:\n",
    "    q, scale, deq = symmetric_quantize(weights, bits)\n",
    "    error = (weights - deq).abs()\n",
    "    \n",
    "    print(f\"INT{bits} Quantization:\")\n",
    "    print(f\"  Scale: {scale:.6f}\")\n",
    "    print(f\"  Quantized values: {q.int().numpy()}\")\n",
    "    print(f\"  Dequantized: {deq.numpy().round(4)}\")\n",
    "    print(f\"  Mean Error: {error.mean():.6f}\")\n",
    "    print(f\"  Max Error: {error.max():.6f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the quantization effect\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Original weights\n",
    "ax = axes[0]\n",
    "x = np.arange(len(weights))\n",
    "ax.bar(x, weights.numpy(), alpha=0.7, label='Original', color='steelblue')\n",
    "ax.set_xlabel('Weight Index')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Original FP32 Weights')\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# INT8\n",
    "ax = axes[1]\n",
    "q8, s8, deq8 = symmetric_quantize(weights, 8)\n",
    "ax.bar(x - 0.2, weights.numpy(), width=0.4, alpha=0.7, label='Original', color='steelblue')\n",
    "ax.bar(x + 0.2, deq8.numpy(), width=0.4, alpha=0.7, label='INT8', color='coral')\n",
    "ax.set_xlabel('Weight Index')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('INT8 Quantization (256 levels)')\n",
    "ax.legend()\n",
    "\n",
    "# INT4\n",
    "ax = axes[2]\n",
    "q4, s4, deq4 = symmetric_quantize(weights, 4)\n",
    "ax.bar(x - 0.2, weights.numpy(), width=0.4, alpha=0.7, label='Original', color='steelblue')\n",
    "ax.bar(x + 0.2, deq4.numpy(), width=0.4, alpha=0.7, label='INT4', color='gold')\n",
    "ax.set_xlabel('Weight Index')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('INT4 Quantization (16 levels)')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nQuantization Summary:\")\n",
    "print(f\"  INT8: 256 possible values, error: {(weights - deq8).abs().mean():.6f}\")\n",
    "print(f\"  INT4: 16 possible values, error:  {(weights - deq4).abs().mean():.6f}\")\n",
    "print(f\"  INT4 has {(weights - deq4).abs().mean() / (weights - deq8).abs().mean():.1f}x more error than INT8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: FP8 - The Blackwell/Hopper Sweet Spot\n",
    "\n",
    "FP8 is a floating-point format with 8 bits. There are two variants:\n",
    "\n",
    "| Format | Exponent | Mantissa | Best For | Range |\n",
    "|--------|----------|----------|----------|-------|\n",
    "| E4M3 | 4 bits | 3 bits | **Inference** | ¬±448 |\n",
    "| E5M2 | 5 bits | 2 bits | **Training** | ¬±57344 |\n",
    "\n",
    "E4M3 has more precision (3 mantissa bits), while E5M2 has larger range (5 exponent bits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate FP8 quantization\n",
    "\n",
    "class FP8Format:\n",
    "    \"\"\"Configuration for FP8 format.\"\"\"\n",
    "    def __init__(self, name, exp_bits, mant_bits):\n",
    "        self.name = name\n",
    "        self.exp_bits = exp_bits\n",
    "        self.mant_bits = mant_bits\n",
    "        self.bias = 2 ** (exp_bits - 1) - 1\n",
    "        \n",
    "        # Calculate max representable value\n",
    "        max_exp = 2 ** exp_bits - 2  # Exclude infinity\n",
    "        max_mant = (2 ** mant_bits - 1) / 2 ** mant_bits\n",
    "        self.max_value = (1 + max_mant) * 2 ** (max_exp - self.bias)\n",
    "\n",
    "E4M3 = FP8Format(\"E4M3\", 4, 3)  # Inference\n",
    "E5M2 = FP8Format(\"E5M2\", 5, 2)  # Training\n",
    "\n",
    "def quantize_to_fp8(tensor, fp8_format):\n",
    "    \"\"\"Simulate FP8 quantization.\"\"\"\n",
    "    # Compute scaling factor to fit in FP8 range\n",
    "    max_val = tensor.abs().max()\n",
    "    scale = max_val / fp8_format.max_value\n",
    "    scale = max(scale.item(), 1e-10)\n",
    "    \n",
    "    # Scale to FP8 range\n",
    "    scaled = tensor / scale\n",
    "    \n",
    "    # Clip to range\n",
    "    clipped = torch.clamp(scaled, -fp8_format.max_value, fp8_format.max_value)\n",
    "    \n",
    "    # Simulate reduced precision\n",
    "    mantissa_mult = 2 ** fp8_format.mant_bits\n",
    "    quantized = torch.round(clipped * mantissa_mult) / mantissa_mult\n",
    "    \n",
    "    # Scale back\n",
    "    dequantized = quantized * scale\n",
    "    \n",
    "    return quantized, scale, dequantized\n",
    "\n",
    "\n",
    "# Compare FP8 formats\n",
    "print(\"FP8 Format Comparison\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Format':<10} {'Exp Bits':>10} {'Mant Bits':>10} {'Max Value':>12}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'E4M3':<10} {4:>10} {3:>10} {E4M3.max_value:>12.1f}\")\n",
    "print(f\"{'E5M2':<10} {5:>10} {2:>10} {E5M2.max_value:>12.1f}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test with weights\n",
    "print(\"\\nQuantization Comparison:\")\n",
    "torch.manual_seed(42)\n",
    "test_weights = torch.randn(10000)\n",
    "\n",
    "for fp8_format in [E4M3, E5M2]:\n",
    "    _, scale, deq = quantize_to_fp8(test_weights, fp8_format)\n",
    "    error = (test_weights - deq).abs()\n",
    "    print(f\"  {fp8_format.name}: Mean Error = {error.mean():.6f}, Max Error = {error.max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: FP4 - The Blackwell Exclusive\n",
    "\n",
    "FP4 (NVFP4) is NVIDIA's 4-bit floating-point format, **exclusive to Blackwell GPUs**.\n",
    "\n",
    "### What Makes FP4 Special?\n",
    "\n",
    "Unlike INT4 which uses linear quantization, FP4 uses:\n",
    "1. **Micro-block scaling**: Each small block of weights has its own scale\n",
    "2. **Dual-level scaling**: Both per-tensor and per-block scales\n",
    "3. **Non-linear values**: 8 positive values (0, 0.5, 1, 1.5, 2, 3, 4, 6)\n",
    "\n",
    "This results in **3.5√ó memory reduction** with **<0.1% accuracy loss** on benchmarks like MMLU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate NVFP4 quantization\n",
    "\n",
    "# FP4 representable values (normalized)\n",
    "FP4_VALUES = torch.tensor([0.0, 0.5, 1.0, 1.5, 2.0, 3.0, 4.0, 6.0])\n",
    "\n",
    "def quantize_to_fp4(tensor, block_size=16):\n",
    "    \"\"\"\n",
    "    Simulate NVFP4 quantization with micro-block scaling.\n",
    "    \n",
    "    Args:\n",
    "        tensor: Input tensor\n",
    "        block_size: Size of micro-blocks for fine-grained scaling\n",
    "    \"\"\"\n",
    "    original_shape = tensor.shape\n",
    "    flat = tensor.flatten()\n",
    "    \n",
    "    # Pad to multiple of block_size\n",
    "    n = flat.numel()\n",
    "    pad = (block_size - n % block_size) % block_size\n",
    "    if pad > 0:\n",
    "        flat = torch.nn.functional.pad(flat, (0, pad))\n",
    "    \n",
    "    # Reshape into blocks\n",
    "    n_blocks = flat.numel() // block_size\n",
    "    blocks = flat.view(n_blocks, block_size)\n",
    "    \n",
    "    # Tensor-level scale (coarse)\n",
    "    tensor_scale = blocks.abs().max()\n",
    "    blocks_normalized = blocks / max(tensor_scale.item(), 1e-10)\n",
    "    \n",
    "    # Block-level scales (fine)\n",
    "    block_max = blocks_normalized.abs().amax(dim=1, keepdim=True)\n",
    "    block_scales = block_max / 6.0  # 6.0 is max FP4 value\n",
    "    block_scales = torch.clamp(block_scales, min=1e-10)\n",
    "    \n",
    "    # Normalize by block scale\n",
    "    normalized = blocks_normalized / block_scales\n",
    "    \n",
    "    # Quantize to nearest FP4 value\n",
    "    signs = torch.sign(normalized)\n",
    "    abs_vals = normalized.abs()\n",
    "    \n",
    "    # Find nearest FP4 value\n",
    "    distances = (abs_vals.unsqueeze(-1) - FP4_VALUES.unsqueeze(0).unsqueeze(0)).abs()\n",
    "    indices = distances.argmin(dim=-1)\n",
    "    quantized = signs * FP4_VALUES[indices]\n",
    "    \n",
    "    # Dequantize\n",
    "    dequantized = quantized * block_scales * tensor_scale\n",
    "    dequantized = dequantized.flatten()[:n].view(original_shape)\n",
    "    \n",
    "    return quantized, block_scales, tensor_scale, dequantized\n",
    "\n",
    "\n",
    "# Compare FP4 with INT4\n",
    "print(\"FP4 vs INT4 Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "weights = torch.randn(1024)  # 1024 weights\n",
    "\n",
    "# INT4\n",
    "_, _, deq_int4 = symmetric_quantize(weights, 4)\n",
    "error_int4 = (weights - deq_int4).abs()\n",
    "\n",
    "# FP4\n",
    "_, _, _, deq_fp4 = quantize_to_fp4(weights)\n",
    "error_fp4 = (weights - deq_fp4).abs()\n",
    "\n",
    "print(f\"{'Method':<15} {'Mean Error':>15} {'Max Error':>15} {'RMSE':>15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'INT4':<15} {error_int4.mean():>15.6f} {error_int4.max():>15.6f} {error_int4.pow(2).mean().sqrt():>15.6f}\")\n",
    "print(f\"{'FP4 (NVFP4)':<15} {error_fp4.mean():>15.6f} {error_fp4.max():>15.6f} {error_fp4.pow(2).mean().sqrt():>15.6f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "improvement = (error_int4.mean() - error_fp4.mean()) / error_int4.mean() * 100\n",
    "print(f\"\\nFP4 reduces mean error by {improvement:.1f}% compared to INT4!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the FP4 vs INT4 difference\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Error distributions\n",
    "ax = axes[0]\n",
    "ax.hist(error_int4.numpy(), bins=50, alpha=0.7, label='INT4', color='coral')\n",
    "ax.hist(error_fp4.numpy(), bins=50, alpha=0.7, label='FP4', color='steelblue')\n",
    "ax.set_xlabel('Absolute Error')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Error Distribution: INT4 vs FP4')\n",
    "ax.legend()\n",
    "\n",
    "# FP4 representable values\n",
    "ax = axes[1]\n",
    "all_fp4 = torch.cat([-FP4_VALUES.flip(0)[:-1], FP4_VALUES])\n",
    "ax.stem(all_fp4.numpy(), [1]*len(all_fp4), linefmt='steelblue', markerfmt='o')\n",
    "ax.set_xlabel('Normalized Value')\n",
    "ax.set_ylabel('Representable')\n",
    "ax.set_title('FP4 Representable Values (16 total)')\n",
    "ax.set_ylim(0, 1.5)\n",
    "\n",
    "# INT4 vs FP4 coverage\n",
    "ax = axes[2]\n",
    "int4_vals = torch.linspace(-7, 7, 15)  # -7 to 7 for INT4\n",
    "int4_normalized = int4_vals / 7  # Normalized to [-1, 1]\n",
    "\n",
    "ax.stem(int4_normalized.numpy() * 6, [0.8]*len(int4_vals), linefmt='coral', \n",
    "        markerfmt='s', label='INT4 (linear)')\n",
    "ax.stem(all_fp4.numpy(), [1.2]*len(all_fp4), linefmt='steelblue', \n",
    "        markerfmt='o', label='FP4 (non-linear)')\n",
    "ax.set_xlabel('Value (scaled)')\n",
    "ax.set_title('INT4 vs FP4 Value Distribution')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight: FP4's non-linear values cluster near zero where most weights are!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Memory Savings Calculator\n",
    "\n",
    "Let's calculate exactly how much memory you save with each quantization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory calculator for model sizes\n",
    "\n",
    "def calculate_model_memory(params_billions, precision):\n",
    "    \"\"\"\n",
    "    Calculate memory for a model in GB.\n",
    "    \n",
    "    Args:\n",
    "        params_billions: Number of parameters in billions\n",
    "        precision: 'fp32', 'fp16', 'bf16', 'int8', 'fp8', 'int4', 'fp4'\n",
    "    \"\"\"\n",
    "    bytes_per_param = {\n",
    "        'fp32': 4,\n",
    "        'fp16': 2,\n",
    "        'bf16': 2,\n",
    "        'int8': 1,\n",
    "        'fp8': 1,\n",
    "        'int4': 0.5,\n",
    "        'fp4': 0.5,\n",
    "        'nvfp4': 0.5,\n",
    "    }\n",
    "    \n",
    "    bpp = bytes_per_param.get(precision.lower(), 2)\n",
    "    return params_billions * 1e9 * bpp / 1e9  # GB\n",
    "\n",
    "\n",
    "# Calculate for common model sizes\n",
    "model_sizes = [1, 3, 7, 13, 34, 70, 100, 200]\n",
    "precisions = ['FP32', 'FP16', 'INT8', 'INT4']\n",
    "\n",
    "print(\"Model Memory Requirements (GB)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Model':<10}\", end=\"\")\n",
    "for p in precisions:\n",
    "    print(f\"{p:>12}\", end=\"\")\n",
    "print(f\"{'Fits Spark?':>14}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for size in model_sizes:\n",
    "    print(f\"{size}B{'':<7}\", end=\"\")\n",
    "    for p in precisions:\n",
    "        mem = calculate_model_memory(size, p)\n",
    "        print(f\"{mem:>12.1f}\", end=\"\")\n",
    "    \n",
    "    # Check if fits in DGX Spark (128GB)\n",
    "    fp4_mem = calculate_model_memory(size, 'fp4')\n",
    "    fits = \" Yes (FP4)\" if fp4_mem < 120 else \" No\"\n",
    "    print(f\"{fits:>14}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nDGX Spark has 128GB unified memory.\")\n",
    "print(\"With FP4/INT4, you can run models up to ~200B parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the memory savings\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Memory by model size\n",
    "ax = axes[0]\n",
    "x = np.arange(len(model_sizes))\n",
    "width = 0.2\n",
    "\n",
    "colors = ['#ff6b6b', '#feca57', '#48dbfb', '#1dd1a1']\n",
    "for i, p in enumerate(precisions):\n",
    "    memories = [calculate_model_memory(s, p) for s in model_sizes]\n",
    "    ax.bar(x + i*width, memories, width, label=p, color=colors[i])\n",
    "\n",
    "ax.axhline(y=128, color='red', linestyle='--', linewidth=2, label='DGX Spark (128GB)')\n",
    "ax.set_xlabel('Model Size (Billions)')\n",
    "ax.set_ylabel('Memory (GB)')\n",
    "ax.set_title('Model Memory by Precision')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels([f'{s}B' for s in model_sizes])\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_ylim(0, 400)\n",
    "\n",
    "# Compression ratio\n",
    "ax = axes[1]\n",
    "compression = {\n",
    "    'FP32 ‚Üí FP16': 2.0,\n",
    "    'FP32 ‚Üí INT8': 4.0,\n",
    "    'FP32 ‚Üí INT4': 8.0,\n",
    "    'FP16 ‚Üí INT8': 2.0,\n",
    "    'FP16 ‚Üí INT4/FP4': 4.0,\n",
    "}\n",
    "\n",
    "bars = ax.barh(list(compression.keys()), list(compression.values()), color='steelblue')\n",
    "ax.set_xlabel('Compression Ratio')\n",
    "ax.set_title('Memory Compression Ratios')\n",
    "\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    ax.text(width + 0.1, bar.get_y() + bar.get_height()/2,\n",
    "            f'{width:.0f}√ó', va='center', fontweight='bold')\n",
    "\n",
    "ax.set_xlim(0, 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Custom Quantization Analysis\n",
    "\n",
    "Pick a model size relevant to your work and calculate:\n",
    "1. Memory at each precision level\n",
    "2. Whether it fits on DGX Spark at each level\n",
    "3. The recommended precision for your use case\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Use the `calculate_model_memory()` function we defined. Consider:\n",
    "- For training: you need ~3-4√ó more memory for optimizer states and gradients\n",
    "- For inference: just the model weights plus ~10-20% for KV cache\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your analysis here\n",
    "\n",
    "# Example: Analyze a 34B model\n",
    "my_model_size = 34  # billion parameters\n",
    "\n",
    "# TODO: Calculate memory at each precision\n",
    "# TODO: Determine if it fits on DGX Spark (128GB)\n",
    "# TODO: What's your recommendation?\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Signal-to-Noise Analysis\n",
    "\n",
    "Calculate the Signal-to-Noise Ratio (SNR) for each quantization method.\n",
    "\n",
    "**Formula:** `SNR_dB = 10 * log10(signal_power / noise_power)`\n",
    "\n",
    "Where:\n",
    "- signal_power = mean(original^2)\n",
    "- noise_power = mean(error^2)\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Higher SNR = better quality preservation. Good quantization should have SNR > 20 dB.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Calculate SNR for each method\n",
    "\n",
    "def calculate_snr(original, quantized):\n",
    "    \"\"\"Calculate Signal-to-Noise Ratio in dB.\"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "# Test with our weights\n",
    "torch.manual_seed(42)\n",
    "test_weights = torch.randn(10000)\n",
    "\n",
    "# TODO: Calculate SNR for INT8, INT4, FP8, FP4\n",
    "# TODO: Which method has the best quality preservation?\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Confusing Bits with Precision\n",
    "\n",
    "```python\n",
    "# Wrong: Assuming fewer bits always means lower quality\n",
    "# INT8 has 256 levels, FP8 E4M3 has ~240 representable values\n",
    "# BUT FP8's non-linear distribution often gives BETTER results!\n",
    "\n",
    "# Right: Choose format based on your data distribution\n",
    "# - For weights with normal distribution: FP formats often win\n",
    "# - For activations with outliers: INT with scaling may be better\n",
    "```\n",
    "\n",
    "### Mistake 2: Ignoring Scale Overhead\n",
    "\n",
    "```python\n",
    "# Wrong: Claiming 8√ó compression for INT4 vs FP32\n",
    "actual_compression = 4 / 0.5  # = 8√ó (ignoring scales)\n",
    "\n",
    "# Right: Account for scale factors\n",
    "# With group_size=128, scales add ~1.5% overhead\n",
    "actual_compression = 4 / (0.5 + 0.015)  # ‚âà 7.8√ó\n",
    "```\n",
    "\n",
    "### Mistake 3: Using Wrong FP8 Format\n",
    "\n",
    "```python\n",
    "# Wrong: Using E5M2 for inference weights\n",
    "model.to(torch.float8_e5m2)  # Larger range, less precision\n",
    "\n",
    "# Right: E4M3 for inference, E5M2 for gradients/training\n",
    "weights.to(torch.float8_e4m3fn)  # More precision for weights\n",
    "gradients.to(torch.float8_e5m2)   # Larger range for gradients\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "\n",
    "- **Data types matter**: FP32‚ÜíFP16‚ÜíINT8‚ÜíINT4 each halve memory\n",
    "- **Floating-point vs Integer**: FP formats preserve precision better for normally-distributed weights\n",
    "- **FP8 is special**: Native Blackwell support gives 2√ó compute with 50% memory\n",
    "- **FP4 is Blackwell-exclusive**: 3.5√ó compression with <0.1% quality loss\n",
    "- **DGX Spark advantage**: 128GB lets you run 70B models in FP16, 200B+ in FP4!\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "**Build a Precision Advisor**\n",
    "\n",
    "Create a function that recommends the optimal precision for a given model and hardware:\n",
    "\n",
    "```python\n",
    "def recommend_precision(\n",
    "    model_params_billions: float,\n",
    "    memory_budget_gb: float = 128,\n",
    "    task: str = 'inference',  # or 'training'\n",
    "    quality_priority: str = 'balanced'  # 'quality', 'balanced', 'memory'\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Recommend optimal precision for deployment.\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'precision', 'memory_gb', 'fits', 'reasoning'\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Build the precision advisor\n",
    "\n",
    "def recommend_precision(\n",
    "    model_params_billions: float,\n",
    "    memory_budget_gb: float = 128,\n",
    "    task: str = 'inference',\n",
    "    quality_priority: str = 'balanced'\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Recommend optimal precision for deployment.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this advisor\n",
    "    pass\n",
    "\n",
    "# Test your advisor\n",
    "# result = recommend_precision(70, memory_budget_gb=128, task='inference')\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [NVIDIA FP8 Format Specification](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/index.html)\n",
    "- [A Survey of Quantization Methods](https://arxiv.org/abs/2103.13630)\n",
    "- [Blackwell Architecture Whitepaper](https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/)\n",
    "- [TensorRT Model Optimizer](https://developer.nvidia.com/tensorrt)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any GPU memory\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory after cleanup: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "print(\"\\nNotebook complete! Ready for Lab 3.2.2: NVFP4 Quantization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, we'll dive deep into **NVFP4 Quantization** - applying FP4 to real 70B models on DGX Spark!\n",
    "\n",
    "‚û°Ô∏è Continue to: [Lab 3.2.2: NVFP4 Quantization](lab-3.2.2-nvfp4-quantization.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
