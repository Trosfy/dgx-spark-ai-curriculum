{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2.4: GPTQ Quantization\n",
    "\n",
    "**Module:** 3.2 - Model Quantization & Optimization  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** â­â­â­â˜†â˜†\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand the GPTQ algorithm and Hessian-based weight correction\n",
    "- [ ] Quantize a model using auto-gptq with different group sizes (32/64/128)\n",
    "- [ ] Compare quality and performance across configurations\n",
    "- [ ] Load and run pre-quantized GPTQ models from Hugging Face\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Prerequisites\n",
    "\n",
    "- Completed: Labs 3.2.1-3.2.3\n",
    "- Library: `auto-gptq` (`pip install auto-gptq`)\n",
    "- Hardware: CUDA GPU (DGX Spark recommended)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ Real-World Context\n",
    "\n",
    "**Why GPTQ is Popular:**\n",
    "- Published in 2022, became THE standard for 4-bit GPU inference\n",
    "- Used by TheBloke's 1000+ quantized models on Hugging Face\n",
    "- Native support in transformers, vLLM, and TensorRT-LLM\n",
    "\n",
    "**The Magic:** GPTQ uses the Hessian (second derivative) to find which weights are most important, then compensates for quantization errors in a layer-by-layer fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ§’ ELI5: How GPTQ Works\n",
    "\n",
    "> **Imagine you're packing a suitcase with a weight limit...**\n",
    ">\n",
    "> **Naive approach:** Remove weight from EVERYTHING equally.\n",
    "> - Problem: Your toothbrush is now too light to use!\n",
    ">\n",
    "> **GPTQ approach:**\n",
    "> 1. **Weigh each item** - figure out which items matter most\n",
    "> 2. **Pack important items carefully** - keep precision where it counts\n",
    "> 3. **Redistribute weight** - if you make one thing lighter, make something else heavier to compensate\n",
    ">\n",
    "> **In AI terms:**\n",
    "> 1. Compute the Hessian to find important weights\n",
    "> 2. Quantize less-important weights more aggressively\n",
    "> 3. Adjust remaining weights to compensate for errors\n",
    ">\n",
    "> The result: 4-bit weights that perform almost like 16-bit!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GPTQ Quantization Lab\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Check for auto-gptq\n",
    "try:\n",
    "    from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "    HAS_AUTOGPTQ = True\n",
    "    print(f\"\\nauto-gptq: Available\")\n",
    "except ImportError:\n",
    "    HAS_AUTOGPTQ = False\n",
    "    print(f\"\\nauto-gptq: Not installed\")\n",
    "    print(\"  Install with: pip install auto-gptq\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding GPTQ Algorithm\n",
    "\n",
    "GPTQ builds on the Optimal Brain Quantization (OBQ) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified GPTQ demonstration\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def simple_gptq_quantize(weights, bits=4, group_size=128):\n",
    "    \"\"\"\n",
    "    Simplified GPTQ-style quantization.\n",
    "    \n",
    "    Real GPTQ uses:\n",
    "    1. Hessian to find weight importance\n",
    "    2. Sequential quantization with error compensation\n",
    "    3. Group-wise scaling\n",
    "    \n",
    "    This demo shows the group-wise concept.\n",
    "    \"\"\"\n",
    "    out_features, in_features = weights.shape\n",
    "    n_groups = in_features // group_size\n",
    "    qmax = 2 ** (bits - 1) - 1\n",
    "    \n",
    "    # Process each group separately\n",
    "    quantized = torch.zeros_like(weights, dtype=torch.int8)\n",
    "    scales = torch.zeros(out_features, n_groups)\n",
    "    \n",
    "    for g in range(n_groups):\n",
    "        start = g * group_size\n",
    "        end = start + group_size\n",
    "        group = weights[:, start:end]\n",
    "        \n",
    "        # Per-group scale (symmetric)\n",
    "        max_val = group.abs().amax(dim=1, keepdim=True)\n",
    "        scale = max_val / qmax\n",
    "        scale = torch.clamp(scale, min=1e-10)\n",
    "        \n",
    "        # Quantize\n",
    "        q = torch.round(group / scale).clamp(-qmax-1, qmax).to(torch.int8)\n",
    "        \n",
    "        quantized[:, start:end] = q\n",
    "        scales[:, g] = scale.squeeze()\n",
    "    \n",
    "    return quantized, scales\n",
    "\n",
    "\n",
    "def dequantize_gptq(quantized, scales, group_size=128):\n",
    "    \"\"\"Dequantize GPTQ-style weights.\"\"\"\n",
    "    out_features, in_features = quantized.shape\n",
    "    n_groups = in_features // group_size\n",
    "    \n",
    "    dequantized = torch.zeros(out_features, in_features, dtype=torch.float32)\n",
    "    \n",
    "    for g in range(n_groups):\n",
    "        start = g * group_size\n",
    "        end = start + group_size\n",
    "        q = quantized[:, start:end].float()\n",
    "        scale = scales[:, g:g+1]\n",
    "        dequantized[:, start:end] = q * scale\n",
    "    \n",
    "    return dequantized\n",
    "\n",
    "\n",
    "# Test with sample weights\n",
    "torch.manual_seed(42)\n",
    "weights = torch.randn(256, 1024)  # [out_features, in_features]\n",
    "\n",
    "print(\"GPTQ Group-Size Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for group_size in [32, 64, 128, 256]:\n",
    "    q, scales = simple_gptq_quantize(weights, bits=4, group_size=group_size)\n",
    "    deq = dequantize_gptq(q, scales, group_size=group_size)\n",
    "    error = (weights - deq).abs()\n",
    "    \n",
    "    # Memory overhead from scales\n",
    "    scale_overhead = scales.numel() * 2 / (weights.numel() * 0.5) * 100\n",
    "    \n",
    "    print(f\"Group size {group_size:>4}: RMSE={error.pow(2).mean().sqrt():.6f}, \"\n",
    "          f\"Max error={error.max():.4f}, \"\n",
    "          f\"Scale overhead={scale_overhead:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the quality-overhead trade-off\n",
    "\n",
    "group_sizes = [16, 32, 64, 128, 256, 512]\n",
    "rmses = []\n",
    "overheads = []\n",
    "\n",
    "for gs in group_sizes:\n",
    "    if 1024 % gs == 0:  # Must divide evenly\n",
    "        q, scales = simple_gptq_quantize(weights, bits=4, group_size=gs)\n",
    "        deq = dequantize_gptq(q, scales, group_size=gs)\n",
    "        rmse = (weights - deq).pow(2).mean().sqrt().item()\n",
    "        overhead = scales.numel() * 2 / (weights.numel() * 0.5) * 100\n",
    "        rmses.append(rmse)\n",
    "        overheads.append(overhead)\n",
    "    else:\n",
    "        rmses.append(None)\n",
    "        overheads.append(None)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Quality vs group size\n",
    "ax = axes[0]\n",
    "valid = [(gs, r) for gs, r in zip(group_sizes, rmses) if r is not None]\n",
    "ax.plot([v[0] for v in valid], [v[1] for v in valid], 'bo-', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Group Size')\n",
    "ax.set_ylabel('RMSE (lower is better)')\n",
    "ax.set_title('Quantization Quality vs Group Size')\n",
    "ax.set_xscale('log', base=2)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Overhead vs group size\n",
    "ax = axes[1]\n",
    "valid = [(gs, o) for gs, o in zip(group_sizes, overheads) if o is not None]\n",
    "ax.plot([v[0] for v in valid], [v[1] for v in valid], 'ro-', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Group Size')\n",
    "ax.set_ylabel('Scale Overhead (%)')\n",
    "ax.set_title('Memory Overhead vs Group Size')\n",
    "ax.set_xscale('log', base=2)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: Smaller group sizes = better quality but more overhead.\")\n",
    "print(\"Common choices: 128 (balanced) or 32 (higher quality)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Quantizing a Real Model with auto-gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection for GPTQ quantization\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Choose a model\n",
    "# For learning: use a smaller model\n",
    "# For production: use your target model\n",
    "\n",
    "MODEL_NAME = \"microsoft/phi-2\"  # 2.7B - good for demo\n",
    "# MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"  # 7B - standard benchmark\n",
    "\n",
    "print(f\"Selected model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / 1e9\n",
    "    return 0\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer loaded. Vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare calibration data\n",
    "\n",
    "def get_calibration_data(tokenizer, num_samples=128, seq_len=512):\n",
    "    \"\"\"Get calibration data for GPTQ quantization.\"\"\"\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        \n",
    "        print(\"Loading calibration data from C4...\")\n",
    "        dataset = load_dataset('allenai/c4', 'en', split='train', streaming=True)\n",
    "        \n",
    "        calibration_data = []\n",
    "        for i, sample in enumerate(dataset):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "            text = sample['text']\n",
    "            if len(text) > 100:\n",
    "                calibration_data.append(text)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not load C4: {e}\")\n",
    "        print(\"Using synthetic calibration data...\")\n",
    "        \n",
    "        calibration_data = [\n",
    "            \"Machine learning models can be compressed through quantization techniques. \"\n",
    "            \"This reduces memory usage and increases inference speed while maintaining accuracy.\",\n",
    "        ] * num_samples\n",
    "    \n",
    "    print(f\"Prepared {len(calibration_data)} calibration samples\")\n",
    "    return calibration_data\n",
    "\n",
    "\n",
    "calibration_data = get_calibration_data(tokenizer, num_samples=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize with auto-gptq\n",
    "\n",
    "if HAS_AUTOGPTQ:\n",
    "    print(\"\\nQuantizing with GPTQ...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Configuration\n",
    "    quantize_config = BaseQuantizeConfig(\n",
    "        bits=4,              # 4-bit quantization\n",
    "        group_size=128,      # Group size (try 32, 64, 128)\n",
    "        desc_act=True,       # Activation order (improves quality)\n",
    "    )\n",
    "    \n",
    "    print(f\"Config: {quantize_config.bits}-bit, group_size={quantize_config.group_size}\")\n",
    "    \n",
    "    # Load model for quantization\n",
    "    print(f\"\\nLoading model for quantization...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = AutoGPTQForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantize_config=quantize_config,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"Model loaded in {time.time() - start_time:.1f}s\")\n",
    "    print(f\"GPU memory: {get_gpu_memory():.2f} GB\")\n",
    "    \n",
    "    # Quantize\n",
    "    print(f\"\\nRunning GPTQ quantization (this may take a while)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.quantize(calibration_data)\n",
    "    \n",
    "    print(f\"Quantization complete in {time.time() - start_time:.1f}s\")\n",
    "    print(f\"GPU memory: {get_gpu_memory():.2f} GB\")\n",
    "    \n",
    "else:\n",
    "    print(\"auto-gptq not available. Showing expected workflow:\")\n",
    "    print(\"\"\"\n",
    "    from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "    \n",
    "    config = BaseQuantizeConfig(bits=4, group_size=128)\n",
    "    model = AutoGPTQForCausalLM.from_pretrained(MODEL_NAME, quantize_config=config)\n",
    "    model.quantize(calibration_data)\n",
    "    model.save_quantized(\"./my-model-gptq-4bit\")\n",
    "    \"\"\")\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the quantized model\n",
    "\n",
    "if model is not None:\n",
    "    test_prompt = \"The key to machine learning is\"\n",
    "    \n",
    "    print(f\"Testing quantized model...\")\n",
    "    print(f\"Prompt: '{test_prompt}'\")\n",
    "    \n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"\\nGenerated: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Loading Pre-Quantized Models from Hugging Face\n",
    "\n",
    "You don't always need to quantize yourself! Many GPTQ models are available on Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up previous model\n",
    "if 'model' in dir() and model is not None:\n",
    "    del model\n",
    "clear_memory()\n",
    "\n",
    "# Load a pre-quantized GPTQ model\n",
    "# TheBloke has quantized most popular models!\n",
    "\n",
    "GPTQ_MODEL = \"TheBloke/Llama-2-7B-GPTQ\"  # Pre-quantized Llama 2 7B\n",
    "\n",
    "print(f\"Loading pre-quantized model: {GPTQ_MODEL}\")\n",
    "print(\"This is much faster than quantizing yourself!\")\n",
    "\n",
    "if HAS_AUTOGPTQ:\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        gptq_model = AutoGPTQForCausalLM.from_quantized(\n",
    "            GPTQ_MODEL,\n",
    "            device=\"cuda:0\",\n",
    "            trust_remote_code=True,\n",
    "            use_safetensors=True,\n",
    "        )\n",
    "        \n",
    "        gptq_tokenizer = AutoTokenizer.from_pretrained(GPTQ_MODEL)\n",
    "        if gptq_tokenizer.pad_token is None:\n",
    "            gptq_tokenizer.pad_token = gptq_tokenizer.eos_token\n",
    "        \n",
    "        print(f\"\\nLoaded in {time.time() - start_time:.1f}s\")\n",
    "        print(f\"GPU memory: {get_gpu_memory():.2f} GB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not load pre-quantized model: {e}\")\n",
    "        print(\"This may require Hugging Face authentication for gated models.\")\n",
    "        gptq_model = None\n",
    "else:\n",
    "    gptq_model = None\n",
    "    print(\"\\nauto-gptq required to load GPTQ models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark the GPTQ model\n",
    "\n",
    "if gptq_model is not None:\n",
    "    print(\"Benchmarking GPTQ model...\")\n",
    "    \n",
    "    prompt = \"Explain the theory of relativity in simple terms:\"\n",
    "    inputs = gptq_tokenizer(prompt, return_tensors=\"pt\").to(gptq_model.device)\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        _ = gptq_model.generate(**inputs, max_new_tokens=10, do_sample=False,\n",
    "                                pad_token_id=gptq_tokenizer.pad_token_id)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    num_tokens = 50\n",
    "    num_runs = 3\n",
    "    times = []\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = gptq_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=num_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=gptq_tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        times.append(time.perf_counter() - start)\n",
    "    \n",
    "    avg_time = sum(times) / len(times)\n",
    "    tokens_per_second = num_tokens / avg_time\n",
    "    \n",
    "    print(f\"\\nGPTQ Performance:\")\n",
    "    print(f\"  Tokens/second: {tokens_per_second:.1f}\")\n",
    "    print(f\"  Latency: {avg_time * 1000:.1f} ms for {num_tokens} tokens\")\n",
    "    print(f\"  Memory: {get_gpu_memory():.2f} GB\")\n",
    "    \n",
    "    # Show generation\n",
    "    response = gptq_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"\\nGenerated:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Comparing Group Sizes\n",
    "\n",
    "Let's compare different GPTQ configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of GPTQ variants (from TheBloke's models)\n",
    "\n",
    "print(\"GPTQ Configuration Comparison\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "Based on TheBloke's benchmarks for Llama 2 7B:\n",
    "\n",
    "| Config | Group Size | desc_act | MMLU | Memory | Speed |\n",
    "|--------|------------|----------|------|--------|-------|\n",
    "| High Quality | 32 | Yes | 45.8% | 4.3 GB | 1.0x |\n",
    "| Balanced | 128 | Yes | 45.5% | 4.1 GB | 1.1x |\n",
    "| Fast | 128 | No | 45.2% | 4.0 GB | 1.2x |\n",
    "| FP16 baseline | - | - | 46.0% | 13.5 GB | 0.5x |\n",
    "\n",
    "Key observations:\n",
    "1. GPTQ retains ~99% of FP16 quality on MMLU\n",
    "2. Smaller group_size = better quality, more memory\n",
    "3. desc_act=True helps quality but slows inference\n",
    "4. 3x memory reduction vs FP16!\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nRecommendations:\")\n",
    "print(\"  - group_size=128: Best balance of quality and speed\")\n",
    "print(\"  - group_size=32: When quality is critical\")\n",
    "print(\"  - desc_act=True: Default, disable only for max speed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ‹ Try It Yourself\n",
    "\n",
    "### Exercise 1: Quantize with Different Group Sizes\n",
    "\n",
    "Quantize the same model with group_size 32, 64, 128 and compare:\n",
    "1. Memory usage\n",
    "2. Inference speed\n",
    "3. Output quality (run the same prompt)\n",
    "\n",
    "### Exercise 2: Find Pre-Quantized Models\n",
    "\n",
    "Search Hugging Face for GPTQ versions of your favorite model.\n",
    "Hint: Search \"TheBloke <model-name> GPTQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Compare group sizes\n",
    "\n",
    "# TODO: Quantize with different group sizes\n",
    "# TODO: Compare memory and speed\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Using Wrong Calibration Data\n",
    "\n",
    "```python\n",
    "# Wrong: Using random text for calibration\n",
    "calib_data = [\"lorem ipsum dolor sit amet...\"]\n",
    "\n",
    "# Right: Use data similar to your deployment use case\n",
    "calib_data = load_your_domain_specific_data()\n",
    "```\n",
    "\n",
    "### Mistake 2: Ignoring desc_act\n",
    "\n",
    "```python\n",
    "# Wrong: Always using desc_act=False for speed\n",
    "config = BaseQuantizeConfig(bits=4, desc_act=False)\n",
    "\n",
    "# Right: Only disable if you've verified quality is acceptable\n",
    "config = BaseQuantizeConfig(bits=4, desc_act=True)  # Default\n",
    "# Run quality evaluation before disabling!\n",
    "```\n",
    "\n",
    "### Mistake 3: Not Using Pre-Quantized Models\n",
    "\n",
    "```python\n",
    "# Wrong: Always quantizing yourself\n",
    "model.quantize(data)  # Takes hours!\n",
    "\n",
    "# Right: Check if a pre-quantized version exists\n",
    "# Search: https://huggingface.co/TheBloke\n",
    "model = AutoGPTQForCausalLM.from_quantized(\"TheBloke/Model-GPTQ\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "\n",
    "- **GPTQ algorithm**: Hessian-based quantization with error compensation\n",
    "- **Group sizes**: Smaller = better quality, more overhead\n",
    "- **Pre-quantized models**: TheBloke has 1000+ models ready to use!\n",
    "- **Memory savings**: 3x reduction vs FP16, <1% quality loss\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [GPTQ Paper](https://arxiv.org/abs/2210.17323)\n",
    "- [auto-gptq GitHub](https://github.com/PanQiWei/AutoGPTQ)\n",
    "- [TheBloke's Models](https://huggingface.co/TheBloke)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "if 'gptq_model' in dir() and gptq_model is not None:\n",
    "    del gptq_model\n",
    "if 'model' in dir() and model is not None:\n",
    "    del model\n",
    "\n",
    "clear_memory()\n",
    "print(\"Notebook complete! Ready for Lab 3.2.5: AWQ Quantization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, we'll explore **AWQ Quantization** - activation-aware quantization that often outperforms GPTQ!\n",
    "\n",
    "âž¡ï¸ Continue to: [Lab 3.2.5: AWQ Quantization](lab-3.2.5-awq-quantization.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
