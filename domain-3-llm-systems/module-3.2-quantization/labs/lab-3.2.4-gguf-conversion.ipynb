{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 11.4: GGUF Conversion\n",
    "\n",
    "**Module:** 11 - Model Quantization & Optimization  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand the GGUF format and its advantages\n",
    "- [ ] Convert models to GGUF format\n",
    "- [ ] Apply various quantization levels (Q2 to Q8)\n",
    "- [ ] Run inference with llama.cpp\n",
    "- [ ] Compare GGUF variants on quality and speed\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Tasks 11.1-11.3\n",
    "- Knowledge of: Basic quantization concepts\n",
    "- Hardware: DGX Spark with 128GB unified memory\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The Problem:** GPTQ and AWQ are great for GPUs, but what if you want to:\n",
    "- Run models on CPUs?\n",
    "- Deploy to edge devices?\n",
    "- Use a simple, dependency-light inference engine?\n",
    "\n",
    "**Enter GGUF (GPT-Generated Unified Format)**:\n",
    "- Created by Georgi Gerganov for llama.cpp\n",
    "- Single-file format with embedded metadata\n",
    "- Works on CPU, GPU, Apple Silicon, Android\n",
    "- Supports 2-8 bit quantization\n",
    "- Powers millions of local LLM deployments!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßí ELI5: What is GGUF?\n",
    "\n",
    "> **Imagine you're sending a recipe to a friend...**\n",
    ">\n",
    "> **Old way (PyTorch format):** Send them:\n",
    "> - The ingredient list (weights)\n",
    "> - Cooking instructions (model architecture)\n",
    "> - Equipment needed (dependencies)\n",
    "> - 10 separate files they need to organize\n",
    ">\n",
    "> **GGUF way:** Send ONE file that contains:\n",
    "> - Everything they need\n",
    "> - Instructions on how to read it\n",
    "> - Works with any cooking style (CPU/GPU/Metal)\n",
    ">\n",
    "> **In AI terms:** GGUF is a self-contained format that packages the model, its architecture info, and quantization details into a single portable file that works everywhere.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding GGUF Quantization Types\n",
    "\n",
    "GGUF supports many quantization variants, each with different tradeoffs:\n",
    "\n",
    "| Type | Bits/Weight | Quality | Speed | Use Case |\n",
    "|------|-------------|---------|-------|----------|\n",
    "| F16 | 16 | Best | Baseline | Full precision |\n",
    "| Q8_0 | 8 | Excellent | Fast | Quality-critical |\n",
    "| Q6_K | 6.6 | Very Good | Fast | Balanced |\n",
    "| Q5_K_M | 5.5 | Good | Faster | Recommended default |\n",
    "| Q4_K_M | 4.8 | Good | Faster | Popular choice |\n",
    "| Q4_0 | 4 | OK | Fast | Memory constrained |\n",
    "| Q3_K_M | 3.4 | Fair | Fastest | Very small |\n",
    "| Q2_K | 2.6 | Poor | Fastest | Extreme compression |\n",
    "\n",
    "### The \"K\" Variants\n",
    "\n",
    "The `K` suffix means \"k-quants\" - a smarter quantization that:\n",
    "- Uses different bit-widths for different layers\n",
    "- Protects important layers (attention) with more bits\n",
    "- Gives better quality than uniform quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import gc\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DGX Spark Environment Check\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize quantization type comparison\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Note: Quality and speed values are approximate estimates.\n# Actual performance varies by model architecture, hardware, and workload.\n# Always benchmark on your specific use case!\nquant_types = {\n    'F16':     {'bits': 16.0, 'quality': 100, 'speed': 50},\n    'Q8_0':    {'bits': 8.0,  'quality': 98,  'speed': 70},\n    'Q6_K':    {'bits': 6.6,  'quality': 95,  'speed': 75},\n    'Q5_K_M':  {'bits': 5.5,  'quality': 92,  'speed': 80},\n    'Q4_K_M':  {'bits': 4.8,  'quality': 88,  'speed': 85},\n    'Q4_0':    {'bits': 4.0,  'quality': 82,  'speed': 88},\n    'Q3_K_M':  {'bits': 3.4,  'quality': 75,  'speed': 92},\n    'Q2_K':    {'bits': 2.6,  'quality': 60,  'speed': 95},\n}\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nnames = list(quant_types.keys())\nbits = [quant_types[n]['bits'] for n in names]\nquality = [quant_types[n]['quality'] for n in names]\nspeed = [quant_types[n]['speed'] for n in names]\n\n# Model size (relative to F16)\nsizes = [b/16 * 100 for b in bits]\n\ncolors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(names)))[::-1]\n\n# Bits per weight\naxes[0].barh(names, bits, color=colors)\naxes[0].set_xlabel('Bits per Weight')\naxes[0].set_title('Storage Efficiency')\naxes[0].invert_yaxis()\n\n# Quality score\naxes[1].barh(names, quality, color=colors)\naxes[1].set_xlabel('Quality Score (100=best)')\naxes[1].set_title('Model Quality')\naxes[1].set_xlim(50, 105)\naxes[1].invert_yaxis()\n\n# Size reduction\naxes[2].barh(names, sizes, color=colors)\naxes[2].set_xlabel('Size (% of F16)')\naxes[2].set_title('Model Size')\naxes[2].invert_yaxis()\n\nplt.tight_layout()\nplt.savefig('gguf_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()\nplt.close(fig)  # Free memory from figure\n\nprint(\"\\nüí° Recommendation: Q4_K_M or Q5_K_M for best balance!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Setting Up llama.cpp\n",
    "\n",
    "llama.cpp is a C/C++ inference engine that:\n",
    "- Runs on CPU (with AVX/AVX2/AVX512 optimizations)\n",
    "- Supports CUDA GPU acceleration\n",
    "- Has Apple Metal support\n",
    "- Is incredibly fast and efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone and build llama.cpp\n",
    "# This needs to be done once\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Allow override via environment variable\n",
    "LLAMA_CPP_DIR = os.environ.get(\"LLAMA_CPP_DIR\", os.path.expanduser(\"~/llama.cpp\"))\n",
    "\n",
    "print(f\"llama.cpp directory: {LLAMA_CPP_DIR}\")\n",
    "\n",
    "if os.path.exists(LLAMA_CPP_DIR):\n",
    "    print(\"‚úÖ Directory already exists\")\n",
    "\n",
    "    # Check git status\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"rev-parse\", \"--short\", \"HEAD\"],\n",
    "            cwd=LLAMA_CPP_DIR,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            print(f\"  Current commit: {result.stdout.strip()}\")\n",
    "\n",
    "        # Check for updates\n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"fetch\", \"--dry-run\"],\n",
    "            cwd=LLAMA_CPP_DIR,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        if result.stderr:\n",
    "            print(\"  ‚ÑπÔ∏è  Updates may be available. Run: cd ~/llama.cpp && git pull\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(\"\\nüí° To use a different location, set LLAMA_CPP_DIR environment variable\")\n",
    "    print(\"   Example: export LLAMA_CPP_DIR=/path/to/llama.cpp\")\n",
    "else:\n",
    "    print(\"Cloning llama.cpp...\")\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"clone\", \"https://github.com/ggerganov/llama.cpp.git\", LLAMA_CPP_DIR],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Clone successful!\")\n",
    "    else:\n",
    "        print(f\"‚ùå Clone failed: {result.stderr}\")\n",
    "        raise RuntimeError(\"Failed to clone llama.cpp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build llama.cpp with CUDA support (for DGX Spark)\n",
    "print(\"Building llama.cpp with CUDA support...\")\n",
    "print(\"(This may take a few minutes)\")\n",
    "\n",
    "build_cmd = f\"\"\"\n",
    "cd {LLAMA_CPP_DIR} && \\\n",
    "cmake -B build -DGGML_CUDA=ON && \\\n",
    "cmake --build build --config Release -j$(nproc)\n",
    "\"\"\"\n",
    "\n",
    "import time as _time\n",
    "_build_start = _time.time()\n",
    "\n",
    "result = subprocess.run(build_cmd, shell=True, capture_output=True, text=True)\n",
    "\n",
    "_build_time = _time.time() - _build_start\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"‚úÖ Build successful! (took {_build_time:.1f}s)\")\n",
    "else:\n",
    "    print(\"‚ùå BUILD FAILED\")\n",
    "    print(\"\\nBuild errors:\")\n",
    "    # Show last 2000 chars of error output\n",
    "    error_output = result.stderr[-2000:] if len(result.stderr) > 2000 else result.stderr\n",
    "    print(error_output)\n",
    "    print(\"\\nPossible solutions:\")\n",
    "    print(\"  1. Ensure CUDA toolkit is installed: nvcc --version\")\n",
    "    print(\"  2. Ensure cmake is installed: cmake --version\")\n",
    "    print(\"  3. Check that you're running in an NGC container with CUDA support\")\n",
    "    raise RuntimeError(\"llama.cpp build failed - cannot continue without compiled binaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify build succeeded before continuing\n# This cell will fail early with a clear error if binaries are missing\n\nquantize_bin = os.path.join(LLAMA_CPP_DIR, \"build\", \"bin\", \"llama-quantize\")\nmain_bin = os.path.join(LLAMA_CPP_DIR, \"build\", \"bin\", \"llama-cli\")\n\nprint(\"Checking for compiled binaries...\")\n\n# Check primary paths first\nif not os.path.exists(quantize_bin):\n    # Check alternative paths (older llama.cpp versions)\n    alt_quantize = os.path.join(LLAMA_CPP_DIR, \"quantize\")\n    alt_main = os.path.join(LLAMA_CPP_DIR, \"main\")\n    \n    if os.path.exists(alt_quantize):\n        quantize_bin = alt_quantize\n        main_bin = alt_main\n        print(\"  Using legacy binary paths (older llama.cpp version)\")\n\n# Final verification - cannot continue without binaries\nif not os.path.exists(quantize_bin):\n    print(\"‚ùå ERROR: llama.cpp binaries not found!\")\n    print(\"\\nChecked the following locations:\")\n    print(f\"  - {os.path.join(LLAMA_CPP_DIR, 'build', 'bin', 'llama-quantize')}\")\n    print(f\"  - {os.path.join(LLAMA_CPP_DIR, 'quantize')}\")\n    print(\"\\nPossible solutions:\")\n    print(\"  1. Run the build cell above successfully first\")\n    print(\"  2. Check that cmake and CUDA toolkit are installed\")\n    print(\"  3. Ensure you're using an NGC container with build tools\")\n    raise FileNotFoundError(\"llama.cpp binaries not found - please run the build cell first\")\n\nprint(f\"  llama-quantize: ‚úì Found at {quantize_bin}\")\nprint(f\"  llama-cli:      {'‚úì Found' if os.path.exists(main_bin) else '‚ö† Not found (optional)'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Converting Models to GGUF\n",
    "\n",
    "The conversion process:\n",
    "1. **Download/load the HuggingFace model**\n",
    "2. **Convert to GGUF format (F16)**\n",
    "3. **Quantize to desired precision**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for conversion\n",
    "!pip install sentencepiece gguf --quiet\n",
    "\n",
    "print(\"Conversion dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a model to convert\n",
    "# Using a small model for this demo\n",
    "model_id = \"facebook/opt-350m\"\n",
    "\n",
    "# For larger models:\n",
    "# model_id = \"meta-llama/Llama-2-7b-hf\"  # Requires HF login\n",
    "# model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "output_dir = \"./gguf_models\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Model: {model_id}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# First, download the model from HuggingFace\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nprint(\"Downloading model from HuggingFace...\")\n\n# Create a local directory for the HF model\nhf_model_dir = os.path.join(output_dir, \"hf_model\")\nos.makedirs(hf_model_dir, exist_ok=True)\n\n# Download with error handling for network issues\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)\nexcept Exception as e:\n    print(f\"‚ùå Failed to download model: {e}\")\n    print(\"\\nPossible solutions:\")\n    print(\"  1. Check your internet connection\")\n    print(\"  2. Verify the model ID is correct\")\n    print(\"  3. For gated models, run: huggingface-cli login\")\n    raise\n\ntokenizer.save_pretrained(hf_model_dir)\nmodel.save_pretrained(hf_model_dir)\n\nprint(f\"‚úÖ Model saved to {hf_model_dir}\")\n\n# Free memory\ndel model\ngc.collect()\ntorch.cuda.empty_cache()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert to GGUF F16 format\nprint(\"Converting to GGUF F16 format...\")\n\ngguf_f16_path = os.path.join(output_dir, \"model-f16.gguf\")\n\n# Check for different script names (llama.cpp has renamed this several times)\npossible_scripts = [\n    os.path.join(LLAMA_CPP_DIR, \"convert_hf_to_gguf.py\"),\n    os.path.join(LLAMA_CPP_DIR, \"convert-hf-to-gguf.py\"),\n    os.path.join(LLAMA_CPP_DIR, \"convert.py\"),\n]\n\nconvert_script = None\nfor script in possible_scripts:\n    if os.path.exists(script):\n        convert_script = script\n        break\n\nif convert_script is None:\n    print(\"‚ùå ERROR: Conversion script not found!\")\n    print(\"\\nChecked the following locations:\")\n    for script in possible_scripts:\n        print(f\"  - {script}\")\n    print(\"\\nPossible solutions:\")\n    print(\"  1. Update llama.cpp: cd ~/llama.cpp && git pull\")\n    print(\"  2. Check that the repository was cloned correctly\")\n    raise FileNotFoundError(\"Conversion script not found in llama.cpp directory\")\n\nprint(f\"Using conversion script: {os.path.basename(convert_script)}\")\n\nconvert_cmd = f\"\"\"\npython3 {convert_script} \\\n    {hf_model_dir} \\\n    --outfile {gguf_f16_path} \\\n    --outtype f16\n\"\"\"\n\nresult = subprocess.run(convert_cmd, shell=True, capture_output=True, text=True)\n\nif os.path.exists(gguf_f16_path):\n    size_mb = os.path.getsize(gguf_f16_path) / 1e6\n    print(f\"‚úì F16 GGUF created: {gguf_f16_path}\")\n    print(f\"  Size: {size_mb:.1f} MB\")\nelse:\n    print(\"‚ùå Conversion failed!\")\n    print(\"\\nConversion output:\")\n    print(result.stdout[-1500:] if len(result.stdout) > 1500 else result.stdout)\n    print(\"\\nError output:\")\n    print(result.stderr[-1500:] if len(result.stderr) > 1500 else result.stderr)\n    raise RuntimeError(\"GGUF conversion failed - see output above for details\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize to different precisions\n",
    "quant_types_to_create = ['Q8_0', 'Q5_K_M', 'Q4_K_M', 'Q4_0', 'Q2_K']\n",
    "\n",
    "quantized_models = {}\n",
    "\n",
    "print(\"Quantizing to different precision levels...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for qtype in quant_types_to_create:\n",
    "    output_path = os.path.join(output_dir, f\"model-{qtype}.gguf\")\n",
    "    \n",
    "    print(f\"\\nCreating {qtype}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    quant_cmd = f\"{quantize_bin} {gguf_f16_path} {output_path} {qtype}\"\n",
    "    result = subprocess.run(quant_cmd, shell=True, capture_output=True, text=True)\n",
    "    \n",
    "    if os.path.exists(output_path):\n",
    "        size_mb = os.path.getsize(output_path) / 1e6\n",
    "        quant_time = time.time() - start_time\n",
    "        quantized_models[qtype] = {\n",
    "            'path': output_path,\n",
    "            'size_mb': size_mb,\n",
    "            'quant_time': quant_time\n",
    "        }\n",
    "        print(f\"  ‚úÖ Size: {size_mb:.1f} MB (took {quant_time:.1f}s)\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå Failed: {result.stderr}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Quantization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Summary of created files\nprint(\"\\nGGUF Files Created:\")\nprint(\"=\" * 60)\nprint(f\"{'Type':<12} {'Size (MB)':>12} {'vs F16':>12} {'Compression':>12}\")\nprint(\"-\" * 60)\n\nf16_size = os.path.getsize(gguf_f16_path) / 1e6\nprint(f\"{'F16':<12} {f16_size:>12.1f} {'baseline':>12} {'1.0x':>12}\")\n\nfor qtype, data in quantized_models.items():\n    ratio = f16_size / data['size_mb']\n    reduction_pct = (1 - data['size_mb'] / f16_size) * 100\n    reduction_str = f\"{reduction_pct:.0f}% less\"\n    print(f\"{qtype:<12} {data['size_mb']:>12.1f} {reduction_str:>12} {ratio:>11.1f}x\")\n\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Running Inference with llama.cpp\n",
    "\n",
    "Now let's test our GGUF models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llama_cpp_inference(model_path, prompt, n_tokens=50, n_gpu_layers=99):\n",
    "    \"\"\"\n",
    "    Run inference with llama.cpp.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to GGUF file\n",
    "        prompt: Input prompt\n",
    "        n_tokens: Number of tokens to generate\n",
    "        n_gpu_layers: Number of layers to offload to GPU (99 = all)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains output, tokens_per_second, etc.\n",
    "    \"\"\"\n",
    "    cmd = f\"\"\"\n",
    "    {main_bin} \\\n",
    "        -m {model_path} \\\n",
    "        -p \"{prompt}\" \\\n",
    "        -n {n_tokens} \\\n",
    "        -ngl {n_gpu_layers} \\\n",
    "        --temp 0 \\\n",
    "        2>&1\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    output = result.stdout\n",
    "    \n",
    "    # Parse timing info from llama.cpp output\n",
    "    tokens_per_sec = None\n",
    "    for line in output.split('\\n'):\n",
    "        if 'eval time' in line.lower() and 'token' in line.lower():\n",
    "            try:\n",
    "                # Parse \"X tokens / Y ms (Z tok/s)\"\n",
    "                parts = line.split('(')\n",
    "                if len(parts) > 1:\n",
    "                    tokens_per_sec = float(parts[-1].split()[0])\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return {\n",
    "        'output': output,\n",
    "        'total_time': total_time,\n",
    "        'tokens_per_sec': tokens_per_sec or n_tokens / total_time\n",
    "    }\n",
    "\n",
    "print(\"Inference function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference with different quantization levels\n",
    "test_prompt = \"The future of artificial intelligence is\"\n",
    "n_tokens = 50\n",
    "\n",
    "print(f\"Prompt: '{test_prompt}'\")\n",
    "print(f\"Generating {n_tokens} tokens...\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "inference_results = {}\n",
    "\n",
    "# Test F16 first\n",
    "print(\"\\nTesting F16...\")\n",
    "result = run_llama_cpp_inference(gguf_f16_path, test_prompt, n_tokens)\n",
    "inference_results['F16'] = result\n",
    "print(f\"  Speed: {result['tokens_per_sec']:.1f} tok/s\")\n",
    "\n",
    "# Test quantized versions\n",
    "for qtype, data in quantized_models.items():\n",
    "    print(f\"\\nTesting {qtype}...\")\n",
    "    result = run_llama_cpp_inference(data['path'], test_prompt, n_tokens)\n",
    "    inference_results[qtype] = result\n",
    "    print(f\"  Speed: {result['tokens_per_sec']:.1f} tok/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"GGUF Inference Comparison\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Type':<12} {'Size (MB)':>12} {'Tok/s':>12} {'Speedup':>12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "baseline_speed = inference_results['F16']['tokens_per_sec']\n",
    "\n",
    "print(f\"{'F16':<12} {f16_size:>12.1f} {baseline_speed:>12.1f} {'1.0x':>12}\")\n",
    "\n",
    "for qtype, data in quantized_models.items():\n",
    "    speed = inference_results[qtype]['tokens_per_sec']\n",
    "    speedup = speed / baseline_speed\n",
    "    print(f\"{qtype:<12} {data['size_mb']:>12.1f} {speed:>12.1f} {speedup:>11.2f}x\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize the comparison\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\ntypes = ['F16'] + list(quantized_models.keys())\nsizes = [f16_size] + [quantized_models[t]['size_mb'] for t in quantized_models.keys()]\nspeeds = [inference_results[t]['tokens_per_sec'] for t in types]\n\ncolors = plt.cm.viridis(np.linspace(0.2, 0.8, len(types)))\n\n# Size comparison\naxes[0].barh(types, sizes, color=colors)\naxes[0].set_xlabel('Size (MB)')\naxes[0].set_title('Model Size')\naxes[0].invert_yaxis()\nfor i, v in enumerate(sizes):\n    axes[0].text(v + 5, i, f'{v:.0f}', va='center')\n\n# Speed comparison\naxes[1].barh(types, speeds, color=colors)\naxes[1].set_xlabel('Tokens/second')\naxes[1].set_title('Inference Speed')\naxes[1].invert_yaxis()\nfor i, v in enumerate(speeds):\n    axes[1].text(v + 0.5, i, f'{v:.1f}', va='center')\n\nplt.tight_layout()\nplt.savefig('gguf_inference_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()\nplt.close(fig)  # Free memory from figure"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Using llama-cpp-python for Python Integration\n",
    "\n",
    "If you want to use GGUF models in Python, `llama-cpp-python` provides a convenient wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install llama-cpp-python with CUDA support\n",
    "# Note: On DGX Spark (ARM64), this compiles from source - may take several minutes\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"Installing llama-cpp-python with CUDA support...\")\n",
    "print(\"‚ö†Ô∏è  On ARM64 (DGX Spark), this compiles from source.\")\n",
    "print(\"   This may take 5-10 minutes. Please be patient...\")\n",
    "\n",
    "# Set environment for CUDA build\n",
    "env = os.environ.copy()\n",
    "env[\"CMAKE_ARGS\"] = \"-DGGML_CUDA=on\"\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"pip\", \"install\", \"llama-cpp-python\", \"--no-cache-dir\", \"--force-reinstall\"],\n",
    "    env=env,\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(f\"‚ùå Installation failed!\")\n",
    "    print(\"Error output (last 1500 chars):\")\n",
    "    print(result.stderr[-1500:] if len(result.stderr) > 1500 else result.stderr)\n",
    "    print(\"\\nüí° Possible solutions:\")\n",
    "    print(\"  1. Ensure you're in an NGC container with build tools\")\n",
    "    print(\"  2. Try: apt-get update && apt-get install -y cmake build-essential\")\n",
    "    print(\"  3. Use a pre-built container with llama-cpp-python installed\")\n",
    "    raise RuntimeError(\"llama-cpp-python installation failed\")\n",
    "else:\n",
    "    print(\"‚úÖ Installation successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Load a quantized model\n",
    "model_path = quantized_models['Q4_K_M']['path']\n",
    "\n",
    "print(f\"Loading {model_path}...\")\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=2048,       # Context window\n",
    "    n_gpu_layers=99,  # Offload all layers to GPU\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference with Python API\n",
    "prompt = \"Explain machine learning in simple terms:\"\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"Response:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "output = llm(\n",
    "    prompt,\n",
    "    max_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    echo=False\n",
    ")\n",
    "\n",
    "inference_time = time.time() - start_time\n",
    "\n",
    "response_text = output['choices'][0]['text']\n",
    "print(response_text)\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"\\nGenerated {output['usage']['completion_tokens']} tokens in {inference_time:.2f}s\")\n",
    "print(f\"Speed: {output['usage']['completion_tokens']/inference_time:.1f} tok/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up Python model\n",
    "del llm\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Convert a Different Model\n",
    "\n",
    "Try converting a different model (e.g., Mistral-7B or Llama-2-7B) to GGUF format.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "```python\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "# Follow the same conversion steps\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert a different model\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Perplexity Evaluation\n",
    "\n",
    "Use llama.cpp's built-in perplexity tool to evaluate quality across quantization levels.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "```bash\n",
    "./build/bin/llama-perplexity -m model.gguf -f wiki.test.txt\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate perplexity\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Using GPU Acceleration\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong: No GPU layers\n",
    "./main -m model.gguf -p \"Hello\"\n",
    "\n",
    "# ‚úÖ Right: Offload to GPU\n",
    "./main -m model.gguf -p \"Hello\" -ngl 99\n",
    "```\n",
    "\n",
    "**Why:** Without `-ngl`, everything runs on CPU. Use `-ngl 99` to offload all layers to GPU.\n",
    "\n",
    "### Mistake 2: Wrong Quantization Type for Use Case\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Q2_K for critical applications\n",
    "model = \"model-Q2_K.gguf\"  # Quality is too low!\n",
    "\n",
    "# ‚úÖ Right: Use Q4_K_M or higher for important tasks\n",
    "model = \"model-Q4_K_M.gguf\"\n",
    "```\n",
    "\n",
    "**Why:** Q2_K has significant quality loss. Use Q4_K_M for the best balance.\n",
    "\n",
    "### Mistake 3: Not Setting Context Size\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Default context may be too small\n",
    "llm = Llama(model_path=path)\n",
    "\n",
    "# ‚úÖ Right: Set appropriate context size\n",
    "llm = Llama(model_path=path, n_ctx=4096)\n",
    "```\n",
    "\n",
    "**Why:** Default context is often 512 tokens. Set it based on your needs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üéâ Checkpoint\n\nYou've completed the learning objectives:\n\n- [x] **Understand the GGUF format and its advantages** - Single-file, portable, works everywhere\n- [x] **Convert models to GGUF format** - Using llama.cpp's conversion scripts\n- [x] **Apply various quantization levels (Q2 to Q8)** - K-quants for smart quantization\n- [x] **Run inference with llama.cpp** - Fast inference on CPU and GPU\n- [x] **Compare GGUF variants on quality and speed** - Q4_K_M recommended for best balance\n\n### Key Takeaways\n\n- ‚úÖ **GGUF format**: Single-file, portable, works everywhere\n- ‚úÖ **K-quants**: Smart quantization that protects important layers\n- ‚úÖ **Q4_K_M recommended**: Best balance of size and quality\n- ‚úÖ **llama.cpp**: Fast inference on CPU and GPU\n- ‚úÖ **Python integration**: llama-cpp-python for easy use\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Build a GGUF Model Server**\n",
    "\n",
    "Create a simple FastAPI server that:\n",
    "1. Loads a GGUF model\n",
    "2. Exposes a `/generate` endpoint\n",
    "3. Supports streaming responses\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "from fastapi.responses import StreamingResponse\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "async def generate(prompt: str, max_tokens: int = 100):\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Further Reading\n",
    "\n",
    "- [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)\n",
    "- [GGUF Specification](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)\n",
    "- [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)\n",
    "- [TheBloke GGUF Models](https://huggingface.co/TheBloke) (Pre-converted models!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GGUF files (optional - comment out to keep them)\n",
    "import shutil\n",
    "\n",
    "# Uncomment to delete:\n",
    "# shutil.rmtree(output_dir, ignore_errors=True)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Cleanup complete!\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, we'll explore **Blackwell FP4 quantization** - the exclusive DGX Spark superpower!\n",
    "\n",
    "‚û°Ô∏è Continue to: [05-fp4-deep-dive.ipynb](05-fp4-deep-dive.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}