{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2.6: GGUF Conversion\n",
    "\n",
    "**Module:** 3.2 - Model Quantization & Optimization  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand the GGUF format and its advantages\n",
    "- [ ] Convert a Hugging Face model to GGUF\n",
    "- [ ] Apply different quantization levels (Q2_K to Q8_0)\n",
    "- [ ] Run inference with llama.cpp\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Labs 3.2.1-3.2.5\n",
    "- Tools: llama.cpp (built from source)\n",
    "- Hardware: Works on CPU and GPU\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**Why GGUF?**\n",
    "- **llama.cpp**: The most popular local LLM inference engine\n",
    "- **Universal**: Works on CPU, CUDA, Metal, ROCm, Vulkan\n",
    "- **Efficient**: Hand-optimized kernels for each platform\n",
    "- **Portable**: Single-file models, easy to share and deploy\n",
    "\n",
    "**GGUF = GGML Universal Format** - successor to GGML, designed for:\n",
    "- Extensibility (add new features without breaking compatibility)\n",
    "- Metadata storage (tokenizer, model config in the same file)\n",
    "- Multiple quantization types in one file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What is GGUF?\n",
    "\n",
    "> **Think of model formats like movie file formats...**\n",
    ">\n",
    "> **PyTorch (.bin/.safetensors):** Like a professional movie master file.\n",
    "> - High quality, but needs special software (PyTorch) to play\n",
    "> - Large file size\n",
    ">\n",
    "> **GGUF:** Like an MP4 file.\n",
    "> - Plays anywhere (CPU, GPU, phone, browser)\n",
    "> - Compressed but still looks great\n",
    "> - Everything in one file (video + audio = model + tokenizer)\n",
    ">\n",
    "> **In AI terms:**\n",
    "> - GGUF packages model weights + tokenizer + config in one file\n",
    "> - Works with llama.cpp, ollama, LM Studio, and many more\n",
    "> - Supports 2-bit to 8-bit quantization (Q2_K to Q8_0)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding GGUF Quantization Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GGUF Conversion Lab\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# GGUF quantization types and their properties\n",
    "GGUF_QUANT_TYPES = {\n",
    "    'Q2_K': {'bits': 2.5, 'quality': 'Low', 'use_case': 'Maximum compression'},\n",
    "    'Q3_K_S': {'bits': 3.4, 'quality': 'Low-Medium', 'use_case': 'Small model, okay quality'},\n",
    "    'Q3_K_M': {'bits': 3.9, 'quality': 'Medium', 'use_case': 'Balanced for small'},\n",
    "    'Q4_0': {'bits': 4.5, 'quality': 'Medium', 'use_case': 'Legacy, use Q4_K_M instead'},\n",
    "    'Q4_K_S': {'bits': 4.5, 'quality': 'Medium-Good', 'use_case': 'Good compression'},\n",
    "    'Q4_K_M': {'bits': 4.8, 'quality': 'Good', 'use_case': 'Recommended default'},\n",
    "    'Q5_0': {'bits': 5.5, 'quality': 'Good', 'use_case': 'Legacy, use Q5_K_M instead'},\n",
    "    'Q5_K_S': {'bits': 5.5, 'quality': 'Very Good', 'use_case': 'Higher quality'},\n",
    "    'Q5_K_M': {'bits': 5.7, 'quality': 'Very Good', 'use_case': 'Best quality/size'},\n",
    "    'Q6_K': {'bits': 6.6, 'quality': 'Excellent', 'use_case': 'Near FP16 quality'},\n",
    "    'Q8_0': {'bits': 8.5, 'quality': 'Excellent', 'use_case': 'Maximum quality'},\n",
    "    'F16': {'bits': 16.0, 'quality': 'Perfect', 'use_case': 'No quantization'},\n",
    "}\n",
    "\n",
    "print(\"\\nGGUF Quantization Types:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Type':<10} {'Bits':>8} {'Quality':<15} {'Use Case'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for qtype, info in GGUF_QUANT_TYPES.items():\n",
    "    print(f\"{qtype:<10} {info['bits']:>8.1f} {info['quality']:<15} {info['use_case']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Recommendation: Q4_K_M for most users (best balance)\")\n",
    "print(\"              Q5_K_M for quality-critical applications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate model sizes for different quantization types\n",
    "\n",
    "def calculate_gguf_size(params_b, quant_type):\n",
    "    \"\"\"Calculate approximate GGUF file size in GB.\"\"\"\n",
    "    bits = GGUF_QUANT_TYPES[quant_type]['bits']\n",
    "    return params_b * bits / 8  # Convert to GB\n",
    "\n",
    "# Common model sizes\n",
    "model_sizes = [7, 13, 34, 70]\n",
    "quant_types = ['Q2_K', 'Q4_K_M', 'Q5_K_M', 'Q8_0', 'F16']\n",
    "\n",
    "print(\"\\nGGUF File Sizes (GB) by Model and Quantization:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Model':<10}\", end=\"\")\n",
    "for qt in quant_types:\n",
    "    print(f\"{qt:>12}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for size in model_sizes:\n",
    "    print(f\"{size}B{'':<7}\", end=\"\")\n",
    "    for qt in quant_types:\n",
    "        gb = calculate_gguf_size(size, qt)\n",
    "        print(f\"{gb:>12.1f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nNote: Actual sizes may vary slightly due to metadata overhead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Setting Up llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if llama.cpp is available\n",
    "\n",
    "LLAMA_CPP_PATH = Path.home() / \"llama.cpp\"  # Default location\n",
    "\n",
    "# You can also set this to your llama.cpp installation\n",
    "# LLAMA_CPP_PATH = Path(\"/path/to/your/llama.cpp\")\n",
    "\n",
    "convert_script = LLAMA_CPP_PATH / \"convert_hf_to_gguf.py\"\n",
    "quantize_binary = LLAMA_CPP_PATH / \"llama-quantize\"\n",
    "main_binary = LLAMA_CPP_PATH / \"llama-cli\"\n",
    "\n",
    "print(\"Checking llama.cpp installation...\")\n",
    "print(f\"Path: {LLAMA_CPP_PATH}\")\n",
    "print()\n",
    "\n",
    "if LLAMA_CPP_PATH.exists():\n",
    "    print(f\"  llama.cpp directory: Found\")\n",
    "    print(f\"  convert_hf_to_gguf.py: {'Found' if convert_script.exists() else 'Missing'}\")\n",
    "    print(f\"  llama-quantize: {'Found' if quantize_binary.exists() else 'Missing'}\")\n",
    "    print(f\"  llama-cli: {'Found' if main_binary.exists() else 'Missing'}\")\n",
    "    \n",
    "    HAS_LLAMA_CPP = convert_script.exists()\n",
    "else:\n",
    "    print(\"  llama.cpp not found!\")\n",
    "    HAS_LLAMA_CPP = False\n",
    "\n",
    "if not HAS_LLAMA_CPP:\n",
    "    print(\"\\nTo install llama.cpp:\")\n",
    "    print(\"\"\"\n",
    "    # Clone repository\n",
    "    git clone https://github.com/ggerganov/llama.cpp\n",
    "    cd llama.cpp\n",
    "    \n",
    "    # Build with CUDA support (for DGX Spark)\n",
    "    cmake -B build -DGGML_CUDA=ON\n",
    "    cmake --build build --config Release -j\n",
    "    \n",
    "    # Install Python dependencies for conversion\n",
    "    pip install -r requirements.txt\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Converting a Model to GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to convert\n",
    "MODEL_NAME = \"microsoft/phi-2\"  # Small model for demo\n",
    "OUTPUT_DIR = Path(\"./gguf_models\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download the model (if not already cached)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"Step 1: Downloading model from Hugging Face...\")\n",
    "\n",
    "# Get cache path\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "try:\n",
    "    model_path = snapshot_download(\n",
    "        MODEL_NAME,\n",
    "        local_dir=OUTPUT_DIR / \"hf_model\",\n",
    "        local_dir_use_symlinks=False,\n",
    "    )\n",
    "    print(f\"Model downloaded to: {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Download failed: {e}\")\n",
    "    print(\"Using HF cache instead...\")\n",
    "    model_path = MODEL_NAME  # Use HF cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Convert to GGUF (FP16 first)\n",
    "\n",
    "if HAS_LLAMA_CPP:\n",
    "    print(\"\\nStep 2: Converting to GGUF (FP16)...\")\n",
    "    \n",
    "    fp16_output = OUTPUT_DIR / \"model-f16.gguf\"\n",
    "    \n",
    "    cmd = [\n",
    "        \"python\", str(convert_script),\n",
    "        model_path,\n",
    "        \"--outfile\", str(fp16_output),\n",
    "        \"--outtype\", \"f16\",\n",
    "    ]\n",
    "    \n",
    "    print(f\"Running: {' '.join(cmd)}\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "        print(\"Conversion successful!\")\n",
    "        print(f\"Output: {fp16_output}\")\n",
    "        \n",
    "        if fp16_output.exists():\n",
    "            size_gb = fp16_output.stat().st_size / 1e9\n",
    "            print(f\"File size: {size_gb:.2f} GB\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Conversion failed: {e.stderr}\")\n",
    "else:\n",
    "    print(\"\\nllama.cpp not available. Showing expected command:\")\n",
    "    print(f\"\"\"\n",
    "    python {LLAMA_CPP_PATH}/convert_hf_to_gguf.py \\\\\n",
    "        {model_path} \\\\\n",
    "        --outfile {OUTPUT_DIR}/model-f16.gguf \\\\\n",
    "        --outtype f16\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Quantize to different types\n",
    "\n",
    "QUANTIZATION_TYPES = ['Q4_K_M', 'Q5_K_M', 'Q8_0']\n",
    "\n",
    "if HAS_LLAMA_CPP and fp16_output.exists():\n",
    "    print(\"\\nStep 3: Quantizing to different types...\")\n",
    "    \n",
    "    for qtype in QUANTIZATION_TYPES:\n",
    "        output_file = OUTPUT_DIR / f\"model-{qtype.lower()}.gguf\"\n",
    "        \n",
    "        print(f\"\\nQuantizing to {qtype}...\")\n",
    "        \n",
    "        cmd = [\n",
    "            str(quantize_binary),\n",
    "            str(fp16_output),\n",
    "            str(output_file),\n",
    "            qtype,\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "            size_gb = output_file.stat().st_size / 1e9\n",
    "            print(f\"  Output: {output_file}\")\n",
    "            print(f\"  Size: {size_gb:.2f} GB\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"  Failed: {e.stderr}\")\n",
    "else:\n",
    "    print(\"\\nQuantization command format:\")\n",
    "    print(f\"\"\"\n",
    "    {quantize_binary} model-f16.gguf model-q4_k_m.gguf Q4_K_M\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Running Inference with llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference with the quantized model\n",
    "\n",
    "if HAS_LLAMA_CPP and main_binary.exists():\n",
    "    print(\"Testing inference with llama.cpp...\")\n",
    "    \n",
    "    # Use Q4_K_M model\n",
    "    model_file = OUTPUT_DIR / \"model-q4_k_m.gguf\"\n",
    "    \n",
    "    if model_file.exists():\n",
    "        prompt = \"The key to machine learning is\"\n",
    "        \n",
    "        cmd = [\n",
    "            str(main_binary),\n",
    "            \"-m\", str(model_file),\n",
    "            \"-p\", prompt,\n",
    "            \"-n\", \"50\",  # Generate 50 tokens\n",
    "            \"-ngl\", \"99\",  # Offload all layers to GPU\n",
    "            \"--no-display-prompt\",\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nPrompt: '{prompt}'\")\n",
    "        print(\"\\nGenerating...\\n\")\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)\n",
    "            print(f\"Output: {result.stdout}\")\n",
    "            \n",
    "            if result.stderr:\n",
    "                # Extract performance info from stderr\n",
    "                for line in result.stderr.split('\\n'):\n",
    "                    if 'eval time' in line.lower() or 'token/s' in line.lower():\n",
    "                        print(line)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"Inference timed out.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    else:\n",
    "        print(f\"Model file not found: {model_file}\")\n",
    "else:\n",
    "    print(\"\\nllama.cpp inference command:\")\n",
    "    print(f\"\"\"\n",
    "    {main_binary} \\\\\n",
    "        -m model-q4_k_m.gguf \\\\\n",
    "        -p \"The key to machine learning is\" \\\\\n",
    "        -n 50 \\\\\n",
    "        -ngl 99  # Offload to GPU\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GGUF models with Python (llama-cpp-python)\n",
    "\n",
    "print(\"Using GGUF with Python (llama-cpp-python)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    from llama_cpp import Llama\n",
    "    HAS_LLAMA_CPP_PYTHON = True\n",
    "    print(\"llama-cpp-python: Available\")\n",
    "except ImportError:\n",
    "    HAS_LLAMA_CPP_PYTHON = False\n",
    "    print(\"llama-cpp-python: Not installed\")\n",
    "    print(\"  Install with: pip install llama-cpp-python\")\n",
    "    print(\"  For CUDA: CMAKE_ARGS='-DGGML_CUDA=on' pip install llama-cpp-python\")\n",
    "\n",
    "if HAS_LLAMA_CPP_PYTHON:\n",
    "    model_file = OUTPUT_DIR / \"model-q4_k_m.gguf\"\n",
    "    \n",
    "    if model_file.exists():\n",
    "        print(f\"\\nLoading {model_file}...\")\n",
    "        \n",
    "        llm = Llama(\n",
    "            model_path=str(model_file),\n",
    "            n_gpu_layers=-1,  # Offload all layers to GPU\n",
    "            n_ctx=2048,       # Context length\n",
    "            verbose=False,\n",
    "        )\n",
    "        \n",
    "        # Generate\n",
    "        prompt = \"The key to machine learning is\"\n",
    "        print(f\"\\nPrompt: '{prompt}'\")\n",
    "        \n",
    "        output = llm(\n",
    "            prompt,\n",
    "            max_tokens=50,\n",
    "            echo=False,\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nGenerated: {output['choices'][0]['text']}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Quality Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare file sizes and expected quality\n",
    "\n",
    "print(\"\\nQuantization Quality vs Size Trade-off\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# List all GGUF files\n",
    "gguf_files = list(OUTPUT_DIR.glob(\"*.gguf\"))\n",
    "\n",
    "if gguf_files:\n",
    "    print(f\"\\n{'File':<30} {'Size (GB)':>12} {'Bits/Weight':>12}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for f in sorted(gguf_files):\n",
    "        size_gb = f.stat().st_size / 1e9\n",
    "        # Estimate bits from filename\n",
    "        name = f.stem.upper()\n",
    "        bits = \"Unknown\"\n",
    "        for qt, info in GGUF_QUANT_TYPES.items():\n",
    "            if qt.replace('_', '') in name.replace('_', '').replace('-', ''):\n",
    "                bits = f\"{info['bits']:.1f}\"\n",
    "                break\n",
    "        \n",
    "        print(f\"{f.name:<30} {size_gb:>12.2f} {bits:>12}\")\n",
    "else:\n",
    "    print(\"\\nNo GGUF files found.\")\n",
    "    print(\"Expected quality comparison:\")\n",
    "    print(\"\"\"\n",
    "    | Quantization | Perplexity Increase | Recommendation |\n",
    "    |--------------|---------------------|----------------|\n",
    "    | Q8_0 | +0.01 | Highest quality |\n",
    "    | Q6_K | +0.02 | Near-lossless |\n",
    "    | Q5_K_M | +0.05 | Excellent |\n",
    "    | Q4_K_M | +0.10 | Great (default) |\n",
    "    | Q3_K_M | +0.20 | Good |\n",
    "    | Q2_K | +0.50 | Acceptable |\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Convert a Different Model\n",
    "\n",
    "Convert a model of your choice to GGUF and test all quantization levels.\n",
    "\n",
    "### Exercise 2: Benchmark Inference Speed\n",
    "\n",
    "Compare inference speed across quantization types:\n",
    "```bash\n",
    "for q in q2_k q4_k_m q5_k_m q8_0; do\n",
    "    echo \"Testing $q:\"\n",
    "    ./llama-cli -m model-$q.gguf -p \"Hello\" -n 100 -ngl 99 2>&1 | grep \"token/s\"\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Your code here\n",
    "\n",
    "# TODO: Convert a different model to GGUF\n",
    "# TODO: Compare sizes and test inference\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Using GPU Offloading\n",
    "\n",
    "```bash\n",
    "# Wrong: Running entirely on CPU (slow!)\n",
    "./llama-cli -m model.gguf -p \"Hello\"\n",
    "\n",
    "# Right: Offload layers to GPU\n",
    "./llama-cli -m model.gguf -p \"Hello\" -ngl 99\n",
    "```\n",
    "\n",
    "### Mistake 2: Wrong Quantization Choice\n",
    "\n",
    "```bash\n",
    "# Wrong: Using Q4_0 (legacy, worse quality)\n",
    "./llama-quantize model-f16.gguf model-q4.gguf Q4_0\n",
    "\n",
    "# Right: Use Q4_K_M (better quality, same size)\n",
    "./llama-quantize model-f16.gguf model-q4.gguf Q4_K_M\n",
    "```\n",
    "\n",
    "### Mistake 3: Forgetting to Build with CUDA\n",
    "\n",
    "```bash\n",
    "# Wrong: Building without CUDA (CPU only)\n",
    "cmake -B build\n",
    "\n",
    "# Right: Enable CUDA for DGX Spark\n",
    "cmake -B build -DGGML_CUDA=ON\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "\n",
    "- **GGUF format**: Universal format for llama.cpp ecosystem\n",
    "- **Quantization types**: Q2_K to Q8_0, with K variants being better\n",
    "- **Conversion workflow**: HF -> F16 GGUF -> Quantized GGUF\n",
    "- **GPU offloading**: Use `-ngl` for fast inference on DGX Spark\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)\n",
    "- [GGUF Format Specification](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)\n",
    "- [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Clean up generated files\n",
    "# Uncomment to delete GGUF files\n",
    "\n",
    "# import shutil\n",
    "# if OUTPUT_DIR.exists():\n",
    "#     shutil.rmtree(OUTPUT_DIR)\n",
    "#     print(f\"Cleaned up {OUTPUT_DIR}\")\n",
    "\n",
    "print(\"Notebook complete! Ready for Lab 3.2.7: Quality Benchmark Suite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, we'll create a **Quality Benchmark Suite** to compare all quantization methods!\n",
    "\n",
    "‚û°Ô∏è Continue to: [Lab 3.2.7: Quality Benchmark Suite](lab-3.2.7-quality-benchmark-suite.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
