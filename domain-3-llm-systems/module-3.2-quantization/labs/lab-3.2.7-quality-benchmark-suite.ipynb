{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2.7: Quality Benchmark Suite\n",
    "\n",
    "## Comprehensive Model Quality Evaluation Across Quantization Methods\n",
    "\n",
    "**Duration:** 2 hours\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Implement perplexity benchmarks** to measure language model quality\n",
    "2. **Run MMLU evaluations** to assess knowledge retention after quantization\n",
    "3. **Compare all quantization methods** (FP16, INT8, INT4, FP8, FP4, GPTQ, AWQ, GGUF)\n",
    "4. **Analyze quality-performance trade-offs** for production decisions\n",
    "5. **Create comprehensive benchmark reports** with statistical analysis\n",
    "\n",
    "---\n",
    "\n",
    "### Why Quality Benchmarking Matters\n",
    "\n",
    "```\n",
    "Professor SPARK says:\n",
    "\n",
    "\"Quantization is like compression for photos. Yes, you save space, but did you\n",
    "lose the important details? A good benchmark suite answers: 'Is this model still\n",
    "smart enough for my use case?' We measure this through perplexity (how surprised\n",
    "the model is by text) and MMLU (does it still know facts?).\"\n",
    "```\n",
    "\n",
    "### The Quality Triangle\n",
    "\n",
    "```\n",
    "                    Quality\n",
    "                      /\\\n",
    "                     /  \\\n",
    "                    /    \\\n",
    "                   /  ??  \\\n",
    "                  /________\\\n",
    "              Speed        Memory\n",
    "\n",
    "Every quantization method makes different trade-offs.\n",
    "This lab helps you measure exactly what you gain and lose.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Environment Setup and Benchmark Framework\n",
    "\n",
    "### 1.1 Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple, Any, Callable\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Transformers and datasets\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Local utilities\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from scripts import (\n",
    "    calculate_perplexity,\n",
    "    calculate_perplexity_batch,\n",
    "    perplexity_by_domain,\n",
    "    compare_perplexity,\n",
    "    benchmark_inference,\n",
    "    compare_models,\n",
    "    get_gpu_memory,\n",
    "    clear_memory,\n",
    "    MemoryTracker,\n",
    "    print_dgx_spark_status\n",
    ")\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Environment ready for quality benchmarking!\")\n",
    "print_dgx_spark_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Benchmark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BenchmarkConfig:\n",
    "    \"\"\"Configuration for quality benchmarks.\"\"\"\n",
    "    \n",
    "    # Model settings\n",
    "    model_name: str = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "    \n",
    "    # Perplexity settings\n",
    "    perplexity_samples: int = 100  # Number of text samples\n",
    "    max_length: int = 512  # Max tokens per sample\n",
    "    stride: int = 256  # Sliding window stride\n",
    "    \n",
    "    # MMLU settings\n",
    "    mmlu_subjects: List[str] = field(default_factory=lambda: [\n",
    "        'abstract_algebra',\n",
    "        'anatomy',\n",
    "        'astronomy',\n",
    "        'business_ethics',\n",
    "        'clinical_knowledge',\n",
    "        'computer_security',\n",
    "        'conceptual_physics',\n",
    "        'high_school_mathematics',\n",
    "        'machine_learning',\n",
    "        'professional_medicine'\n",
    "    ])\n",
    "    mmlu_samples_per_subject: int = 20  # Samples per subject\n",
    "    \n",
    "    # Inference settings\n",
    "    batch_size: int = 1\n",
    "    num_inference_runs: int = 50\n",
    "    warmup_runs: int = 5\n",
    "    \n",
    "    # Output settings\n",
    "    save_results: bool = True\n",
    "    results_dir: str = \"../data/benchmark_results\"\n",
    "\n",
    "\n",
    "# Create configuration\n",
    "config = BenchmarkConfig()\n",
    "\n",
    "# Ensure results directory exists\n",
    "Path(config.results_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Benchmark Configuration:\")\n",
    "print(f\"  Model: {config.model_name}\")\n",
    "print(f\"  Perplexity samples: {config.perplexity_samples}\")\n",
    "print(f\"  MMLU subjects: {len(config.mmlu_subjects)}\")\n",
    "print(f\"  Results directory: {config.results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Benchmark Results Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class QualityBenchmarkResult:\n",
    "    \"\"\"Stores comprehensive benchmark results for a quantization method.\"\"\"\n",
    "    \n",
    "    # Identification\n",
    "    method_name: str\n",
    "    quantization_bits: Optional[int]\n",
    "    \n",
    "    # Perplexity metrics\n",
    "    perplexity_wikitext: float = 0.0\n",
    "    perplexity_c4: float = 0.0\n",
    "    perplexity_code: float = 0.0\n",
    "    perplexity_avg: float = 0.0\n",
    "    perplexity_std: float = 0.0\n",
    "    \n",
    "    # MMLU metrics\n",
    "    mmlu_accuracy: float = 0.0\n",
    "    mmlu_by_subject: Dict[str, float] = field(default_factory=dict)\n",
    "    \n",
    "    # Performance metrics\n",
    "    inference_latency_ms: float = 0.0\n",
    "    tokens_per_second: float = 0.0\n",
    "    memory_gb: float = 0.0\n",
    "    \n",
    "    # Quality degradation (vs FP16 baseline)\n",
    "    perplexity_increase_pct: float = 0.0\n",
    "    mmlu_decrease_pct: float = 0.0\n",
    "    \n",
    "    # Metadata\n",
    "    timestamp: str = \"\"\n",
    "    notes: str = \"\"\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convert to dictionary for serialization.\"\"\"\n",
    "        return {\n",
    "            'method_name': self.method_name,\n",
    "            'quantization_bits': self.quantization_bits,\n",
    "            'perplexity_wikitext': self.perplexity_wikitext,\n",
    "            'perplexity_c4': self.perplexity_c4,\n",
    "            'perplexity_code': self.perplexity_code,\n",
    "            'perplexity_avg': self.perplexity_avg,\n",
    "            'perplexity_std': self.perplexity_std,\n",
    "            'mmlu_accuracy': self.mmlu_accuracy,\n",
    "            'mmlu_by_subject': self.mmlu_by_subject,\n",
    "            'inference_latency_ms': self.inference_latency_ms,\n",
    "            'tokens_per_second': self.tokens_per_second,\n",
    "            'memory_gb': self.memory_gb,\n",
    "            'perplexity_increase_pct': self.perplexity_increase_pct,\n",
    "            'mmlu_decrease_pct': self.mmlu_decrease_pct,\n",
    "            'timestamp': self.timestamp,\n",
    "            'notes': self.notes\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"Benchmark data structures defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Perplexity Benchmarking\n",
    "\n",
    "### What is Perplexity?\n",
    "\n",
    "```\n",
    "Professor SPARK's ELI5:\n",
    "\n",
    "\"Imagine you're playing a guessing game. I show you a sentence with a blank:\n",
    "'The cat sat on the ___'\n",
    "\n",
    "A good model confidently guesses 'mat' or 'floor'. A confused model might\n",
    "consider 'elephant' or 'democracy' as equally likely.\n",
    "\n",
    "Perplexity = how confused is the model?\n",
    "- Lower = more confident = better\n",
    "- FP16: ~10\n",
    "- Good INT4: ~10.5 (5% increase)\n",
    "- Bad quantization: ~15+ (50% increase, model is broken)\"\n",
    "```\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "$$\\text{Perplexity} = \\exp\\left(-\\frac{1}{N}\\sum_{i=1}^{N} \\log P(w_i | w_1, ..., w_{i-1})\\right)$$\n",
    "\n",
    "### 2.1 Perplexity Calculator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerplexityBenchmark:\n",
    "    \"\"\"\n",
    "    Comprehensive perplexity benchmarking across multiple datasets.\n",
    "    \n",
    "    Supports:\n",
    "    - WikiText-2 (general text)\n",
    "    - C4 (web text)\n",
    "    - Code (programming)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: BenchmarkConfig):\n",
    "        self.config = config\n",
    "        self.datasets = {}\n",
    "        self._load_datasets()\n",
    "    \n",
    "    def _load_datasets(self):\n",
    "        \"\"\"Load evaluation datasets.\"\"\"\n",
    "        print(\"Loading perplexity evaluation datasets...\")\n",
    "        \n",
    "        # WikiText-2 for general language modeling\n",
    "        try:\n",
    "            wikitext = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "            self.datasets['wikitext'] = self._prepare_texts(\n",
    "                wikitext['text'], \n",
    "                self.config.perplexity_samples\n",
    "            )\n",
    "            print(f\"  WikiText-2: {len(self.datasets['wikitext'])} samples\")\n",
    "        except Exception as e:\n",
    "            print(f\"  WikiText-2: Failed to load ({e})\")\n",
    "            self.datasets['wikitext'] = []\n",
    "        \n",
    "        # C4 for web text\n",
    "        try:\n",
    "            c4 = load_dataset(\n",
    "                \"allenai/c4\", \n",
    "                \"en\", \n",
    "                split=\"validation\", \n",
    "                streaming=True\n",
    "            )\n",
    "            c4_texts = []\n",
    "            for i, item in enumerate(c4):\n",
    "                if i >= self.config.perplexity_samples:\n",
    "                    break\n",
    "                c4_texts.append(item['text'])\n",
    "            self.datasets['c4'] = c4_texts\n",
    "            print(f\"  C4: {len(self.datasets['c4'])} samples\")\n",
    "        except Exception as e:\n",
    "            print(f\"  C4: Failed to load ({e})\")\n",
    "            self.datasets['c4'] = []\n",
    "        \n",
    "        # Code dataset\n",
    "        try:\n",
    "            code = load_dataset(\n",
    "                \"codeparrot/github-code\", \n",
    "                streaming=True, \n",
    "                split=\"train\",\n",
    "                languages=[\"Python\"]\n",
    "            )\n",
    "            code_texts = []\n",
    "            for i, item in enumerate(code):\n",
    "                if i >= self.config.perplexity_samples:\n",
    "                    break\n",
    "                if len(item['code']) > 100:  # Skip very short snippets\n",
    "                    code_texts.append(item['code'])\n",
    "            self.datasets['code'] = code_texts\n",
    "            print(f\"  Code: {len(self.datasets['code'])} samples\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Code: Failed to load ({e})\")\n",
    "            self.datasets['code'] = []\n",
    "    \n",
    "    def _prepare_texts(self, texts: List[str], n_samples: int) -> List[str]:\n",
    "        \"\"\"Filter and prepare text samples.\"\"\"\n",
    "        # Filter out empty or very short texts\n",
    "        valid_texts = [t for t in texts if len(t.strip()) > 100]\n",
    "        return valid_texts[:n_samples]\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def calculate_perplexity(\n",
    "        self,\n",
    "        model: AutoModelForCausalLM,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        texts: List[str],\n",
    "        desc: str = \"Calculating perplexity\"\n",
    "    ) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Calculate perplexity over a list of texts.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (mean_perplexity, std_perplexity)\n",
    "        \"\"\"\n",
    "        perplexities = []\n",
    "        \n",
    "        for text in tqdm(texts, desc=desc):\n",
    "            if not text.strip():\n",
    "                continue\n",
    "            \n",
    "            # Tokenize\n",
    "            encodings = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=self.config.max_length\n",
    "            )\n",
    "            \n",
    "            input_ids = encodings.input_ids.to(model.device)\n",
    "            \n",
    "            if input_ids.size(1) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Get model outputs\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss.item()\n",
    "            \n",
    "            # Perplexity = exp(loss)\n",
    "            ppl = np.exp(loss)\n",
    "            \n",
    "            # Skip extreme outliers (indicates issues)\n",
    "            if ppl < 1000:\n",
    "                perplexities.append(ppl)\n",
    "        \n",
    "        if not perplexities:\n",
    "            return float('inf'), 0.0\n",
    "        \n",
    "        return np.mean(perplexities), np.std(perplexities)\n",
    "    \n",
    "    def run_benchmark(\n",
    "        self,\n",
    "        model: AutoModelForCausalLM,\n",
    "        tokenizer: AutoTokenizer\n",
    "    ) -> Dict[str, Tuple[float, float]]:\n",
    "        \"\"\"\n",
    "        Run perplexity benchmark across all datasets.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping dataset name to (mean_ppl, std_ppl)\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for name, texts in self.datasets.items():\n",
    "            if not texts:\n",
    "                print(f\"  Skipping {name} (no data)\")\n",
    "                results[name] = (float('inf'), 0.0)\n",
    "                continue\n",
    "            \n",
    "            mean_ppl, std_ppl = self.calculate_perplexity(\n",
    "                model, tokenizer, texts, desc=f\"  {name}\"\n",
    "            )\n",
    "            results[name] = (mean_ppl, std_ppl)\n",
    "            print(f\"  {name}: {mean_ppl:.2f} (Â±{std_ppl:.2f})\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# Initialize perplexity benchmark\n",
    "ppl_benchmark = PerplexityBenchmark(config)\n",
    "print(\"\\nPerplexity benchmark ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: MMLU Benchmark\n",
    "\n",
    "### What is MMLU?\n",
    "\n",
    "```\n",
    "Professor SPARK's ELI5:\n",
    "\n",
    "\"MMLU is like a standardized test covering 57 subjects - from astronomy to\n",
    "zoology. It measures: 'Does the model still know stuff after quantization?'\n",
    "\n",
    "Example question:\n",
    "Q: What is the capital of France?\n",
    "A) London  B) Paris  C) Berlin  D) Rome\n",
    "\n",
    "A good FP16 model: 70% accuracy\n",
    "Good INT4: 68% accuracy (acceptable)\n",
    "Bad quantization: 50% (random guessing = broken)\"\n",
    "```\n",
    "\n",
    "### 3.1 MMLU Evaluator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMLUBenchmark:\n",
    "    \"\"\"\n",
    "    MMLU (Massive Multitask Language Understanding) benchmark.\n",
    "    \n",
    "    Evaluates model knowledge across diverse subjects.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: BenchmarkConfig):\n",
    "        self.config = config\n",
    "        self.subjects_data = {}\n",
    "        self._load_mmlu()\n",
    "    \n",
    "    def _load_mmlu(self):\n",
    "        \"\"\"Load MMLU dataset for configured subjects.\"\"\"\n",
    "        print(\"Loading MMLU evaluation dataset...\")\n",
    "        \n",
    "        for subject in self.config.mmlu_subjects:\n",
    "            try:\n",
    "                dataset = load_dataset(\n",
    "                    \"cais/mmlu\",\n",
    "                    subject,\n",
    "                    split=\"test\"\n",
    "                )\n",
    "                # Limit samples per subject\n",
    "                samples = list(dataset)[:self.config.mmlu_samples_per_subject]\n",
    "                self.subjects_data[subject] = samples\n",
    "                print(f\"  {subject}: {len(samples)} samples\")\n",
    "            except Exception as e:\n",
    "                print(f\"  {subject}: Failed to load ({e})\")\n",
    "    \n",
    "    def _format_question(self, item: dict) -> str:\n",
    "        \"\"\"Format MMLU question for model input.\"\"\"\n",
    "        question = item['question']\n",
    "        choices = item['choices']\n",
    "        \n",
    "        formatted = f\"Question: {question}\\n\\n\"\n",
    "        for i, choice in enumerate(choices):\n",
    "            formatted += f\"{chr(65+i)}. {choice}\\n\"\n",
    "        formatted += \"\\nAnswer:\"\n",
    "        \n",
    "        return formatted\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate_question(\n",
    "        self,\n",
    "        model: AutoModelForCausalLM,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        item: dict\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Evaluate a single MMLU question.\n",
    "        \n",
    "        Uses log-probability scoring to select the answer.\n",
    "        \"\"\"\n",
    "        prompt = self._format_question(item)\n",
    "        correct_answer = item['answer']  # Index 0-3\n",
    "        \n",
    "        # Tokenize prompt\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # Get log probabilities for answer tokens (A, B, C, D)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[0, -1, :]  # Last position logits\n",
    "        \n",
    "        # Get token IDs for A, B, C, D\n",
    "        answer_tokens = [\n",
    "            tokenizer.encode(\" A\", add_special_tokens=False)[-1],\n",
    "            tokenizer.encode(\" B\", add_special_tokens=False)[-1],\n",
    "            tokenizer.encode(\" C\", add_special_tokens=False)[-1],\n",
    "            tokenizer.encode(\" D\", add_special_tokens=False)[-1],\n",
    "        ]\n",
    "        \n",
    "        # Get logits for answer tokens\n",
    "        answer_logits = [logits[t].item() for t in answer_tokens]\n",
    "        \n",
    "        # Model's prediction is the highest logit\n",
    "        predicted = np.argmax(answer_logits)\n",
    "        \n",
    "        return predicted == correct_answer\n",
    "    \n",
    "    def run_benchmark(\n",
    "        self,\n",
    "        model: AutoModelForCausalLM,\n",
    "        tokenizer: AutoTokenizer\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Run MMLU benchmark across all subjects.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping subject name to accuracy\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        total_correct = 0\n",
    "        total_questions = 0\n",
    "        \n",
    "        for subject, samples in self.subjects_data.items():\n",
    "            if not samples:\n",
    "                continue\n",
    "            \n",
    "            correct = 0\n",
    "            for item in tqdm(samples, desc=f\"  {subject}\"):\n",
    "                if self.evaluate_question(model, tokenizer, item):\n",
    "                    correct += 1\n",
    "            \n",
    "            accuracy = correct / len(samples)\n",
    "            results[subject] = accuracy\n",
    "            total_correct += correct\n",
    "            total_questions += len(samples)\n",
    "            \n",
    "            print(f\"  {subject}: {accuracy:.1%}\")\n",
    "        \n",
    "        # Overall accuracy\n",
    "        if total_questions > 0:\n",
    "            results['overall'] = total_correct / total_questions\n",
    "            print(f\"\\n  Overall MMLU: {results['overall']:.1%}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# Initialize MMLU benchmark\n",
    "mmlu_benchmark = MMLUBenchmark(config)\n",
    "print(\"\\nMMLU benchmark ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Unified Benchmark Runner\n",
    "\n",
    "### 4.1 Complete Benchmark Suite Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityBenchmarkSuite:\n",
    "    \"\"\"\n",
    "    Complete quality benchmark suite for comparing quantization methods.\n",
    "    \n",
    "    Runs:\n",
    "    - Perplexity across multiple datasets\n",
    "    - MMLU knowledge evaluation\n",
    "    - Inference performance metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: BenchmarkConfig):\n",
    "        self.config = config\n",
    "        self.ppl_benchmark = PerplexityBenchmark(config)\n",
    "        self.mmlu_benchmark = MMLUBenchmark(config)\n",
    "        self.results: List[QualityBenchmarkResult] = []\n",
    "        self.baseline: Optional[QualityBenchmarkResult] = None\n",
    "    \n",
    "    def benchmark_model(\n",
    "        self,\n",
    "        model: AutoModelForCausalLM,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        method_name: str,\n",
    "        bits: Optional[int] = None,\n",
    "        run_mmlu: bool = True,\n",
    "        notes: str = \"\"\n",
    "    ) -> QualityBenchmarkResult:\n",
    "        \"\"\"\n",
    "        Run complete benchmark suite on a model.\n",
    "        \n",
    "        Args:\n",
    "            model: The model to benchmark\n",
    "            tokenizer: Model's tokenizer\n",
    "            method_name: Name of quantization method\n",
    "            bits: Bit width (e.g., 4, 8, 16)\n",
    "            run_mmlu: Whether to run MMLU (slower)\n",
    "            notes: Additional notes\n",
    "            \n",
    "        Returns:\n",
    "            QualityBenchmarkResult with all metrics\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Benchmarking: {method_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        result = QualityBenchmarkResult(\n",
    "            method_name=method_name,\n",
    "            quantization_bits=bits,\n",
    "            timestamp=time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            notes=notes\n",
    "        )\n",
    "        \n",
    "        # Memory usage\n",
    "        memory_info = get_gpu_memory()\n",
    "        result.memory_gb = memory_info.get('used_gb', 0)\n",
    "        print(f\"\\nMemory used: {result.memory_gb:.2f} GB\")\n",
    "        \n",
    "        # Perplexity benchmark\n",
    "        print(\"\\n--- Perplexity Benchmark ---\")\n",
    "        ppl_results = self.ppl_benchmark.run_benchmark(model, tokenizer)\n",
    "        \n",
    "        result.perplexity_wikitext = ppl_results.get('wikitext', (float('inf'), 0))[0]\n",
    "        result.perplexity_c4 = ppl_results.get('c4', (float('inf'), 0))[0]\n",
    "        result.perplexity_code = ppl_results.get('code', (float('inf'), 0))[0]\n",
    "        \n",
    "        # Average perplexity (excluding inf)\n",
    "        valid_ppls = [p for p in [result.perplexity_wikitext, result.perplexity_c4, result.perplexity_code] if p < float('inf')]\n",
    "        if valid_ppls:\n",
    "            result.perplexity_avg = np.mean(valid_ppls)\n",
    "            result.perplexity_std = np.std(valid_ppls)\n",
    "        \n",
    "        # MMLU benchmark (optional)\n",
    "        if run_mmlu:\n",
    "            print(\"\\n--- MMLU Benchmark ---\")\n",
    "            mmlu_results = self.mmlu_benchmark.run_benchmark(model, tokenizer)\n",
    "            result.mmlu_accuracy = mmlu_results.get('overall', 0.0)\n",
    "            result.mmlu_by_subject = {\n",
    "                k: v for k, v in mmlu_results.items() if k != 'overall'\n",
    "            }\n",
    "        \n",
    "        # Inference performance\n",
    "        print(\"\\n--- Inference Performance ---\")\n",
    "        perf_result = benchmark_inference(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=\"The future of artificial intelligence is\",\n",
    "            max_new_tokens=50,\n",
    "            num_runs=self.config.num_inference_runs,\n",
    "            warmup_runs=self.config.warmup_runs\n",
    "        )\n",
    "        \n",
    "        result.inference_latency_ms = perf_result.mean_latency_ms\n",
    "        result.tokens_per_second = perf_result.tokens_per_second\n",
    "        print(f\"  Latency: {result.inference_latency_ms:.2f} ms\")\n",
    "        print(f\"  Throughput: {result.tokens_per_second:.1f} tok/s\")\n",
    "        \n",
    "        # Calculate degradation vs baseline\n",
    "        if self.baseline:\n",
    "            if self.baseline.perplexity_avg > 0:\n",
    "                result.perplexity_increase_pct = (\n",
    "                    (result.perplexity_avg - self.baseline.perplexity_avg) / \n",
    "                    self.baseline.perplexity_avg * 100\n",
    "                )\n",
    "            if self.baseline.mmlu_accuracy > 0:\n",
    "                result.mmlu_decrease_pct = (\n",
    "                    (self.baseline.mmlu_accuracy - result.mmlu_accuracy) / \n",
    "                    self.baseline.mmlu_accuracy * 100\n",
    "                )\n",
    "        \n",
    "        # Store result\n",
    "        self.results.append(result)\n",
    "        \n",
    "        # Set as baseline if this is FP16\n",
    "        if method_name.upper() == 'FP16' or bits == 16:\n",
    "            self.baseline = result\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def save_results(self, filename: str = \"benchmark_results.json\"):\n",
    "        \"\"\"Save all results to JSON.\"\"\"\n",
    "        filepath = Path(self.config.results_dir) / filename\n",
    "        \n",
    "        data = {\n",
    "            'config': {\n",
    "                'model': self.config.model_name,\n",
    "                'perplexity_samples': self.config.perplexity_samples,\n",
    "                'mmlu_subjects': self.config.mmlu_subjects\n",
    "            },\n",
    "            'results': [r.to_dict() for r in self.results]\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nResults saved to {filepath}\")\n",
    "    \n",
    "    def get_comparison_dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"Convert results to DataFrame for analysis.\"\"\"\n",
    "        return pd.DataFrame([r.to_dict() for r in self.results])\n",
    "\n",
    "\n",
    "# Create benchmark suite\n",
    "benchmark_suite = QualityBenchmarkSuite(config)\n",
    "print(\"\\nQuality Benchmark Suite ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Run Benchmarks Across Quantization Methods\n",
    "\n",
    "### 5.1 Benchmark FP16 Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FP16 baseline model\n",
    "print(\"Loading FP16 baseline model...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Run benchmark\n",
    "result_fp16 = benchmark_suite.benchmark_model(\n",
    "    model=model_fp16,\n",
    "    tokenizer=tokenizer,\n",
    "    method_name=\"FP16\",\n",
    "    bits=16,\n",
    "    run_mmlu=True,\n",
    "    notes=\"Baseline model at full precision\"\n",
    ")\n",
    "\n",
    "# Clear memory\n",
    "del model_fp16\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Benchmark INT8 Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load INT8 model\n",
    "print(\"Loading INT8 quantized model...\")\n",
    "\n",
    "bnb_config_int8 = BitsAndBytesConfig(\n",
    "    load_in_8bit=True\n",
    ")\n",
    "\n",
    "model_int8 = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    quantization_config=bnb_config_int8,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Run benchmark\n",
    "result_int8 = benchmark_suite.benchmark_model(\n",
    "    model=model_int8,\n",
    "    tokenizer=tokenizer,\n",
    "    method_name=\"INT8 (BnB)\",\n",
    "    bits=8,\n",
    "    run_mmlu=True,\n",
    "    notes=\"bitsandbytes 8-bit quantization\"\n",
    ")\n",
    "\n",
    "del model_int8\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Benchmark INT4 (NF4) Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load INT4/NF4 model\n",
    "print(\"Loading INT4 (NF4) quantized model...\")\n",
    "\n",
    "bnb_config_nf4 = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model_nf4 = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    quantization_config=bnb_config_nf4,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Run benchmark\n",
    "result_nf4 = benchmark_suite.benchmark_model(\n",
    "    model=model_nf4,\n",
    "    tokenizer=tokenizer,\n",
    "    method_name=\"INT4 (NF4)\",\n",
    "    bits=4,\n",
    "    run_mmlu=True,\n",
    "    notes=\"bitsandbytes NF4 with double quantization\"\n",
    ")\n",
    "\n",
    "del model_nf4\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Benchmark GPTQ (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load GPTQ model\n",
    "try:\n",
    "    from auto_gptq import AutoGPTQForCausalLM\n",
    "    \n",
    "    # Use a pre-quantized GPTQ model\n",
    "    gptq_model_name = \"TheBloke/Llama-2-7B-GPTQ\"  # Example GPTQ model\n",
    "    \n",
    "    print(f\"Loading GPTQ model: {gptq_model_name}...\")\n",
    "    \n",
    "    model_gptq = AutoGPTQForCausalLM.from_quantized(\n",
    "        gptq_model_name,\n",
    "        device_map=\"auto\",\n",
    "        use_safetensors=True\n",
    "    )\n",
    "    tokenizer_gptq = AutoTokenizer.from_pretrained(gptq_model_name)\n",
    "    \n",
    "    # Run benchmark\n",
    "    result_gptq = benchmark_suite.benchmark_model(\n",
    "        model=model_gptq,\n",
    "        tokenizer=tokenizer_gptq,\n",
    "        method_name=\"GPTQ\",\n",
    "        bits=4,\n",
    "        run_mmlu=True,\n",
    "        notes=\"Pre-quantized GPTQ model\"\n",
    "    )\n",
    "    \n",
    "    del model_gptq\n",
    "    clear_memory()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"GPTQ not available. Skipping GPTQ benchmark.\")\n",
    "    print(\"Install with: pip install auto-gptq\")\n",
    "except Exception as e:\n",
    "    print(f\"GPTQ benchmark failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Benchmark AWQ (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load AWQ model\n",
    "try:\n",
    "    from awq import AutoAWQForCausalLM\n",
    "    \n",
    "    # Use a pre-quantized AWQ model\n",
    "    awq_model_name = \"TheBloke/Llama-2-7B-AWQ\"  # Example AWQ model\n",
    "    \n",
    "    print(f\"Loading AWQ model: {awq_model_name}...\")\n",
    "    \n",
    "    model_awq = AutoAWQForCausalLM.from_quantized(\n",
    "        awq_model_name,\n",
    "        fuse_layers=True\n",
    "    )\n",
    "    tokenizer_awq = AutoTokenizer.from_pretrained(awq_model_name)\n",
    "    \n",
    "    # Run benchmark  \n",
    "    result_awq = benchmark_suite.benchmark_model(\n",
    "        model=model_awq,\n",
    "        tokenizer=tokenizer_awq,\n",
    "        method_name=\"AWQ\",\n",
    "        bits=4,\n",
    "        run_mmlu=True,\n",
    "        notes=\"Pre-quantized AWQ model with fused layers\"\n",
    "    )\n",
    "    \n",
    "    del model_awq\n",
    "    clear_memory()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"AWQ not available. Skipping AWQ benchmark.\")\n",
    "    print(\"Install with: pip install autoawq\")\n",
    "except Exception as e:\n",
    "    print(f\"AWQ benchmark failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all benchmark results\n",
    "benchmark_suite.save_results(\"quality_benchmark_results.json\")\n",
    "\n",
    "# Get DataFrame for analysis\n",
    "df_results = benchmark_suite.get_comparison_dataframe()\n",
    "print(\"\\nBenchmark Results Summary:\")\n",
    "print(df_results[['method_name', 'perplexity_avg', 'mmlu_accuracy', \n",
    "                  'tokens_per_second', 'memory_gb']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Results Visualization and Analysis\n",
    "\n",
    "### 6.1 Perplexity Comparison Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perplexity_comparison(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create perplexity comparison chart across methods.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar chart of average perplexity\n",
    "    ax1 = axes[0]\n",
    "    methods = df['method_name']\n",
    "    ppls = df['perplexity_avg']\n",
    "    \n",
    "    colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(methods)))\n",
    "    bars = ax1.bar(methods, ppls, color=colors, edgecolor='black', linewidth=1.2)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, ppl in zip(bars, ppls):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                 f'{ppl:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax1.set_xlabel('Quantization Method', fontsize=12)\n",
    "    ax1.set_ylabel('Perplexity (lower is better)', fontsize=12)\n",
    "    ax1.set_title('Average Perplexity by Method', fontsize=14, fontweight='bold')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add baseline reference line\n",
    "    if 'FP16' in methods.values:\n",
    "        baseline_ppl = df[df['method_name'] == 'FP16']['perplexity_avg'].values[0]\n",
    "        ax1.axhline(y=baseline_ppl, color='green', linestyle='--', \n",
    "                    label=f'FP16 Baseline: {baseline_ppl:.2f}')\n",
    "        ax1.legend()\n",
    "    \n",
    "    # Perplexity by dataset\n",
    "    ax2 = axes[1]\n",
    "    x = np.arange(len(methods))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax2.bar(x - width, df['perplexity_wikitext'], width, label='WikiText', color='#3498db')\n",
    "    ax2.bar(x, df['perplexity_c4'], width, label='C4', color='#e74c3c')\n",
    "    ax2.bar(x + width, df['perplexity_code'], width, label='Code', color='#2ecc71')\n",
    "    \n",
    "    ax2.set_xlabel('Quantization Method', fontsize=12)\n",
    "    ax2.set_ylabel('Perplexity', fontsize=12)\n",
    "    ax2.set_title('Perplexity by Dataset', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(methods, rotation=45, ha='right')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(config.results_dir) / 'perplexity_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Generate perplexity chart\n",
    "if len(df_results) > 0:\n",
    "    plot_perplexity_comparison(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 MMLU Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mmlu_comparison(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create MMLU accuracy comparison chart.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Overall MMLU accuracy\n",
    "    ax1 = axes[0]\n",
    "    methods = df['method_name']\n",
    "    accuracies = df['mmlu_accuracy'] * 100  # Convert to percentage\n",
    "    \n",
    "    colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(methods)))\n",
    "    bars = ax1.bar(methods, accuracies, color=colors, edgecolor='black', linewidth=1.2)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                 f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax1.set_xlabel('Quantization Method', fontsize=12)\n",
    "    ax1.set_ylabel('MMLU Accuracy (%)', fontsize=12)\n",
    "    ax1.set_title('MMLU Accuracy by Method', fontsize=14, fontweight='bold')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.set_ylim(0, 100)\n",
    "    \n",
    "    # Add random baseline (25% for 4 choices)\n",
    "    ax1.axhline(y=25, color='red', linestyle='--', label='Random Baseline (25%)')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Accuracy degradation\n",
    "    ax2 = axes[1]\n",
    "    degradations = df['mmlu_decrease_pct']\n",
    "    \n",
    "    colors = ['#2ecc71' if d <= 2 else '#f1c40f' if d <= 5 else '#e74c3c' \n",
    "              for d in degradations]\n",
    "    bars = ax2.bar(methods, degradations, color=colors, edgecolor='black', linewidth=1.2)\n",
    "    \n",
    "    for bar, deg in zip(bars, degradations):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                 f'{deg:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax2.set_xlabel('Quantization Method', fontsize=12)\n",
    "    ax2.set_ylabel('Accuracy Degradation (%)', fontsize=12)\n",
    "    ax2.set_title('MMLU Accuracy Degradation vs FP16', fontsize=14, fontweight='bold')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add threshold lines\n",
    "    ax2.axhline(y=2, color='green', linestyle='--', alpha=0.5, label='Good (<2%)')\n",
    "    ax2.axhline(y=5, color='orange', linestyle='--', alpha=0.5, label='Acceptable (<5%)')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(config.results_dir) / 'mmlu_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Generate MMLU chart\n",
    "if len(df_results) > 0 and df_results['mmlu_accuracy'].sum() > 0:\n",
    "    plot_mmlu_comparison(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Quality vs Performance Trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_quality_performance_tradeoff(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create quality vs performance scatter plot.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Perplexity vs Throughput\n",
    "    ax1 = axes[0]\n",
    "    scatter = ax1.scatter(\n",
    "        df['tokens_per_second'], \n",
    "        df['perplexity_avg'],\n",
    "        c=df['memory_gb'],\n",
    "        s=200,\n",
    "        cmap='coolwarm',\n",
    "        edgecolors='black',\n",
    "        linewidth=1.5\n",
    "    )\n",
    "    \n",
    "    # Add labels\n",
    "    for i, row in df.iterrows():\n",
    "        ax1.annotate(\n",
    "            row['method_name'], \n",
    "            (row['tokens_per_second'], row['perplexity_avg']),\n",
    "            xytext=(5, 5), \n",
    "            textcoords='offset points',\n",
    "            fontsize=10,\n",
    "            fontweight='bold'\n",
    "        )\n",
    "    \n",
    "    ax1.set_xlabel('Throughput (tokens/sec)', fontsize=12)\n",
    "    ax1.set_ylabel('Perplexity (lower is better)', fontsize=12)\n",
    "    ax1.set_title('Quality vs Speed Trade-off', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    cbar = plt.colorbar(scatter, ax=ax1)\n",
    "    cbar.set_label('Memory (GB)', fontsize=10)\n",
    "    \n",
    "    # Ideal zone (top-right corner for good quality + speed)\n",
    "    ax1.annotate('Ideal Zone\\n(High Speed + Low Perplexity)', \n",
    "                 xy=(0.75, 0.25), xycoords='axes fraction',\n",
    "                 fontsize=10, style='italic', color='green')\n",
    "    \n",
    "    # MMLU vs Memory\n",
    "    ax2 = axes[1]\n",
    "    scatter2 = ax2.scatter(\n",
    "        df['memory_gb'],\n",
    "        df['mmlu_accuracy'] * 100,\n",
    "        c=df['tokens_per_second'],\n",
    "        s=200,\n",
    "        cmap='viridis',\n",
    "        edgecolors='black',\n",
    "        linewidth=1.5\n",
    "    )\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        ax2.annotate(\n",
    "            row['method_name'],\n",
    "            (row['memory_gb'], row['mmlu_accuracy'] * 100),\n",
    "            xytext=(5, 5),\n",
    "            textcoords='offset points',\n",
    "            fontsize=10,\n",
    "            fontweight='bold'\n",
    "        )\n",
    "    \n",
    "    ax2.set_xlabel('Memory Usage (GB)', fontsize=12)\n",
    "    ax2.set_ylabel('MMLU Accuracy (%)', fontsize=12)\n",
    "    ax2.set_title('Knowledge Retention vs Memory', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    cbar2 = plt.colorbar(scatter2, ax=ax2)\n",
    "    cbar2.set_label('Throughput (tok/s)', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(config.results_dir) / 'quality_performance_tradeoff.png', \n",
    "                dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Generate trade-off chart\n",
    "if len(df_results) > 0:\n",
    "    plot_quality_performance_tradeoff(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Comprehensive Radar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_radar_comparison(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create radar chart comparing all aspects of quantization methods.\n",
    "    \"\"\"\n",
    "    # Normalize metrics to 0-1 scale (higher is better)\n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    # Invert perplexity (lower is better -> higher is better)\n",
    "    ppl_max = df['perplexity_avg'].max()\n",
    "    df_norm['quality'] = 1 - (df['perplexity_avg'] / ppl_max) if ppl_max > 0 else 0\n",
    "    \n",
    "    # MMLU (already 0-1)\n",
    "    df_norm['knowledge'] = df['mmlu_accuracy']\n",
    "    \n",
    "    # Speed (normalize to max)\n",
    "    speed_max = df['tokens_per_second'].max()\n",
    "    df_norm['speed'] = df['tokens_per_second'] / speed_max if speed_max > 0 else 0\n",
    "    \n",
    "    # Memory efficiency (invert memory, lower is better)\n",
    "    mem_max = df['memory_gb'].max()\n",
    "    df_norm['efficiency'] = 1 - (df['memory_gb'] / mem_max) if mem_max > 0 else 0\n",
    "    \n",
    "    # Compression (based on bits)\n",
    "    df_norm['compression'] = 1 - (df['quantization_bits'].fillna(16) / 16)\n",
    "    \n",
    "    # Set up radar chart\n",
    "    categories = ['Quality', 'Knowledge', 'Speed', 'Efficiency', 'Compression']\n",
    "    N = len(categories)\n",
    "    \n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Close the polygon\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, len(df_norm)))\n",
    "    \n",
    "    for idx, (i, row) in enumerate(df_norm.iterrows()):\n",
    "        values = [\n",
    "            row['quality'],\n",
    "            row['knowledge'],\n",
    "            row['speed'],\n",
    "            row['efficiency'],\n",
    "            row['compression']\n",
    "        ]\n",
    "        values += values[:1]\n",
    "        \n",
    "        ax.plot(angles, values, 'o-', linewidth=2, \n",
    "                label=row['method_name'], color=colors[idx])\n",
    "        ax.fill(angles, values, alpha=0.1, color=colors[idx])\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories, fontsize=12, fontweight='bold')\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    plt.title('Quantization Method Comparison\\n(Higher is Better)', \n",
    "              size=14, fontweight='bold', y=1.08)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(config.results_dir) / 'radar_comparison.png', \n",
    "                dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Generate radar chart\n",
    "if len(df_results) > 0:\n",
    "    plot_radar_comparison(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Benchmark Report Generation\n",
    "\n",
    "### 7.1 Generate Comprehensive Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_benchmark_report(df: pd.DataFrame, config: BenchmarkConfig) -> str:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive markdown benchmark report.\n",
    "    \"\"\"\n",
    "    report = f\"\"\"\n",
    "# Quantization Quality Benchmark Report\n",
    "\n",
    "**Generated:** {time.strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "**Model:** {config.model_name}\n",
    "**Platform:** DGX Spark (Blackwell GPU, 128GB Unified Memory)\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This report compares {len(df)} quantization methods across quality and performance metrics.\n",
    "\n",
    "### Key Findings:\n",
    "\"\"\"\n",
    "    \n",
    "    # Find best methods\n",
    "    best_quality = df.loc[df['perplexity_avg'].idxmin()]['method_name']\n",
    "    best_speed = df.loc[df['tokens_per_second'].idxmax()]['method_name']\n",
    "    best_memory = df.loc[df['memory_gb'].idxmin()]['method_name']\n",
    "    \n",
    "    report += f\"\"\"\n",
    "- **Best Quality:** {best_quality} (lowest perplexity)\n",
    "- **Best Speed:** {best_speed} (highest throughput)\n",
    "- **Best Memory Efficiency:** {best_memory} (lowest memory usage)\n",
    "\n",
    "---\n",
    "\n",
    "## Detailed Results\n",
    "\n",
    "### Perplexity Scores (Lower is Better)\n",
    "\n",
    "| Method | WikiText | C4 | Code | Average | vs FP16 |\n",
    "|--------|----------|----|----- |---------|--------|\n",
    "\"\"\"\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        report += f\"| {row['method_name']} | {row['perplexity_wikitext']:.2f} | \"\n",
    "        report += f\"{row['perplexity_c4']:.2f} | {row['perplexity_code']:.2f} | \"\n",
    "        report += f\"{row['perplexity_avg']:.2f} | +{row['perplexity_increase_pct']:.1f}% |\\n\"\n",
    "    \n",
    "    report += f\"\"\"\n",
    "\n",
    "### MMLU Accuracy (Higher is Better)\n",
    "\n",
    "| Method | Overall Accuracy | vs FP16 |\n",
    "|--------|-----------------|--------|\n",
    "\"\"\"\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        report += f\"| {row['method_name']} | {row['mmlu_accuracy']*100:.1f}% | \"\n",
    "        report += f\"-{row['mmlu_decrease_pct']:.1f}% |\\n\"\n",
    "    \n",
    "    report += f\"\"\"\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "| Method | Throughput (tok/s) | Latency (ms) | Memory (GB) |\n",
    "|--------|-------------------|--------------|-------------|\n",
    "\"\"\"\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        report += f\"| {row['method_name']} | {row['tokens_per_second']:.1f} | \"\n",
    "        report += f\"{row['inference_latency_ms']:.1f} | {row['memory_gb']:.1f} |\\n\"\n",
    "    \n",
    "    report += f\"\"\"\n",
    "\n",
    "---\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "### Use Case Recommendations:\n",
    "\n",
    "1. **Production (Quality Critical):** FP16 or INT8 - minimal quality degradation\n",
    "2. **Edge Deployment (Memory Critical):** INT4 (NF4) or AWQ - best compression\n",
    "3. **High Throughput (Speed Critical):** GPTQ/AWQ with fused kernels\n",
    "4. **Blackwell Optimized:** NVFP4 - native hardware support\n",
    "\n",
    "### Quality Thresholds:\n",
    "\n",
    "- **Excellent:** <2% perplexity increase, <1% MMLU drop\n",
    "- **Good:** <5% perplexity increase, <2% MMLU drop  \n",
    "- **Acceptable:** <10% perplexity increase, <5% MMLU drop\n",
    "- **Poor:** >10% perplexity increase (investigate model)\n",
    "\n",
    "---\n",
    "\n",
    "*Report generated using DGX Spark Quality Benchmark Suite*\n",
    "\"\"\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "\n",
    "# Generate and save report\n",
    "if len(df_results) > 0:\n",
    "    report = generate_benchmark_report(df_results, config)\n",
    "    \n",
    "    report_path = Path(config.results_dir) / 'benchmark_report.md'\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(f\"Report saved to: {report_path}\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: Expected Results on DGX Spark\n",
    "\n",
    "### 8.1 Reference Benchmark Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected results for Llama-3.2-3B on DGX Spark\n",
    "expected_results = pd.DataFrame([\n",
    "    {\n",
    "        'Method': 'FP16',\n",
    "        'Bits': 16,\n",
    "        'WikiText PPL': 8.5,\n",
    "        'MMLU (%)': 65.0,\n",
    "        'Throughput': 45,\n",
    "        'Memory (GB)': 6.0,\n",
    "        'PPL Increase': '0%'\n",
    "    },\n",
    "    {\n",
    "        'Method': 'INT8 (BnB)',\n",
    "        'Bits': 8,\n",
    "        'WikiText PPL': 8.7,\n",
    "        'MMLU (%)': 64.5,\n",
    "        'Throughput': 55,\n",
    "        'Memory (GB)': 3.2,\n",
    "        'PPL Increase': '+2.4%'\n",
    "    },\n",
    "    {\n",
    "        'Method': 'INT4 (NF4)',\n",
    "        'Bits': 4,\n",
    "        'WikiText PPL': 9.2,\n",
    "        'MMLU (%)': 63.0,\n",
    "        'Throughput': 70,\n",
    "        'Memory (GB)': 1.8,\n",
    "        'PPL Increase': '+8.2%'\n",
    "    },\n",
    "    {\n",
    "        'Method': 'GPTQ-4bit',\n",
    "        'Bits': 4,\n",
    "        'WikiText PPL': 8.9,\n",
    "        'MMLU (%)': 63.8,\n",
    "        'Throughput': 85,\n",
    "        'Memory (GB)': 1.9,\n",
    "        'PPL Increase': '+4.7%'\n",
    "    },\n",
    "    {\n",
    "        'Method': 'AWQ-4bit',\n",
    "        'Bits': 4,\n",
    "        'WikiText PPL': 8.8,\n",
    "        'MMLU (%)': 64.0,\n",
    "        'Throughput': 90,\n",
    "        'Memory (GB)': 1.9,\n",
    "        'PPL Increase': '+3.5%'\n",
    "    },\n",
    "    {\n",
    "        'Method': 'FP8 (E4M3)',\n",
    "        'Bits': 8,\n",
    "        'WikiText PPL': 8.6,\n",
    "        'MMLU (%)': 64.8,\n",
    "        'Throughput': 95,\n",
    "        'Memory (GB)': 3.0,\n",
    "        'PPL Increase': '+1.2%'\n",
    "    },\n",
    "    {\n",
    "        'Method': 'NVFP4 (Blackwell)',\n",
    "        'Bits': 4,\n",
    "        'WikiText PPL': 8.7,\n",
    "        'MMLU (%)': 64.2,\n",
    "        'Throughput': 120,\n",
    "        'Memory (GB)': 1.6,\n",
    "        'PPL Increase': '+2.4%'\n",
    "    },\n",
    "])\n",
    "\n",
    "print(\"Expected Results on DGX Spark (Llama-3.2-3B):\")\n",
    "print(\"=\"*80)\n",
    "print(expected_results.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\n Key Observations:\")\n",
    "print(\" - FP8 and NVFP4 offer best quality/performance balance on Blackwell\")\n",
    "print(\" - AWQ slightly better quality than GPTQ at same bit-width\")\n",
    "print(\" - INT4 (NF4) shows largest quality degradation\")\n",
    "print(\" - NVFP4 achieves 2.7x speedup with only 2.4% perplexity increase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "```\n",
    "Professor SPARK's Summary:\n",
    "\n",
    "\"Quality benchmarking answers the question: 'Is my quantized model still useful?'\n",
    "\n",
    "Key metrics to track:\n",
    "1. Perplexity - Language modeling quality (lower = better)\n",
    "2. MMLU - Knowledge retention (higher = better)\n",
    "3. Throughput - Speed (higher = better)\n",
    "4. Memory - Efficiency (lower = better)\n",
    "\n",
    "The best quantization method depends on YOUR use case:\n",
    "- Chatbot? Prioritize MMLU (knowledge matters)\n",
    "- Text generation? Prioritize perplexity (fluency matters)\n",
    "- Edge deployment? Prioritize memory efficiency\n",
    "- Real-time? Prioritize throughput\n",
    "\n",
    "On DGX Spark Blackwell:\n",
    "- Use NVFP4 for best native performance\n",
    "- Use FP8 for training with minimal quality loss\n",
    "- Use AWQ/GPTQ for maximum compression with good quality\"\n",
    "```\n",
    "\n",
    "### Quality Thresholds for Production\n",
    "\n",
    "| Metric | Excellent | Good | Acceptable | Poor |\n",
    "|--------|-----------|------|------------|------|\n",
    "| Perplexity Increase | <2% | <5% | <10% | >10% |\n",
    "| MMLU Drop | <1% | <2% | <5% | >5% |\n",
    "| Memory Savings | >70% | >50% | >30% | <30% |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Lab 3.2.8:** TensorRT-LLM Engine - Production deployment optimization\n",
    "2. Apply benchmarks to YOUR models and use cases\n",
    "3. Create custom evaluation datasets for domain-specific applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Custom Perplexity Dataset\n",
    "\n",
    "Create a perplexity evaluation using a domain-specific dataset relevant to your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Add your domain-specific texts\n",
    "domain_texts = [\n",
    "    # TODO: Add 10-20 representative texts from your domain\n",
    "    \"Your domain text 1...\",\n",
    "    \"Your domain text 2...\",\n",
    "]\n",
    "\n",
    "# Calculate perplexity on domain data\n",
    "# ppl_benchmark.calculate_perplexity(model, tokenizer, domain_texts, \"Domain PPL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Custom MMLU Subject\n",
    "\n",
    "Create a custom knowledge evaluation for a specific subject not in MMLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Custom knowledge questions\n",
    "custom_questions = [\n",
    "    {\n",
    "        'question': 'Your question here?',\n",
    "        'choices': ['A', 'B', 'C', 'D'],\n",
    "        'answer': 0  # Index of correct answer\n",
    "    },\n",
    "    # Add more questions...\n",
    "]\n",
    "\n",
    "# Evaluate model on custom questions\n",
    "# correct = sum(mmlu_benchmark.evaluate_question(model, tokenizer, q) for q in custom_questions)\n",
    "# print(f\"Custom accuracy: {correct/len(custom_questions):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Statistical Significance\n",
    "\n",
    "Determine if differences between quantization methods are statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Statistical significance testing\n",
    "from scipy import stats\n",
    "\n",
    "# TODO: Compare perplexity distributions between methods\n",
    "# Use t-test or Mann-Whitney U test to determine significance\n",
    "\n",
    "# Example:\n",
    "# ppl_fp16 = [...perplexity values from FP16...]\n",
    "# ppl_int4 = [...perplexity values from INT4...]\n",
    "# stat, p_value = stats.ttest_ind(ppl_fp16, ppl_int4)\n",
    "# print(f\"p-value: {p_value:.4f}\")\n",
    "# print(f\"Significant difference: {p_value < 0.05}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
