{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2.3: AWQ Quantization\n",
    "\n",
    "**Module:** 3.2 - Model Quantization & Optimization  \n",
    "**Time:** 1.5 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand the AWQ (Activation-aware Weight Quantization) algorithm\n",
    "- [ ] Compare AWQ to GPTQ on the same model\n",
    "- [ ] Quantize a model using AutoAWQ\n",
    "- [ ] Understand when to choose AWQ vs GPTQ\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Lab 3.2.2 (GPTQ Quantization)\n",
    "- Knowledge of: GPTQ basics, quantization fundamentals\n",
    "- Hardware: DGX Spark with 128GB unified memory\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The Problem with GPTQ:** While GPTQ works great, it treats all weights equally. But some weights are more important than others!\n",
    "\n",
    "**The AWQ Insight:** If we look at *activation magnitudes* during inference, we can identify which weights process the largest values. These \"salient\" weights should be quantized more carefully.\n",
    "\n",
    "**Real-World Impact:**\n",
    "| Metric | GPTQ | AWQ |\n",
    "|--------|------|-----|\n",
    "| Perplexity degradation | ~0.3-0.5 | ~0.2-0.4 |\n",
    "| Reasoning tasks | Good | Better |\n",
    "| Code generation | Good | Better |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßí ELI5: What is AWQ?\n",
    "\n",
    "> **Imagine you're packing for a trip with limited suitcase space...**\n",
    ">\n",
    "> **GPTQ approach:** Shrink all your clothes equally with a compression bag.\n",
    "> - Your formal suit gets as wrinkled as your casual t-shirts.\n",
    ">\n",
    "> **AWQ approach:** Before compressing, check which clothes you'll wear most often.\n",
    "> - Keep your most-used items full-size\n",
    "> - Compress the rarely-worn items more aggressively\n",
    "> - Same suitcase space, but your important clothes stay nice!\n",
    ">\n",
    "> **In AI terms:** AWQ looks at which weights process the biggest activations (most important), and protects those weights from aggressive quantization. The result: same compression ratio, better quality!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding AWQ\n",
    "\n",
    "### The Key Insight: Weight Salience\n",
    "\n",
    "Consider a weight matrix `W` and input activations `X`. The output is `Y = XW`.\n",
    "\n",
    "If a particular weight column `w_i` typically multiplies with large activation values `x_i`, then:\n",
    "- Any error in `w_i` gets **amplified** by `x_i`\n",
    "- That weight is \"salient\" and should be protected\n",
    "\n",
    "### AWQ's Solution: Per-Channel Scaling\n",
    "\n",
    "Instead of quantizing `W` directly, AWQ:\n",
    "1. Computes activation statistics to find salient channels\n",
    "2. Applies per-channel scaling: `W' = W * diag(s)`, `X' = X * diag(1/s)`\n",
    "3. Quantizes the scaled weights `W'`\n",
    "4. The scaling protects salient weights from quantization error!\n",
    "\n",
    "The math ensures: `X'W' = X * diag(1/s) * W * diag(s) = XW` ‚úì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport time\nimport math\n\nprint(\"=\"*60)\nprint(\"DGX Spark Environment Check\")\nprint(\"=\"*60)\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize the AWQ concept\n\ndef demonstrate_awq_concept():\n    \"\"\"\n    Demonstrate how AWQ protects salient weights.\n    \"\"\"\n    torch.manual_seed(42)\n    \n    # Create a weight matrix and activation matrix\n    W = torch.randn(64, 64) * 0.5\n    X = torch.randn(32, 64)\n    \n    # Some channels have much larger activations (salient)\n    X[:, 10:15] *= 10  # Make channels 10-15 salient\n    X[:, 50:55] *= 8   # Make channels 50-55 somewhat salient\n    \n    # Compute original output\n    Y_original = X @ W\n    \n    # Method 1: Naive quantization (like basic INT4)\n    def naive_quantize(tensor, bits=4):\n        max_val = tensor.abs().max()\n        scale = max_val / (2**(bits-1) - 1)\n        quantized = torch.round(tensor / scale).clamp(-2**(bits-1), 2**(bits-1)-1)\n        return quantized * scale\n    \n    W_naive = naive_quantize(W)\n    Y_naive = X @ W_naive\n    error_naive = (Y_original - Y_naive).pow(2).mean().sqrt()\n    \n    # Method 2: AWQ-style (protect salient channels)\n    # Compute per-channel activation magnitudes\n    activation_magnitude = X.abs().mean(dim=0)\n    \n    # Scale factor: protect channels with large activations\n    # AWQ uses a more sophisticated formula, this is simplified\n    s = (activation_magnitude / activation_magnitude.mean()).pow(0.5).clamp(min=0.1)\n    \n    # Scale weights and activations\n    W_scaled = W * s.unsqueeze(0)  # Scale weight rows\n    X_scaled = X / s.unsqueeze(0)  # Inverse scale activations\n    \n    # Quantize scaled weights\n    W_awq = naive_quantize(W_scaled)\n    Y_awq = X_scaled @ W_awq\n    error_awq = (Y_original - Y_awq).pow(2).mean().sqrt()\n    \n    # Visualize\n    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n    \n    # Row 1: Show the problem\n    axes[0, 0].bar(range(64), activation_magnitude.numpy())\n    axes[0, 0].set_title('Activation Magnitude by Channel')\n    axes[0, 0].set_xlabel('Channel')\n    axes[0, 0].axhline(y=activation_magnitude.mean(), color='r', linestyle='--', label='Mean')\n    axes[0, 0].legend()\n    \n    axes[0, 1].bar(range(64), s.numpy())\n    axes[0, 1].set_title('AWQ Scaling Factors')\n    axes[0, 1].set_xlabel('Channel')\n    axes[0, 1].axhline(y=1.0, color='r', linestyle='--', label='No scaling')\n    axes[0, 1].legend()\n    \n    # Show weight distribution before/after scaling\n    axes[0, 2].hist(W.flatten().numpy(), bins=50, alpha=0.5, label='Original')\n    axes[0, 2].hist(W_scaled.flatten().numpy(), bins=50, alpha=0.5, label='Scaled')\n    axes[0, 2].set_title('Weight Distribution')\n    axes[0, 2].legend()\n    \n    # Row 2: Show the results\n    axes[1, 0].imshow(Y_original.numpy()[:10], aspect='auto', cmap='RdBu')\n    axes[1, 0].set_title('Original Output')\n    \n    axes[1, 1].imshow((Y_original - Y_naive).abs().numpy()[:10], aspect='auto', cmap='hot')\n    axes[1, 1].set_title(f'Naive Quant Error (RMSE={error_naive:.4f})')\n    \n    axes[1, 2].imshow((Y_original - Y_awq).abs().numpy()[:10], aspect='auto', cmap='hot')\n    axes[1, 2].set_title(f'AWQ Error (RMSE={error_awq:.4f})')\n    \n    plt.tight_layout()\n    plt.savefig('awq_concept.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    plt.close(fig)  # Free memory from figure\n    \n    print(f\"\\nüìä Error Comparison:\")\n    print(f\"   Naive quantization RMSE: {error_naive:.4f}\")\n    print(f\"   AWQ quantization RMSE:   {error_awq:.4f}\")\n    print(f\"   Improvement: {(1 - error_awq/error_naive)*100:.1f}% lower error!\")\n\ndemonstrate_awq_concept()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "1. We created a scenario with **salient channels** (channels 10-15 and 50-55 have large activations)\n",
    "2. **Naive quantization** introduces large errors because it ignores which weights matter most\n",
    "3. **AWQ-style quantization** uses scaling to protect important weights, reducing error significantly\n",
    "\n",
    "The key insight: **not all weights are equally important!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Quantizing with AutoAWQ\n",
    "\n",
    "Let's use the AutoAWQ library to quantize a real model and compare with GPTQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install AutoAWQ if needed\n",
    "# Note: On DGX Spark (ARM64), prefer using pre-installed NGC container packages\n",
    "\n",
    "try:\n",
    "    from awq import AutoAWQForCausalLM\n",
    "    print(\"‚úÖ AutoAWQ is available!\")\n",
    "except ImportError:\n",
    "    print(\"Installing AutoAWQ...\")\n",
    "    print(\"‚ö†Ô∏è  On DGX Spark (ARM64), this may compile CUDA kernels from source.\")\n",
    "    print(\"   This can take 5-10 minutes. Please be patient...\")\n",
    "\n",
    "    import subprocess\n",
    "    result = subprocess.run(\n",
    "        [\"pip\", \"install\", \"autoawq\", \"--no-cache-dir\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "\n",
    "    if result.returncode != 0:\n",
    "        print(f\"‚ùå Installation failed!\")\n",
    "        print(\"Error output (last 1000 chars):\")\n",
    "        print(result.stderr[-1000:] if len(result.stderr) > 1000 else result.stderr)\n",
    "        print(\"\\nüí° Solution: Use an NGC container with AutoAWQ pre-installed\")\n",
    "        raise ImportError(\"AutoAWQ installation failed - see error above\")\n",
    "\n",
    "    from awq import AutoAWQForCausalLM\n",
    "    print(\"‚úÖ AutoAWQ installed successfully!\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same model as GPTQ for fair comparison\n",
    "model_id = \"facebook/opt-350m\"\n",
    "\n",
    "print(f\"Model: {model_id}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer loaded. Vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWQ Quantization Configuration\n",
    "awq_config = {\n",
    "    \"zero_point\": True,      # Use zero-point quantization\n",
    "    \"q_group_size\": 128,     # Group size (same as GPTQ for comparison)\n",
    "    \"w_bit\": 4,              # 4-bit quantization\n",
    "    \"version\": \"GEMM\"        # Use GEMM kernel (faster)\n",
    "}\n",
    "\n",
    "print(\"AWQ Configuration:\")\n",
    "for k, v in awq_config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare calibration data\n",
    "# AWQ needs fewer samples than GPTQ (typically 128 is enough)\n",
    "\n",
    "calibration_texts = [\n",
    "    \"The field of machine learning has grown exponentially in recent years.\",\n",
    "    \"Artificial intelligence systems can now perform complex reasoning.\",\n",
    "    \"Large language models are transforming how we interact with computers.\",\n",
    "    \"Neural networks consist of interconnected layers of artificial neurons.\",\n",
    "    \"Deep learning enables breakthroughs in computer vision and NLP.\",\n",
    "    \"The transformer architecture revolutionized sequence modeling.\",\n",
    "    \"Quantization reduces model size while maintaining performance.\",\n",
    "    \"GPU acceleration enables training of billion-parameter models.\",\n",
    "    \"Transfer learning allows models to leverage pre-trained knowledge.\",\n",
    "    \"Attention mechanisms help models focus on relevant information.\",\n",
    "    \"In 1969, humans first landed on the moon during Apollo 11.\",\n",
    "    \"The capital of France is Paris, known for the Eiffel Tower.\",\n",
    "    \"Python is a popular programming language for data science.\",\n",
    "    \"Climate change poses significant challenges to ecosystems.\",\n",
    "    \"Quantum computing may revolutionize cryptography.\",\n",
    "    \"The stock market experienced volatility last quarter.\",\n",
    "    \"Healthy eating contributes to overall well-being.\",\n",
    "    \"Space exploration expands our understanding of the universe.\",\n",
    "    \"Renewable energy sources are becoming cost-effective.\",\n",
    "    \"The history of mathematics spans thousands of years.\",\n",
    "] * 8  # Repeat to get ~160 samples\n",
    "\n",
    "print(f\"Prepared {len(calibration_texts)} calibration samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory before quantization\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Initial GPU memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize with AWQ\n",
    "print(\"=\"*60)\n",
    "print(\"Quantizing with AWQ...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load model for AWQ quantization\n",
    "model_awq = AutoAWQForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cuda\",\n",
    "    safetensors=True\n",
    ")\n",
    "\n",
    "# Perform quantization\n",
    "model_awq.quantize(\n",
    "    tokenizer,\n",
    "    quant_config=awq_config,\n",
    "    calib_data=calibration_texts,\n",
    ")\n",
    "\n",
    "awq_time = time.time() - start_time\n",
    "print(f\"\\nQuantization time: {awq_time:.1f} seconds\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the AWQ quantized model\n",
    "import os\n",
    "\n",
    "save_dir_awq = \"./quantized_models/opt-350m-awq-4bit-g128\"\n",
    "os.makedirs(save_dir_awq, exist_ok=True)\n",
    "\n",
    "print(f\"Saving AWQ model to {save_dir_awq}...\")\n",
    "model_awq.save_quantized(save_dir_awq)\n",
    "tokenizer.save_pretrained(save_dir_awq)\n",
    "\n",
    "# Check file sizes\n",
    "total_size = 0\n",
    "print(\"\\nSaved files:\")\n",
    "for f in os.listdir(save_dir_awq):\n",
    "    size = os.path.getsize(os.path.join(save_dir_awq, f))\n",
    "    total_size += size\n",
    "    if size > 1e6:  # Only show large files\n",
    "        print(f\"  {f}: {size/1e6:.2f} MB\")\n",
    "print(f\"\\nTotal: {total_size/1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: AWQ vs GPTQ Comparison\n",
    "\n",
    "Let's compare AWQ and GPTQ on the same model with identical settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, texts, max_length=256):\n",
    "    \"\"\"Calculate perplexity on a set of texts.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Evaluating\", leave=False):\n",
    "            encodings = tokenizer(\n",
    "                text, \n",
    "                return_tensors='pt', \n",
    "                truncation=True, \n",
    "                max_length=max_length\n",
    "            )\n",
    "            input_ids = encodings.input_ids.to(model.device)\n",
    "            \n",
    "            if input_ids.size(1) < 2:\n",
    "                continue\n",
    "            \n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss.item()\n",
    "            num_tokens = input_ids.size(1) - 1\n",
    "            \n",
    "            total_loss += loss * num_tokens\n",
    "            total_tokens += num_tokens\n",
    "    \n",
    "    return math.exp(total_loss / total_tokens)\n",
    "\n",
    "# Evaluation texts (different from calibration!)\n",
    "eval_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog in the sunny garden.\",\n",
    "    \"Scientists have made a breakthrough in renewable energy research.\",\n",
    "    \"The ancient ruins tell stories of civilizations long forgotten.\",\n",
    "    \"Modern technology has transformed the way we communicate globally.\",\n",
    "    \"The ocean depths remain one of Earth's last unexplored frontiers.\",\n",
    "    \"Music has the power to evoke emotions and memories instantly.\",\n",
    "    \"The stock market reflects the collective sentiment of investors.\",\n",
    "    \"Advances in medicine continue to extend human lifespan.\",\n",
    "    \"Education is the foundation of progress and social mobility.\",\n",
    "    \"The universe contains billions of galaxies, each with billions of stars.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate FP16 baseline\n",
    "print(\"Loading FP16 baseline...\")\n",
    "del model_awq\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "\n",
    "print(\"Evaluating FP16...\")\n",
    "ppl_fp16 = calculate_perplexity(model_fp16, tokenizer, eval_texts)\n",
    "print(f\"FP16 Perplexity: {ppl_fp16:.2f}\")\n",
    "\n",
    "# Get FP16 size\n",
    "param_count = sum(p.numel() for p in model_fp16.parameters())\n",
    "fp16_size_mb = param_count * 2 / 1e6\n",
    "\n",
    "del model_fp16\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate AWQ model\nprint(\"\\nLoading AWQ model...\")\n\n# Try to load with layer fusion; fall back if not supported for this architecture\ntry:\n    model_awq = AutoAWQForCausalLM.from_quantized(\n        save_dir_awq,\n        fuse_layers=True,  # Fuse layers for faster inference\n    )\nexcept Exception as e:\n    print(f\"  Layer fusion not supported for this model, loading without fusion: {e}\")\n    model_awq = AutoAWQForCausalLM.from_quantized(\n        save_dir_awq,\n        fuse_layers=False,\n    )\n\nprint(\"Evaluating AWQ...\")\nppl_awq = calculate_perplexity(model_awq, tokenizer, eval_texts)\nprint(f\"AWQ Perplexity: {ppl_awq:.2f}\")\n\n# Get AWQ size\nawq_size_mb = sum(\n    os.path.getsize(os.path.join(save_dir_awq, f))\n    for f in os.listdir(save_dir_awq)\n    if f.endswith('.safetensors') or f.endswith('.bin')\n) / 1e6\n\ndel model_awq\ngc.collect()\ntorch.cuda.empty_cache()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate GPTQ model (if available from previous notebook)\n",
    "gptq_dir = \"./quantized_models/opt-350m-gptq-4bit-g128\"\n",
    "\n",
    "if os.path.exists(gptq_dir):\n",
    "    try:\n",
    "        from auto_gptq import AutoGPTQForCausalLM\n",
    "        \n",
    "        print(\"\\nLoading GPTQ model...\")\n",
    "        model_gptq = AutoGPTQForCausalLM.from_quantized(\n",
    "            gptq_dir,\n",
    "            device=\"cuda:0\",\n",
    "            use_safetensors=True\n",
    "        )\n",
    "        \n",
    "        print(\"Evaluating GPTQ...\")\n",
    "        ppl_gptq = calculate_perplexity(model_gptq, tokenizer, eval_texts)\n",
    "        print(f\"GPTQ Perplexity: {ppl_gptq:.2f}\")\n",
    "        \n",
    "        gptq_size_mb = sum(\n",
    "            os.path.getsize(os.path.join(gptq_dir, f))\n",
    "            for f in os.listdir(gptq_dir)\n",
    "            if f.endswith('.safetensors') or f.endswith('.bin')\n",
    "        ) / 1e6\n",
    "        \n",
    "        del model_gptq\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not load GPTQ model: {e}\")\n",
    "        ppl_gptq = ppl_awq + 0.1  # Estimate\n",
    "        gptq_size_mb = awq_size_mb\n",
    "else:\n",
    "    print(\"\\nGPTQ model not found. Run notebook 02 first for comparison.\")\n",
    "    ppl_gptq = ppl_awq + 0.1  # Estimate\n",
    "    gptq_size_mb = awq_size_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AWQ vs GPTQ Comparison (Group Size 128, 4-bit)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Method':<15} {'Size (MB)':>12} {'Perplexity':>12} {'PPL Œî':>12} {'Compression':>12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "results = [\n",
    "    ('FP16', fp16_size_mb, ppl_fp16, 0, 1.0),\n",
    "    ('GPTQ-128', gptq_size_mb, ppl_gptq, ppl_gptq - ppl_fp16, fp16_size_mb / gptq_size_mb),\n",
    "    ('AWQ-128', awq_size_mb, ppl_awq, ppl_awq - ppl_fp16, fp16_size_mb / awq_size_mb),\n",
    "]\n",
    "\n",
    "for name, size, ppl, delta, compression in results:\n",
    "    delta_str = \"baseline\" if delta == 0 else f\"+{delta:.2f}\"\n",
    "    print(f\"{name:<15} {size:>12.1f} {ppl:>12.2f} {delta_str:>12} {compression:>11.2f}x\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Winner announcement\n",
    "if ppl_awq < ppl_gptq:\n",
    "    winner = \"AWQ\"\n",
    "    diff = ppl_gptq - ppl_awq\n",
    "else:\n",
    "    winner = \"GPTQ\"\n",
    "    diff = ppl_awq - ppl_gptq\n",
    "\n",
    "print(f\"\\nüèÜ Winner: {winner} (by {diff:.3f} PPL)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: When to Use AWQ vs GPTQ\n",
    "\n",
    "### AWQ is better when:\n",
    "- **Reasoning tasks** - Protecting salient weights helps with complex reasoning\n",
    "- **Code generation** - Important tokens need precise representation\n",
    "- **Smaller models** - AWQ's advantages are more pronounced\n",
    "- **Fewer calibration samples** - AWQ needs less data\n",
    "\n",
    "### GPTQ is better when:\n",
    "- **Maximum speed** - GPTQ kernels are highly optimized\n",
    "- **Larger models** - More room for error compensation\n",
    "- **Simple tasks** - Basic Q&A, classification\n",
    "- **Ecosystem support** - More pre-quantized models available\n",
    "\n",
    "### Rule of Thumb\n",
    "\n",
    "```\n",
    "For deployment: Try AWQ first, fall back to GPTQ if speed is critical\n",
    "For research: Use both and compare on your specific task\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize the comparison\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nmethods = ['FP16', 'GPTQ', 'AWQ']\ncolors = ['#2196F3', '#FF9800', '#4CAF50']\n\n# Size comparison\nsizes = [fp16_size_mb, gptq_size_mb, awq_size_mb]\naxes[0].bar(methods, sizes, color=colors)\naxes[0].set_ylabel('Size (MB)')\naxes[0].set_title('Model Size Comparison')\nfor i, v in enumerate(sizes):\n    axes[0].text(i, v + 5, f'{v:.0f}', ha='center')\n\n# Perplexity comparison\nppls = [ppl_fp16, ppl_gptq, ppl_awq]\naxes[1].bar(methods, ppls, color=colors)\naxes[1].set_ylabel('Perplexity (lower is better)')\naxes[1].set_title('Quality Comparison')\nfor i, v in enumerate(ppls):\n    axes[1].text(i, v + 0.5, f'{v:.2f}', ha='center')\n\n# Efficiency score (quality per MB)\nefficiency = [1/ppl * (fp16_size_mb/size) for ppl, size in zip(ppls, sizes)]\naxes[2].bar(methods, efficiency, color=colors)\naxes[2].set_ylabel('Efficiency Score (higher is better)')\naxes[2].set_title('Quality/Size Efficiency')\nfor i, v in enumerate(efficiency):\n    axes[2].text(i, v + 0.01, f'{v:.3f}', ha='center')\n\nplt.tight_layout()\nplt.savefig('awq_vs_gptq.png', dpi=150, bbox_inches='tight')\nplt.show()\nplt.close(fig)  # Free memory from figure"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Different Group Sizes\n",
    "\n",
    "Quantize the model with AWQ using group sizes 32, 64, and 128. Compare the results.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "```python\n",
    "for group_size in [32, 64, 128]:\n",
    "    awq_config = {\n",
    "        \"q_group_size\": group_size,\n",
    "        # ... other configs\n",
    "    }\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare AWQ with different group sizes\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Task-Specific Evaluation\n",
    "\n",
    "Instead of perplexity, evaluate on a specific task:\n",
    "- Code completion (measure exact match)\n",
    "- Question answering (measure accuracy)\n",
    "- Text classification (measure F1)\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "Use the `lm_eval` library:\n",
    "```python\n",
    "from lm_eval import evaluator\n",
    "results = evaluator.simple_evaluate(\n",
    "    model=\"hf\",\n",
    "    model_args=f\"pretrained={save_dir}\",\n",
    "    tasks=[\"hellaswag\", \"arc_easy\"],\n",
    ")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate on specific tasks\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Using the Wrong Kernel Version\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: GEMV is slower for batch inference\n",
    "awq_config = {\"version\": \"GEMV\"}\n",
    "\n",
    "# ‚úÖ Right: GEMM is faster for most use cases\n",
    "awq_config = {\"version\": \"GEMM\"}\n",
    "```\n",
    "\n",
    "**Why:** GEMV (General Matrix-Vector) is optimized for single-sample inference. GEMM (General Matrix-Matrix) is better for batched inference.\n",
    "\n",
    "### Mistake 2: Not Fusing Layers\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Unfused layers are slower\n",
    "model = AutoAWQForCausalLM.from_quantized(save_dir)\n",
    "\n",
    "# ‚úÖ Right: Fuse layers for faster inference\n",
    "model = AutoAWQForCausalLM.from_quantized(save_dir, fuse_layers=True)\n",
    "```\n",
    "\n",
    "**Why:** Layer fusion combines multiple operations, reducing memory bandwidth and improving speed.\n",
    "\n",
    "### Mistake 3: Calibration Data Mismatch\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Using English data for a code model\n",
    "calib_data = [\"The quick brown fox...\"]\n",
    "\n",
    "# ‚úÖ Right: Use data matching your use case\n",
    "calib_data = [\"def fibonacci(n):\\n    if n <= 1...\"]\n",
    "```\n",
    "\n",
    "**Why:** AWQ uses calibration data to find salient weights. If the data doesn't match your use case, the wrong weights get protected.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "\n",
    "- ‚úÖ **AWQ protects important weights**: By analyzing activation magnitudes\n",
    "- ‚úÖ **Per-channel scaling**: Mathematically preserves output while protecting salient weights\n",
    "- ‚úÖ **AWQ vs GPTQ**: AWQ often has better quality, GPTQ has faster kernels\n",
    "- ‚úÖ **Use case matters**: Choose based on your specific task and constraints\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Build an Automated Quantization Selector**\n",
    "\n",
    "Create a function that:\n",
    "1. Takes a model and sample task data\n",
    "2. Quantizes with both AWQ and GPTQ\n",
    "3. Evaluates on the task\n",
    "4. Recommends the best method\n",
    "\n",
    "```python\n",
    "def select_best_quantization(\n",
    "    model_id: str,\n",
    "    task_data: List[str],\n",
    "    evaluation_fn: Callable,\n",
    "    metric: str = \"perplexity\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Automatically select the best quantization method.\n",
    "    \n",
    "    Returns: 'awq' or 'gptq'\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Further Reading\n",
    "\n",
    "- [AWQ Paper: Activation-aware Weight Quantization](https://arxiv.org/abs/2306.00978)\n",
    "- [AutoAWQ GitHub](https://github.com/casper-hansen/AutoAWQ)\n",
    "- [AWQ vs GPTQ Benchmark](https://huggingface.co/blog/awq-quantization)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up (optional - comment out to keep models)\n",
    "# import shutil\n",
    "# shutil.rmtree(save_dir_awq, ignore_errors=True)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Cleanup complete!\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, we'll explore **GGUF Conversion** for llama.cpp compatibility - run your models on CPUs and edge devices!\n",
    "\n",
    "‚û°Ô∏è Continue to: [04-gguf-conversion.ipynb](04-gguf-conversion.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}