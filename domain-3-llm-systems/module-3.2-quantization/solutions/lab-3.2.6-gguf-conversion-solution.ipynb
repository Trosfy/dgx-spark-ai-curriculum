{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2.6: GGUF Conversion - Solutions\n",
    "\n",
    "This notebook contains solutions for all exercises in Lab 3.2.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: GGUF Format Parser\n",
    "\n",
    "Implement a parser to read GGUF file headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GGUFParser:\n",
    "    \"\"\"\n",
    "    Parser for GGUF file format.\n",
    "    \n",
    "    GGUF is the standard format for llama.cpp models.\n",
    "    \"\"\"\n",
    "    \n",
    "    GGUF_MAGIC = 0x46554747  # 'GGUF' in little-endian\n",
    "    GGUF_VERSION = 3\n",
    "    \n",
    "    # GGML types\n",
    "    GGML_TYPES = {\n",
    "        0: 'F32',\n",
    "        1: 'F16',\n",
    "        2: 'Q4_0',\n",
    "        3: 'Q4_1',\n",
    "        6: 'Q5_0',\n",
    "        7: 'Q5_1',\n",
    "        8: 'Q8_0',\n",
    "        9: 'Q8_1',\n",
    "        10: 'Q2_K',\n",
    "        11: 'Q3_K',\n",
    "        12: 'Q4_K',\n",
    "        13: 'Q5_K',\n",
    "        14: 'Q6_K',\n",
    "        15: 'Q8_K',\n",
    "        16: 'IQ2_XXS',\n",
    "        17: 'IQ2_XS',\n",
    "        18: 'IQ3_XXS',\n",
    "        19: 'IQ1_S',\n",
    "        20: 'IQ4_NL',\n",
    "    }\n",
    "    \n",
    "    def __init__(self, filepath: str):\n",
    "        self.filepath = filepath\n",
    "        self.metadata = {}\n",
    "        self.tensors = {}\n",
    "    \n",
    "    def _read_string(self, f) -> str:\n",
    "        \"\"\"Read a GGUF string (length-prefixed).\"\"\"\n",
    "        length = struct.unpack('<Q', f.read(8))[0]\n",
    "        return f.read(length).decode('utf-8')\n",
    "    \n",
    "    def _read_value(self, f, value_type: int):\n",
    "        \"\"\"Read a metadata value based on type.\"\"\"\n",
    "        if value_type == 0:  # UINT8\n",
    "            return struct.unpack('<B', f.read(1))[0]\n",
    "        elif value_type == 1:  # INT8\n",
    "            return struct.unpack('<b', f.read(1))[0]\n",
    "        elif value_type == 2:  # UINT16\n",
    "            return struct.unpack('<H', f.read(2))[0]\n",
    "        elif value_type == 3:  # INT16\n",
    "            return struct.unpack('<h', f.read(2))[0]\n",
    "        elif value_type == 4:  # UINT32\n",
    "            return struct.unpack('<I', f.read(4))[0]\n",
    "        elif value_type == 5:  # INT32\n",
    "            return struct.unpack('<i', f.read(4))[0]\n",
    "        elif value_type == 6:  # FLOAT32\n",
    "            return struct.unpack('<f', f.read(4))[0]\n",
    "        elif value_type == 7:  # BOOL\n",
    "            return struct.unpack('<B', f.read(1))[0] != 0\n",
    "        elif value_type == 8:  # STRING\n",
    "            return self._read_string(f)\n",
    "        elif value_type == 9:  # ARRAY\n",
    "            array_type = struct.unpack('<I', f.read(4))[0]\n",
    "            array_len = struct.unpack('<Q', f.read(8))[0]\n",
    "            return [self._read_value(f, array_type) for _ in range(array_len)]\n",
    "        elif value_type == 10:  # UINT64\n",
    "            return struct.unpack('<Q', f.read(8))[0]\n",
    "        elif value_type == 11:  # INT64\n",
    "            return struct.unpack('<q', f.read(8))[0]\n",
    "        elif value_type == 12:  # FLOAT64\n",
    "            return struct.unpack('<d', f.read(8))[0]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown value type: {value_type}\")\n",
    "    \n",
    "    def parse_header(self) -> dict:\n",
    "        \"\"\"\n",
    "        Parse GGUF file header.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with file information\n",
    "        \"\"\"\n",
    "        with open(self.filepath, 'rb') as f:\n",
    "            # Read magic\n",
    "            magic = struct.unpack('<I', f.read(4))[0]\n",
    "            if magic != self.GGUF_MAGIC:\n",
    "                raise ValueError(f\"Invalid GGUF magic: {hex(magic)}\")\n",
    "            \n",
    "            # Read version\n",
    "            version = struct.unpack('<I', f.read(4))[0]\n",
    "            \n",
    "            # Read counts\n",
    "            tensor_count = struct.unpack('<Q', f.read(8))[0]\n",
    "            metadata_count = struct.unpack('<Q', f.read(8))[0]\n",
    "            \n",
    "            # Read metadata\n",
    "            for _ in range(metadata_count):\n",
    "                key = self._read_string(f)\n",
    "                value_type = struct.unpack('<I', f.read(4))[0]\n",
    "                value = self._read_value(f, value_type)\n",
    "                self.metadata[key] = value\n",
    "            \n",
    "            # Read tensor info\n",
    "            for _ in range(tensor_count):\n",
    "                name = self._read_string(f)\n",
    "                n_dims = struct.unpack('<I', f.read(4))[0]\n",
    "                dims = [struct.unpack('<Q', f.read(8))[0] for _ in range(n_dims)]\n",
    "                ggml_type = struct.unpack('<I', f.read(4))[0]\n",
    "                offset = struct.unpack('<Q', f.read(8))[0]\n",
    "                \n",
    "                self.tensors[name] = {\n",
    "                    'dims': dims,\n",
    "                    'type': self.GGML_TYPES.get(ggml_type, f'UNKNOWN({ggml_type})'),\n",
    "                    'offset': offset\n",
    "                }\n",
    "        \n",
    "        return {\n",
    "            'version': version,\n",
    "            'tensor_count': tensor_count,\n",
    "            'metadata_count': metadata_count,\n",
    "            'metadata': self.metadata,\n",
    "            'tensors': self.tensors\n",
    "        }\n",
    "\n",
    "\n",
    "# Demonstrate parser (would need actual GGUF file)\n",
    "print(\"GGUF Parser Implementation\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  parser = GGUFParser('model.gguf')\")\n",
    "print(\"  info = parser.parse_header()\")\n",
    "print(\"  print(info['metadata'])\")\n",
    "print(\"  print(info['tensors'])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Quantization Type Comparison\n",
    "\n",
    "Compare different GGUF quantization types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_gguf_quantization(values: np.ndarray, quant_type: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Simulate different GGUF quantization types.\n",
    "    \n",
    "    Args:\n",
    "        values: Values to quantize\n",
    "        quant_type: One of Q2_K, Q3_K, Q4_0, Q4_K, Q5_K, Q6_K, Q8_0\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (dequantized_values, effective_bits, mse)\n",
    "    \"\"\"\n",
    "    # Block sizes for different types\n",
    "    block_configs = {\n",
    "        'Q2_K': {'bits': 2, 'block_size': 256, 'super_block': True},\n",
    "        'Q3_K': {'bits': 3, 'block_size': 256, 'super_block': True},\n",
    "        'Q4_0': {'bits': 4, 'block_size': 32, 'super_block': False},\n",
    "        'Q4_K': {'bits': 4, 'block_size': 256, 'super_block': True},\n",
    "        'Q5_0': {'bits': 5, 'block_size': 32, 'super_block': False},\n",
    "        'Q5_K': {'bits': 5, 'block_size': 256, 'super_block': True},\n",
    "        'Q6_K': {'bits': 6, 'block_size': 256, 'super_block': True},\n",
    "        'Q8_0': {'bits': 8, 'block_size': 32, 'super_block': False},\n",
    "    }\n",
    "    \n",
    "    if quant_type not in block_configs:\n",
    "        raise ValueError(f\"Unknown quant type: {quant_type}\")\n",
    "    \n",
    "    config = block_configs[quant_type]\n",
    "    bits = config['bits']\n",
    "    block_size = config['block_size']\n",
    "    super_block = config['super_block']\n",
    "    \n",
    "    # Pad to block size\n",
    "    n = len(values)\n",
    "    n_padded = ((n + block_size - 1) // block_size) * block_size\n",
    "    padded = np.zeros(n_padded, dtype=np.float32)\n",
    "    padded[:n] = values\n",
    "    \n",
    "    # Quantize per block\n",
    "    qmax = 2 ** bits - 1\n",
    "    num_blocks = n_padded // block_size\n",
    "    \n",
    "    dequantized = np.zeros(n_padded, dtype=np.float32)\n",
    "    \n",
    "    for b in range(num_blocks):\n",
    "        start = b * block_size\n",
    "        end = start + block_size\n",
    "        block = padded[start:end]\n",
    "        \n",
    "        # Compute scale (use min/max for simplicity)\n",
    "        b_min, b_max = block.min(), block.max()\n",
    "        scale = (b_max - b_min) / qmax if b_max > b_min else 1.0\n",
    "        \n",
    "        # Quantize\n",
    "        q = np.clip(np.round((block - b_min) / (scale + 1e-10)), 0, qmax)\n",
    "        \n",
    "        # Dequantize\n",
    "        dequantized[start:end] = q * scale + b_min\n",
    "    \n",
    "    # Calculate effective bits (including scale overhead)\n",
    "    weight_bits = n * bits\n",
    "    scale_bits = num_blocks * 16  # FP16 scale\n",
    "    min_bits = num_blocks * 16 if not super_block else num_blocks * 8\n",
    "    total_bits = weight_bits + scale_bits + min_bits\n",
    "    effective_bits = total_bits / n\n",
    "    \n",
    "    # MSE\n",
    "    mse = np.mean((values - dequantized[:n]) ** 2)\n",
    "    \n",
    "    return dequantized[:n], effective_bits, mse\n",
    "\n",
    "\n",
    "# Compare all quantization types\n",
    "np.random.seed(42)\n",
    "weights = np.random.randn(4096).astype(np.float32) * 0.02\n",
    "\n",
    "quant_types = ['Q2_K', 'Q3_K', 'Q4_0', 'Q4_K', 'Q5_0', 'Q5_K', 'Q6_K', 'Q8_0']\n",
    "\n",
    "print(\"GGUF Quantization Type Comparison\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Type':<10} {'Eff. Bits':<12} {'Compression':<12} {'MSE':<15}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "results = {}\n",
    "for qt in quant_types:\n",
    "    dequant, eff_bits, mse = simulate_gguf_quantization(weights, qt)\n",
    "    compression = 32 / eff_bits\n",
    "    results[qt] = {'eff_bits': eff_bits, 'compression': compression, 'mse': mse}\n",
    "    print(f\"{qt:<10} {eff_bits:<12.2f} {compression:<12.1f}x {mse:<15.8f}\")\n",
    "\n",
    "print(\"\\nRecommendations:\")\n",
    "print(\"  Q4_K: Best balance for most use cases\")\n",
    "print(\"  Q5_K: Good quality, slightly larger\")\n",
    "print(\"  Q2_K: Maximum compression, use for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 Solution: Custom GGUF Writer\n",
    "\n",
    "Implement a simple GGUF file writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GGUFWriter:\n",
    "    \"\"\"\n",
    "    Simple GGUF file writer.\n",
    "    \"\"\"\n",
    "    \n",
    "    GGUF_MAGIC = 0x46554747\n",
    "    GGUF_VERSION = 3\n",
    "    \n",
    "    def __init__(self, filepath: str):\n",
    "        self.filepath = filepath\n",
    "        self.metadata = {}\n",
    "        self.tensors = []\n",
    "        self.tensor_data = []\n",
    "    \n",
    "    def add_metadata(self, key: str, value, value_type: str = 'auto'):\n",
    "        \"\"\"\n",
    "        Add metadata to GGUF file.\n",
    "        \n",
    "        Args:\n",
    "            key: Metadata key\n",
    "            value: Metadata value\n",
    "            value_type: Type hint ('string', 'int', 'float', 'auto')\n",
    "        \"\"\"\n",
    "        if value_type == 'auto':\n",
    "            if isinstance(value, str):\n",
    "                value_type = 'string'\n",
    "            elif isinstance(value, bool):\n",
    "                value_type = 'bool'\n",
    "            elif isinstance(value, int):\n",
    "                value_type = 'uint32'\n",
    "            elif isinstance(value, float):\n",
    "                value_type = 'float32'\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot auto-detect type for {type(value)}\")\n",
    "        \n",
    "        self.metadata[key] = (value, value_type)\n",
    "    \n",
    "    def add_tensor(self, name: str, data: np.ndarray, quant_type: str = 'F32'):\n",
    "        \"\"\"\n",
    "        Add tensor to GGUF file.\n",
    "        \n",
    "        Args:\n",
    "            name: Tensor name\n",
    "            data: Tensor data\n",
    "            quant_type: Quantization type\n",
    "        \"\"\"\n",
    "        # Convert to appropriate dtype\n",
    "        if quant_type == 'F32':\n",
    "            data = data.astype(np.float32)\n",
    "            ggml_type = 0\n",
    "        elif quant_type == 'F16':\n",
    "            data = data.astype(np.float16)\n",
    "            ggml_type = 1\n",
    "        else:\n",
    "            # For quantized types, would need actual quantization\n",
    "            data = data.astype(np.float32)\n",
    "            ggml_type = 0\n",
    "        \n",
    "        self.tensors.append({\n",
    "            'name': name,\n",
    "            'dims': list(data.shape),\n",
    "            'type': ggml_type\n",
    "        })\n",
    "        self.tensor_data.append(data)\n",
    "    \n",
    "    def _write_string(self, f, s: str):\n",
    "        \"\"\"Write GGUF string.\"\"\"\n",
    "        data = s.encode('utf-8')\n",
    "        f.write(struct.pack('<Q', len(data)))\n",
    "        f.write(data)\n",
    "    \n",
    "    def _write_metadata_value(self, f, value, value_type: str):\n",
    "        \"\"\"Write metadata value.\"\"\"\n",
    "        type_map = {\n",
    "            'uint8': (0, '<B'),\n",
    "            'int8': (1, '<b'),\n",
    "            'uint16': (2, '<H'),\n",
    "            'int16': (3, '<h'),\n",
    "            'uint32': (4, '<I'),\n",
    "            'int32': (5, '<i'),\n",
    "            'float32': (6, '<f'),\n",
    "            'bool': (7, '<B'),\n",
    "            'string': (8, None),\n",
    "            'uint64': (10, '<Q'),\n",
    "            'int64': (11, '<q'),\n",
    "            'float64': (12, '<d'),\n",
    "        }\n",
    "        \n",
    "        type_id, fmt = type_map[value_type]\n",
    "        f.write(struct.pack('<I', type_id))\n",
    "        \n",
    "        if value_type == 'string':\n",
    "            self._write_string(f, value)\n",
    "        elif value_type == 'bool':\n",
    "            f.write(struct.pack(fmt, 1 if value else 0))\n",
    "        else:\n",
    "            f.write(struct.pack(fmt, value))\n",
    "    \n",
    "    def write(self):\n",
    "        \"\"\"\n",
    "        Write GGUF file.\n",
    "        \"\"\"\n",
    "        with open(self.filepath, 'wb') as f:\n",
    "            # Write header\n",
    "            f.write(struct.pack('<I', self.GGUF_MAGIC))\n",
    "            f.write(struct.pack('<I', self.GGUF_VERSION))\n",
    "            f.write(struct.pack('<Q', len(self.tensors)))\n",
    "            f.write(struct.pack('<Q', len(self.metadata)))\n",
    "            \n",
    "            # Write metadata\n",
    "            for key, (value, value_type) in self.metadata.items():\n",
    "                self._write_string(f, key)\n",
    "                self._write_metadata_value(f, value, value_type)\n",
    "            \n",
    "            # Calculate tensor data offset\n",
    "            data_offset = f.tell()\n",
    "            \n",
    "            # Account for tensor info headers\n",
    "            for tensor in self.tensors:\n",
    "                data_offset += 8 + len(tensor['name'].encode('utf-8'))  # name\n",
    "                data_offset += 4  # n_dims\n",
    "                data_offset += 8 * len(tensor['dims'])  # dims\n",
    "                data_offset += 4  # type\n",
    "                data_offset += 8  # offset\n",
    "            \n",
    "            # Align to 32 bytes\n",
    "            data_offset = ((data_offset + 31) // 32) * 32\n",
    "            \n",
    "            # Write tensor info\n",
    "            current_offset = 0\n",
    "            for i, tensor in enumerate(self.tensors):\n",
    "                self._write_string(f, tensor['name'])\n",
    "                f.write(struct.pack('<I', len(tensor['dims'])))\n",
    "                for dim in tensor['dims']:\n",
    "                    f.write(struct.pack('<Q', dim))\n",
    "                f.write(struct.pack('<I', tensor['type']))\n",
    "                f.write(struct.pack('<Q', current_offset))\n",
    "                \n",
    "                # Update offset for next tensor\n",
    "                current_offset += self.tensor_data[i].nbytes\n",
    "            \n",
    "            # Pad to alignment\n",
    "            current_pos = f.tell()\n",
    "            padding = data_offset - current_pos\n",
    "            if padding > 0:\n",
    "                f.write(b'\\x00' * padding)\n",
    "            \n",
    "            # Write tensor data\n",
    "            for data in self.tensor_data:\n",
    "                f.write(data.tobytes())\n",
    "        \n",
    "        print(f\"Written GGUF file: {self.filepath}\")\n",
    "        print(f\"  Tensors: {len(self.tensors)}\")\n",
    "        print(f\"  Metadata: {len(self.metadata)}\")\n",
    "\n",
    "\n",
    "# Demonstrate writer\n",
    "print(\"GGUF Writer Implementation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create sample GGUF\n",
    "writer = GGUFWriter('../data/sample.gguf')\n",
    "\n",
    "# Add metadata\n",
    "writer.add_metadata('general.architecture', 'llama')\n",
    "writer.add_metadata('general.name', 'test-model')\n",
    "writer.add_metadata('llama.context_length', 4096)\n",
    "writer.add_metadata('llama.embedding_length', 4096)\n",
    "writer.add_metadata('llama.block_count', 32)\n",
    "\n",
    "# Add sample tensors\n",
    "np.random.seed(42)\n",
    "writer.add_tensor('token_embd.weight', np.random.randn(32000, 4096).astype(np.float32) * 0.02)\n",
    "\n",
    "# Write file\n",
    "writer.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key findings:\n",
    "\n",
    "1. **GGUF format** is well-structured with magic, version, metadata, and tensors\n",
    "2. **K-quant types** (Q4_K, Q5_K) provide better quality than simple quants\n",
    "3. **Q4_K** is the recommended choice for most applications\n",
    "4. **GGUF files** can be parsed and written with standard binary I/O"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}