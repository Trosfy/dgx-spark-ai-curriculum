{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2.8: TensorRT-LLM Engine - Solutions\n",
    "\n",
    "This notebook contains solutions for all exercises in Lab 3.2.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Build Custom Engine\n",
    "\n",
    "Create a custom TensorRT-LLM engine configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class CustomTRTLLMConfig:\n",
    "    \"\"\"\n",
    "    Custom configuration for TensorRT-LLM engine building.\n",
    "    \"\"\"\n",
    "    # Model\n",
    "    model_name: str\n",
    "    \n",
    "    # Quantization\n",
    "    quantization: str = \"fp8\"  # none, int8, fp8, int4_awq, fp4\n",
    "    \n",
    "    # Batch settings\n",
    "    max_batch_size: int = 8\n",
    "    max_input_len: int = 2048\n",
    "    max_output_len: int = 512\n",
    "    \n",
    "    # KV-cache\n",
    "    kv_cache_type: str = \"paged\"\n",
    "    max_num_tokens: int = 8192\n",
    "    \n",
    "    # Parallelism\n",
    "    tp_size: int = 1\n",
    "    pp_size: int = 1\n",
    "    \n",
    "    # Optimization level (0-5)\n",
    "    opt_level: int = 3\n",
    "    \n",
    "    def generate_convert_command(self, output_dir: str) -> str:\n",
    "        \"\"\"Generate checkpoint conversion command.\"\"\"\n",
    "        cmd = f\"\"\"python -m tensorrt_llm.commands.convert_checkpoint \\\\\n",
    "    --model_dir {self.model_name} \\\\\n",
    "    --output_dir {output_dir}/checkpoint \\\\\n",
    "    --dtype float16 \\\\\n",
    "    --tp_size {self.tp_size}\"\"\"\n",
    "        \n",
    "        # Add quantization flags\n",
    "        if self.quantization == \"fp8\":\n",
    "            cmd += \" \\\\\n    --use_fp8\"\n",
    "        elif self.quantization == \"int8\":\n",
    "            cmd += \" \\\\\n    --int8_kv_cache\"\n",
    "        elif self.quantization == \"int4_awq\":\n",
    "            cmd += \" \\\\\n    --use_weight_only --weight_only_precision int4_awq\"\n",
    "        elif self.quantization == \"fp4\":\n",
    "            cmd += \" \\\\\n    --use_fp4\"  # Blackwell native\n",
    "        \n",
    "        return cmd\n",
    "    \n",
    "    def generate_build_command(self, output_dir: str) -> str:\n",
    "        \"\"\"Generate engine build command.\"\"\"\n",
    "        max_seq_len = self.max_input_len + self.max_output_len\n",
    "        \n",
    "        cmd = f\"\"\"trtllm-build \\\\\n",
    "    --checkpoint_dir {output_dir}/checkpoint \\\\\n",
    "    --output_dir {output_dir}/engine \\\\\n",
    "    --max_batch_size {self.max_batch_size} \\\\\n",
    "    --max_input_len {self.max_input_len} \\\\\n",
    "    --max_seq_len {max_seq_len} \\\\\n",
    "    --gemm_plugin float16 \\\\\n",
    "    --gpt_attention_plugin float16 \\\\\n",
    "    --paged_kv_cache enable \\\\\n",
    "    --remove_input_padding enable \\\\\n",
    "    --builder_opt {self.opt_level}\"\"\"\n",
    "        \n",
    "        return cmd\n",
    "    \n",
    "    def generate_run_command(self, output_dir: str) -> str:\n",
    "        \"\"\"Generate inference run command.\"\"\"\n",
    "        cmd = f\"\"\"python -m tensorrt_llm.commands.run \\\\\n",
    "    --engine_dir {output_dir}/engine \\\\\n",
    "    --tokenizer_dir {self.model_name} \\\\\n",
    "    --max_output_len {self.max_output_len} \\\\\n",
    "    --input_text \"Hello, how are you?\"\"\"\"\n",
    "        \n",
    "        return cmd\n",
    "    \n",
    "    def print_full_workflow(self, output_dir: str):\n",
    "        \"\"\"Print complete build and run workflow.\"\"\"\n",
    "        print(f\"TensorRT-LLM Build Workflow for: {self.model_name}\")\n",
    "        print(f\"Quantization: {self.quantization}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\n# Step 1: Convert checkpoint\")\n",
    "        print(self.generate_convert_command(output_dir))\n",
    "        \n",
    "        print(\"\\n# Step 2: Build engine\")\n",
    "        print(self.generate_build_command(output_dir))\n",
    "        \n",
    "        print(\"\\n# Step 3: Run inference\")\n",
    "        print(self.generate_run_command(output_dir))\n",
    "\n",
    "\n",
    "# Example configurations\n",
    "print(\"Example 1: High-throughput chatbot (FP8)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "chatbot_config = CustomTRTLLMConfig(\n",
    "    model_name=\"Qwen/Qwen3-4B-Instruct\",\n",
    "    quantization=\"fp8\",\n",
    "    max_batch_size=16,\n",
    "    max_input_len=2048,\n",
    "    max_output_len=512,\n",
    "    opt_level=4\n",
    ")\n",
    "chatbot_config.print_full_workflow(\"./engines/chatbot\")\n",
    "\n",
    "print(\"\\n\\nExample 2: Code generation (FP4 for speed)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "code_config = CustomTRTLLMConfig(\n",
    "    model_name=\"codellama/CodeLlama-7b-Instruct-hf\",\n",
    "    quantization=\"fp4\",\n",
    "    max_batch_size=8,\n",
    "    max_input_len=4096,\n",
    "    max_output_len=1024,\n",
    "    opt_level=5\n",
    ")\n",
    "code_config.print_full_workflow(\"./engines/codegen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Batch Size Optimization\n",
    "\n",
    "Find the optimal batch size for a given latency requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_batch_performance(\n",
    "    batch_sizes: list,\n",
    "    base_latency_ms: float = 50,\n",
    "    tokens_per_request: int = 100,\n",
    "    max_memory_gb: float = 20\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Simulate performance at different batch sizes.\n",
    "    \n",
    "    Models typical TensorRT-LLM behavior:\n",
    "    - Latency increases sub-linearly with batch size\n",
    "    - Throughput increases near-linearly until memory bound\n",
    "    \n",
    "    Args:\n",
    "        batch_sizes: List of batch sizes to test\n",
    "        base_latency_ms: Single-request latency\n",
    "        tokens_per_request: Tokens generated per request\n",
    "        max_memory_gb: Maximum GPU memory\n",
    "        \n",
    "    Returns:\n",
    "        Performance results per batch size\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        # Latency model: sub-linear increase\n",
    "        # sqrt scaling is typical for well-optimized systems\n",
    "        latency_ms = base_latency_ms * (1 + 0.1 * np.sqrt(batch_size - 1))\n",
    "        \n",
    "        # Memory model: linear with batch\n",
    "        # Base memory + per-request KV-cache\n",
    "        base_memory = 3.0  # Model weights\n",
    "        kv_per_request = 0.5  # GB per request for KV-cache\n",
    "        memory_gb = base_memory + kv_per_request * batch_size\n",
    "        \n",
    "        # Check memory limit\n",
    "        if memory_gb > max_memory_gb:\n",
    "            # Memory bound - latency increases sharply\n",
    "            latency_ms *= 2\n",
    "            memory_gb = max_memory_gb\n",
    "        \n",
    "        # Throughput calculations\n",
    "        total_tokens = batch_size * tokens_per_request\n",
    "        tokens_per_sec = total_tokens / (latency_ms / 1000)\n",
    "        requests_per_sec = batch_size / (latency_ms / 1000)\n",
    "        \n",
    "        # Per-request latency\n",
    "        per_request_latency = latency_ms\n",
    "        \n",
    "        results[batch_size] = {\n",
    "            'batch_latency_ms': latency_ms,\n",
    "            'per_request_latency_ms': per_request_latency,\n",
    "            'tokens_per_second': tokens_per_sec,\n",
    "            'requests_per_second': requests_per_sec,\n",
    "            'memory_gb': memory_gb,\n",
    "            'gpu_utilization': min(100, batch_size * 15)  # Rough estimate\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def find_optimal_batch_size(\n",
    "    results: dict,\n",
    "    max_latency_ms: float = 100,\n",
    "    min_throughput: float = 0\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Find optimal batch size given constraints.\n",
    "    \n",
    "    Args:\n",
    "        results: Performance results from simulate_batch_performance\n",
    "        max_latency_ms: Maximum acceptable latency\n",
    "        min_throughput: Minimum required throughput\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (optimal_batch_size, reason)\n",
    "    \"\"\"\n",
    "    valid_configs = []\n",
    "    \n",
    "    for batch_size, metrics in results.items():\n",
    "        if metrics['per_request_latency_ms'] <= max_latency_ms:\n",
    "            if metrics['tokens_per_second'] >= min_throughput:\n",
    "                valid_configs.append((batch_size, metrics))\n",
    "    \n",
    "    if not valid_configs:\n",
    "        return None, \"No configuration meets requirements\"\n",
    "    \n",
    "    # Choose highest throughput within constraints\n",
    "    best = max(valid_configs, key=lambda x: x[1]['tokens_per_second'])\n",
    "    \n",
    "    return best[0], f\"Max throughput ({best[1]['tokens_per_second']:.0f} tok/s) within {max_latency_ms}ms latency\"\n",
    "\n",
    "\n",
    "# Run analysis\n",
    "print(\"Batch Size Optimization Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "batch_sizes = [1, 2, 4, 8, 16, 32, 64]\n",
    "results = simulate_batch_performance(batch_sizes, base_latency_ms=40)\n",
    "\n",
    "print(f\"{'Batch':<8} {'Latency':<12} {'Tok/s':<12} {'Req/s':<10} {'Memory':<10} {'GPU%':<8}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    r = results[bs]\n",
    "    print(f\"{bs:<8} {r['per_request_latency_ms']:<12.1f} {r['tokens_per_second']:<12.0f} \"\n",
    "          f\"{r['requests_per_second']:<10.1f} {r['memory_gb']:<10.1f} {r['gpu_utilization']:<8.0f}\")\n",
    "\n",
    "# Find optimal for different scenarios\n",
    "print(\"\\nOptimal Batch Sizes:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "scenarios = [\n",
    "    (\"Low latency (<50ms)\", 50, 0),\n",
    "    (\"Balanced (<100ms)\", 100, 0),\n",
    "    (\"High throughput (<200ms)\", 200, 0),\n",
    "    (\"Minimum 5000 tok/s\", float('inf'), 5000),\n",
    "]\n",
    "\n",
    "for name, max_lat, min_tput in scenarios:\n",
    "    optimal, reason = find_optimal_batch_size(results, max_lat, min_tput)\n",
    "    if optimal:\n",
    "        print(f\"  {name}: batch_size={optimal} - {reason}\")\n",
    "    else:\n",
    "        print(f\"  {name}: {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 Solution: Triton Deployment\n",
    "\n",
    "Create a complete Triton deployment configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triton_deployment(\n",
    "    model_name: str,\n",
    "    engine_dir: str,\n",
    "    output_dir: str,\n",
    "    max_batch_size: int = 8\n",
    "):\n",
    "    \"\"\"\n",
    "    Create complete Triton deployment files.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Model identifier\n",
    "        engine_dir: Path to TensorRT-LLM engine\n",
    "        output_dir: Output directory for deployment files\n",
    "        max_batch_size: Maximum batch size\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Model repository structure\n",
    "    model_repo = output_path / \"model_repository\" / model_name\n",
    "    model_repo.mkdir(parents=True, exist_ok=True)\n",
    "    (model_repo / \"1\").mkdir(exist_ok=True)\n",
    "    \n",
    "    # Config.pbtxt\n",
    "    config_content = f'''name: \"{model_name}\"\n",
    "backend: \"tensorrtllm\"\n",
    "max_batch_size: {max_batch_size}\n",
    "\n",
    "model_transaction_policy {{\n",
    "  decoupled: True\n",
    "}}\n",
    "\n",
    "input [\n",
    "  {{\n",
    "    name: \"text_input\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ -1 ]\n",
    "  }},\n",
    "  {{\n",
    "    name: \"max_tokens\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 1 ]\n",
    "  }},\n",
    "  {{\n",
    "    name: \"temperature\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]\n",
    "    optional: true\n",
    "  }},\n",
    "  {{\n",
    "    name: \"top_p\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]\n",
    "    optional: true\n",
    "  }},\n",
    "  {{\n",
    "    name: \"stream\"\n",
    "    data_type: TYPE_BOOL\n",
    "    dims: [ 1 ]\n",
    "    optional: true\n",
    "  }}\n",
    "]\n",
    "\n",
    "output [\n",
    "  {{\n",
    "    name: \"text_output\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ -1 ]\n",
    "  }}\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "  {{\n",
    "    count: 1\n",
    "    kind: KIND_GPU\n",
    "  }}\n",
    "]\n",
    "\n",
    "parameters: {{\n",
    "  key: \"gpt_model_type\"\n",
    "  value: {{\n",
    "    string_value: \"inflight_fused_batching\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "parameters: {{\n",
    "  key: \"engine_dir\"\n",
    "  value: {{\n",
    "    string_value: \"{engine_dir}\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "parameters: {{\n",
    "  key: \"kv_cache_type\"\n",
    "  value: {{\n",
    "    string_value: \"paged\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "dynamic_batching {{\n",
    "  preferred_batch_size: [1, 2, 4, {max_batch_size}]\n",
    "  max_queue_delay_microseconds: 100\n",
    "}}\n",
    "'''\n",
    "    \n",
    "    with open(model_repo / \"config.pbtxt\", \"w\") as f:\n",
    "        f.write(config_content)\n",
    "    \n",
    "    # Docker compose\n",
    "    compose_content = f'''version: '3.8'\n",
    "\n",
    "services:\n",
    "  triton:\n",
    "    image: nvcr.io/nvidia/tritonserver:24.11-trtllm-python-py3\n",
    "    container_name: triton-{model_name}\n",
    "    ports:\n",
    "      - \"8000:8000\"  # HTTP\n",
    "      - \"8001:8001\"  # gRPC\n",
    "      - \"8002:8002\"  # Metrics\n",
    "    volumes:\n",
    "      - ./model_repository:/models\n",
    "      - {engine_dir}:{engine_dir}\n",
    "    environment:\n",
    "      - CUDA_VISIBLE_DEVICES=0\n",
    "    command: tritonserver --model-repository=/models\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              count: 1\n",
    "              capabilities: [gpu]\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/v2/health/ready\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "'''\n",
    "    \n",
    "    with open(output_path / \"docker-compose.yml\", \"w\") as f:\n",
    "        f.write(compose_content)\n",
    "    \n",
    "    # Client script\n",
    "    client_content = '''#!/usr/bin/env python3\n",
    "\"\"\"Simple Triton client for TensorRT-LLM model.\"\"\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "def generate(\n",
    "    prompt: str,\n",
    "    max_tokens: int = 100,\n",
    "    temperature: float = 0.7,\n",
    "    server_url: str = \"http://localhost:8000\"\n",
    ") -> str:\n",
    "    \"\"\"Generate text from prompt.\"\"\"\n",
    "    \n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            {\"name\": \"text_input\", \"shape\": [1, 1], \"datatype\": \"BYTES\", \n",
    "             \"data\": [prompt]},\n",
    "            {\"name\": \"max_tokens\", \"shape\": [1, 1], \"datatype\": \"INT32\",\n",
    "             \"data\": [max_tokens]},\n",
    "            {\"name\": \"temperature\", \"shape\": [1, 1], \"datatype\": \"FP32\",\n",
    "             \"data\": [temperature]}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{server_url}/v2/models/llm/infer\",\n",
    "        json=payload\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        return result[\"outputs\"][0][\"data\"][0]\n",
    "    else:\n",
    "        raise Exception(f\"Error: {response.status_code} - {response.text}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--prompt\", default=\"Hello, how are you?\")\n",
    "    parser.add_argument(\"--max-tokens\", type=int, default=100)\n",
    "    parser.add_argument(\"--temperature\", type=float, default=0.7)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    result = generate(args.prompt, args.max_tokens, args.temperature)\n",
    "    print(result)\n",
    "'''\n",
    "    \n",
    "    with open(output_path / \"client.py\", \"w\") as f:\n",
    "        f.write(client_content)\n",
    "    \n",
    "    # README\n",
    "    readme_content = f'''# Triton Deployment for {model_name}\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. Start the server:\n",
    "```bash\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "2. Check health:\n",
    "```bash\n",
    "curl localhost:8000/v2/health/ready\n",
    "```\n",
    "\n",
    "3. Generate text:\n",
    "```bash\n",
    "python client.py --prompt \"Hello, how are you?\"\n",
    "```\n",
    "\n",
    "## Endpoints\n",
    "\n",
    "- HTTP: http://localhost:8000\n",
    "- gRPC: localhost:8001\n",
    "- Metrics: http://localhost:8002/metrics\n",
    "\n",
    "## Configuration\n",
    "\n",
    "- Engine: {engine_dir}\n",
    "- Max batch size: {max_batch_size}\n",
    "- Dynamic batching: enabled\n",
    "- KV-cache: paged\n",
    "'''\n",
    "    \n",
    "    with open(output_path / \"README.md\", \"w\") as f:\n",
    "        f.write(readme_content)\n",
    "    \n",
    "    print(f\"Created Triton deployment in: {output_path}\")\n",
    "    print(f\"\\nFiles created:\")\n",
    "    for f in output_path.rglob(\"*\"):\n",
    "        if f.is_file():\n",
    "            print(f\"  {f.relative_to(output_path)}\")\n",
    "\n",
    "\n",
    "# Create deployment\n",
    "print(\"Creating Triton Deployment\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "create_triton_deployment(\n",
    "    model_name=\"llama-3b-fp8\",\n",
    "    engine_dir=\"/engines/llama-3b-fp8\",\n",
    "    output_dir=\"../data/triton_deployment\",\n",
    "    max_batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key findings:\n",
    "\n",
    "1. **Custom configurations** allow tuning for specific use cases\n",
    "2. **Batch size optimization** balances latency vs throughput\n",
    "3. **Triton deployment** provides production-ready serving with:\n",
    "   - Dynamic batching\n",
    "   - Health checks\n",
    "   - Metrics monitoring\n",
    "   - gRPC and HTTP endpoints\n",
    "\n",
    "### Production Recommendations\n",
    "\n",
    "| Use Case | Quantization | Batch Size | Notes |\n",
    "|----------|-------------|------------|-------|\n",
    "| Chatbot (low latency) | FP8 | 8-16 | Prioritize response time |\n",
    "| Batch processing | FP4 | 32-64 | Maximum throughput |\n",
    "| Code generation | FP8 | 16-32 | Balance quality/speed |\n",
    "| Edge deployment | INT4-AWQ | 4-8 | Memory constrained |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
