{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2.4 Solutions: GGUF Conversion\n",
    "\n",
    "This notebook contains solutions to the exercises from Lab 3.2.4.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import torch\n",
    "import subprocess\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "LLAMA_CPP = os.path.expanduser(\"~/llama.cpp\")\n",
    "print(f\"llama.cpp path: {LLAMA_CPP}\")\n",
    "print(f\"Exists: {os.path.exists(LLAMA_CPP)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Convert a Different Model\n",
    "\n",
    "Convert Mistral-7B or Llama-2-7B to GGUF format with multiple quantization types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def convert_to_gguf(\n",
    "    model_id: str,\n",
    "    output_dir: str = \"./gguf_models\",\n",
    "    quant_types: list = [\"Q4_K_M\", \"Q5_K_M\", \"Q8_0\"]\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Convert a HuggingFace model to GGUF format with multiple quantization types.\n",
    "    \n",
    "    Args:\n",
    "        model_id: HuggingFace model ID (e.g., \"mistralai/Mistral-7B-v0.1\")\n",
    "        output_dir: Directory to save GGUF files\n",
    "        quant_types: List of GGUF quantization types\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping quantization types to file paths\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model_name = model_id.split('/')[-1]\n",
    "    \n",
    "    # Step 1: Download and save HuggingFace model\n",
    "    print(f\"Step 1: Downloading {model_id}...\")\n",
    "    hf_dir = os.path.join(output_dir, \"hf_model\")\n",
    "    os.makedirs(hf_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, \n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        tokenizer.save_pretrained(hf_dir)\n",
    "        model.save_pretrained(hf_dir)\n",
    "        del model\n",
    "        clear_memory()\n",
    "        print(f\"  Saved to {hf_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error downloading model: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Step 2: Convert to GGUF F16\n",
    "    print(\"Step 2: Converting to GGUF F16...\")\n",
    "    f16_path = os.path.join(output_dir, f\"{model_name}-f16.gguf\")\n",
    "    convert_script = os.path.join(LLAMA_CPP, \"convert_hf_to_gguf.py\")\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        [\"python3\", convert_script, hf_dir, \"--outfile\", f16_path, \"--outtype\", \"f16\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(f\"  Error: {result.stderr}\")\n",
    "        raise RuntimeError(\"F16 conversion failed\")\n",
    "    \n",
    "    f16_size = os.path.getsize(f16_path) / 1e9\n",
    "    print(f\"  Created: {f16_path} ({f16_size:.2f} GB)\")\n",
    "    \n",
    "    # Step 3: Quantize to different types\n",
    "    print(\"Step 3: Quantizing...\")\n",
    "    quantize_bin = os.path.join(LLAMA_CPP, \"build\", \"bin\", \"llama-quantize\")\n",
    "    \n",
    "    results = {'F16': {'path': f16_path, 'size_gb': f16_size}}\n",
    "    \n",
    "    for qtype in quant_types:\n",
    "        print(f\"  Creating {qtype}...\")\n",
    "        output_path = os.path.join(output_dir, f\"{model_name}-{qtype}.gguf\")\n",
    "        \n",
    "        result = subprocess.run(\n",
    "            [quantize_bin, f16_path, output_path, qtype],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0 and os.path.exists(output_path):\n",
    "            size_gb = os.path.getsize(output_path) / 1e9\n",
    "            results[qtype] = {'path': output_path, 'size_gb': size_gb}\n",
    "            print(f\"    Size: {size_gb:.2f} GB ({f16_size/size_gb:.1f}x compression)\")\n",
    "        else:\n",
    "            print(f\"    Failed: {result.stderr}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"GGUF Conversion Summary\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"{'Type':<12} {'Size (GB)':>12} {'Compression':>12}\")\n",
    "    print(\"-\"*50)\n",
    "    for qtype, data in results.items():\n",
    "        compression = f16_size / data['size_gb']\n",
    "        print(f\"{qtype:<12} {data['size_gb']:>12.2f} {compression:>11.1f}x\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# results = convert_to_gguf(\"mistralai/Mistral-7B-v0.1\")\n",
    "# results = convert_to_gguf(\"meta-llama/Llama-2-7b-hf\")\n",
    "print(\"GGUF conversion function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Perplexity Evaluation with llama.cpp\n",
    "\n",
    "Use llama.cpp's built-in perplexity tool to evaluate quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gguf_perplexity(\n",
    "    model_path: str, \n",
    "    test_file: str,\n",
    "    n_gpu_layers: int = 99\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluate GGUF model perplexity using llama.cpp.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to GGUF file\n",
    "        test_file: Path to text file for evaluation\n",
    "        n_gpu_layers: Number of layers to offload to GPU\n",
    "    \n",
    "    Returns:\n",
    "        Perplexity value\n",
    "    \"\"\"\n",
    "    perplexity_bin = os.path.join(LLAMA_CPP, \"build\", \"bin\", \"llama-perplexity\")\n",
    "    \n",
    "    if not os.path.exists(perplexity_bin):\n",
    "        print(f\"Error: {perplexity_bin} not found\")\n",
    "        print(\"Make sure llama.cpp is built with: cmake --build build\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Evaluating perplexity on {model_path}...\")\n",
    "    print(f\"Test file: {test_file}\")\n",
    "    \n",
    "    cmd = [\n",
    "        perplexity_bin,\n",
    "        \"-m\", model_path,\n",
    "        \"-f\", test_file,\n",
    "        \"-ngl\", str(n_gpu_layers)\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    # Parse perplexity from output\n",
    "    perplexity = None\n",
    "    for line in result.stdout.split('\\n'):\n",
    "        if 'perplexity' in line.lower():\n",
    "            print(line)\n",
    "            # Try to extract the number\n",
    "            try:\n",
    "                parts = line.split()\n",
    "                for i, part in enumerate(parts):\n",
    "                    if 'perplexity' in part.lower() and i + 1 < len(parts):\n",
    "                        perplexity = float(parts[i + 1].strip('=:'))\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def create_test_file(filepath: str = \"./test_data.txt\") -> str:\n",
    "    \"\"\"\n",
    "    Create a test file for perplexity evaluation.\n",
    "    \"\"\"\n",
    "    test_text = \"\"\"Machine learning is a subset of artificial intelligence that enables systems to learn from data.\n",
    "Deep learning uses neural networks with multiple layers to extract complex patterns.\n",
    "Natural language processing allows computers to understand and generate human language.\n",
    "Computer vision enables machines to interpret and analyze visual information from the world.\n",
    "Reinforcement learning trains agents through rewards and penalties in an environment.\n",
    "Transfer learning leverages knowledge from one task to improve performance on another.\n",
    "The transformer architecture revolutionized natural language processing in 2017.\n",
    "Attention mechanisms help models focus on relevant parts of the input.\n",
    "Large language models can generate human-like text and answer questions.\n",
    "Quantization reduces model precision to enable efficient deployment.\"\"\"\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write(test_text)\n",
    "    \n",
    "    print(f\"Created test file: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# test_file = create_test_file()\n",
    "# ppl = evaluate_gguf_perplexity(\"./model-Q4_K_M.gguf\", test_file)\n",
    "print(\"Perplexity evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare perplexity across quantization types\n",
    "\n",
    "def compare_gguf_quality(\n",
    "    model_dir: str,\n",
    "    model_name: str,\n",
    "    quant_types: list = [\"F16\", \"Q8_0\", \"Q5_K_M\", \"Q4_K_M\", \"Q4_0\", \"Q2_K\"]\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compare perplexity across different GGUF quantization types.\n",
    "    \n",
    "    Args:\n",
    "        model_dir: Directory containing GGUF files\n",
    "        model_name: Base model name (e.g., \"Mistral-7B-v0.1\")\n",
    "        quant_types: Quantization types to compare\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results\n",
    "    \"\"\"\n",
    "    # Create test file\n",
    "    test_file = create_test_file(\"./perplexity_test.txt\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for qtype in quant_types:\n",
    "        # Construct filename\n",
    "        if qtype == \"F16\":\n",
    "            filename = f\"{model_name}-f16.gguf\"\n",
    "        else:\n",
    "            filename = f\"{model_name}-{qtype}.gguf\"\n",
    "        \n",
    "        filepath = os.path.join(model_dir, filename)\n",
    "        \n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"\\nEvaluating {qtype}...\")\n",
    "            ppl = evaluate_gguf_perplexity(filepath, test_file)\n",
    "            size_gb = os.path.getsize(filepath) / 1e9\n",
    "            \n",
    "            results[qtype] = {\n",
    "                'perplexity': ppl,\n",
    "                'size_gb': size_gb\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Skipping {qtype}: {filepath} not found\")\n",
    "    \n",
    "    # Summary\n",
    "    if results:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"GGUF Quality Comparison\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"{'Type':<12} {'Size (GB)':>12} {'Perplexity':>12} {'PPL Delta':>12}\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        baseline_ppl = results.get('F16', {}).get('perplexity', 0)\n",
    "        \n",
    "        for qtype, data in results.items():\n",
    "            ppl = data['perplexity']\n",
    "            if ppl and baseline_ppl:\n",
    "                delta = ppl - baseline_ppl\n",
    "                delta_str = f\"+{delta:.2f}\" if delta > 0 else f\"{delta:.2f}\"\n",
    "            else:\n",
    "                delta_str = \"N/A\"\n",
    "            \n",
    "            ppl_str = f\"{ppl:.2f}\" if ppl else \"N/A\"\n",
    "            print(f\"{qtype:<12} {data['size_gb']:>12.2f} {ppl_str:>12} {delta_str:>12}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# results = compare_gguf_quality(\"./gguf_models\", \"Mistral-7B-v0.1\")\n",
    "print(\"Quality comparison function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **GGUF is portable** - Works on CPU, GPU, Metal, anywhere\n",
    "2. **K-quants are smart** - They protect important layers\n",
    "3. **Q4_K_M is the sweet spot** - Best balance of size and quality\n",
    "4. **Use llama-perplexity** - Built-in tool for quality evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}