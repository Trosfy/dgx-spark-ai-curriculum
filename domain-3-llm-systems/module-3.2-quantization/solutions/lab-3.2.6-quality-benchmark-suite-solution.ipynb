{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 11.6 Solutions: Quality Benchmark Suite\n",
    "\n",
    "This notebook contains solutions to the exercises from Task 11.6.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Benchmark a Larger Model\n",
    "\n",
    "Run the full benchmark suite on Llama-2-7B or Mistral-7B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    \"\"\"Store benchmark results for a single model.\"\"\"\n",
    "    model_name: str\n",
    "    quantization_type: str\n",
    "    model_size_mb: float\n",
    "    perplexity: Optional[float] = None\n",
    "    tokens_per_second: Optional[float] = None\n",
    "    memory_used_gb: Optional[float] = None\n",
    "    task_scores: Dict[str, float] = field(default_factory=dict)\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        result = {\n",
    "            'Model': self.model_name,\n",
    "            'Quantization': self.quantization_type,\n",
    "            'Size (MB)': self.model_size_mb,\n",
    "            'Perplexity': self.perplexity,\n",
    "            'Tokens/s': self.tokens_per_second,\n",
    "            'Memory (GB)': self.memory_used_gb,\n",
    "        }\n",
    "        result.update(self.task_scores)\n",
    "        return result\n",
    "\n",
    "\n",
    "def benchmark_larger_model(\n",
    "    model_id: str = \"meta-llama/Llama-2-7b-hf\",\n",
    "    eval_texts: List[str] = None\n",
    ") -> List[BenchmarkResult]:\n",
    "    \"\"\"\n",
    "    Run comprehensive benchmark on a larger model.\n",
    "    \n",
    "    Args:\n",
    "        model_id: HuggingFace model ID\n",
    "        eval_texts: List of texts for perplexity evaluation\n",
    "    \n",
    "    Returns:\n",
    "        List of BenchmarkResult for each precision\n",
    "    \"\"\"\n",
    "    if eval_texts is None:\n",
    "        eval_texts = [\n",
    "            \"The history of human civilization spans thousands of years.\",\n",
    "            \"Machine learning algorithms learn patterns from data.\",\n",
    "            \"The solar system contains eight planets orbiting the Sun.\",\n",
    "            \"Climate change affects ecosystems around the world.\",\n",
    "            \"Medical advances have improved human health outcomes.\",\n",
    "        ]\n",
    "    \n",
    "    results = []\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    def calc_perplexity(model):\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "        with torch.no_grad():\n",
    "            for text in tqdm(eval_texts, desc=\"Perplexity\", leave=False):\n",
    "                enc = tokenizer(text, return_tensors='pt', truncation=True, max_length=256)\n",
    "                input_ids = enc.input_ids.to(model.device)\n",
    "                if input_ids.size(1) < 2:\n",
    "                    continue\n",
    "                outputs = model(input_ids, labels=input_ids)\n",
    "                total_loss += outputs.loss.item() * (input_ids.size(1) - 1)\n",
    "                total_tokens += input_ids.size(1) - 1\n",
    "        return math.exp(total_loss / total_tokens)\n",
    "    \n",
    "    def calc_speed(model):\n",
    "        inputs = tokenizer(\"Hello\", return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(**inputs, max_new_tokens=10, do_sample=False)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(**inputs, max_new_tokens=50, pad_token_id=tokenizer.pad_token_id)\n",
    "        torch.cuda.synchronize()\n",
    "        return 50 / (time.perf_counter() - start)\n",
    "    \n",
    "    def get_size(model):\n",
    "        return sum(p.numel() * p.element_size() for p in model.parameters()) / 1e6\n",
    "    \n",
    "    # Configurations to test\n",
    "    configs = [\n",
    "        ('FP16', {'torch_dtype': torch.float16}),\n",
    "        ('INT8', {'quantization_config': BitsAndBytesConfig(load_in_8bit=True)}),\n",
    "        ('INT4/NF4', {'quantization_config': BitsAndBytesConfig(\n",
    "            load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type=\"nf4\"\n",
    "        )}),\n",
    "    ]\n",
    "    \n",
    "    for name, kwargs in configs:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Benchmarking {name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        clear_memory()\n",
    "        \n",
    "        try:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id, device_map=\"cuda\", **kwargs\n",
    "            )\n",
    "            \n",
    "            size = get_size(model)\n",
    "            memory = torch.cuda.memory_allocated() / 1e9\n",
    "            \n",
    "            print(\"Calculating perplexity...\")\n",
    "            ppl = calc_perplexity(model)\n",
    "            \n",
    "            print(\"Benchmarking speed...\")\n",
    "            speed = calc_speed(model)\n",
    "            \n",
    "            result = BenchmarkResult(\n",
    "                model_name=model_id.split('/')[-1],\n",
    "                quantization_type=name,\n",
    "                model_size_mb=size,\n",
    "                perplexity=ppl,\n",
    "                tokens_per_second=speed,\n",
    "                memory_used_gb=memory\n",
    "            )\n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"\\nResults:\")\n",
    "            print(f\"  Size: {size:.1f} MB\")\n",
    "            print(f\"  Memory: {memory:.2f} GB\")\n",
    "            print(f\"  Perplexity: {ppl:.2f}\")\n",
    "            print(f\"  Speed: {speed:.1f} tok/s\")\n",
    "            \n",
    "            del model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        \n",
    "        clear_memory()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"BENCHMARK SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    df = pd.DataFrame([r.to_dict() for r in results])\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# results = benchmark_larger_model(\"meta-llama/Llama-2-7b-hf\")\n",
    "# results = benchmark_larger_model(\"mistralai/Mistral-7B-v0.1\")\n",
    "print(\"Larger model benchmark function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Task-Specific Evaluation\n",
    "\n",
    "Use lm-eval to evaluate on specific tasks (HellaSwag, ARC, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_eval import evaluator\n",
    "\n",
    "def evaluate_on_tasks(\n",
    "    model_path: str,\n",
    "    tasks: List[str] = [\"hellaswag\", \"arc_easy\"],\n",
    "    batch_size: int = 8,\n",
    "    num_fewshot: int = 0\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate a model on specific benchmark tasks using lm-eval.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to model (HF model ID or local path)\n",
    "        tasks: List of task names to evaluate on\n",
    "        batch_size: Batch size for evaluation\n",
    "        num_fewshot: Number of few-shot examples\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping task names to accuracy scores\n",
    "    \"\"\"\n",
    "    print(f\"Evaluating {model_path} on tasks: {tasks}\")\n",
    "    print(\"This may take a while...\")\n",
    "    \n",
    "    results = evaluator.simple_evaluate(\n",
    "        model=\"hf\",\n",
    "        model_args=f\"pretrained={model_path}\",\n",
    "        tasks=tasks,\n",
    "        batch_size=batch_size,\n",
    "        num_fewshot=num_fewshot,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "    \n",
    "    # Extract scores\n",
    "    scores = {}\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Evaluation Results\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for task, task_results in results['results'].items():\n",
    "        # Try different accuracy key names\n",
    "        acc = None\n",
    "        for key in ['acc', 'acc_norm', 'acc,none', 'accuracy']:\n",
    "            if key in task_results:\n",
    "                acc = task_results[key]\n",
    "                break\n",
    "        \n",
    "        if acc is not None:\n",
    "            scores[task] = acc\n",
    "            print(f\"{task}: {acc:.3f} ({acc*100:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"{task}: Unable to extract accuracy\")\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "def compare_quantized_on_tasks(\n",
    "    model_paths: Dict[str, str],\n",
    "    tasks: List[str] = [\"hellaswag\"]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare multiple quantized models on benchmark tasks.\n",
    "    \n",
    "    Args:\n",
    "        model_paths: Dict mapping name to model path\n",
    "        tasks: Tasks to evaluate on\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with comparison results\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for name, path in model_paths.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Evaluating: {name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        scores = evaluate_on_tasks(path, tasks)\n",
    "        \n",
    "        result = {'Model': name}\n",
    "        result.update(scores)\n",
    "        all_results.append(result)\n",
    "        \n",
    "        clear_memory()\n",
    "    \n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Print comparison\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"COMPARISON SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Find best for each task\n",
    "    print(f\"\\nBest models:\")\n",
    "    for task in tasks:\n",
    "        if task in df.columns:\n",
    "            best_idx = df[task].idxmax()\n",
    "            best_model = df.loc[best_idx, 'Model']\n",
    "            best_score = df.loc[best_idx, task]\n",
    "            print(f\"  {task}: {best_model} ({best_score:.3f})\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# scores = evaluate_on_tasks(\"facebook/opt-350m\", [\"hellaswag\"])\n",
    "#\n",
    "# comparison = compare_quantized_on_tasks(\n",
    "#     {\n",
    "#         \"FP16\": \"facebook/opt-350m\",\n",
    "#         \"GPTQ\": \"./quantized_models/opt-350m-gptq-4bit-g128\",\n",
    "#         \"AWQ\": \"./quantized_models/opt-350m-awq-4bit-g128\"\n",
    "#     },\n",
    "#     tasks=[\"hellaswag\", \"arc_easy\"]\n",
    "# )\n",
    "print(\"Task evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Automated Model Selection Pipeline\n",
    "\n",
    "Create a function that automatically selects the best quantization based on constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_model(\n",
    "    model_id: str,\n",
    "    max_size_mb: float = 1000,\n",
    "    max_ppl_increase: float = 0.5,\n",
    "    min_speed_tok_s: float = 20,\n",
    "    eval_texts: List[str] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Automatically select the best quantization method based on constraints.\n",
    "    \n",
    "    Args:\n",
    "        model_id: HuggingFace model ID\n",
    "        max_size_mb: Maximum acceptable model size\n",
    "        max_ppl_increase: Maximum acceptable perplexity increase vs FP16\n",
    "        min_speed_tok_s: Minimum acceptable generation speed\n",
    "        eval_texts: Texts for perplexity evaluation\n",
    "    \n",
    "    Returns:\n",
    "        Dict with recommendation and all results\n",
    "    \"\"\"\n",
    "    if eval_texts is None:\n",
    "        eval_texts = [\n",
    "            \"Machine learning is transforming industries worldwide.\",\n",
    "            \"The neural network learns patterns from training data.\",\n",
    "            \"Deep learning enables breakthrough applications in AI.\",\n",
    "            \"Natural language processing understands human text.\",\n",
    "            \"Computer vision interprets images and video content.\",\n",
    "        ]\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    results = []\n",
    "    baseline_ppl = None\n",
    "    \n",
    "    def calc_ppl(model):\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "        with torch.no_grad():\n",
    "            for text in eval_texts:\n",
    "                enc = tokenizer(text, return_tensors='pt', truncation=True, max_length=256)\n",
    "                input_ids = enc.input_ids.to(model.device)\n",
    "                if input_ids.size(1) < 2:\n",
    "                    continue\n",
    "                outputs = model(input_ids, labels=input_ids)\n",
    "                total_loss += outputs.loss.item() * (input_ids.size(1) - 1)\n",
    "                total_tokens += input_ids.size(1) - 1\n",
    "        return math.exp(total_loss / total_tokens)\n",
    "    \n",
    "    def calc_speed(model):\n",
    "        inputs = tokenizer(\"Hello\", return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(**inputs, max_new_tokens=10, do_sample=False)\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(**inputs, max_new_tokens=50, pad_token_id=tokenizer.pad_token_id)\n",
    "        torch.cuda.synchronize()\n",
    "        return 50 / (time.perf_counter() - start)\n",
    "    \n",
    "    # Configurations\n",
    "    configs = [\n",
    "        ('FP16', {}),\n",
    "        ('INT8', {'quantization_config': BitsAndBytesConfig(load_in_8bit=True)}),\n",
    "        ('INT4', {'quantization_config': BitsAndBytesConfig(\n",
    "            load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )}),\n",
    "    ]\n",
    "    \n",
    "    for name, kwargs in configs:\n",
    "        print(f\"\\nTesting {name}...\")\n",
    "        clear_memory()\n",
    "        \n",
    "        try:\n",
    "            if name == 'FP16':\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_id, torch_dtype=torch.float16, device_map=\"cuda\"\n",
    "                )\n",
    "            else:\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_id, device_map=\"cuda\", **kwargs\n",
    "                )\n",
    "            \n",
    "            size = sum(p.numel() * p.element_size() for p in model.parameters()) / 1e6\n",
    "            ppl = calc_ppl(model)\n",
    "            speed = calc_speed(model)\n",
    "            \n",
    "            if baseline_ppl is None:\n",
    "                baseline_ppl = ppl\n",
    "            \n",
    "            result = {\n",
    "                'name': name,\n",
    "                'size_mb': size,\n",
    "                'perplexity': ppl,\n",
    "                'ppl_delta': ppl - baseline_ppl,\n",
    "                'speed': speed,\n",
    "                'meets_constraints': (\n",
    "                    size <= max_size_mb and\n",
    "                    (ppl - baseline_ppl) <= max_ppl_increase and\n",
    "                    speed >= min_speed_tok_s\n",
    "                )\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"  Size: {size:.1f} MB\")\n",
    "            print(f\"  PPL: {ppl:.2f} (delta: +{ppl - baseline_ppl:.2f})\")\n",
    "            print(f\"  Speed: {speed:.1f} tok/s\")\n",
    "            print(f\"  Meets constraints: {result['meets_constraints']}\")\n",
    "            \n",
    "            del model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "    \n",
    "    clear_memory()\n",
    "    \n",
    "    # Find best option\n",
    "    valid = [r for r in results if r['meets_constraints']]\n",
    "    \n",
    "    if not valid:\n",
    "        print(\"\\n‚ö†Ô∏è  No model meets all constraints!\")\n",
    "        # Return the most compressed as fallback\n",
    "        recommendation = min(results, key=lambda x: x['size_mb'])['name']\n",
    "    else:\n",
    "        # Prefer smallest that meets constraints\n",
    "        valid.sort(key=lambda x: x['size_mb'])\n",
    "        recommendation = valid[0]['name']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üèÜ RECOMMENDATION: {recommendation}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return {\n",
    "        'recommendation': recommendation,\n",
    "        'results': results,\n",
    "        'constraints': {\n",
    "            'max_size_mb': max_size_mb,\n",
    "            'max_ppl_increase': max_ppl_increase,\n",
    "            'min_speed_tok_s': min_speed_tok_s\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# result = select_best_model(\n",
    "#     \"facebook/opt-350m\",\n",
    "#     max_size_mb=500,\n",
    "#     max_ppl_increase=1.0,\n",
    "#     min_speed_tok_s=15\n",
    "# )\n",
    "print(\"Model selection pipeline defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended version with GPTQ and AWQ support\n",
    "\n",
    "def select_best_model_extended(\n",
    "    model_id: str,\n",
    "    gptq_path: str = None,\n",
    "    awq_path: str = None,\n",
    "    max_size_mb: float = 1000,\n",
    "    max_ppl_increase: float = 0.5,\n",
    "    min_speed_tok_s: float = 20,\n",
    "    eval_texts: List[str] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Extended model selection including GPTQ and AWQ models.\n",
    "    \n",
    "    Args:\n",
    "        model_id: Base model HuggingFace ID\n",
    "        gptq_path: Path to GPTQ quantized model\n",
    "        awq_path: Path to AWQ quantized model\n",
    "        max_size_mb: Maximum model size constraint\n",
    "        max_ppl_increase: Maximum perplexity increase\n",
    "        min_speed_tok_s: Minimum speed constraint\n",
    "        eval_texts: Texts for perplexity evaluation\n",
    "    \n",
    "    Returns:\n",
    "        Dict with recommendation and all results\n",
    "    \"\"\"\n",
    "    if eval_texts is None:\n",
    "        eval_texts = [\n",
    "            \"Machine learning is transforming industries.\",\n",
    "            \"Neural networks learn from training data.\",\n",
    "            \"Deep learning enables new applications.\",\n",
    "        ] * 5\n",
    "    \n",
    "    # First run basic benchmarks\n",
    "    basic_result = select_best_model(\n",
    "        model_id, max_size_mb, max_ppl_increase, min_speed_tok_s, eval_texts\n",
    "    )\n",
    "    \n",
    "    results = basic_result['results'].copy()\n",
    "    baseline_ppl = results[0]['perplexity']  # FP16 is first\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    def calc_ppl(model):\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "        with torch.no_grad():\n",
    "            for text in eval_texts:\n",
    "                enc = tokenizer(text, return_tensors='pt', truncation=True, max_length=256)\n",
    "                input_ids = enc.input_ids.to(model.device)\n",
    "                if input_ids.size(1) < 2:\n",
    "                    continue\n",
    "                outputs = model(input_ids, labels=input_ids)\n",
    "                total_loss += outputs.loss.item() * (input_ids.size(1) - 1)\n",
    "                total_tokens += input_ids.size(1) - 1\n",
    "        return math.exp(total_loss / total_tokens)\n",
    "    \n",
    "    def calc_speed(model):\n",
    "        inputs = tokenizer(\"Hello\", return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(**inputs, max_new_tokens=10, do_sample=False)\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(**inputs, max_new_tokens=50, pad_token_id=tokenizer.pad_token_id)\n",
    "        torch.cuda.synchronize()\n",
    "        return 50 / (time.perf_counter() - start)\n",
    "    \n",
    "    # Test GPTQ if available\n",
    "    if gptq_path and os.path.exists(gptq_path):\n",
    "        print(f\"\\nTesting GPTQ...\")\n",
    "        try:\n",
    "            from auto_gptq import AutoGPTQForCausalLM\n",
    "            clear_memory()\n",
    "            \n",
    "            model = AutoGPTQForCausalLM.from_quantized(\n",
    "                gptq_path, device=\"cuda:0\", use_safetensors=True\n",
    "            )\n",
    "            \n",
    "            size = sum(\n",
    "                os.path.getsize(os.path.join(gptq_path, f))\n",
    "                for f in os.listdir(gptq_path)\n",
    "                if f.endswith('.safetensors') or f.endswith('.bin')\n",
    "            ) / 1e6\n",
    "            ppl = calc_ppl(model)\n",
    "            speed = calc_speed(model)\n",
    "            \n",
    "            results.append({\n",
    "                'name': 'GPTQ',\n",
    "                'size_mb': size,\n",
    "                'perplexity': ppl,\n",
    "                'ppl_delta': ppl - baseline_ppl,\n",
    "                'speed': speed,\n",
    "                'meets_constraints': (\n",
    "                    size <= max_size_mb and\n",
    "                    (ppl - baseline_ppl) <= max_ppl_increase and\n",
    "                    speed >= min_speed_tok_s\n",
    "                )\n",
    "            })\n",
    "            \n",
    "            print(f\"  Size: {size:.1f} MB\")\n",
    "            print(f\"  PPL: {ppl:.2f}\")\n",
    "            print(f\"  Speed: {speed:.1f} tok/s\")\n",
    "            \n",
    "            del model\n",
    "            clear_memory()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  GPTQ error: {e}\")\n",
    "    \n",
    "    # Test AWQ if available\n",
    "    if awq_path and os.path.exists(awq_path):\n",
    "        print(f\"\\nTesting AWQ...\")\n",
    "        try:\n",
    "            from awq import AutoAWQForCausalLM\n",
    "            clear_memory()\n",
    "            \n",
    "            model = AutoAWQForCausalLM.from_quantized(awq_path, fuse_layers=True)\n",
    "            \n",
    "            size = sum(\n",
    "                os.path.getsize(os.path.join(awq_path, f))\n",
    "                for f in os.listdir(awq_path)\n",
    "                if f.endswith('.safetensors') or f.endswith('.bin')\n",
    "            ) / 1e6\n",
    "            ppl = calc_ppl(model)\n",
    "            speed = calc_speed(model)\n",
    "            \n",
    "            results.append({\n",
    "                'name': 'AWQ',\n",
    "                'size_mb': size,\n",
    "                'perplexity': ppl,\n",
    "                'ppl_delta': ppl - baseline_ppl,\n",
    "                'speed': speed,\n",
    "                'meets_constraints': (\n",
    "                    size <= max_size_mb and\n",
    "                    (ppl - baseline_ppl) <= max_ppl_increase and\n",
    "                    speed >= min_speed_tok_s\n",
    "                )\n",
    "            })\n",
    "            \n",
    "            print(f\"  Size: {size:.1f} MB\")\n",
    "            print(f\"  PPL: {ppl:.2f}\")\n",
    "            print(f\"  Speed: {speed:.1f} tok/s\")\n",
    "            \n",
    "            del model\n",
    "            clear_memory()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  AWQ error: {e}\")\n",
    "    \n",
    "    # Find best\n",
    "    valid = [r for r in results if r['meets_constraints']]\n",
    "    \n",
    "    if not valid:\n",
    "        recommendation = min(results, key=lambda x: x['size_mb'])['name']\n",
    "    else:\n",
    "        valid.sort(key=lambda x: x['size_mb'])\n",
    "        recommendation = valid[0]['name']\n",
    "    \n",
    "    # Summary table\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"COMPLETE BENCHMARK RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'Method':<10} {'Size (MB)':>12} {'PPL':>8} {'PPL Œî':>8} {'Speed':>10} {'Valid':>8}\")\n",
    "    print(\"-\"*60)\n",
    "    for r in results:\n",
    "        valid_str = \"‚úì\" if r['meets_constraints'] else \"‚úó\"\n",
    "        print(f\"{r['name']:<10} {r['size_mb']:>12.1f} {r['perplexity']:>8.2f} {r['ppl_delta']:>+8.2f} {r['speed']:>10.1f} {valid_str:>8}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ RECOMMENDATION: {recommendation}\")\n",
    "    \n",
    "    return {\n",
    "        'recommendation': recommendation,\n",
    "        'results': results,\n",
    "        'constraints': {\n",
    "            'max_size_mb': max_size_mb,\n",
    "            'max_ppl_increase': max_ppl_increase,\n",
    "            'min_speed_tok_s': min_speed_tok_s\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# result = select_best_model_extended(\n",
    "#     \"facebook/opt-350m\",\n",
    "#     gptq_path=\"./quantized_models/opt-350m-gptq-4bit-g128\",\n",
    "#     awq_path=\"./quantized_models/opt-350m-awq-4bit-g128\",\n",
    "#     max_size_mb=500\n",
    "# )\n",
    "print(\"Extended model selection pipeline defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Comprehensive benchmarking** - Test size, speed, quality, and task accuracy\n",
    "2. **lm-eval is the standard** - Use it for reproducible benchmarks\n",
    "3. **Constraints-based selection** - Define your priorities upfront\n",
    "4. **Data-driven decisions** - Let the numbers guide your deployment choice"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
