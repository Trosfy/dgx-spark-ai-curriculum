{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2.7: Quality Benchmark Suite - Solutions\n",
    "\n",
    "This notebook contains solutions for all exercises in Lab 3.2.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Custom Perplexity Dataset\n",
    "\n",
    "Create a perplexity evaluation using domain-specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainPerplexityEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluate perplexity on domain-specific text.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, domain_name: str):\n",
    "        self.domain_name = domain_name\n",
    "        self.texts = []\n",
    "    \n",
    "    def add_text(self, text: str):\n",
    "        \"\"\"Add a domain-specific text sample.\"\"\"\n",
    "        if len(text.strip()) > 50:  # Minimum length\n",
    "            self.texts.append(text.strip())\n",
    "    \n",
    "    def add_texts_from_file(self, filepath: str):\n",
    "        \"\"\"Load texts from a file (one per line or paragraph).\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                for line in f:\n",
    "                    self.add_text(line)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {filepath}\")\n",
    "    \n",
    "    def calculate_perplexity_simulated(self, model_quality: float = 1.0) -> dict:\n",
    "        \"\"\"\n",
    "        Simulate perplexity calculation.\n",
    "        \n",
    "        In real implementation, this would use actual model inference.\n",
    "        \n",
    "        Args:\n",
    "            model_quality: Quality factor (1.0 = baseline, lower = worse)\n",
    "            \n",
    "        Returns:\n",
    "            Perplexity statistics\n",
    "        \"\"\"\n",
    "        if not self.texts:\n",
    "            return {'mean': float('inf'), 'std': 0, 'samples': 0}\n",
    "        \n",
    "        # Simulate perplexity based on text complexity and model quality\n",
    "        perplexities = []\n",
    "        \n",
    "        for text in self.texts:\n",
    "            # Base perplexity from text complexity\n",
    "            word_count = len(text.split())\n",
    "            unique_words = len(set(text.lower().split()))\n",
    "            complexity = unique_words / (word_count + 1)\n",
    "            \n",
    "            # Simulate perplexity (lower quality = higher perplexity)\n",
    "            base_ppl = 5 + 10 * complexity\n",
    "            ppl = base_ppl / model_quality + np.random.normal(0, 0.5)\n",
    "            perplexities.append(max(1.0, ppl))\n",
    "        \n",
    "        return {\n",
    "            'mean': np.mean(perplexities),\n",
    "            'std': np.std(perplexities),\n",
    "            'min': np.min(perplexities),\n",
    "            'max': np.max(perplexities),\n",
    "            'samples': len(perplexities)\n",
    "        }\n",
    "\n",
    "\n",
    "# Create domain-specific evaluator\n",
    "print(\"Domain-Specific Perplexity Evaluation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Example: Technical documentation domain\n",
    "tech_evaluator = DomainPerplexityEvaluator(\"technical_docs\")\n",
    "\n",
    "# Add sample technical texts\n",
    "tech_texts = [\n",
    "    \"The CUDA kernel is launched with a grid of thread blocks, where each block contains multiple threads that execute in parallel.\",\n",
    "    \"Tensor parallelism distributes model layers across multiple GPUs, enabling training of models that exceed single GPU memory.\",\n",
    "    \"Quantization reduces model precision from FP32 to INT8 or INT4, trading accuracy for inference speed and memory efficiency.\",\n",
    "    \"The attention mechanism computes weighted sums of value vectors, with weights determined by query-key similarity scores.\",\n",
    "    \"Gradient checkpointing trades compute for memory by recomputing activations during backpropagation instead of storing them.\",\n",
    "    \"Flash attention uses tiling and recomputation to achieve O(N) memory complexity while maintaining exact attention computation.\",\n",
    "    \"The KV-cache stores computed key and value tensors for autoregressive generation, avoiding redundant computation.\",\n",
    "    \"Mixed precision training uses FP16 for forward and backward passes while maintaining FP32 master weights for stability.\",\n",
    "    \"LoRA fine-tuning adds low-rank adaptation matrices to frozen pretrained weights, reducing trainable parameter count.\",\n",
    "    \"The tokenizer converts text to token IDs using a learned vocabulary, typically based on BPE or SentencePiece.\",\n",
    "]\n",
    "\n",
    "for text in tech_texts:\n",
    "    tech_evaluator.add_text(text)\n",
    "\n",
    "print(f\"Domain: {tech_evaluator.domain_name}\")\n",
    "print(f\"Samples: {len(tech_evaluator.texts)}\")\n",
    "\n",
    "# Evaluate with different model qualities (simulating quantization)\n",
    "quality_levels = {\n",
    "    'FP16 (baseline)': 1.0,\n",
    "    'INT8': 0.98,\n",
    "    'INT4 (NF4)': 0.92,\n",
    "    'GPTQ-4bit': 0.95,\n",
    "    'AWQ-4bit': 0.96,\n",
    "    'FP8': 0.99,\n",
    "    'FP4': 0.94,\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Method':<20} {'Perplexity':<12} {'vs Baseline':<12}\")\n",
    "print(\"-\"*44)\n",
    "\n",
    "baseline_ppl = None\n",
    "for method, quality in quality_levels.items():\n",
    "    result = tech_evaluator.calculate_perplexity_simulated(quality)\n",
    "    ppl = result['mean']\n",
    "    \n",
    "    if baseline_ppl is None:\n",
    "        baseline_ppl = ppl\n",
    "    \n",
    "    ppl_increase = (ppl - baseline_ppl) / baseline_ppl * 100\n",
    "    print(f\"{method:<20} {ppl:<12.2f} +{ppl_increase:<10.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Custom MMLU Subject\n",
    "\n",
    "Create a custom knowledge evaluation for a specific subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomKnowledgeEvaluator:\n",
    "    \"\"\"\n",
    "    Custom knowledge evaluation with multiple-choice questions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, subject_name: str):\n",
    "        self.subject_name = subject_name\n",
    "        self.questions = []\n",
    "    \n",
    "    def add_question(self, question: str, choices: list, correct_idx: int):\n",
    "        \"\"\"\n",
    "        Add a multiple-choice question.\n",
    "        \n",
    "        Args:\n",
    "            question: The question text\n",
    "            choices: List of 4 answer choices\n",
    "            correct_idx: Index of correct answer (0-3)\n",
    "        \"\"\"\n",
    "        if len(choices) != 4:\n",
    "            raise ValueError(\"Must have exactly 4 choices\")\n",
    "        if not 0 <= correct_idx <= 3:\n",
    "            raise ValueError(\"correct_idx must be 0-3\")\n",
    "        \n",
    "        self.questions.append({\n",
    "            'question': question,\n",
    "            'choices': choices,\n",
    "            'answer': correct_idx\n",
    "        })\n",
    "    \n",
    "    def format_question(self, q: dict) -> str:\n",
    "        \"\"\"Format question for model input.\"\"\"\n",
    "        formatted = f\"Question: {q['question']}\\n\\n\"\n",
    "        for i, choice in enumerate(q['choices']):\n",
    "            formatted += f\"{chr(65+i)}. {choice}\\n\"\n",
    "        formatted += \"\\nAnswer:\"\n",
    "        return formatted\n",
    "    \n",
    "    def evaluate_simulated(self, model_accuracy: float = 0.7) -> dict:\n",
    "        \"\"\"\n",
    "        Simulate model evaluation.\n",
    "        \n",
    "        Args:\n",
    "            model_accuracy: Expected accuracy (0-1)\n",
    "            \n",
    "        Returns:\n",
    "            Evaluation results\n",
    "        \"\"\"\n",
    "        if not self.questions:\n",
    "            return {'accuracy': 0, 'correct': 0, 'total': 0}\n",
    "        \n",
    "        correct = 0\n",
    "        results = []\n",
    "        \n",
    "        for q in self.questions:\n",
    "            # Simulate model answer based on accuracy\n",
    "            is_correct = np.random.random() < model_accuracy\n",
    "            if is_correct:\n",
    "                predicted = q['answer']\n",
    "                correct += 1\n",
    "            else:\n",
    "                # Pick wrong answer\n",
    "                wrong_choices = [i for i in range(4) if i != q['answer']]\n",
    "                predicted = np.random.choice(wrong_choices)\n",
    "            \n",
    "            results.append({\n",
    "                'question': q['question'][:50] + '...',\n",
    "                'predicted': chr(65 + predicted),\n",
    "                'correct': chr(65 + q['answer']),\n",
    "                'is_correct': is_correct\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'accuracy': correct / len(self.questions),\n",
    "            'correct': correct,\n",
    "            'total': len(self.questions),\n",
    "            'results': results\n",
    "        }\n",
    "\n",
    "\n",
    "# Create custom ML knowledge evaluator\n",
    "print(\"Custom Knowledge Evaluation: Machine Learning\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "ml_eval = CustomKnowledgeEvaluator(\"machine_learning\")\n",
    "\n",
    "# Add ML-focused questions\n",
    "ml_eval.add_question(\n",
    "    \"What does the softmax function output?\",\n",
    "    [\"Unbounded real numbers\", \"Probability distribution\", \"Binary values\", \"Negative numbers only\"],\n",
    "    1\n",
    ")\n",
    "\n",
    "ml_eval.add_question(\n",
    "    \"Which optimizer uses momentum and adaptive learning rates?\",\n",
    "    [\"SGD\", \"Adam\", \"Gradient Descent\", \"L-BFGS\"],\n",
    "    1\n",
    ")\n",
    "\n",
    "ml_eval.add_question(\n",
    "    \"What is the purpose of dropout in neural networks?\",\n",
    "    [\"Speed up training\", \"Reduce memory usage\", \"Prevent overfitting\", \"Increase model size\"],\n",
    "    2\n",
    ")\n",
    "\n",
    "ml_eval.add_question(\n",
    "    \"What does GPTQ stand for in model quantization?\",\n",
    "    [\"General Purpose Tensor Quantization\", \"GPT Quantization\", \"Gradient-based Post-Training Quantization\", \"GPU Tensor Query\"],\n",
    "    2\n",
    ")\n",
    "\n",
    "ml_eval.add_question(\n",
    "    \"Which data type uses 4 bits for exponent and 3 bits for mantissa?\",\n",
    "    [\"FP8 E5M2\", \"FP8 E4M3\", \"INT8\", \"BF16\"],\n",
    "    1\n",
    ")\n",
    "\n",
    "print(f\"Subject: {ml_eval.subject_name}\")\n",
    "print(f\"Questions: {len(ml_eval.questions)}\")\n",
    "\n",
    "# Evaluate with different quantization levels\n",
    "quant_accuracies = {\n",
    "    'FP16': 0.75,\n",
    "    'INT8': 0.73,\n",
    "    'INT4': 0.68,\n",
    "    'GPTQ': 0.71,\n",
    "    'AWQ': 0.72,\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Method':<12} {'Accuracy':<12} {'Correct':<10}\")\n",
    "print(\"-\"*34)\n",
    "\n",
    "for method, acc in quant_accuracies.items():\n",
    "    result = ml_eval.evaluate_simulated(acc)\n",
    "    print(f\"{method:<12} {result['accuracy']*100:<12.1f}% {result['correct']}/{result['total']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 Solution: Statistical Significance Testing\n",
    "\n",
    "Determine if differences between quantization methods are statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_significance_test(\n",
    "    baseline_samples: np.ndarray,\n",
    "    quantized_samples: np.ndarray,\n",
    "    alpha: float = 0.05\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Test statistical significance between baseline and quantized perplexity.\n",
    "    \n",
    "    Uses Welch's t-test (doesn't assume equal variances).\n",
    "    \n",
    "    Args:\n",
    "        baseline_samples: Perplexity samples from baseline model\n",
    "        quantized_samples: Perplexity samples from quantized model\n",
    "        alpha: Significance level\n",
    "        \n",
    "    Returns:\n",
    "        Test results\n",
    "    \"\"\"\n",
    "    # Descriptive statistics\n",
    "    baseline_mean = np.mean(baseline_samples)\n",
    "    baseline_std = np.std(baseline_samples)\n",
    "    quant_mean = np.mean(quantized_samples)\n",
    "    quant_std = np.std(quantized_samples)\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt((baseline_std**2 + quant_std**2) / 2)\n",
    "    cohens_d = (quant_mean - baseline_mean) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    # Welch's t-test\n",
    "    t_stat, p_value = stats.ttest_ind(baseline_samples, quantized_samples, equal_var=False)\n",
    "    \n",
    "    # Confidence interval for difference\n",
    "    diff = quant_mean - baseline_mean\n",
    "    se_diff = np.sqrt(baseline_std**2/len(baseline_samples) + quant_std**2/len(quantized_samples))\n",
    "    ci_low = diff - 1.96 * se_diff\n",
    "    ci_high = diff + 1.96 * se_diff\n",
    "    \n",
    "    # Interpretation\n",
    "    is_significant = p_value < alpha\n",
    "    \n",
    "    if abs(cohens_d) < 0.2:\n",
    "        effect_interpretation = \"negligible\"\n",
    "    elif abs(cohens_d) < 0.5:\n",
    "        effect_interpretation = \"small\"\n",
    "    elif abs(cohens_d) < 0.8:\n",
    "        effect_interpretation = \"medium\"\n",
    "    else:\n",
    "        effect_interpretation = \"large\"\n",
    "    \n",
    "    return {\n",
    "        'baseline_mean': baseline_mean,\n",
    "        'baseline_std': baseline_std,\n",
    "        'quantized_mean': quant_mean,\n",
    "        'quantized_std': quant_std,\n",
    "        'difference': diff,\n",
    "        'difference_percent': (diff / baseline_mean) * 100,\n",
    "        'confidence_interval': (ci_low, ci_high),\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'cohens_d': cohens_d,\n",
    "        'is_significant': is_significant,\n",
    "        'effect_size': effect_interpretation\n",
    "    }\n",
    "\n",
    "\n",
    "# Simulate perplexity measurements\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Statistical Significance Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Baseline (FP16) perplexity samples\n",
    "n_samples = 100\n",
    "baseline_ppl = np.random.normal(10.0, 0.5, n_samples)\n",
    "\n",
    "# Different quantization methods\n",
    "quant_methods = {\n",
    "    'INT8': np.random.normal(10.2, 0.6, n_samples),\n",
    "    'INT4 (NF4)': np.random.normal(10.8, 0.8, n_samples),\n",
    "    'GPTQ-4bit': np.random.normal(10.5, 0.6, n_samples),\n",
    "    'AWQ-4bit': np.random.normal(10.4, 0.5, n_samples),\n",
    "    'FP8': np.random.normal(10.1, 0.5, n_samples),\n",
    "}\n",
    "\n",
    "print(f\"Baseline (FP16): mean={np.mean(baseline_ppl):.2f}, std={np.std(baseline_ppl):.2f}\")\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "\n",
    "for method, samples in quant_methods.items():\n",
    "    result = statistical_significance_test(baseline_ppl, samples)\n",
    "    \n",
    "    sig_marker = \"*\" if result['is_significant'] else \"\"\n",
    "    \n",
    "    print(f\"\\n{method}:\")\n",
    "    print(f\"  Mean: {result['quantized_mean']:.2f} (Δ = +{result['difference']:.2f}, +{result['difference_percent']:.1f}%)\")\n",
    "    print(f\"  95% CI: [{result['confidence_interval'][0]:.2f}, {result['confidence_interval'][1]:.2f}]\")\n",
    "    print(f\"  p-value: {result['p_value']:.4f} {sig_marker}\")\n",
    "    print(f\"  Cohen's d: {result['cohens_d']:.3f} ({result['effect_size']})\")\n",
    "    print(f\"  Significant: {'Yes' if result['is_significant'] else 'No'} (α=0.05)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"* = statistically significant at α=0.05\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - INT8 and FP8 show minimal quality degradation\")\n",
    "print(\"  - INT4 shows larger but potentially acceptable degradation\")\n",
    "print(\"  - AWQ and GPTQ help reduce INT4 quality loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key findings:\n",
    "\n",
    "1. **Domain-specific evaluation** reveals different sensitivities to quantization\n",
    "2. **Custom knowledge tests** can target specific model capabilities\n",
    "3. **Statistical testing** helps distinguish meaningful differences from noise\n",
    "4. **Effect size** (Cohen's d) provides practical interpretation of differences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
