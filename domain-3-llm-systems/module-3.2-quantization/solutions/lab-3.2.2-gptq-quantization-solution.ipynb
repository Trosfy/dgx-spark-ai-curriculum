{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2.2 Solutions: GPTQ Quantization\n",
    "\n",
    "This notebook contains solutions to the exercises from Lab 3.2.2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Quantize a Larger Model\n",
    "\n",
    "Quantize OPT-1.3B or Llama-2-7B with different group sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def quantize_model_gptq(\n",
    "    model_id: str,\n",
    "    group_size: int = 128,\n",
    "    bits: int = 4,\n",
    "    calibration_samples: int = 128\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Quantize any model with GPTQ.\n",
    "    \n",
    "    Works with: OPT, Llama, Mistral, etc.\n",
    "    \n",
    "    Args:\n",
    "        model_id: HuggingFace model ID\n",
    "        group_size: GPTQ group size (32, 64, 128)\n",
    "        bits: Quantization bits (typically 4)\n",
    "        calibration_samples: Number of calibration samples\n",
    "    \n",
    "    Returns:\n",
    "        Path to saved quantized model\n",
    "    \"\"\"\n",
    "    print(f\"Quantizing {model_id} with GPTQ (bits={bits}, group_size={group_size})...\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Create calibration data\n",
    "    calibration_texts = [\n",
    "        \"Machine learning is transforming technology and industry.\",\n",
    "        \"The neural network learns patterns from training data.\",\n",
    "        \"Deep learning requires large datasets and computational power.\",\n",
    "        \"Artificial intelligence systems can perform complex tasks.\",\n",
    "        \"Natural language processing enables computers to understand text.\",\n",
    "        \"Computer vision allows machines to interpret visual information.\",\n",
    "        \"Reinforcement learning agents learn through trial and error.\",\n",
    "        \"Transfer learning leverages pre-trained model knowledge.\",\n",
    "    ] * ((calibration_samples // 8) + 1)\n",
    "    \n",
    "    calibration_data = [\n",
    "        tokenizer.encode(text, truncation=True, max_length=512)\n",
    "        for text in calibration_texts[:calibration_samples]\n",
    "    ]\n",
    "    \n",
    "    # GPTQ config\n",
    "    config = BaseQuantizeConfig(\n",
    "        bits=bits,\n",
    "        group_size=group_size,\n",
    "        desc_act=True,\n",
    "        sym=False,\n",
    "        damp_percent=0.1\n",
    "    )\n",
    "    \n",
    "    # Load and quantize\n",
    "    clear_memory()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = AutoGPTQForCausalLM.from_pretrained(model_id, config)\n",
    "    model.quantize(calibration_data, batch_size=4)\n",
    "    \n",
    "    quant_time = time.time() - start_time\n",
    "    print(f\"Quantization completed in {quant_time:.1f}s\")\n",
    "    \n",
    "    # Save\n",
    "    model_name = model_id.split('/')[-1]\n",
    "    save_path = f\"./quantized_{model_name}_gptq_{bits}bit_g{group_size}\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    model.save_quantized(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    \n",
    "    # Report size\n",
    "    total_size = sum(\n",
    "        os.path.getsize(os.path.join(save_path, f))\n",
    "        for f in os.listdir(save_path)\n",
    "        if f.endswith('.safetensors') or f.endswith('.bin')\n",
    "    ) / 1e6\n",
    "    \n",
    "    print(f\"Saved to {save_path}\")\n",
    "    print(f\"Model size: {total_size:.1f} MB\")\n",
    "    \n",
    "    del model\n",
    "    clear_memory()\n",
    "    \n",
    "    return save_path\n",
    "\n",
    "# Example usage:\n",
    "# save_path = quantize_model_gptq(\"facebook/opt-1.3b\", group_size=128)\n",
    "# save_path = quantize_model_gptq(\"meta-llama/Llama-2-7b-hf\", group_size=128)\n",
    "print(\"GPTQ quantization function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Custom Calibration Data\n",
    "\n",
    "Create domain-specific calibration data for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain-specific calibration data examples\n",
    "\n",
    "# For a coding assistant\n",
    "CODE_CALIBRATION = [\n",
    "    \"\"\"def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci(n-1) + fibonacci(n-2)\"\"\",\n",
    "    \n",
    "    \"\"\"class DataProcessor:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def process(self):\n",
    "        return [x * 2 for x in self.data]\"\"\",\n",
    "    \n",
    "    \"\"\"import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_preprocess(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    return df.dropna().reset_index(drop=True)\"\"\",\n",
    "    \n",
    "    \"\"\"async def fetch_data(url):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url) as response:\n",
    "            return await response.json()\"\"\",\n",
    "    \n",
    "    \"\"\"@dataclass\n",
    "class Config:\n",
    "    learning_rate: float = 0.001\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 100\"\"\",\n",
    "]\n",
    "\n",
    "# For a scientific assistant\n",
    "SCIENCE_CALIBRATION = [\n",
    "    \"The mitochondria is the powerhouse of the cell, responsible for ATP production through oxidative phosphorylation.\",\n",
    "    \"Quantum entanglement occurs when particles become correlated such that the quantum state of each particle cannot be described independently.\",\n",
    "    \"Neural networks approximate functions through compositions of linear transformations and nonlinear activations.\",\n",
    "    \"The Standard Model describes fundamental particles: quarks, leptons, and gauge bosons mediating the electromagnetic, weak, and strong forces.\",\n",
    "    \"CRISPR-Cas9 enables precise genome editing by using guide RNA to direct the Cas9 nuclease to specific DNA sequences.\",\n",
    "]\n",
    "\n",
    "# For a chatbot\n",
    "CHAT_CALIBRATION = [\n",
    "    \"User: How are you today?\\nAssistant: I'm doing well, thank you for asking! How can I help you today?\",\n",
    "    \"User: What's the weather like?\\nAssistant: I don't have access to real-time weather data, but I can help you find local weather services or forecast websites.\",\n",
    "    \"User: Tell me a joke.\\nAssistant: Why did the programmer quit? Because they didn't get arrays! Would you like to hear another one?\",\n",
    "    \"User: Can you help me with my homework?\\nAssistant: Of course! I'd be happy to help. What subject are you working on?\",\n",
    "    \"User: Thank you for your help!\\nAssistant: You're welcome! Feel free to ask if you have any other questions.\",\n",
    "]\n",
    "\n",
    "\n",
    "def create_domain_calibration(domain: str, num_samples: int = 128) -> list:\n",
    "    \"\"\"\n",
    "    Create domain-specific calibration data.\n",
    "    \n",
    "    Args:\n",
    "        domain: One of 'code', 'science', 'chat'\n",
    "        num_samples: Number of samples to generate\n",
    "    \n",
    "    Returns:\n",
    "        List of calibration texts\n",
    "    \"\"\"\n",
    "    domain_data = {\n",
    "        'code': CODE_CALIBRATION,\n",
    "        'science': SCIENCE_CALIBRATION,\n",
    "        'chat': CHAT_CALIBRATION,\n",
    "    }\n",
    "    \n",
    "    if domain not in domain_data:\n",
    "        raise ValueError(f\"Unknown domain: {domain}. Choose from: {list(domain_data.keys())}\")\n",
    "    \n",
    "    base = domain_data[domain]\n",
    "    # Extend to desired number\n",
    "    extended = (base * ((num_samples // len(base)) + 1))[:num_samples]\n",
    "    return extended\n",
    "\n",
    "\n",
    "# Test\n",
    "print(\"Domain calibration examples:\")\n",
    "for domain in ['code', 'science', 'chat']:\n",
    "    samples = create_domain_calibration(domain, 10)\n",
    "    print(f\"  {domain}: {len(samples)} samples\")\n",
    "    print(f\"    Sample: {samples[0][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete example: Quantize with domain-specific calibration\n",
    "\n",
    "def quantize_for_domain(\n",
    "    model_id: str,\n",
    "    domain: str,\n",
    "    group_size: int = 128\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Quantize a model with domain-specific calibration data.\n",
    "    \n",
    "    Args:\n",
    "        model_id: HuggingFace model ID\n",
    "        domain: 'code', 'science', or 'chat'\n",
    "        group_size: GPTQ group size\n",
    "    \n",
    "    Returns:\n",
    "        Path to saved model\n",
    "    \"\"\"\n",
    "    from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "    from transformers import AutoTokenizer\n",
    "    \n",
    "    print(f\"Quantizing {model_id} for {domain} domain...\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Get domain-specific calibration data\n",
    "    calibration_texts = create_domain_calibration(domain, num_samples=128)\n",
    "    calibration_data = [\n",
    "        tokenizer.encode(text, truncation=True, max_length=512)\n",
    "        for text in calibration_texts\n",
    "    ]\n",
    "    \n",
    "    # GPTQ config\n",
    "    config = BaseQuantizeConfig(\n",
    "        bits=4,\n",
    "        group_size=group_size,\n",
    "        desc_act=True\n",
    "    )\n",
    "    \n",
    "    # Quantize\n",
    "    model = AutoGPTQForCausalLM.from_pretrained(model_id, config)\n",
    "    model.quantize(calibration_data, batch_size=4)\n",
    "    \n",
    "    # Save\n",
    "    model_name = model_id.split('/')[-1]\n",
    "    save_path = f\"./quantized_{model_name}_gptq_{domain}\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    model.save_quantized(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    \n",
    "    print(f\"Saved to {save_path}\")\n",
    "    \n",
    "    del model\n",
    "    clear_memory()\n",
    "    \n",
    "    return save_path\n",
    "\n",
    "# Example usage:\n",
    "# code_model = quantize_for_domain(\"facebook/opt-350m\", \"code\")\n",
    "# chat_model = quantize_for_domain(\"facebook/opt-350m\", \"chat\")\n",
    "print(\"Domain-specific quantization function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **GPTQ works on any causal LM** - OPT, Llama, Mistral, etc.\n",
    "2. **Calibration data matters** - Use domain-specific data for best results\n",
    "3. **Group size tradeoff** - Smaller = better quality, larger = faster\n",
    "4. **desc_act=True** - Almost always improves quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}