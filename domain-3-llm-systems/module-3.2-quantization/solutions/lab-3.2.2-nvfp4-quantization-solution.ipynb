{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2.2: NVFP4 Quantization - Solutions\n",
    "\n",
    "This notebook contains solutions for all exercises in Lab 3.2.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from scripts import quantize_to_fp4, dequantize_from_fp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Micro-Block Size Comparison\n",
    "\n",
    "Experiment with different micro-block sizes and analyze their effect on quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_microblock_sizes(weights: np.ndarray, block_sizes: list) -> dict:\n",
    "    \"\"\"\n",
    "    Compare NVFP4 quantization with different micro-block sizes.\n",
    "    \n",
    "    Args:\n",
    "        weights: Weight tensor to quantize\n",
    "        block_sizes: List of block sizes to try\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with results for each block size\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for block_size in block_sizes:\n",
    "        # Quantize\n",
    "        quantized, scales, _ = quantize_to_fp4(weights, block_size=block_size)\n",
    "        \n",
    "        # Dequantize\n",
    "        reconstructed = dequantize_from_fp4(quantized, scales, block_size=block_size)\n",
    "        \n",
    "        # Calculate error metrics\n",
    "        mse = np.mean((weights - reconstructed) ** 2)\n",
    "        max_error = np.max(np.abs(weights - reconstructed))\n",
    "        snr = 10 * np.log10(np.mean(weights ** 2) / (mse + 1e-10))\n",
    "        \n",
    "        # Calculate overhead (scales storage)\n",
    "        num_blocks = np.ceil(weights.size / block_size)\n",
    "        scale_bits = num_blocks * 8  # 8-bit scales\n",
    "        weight_bits = weights.size * 4  # 4-bit weights\n",
    "        total_bits = scale_bits + weight_bits\n",
    "        effective_bits = total_bits / weights.size\n",
    "        \n",
    "        results[block_size] = {\n",
    "            'mse': mse,\n",
    "            'max_error': max_error,\n",
    "            'snr_db': snr,\n",
    "            'effective_bits': effective_bits,\n",
    "            'compression': 32 / effective_bits\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nBlock Size {block_size}:\")\n",
    "        print(f\"  MSE: {mse:.8f}\")\n",
    "        print(f\"  SNR: {snr:.2f} dB\")\n",
    "        print(f\"  Effective bits: {effective_bits:.2f}\")\n",
    "        print(f\"  Compression: {results[block_size]['compression']:.1f}x\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test with realistic weight distribution\n",
    "np.random.seed(42)\n",
    "weights = np.random.randn(4096, 4096).astype(np.float32) * 0.02\n",
    "\n",
    "print(\"NVFP4 Micro-Block Size Comparison\")\n",
    "print(\"=\"*50)\n",
    "block_sizes = [16, 32, 64, 128, 256]\n",
    "results = compare_microblock_sizes(weights.flatten(), block_sizes)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# SNR vs Block Size\n",
    "ax1 = axes[0]\n",
    "snrs = [results[bs]['snr_db'] for bs in block_sizes]\n",
    "ax1.plot(block_sizes, snrs, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Micro-Block Size')\n",
    "ax1.set_ylabel('SNR (dB)')\n",
    "ax1.set_title('Quality vs Block Size')\n",
    "ax1.set_xscale('log', base=2)\n",
    "ax1.grid(True)\n",
    "\n",
    "# Compression vs Block Size\n",
    "ax2 = axes[1]\n",
    "compressions = [results[bs]['compression'] for bs in block_sizes]\n",
    "ax2.plot(block_sizes, compressions, 'ro-', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Micro-Block Size')\n",
    "ax2.set_ylabel('Compression Ratio')\n",
    "ax2.set_title('Compression vs Block Size')\n",
    "ax2.set_xscale('log', base=2)\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Recommendation: Block size 32 or 64 offers best quality/compression trade-off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Layer-wise Analysis\n",
    "\n",
    "Analyze which layers benefit most from FP4 quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_layer_sensitivity(model_weights: dict, block_size: int = 32) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze FP4 quantization sensitivity per layer.\n",
    "    \n",
    "    Args:\n",
    "        model_weights: Dictionary of layer_name -> weights\n",
    "        block_size: Micro-block size for FP4\n",
    "        \n",
    "    Returns:\n",
    "        Sensitivity analysis per layer\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for name, weights in model_weights.items():\n",
    "        flat_weights = weights.flatten()\n",
    "        \n",
    "        # Quantize to FP4\n",
    "        quantized, scales, _ = quantize_to_fp4(flat_weights, block_size=block_size)\n",
    "        reconstructed = dequantize_from_fp4(quantized, scales, block_size=block_size)\n",
    "        \n",
    "        # Error metrics\n",
    "        mse = np.mean((flat_weights - reconstructed) ** 2)\n",
    "        relative_error = np.mean(np.abs(flat_weights - reconstructed) / (np.abs(flat_weights) + 1e-10))\n",
    "        \n",
    "        # Weight statistics\n",
    "        weight_range = flat_weights.max() - flat_weights.min()\n",
    "        weight_std = np.std(flat_weights)\n",
    "        outlier_ratio = np.mean(np.abs(flat_weights) > 3 * weight_std)\n",
    "        \n",
    "        results[name] = {\n",
    "            'mse': mse,\n",
    "            'relative_error': relative_error,\n",
    "            'weight_range': weight_range,\n",
    "            'weight_std': weight_std,\n",
    "            'outlier_ratio': outlier_ratio,\n",
    "            'sensitivity': relative_error * 100  # Sensitivity score\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Simulate layers with different characteristics\n",
    "np.random.seed(42)\n",
    "simulated_layers = {\n",
    "    'embed_tokens': np.random.randn(32000, 4096).astype(np.float32) * 0.01,\n",
    "    'layer_0.attn.q_proj': np.random.randn(4096, 4096).astype(np.float32) * 0.02,\n",
    "    'layer_0.attn.k_proj': np.random.randn(1024, 4096).astype(np.float32) * 0.02,\n",
    "    'layer_0.attn.v_proj': np.random.randn(1024, 4096).astype(np.float32) * 0.02,\n",
    "    'layer_0.attn.o_proj': np.random.randn(4096, 4096).astype(np.float32) * 0.02,\n",
    "    'layer_0.mlp.gate_proj': np.random.randn(14336, 4096).astype(np.float32) * 0.015,\n",
    "    'layer_0.mlp.up_proj': np.random.randn(14336, 4096).astype(np.float32) * 0.015,\n",
    "    'layer_0.mlp.down_proj': np.random.randn(4096, 14336).astype(np.float32) * 0.015,\n",
    "    'lm_head': np.random.randn(32000, 4096).astype(np.float32) * 0.01,\n",
    "}\n",
    "\n",
    "print(\"Layer-wise FP4 Quantization Sensitivity Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "layer_results = analyze_layer_sensitivity(simulated_layers)\n",
    "\n",
    "# Sort by sensitivity\n",
    "sorted_layers = sorted(layer_results.items(), key=lambda x: x[1]['sensitivity'], reverse=True)\n",
    "\n",
    "print(f\"\\n{'Layer':<30} {'Sensitivity':<12} {'Outliers':<12}\")\n",
    "print(\"-\"*54)\n",
    "for name, metrics in sorted_layers:\n",
    "    print(f\"{name:<30} {metrics['sensitivity']:>8.2f}%    {metrics['outlier_ratio']*100:>8.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Recommendation: Keep embedding and lm_head in higher precision if quality drops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 Solution: Custom FP4 Format\n",
    "\n",
    "Implement a custom FP4 format with adjustable exponent/mantissa split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_fp4_quantize(values: np.ndarray, e_bits: int = 2, m_bits: int = 1, bias: int = 1) -> tuple:\n",
    "    \"\"\"\n",
    "    Custom FP4 quantization with configurable format.\n",
    "    \n",
    "    4 bits total: 1 sign + e_bits exponent + m_bits mantissa\n",
    "    \n",
    "    Args:\n",
    "        values: Values to quantize\n",
    "        e_bits: Number of exponent bits (1-2)\n",
    "        m_bits: Number of mantissa bits (1-2)\n",
    "        bias: Exponent bias\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (quantized codes, format info)\n",
    "    \"\"\"\n",
    "    assert e_bits + m_bits == 3, \"Must have 3 bits for exponent + mantissa\"\n",
    "    \n",
    "    # Build representable values\n",
    "    representable = [0.0]  # Zero\n",
    "    \n",
    "    for sign in [1, -1]:\n",
    "        for exp in range(2**e_bits):\n",
    "            for mant in range(2**m_bits):\n",
    "                # Value = sign * 2^(exp-bias) * (1 + mant/2^m_bits)\n",
    "                if exp == 0:  # Subnormal\n",
    "                    val = sign * (2**(1-bias)) * (mant / 2**m_bits)\n",
    "                else:  # Normal\n",
    "                    val = sign * (2**(exp-bias)) * (1 + mant / 2**m_bits)\n",
    "                if val != 0:\n",
    "                    representable.append(val)\n",
    "    \n",
    "    representable = sorted(set(representable))\n",
    "    \n",
    "    # Quantize by nearest value\n",
    "    quantized = np.zeros_like(values, dtype=np.int8)\n",
    "    dequantized = np.zeros_like(values)\n",
    "    \n",
    "    for i, v in enumerate(values.flat):\n",
    "        # Find nearest representable value\n",
    "        idx = np.argmin(np.abs(np.array(representable) - v))\n",
    "        quantized.flat[i] = idx\n",
    "        dequantized.flat[i] = representable[idx]\n",
    "    \n",
    "    format_info = {\n",
    "        'e_bits': e_bits,\n",
    "        'm_bits': m_bits,\n",
    "        'bias': bias,\n",
    "        'representable': representable,\n",
    "        'num_values': len(representable),\n",
    "        'max_value': max(representable),\n",
    "        'min_positive': min(v for v in representable if v > 0)\n",
    "    }\n",
    "    \n",
    "    return quantized, dequantized, format_info\n",
    "\n",
    "\n",
    "# Compare E2M1 vs E1M2 formats\n",
    "np.random.seed(42)\n",
    "test_values = np.random.randn(1000).astype(np.float32) * 0.5\n",
    "\n",
    "print(\"Custom FP4 Format Comparison\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "formats = [\n",
    "    (2, 1, 1, \"E2M1 (NVFP4-like)\"),\n",
    "    (1, 2, 0, \"E1M2 (higher precision)\"),\n",
    "]\n",
    "\n",
    "for e_bits, m_bits, bias, name in formats:\n",
    "    _, dequant, info = custom_fp4_quantize(test_values, e_bits, m_bits, bias)\n",
    "    mse = np.mean((test_values - dequant) ** 2)\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Exponent bits: {e_bits}\")\n",
    "    print(f\"  Mantissa bits: {m_bits}\")\n",
    "    print(f\"  Unique values: {info['num_values']}\")\n",
    "    print(f\"  Value range: [{-info['max_value']:.4f}, {info['max_value']:.4f}]\")\n",
    "    print(f\"  Min positive: {info['min_positive']:.6f}\")\n",
    "    print(f\"  MSE: {mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key findings:\n",
    "\n",
    "1. **Micro-block size 32-64** offers the best quality/compression trade-off for NVFP4\n",
    "2. **Embedding and lm_head layers** are most sensitive to quantization\n",
    "3. **E2M1 format** (NVFP4) provides better dynamic range for weights\n",
    "4. **Blackwell's native FP4 support** eliminates software overhead"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
