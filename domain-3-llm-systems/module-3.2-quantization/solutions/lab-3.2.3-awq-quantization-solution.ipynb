{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2.3 Solutions: AWQ Quantization\n",
    "\n",
    "This notebook contains solutions to the exercises from Lab 3.2.3.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Different Group Sizes\n",
    "\n",
    "Compare AWQ quantization with different group sizes (32, 64, 128)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def compare_awq_group_sizes(\n",
    "    model_id: str, \n",
    "    group_sizes: list = [32, 64, 128]\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compare AWQ quantization with different group sizes.\n",
    "    \n",
    "    Args:\n",
    "        model_id: HuggingFace model ID\n",
    "        group_sizes: List of group sizes to compare\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results for each group size\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Calibration data\n",
    "    calibration_texts = [\n",
    "        \"Machine learning is a field of artificial intelligence.\",\n",
    "        \"Deep learning uses neural networks with many layers.\",\n",
    "        \"Natural language processing enables text understanding.\",\n",
    "        \"Computer vision allows machines to interpret images.\",\n",
    "    ] * 32\n",
    "    \n",
    "    for gs in group_sizes:\n",
    "        print(f\"\\nQuantizing with group_size={gs}...\")\n",
    "        \n",
    "        clear_memory()\n",
    "        \n",
    "        awq_config = {\n",
    "            \"zero_point\": True,\n",
    "            \"q_group_size\": gs,\n",
    "            \"w_bit\": 4,\n",
    "            \"version\": \"GEMM\"\n",
    "        }\n",
    "        \n",
    "        # Load model\n",
    "        model = AutoAWQForCausalLM.from_pretrained(\n",
    "            model_id, \n",
    "            device_map=\"cuda\", \n",
    "            safetensors=True\n",
    "        )\n",
    "        \n",
    "        # Quantize\n",
    "        start = time.time()\n",
    "        model.quantize(\n",
    "            tokenizer, \n",
    "            quant_config=awq_config, \n",
    "            calib_data=calibration_texts\n",
    "        )\n",
    "        quant_time = time.time() - start\n",
    "        \n",
    "        # Save and measure size\n",
    "        save_path = f\"./awq_g{gs}\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        model.save_quantized(save_path)\n",
    "        tokenizer.save_pretrained(save_path)\n",
    "        \n",
    "        model_size = sum(\n",
    "            os.path.getsize(os.path.join(save_path, f))\n",
    "            for f in os.listdir(save_path)\n",
    "            if f.endswith('.safetensors') or f.endswith('.bin')\n",
    "        ) / 1e6\n",
    "        \n",
    "        results[gs] = {\n",
    "            'size_mb': model_size,\n",
    "            'quant_time': quant_time,\n",
    "            'save_path': save_path\n",
    "        }\n",
    "        \n",
    "        print(f\"  Size: {model_size:.1f} MB\")\n",
    "        print(f\"  Time: {quant_time:.1f}s\")\n",
    "        \n",
    "        del model\n",
    "        clear_memory()\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"AWQ Group Size Comparison\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"{'Group Size':>12} {'Size (MB)':>12} {'Time (s)':>12}\")\n",
    "    print(\"-\"*50)\n",
    "    for gs, data in results.items():\n",
    "        print(f\"{gs:>12} {data['size_mb']:>12.1f} {data['quant_time']:>12.1f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# results = compare_awq_group_sizes(\"facebook/opt-350m\", [32, 64, 128])\n",
    "print(\"AWQ group size comparison function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Task-Specific Evaluation\n",
    "\n",
    "Use lm-eval to evaluate on specific benchmark tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_eval import evaluator\n",
    "\n",
    "def evaluate_on_tasks(\n",
    "    model_path: str, \n",
    "    tasks: list = [\"hellaswag\", \"arc_easy\"],\n",
    "    batch_size: int = 8\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate a quantized model on specific benchmark tasks.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the quantized model\n",
    "        tasks: List of tasks to evaluate on\n",
    "        batch_size: Batch size for evaluation\n",
    "    \n",
    "    Returns:\n",
    "        Evaluation results dictionary\n",
    "    \"\"\"\n",
    "    print(f\"Evaluating {model_path} on tasks: {tasks}\")\n",
    "    print(\"This may take a while...\")\n",
    "    \n",
    "    results = evaluator.simple_evaluate(\n",
    "        model=\"hf\",\n",
    "        model_args=f\"pretrained={model_path}\",\n",
    "        tasks=tasks,\n",
    "        batch_size=batch_size,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Evaluation Results\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for task, scores in results['results'].items():\n",
    "        # Try different accuracy keys\n",
    "        acc = scores.get('acc', scores.get('acc_norm', scores.get('acc,none', 'N/A')))\n",
    "        if isinstance(acc, float):\n",
    "            print(f\"{task}: {acc:.3f} ({acc*100:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"{task}: {acc}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# results = evaluate_on_tasks(\"./awq_g128\", [\"hellaswag\", \"arc_easy\"])\n",
    "print(\"Task evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete comparison: AWQ vs GPTQ on benchmark tasks\n",
    "\n",
    "def compare_quantization_quality(\n",
    "    awq_path: str,\n",
    "    gptq_path: str,\n",
    "    tasks: list = [\"hellaswag\"]\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compare AWQ and GPTQ on benchmark tasks.\n",
    "    \n",
    "    Args:\n",
    "        awq_path: Path to AWQ quantized model\n",
    "        gptq_path: Path to GPTQ quantized model\n",
    "        tasks: Tasks to evaluate on\n",
    "    \n",
    "    Returns:\n",
    "        Comparison results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for name, path in [('AWQ', awq_path), ('GPTQ', gptq_path)]:\n",
    "        print(f\"\\nEvaluating {name}...\")\n",
    "        \n",
    "        eval_results = evaluator.simple_evaluate(\n",
    "            model=\"hf\",\n",
    "            model_args=f\"pretrained={path}\",\n",
    "            tasks=tasks,\n",
    "            batch_size=8,\n",
    "            device=\"cuda\"\n",
    "        )\n",
    "        \n",
    "        results[name] = eval_results['results']\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"AWQ vs GPTQ Comparison\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Task':<20} {'AWQ':>15} {'GPTQ':>15} {'Difference':>15}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    for task in tasks:\n",
    "        awq_acc = results['AWQ'].get(task, {}).get('acc', 0)\n",
    "        gptq_acc = results['GPTQ'].get(task, {}).get('acc', 0)\n",
    "        diff = awq_acc - gptq_acc\n",
    "        \n",
    "        diff_str = f\"+{diff:.3f}\" if diff > 0 else f\"{diff:.3f}\"\n",
    "        winner = \"AWQ\" if diff > 0 else \"GPTQ\" if diff < 0 else \"Tie\"\n",
    "        \n",
    "        print(f\"{task:<20} {awq_acc:>15.3f} {gptq_acc:>15.3f} {diff_str:>15} ({winner})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# results = compare_quantization_quality(\"./awq_g128\", \"./gptq_g128\", [\"hellaswag\"])\n",
    "print(\"Comparison function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **AWQ protects salient weights** - Better quality than GPTQ in many cases\n",
    "2. **Group size affects model size** - Smaller groups = slightly larger model\n",
    "3. **lm-eval is the standard** - Use it for reproducible benchmarks\n",
    "4. **Task-specific evaluation matters** - General benchmarks may not reflect your use case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}