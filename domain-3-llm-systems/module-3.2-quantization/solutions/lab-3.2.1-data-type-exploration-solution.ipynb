{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2.1: Data Type Exploration - Solutions\n",
    "\n",
    "This notebook contains solutions for all exercises in Lab 3.2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import utilities\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from scripts import symmetric_quantize, asymmetric_quantize, dequantize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Custom Precision Analyzer\n",
    "\n",
    "Create a function that analyzes how different data types affect a neural network layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_layer_precision(weights: np.ndarray, activations: np.ndarray, dtype: str) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze precision loss when performing layer computation in different dtypes.\n",
    "    \n",
    "    Args:\n",
    "        weights: Weight matrix (out_features, in_features)\n",
    "        activations: Input activations (batch, in_features)\n",
    "        dtype: Target dtype ('fp32', 'fp16', 'bf16', 'int8')\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with analysis metrics\n",
    "    \"\"\"\n",
    "    # Reference output in FP32\n",
    "    ref_output = activations @ weights.T\n",
    "    \n",
    "    if dtype == 'fp32':\n",
    "        output = ref_output\n",
    "    elif dtype == 'fp16':\n",
    "        w_fp16 = weights.astype(np.float16)\n",
    "        a_fp16 = activations.astype(np.float16)\n",
    "        output = (a_fp16 @ w_fp16.T).astype(np.float32)\n",
    "    elif dtype == 'bf16':\n",
    "        # Simulate bfloat16 by truncating mantissa\n",
    "        w_torch = torch.from_numpy(weights).to(torch.bfloat16)\n",
    "        a_torch = torch.from_numpy(activations).to(torch.bfloat16)\n",
    "        output = (a_torch @ w_torch.T).float().numpy()\n",
    "    elif dtype == 'int8':\n",
    "        # Symmetric quantization\n",
    "        w_quant, w_scale = symmetric_quantize(weights, 8)\n",
    "        a_quant, a_scale = symmetric_quantize(activations, 8)\n",
    "        # Integer matmul and dequantize\n",
    "        int_output = a_quant.astype(np.int32) @ w_quant.T.astype(np.int32)\n",
    "        output = int_output.astype(np.float32) * (a_scale * w_scale)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dtype: {dtype}\")\n",
    "    \n",
    "    # Compute metrics\n",
    "    abs_error = np.abs(output - ref_output)\n",
    "    rel_error = abs_error / (np.abs(ref_output) + 1e-10)\n",
    "    \n",
    "    return {\n",
    "        'dtype': dtype,\n",
    "        'mean_abs_error': np.mean(abs_error),\n",
    "        'max_abs_error': np.max(abs_error),\n",
    "        'mean_rel_error': np.mean(rel_error),\n",
    "        'max_rel_error': np.max(rel_error),\n",
    "        'snr_db': 10 * np.log10(np.mean(ref_output**2) / (np.mean(abs_error**2) + 1e-10)),\n",
    "        'output_range': (output.min(), output.max())\n",
    "    }\n",
    "\n",
    "\n",
    "# Test the function\n",
    "np.random.seed(42)\n",
    "weights = np.random.randn(256, 512).astype(np.float32) * 0.1\n",
    "activations = np.random.randn(32, 512).astype(np.float32)\n",
    "\n",
    "print(\"Layer Precision Analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for dtype in ['fp32', 'fp16', 'bf16', 'int8']:\n",
    "    result = analyze_layer_precision(weights, activations, dtype)\n",
    "    print(f\"\\n{dtype.upper()}:\")\n",
    "    print(f\"  Mean Abs Error: {result['mean_abs_error']:.6f}\")\n",
    "    print(f\"  Max Abs Error:  {result['max_abs_error']:.6f}\")\n",
    "    print(f\"  SNR:            {result['snr_db']:.1f} dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Quantization Error Visualization\n",
    "\n",
    "Visualize how quantization error varies across the value range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_quantization_error(values: np.ndarray, bits: int):\n",
    "    \"\"\"\n",
    "    Visualize quantization error distribution.\n",
    "    \"\"\"\n",
    "    # Quantize and dequantize\n",
    "    quantized, scale = symmetric_quantize(values, bits)\n",
    "    dequantized = dequantize(quantized, scale)\n",
    "    \n",
    "    errors = values - dequantized\n",
    "    rel_errors = np.abs(errors) / (np.abs(values) + 1e-10)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Original vs Quantized scatter\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.scatter(values, dequantized, alpha=0.3, s=1)\n",
    "    ax1.plot([values.min(), values.max()], [values.min(), values.max()], \n",
    "             'r--', label='Perfect')\n",
    "    ax1.set_xlabel('Original Values')\n",
    "    ax1.set_ylabel('Quantized Values')\n",
    "    ax1.set_title(f'{bits}-bit Quantization: Original vs Reconstructed')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Error histogram\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.hist(errors, bins=50, edgecolor='black', alpha=0.7)\n",
    "    ax2.axvline(0, color='r', linestyle='--')\n",
    "    ax2.set_xlabel('Quantization Error')\n",
    "    ax2.set_ylabel('Count')\n",
    "    ax2.set_title(f'Error Distribution (std={np.std(errors):.6f})')\n",
    "    \n",
    "    # Error vs value magnitude\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.scatter(np.abs(values), np.abs(errors), alpha=0.3, s=1)\n",
    "    ax3.set_xlabel('|Original Value|')\n",
    "    ax3.set_ylabel('|Quantization Error|')\n",
    "    ax3.set_title('Error vs Value Magnitude')\n",
    "    \n",
    "    # Relative error vs value\n",
    "    ax4 = axes[1, 1]\n",
    "    mask = np.abs(values) > 0.01  # Avoid division by near-zero\n",
    "    ax4.scatter(np.abs(values[mask]), rel_errors[mask] * 100, alpha=0.3, s=1)\n",
    "    ax4.set_xlabel('|Original Value|')\n",
    "    ax4.set_ylabel('Relative Error (%)')\n",
    "    ax4.set_title('Relative Error vs Value Magnitude')\n",
    "    ax4.set_yscale('log')\n",
    "    \n",
    "    plt.suptitle(f'{bits}-bit Symmetric Quantization Analysis', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n{bits}-bit Quantization Statistics:\")\n",
    "    print(f\"  Scale: {scale:.6f}\")\n",
    "    print(f\"  Mean Error: {np.mean(np.abs(errors)):.6f}\")\n",
    "    print(f\"  Max Error: {np.max(np.abs(errors)):.6f}\")\n",
    "    print(f\"  RMSE: {np.sqrt(np.mean(errors**2)):.6f}\")\n",
    "\n",
    "\n",
    "# Test with different distributions\n",
    "np.random.seed(42)\n",
    "\n",
    "# Normal distribution (typical weights)\n",
    "normal_values = np.random.randn(10000).astype(np.float32)\n",
    "print(\"Normal Distribution:\")\n",
    "visualize_quantization_error(normal_values, 8)\n",
    "\n",
    "# Heavy-tailed distribution (activations)\n",
    "print(\"\\nHeavy-Tailed Distribution:\")\n",
    "heavy_tail = np.random.standard_t(3, 10000).astype(np.float32)\n",
    "visualize_quantization_error(heavy_tail, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 Solution: Optimal Bit-Width Selection\n",
    "\n",
    "Determine the minimum bit-width needed to maintain acceptable error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_bitwidth(\n",
    "    values: np.ndarray,\n",
    "    max_error_threshold: float = 0.01,\n",
    "    max_bits: int = 16\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Find minimum bits needed to keep max relative error below threshold.\n",
    "    \n",
    "    Args:\n",
    "        values: Array to quantize\n",
    "        max_error_threshold: Maximum acceptable relative error\n",
    "        max_bits: Maximum bits to try\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with optimal bit-width and analysis\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for bits in range(2, max_bits + 1):\n",
    "        quantized, scale = symmetric_quantize(values, bits)\n",
    "        dequantized = dequantize(quantized, scale)\n",
    "        \n",
    "        abs_errors = np.abs(values - dequantized)\n",
    "        # Relative error (avoid division by zero)\n",
    "        rel_errors = abs_errors / (np.abs(values) + 1e-10)\n",
    "        \n",
    "        max_rel_error = np.max(rel_errors[np.abs(values) > 0.01])\n",
    "        mean_rel_error = np.mean(rel_errors[np.abs(values) > 0.01])\n",
    "        \n",
    "        # Compression ratio\n",
    "        compression = 32 / bits\n",
    "        \n",
    "        results.append({\n",
    "            'bits': bits,\n",
    "            'max_rel_error': max_rel_error,\n",
    "            'mean_rel_error': mean_rel_error,\n",
    "            'compression': compression,\n",
    "            'meets_threshold': max_rel_error < max_error_threshold\n",
    "        })\n",
    "    \n",
    "    # Find optimal\n",
    "    optimal = None\n",
    "    for r in results:\n",
    "        if r['meets_threshold']:\n",
    "            optimal = r\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        'optimal_bits': optimal['bits'] if optimal else max_bits,\n",
    "        'threshold': max_error_threshold,\n",
    "        'all_results': results,\n",
    "        'recommendation': f\"{optimal['bits']}-bit\" if optimal else f\"{max_bits}-bit (threshold not met)\"\n",
    "    }\n",
    "\n",
    "\n",
    "# Test with model weights\n",
    "np.random.seed(42)\n",
    "weights = np.random.randn(10000).astype(np.float32) * 0.02  # Typical weight scale\n",
    "\n",
    "# Find optimal for different thresholds\n",
    "for threshold in [0.1, 0.05, 0.01, 0.001]:\n",
    "    result = find_optimal_bitwidth(weights, threshold)\n",
    "    print(f\"\\nThreshold {threshold*100:.1f}%: {result['recommendation']}\")\n",
    "    \n",
    "    # Show compression\n",
    "    bits = result['optimal_bits']\n",
    "    print(f\"  Compression ratio: {32/bits:.1f}x\")\n",
    "    print(f\"  Memory savings: {(1 - bits/32)*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key findings from the exercises:\n",
    "\n",
    "1. **FP16 vs INT8**: INT8 has higher quantization error but better throughput\n",
    "2. **Error distribution**: Quantization error is roughly uniform for symmetric quantization\n",
    "3. **Optimal bit-width**: For 1% error threshold, typically 8 bits is sufficient for weights\n",
    "4. **Heavy-tailed distributions**: Require more bits or clipping for good accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
