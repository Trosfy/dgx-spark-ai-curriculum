{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 11.5 Solutions: FP4 Deep Dive\n",
    "\n",
    "This notebook contains solutions to the exercises from Task 11.5.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def clear_buffer_cache():\n",
    "    \"\"\"Clear system buffer cache for optimal memory availability.\"\"\"\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            \"sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'\",\n",
    "            shell=True, check=True, capture_output=True\n",
    "        )\n",
    "        print(\"Buffer cache cleared\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"Note: Could not clear buffer cache (may need sudo)\")\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Quantize Llama 2 7B with NVFP4\n",
    "\n",
    "Apply FP4 quantization to a larger model and measure quality/speed tradeoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modelopt.torch.quantization as mtq\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def quantize_model_fp4(\n",
    "    model_id: str,\n",
    "    calibration_samples: int = 128,\n",
    "    use_nvfp4: bool = True\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Quantize a model to FP4 using TensorRT Model Optimizer.\n",
    "    \n",
    "    Args:\n",
    "        model_id: HuggingFace model ID\n",
    "        calibration_samples: Number of calibration samples\n",
    "        use_nvfp4: Use NVFP4 (True) or MXFP4 (False)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (quantized_model, tokenizer)\n",
    "    \"\"\"\n",
    "    print(f\"Loading {model_id}...\")\n",
    "    \n",
    "    # Clear memory before loading large model\n",
    "    clear_buffer_cache()\n",
    "    clear_memory()\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"cuda\"\n",
    "    )\n",
    "    \n",
    "    fp16_memory = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"FP16 memory: {fp16_memory:.2f} GB\")\n",
    "    \n",
    "    # Create calibration data\n",
    "    calibration_texts = [\n",
    "        \"Machine learning is transforming the world of technology.\",\n",
    "        \"Neural networks learn complex patterns from data.\",\n",
    "        \"Deep learning enables breakthrough applications in AI.\",\n",
    "        \"The transformer architecture revolutionized NLP.\",\n",
    "        \"Quantization reduces model precision for efficiency.\",\n",
    "        \"GPU acceleration enables fast neural network training.\",\n",
    "        \"Transfer learning leverages pre-trained knowledge.\",\n",
    "        \"Attention mechanisms help focus on relevant information.\",\n",
    "    ] * ((calibration_samples // 8) + 1)\n",
    "    calibration_texts = calibration_texts[:calibration_samples]\n",
    "    \n",
    "    # Tokenize\n",
    "    encodings = tokenizer(\n",
    "        calibration_texts,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    dataset = TensorDataset(encodings.input_ids, encodings.attention_mask)\n",
    "    calib_loader = DataLoader(dataset, batch_size=8)\n",
    "    \n",
    "    # Calibration forward function\n",
    "    def calib_forward(model):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (input_ids, attention_mask) in enumerate(calib_loader):\n",
    "                _ = model(\n",
    "                    input_ids=input_ids.cuda(),\n",
    "                    attention_mask=attention_mask.cuda()\n",
    "                )\n",
    "                if batch_idx >= 15:\n",
    "                    break\n",
    "    \n",
    "    # Apply quantization\n",
    "    print(f\"Applying {'NVFP4' if use_nvfp4 else 'MXFP4'} quantization...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        if use_nvfp4:\n",
    "            config = mtq.NVFP4_DEFAULT_CFG\n",
    "        else:\n",
    "            config = mtq.MXFP4_DEFAULT_CFG\n",
    "        \n",
    "        model_fp4 = mtq.quantize(model, config, forward_loop=calib_forward)\n",
    "        quant_time = time.time() - start_time\n",
    "        print(f\"Quantization complete in {quant_time:.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"FP4 not available: {e}\")\n",
    "        print(\"Falling back to FP8...\")\n",
    "        \n",
    "        config = mtq.FP8_DEFAULT_CFG\n",
    "        model_fp4 = mtq.quantize(model, config, forward_loop=calib_forward)\n",
    "    \n",
    "    # Measure FP4 memory\n",
    "    clear_memory()\n",
    "    fp4_memory = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"FP4 memory: {fp4_memory:.2f} GB\")\n",
    "    print(f\"Memory reduction: {fp16_memory/fp4_memory:.2f}x\")\n",
    "    \n",
    "    return model_fp4, tokenizer\n",
    "\n",
    "\n",
    "# Example usage (uncomment to run on Blackwell):\n",
    "# model_fp4, tokenizer = quantize_model_fp4(\"meta-llama/Llama-2-7b-hf\")\n",
    "print(\"FP4 quantization function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate FP4 model quality\n",
    "\n",
    "def evaluate_fp4_quality(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    eval_texts: list = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate FP4 model quality with perplexity and generation tests.\n",
    "    \n",
    "    Args:\n",
    "        model: Quantized model\n",
    "        tokenizer: Tokenizer\n",
    "        eval_texts: List of texts for perplexity evaluation\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with evaluation results\n",
    "    \"\"\"\n",
    "    if eval_texts is None:\n",
    "        eval_texts = [\n",
    "            \"The quick brown fox jumps over the lazy dog.\",\n",
    "            \"Machine learning enables computers to learn from data.\",\n",
    "            \"The ancient civilization built impressive structures.\",\n",
    "            \"Modern medicine has extended human lifespan.\",\n",
    "            \"Climate models predict significant changes.\",\n",
    "        ]\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    print(\"Calculating perplexity...\")\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(eval_texts, desc=\"Perplexity\"):\n",
    "            encodings = tokenizer(\n",
    "                text,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                max_length=256\n",
    "            )\n",
    "            input_ids = encodings.input_ids.to(model.device)\n",
    "            \n",
    "            if input_ids.size(1) < 2:\n",
    "                continue\n",
    "            \n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss.item()\n",
    "            num_tokens = input_ids.size(1) - 1\n",
    "            \n",
    "            total_loss += loss * num_tokens\n",
    "            total_tokens += num_tokens\n",
    "    \n",
    "    perplexity = math.exp(total_loss / total_tokens)\n",
    "    \n",
    "    # Test generation\n",
    "    print(\"Testing generation...\")\n",
    "    prompt = \"The future of artificial intelligence\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(**inputs, max_new_tokens=10, do_sample=False)\n",
    "    \n",
    "    # Benchmark\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    gen_time = time.perf_counter() - start\n",
    "    \n",
    "    tokens_generated = outputs.shape[1] - inputs['input_ids'].shape[1]\n",
    "    tokens_per_second = tokens_generated / gen_time\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    results = {\n",
    "        'perplexity': perplexity,\n",
    "        'tokens_per_second': tokens_per_second,\n",
    "        'generated_text': generated_text\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Perplexity: {perplexity:.2f}\")\n",
    "    print(f\"  Speed: {tokens_per_second:.1f} tok/s\")\n",
    "    print(f\"  Generated: {generated_text[:100]}...\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# results = evaluate_fp4_quality(model_fp4, tokenizer)\n",
    "print(\"FP4 evaluation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Compare Calibration Data Quality\n",
    "\n",
    "Try quantizing with different calibration datasets and compare quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different calibration data types\n",
    "\n",
    "RANDOM_CALIBRATION = [\n",
    "    \"Lorem ipsum dolor sit amet consectetur adipiscing elit.\",\n",
    "    \"The quick brown fox jumps over the lazy dog repeatedly.\",\n",
    "    \"Random words strung together in sentences for testing.\",\n",
    "    \"Numbers and letters 123 ABC mixed together here.\",\n",
    "] * 32\n",
    "\n",
    "DOMAIN_CALIBRATION = [\n",
    "    \"Machine learning is a subset of artificial intelligence that enables systems to learn from data.\",\n",
    "    \"Deep learning uses neural networks with multiple layers to extract complex patterns.\",\n",
    "    \"Natural language processing allows computers to understand and generate human language.\",\n",
    "    \"Computer vision enables machines to interpret and analyze visual information from the world.\",\n",
    "    \"Reinforcement learning trains agents through rewards and penalties in an environment.\",\n",
    "    \"Transfer learning leverages knowledge from one task to improve performance on another.\",\n",
    "    \"The transformer architecture revolutionized natural language processing in 2017.\",\n",
    "    \"Attention mechanisms help models focus on relevant parts of the input.\",\n",
    "] * 16\n",
    "\n",
    "CODE_CALIBRATION = [\n",
    "    \"def fibonacci(n):\\n    if n <= 1:\\n        return n\\n    return fibonacci(n-1) + fibonacci(n-2)\",\n",
    "    \"class DataProcessor:\\n    def __init__(self, data):\\n        self.data = data\\n    def process(self):\\n        return [x * 2 for x in self.data]\",\n",
    "    \"import numpy as np\\ndef matrix_multiply(a, b):\\n    return np.dot(a, b)\",\n",
    "    \"async def fetch_data(url):\\n    async with aiohttp.ClientSession() as session:\\n        async with session.get(url) as response:\\n            return await response.json()\",\n",
    "    \"for i in range(10):\\n    print(f'Iteration {i}')\",\n",
    "    \"try:\\n    result = process_data(input)\\nexcept Exception as e:\\n    logger.error(f'Error: {e}')\",\n",
    "] * 22\n",
    "\n",
    "print(f\"Random calibration samples: {len(RANDOM_CALIBRATION)}\")\n",
    "print(f\"Domain calibration samples: {len(DOMAIN_CALIBRATION)}\")\n",
    "print(f\"Code calibration samples: {len(CODE_CALIBRATION)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_calibration_quality(\n",
    "    model_id: str,\n",
    "    calibration_datasets: dict\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compare FP4 quantization quality with different calibration datasets.\n",
    "    \n",
    "    Args:\n",
    "        model_id: HuggingFace model ID\n",
    "        calibration_datasets: Dict mapping dataset name to list of texts\n",
    "    \n",
    "    Returns:\n",
    "        Dict with results for each calibration dataset\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Evaluation texts (held out from calibration)\n",
    "    eval_texts = [\n",
    "        \"The history of computing spans several decades of innovation.\",\n",
    "        \"Scientific research requires careful methodology and analysis.\",\n",
    "        \"Economic factors influence market behavior significantly.\",\n",
    "        \"Space exploration continues to push boundaries of knowledge.\",\n",
    "        \"Medical advances have improved human health outcomes.\",\n",
    "    ]\n",
    "    \n",
    "    for name, calib_texts in calibration_datasets.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing calibration: {name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        clear_buffer_cache()\n",
    "        clear_memory()\n",
    "        \n",
    "        # Load fresh model\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"cuda\"\n",
    "        )\n",
    "        \n",
    "        # Create calibration dataloader\n",
    "        encodings = tokenizer(\n",
    "            calib_texts[:128],\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        dataset = TensorDataset(encodings.input_ids, encodings.attention_mask)\n",
    "        calib_loader = DataLoader(dataset, batch_size=8)\n",
    "        \n",
    "        def calib_forward(model):\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (input_ids, attention_mask) in enumerate(calib_loader):\n",
    "                    _ = model(\n",
    "                        input_ids=input_ids.cuda(),\n",
    "                        attention_mask=attention_mask.cuda()\n",
    "                    )\n",
    "                    if batch_idx >= 15:\n",
    "                        break\n",
    "        \n",
    "        # Quantize\n",
    "        try:\n",
    "            config = mtq.NVFP4_DEFAULT_CFG\n",
    "            model_fp4 = mtq.quantize(model, config, forward_loop=calib_forward)\n",
    "        except Exception as e:\n",
    "            print(f\"FP4 not available, using FP8: {e}\")\n",
    "            config = mtq.FP8_DEFAULT_CFG\n",
    "            model_fp4 = mtq.quantize(model, config, forward_loop=calib_forward)\n",
    "        \n",
    "        # Evaluate\n",
    "        eval_result = evaluate_fp4_quality(model_fp4, tokenizer, eval_texts)\n",
    "        results[name] = eval_result\n",
    "        \n",
    "        del model, model_fp4\n",
    "        clear_memory()\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Calibration Comparison Summary\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"{'Dataset':<20} {'Perplexity':>12} {'Speed (tok/s)':>15}\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    for name, result in results.items():\n",
    "        print(f\"{name:<20} {result['perplexity']:>12.2f} {result['tokens_per_second']:>15.1f}\")\n",
    "    \n",
    "    # Find best\n",
    "    best = min(results.items(), key=lambda x: x[1]['perplexity'])\n",
    "    print(f\"\\nBest calibration: {best[0]} (PPL: {best[1]['perplexity']:.2f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# results = compare_calibration_quality(\n",
    "#     \"facebook/opt-350m\",\n",
    "#     {\n",
    "#         \"Random\": RANDOM_CALIBRATION,\n",
    "#         \"Domain-specific\": DOMAIN_CALIBRATION,\n",
    "#         \"Code\": CODE_CALIBRATION\n",
    "#     }\n",
    "# )\n",
    "print(\"Calibration comparison function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **FP4 is Blackwell-exclusive** - Native tensor core support for maximum performance\n",
    "2. **Calibration data matters** - Domain-specific data typically yields better quality\n",
    "3. **3.5Ã— memory reduction** - Enables running larger models on DGX Spark\n",
    "4. **<1% quality loss** - With proper calibration and dual-level scaling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
