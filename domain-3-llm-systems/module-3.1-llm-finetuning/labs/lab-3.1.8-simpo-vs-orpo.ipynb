{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1.8: SimPO vs ORPO - Modern Preference Optimization\n",
    "\n",
    "**Module:** 3.1 - Large Language Model Fine-Tuning  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐⭐⭐☆\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand SimPO and ORPO as DPO alternatives\n",
    "- [ ] Know when to use each method\n",
    "- [ ] Implement both with TRL\n",
    "- [ ] Compare their memory and quality tradeoffs\n",
    "\n",
    "---\n",
    "\n",
    "## Why Move Beyond DPO?\n",
    "\n",
    "DPO is great, but has some limitations:\n",
    "\n",
    "| Issue | DPO | SimPO | ORPO |\n",
    "|-------|-----|-------|------|\n",
    "| Needs reference model | ✅ Yes (2x memory) | ❌ No | ❌ No |\n",
    "| Quality on AlpacaEval | Baseline | **+6.4 points** | Comparable |\n",
    "| Memory usage | High | Medium | **50% less** |\n",
    "| Complexity | Medium | Low | Low |\n",
    "\n",
    "**SimPO** = Better quality, simpler  \n",
    "**ORPO** = Less memory, single-stage training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELI5: SimPO vs ORPO\n",
    "\n",
    "> **Imagine you're grading essays:**\n",
    ">\n",
    "> **DPO** is like: \"Compare this essay to the original student's average work, then decide if this is better or worse.\"\n",
    "> - Requires keeping the \"original\" for comparison\n",
    ">\n",
    "> **SimPO (Simple Preference Optimization)** is like: \"Just look at the two essays and pick the better one. Trust your instincts!\"\n",
    "> - No reference needed\n",
    "> - Uses length-normalized scoring (longer isn't always better)\n",
    ">\n",
    "> **ORPO (Odds Ratio Preference Optimization)** is like: \"Score both essays, then boost the good one and penalize the bad one in the same step.\"\n",
    "> - Combines SFT + preference learning in one stage\n",
    "> - Most memory-efficient\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Algorithms\n",
    "\n",
    "### SimPO (Simple Preference Optimization)\n",
    "\n",
    "SimPO removes the reference model and uses length-normalized log probabilities:\n",
    "\n",
    "$$\\mathcal{L}_{SimPO} = -\\log \\sigma\\left(\\frac{\\beta}{|y_w|} \\log \\pi_\\theta(y_w|x) - \\frac{\\beta}{|y_l|} \\log \\pi_\\theta(y_l|x) - \\gamma\\right)$$\n",
    "\n",
    "Key innovations:\n",
    "- **No reference model** - saves 50% memory\n",
    "- **Length normalization** - prevents favoring longer responses\n",
    "- **Gamma margin** - ensures chosen is preferred by at least γ\n",
    "\n",
    "### ORPO (Odds Ratio Preference Optimization)\n",
    "\n",
    "ORPO combines SFT and preference learning:\n",
    "\n",
    "$$\\mathcal{L}_{ORPO} = \\mathcal{L}_{SFT}(y_w) + \\lambda \\cdot \\mathcal{L}_{OR}$$\n",
    "\n",
    "Where the odds ratio loss is:\n",
    "\n",
    "$$\\mathcal{L}_{OR} = -\\log \\sigma\\left(\\log \\frac{P(y_w|x)}{1-P(y_w|x)} - \\log \\frac{P(y_l|x)}{1-P(y_l|x)}\\right)$$\n",
    "\n",
    "Key innovations:\n",
    "- **Single training stage** - no separate SFT then DPO\n",
    "- **No reference model needed**\n",
    "- **Uses odds ratios** - more stable than log probs alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import gc\n",
    "from typing import Dict, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import ORPOTrainer, ORPOConfig\n",
    "from datasets import Dataset\n",
    "\n",
    "# Note: SimPO requires TRL >= 0.9.0\n",
    "# from trl import SimPOTrainer, SimPOConfig\n",
    "\n",
    "print(\"Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Preference Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same preference data format as DPO\n",
    "preference_data = [\n",
    "    {\n",
    "        \"prompt\": \"Explain neural networks simply.\",\n",
    "        \"chosen\": \"Neural networks are computing systems inspired by biological brains. They consist of layers of connected nodes that learn patterns from data. Like how you learn to recognize faces by seeing many examples, neural networks learn by processing thousands of examples until they can make accurate predictions.\",\n",
    "        \"rejected\": \"Neural networks are machine learning models used in AI.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What makes good code?\",\n",
    "        \"chosen\": \"Good code is: 1) Readable - others can understand it. 2) Maintainable - easy to fix and extend. 3) Tested - has automated tests. 4) Simple - does one thing well. 5) Documented - explains the 'why'. Remember: code is read more often than written!\",\n",
    "        \"rejected\": \"Good code works correctly and is efficient.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"How do I stay motivated?\",\n",
    "        \"chosen\": \"Build sustainable motivation: Start small (2-minute rule), track your wins, connect tasks to bigger goals, celebrate progress not perfection, rest when needed (burnout kills motivation), and surround yourself with supportive people. Remember: motivation follows action, not the other way around.\",\n",
    "        \"rejected\": \"Just push through and don't give up. Think positive thoughts.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Explain recursion.\",\n",
    "        \"chosen\": \"Recursion is when a function calls itself to solve smaller versions of the same problem. Example: To count people in a line, ask the person in front 'how many are ahead of you?' and add 1. They ask the next person, and so on until someone says 'zero'. That's the base case that stops the recursion.\",\n",
    "        \"rejected\": \"Recursion is when a function calls itself. It needs a base case.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What is overfitting?\",\n",
    "        \"chosen\": \"Overfitting is when a model memorizes training data instead of learning general patterns. Imagine a student who memorizes test answers but can't solve new problems. Signs: perfect training score but poor test score. Solutions: more data, simpler models, regularization, or dropout.\",\n",
    "        \"rejected\": \"Overfitting happens when a model is too complex for the data.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Expand dataset\n",
    "expanded_data = []\n",
    "for item in preference_data:\n",
    "    expanded_data.append(item)\n",
    "    expanded_data.append({\n",
    "        \"prompt\": \"Can you \" + item[\"prompt\"].lower(),\n",
    "        \"chosen\": item[\"chosen\"],\n",
    "        \"rejected\": item[\"rejected\"],\n",
    "    })\n",
    "\n",
    "preference_dataset = Dataset.from_list(expanded_data)\n",
    "print(f\"Dataset: {len(preference_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: ORPO Training\n",
    "\n",
    "ORPO is fully integrated in TRL and is very memory-efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "print(f\"Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Add LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(f\"\\nModel loaded!\")\n",
    "model.print_trainable_parameters()\n",
    "print(f\"Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORPO Configuration\n",
    "orpo_config = ORPOConfig(\n",
    "    output_dir=\"./orpo_output\",\n",
    "    \n",
    "    # ORPO-specific\n",
    "    beta=0.1,  # Weight of the odds ratio loss\n",
    "    \n",
    "    # Training\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-5,\n",
    "    \n",
    "    # Sequence\n",
    "    max_length=512,\n",
    "    max_prompt_length=256,\n",
    "    \n",
    "    # Optimization\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    bf16=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=5,\n",
    "    \n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"ORPO configuration created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ORPO trainer\n",
    "orpo_trainer = ORPOTrainer(\n",
    "    model=model,\n",
    "    args=orpo_config,\n",
    "    train_dataset=preference_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"ORPO Trainer created!\")\n",
    "print(f\"Memory after setup: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "print(\"\\nNote: No reference model needed! (50% less memory than DPO)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with ORPO\n",
    "print(\"=\"*50)\n",
    "print(\"STARTING ORPO TRAINING\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nORPO combines SFT + preference learning in one stage!\")\n",
    "\n",
    "orpo_result = orpo_trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ORPO TRAINING COMPLETE!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print metrics\n",
    "print(\"\\nORPO Training Metrics:\")\n",
    "for key, value in orpo_result.metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: SimPO (Reference Implementation)\n",
    "\n",
    "SimPO may require a newer version of TRL. Here's how it works conceptually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimPO conceptual implementation\n",
    "# (TRL may have SimPOTrainer in newer versions)\n",
    "\n",
    "simpo_pseudocode = \"\"\"\n",
    "# SimPO Training (when available in TRL)\n",
    "\n",
    "from trl import SimPOTrainer, SimPOConfig\n",
    "\n",
    "simpo_config = SimPOConfig(\n",
    "    output_dir=\"./simpo_output\",\n",
    "    \n",
    "    # SimPO-specific parameters\n",
    "    beta=2.0,              # Controls preference strength\n",
    "    gamma_beta_ratio=0.5,  # Margin γ = 0.5 * β = 1.0\n",
    "    \n",
    "    # Note: No reference model needed!\n",
    "    \n",
    "    # Standard training params\n",
    "    per_device_train_batch_size=2,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=1,\n",
    "    ...\n",
    ")\n",
    "\n",
    "trainer = SimPOTrainer(\n",
    "    model=model,\n",
    "    args=simpo_config,\n",
    "    train_dataset=preference_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\"\"\"\n",
    "\n",
    "print(\"SimPO Pseudocode:\")\n",
    "print(simpo_pseudocode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual SimPO loss implementation for understanding\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def simpo_loss(\n",
    "    policy_chosen_logps: torch.Tensor,\n",
    "    policy_rejected_logps: torch.Tensor,\n",
    "    chosen_lengths: torch.Tensor,\n",
    "    rejected_lengths: torch.Tensor,\n",
    "    beta: float = 2.0,\n",
    "    gamma: float = 1.0,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute SimPO loss.\n",
    "    \n",
    "    SimPO uses length-normalized log probabilities and a margin.\n",
    "    No reference model needed!\n",
    "    \"\"\"\n",
    "    # Length-normalize the log probabilities\n",
    "    chosen_rewards = beta * policy_chosen_logps / chosen_lengths\n",
    "    rejected_rewards = beta * policy_rejected_logps / rejected_lengths\n",
    "    \n",
    "    # Compute loss with margin\n",
    "    logits = chosen_rewards - rejected_rewards - gamma\n",
    "    loss = -F.logsigmoid(logits).mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Demo\n",
    "chosen_logps = torch.tensor([-10.0, -15.0, -12.0])\n",
    "rejected_logps = torch.tensor([-20.0, -25.0, -22.0])\n",
    "chosen_lens = torch.tensor([50.0, 60.0, 55.0])\n",
    "rejected_lens = torch.tensor([20.0, 25.0, 22.0])\n",
    "\n",
    "loss = simpo_loss(chosen_logps, rejected_logps, chosen_lens, rejected_lens)\n",
    "print(f\"Example SimPO loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: When to Use Which Method\n",
    "\n",
    "### Decision Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════════════╗\n",
    "║                    CHOOSING YOUR PREFERENCE METHOD                            ║\n",
    "╠══════════════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                              ║\n",
    "║  Use DPO when:                                                               ║\n",
    "║    ✓ You have plenty of memory (can load 2x model)                           ║\n",
    "║    ✓ You want a well-tested, proven method                                   ║\n",
    "║    ✓ You already have an SFT model as starting point                         ║\n",
    "║                                                                              ║\n",
    "║  Use SimPO when:                                                             ║\n",
    "║    ✓ You want BEST quality (+6.4 on AlpacaEval)                              ║\n",
    "║    ✓ You want simpler training (no reference model)                          ║\n",
    "║    ✓ Your responses vary significantly in length                             ║\n",
    "║                                                                              ║\n",
    "║  Use ORPO when:                                                              ║\n",
    "║    ✓ Memory is constrained (50% less than DPO)                               ║\n",
    "║    ✓ You want single-stage training (no separate SFT)                        ║\n",
    "║    ✓ You're on DGX Spark with 70B+ models                                    ║\n",
    "║                                                                              ║\n",
    "║  Use KTO when:                                                               ║\n",
    "║    ✓ You only have binary feedback (thumbs up/down)                          ║\n",
    "║    ✓ You don't have preference pairs                                         ║\n",
    "║    ✓ Human-aligned loss function is important                                ║\n",
    "║                                                                              ║\n",
    "╠══════════════════════════════════════════════════════════════════════════════╣\n",
    "║  DGX Spark Recommendations:                                                  ║\n",
    "║    • 8B models: Any method works well                                        ║\n",
    "║    • 70B models: Use ORPO for memory efficiency                              ║\n",
    "║    • Maximum quality: Use SimPO                                              ║\n",
    "╚══════════════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Memory Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_memory(model_size_gb: float, method: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Estimate memory requirements for different preference methods.\n",
    "    \"\"\"\n",
    "    estimates = {\n",
    "        \"dpo\": {\n",
    "            \"policy_model\": model_size_gb,\n",
    "            \"reference_model\": model_size_gb,  # DPO needs reference!\n",
    "            \"optimizer_states\": model_size_gb * 0.1,  # LoRA params only\n",
    "            \"gradients\": model_size_gb * 0.1,\n",
    "        },\n",
    "        \"simpo\": {\n",
    "            \"policy_model\": model_size_gb,\n",
    "            \"reference_model\": 0,  # No reference needed!\n",
    "            \"optimizer_states\": model_size_gb * 0.1,\n",
    "            \"gradients\": model_size_gb * 0.1,\n",
    "        },\n",
    "        \"orpo\": {\n",
    "            \"policy_model\": model_size_gb,\n",
    "            \"reference_model\": 0,  # No reference needed!\n",
    "            \"optimizer_states\": model_size_gb * 0.1,\n",
    "            \"gradients\": model_size_gb * 0.1,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    result = estimates[method]\n",
    "    result[\"total\"] = sum(result.values())\n",
    "    return result\n",
    "\n",
    "# Compare for 70B model (in 4-bit = ~35GB)\n",
    "model_size = 35.0\n",
    "\n",
    "print(\"Memory Comparison for 70B Model (4-bit):\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for method in [\"dpo\", \"simpo\", \"orpo\"]:\n",
    "    mem = estimate_memory(model_size, method)\n",
    "    print(f\"\\n{method.upper()}:\")\n",
    "    print(f\"  Policy model:    {mem['policy_model']:.1f} GB\")\n",
    "    print(f\"  Reference model: {mem['reference_model']:.1f} GB\")\n",
    "    print(f\"  Optimizer:       {mem['optimizer_states']:.1f} GB\")\n",
    "    print(f\"  Gradients:       {mem['gradients']:.1f} GB\")\n",
    "    print(f\"  ─────────────────────\")\n",
    "    print(f\"  TOTAL:           {mem['total']:.1f} GB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"\\nDGX Spark capacity: 128 GB\")\n",
    "print(f\"DPO:   {'✅ Fits' if estimate_memory(model_size, 'dpo')['total'] < 128 else '❌ Too big'}\")\n",
    "print(f\"SimPO: {'✅ Fits' if estimate_memory(model_size, 'simpo')['total'] < 128 else '❌ Too big'}\")\n",
    "print(f\"ORPO:  {'✅ Fits' if estimate_memory(model_size, 'orpo')['total'] < 128 else '❌ Too big'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ✅ SimPO eliminates reference model and adds length normalization\n",
    "- ✅ ORPO combines SFT + preference in one stage\n",
    "- ✅ When to use each method\n",
    "- ✅ Memory tradeoffs for 70B models\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [SimPO Paper](https://arxiv.org/abs/2405.14734) - Simple Preference Optimization\n",
    "- [ORPO Paper](https://arxiv.org/abs/2403.07691) - Odds Ratio Preference Optimization\n",
    "- [TRL Documentation](https://huggingface.co/docs/trl)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory\n",
    "del model, orpo_trainer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to:\n",
    "\n",
    "**[Lab 3.1.9: KTO Binary Feedback](lab-3.1.9-kto-binary-feedback.ipynb)** - Learn to train with just thumbs up/down data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
