{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1.4: 8B Model LoRA Fine-Tuning\n",
    "\n",
    "**Module:** 3.1 - Large Language Model Fine-Tuning  \n",
    "**Time:** 3 hours  \n",
    "**Difficulty:** â­â­â­â˜†â˜†\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Fine-tune a real 8B parameter LLM on your DGX Spark\n",
    "- [ ] Combine LoRA/DoRA with NEFTune for optimal quality\n",
    "- [ ] Use QLoRA (4-bit quantization) for efficient training\n",
    "- [ ] Evaluate your fine-tuned model\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Labs 3.1.1-3.1.3 (LoRA Theory, DoRA, NEFTune)\n",
    "- Model access: Request access to Llama 3.1 8B on HuggingFace (or use Mistral 7B)\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "### Your First Production Fine-Tune!\n",
    "\n",
    "This is where everything comes together. You're going to:\n",
    "1. Load a real 8B parameter model with 4-bit quantization\n",
    "2. Apply DoRA adapters to attention layers\n",
    "3. Enable NEFTune for better generalization\n",
    "4. Fine-tune on a real instruction dataset\n",
    "5. Evaluate and save your custom model\n",
    "\n",
    "**On DGX Spark with 128GB unified memory:**\n",
    "- 8B model in 4-bit: ~5-6GB\n",
    "- LoRA adapters: ~100MB\n",
    "- Training state: ~2-3GB\n",
    "- **Plenty of room for larger batch sizes!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELI5: What Are We Building?\n",
    "\n",
    "> **Imagine you hired a brilliant new assistant (Llama 8B) who knows everything but needs to learn YOUR way of doing things.**\n",
    ">\n",
    "> Instead of sending them back to school for 4 years (full fine-tuning), you:\n",
    "> 1. Give them a small notebook with YOUR preferences (LoRA/DoRA)\n",
    "> 2. Have them practice with slight variations to build intuition (NEFTune)\n",
    "> 3. Let them use a compressed version of their brain to save space (4-bit quantization)\n",
    ">\n",
    "> After a few hours of practice, they'll respond exactly how YOU want, while keeping all their original knowledge!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup\n",
    "\n",
    "First, let's make sure all dependencies are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in NGC container (recommended)\n",
    "import subprocess\n",
    "result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "print(result.stdout[:500] if result.returncode == 0 else \"GPU not detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if not in NGC container)\n",
    "# In NGC container, most are pre-installed\n",
    "# !pip install -q transformers>=4.40.0 peft>=0.10.0 trl>=0.8.0 datasets bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from typing import Dict, List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"Total GPU Memory: {total_memory:.1f} GB\")\n",
    "    \n",
    "    # DGX Spark detection\n",
    "    if total_memory > 100:\n",
    "        print(\"\\nðŸš€ DGX Spark detected! You have 128GB unified memory.\")\n",
    "        print(\"   This means PLENTY of headroom for 8B models!\")\n",
    "\n",
    "# Clear any existing GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Model Configuration\n",
    "\n",
    "### Choose Your Model\n",
    "\n",
    "We support two options:\n",
    "\n",
    "| Model | Size | Access | Notes |\n",
    "|-------|------|--------|-------|\n",
    "| `meta-llama/Llama-3.1-8B-Instruct` | 8B | Requires approval | Best quality |\n",
    "| `mistralai/Mistral-7B-Instruct-v0.2` | 7B | Open | Great alternative |\n",
    "| `TinyLlama/TinyLlama-1.1B-Chat-v1.0` | 1.1B | Open | For quick testing |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘                    CONFIGURATION                                â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Choose your model (uncomment one)\n",
    "# MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"  # Requires approval\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # For quick testing\n",
    "# MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"  # Open alternative\n",
    "\n",
    "# Training configuration\n",
    "OUTPUT_DIR = \"./finetuned-model\"\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "BATCH_SIZE = 4  # DGX Spark can handle more!\n",
    "GRADIENT_ACCUMULATION = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCHS = 1  # Increase for production\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "# LoRA/DoRA configuration\n",
    "LORA_RANK = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "USE_DORA = True  # Enable DoRA for +3.7 points!\n",
    "\n",
    "# NEFTune configuration\n",
    "NEFTUNE_ALPHA = 5.0  # 5-15 recommended\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  LoRA rank: {LORA_RANK} (alpha={LORA_ALPHA})\")\n",
    "print(f\"  DoRA: {'Enabled' if USE_DORA else 'Disabled'}\")\n",
    "print(f\"  NEFTune: Î±={NEFTUNE_ALPHA}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE} Ã— {GRADIENT_ACCUMULATION} = {BATCH_SIZE * GRADIENT_ACCUMULATION} effective\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Load Model with 4-bit Quantization\n",
    "\n",
    "We'll use QLoRA's NF4 quantization to reduce memory usage by 4x while maintaining quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    \"\"\"Get current GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        return f\"Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\"\n",
    "    return \"GPU not available\"\n",
    "\n",
    "print(f\"Before loading: {get_memory_usage()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA Configuration: 4-bit quantization with NF4\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                     # Enable 4-bit loading\n",
    "    bnb_4bit_quant_type=\"nf4\",             # NormalFloat4 - best for LLMs\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Compute in bfloat16 (DGX Spark native)\n",
    "    bnb_4bit_use_double_quant=True,        # Quantize the quantization constants too\n",
    ")\n",
    "\n",
    "print(\"4-bit quantization config created\")\n",
    "print(f\"  Quant type: NF4 (NormalFloat 4-bit)\")\n",
    "print(f\"  Compute dtype: bfloat16\")\n",
    "print(f\"  Double quantization: Enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(f\"Loading tokenizer from {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# Set padding token if not present (common with Llama models)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Tokenizer loaded:\")\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"  Pad token: {tokenizer.pad_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with quantization\n",
    "print(f\"\\nLoading model {MODEL_NAME} with 4-bit quantization...\")\n",
    "print(\"This may take a few minutes for larger models...\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"sdpa\",  # Use PyTorch's SDPA for efficiency\n",
    ")\n",
    "\n",
    "print(f\"\\nModel loaded!\")\n",
    "print(f\"  {get_memory_usage()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(\n",
    "    model,\n",
    "    use_gradient_checkpointing=True,  # Trade compute for memory\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel prepared for training:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable (before LoRA): {trainable_params:,}\")\n",
    "print(f\"  Gradient checkpointing: Enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Add LoRA/DoRA Adapters\n",
    "\n",
    "Now we'll add our efficient adapters to the attention layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find target modules (model-specific)\n",
    "# Print some layer names to identify the pattern\n",
    "print(\"Looking for attention layers...\")\n",
    "for name, _ in model.named_modules():\n",
    "    if 'proj' in name.lower() or 'attention' in name.lower():\n",
    "        print(f\"  {name}\")\n",
    "        if len([n for n, _ in model.named_modules() if 'proj' in n.lower()]) > 10:\n",
    "            print(\"  ...\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA/DoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",      # MLP (optional but helps)\n",
    "    ],\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    use_dora=USE_DORA,  # Enable DoRA!\n",
    ")\n",
    "\n",
    "print(f\"{'DoRA' if USE_DORA else 'LoRA'} Configuration:\")\n",
    "print(f\"  Rank: {LORA_RANK}\")\n",
    "print(f\"  Alpha: {LORA_ALPHA} (scaling = {LORA_ALPHA/LORA_RANK})\")\n",
    "print(f\"  Dropout: {LORA_DROPOUT}\")\n",
    "print(f\"  Target modules: {lora_config.target_modules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA/DoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(f\"\\n{get_memory_usage()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "We've added small adapter matrices to each attention and MLP projection layer. The model now has:\n",
    "- **Frozen base weights** (~1.1B for TinyLlama, ~8B for Llama)\n",
    "- **Trainable LoRA/DoRA adapters** (~0.5-1% of total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Prepare Dataset\n",
    "\n",
    "We'll use a sample instruction-following dataset. For production, use your own data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample instruction dataset\n",
    "# Using a small subset for demonstration\n",
    "print(\"Loading dataset...\")\n",
    "\n",
    "# Using Alpaca-style dataset\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:1000]\")\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset)} examples\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Instruction: {dataset[0]['instruction'][:100]}...\")\n",
    "print(f\"  Output: {dataset[0]['output'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data for training\n",
    "def format_instruction(example):\n",
    "    \"\"\"\n",
    "    Format Alpaca-style data into a prompt.\n",
    "    \n",
    "    For Llama 3.1, we use the official chat template.\n",
    "    For other models, we use a simple instruction format.\n",
    "    \"\"\"\n",
    "    instruction = example['instruction']\n",
    "    input_text = example.get('input', '')\n",
    "    output = example['output']\n",
    "    \n",
    "    if 'llama' in MODEL_NAME.lower() and '3' in MODEL_NAME:\n",
    "        # Llama 3 format\n",
    "        if input_text:\n",
    "            prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{instruction}\n",
    "\n",
    "Input: {input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{output}<|eot_id|>\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{output}<|eot_id|>\"\"\"\n",
    "    else:\n",
    "        # Generic instruction format\n",
    "        if input_text:\n",
    "            prompt = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "    \n",
    "    return {\"text\": prompt}\n",
    "\n",
    "\n",
    "# Apply formatting\n",
    "formatted_dataset = dataset.map(format_instruction)\n",
    "\n",
    "print(\"Formatted example:\")\n",
    "print(\"=\"*50)\n",
    "print(formatted_dataset[0]['text'][:500])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Training with NEFTune\n",
    "\n",
    "Now we'll set up training with all our optimizations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # Batch size\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    \n",
    "    # Training schedule\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    \n",
    "    # Optimization\n",
    "    optim=\"paged_adamw_8bit\",  # Memory-efficient optimizer\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    \n",
    "    # Precision\n",
    "    bf16=True,  # Use bfloat16 (DGX Spark native)\n",
    "    \n",
    "    # Sequence length\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    packing=True,  # Pack multiple examples into one sequence\n",
    "    \n",
    "    # NEFTune! The magic ingredient\n",
    "    neftune_noise_alpha=NEFTUNE_ALPHA,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    \n",
    "    # Saving\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Other\n",
    "    report_to=\"none\",  # Disable W&B for this demo\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  NEFTune noise: Î±={NEFTUNE_ALPHA}\")\n",
    "print(f\"  Precision: bfloat16\")\n",
    "print(f\"  Max sequence length: {MAX_SEQ_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=formatted_dataset,\n",
    "    args=training_args,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "print(\"Trainer created!\")\n",
    "print(f\"\\n{get_memory_usage()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"=\"*60)\n",
    "print(\"Starting training...\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Method: {'DoRA' if USE_DORA else 'LoRA'} + NEFTune\")\n",
    "print(f\"  Dataset: {len(formatted_dataset)} examples\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print training metrics\n",
    "print(\"\\nTraining Metrics:\")\n",
    "for key, value in train_result.metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\n{get_memory_usage()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Test Your Fine-Tuned Model\n",
    "\n",
    "Let's see how well your model learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation\n",
    "def generate_response(prompt: str, max_new_tokens: int = 256) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response from the fine-tuned model.\n",
    "    \"\"\"\n",
    "    # Format prompt\n",
    "    if 'llama' in MODEL_NAME.lower() and '3' in MODEL_NAME:\n",
    "        formatted = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    else:\n",
    "        formatted = f\"\"\"### Instruction:\n",
    "{prompt}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the response part\n",
    "    if \"### Response:\" in response:\n",
    "        response = response.split(\"### Response:\")[-1].strip()\n",
    "    elif \"assistant\" in response.lower():\n",
    "        parts = response.split(\"assistant\")\n",
    "        response = parts[-1].strip() if len(parts) > 1 else response\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with some prompts\n",
    "test_prompts = [\n",
    "    \"Explain what machine learning is in simple terms.\",\n",
    "    \"Write a short poem about artificial intelligence.\",\n",
    "    \"What are three benefits of exercise?\",\n",
    "]\n",
    "\n",
    "print(\"Testing fine-tuned model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"-\"*40)\n",
    "    response = generate_response(prompt)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Save Your Model\n",
    "\n",
    "Save the LoRA adapters for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters\n",
    "adapter_path = os.path.join(OUTPUT_DIR, \"final_adapter\")\n",
    "model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "\n",
    "print(f\"Adapter saved to: {adapter_path}\")\n",
    "\n",
    "# Show saved files\n",
    "import os\n",
    "print(\"\\nSaved files:\")\n",
    "for f in os.listdir(adapter_path):\n",
    "    size = os.path.getsize(os.path.join(adapter_path, f)) / 1e6\n",
    "    print(f\"  {f}: {size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to load later:\n",
    "load_example = f\"\"\"\n",
    "# To load your fine-tuned model later:\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"{MODEL_NAME}\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{adapter_path}\")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"{adapter_path}\")\n",
    "\n",
    "# Ready to use!\n",
    "\"\"\"\n",
    "\n",
    "print(\"How to load your model later:\")\n",
    "print(load_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Merge Weights (Optional)\n",
    "\n",
    "For deployment, you can merge LoRA weights into the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Merge LoRA weights into base model\n",
    "# This removes the adapter overhead during inference\n",
    "\n",
    "# Uncomment to merge:\n",
    "# merged_model = model.merge_and_unload()\n",
    "# merged_path = os.path.join(OUTPUT_DIR, \"merged_model\")\n",
    "# merged_model.save_pretrained(merged_path)\n",
    "# tokenizer.save_pretrained(merged_path)\n",
    "# print(f\"Merged model saved to: {merged_path}\")\n",
    "\n",
    "print(\"To merge weights, uncomment the code above.\")\n",
    "print(\"Merging creates a standalone model without adapter overhead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "\n",
    "1. âœ… Loaded an 8B model with 4-bit quantization (QLoRA)\n",
    "2. âœ… Added DoRA adapters (0.5-1% trainable params)\n",
    "3. âœ… Enabled NEFTune for better generalization\n",
    "4. âœ… Fine-tuned on instruction data\n",
    "5. âœ… Tested your custom model\n",
    "6. âœ… Saved adapters for later use\n",
    "\n",
    "### Memory Usage on DGX Spark\n",
    "\n",
    "| Component | Memory |\n",
    "|-----------|--------|\n",
    "| Base model (4-bit) | ~5-6GB |\n",
    "| LoRA adapters | ~100MB |\n",
    "| Training state | ~2-3GB |\n",
    "| **Total** | **~8-10GB** |\n",
    "| **Remaining** | **~118GB** |\n",
    "\n",
    "**You have LOTS of headroom for larger batch sizes or bigger models!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting to Clear Cache Before Large Models\n",
    "\n",
    "```python\n",
    "# Wrong: Load new model without clearing\n",
    "model = AutoModelForCausalLM.from_pretrained(...)  # OOM!\n",
    "\n",
    "# Right: Clear cache first\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "model = AutoModelForCausalLM.from_pretrained(...)\n",
    "```\n",
    "\n",
    "### Mistake 2: Wrong Padding Side\n",
    "\n",
    "```python\n",
    "# Wrong: Default padding (varies by model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Right: Set padding side explicitly for causal LM\n",
    "tokenizer.padding_side = \"right\"  # For training\n",
    "# tokenizer.padding_side = \"left\"  # For generation\n",
    "```\n",
    "\n",
    "### Mistake 3: Not Using Gradient Checkpointing for Large Models\n",
    "\n",
    "```python\n",
    "# Wrong: No gradient checkpointing\n",
    "model = get_peft_model(model, lora_config)  # May OOM on 70B\n",
    "\n",
    "# Right: Enable gradient checkpointing\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "model = get_peft_model(model, lora_config)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "del model, trainer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"Cleanup complete!\")\n",
    "print(f\"{get_memory_usage()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Ready for the big challenge? Continue to:\n",
    "\n",
    "**[Lab 3.1.5: 70B Model QLoRA Fine-Tuning](lab-3.1.5-70b-qlora-finetuning.ipynb)** - The DGX Spark showcase! Fine-tune a 70B model on your desktop!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
