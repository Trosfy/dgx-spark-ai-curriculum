{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1.4: Dataset Preparation for LLM Fine-Tuning\n",
    "\n",
    "**Module:** 3.1 - Large Language Model Fine-Tuning  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐⭐☆☆\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Convert raw data to Alpaca instruction format\n",
    "- [ ] Implement ChatML and Llama chat templates\n",
    "- [ ] Design effective system prompts\n",
    "- [ ] Implement data cleaning and quality filtering\n",
    "- [ ] Create proper train/validation/test splits\n",
    "- [ ] Handle multi-turn conversations\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic Python data manipulation\n",
    "- Understanding of JSON format\n",
    "- Familiarity with pandas (helpful but not required)\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "### The Data Quality Principle\n",
    "\n",
    "**\"Garbage in, garbage out\"** is especially true for LLM fine-tuning. The quality of your dataset directly determines:\n",
    "- How well your model follows instructions\n",
    "- The quality and style of responses\n",
    "- Whether it learns your domain knowledge correctly\n",
    "\n",
    "**Companies like OpenAI, Anthropic, and Google** spend enormous resources on data curation. Some estimates suggest data preparation takes 70-80% of the effort in successful fine-tuning projects.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELI5: What is Dataset Preparation?\n",
    "\n",
    "> **Imagine you're teaching a new employee.** You wouldn't just dump random documents on their desk and expect them to learn your job.\n",
    ">\n",
    "> Instead, you'd:\n",
    "> 1. **Organize** the training materials in a logical order\n",
    "> 2. **Format** them consistently so they're easy to follow\n",
    "> 3. **Remove** outdated or incorrect information\n",
    "> 4. **Add** examples of exactly how you want tasks done\n",
    "> 5. **Test** them periodically to ensure they're learning\n",
    ">\n",
    "> **Dataset preparation is exactly that** for your LLM - organizing and formatting examples so the model learns exactly what you want it to do.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Dataset Formats\n",
    "\n",
    "There are several common formats for instruction-tuning datasets. Let's explore each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nimport json\nimport re\nimport random\nfrom typing import Dict, List, Optional, Union, Tuple\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nimport hashlib\n\n# Set random seed for reproducibility\nrandom.seed(42)\n\n# NOTE: Reusable versions of all utilities in this notebook are available in:\n# - scripts/dataset_utils.py (DataCleaner, ChatTemplateFormatter, DatasetConverter, etc.)\n# You can import them with:\n#   import sys; sys.path.insert(0, '..')\n#   from scripts.dataset_utils import DataCleaner, ChatTemplateFormatter\n# For learning purposes, we implement them from scratch below."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Alpaca Format\n",
    "\n",
    "The **Alpaca format** is one of the most common formats for instruction-tuning. It was introduced by Stanford's Alpaca project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpaca format example\n",
    "alpaca_example = {\n",
    "    \"instruction\": \"Write a function to calculate the factorial of a number.\",\n",
    "    \"input\": \"\",  # Optional: additional context\n",
    "    \"output\": \"\"\"def factorial(n):\n",
    "    if n < 0:\n",
    "        raise ValueError(\"Factorial is not defined for negative numbers\")\n",
    "    if n <= 1:\n",
    "        return 1\n",
    "    return n * factorial(n - 1)\"\"\"\n",
    "}\n",
    "\n",
    "alpaca_with_input = {\n",
    "    \"instruction\": \"Summarize the following text in one sentence.\",\n",
    "    \"input\": \"\"\"The transformer architecture was introduced in 2017 in the paper \n",
    "    'Attention is All You Need'. It revolutionized natural language processing \n",
    "    by enabling parallel processing of sequences and capturing long-range \n",
    "    dependencies through self-attention mechanisms.\"\"\",\n",
    "    \"output\": \"The transformer architecture, introduced in 2017, transformed NLP by enabling parallel sequence processing and capturing long-range dependencies through self-attention.\"\n",
    "}\n",
    "\n",
    "print(\"Alpaca Format (without input):\")\n",
    "print(json.dumps(alpaca_example, indent=2))\n",
    "print(\"\\nAlpaca Format (with input):\")\n",
    "print(json.dumps(alpaca_with_input, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 ShareGPT/Conversation Format\n",
    "\n",
    "The **ShareGPT format** supports multi-turn conversations, which is essential for chat models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ShareGPT format example\n",
    "sharegpt_example = {\n",
    "    \"conversations\": [\n",
    "        {\"from\": \"system\", \"value\": \"You are a helpful coding assistant.\"},\n",
    "        {\"from\": \"human\", \"value\": \"How do I read a file in Python?\"},\n",
    "        {\"from\": \"gpt\", \"value\": \"\"\"You can read a file in Python using the `open()` function:\n",
    "\n",
    "```python\n",
    "# Method 1: Using with statement (recommended)\n",
    "with open('file.txt', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Method 2: Read line by line\n",
    "with open('file.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        print(line)\n",
    "```\n",
    "\n",
    "The `with` statement ensures the file is properly closed after reading.\"\"\"},\n",
    "        {\"from\": \"human\", \"value\": \"What about reading a JSON file?\"},\n",
    "        {\"from\": \"gpt\", \"value\": \"\"\"For JSON files, use the `json` module:\n",
    "\n",
    "```python\n",
    "import json\n",
    "\n",
    "with open('data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# data is now a Python dictionary or list\n",
    "print(data)\n",
    "```\n",
    "\n",
    "This automatically parses the JSON into Python data structures.\"\"\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"ShareGPT/Conversation Format:\")\n",
    "print(json.dumps(sharegpt_example, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Chat Template Formats\n",
    "\n",
    "Different models use different chat templates. Let's implement the most common ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ChatMessage:\n",
    "    \"\"\"Represents a single message in a conversation.\"\"\"\n",
    "    role: str  # 'system', 'user', or 'assistant'\n",
    "    content: str\n",
    "\n",
    "@dataclass\n",
    "class Conversation:\n",
    "    \"\"\"Represents a full conversation.\"\"\"\n",
    "    messages: List[ChatMessage] = field(default_factory=list)\n",
    "    \n",
    "    def add_message(self, role: str, content: str):\n",
    "        self.messages.append(ChatMessage(role=role, content=content))\n",
    "\n",
    "\n",
    "class ChatTemplateFormatter:\n",
    "    \"\"\"Format conversations for different model chat templates.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_chatml(conversation: Conversation) -> str:\n",
    "        \"\"\"\n",
    "        Format conversation in ChatML format.\n",
    "        Used by: OpenAI models, some open-source models\n",
    "        \"\"\"\n",
    "        formatted = \"\"\n",
    "        for msg in conversation.messages:\n",
    "            formatted += f\"<|im_start|>{msg.role}\\n{msg.content}<|im_end|>\\n\"\n",
    "        return formatted.strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_llama3(conversation: Conversation) -> str:\n",
    "        \"\"\"\n",
    "        Format conversation in Llama 3.1 format.\n",
    "        Used by: Llama 3, Llama 3.1 models\n",
    "        \"\"\"\n",
    "        formatted = \"<|begin_of_text|>\"\n",
    "        for msg in conversation.messages:\n",
    "            formatted += f\"<|start_header_id|>{msg.role}<|end_header_id|>\\n\\n\"\n",
    "            formatted += f\"{msg.content}<|eot_id|>\"\n",
    "        return formatted\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_llama2(conversation: Conversation) -> str:\n",
    "        \"\"\"\n",
    "        Format conversation in Llama 2 format.\n",
    "        Used by: Llama 2 models\n",
    "        \"\"\"\n",
    "        formatted = \"<s>\"\n",
    "        system_msg = None\n",
    "        \n",
    "        for msg in conversation.messages:\n",
    "            if msg.role == \"system\":\n",
    "                system_msg = msg.content\n",
    "            elif msg.role == \"user\":\n",
    "                if system_msg:\n",
    "                    formatted += f\"[INST] <<SYS>>\\n{system_msg}\\n<</SYS>>\\n\\n\"\n",
    "                    system_msg = None\n",
    "                else:\n",
    "                    formatted += \"[INST] \"\n",
    "                formatted += f\"{msg.content} [/INST]\"\n",
    "            elif msg.role == \"assistant\":\n",
    "                formatted += f\" {msg.content} </s>\"\n",
    "        \n",
    "        return formatted\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_mistral(conversation: Conversation) -> str:\n",
    "        \"\"\"\n",
    "        Format conversation in Mistral/Mixtral format.\n",
    "        Used by: Mistral, Mixtral models\n",
    "        \"\"\"\n",
    "        formatted = \"<s>\"\n",
    "        for msg in conversation.messages:\n",
    "            if msg.role == \"user\":\n",
    "                formatted += f\"[INST] {msg.content} [/INST]\"\n",
    "            elif msg.role == \"assistant\":\n",
    "                formatted += f\" {msg.content}</s>\"\n",
    "        return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate chat templates\n",
    "conv = Conversation()\n",
    "conv.add_message(\"system\", \"You are a helpful AI assistant.\")\n",
    "conv.add_message(\"user\", \"What is machine learning?\")\n",
    "conv.add_message(\"assistant\", \"Machine learning is a branch of AI that enables computers to learn patterns from data.\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ChatML Format:\")\n",
    "print(\"=\"*60)\n",
    "print(ChatTemplateFormatter.to_chatml(conv))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Llama 3.1 Format:\")\n",
    "print(\"=\"*60)\n",
    "print(ChatTemplateFormatter.to_llama3(conv))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Llama 2 Format:\")\n",
    "print(\"=\"*60)\n",
    "print(ChatTemplateFormatter.to_llama2(conv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Data Conversion Utilities\n",
    "\n",
    "Let's create utilities to convert between different formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetConverter:\n",
    "    \"\"\"Convert between different dataset formats.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def alpaca_to_conversation(\n",
    "        alpaca_data: Dict,\n",
    "        system_prompt: str = \"You are a helpful assistant.\"\n",
    "    ) -> Conversation:\n",
    "        \"\"\"\n",
    "        Convert Alpaca format to Conversation format.\n",
    "        \n",
    "        Args:\n",
    "            alpaca_data: Dict with 'instruction', 'input' (optional), 'output'\n",
    "            system_prompt: System message to prepend\n",
    "        \n",
    "        Returns:\n",
    "            Conversation object\n",
    "        \"\"\"\n",
    "        conv = Conversation()\n",
    "        conv.add_message(\"system\", system_prompt)\n",
    "        \n",
    "        # Combine instruction and input\n",
    "        user_message = alpaca_data[\"instruction\"]\n",
    "        if alpaca_data.get(\"input\", \"\").strip():\n",
    "            user_message += f\"\\n\\n{alpaca_data['input']}\"\n",
    "        \n",
    "        conv.add_message(\"user\", user_message)\n",
    "        conv.add_message(\"assistant\", alpaca_data[\"output\"])\n",
    "        \n",
    "        return conv\n",
    "    \n",
    "    @staticmethod\n",
    "    def sharegpt_to_conversation(sharegpt_data: Dict) -> Conversation:\n",
    "        \"\"\"\n",
    "        Convert ShareGPT format to Conversation format.\n",
    "        \n",
    "        Args:\n",
    "            sharegpt_data: Dict with 'conversations' list\n",
    "        \n",
    "        Returns:\n",
    "            Conversation object\n",
    "        \"\"\"\n",
    "        conv = Conversation()\n",
    "        \n",
    "        role_mapping = {\n",
    "            \"system\": \"system\",\n",
    "            \"human\": \"user\",\n",
    "            \"user\": \"user\",\n",
    "            \"gpt\": \"assistant\",\n",
    "            \"assistant\": \"assistant\",\n",
    "        }\n",
    "        \n",
    "        for turn in sharegpt_data[\"conversations\"]:\n",
    "            role = role_mapping.get(turn[\"from\"], turn[\"from\"])\n",
    "            conv.add_message(role, turn[\"value\"])\n",
    "        \n",
    "        return conv\n",
    "    \n",
    "    @staticmethod\n",
    "    def conversation_to_alpaca(conv: Conversation) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Convert Conversation to Alpaca format.\n",
    "        Multi-turn conversations become multiple examples.\n",
    "        \n",
    "        Returns:\n",
    "            List of Alpaca-format dicts\n",
    "        \"\"\"\n",
    "        examples = []\n",
    "        system_msg = \"\"\n",
    "        current_instruction = \"\"\n",
    "        \n",
    "        for msg in conv.messages:\n",
    "            if msg.role == \"system\":\n",
    "                system_msg = msg.content\n",
    "            elif msg.role == \"user\":\n",
    "                current_instruction = msg.content\n",
    "            elif msg.role == \"assistant\":\n",
    "                examples.append({\n",
    "                    \"instruction\": current_instruction,\n",
    "                    \"input\": f\"System: {system_msg}\" if system_msg else \"\",\n",
    "                    \"output\": msg.content\n",
    "                })\n",
    "        \n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test conversion\n",
    "converted_conv = DatasetConverter.alpaca_to_conversation(\n",
    "    alpaca_example,\n",
    "    system_prompt=\"You are an expert Python programmer.\"\n",
    ")\n",
    "\n",
    "print(\"Alpaca → Conversation → Llama 3.1 Format:\")\n",
    "print(\"=\"*60)\n",
    "print(ChatTemplateFormatter.to_llama3(converted_conv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Data Cleaning and Quality Filtering\n",
    "\n",
    "Clean data is essential for good fine-tuning results. Let's implement comprehensive cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    \"\"\"Clean and filter training data for quality.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        min_instruction_length: int = 10,\n",
    "        max_instruction_length: int = 1000,\n",
    "        min_output_length: int = 20,\n",
    "        max_output_length: int = 4000,\n",
    "        remove_duplicates: bool = True,\n",
    "    ):\n",
    "        self.min_instruction_length = min_instruction_length\n",
    "        self.max_instruction_length = max_instruction_length\n",
    "        self.min_output_length = min_output_length\n",
    "        self.max_output_length = max_output_length\n",
    "        self.remove_duplicates = remove_duplicates\n",
    "        self.seen_hashes = set()\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean a single text string.\"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        \n",
    "        # Remove leading/trailing whitespace\n",
    "        text = text.strip()\n",
    "        \n",
    "        # Fix common issues\n",
    "        text = text.replace('\\r\\n', '\\n')\n",
    "        text = text.replace('\\r', '\\n')\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def is_valid_example(self, example: Dict) -> Tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Check if an example meets quality criteria.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (is_valid, reason if invalid)\n",
    "        \"\"\"\n",
    "        instruction = example.get(\"instruction\", \"\")\n",
    "        output = example.get(\"output\", \"\")\n",
    "        \n",
    "        # Check instruction length\n",
    "        if len(instruction) < self.min_instruction_length:\n",
    "            return False, f\"Instruction too short ({len(instruction)} chars)\"\n",
    "        if len(instruction) > self.max_instruction_length:\n",
    "            return False, f\"Instruction too long ({len(instruction)} chars)\"\n",
    "        \n",
    "        # Check output length\n",
    "        if len(output) < self.min_output_length:\n",
    "            return False, f\"Output too short ({len(output)} chars)\"\n",
    "        if len(output) > self.max_output_length:\n",
    "            return False, f\"Output too long ({len(output)} chars)\"\n",
    "        \n",
    "        # Check for duplicate\n",
    "        if self.remove_duplicates:\n",
    "            content_hash = hashlib.md5(\n",
    "                (instruction + output).encode()\n",
    "            ).hexdigest()\n",
    "            if content_hash in self.seen_hashes:\n",
    "                return False, \"Duplicate example\"\n",
    "            self.seen_hashes.add(content_hash)\n",
    "        \n",
    "        # Check for empty or placeholder content\n",
    "        placeholder_patterns = [\n",
    "            r'^TODO',\n",
    "            r'^TBD',\n",
    "            r'^\\[.*\\]$',\n",
    "            r'^N/A$',\n",
    "        ]\n",
    "        for pattern in placeholder_patterns:\n",
    "            if re.match(pattern, output, re.IGNORECASE):\n",
    "                return False, \"Placeholder content detected\"\n",
    "        \n",
    "        return True, \"\"\n",
    "    \n",
    "    def clean_example(self, example: Dict) -> Dict:\n",
    "        \"\"\"Clean a single example.\"\"\"\n",
    "        return {\n",
    "            \"instruction\": self.clean_text(example.get(\"instruction\", \"\")),\n",
    "            \"input\": self.clean_text(example.get(\"input\", \"\")),\n",
    "            \"output\": self.clean_text(example.get(\"output\", \"\")),\n",
    "        }\n",
    "    \n",
    "    def process_dataset(\n",
    "        self,\n",
    "        data: List[Dict],\n",
    "        verbose: bool = True\n",
    "    ) -> Tuple[List[Dict], Dict]:\n",
    "        \"\"\"\n",
    "        Process and clean a full dataset.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (cleaned_data, statistics)\n",
    "        \"\"\"\n",
    "        cleaned = []\n",
    "        stats = {\n",
    "            \"total\": len(data),\n",
    "            \"passed\": 0,\n",
    "            \"failed\": 0,\n",
    "            \"failure_reasons\": {}\n",
    "        }\n",
    "        \n",
    "        for example in data:\n",
    "            cleaned_example = self.clean_example(example)\n",
    "            is_valid, reason = self.is_valid_example(cleaned_example)\n",
    "            \n",
    "            if is_valid:\n",
    "                cleaned.append(cleaned_example)\n",
    "                stats[\"passed\"] += 1\n",
    "            else:\n",
    "                stats[\"failed\"] += 1\n",
    "                stats[\"failure_reasons\"][reason] = stats[\"failure_reasons\"].get(reason, 0) + 1\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Dataset Processing Results:\")\n",
    "            print(f\"  Total examples: {stats['total']}\")\n",
    "            print(f\"  Passed: {stats['passed']} ({100*stats['passed']/stats['total']:.1f}%)\")\n",
    "            print(f\"  Failed: {stats['failed']} ({100*stats['failed']/stats['total']:.1f}%)\")\n",
    "            if stats[\"failure_reasons\"]:\n",
    "                print(f\"  Failure reasons:\")\n",
    "                for reason, count in stats[\"failure_reasons\"].items():\n",
    "                    print(f\"    - {reason}: {count}\")\n",
    "        \n",
    "        return cleaned, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample data including some bad examples\n",
    "test_data = [\n",
    "    # Good example\n",
    "    {\n",
    "        \"instruction\": \"Explain the concept of overfitting in machine learning.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"\"\"Overfitting occurs when a machine learning model learns the training data too well, \n",
    "        including its noise and random fluctuations. The model becomes very accurate on training data \n",
    "        but performs poorly on new, unseen data. Common signs include low training error but high \n",
    "        validation error. Solutions include regularization, cross-validation, and increasing training data.\"\"\"\n",
    "    },\n",
    "    # Too short instruction\n",
    "    {\n",
    "        \"instruction\": \"Explain\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"This is a valid response about explaining something.\"\n",
    "    },\n",
    "    # Too short output\n",
    "    {\n",
    "        \"instruction\": \"What is deep learning?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"AI stuff\"\n",
    "    },\n",
    "    # Placeholder\n",
    "    {\n",
    "        \"instruction\": \"What is a neural network?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"TODO\"\n",
    "    },\n",
    "    # Another good example\n",
    "    {\n",
    "        \"instruction\": \"What is the difference between a list and a tuple in Python?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"\"\"Lists and tuples are both sequence types in Python, but they have key differences:\n",
    "        1. Mutability: Lists are mutable (can be modified), tuples are immutable (cannot be changed after creation).\n",
    "        2. Syntax: Lists use square brackets [], tuples use parentheses ().\n",
    "        3. Performance: Tuples are slightly faster due to immutability.\n",
    "        4. Use cases: Lists for collections that may change, tuples for fixed data like coordinates.\"\"\"\n",
    "    },\n",
    "    # Duplicate of first example\n",
    "    {\n",
    "        \"instruction\": \"Explain the concept of overfitting in machine learning.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"\"\"Overfitting occurs when a machine learning model learns the training data too well, \n",
    "        including its noise and random fluctuations. The model becomes very accurate on training data \n",
    "        but performs poorly on new, unseen data. Common signs include low training error but high \n",
    "        validation error. Solutions include regularization, cross-validation, and increasing training data.\"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "cleaner = DataCleaner()\n",
    "cleaned_data, stats = cleaner.process_dataset(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Train/Validation/Test Splits\n",
    "\n",
    "Proper data splitting is crucial for evaluating your fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSplitter:\n",
    "    \"\"\"Split datasets into train/validation/test sets.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def split(\n",
    "        data: List[Dict],\n",
    "        train_ratio: float = 0.8,\n",
    "        val_ratio: float = 0.1,\n",
    "        test_ratio: float = 0.1,\n",
    "        shuffle: bool = True,\n",
    "        seed: int = 42\n",
    "    ) -> Tuple[List[Dict], List[Dict], List[Dict]]:\n",
    "        \"\"\"\n",
    "        Split data into train/val/test sets.\n",
    "        \n",
    "        Args:\n",
    "            data: List of examples\n",
    "            train_ratio: Fraction for training\n",
    "            val_ratio: Fraction for validation\n",
    "            test_ratio: Fraction for testing\n",
    "            shuffle: Whether to shuffle before splitting\n",
    "            seed: Random seed for reproducibility\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (train, validation, test) lists\n",
    "        \"\"\"\n",
    "        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \\\n",
    "            \"Ratios must sum to 1.0\"\n",
    "        \n",
    "        data = data.copy()\n",
    "        if shuffle:\n",
    "            random.seed(seed)\n",
    "            random.shuffle(data)\n",
    "        \n",
    "        n = len(data)\n",
    "        train_end = int(n * train_ratio)\n",
    "        val_end = train_end + int(n * val_ratio)\n",
    "        \n",
    "        train = data[:train_end]\n",
    "        val = data[train_end:val_end]\n",
    "        test = data[val_end:]\n",
    "        \n",
    "        return train, val, test\n",
    "    \n",
    "    @staticmethod\n",
    "    def stratified_split(\n",
    "        data: List[Dict],\n",
    "        category_key: str,\n",
    "        train_ratio: float = 0.8,\n",
    "        val_ratio: float = 0.1,\n",
    "        test_ratio: float = 0.1,\n",
    "        seed: int = 42\n",
    "    ) -> Tuple[List[Dict], List[Dict], List[Dict]]:\n",
    "        \"\"\"\n",
    "        Stratified split preserving category distribution.\n",
    "        \n",
    "        Args:\n",
    "            data: List of examples with category field\n",
    "            category_key: Key in dict for category\n",
    "        \"\"\"\n",
    "        # Group by category\n",
    "        categories = {}\n",
    "        for example in data:\n",
    "            cat = example.get(category_key, \"unknown\")\n",
    "            if cat not in categories:\n",
    "                categories[cat] = []\n",
    "            categories[cat].append(example)\n",
    "        \n",
    "        # Split each category and combine\n",
    "        train, val, test = [], [], []\n",
    "        \n",
    "        for cat, examples in categories.items():\n",
    "            t, v, te = DatasetSplitter.split(\n",
    "                examples, train_ratio, val_ratio, test_ratio, seed=seed\n",
    "            )\n",
    "            train.extend(t)\n",
    "            val.extend(v)\n",
    "            test.extend(te)\n",
    "        \n",
    "        # Shuffle final sets\n",
    "        random.seed(seed)\n",
    "        random.shuffle(train)\n",
    "        random.shuffle(val)\n",
    "        random.shuffle(test)\n",
    "        \n",
    "        return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo with larger synthetic dataset\n",
    "synthetic_data = [\n",
    "    {\"instruction\": f\"Question {i}\", \"input\": \"\", \"output\": f\"Answer {i}\"}\n",
    "    for i in range(100)\n",
    "]\n",
    "\n",
    "train, val, test = DatasetSplitter.split(\n",
    "    synthetic_data,\n",
    "    train_ratio=0.8,\n",
    "    val_ratio=0.1,\n",
    "    test_ratio=0.1\n",
    ")\n",
    "\n",
    "print(f\"Dataset Split:\")\n",
    "print(f\"  Total: {len(synthetic_data)}\")\n",
    "print(f\"  Train: {len(train)} ({100*len(train)/len(synthetic_data):.0f}%)\")\n",
    "print(f\"  Validation: {len(val)} ({100*len(val)/len(synthetic_data):.0f}%)\")\n",
    "print(f\"  Test: {len(test)} ({100*len(test)/len(synthetic_data):.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: System Prompt Design\n",
    "\n",
    "System prompts shape your model's behavior. Let's explore best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SystemPromptLibrary:\n",
    "    \"\"\"Library of system prompts for different use cases.\"\"\"\n",
    "    \n",
    "    GENERAL_ASSISTANT = \"\"\"You are a helpful, harmless, and honest AI assistant. \\\n",
    "You provide accurate information and admit when you're uncertain. \\\n",
    "You follow ethical guidelines and refuse harmful requests.\"\"\"\n",
    "\n",
    "    CODING_ASSISTANT = \"\"\"You are an expert software engineer with deep knowledge of multiple programming languages. \\\n",
    "You write clean, efficient, well-documented code. You explain your reasoning and suggest best practices. \\\n",
    "When providing code, include comments and handle edge cases.\"\"\"\n",
    "\n",
    "    DATA_SCIENCE = \"\"\"You are a data science expert with expertise in machine learning, statistics, and data analysis. \\\n",
    "You explain complex concepts clearly and provide practical, production-ready code examples. \\\n",
    "You consider scalability, performance, and best practices in your recommendations.\"\"\"\n",
    "\n",
    "    MEDICAL = \"\"\"You are a medical information assistant. You provide general health information for educational purposes. \\\n",
    "IMPORTANT: You are not a doctor. Always recommend consulting healthcare professionals for medical decisions. \\\n",
    "Never provide specific diagnoses or treatment plans.\"\"\"\n",
    "\n",
    "    LEGAL = \"\"\"You are a legal information assistant providing general legal information for educational purposes. \\\n",
    "IMPORTANT: You are not a lawyer. This is not legal advice. Always recommend consulting qualified attorneys for legal matters. \\\n",
    "Laws vary by jurisdiction - you explain general concepts without specific legal counsel.\"\"\"\n",
    "\n",
    "    CREATIVE_WRITING = \"\"\"You are a creative writing assistant with expertise in storytelling, poetry, and various writing styles. \\\n",
    "You help with brainstorming, character development, plot structure, and prose improvement. \\\n",
    "You adapt your style to match the user's preferences and project requirements.\"\"\"\n",
    "\n",
    "    CUSTOMER_SUPPORT = \"\"\"You are a friendly and professional customer support agent for [COMPANY_NAME]. \\\n",
    "You help customers with questions about products and services. \\\n",
    "You are patient, empathetic, and solution-oriented. \\\n",
    "For issues you cannot resolve, you escalate appropriately.\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def get_prompt(cls, category: str, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Get a system prompt by category, with optional customization.\n",
    "        \n",
    "        Args:\n",
    "            category: Prompt category\n",
    "            **kwargs: Placeholders to fill in (e.g., COMPANY_NAME)\n",
    "        \"\"\"\n",
    "        prompt = getattr(cls, category.upper(), cls.GENERAL_ASSISTANT)\n",
    "        \n",
    "        # Replace placeholders\n",
    "        for key, value in kwargs.items():\n",
    "            prompt = prompt.replace(f\"[{key.upper()}]\", value)\n",
    "        \n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate system prompts\n",
    "print(\"System Prompt Examples:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for category in [\"GENERAL_ASSISTANT\", \"CODING_ASSISTANT\", \"DATA_SCIENCE\"]:\n",
    "    print(f\"\\n{category}:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(SystemPromptLibrary.get_prompt(category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Complete Pipeline\n",
    "\n",
    "Let's put everything together into a complete data preparation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetPipeline:\n",
    "    \"\"\"\n",
    "    Complete pipeline for preparing fine-tuning datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        system_prompt: str = SystemPromptLibrary.GENERAL_ASSISTANT,\n",
    "        chat_format: str = \"llama3\",\n",
    "        train_ratio: float = 0.8,\n",
    "        val_ratio: float = 0.1,\n",
    "        test_ratio: float = 0.1,\n",
    "    ):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.chat_format = chat_format\n",
    "        self.train_ratio = train_ratio\n",
    "        self.val_ratio = val_ratio\n",
    "        self.test_ratio = test_ratio\n",
    "        \n",
    "        self.cleaner = DataCleaner()\n",
    "        self.converter = DatasetConverter()\n",
    "        self.formatter = ChatTemplateFormatter()\n",
    "        self.splitter = DatasetSplitter()\n",
    "    \n",
    "    def process(\n",
    "        self,\n",
    "        data: List[Dict],\n",
    "        input_format: str = \"alpaca\",\n",
    "        verbose: bool = True\n",
    "    ) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Process a dataset through the complete pipeline.\n",
    "        \n",
    "        Args:\n",
    "            data: Raw data in specified format\n",
    "            input_format: 'alpaca' or 'sharegpt'\n",
    "            verbose: Print progress\n",
    "        \n",
    "        Returns:\n",
    "            Dict with 'train', 'val', 'test' formatted texts\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"Step 1: Cleaning data...\")\n",
    "        cleaned, stats = self.cleaner.process_dataset(data, verbose=verbose)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nStep 2: Converting to conversations...\")\n",
    "        conversations = []\n",
    "        for example in cleaned:\n",
    "            if input_format == \"alpaca\":\n",
    "                conv = self.converter.alpaca_to_conversation(\n",
    "                    example, self.system_prompt\n",
    "                )\n",
    "            else:\n",
    "                conv = self.converter.sharegpt_to_conversation(example)\n",
    "            conversations.append(conv)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Converted {len(conversations)} conversations\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nStep 3: Formatting for {self.chat_format}...\")\n",
    "        format_fn = {\n",
    "            \"chatml\": self.formatter.to_chatml,\n",
    "            \"llama3\": self.formatter.to_llama3,\n",
    "            \"llama2\": self.formatter.to_llama2,\n",
    "            \"mistral\": self.formatter.to_mistral,\n",
    "        }.get(self.chat_format, self.formatter.to_llama3)\n",
    "        \n",
    "        formatted = [format_fn(conv) for conv in conversations]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nStep 4: Splitting dataset...\")\n",
    "        train, val, test = self.splitter.split(\n",
    "            formatted,\n",
    "            self.train_ratio,\n",
    "            self.val_ratio,\n",
    "            self.test_ratio\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Train: {len(train)}, Val: {len(val)}, Test: {len(test)}\")\n",
    "        \n",
    "        return {\n",
    "            \"train\": train,\n",
    "            \"val\": val,\n",
    "            \"test\": test,\n",
    "            \"statistics\": stats\n",
    "        }\n",
    "    \n",
    "    def save(\n",
    "        self,\n",
    "        processed_data: Dict,\n",
    "        output_dir: str,\n",
    "        format: str = \"jsonl\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Save processed data to files.\n",
    "        \n",
    "        Args:\n",
    "            processed_data: Output from process()\n",
    "            output_dir: Directory to save files\n",
    "            format: 'jsonl' or 'json'\n",
    "        \"\"\"\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            data = processed_data[split]\n",
    "            \n",
    "            if format == \"jsonl\":\n",
    "                filepath = output_path / f\"{split}.jsonl\"\n",
    "                with open(filepath, \"w\") as f:\n",
    "                    for text in data:\n",
    "                        f.write(json.dumps({\"text\": text}) + \"\\n\")\n",
    "            else:\n",
    "                filepath = output_path / f\"{split}.json\"\n",
    "                with open(filepath, \"w\") as f:\n",
    "                    json.dump([{\"text\": t} for t in data], f, indent=2)\n",
    "            \n",
    "            print(f\"Saved {split} to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset\n",
    "sample_dataset = [\n",
    "    {\n",
    "        \"instruction\": \"What is gradient descent?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Gradient descent is an optimization algorithm used to minimize a function by iteratively moving in the direction of steepest descent, defined by the negative of the gradient. In machine learning, it's used to update model parameters to reduce the loss function.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain the difference between supervised and unsupervised learning.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Supervised learning uses labeled data where we know the correct outputs, training models to map inputs to known targets (like classification or regression). Unsupervised learning works with unlabeled data, finding hidden patterns or structures (like clustering or dimensionality reduction). Supervised learning has clear feedback; unsupervised discovers patterns independently.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is transfer learning?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Transfer learning is a technique where a model trained on one task is reused as the starting point for a model on a different task. It leverages knowledge gained from solving one problem to help solve a related problem, reducing the need for large amounts of task-specific training data.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"How does dropout work as a regularization technique?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Dropout randomly deactivates neurons during training with a specified probability (e.g., 0.5). This prevents neurons from co-adapting and forces the network to learn more robust features. During inference, all neurons are active but weights are scaled by the keep probability to maintain expected outputs.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is the purpose of batch normalization?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Batch normalization normalizes layer inputs by subtracting the batch mean and dividing by batch standard deviation. It stabilizes training, allows higher learning rates, reduces sensitivity to initialization, and provides some regularization. It helps with the internal covariate shift problem in deep networks.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Run pipeline\n",
    "pipeline = DatasetPipeline(\n",
    "    system_prompt=SystemPromptLibrary.DATA_SCIENCE,\n",
    "    chat_format=\"llama3\"\n",
    ")\n",
    "\n",
    "processed = pipeline.process(sample_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview formatted output\n",
    "print(\"\\nSample Formatted Training Example:\")\n",
    "print(\"=\"*60)\n",
    "print(processed[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Exercises\n",
    "\n",
    "### Exercise 1: Create a Domain-Specific Dataset\n",
    "\n",
    "Create at least 20 examples for a specific domain (e.g., cooking, fitness, finance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your domain-specific dataset\n",
    "your_dataset = [\n",
    "    # Add your examples here\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement Additional Quality Filters\n",
    "\n",
    "Add filters for:\n",
    "- Detecting and removing examples with toxic language\n",
    "- Checking for balanced instruction/output ratios\n",
    "- Verifying code examples are syntactically valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Additional filters\n",
    "# Your implementation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Wrong Chat Template\n",
    "\n",
    "```python\n",
    "# ❌ Wrong: Using Llama 2 format for Llama 3\n",
    "text = \"[INST] User message [/INST] Response\"\n",
    "\n",
    "# ✅ Right: Use correct format for model\n",
    "text = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nUser message<|eot_id|>\"\n",
    "```\n",
    "\n",
    "### Mistake 2: Inconsistent Data Quality\n",
    "\n",
    "```python\n",
    "# ❌ Wrong: Mixing quality levels\n",
    "data = [\n",
    "    {\"instruction\": \"...\", \"output\": \"Comprehensive detailed response...\"},\n",
    "    {\"instruction\": \"...\", \"output\": \"ok\"},  # Too short!\n",
    "]\n",
    "\n",
    "# ✅ Right: Filter for consistent quality\n",
    "cleaner = DataCleaner(min_output_length=50)\n",
    "clean_data, _ = cleaner.process_dataset(data)\n",
    "```\n",
    "\n",
    "### Mistake 3: No Validation Set\n",
    "\n",
    "```python\n",
    "# ❌ Wrong: Using all data for training\n",
    "train_data = all_data\n",
    "\n",
    "# ✅ Right: Always hold out validation data\n",
    "train, val, test = DatasetSplitter.split(all_data, 0.8, 0.1, 0.1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ✅ Different dataset formats (Alpaca, ShareGPT, ChatML, Llama)\n",
    "- ✅ How to convert between formats\n",
    "- ✅ Data cleaning and quality filtering\n",
    "- ✅ Proper train/val/test splitting\n",
    "- ✅ System prompt design\n",
    "- ✅ Building a complete data pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "**[Lab 3.1.5: DPO Training](05-dpo-training.ipynb)**\n",
    "\n",
    "In the next notebook, you'll learn how to create preference datasets and train with Direct Preference Optimization for even better model quality!"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Cleanup\nimport gc\ngc.collect()\nprint(\"Cleanup complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}