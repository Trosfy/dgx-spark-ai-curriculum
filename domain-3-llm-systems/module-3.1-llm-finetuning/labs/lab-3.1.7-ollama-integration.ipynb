{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 3.1.7: Ollama Integration - Deploy Your Fine-Tuned Model\n\n**Module:** 3.1 - Large Language Model Fine-Tuning  \n**Time:** 2 hours  \n**Difficulty:** ⭐⭐⭐☆☆\n\n---\n\n## Learning Objectives\n\nBy the end of this notebook, you will:\n- [ ] Merge LoRA weights with the base model\n- [ ] Convert the merged model to GGUF format\n- [ ] Import your fine-tuned model into Ollama\n- [ ] Test the model locally\n- [ ] Benchmark performance\n\n---\n\n## Why Ollama?\n\n**Ollama** is a powerful tool for running LLMs locally. After fine-tuning, deploying to Ollama gives you:\n\n- **Easy API access**: Simple REST API for integration\n- **Optimized inference**: Uses llama.cpp under the hood\n- **Model management**: Easy to load, switch, and share models\n- **Community integration**: Works with many tools and UIs\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELI5: From Training to Production\n",
    "\n",
    "> **Imagine you just trained a new chef (fine-tuning).** Now you want them to work in your restaurant.\n",
    ">\n",
    "> **Step 1 - Merge the Knowledge:**  \n",
    "> Your chef learned new skills (LoRA) but still uses their original training (base model). Merging combines everything into one complete chef.\n",
    ">\n",
    "> **Step 2 - Pack Their Tools:**  \n",
    "> Converting to GGUF is like packing their cooking tools into a standardized, efficient kit that works anywhere.\n",
    ">\n",
    "> **Step 3 - Open the Restaurant:**  \n",
    "> Ollama is like opening your restaurant - customers can now place orders (send prompts) and get dishes (responses).\n",
    ">\n",
    "> **The result:** Your fine-tuned model is ready to serve, accessible via simple commands or API calls!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have:\n",
    "- A fine-tuned LoRA adapter from Task 10.2 or 10.3\n",
    "- Ollama installed on your DGX Spark\n",
    "- llama.cpp for GGUF conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup and imports\nimport os\nimport json\nimport subprocess\nimport shutil\nimport time  # Required for benchmarking\nfrom pathlib import Path\nfrom typing import Optional, Dict, List\nimport requests\n\n# Check Ollama installation\ndef check_ollama():\n    \"\"\"Check if Ollama is installed and running.\"\"\"\n    try:\n        result = subprocess.run(['ollama', '--version'], capture_output=True, text=True)\n        print(f\"Ollama version: {result.stdout.strip()}\")\n        \n        # Check if Ollama server is running\n        try:\n            response = requests.get('http://localhost:11434/api/tags', timeout=5)\n            if response.status_code == 200:\n                print(\"Ollama server is running\")\n                models = response.json().get('models', [])\n                print(f\"Installed models: {len(models)}\")\n                return True\n        except requests.exceptions.ConnectionError:\n            print(\"Ollama server is not running. Start with: ollama serve\")\n            return False\n    except FileNotFoundError:\n        print(\"Ollama is not installed.\")\n        print(\"Install with: curl -fsSL https://ollama.com/install.sh | sh\")\n        return False\n\ncheck_ollama()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation instructions\n",
    "installation_guide = \"\"\"\n",
    "INSTALLATION GUIDE\n",
    "==================\n",
    "\n",
    "1. INSTALL OLLAMA\n",
    "-----------------\n",
    "# Linux (including DGX Spark)\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Verify\n",
    "ollama --version\n",
    "\n",
    "# Start the server\n",
    "ollama serve\n",
    "\n",
    "2. INSTALL LLAMA.CPP (for GGUF conversion)\n",
    "-----------------------------------------\n",
    "# Clone the repository\n",
    "git clone https://github.com/ggerganov/llama.cpp\n",
    "cd llama.cpp\n",
    "\n",
    "# Build with CUDA support (for DGX Spark)\n",
    "make LLAMA_CUDA=1\n",
    "\n",
    "# Or use pip for Python bindings\n",
    "pip install llama-cpp-python\n",
    "\n",
    "3. VERIFY SETUP\n",
    "---------------\n",
    "# Check Ollama\n",
    "ollama list\n",
    "\n",
    "# Check llama.cpp\n",
    "ls llama.cpp/convert*.py\n",
    "\"\"\"\n",
    "\n",
    "print(installation_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Merging LoRA Weights\n",
    "\n",
    "First, we need to merge the LoRA adapter with the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Configuration\n",
    "BASE_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\"  # Base model\n",
    "ADAPTER_PATH = \"./llama3-8b-lora-adapter\"  # Your LoRA adapter\n",
    "MERGED_OUTPUT_PATH = \"./llama3-8b-merged\"  # Output path\n",
    "\n",
    "print(f\"Base model: {BASE_MODEL}\")\n",
    "print(f\"Adapter path: {ADAPTER_PATH}\")\n",
    "print(f\"Output path: {MERGED_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "def merge_lora_weights(\n",
    "    base_model_path: str,\n",
    "    adapter_path: str,\n",
    "    output_path: str,\n",
    "    push_to_hub: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Merge LoRA adapter weights into the base model.\n",
    "    \n",
    "    Args:\n",
    "        base_model_path: HuggingFace model ID or local path\n",
    "        adapter_path: Path to LoRA adapter\n",
    "        output_path: Where to save merged model\n",
    "        push_to_hub: Whether to upload to HuggingFace Hub\n",
    "    \"\"\"\n",
    "    print(\"Step 1: Loading base model...\")\n",
    "    \n",
    "    # Load base model in float16 for merging\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"  Base model loaded: {sum(p.numel() for p in base_model.parameters()):,} parameters\")\n",
    "    \n",
    "    print(\"\\nStep 2: Loading LoRA adapter...\")\n",
    "    \n",
    "    # Load adapter\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        adapter_path,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    \n",
    "    print(\"  Adapter loaded\")\n",
    "    \n",
    "    print(\"\\nStep 3: Merging weights...\")\n",
    "    \n",
    "    # Merge and unload adapter\n",
    "    model = model.merge_and_unload()\n",
    "    \n",
    "    print(\"  Weights merged\")\n",
    "    \n",
    "    print(\"\\nStep 4: Saving merged model...\")\n",
    "    \n",
    "    # Save merged model\n",
    "    model.save_pretrained(output_path, safe_serialization=True)\n",
    "    \n",
    "    # Save tokenizer too\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "    tokenizer.save_pretrained(output_path)\n",
    "    \n",
    "    print(f\"  Model saved to {output_path}\")\n",
    "    \n",
    "    # Calculate size\n",
    "    total_size = sum(\n",
    "        f.stat().st_size for f in Path(output_path).glob('**/*') if f.is_file()\n",
    "    )\n",
    "    print(f\"  Total size: {total_size / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, base_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "print(\"Merge function ready!\")\n",
    "print(\"\\nTo merge, run:\")\n",
    "print(f\"merge_lora_weights('{BASE_MODEL}', '{ADAPTER_PATH}', '{MERGED_OUTPUT_PATH}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to actually merge (requires adapter from Task 10.2)\n",
    "# merged_path = merge_lora_weights(BASE_MODEL, ADAPTER_PATH, MERGED_OUTPUT_PATH)\n",
    "\n",
    "# For demo purposes, we'll use the base model\n",
    "merged_path = MERGED_OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Converting to GGUF Format\n",
    "\n",
    "GGUF (GPT-Generated Unified Format) is the format used by llama.cpp and Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GGUF conversion options\n",
    "gguf_quantization_options = \"\"\"\n",
    "GGUF QUANTIZATION OPTIONS\n",
    "=========================\n",
    "\n",
    "| Type     | Size   | Quality | Speed | Use Case                    |\n",
    "|----------|--------|---------|-------|-----------------------------|\n",
    "| F32      | ~140GB | Best    | Slow  | Debugging only              |\n",
    "| F16      | ~14GB  | Best    | Good  | When you have lots of RAM   |\n",
    "| Q8_0     | ~8GB   | Great   | Fast  | Best quality/size trade-off |\n",
    "| Q6_K     | ~6GB   | Great   | Fast  | Good balance                |\n",
    "| Q5_K_M   | ~5GB   | Good    | Fast  | Recommended for most users  |\n",
    "| Q4_K_M   | ~4GB   | Good    | Fast  | When memory is tight        |\n",
    "| Q4_0     | ~4GB   | Fair    | Fast  | Maximum compression         |\n",
    "| Q3_K_M   | ~3GB   | Fair    | Fast  | Extreme compression         |\n",
    "| Q2_K     | ~2GB   | Poor    | Fast  | Emergency use only          |\n",
    "\n",
    "FOR DGX SPARK (128GB):\n",
    "- F16 or Q8_0 recommended for best quality\n",
    "- You have plenty of memory, prioritize quality\n",
    "\n",
    "FOR 8B MODELS:\n",
    "- Q8_0: ~8GB - Excellent quality, fits easily\n",
    "- Q5_K_M: ~5GB - Great quality, smaller\n",
    "\n",
    "FOR 70B MODELS:\n",
    "- Q4_K_M: ~40GB - Good quality, fits in 128GB\n",
    "- Q5_K_M: ~50GB - Better quality, still fits\n",
    "\"\"\"\n",
    "\n",
    "print(gguf_quantization_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_gguf(\n",
    "    model_path: str,\n",
    "    output_path: str,\n",
    "    quantization: str = \"Q5_K_M\",\n",
    "    llama_cpp_path: str = \"./llama.cpp\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert a HuggingFace model to GGUF format.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to merged HF model\n",
    "        output_path: Where to save GGUF file\n",
    "        quantization: Quantization type (e.g., Q5_K_M)\n",
    "        llama_cpp_path: Path to llama.cpp directory\n",
    "    \"\"\"\n",
    "    print(\"Step 1: Converting to GGUF (FP16)...\")\n",
    "    \n",
    "    # First, convert to GGUF FP16\n",
    "    fp16_output = output_path.replace('.gguf', '-f16.gguf')\n",
    "    \n",
    "    convert_script = f\"{llama_cpp_path}/convert_hf_to_gguf.py\"\n",
    "    \n",
    "    cmd = [\n",
    "        \"python\", convert_script,\n",
    "        model_path,\n",
    "        \"--outfile\", fp16_output,\n",
    "        \"--outtype\", \"f16\",\n",
    "    ]\n",
    "    \n",
    "    print(f\"  Running: {' '.join(cmd)}\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  FP16 GGUF created: {fp16_output}\")\n",
    "    \n",
    "    if quantization.upper() != \"F16\":\n",
    "        print(f\"\\nStep 2: Quantizing to {quantization}...\")\n",
    "        \n",
    "        quantize_bin = f\"{llama_cpp_path}/llama-quantize\"\n",
    "        \n",
    "        cmd = [\n",
    "            quantize_bin,\n",
    "            fp16_output,\n",
    "            output_path,\n",
    "            quantization,\n",
    "        ]\n",
    "        \n",
    "        print(f\"  Running: {' '.join(cmd)}\")\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            print(f\"Error: {result.stderr}\")\n",
    "            return fp16_output  # Return FP16 if quantization fails\n",
    "        \n",
    "        print(f\"  Quantized GGUF created: {output_path}\")\n",
    "        \n",
    "        # Cleanup FP16 intermediate\n",
    "        os.remove(fp16_output)\n",
    "    else:\n",
    "        output_path = fp16_output\n",
    "    \n",
    "    # Check file size\n",
    "    size = os.path.getsize(output_path) / 1e9\n",
    "    print(f\"\\nFinal GGUF size: {size:.2f} GB\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "print(\"GGUF conversion function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Using Python llama-cpp-python for conversion\n",
    "python_conversion = \"\"\"\n",
    "PYTHON-BASED CONVERSION (Alternative)\n",
    "=====================================\n",
    "\n",
    "If you don't have llama.cpp compiled, use the Python approach:\n",
    "\n",
    "1. Install llama-cpp-python:\n",
    "   pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
    "\n",
    "2. Use HuggingFace's tool:\n",
    "   pip install huggingface_hub\n",
    "   \n",
    "   from huggingface_hub import snapshot_download\n",
    "   \n",
    "   # Download pre-quantized GGUF if available\n",
    "   snapshot_download(\n",
    "       repo_id=\"your-username/your-model-GGUF\",\n",
    "       local_dir=\"./model-gguf\"\n",
    "   )\n",
    "\n",
    "3. Or use LLaMA Factory export (recommended):\n",
    "   llamafactory-cli export \\\\\n",
    "       --model_name_or_path ./merged-model \\\\\n",
    "       --export_dir ./gguf-output \\\\\n",
    "       --export_quantization_bit 4 \\\\\n",
    "       --export_legacy_format false\n",
    "\"\"\"\n",
    "\n",
    "print(python_conversion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Importing to Ollama\n",
    "\n",
    "Once you have the GGUF file, importing to Ollama is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_modelfile(gguf_path: str, model_name: str, output_path: str = \"./Modelfile\"):\n",
    "    \"\"\"\n",
    "    Create an Ollama Modelfile for importing a GGUF model.\n",
    "    \n",
    "    Args:\n",
    "        gguf_path: Path to GGUF file\n",
    "        model_name: Name for the model in Ollama\n",
    "        output_path: Where to save Modelfile\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get absolute path\n",
    "    gguf_abs_path = os.path.abspath(gguf_path)\n",
    "    \n",
    "    modelfile_content = f'''# Modelfile for {model_name}\n",
    "# Fine-tuned model imported from GGUF\n",
    "\n",
    "# Base model from GGUF file\n",
    "FROM {gguf_abs_path}\n",
    "\n",
    "# Model parameters (adjust as needed)\n",
    "PARAMETER temperature 0.7\n",
    "PARAMETER top_p 0.9\n",
    "PARAMETER top_k 40\n",
    "PARAMETER num_ctx 4096\n",
    "PARAMETER num_predict 512\n",
    "\n",
    "# System prompt (customize for your fine-tuned model)\n",
    "SYSTEM \"\"\"You are a helpful AI assistant that has been fine-tuned for specific tasks. \n",
    "You provide accurate, helpful, and concise responses.\"\"\"\n",
    "\n",
    "# Chat template (for Llama 3.1 style)\n",
    "TEMPLATE \"\"\"{{{{ if .System }}}}<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{{{{ .System }}}}<|eot_id|>{{{{ end }}}}{{{{ if .Prompt }}}}<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{{{{ .Prompt }}}}<|eot_id|>{{{{ end }}}}<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{{{{ .Response }}}}<|eot_id|>\"\"\"\n",
    "\n",
    "# License (update as appropriate)\n",
    "LICENSE \"\"\"This model is based on Llama 3.1 and subject to Meta's license.\n",
    "Fine-tuning performed by [Your Name].\"\"\"\n",
    "'''\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(modelfile_content)\n",
    "    \n",
    "    print(f\"Modelfile created: {output_path}\")\n",
    "    print(f\"\\nTo import into Ollama, run:\")\n",
    "    print(f\"  ollama create {model_name} -f {output_path}\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Example\n",
    "# create_modelfile(\"./model.gguf\", \"my-finetuned-llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete import workflow\n",
    "import_workflow = \"\"\"\n",
    "COMPLETE OLLAMA IMPORT WORKFLOW\n",
    "================================\n",
    "\n",
    "OPTION 1: Using Modelfile (Recommended)\n",
    "----------------------------------------\n",
    "\n",
    "# Step 1: Create Modelfile\n",
    "cat > Modelfile << 'EOF'\n",
    "FROM ./my-model-q5.gguf\n",
    "\n",
    "PARAMETER temperature 0.7\n",
    "PARAMETER num_ctx 4096\n",
    "\n",
    "SYSTEM \"You are a helpful assistant.\"\n",
    "\n",
    "TEMPLATE \"\"\"<|start_header_id|>user<|end_header_id|>\n",
    "{{ .Prompt }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "{{ .Response }}<|eot_id|>\"\"\"\n",
    "EOF\n",
    "\n",
    "# Step 2: Create model in Ollama\n",
    "ollama create my-finetuned-model -f Modelfile\n",
    "\n",
    "# Step 3: Verify import\n",
    "ollama list\n",
    "\n",
    "# Step 4: Test the model\n",
    "ollama run my-finetuned-model \"Hello, test prompt\"\n",
    "\n",
    "\n",
    "OPTION 2: Direct GGUF Import (Simpler)\n",
    "--------------------------------------\n",
    "\n",
    "# Ollama can import GGUF directly with default settings\n",
    "ollama create my-model -f ./Modelfile\n",
    "\n",
    "# Where Modelfile just contains:\n",
    "# FROM ./my-model.gguf\n",
    "\n",
    "\n",
    "OPTION 3: Push to Ollama Library\n",
    "--------------------------------\n",
    "\n",
    "# If you want to share your model\n",
    "# First, create account at ollama.com\n",
    "\n",
    "# Tag your model\n",
    "ollama cp my-model username/my-model\n",
    "\n",
    "# Push to library\n",
    "ollama push username/my-model\n",
    "\n",
    "# Others can then use:\n",
    "ollama pull username/my-model\n",
    "\"\"\"\n",
    "\n",
    "print(import_workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Testing the Imported Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import requests\nimport json\n\nclass OllamaClient:\n    \"\"\"\n    Simple client for interacting with Ollama API.\n    \n    Args:\n        base_url: Ollama server URL (default: http://localhost:11434)\n        timeout: Request timeout in seconds (default: 120)\n                 Note: Large model generation can take 30-60+ seconds\n    \"\"\"\n    \n    def __init__(self, base_url: str = \"http://localhost:11434\", timeout: int = 120):\n        self.base_url = base_url\n        self.timeout = timeout  # Default timeout in seconds\n    \n    def list_models(self) -> list:\n        \"\"\"List all available models.\"\"\"\n        try:\n            response = requests.get(f\"{self.base_url}/api/tags\", timeout=self.timeout)\n            response.raise_for_status()\n            return response.json().get('models', [])\n        except requests.exceptions.Timeout:\n            print(f\"Error: Request timed out after {self.timeout}s while listing models.\")\n            print(\"  Hint: If Ollama is loading a large model, try increasing timeout.\")\n            return []\n        except requests.exceptions.ConnectionError:\n            print(f\"Error: Cannot connect to Ollama at {self.base_url}\")\n            print(\"  Hint: Make sure Ollama is running with 'ollama serve'\")\n            return []\n        except requests.exceptions.RequestException as e:\n            print(f\"Error listing models: {e}\")\n            return []\n    \n    def generate(\n        self,\n        model: str,\n        prompt: str,\n        system: str = None,\n        stream: bool = False,\n    ) -> str:\n        \"\"\"\n        Generate a response from a model.\n        \n        Args:\n            model: Model name\n            prompt: User prompt\n            system: Optional system prompt\n            stream: Whether to stream response\n        \n        Returns:\n            Generated text or error message\n        \n        Note:\n            Generation can take 30-60+ seconds for large models or long responses.\n            Increase timeout in __init__ if you encounter timeout errors.\n        \"\"\"\n        payload = {\n            \"model\": model,\n            \"prompt\": prompt,\n            \"stream\": stream,\n        }\n        \n        if system:\n            payload[\"system\"] = system\n        \n        try:\n            response = requests.post(\n                f\"{self.base_url}/api/generate\",\n                json=payload,\n                timeout=self.timeout\n            )\n            response.raise_for_status()\n            return response.json().get('response', '')\n        except requests.exceptions.Timeout:\n            return (\n                f\"Error: Request timed out after {self.timeout}s.\\n\"\n                f\"Possible causes:\\n\"\n                f\"  1. Model '{model}' is still loading (first request after model change)\\n\"\n                f\"  2. Response generation is taking longer than expected\\n\"\n                f\"  3. System is under heavy load\\n\"\n                f\"Solutions:\\n\"\n                f\"  - Create client with higher timeout: OllamaClient(timeout=300)\\n\"\n                f\"  - Use a smaller/faster model\\n\"\n                f\"  - Reduce prompt complexity or max_tokens\"\n            )\n        except requests.exceptions.ConnectionError:\n            return (\n                f\"Error: Cannot connect to Ollama at {self.base_url}\\n\"\n                f\"Make sure Ollama is running with 'ollama serve'\"\n            )\n        except requests.exceptions.RequestException as e:\n            return f\"Error: {str(e)}\"\n    \n    def chat(\n        self,\n        model: str,\n        messages: list,\n        stream: bool = False,\n    ) -> str:\n        \"\"\"\n        Chat with a model.\n        \n        Args:\n            model: Model name\n            messages: List of {\"role\": ..., \"content\": ...}\n            stream: Whether to stream response\n        \n        Returns:\n            Assistant's response or error message\n        \"\"\"\n        payload = {\n            \"model\": model,\n            \"messages\": messages,\n            \"stream\": stream,\n        }\n        \n        try:\n            response = requests.post(\n                f\"{self.base_url}/api/chat\",\n                json=payload,\n                timeout=self.timeout\n            )\n            response.raise_for_status()\n            return response.json().get('message', {}).get('content', '')\n        except requests.exceptions.Timeout:\n            return (\n                f\"Error: Chat request timed out after {self.timeout}s.\\n\"\n                f\"Try creating client with higher timeout: OllamaClient(timeout=300)\"\n            )\n        except requests.exceptions.ConnectionError:\n            return f\"Error: Cannot connect to Ollama at {self.base_url}\"\n        except requests.exceptions.RequestException as e:\n            return f\"Error: {str(e)}\"\n\n\n# Create client with 120 second timeout (appropriate for large model inference)\n# Increase timeout if you experience timeout errors with large models\nclient = OllamaClient(timeout=120)\n\n# List models\ntry:\n    models = client.list_models()\n    print(\"Available Ollama models:\")\n    for m in models:\n        print(f\"  - {m['name']}: {m.get('size', 'unknown size')}\")\n    if not models:\n        print(\"  (No models found - try 'ollama pull llama3.2' to download a model)\")\nexcept Exception as e:\n    print(f\"Could not connect to Ollama: {e}\")\n    print(\"Make sure Ollama is running: ollama serve\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts for evaluating fine-tuned model\n",
    "test_prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain the difference between supervised and unsupervised learning.\",\n",
    "    \"Write a Python function to calculate the factorial of a number.\",\n",
    "    \"What are the benefits of using LoRA for fine-tuning?\",\n",
    "]\n",
    "\n",
    "def test_model(model_name: str, prompts: list, client: OllamaClient):\n",
    "    \"\"\"\n",
    "    Test a model with multiple prompts.\n",
    "    \"\"\"\n",
    "    print(f\"Testing model: {model_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            response = client.generate(model_name, prompt)\n",
    "            print(f\"Response: {response[:500]}...\" if len(response) > 500 else f\"Response: {response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "# Uncomment to test (requires model to be loaded in Ollama)\n",
    "# test_model(\"my-finetuned-model\", test_prompts, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Dict, List\n",
    "\n",
    "def benchmark_model(\n",
    "    model_name: str,\n",
    "    prompts: List[str],\n",
    "    client: OllamaClient,\n",
    "    num_runs: int = 3,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Benchmark model performance.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with timing statistics\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"model\": model_name,\n",
    "        \"num_prompts\": len(prompts),\n",
    "        \"num_runs\": num_runs,\n",
    "        \"times\": [],\n",
    "        \"tokens\": [],\n",
    "    }\n",
    "    \n",
    "    print(f\"Benchmarking {model_name}...\")\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        print(f\"  Run {run + 1}/{num_runs}\")\n",
    "        \n",
    "        for prompt in prompts:\n",
    "            start = time.time()\n",
    "            response = client.generate(model_name, prompt)\n",
    "            elapsed = time.time() - start\n",
    "            \n",
    "            # Rough token count (words * 1.3)\n",
    "            tokens = len(response.split()) * 1.3\n",
    "            \n",
    "            results[\"times\"].append(elapsed)\n",
    "            results[\"tokens\"].append(tokens)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    results[\"avg_time\"] = sum(results[\"times\"]) / len(results[\"times\"])\n",
    "    results[\"avg_tokens\"] = sum(results[\"tokens\"]) / len(results[\"tokens\"])\n",
    "    results[\"tokens_per_second\"] = results[\"avg_tokens\"] / results[\"avg_time\"]\n",
    "    \n",
    "    print(f\"\\nResults for {model_name}:\")\n",
    "    print(f\"  Average response time: {results['avg_time']:.2f}s\")\n",
    "    print(f\"  Average tokens: {results['avg_tokens']:.0f}\")\n",
    "    print(f\"  Tokens/second: {results['tokens_per_second']:.1f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Benchmark function ready!\")\n",
    "print(\"\\nUsage: results = benchmark_model('my-model', test_prompts, client)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison template\n",
    "comparison_template = \"\"\"\n",
    "MODEL COMPARISON TEMPLATE\n",
    "=========================\n",
    "\n",
    "| Metric             | Base Model | Fine-tuned | Improvement |\n",
    "|--------------------|------------|------------|-------------|\n",
    "| Response Time (s)  |            |            |             |\n",
    "| Tokens/Second      |            |            |             |\n",
    "| Quality (1-5)      |            |            |             |\n",
    "| Relevance (1-5)    |            |            |             |\n",
    "| Accuracy (%)       |            |            |             |\n",
    "\n",
    "QUALITATIVE ASSESSMENT\n",
    "----------------------\n",
    "\n",
    "Strengths of fine-tuned model:\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "\n",
    "Areas for improvement:\n",
    "1. \n",
    "2. \n",
    "\n",
    "Recommendations:\n",
    "1. \n",
    "2. \n",
    "\"\"\"\n",
    "\n",
    "print(comparison_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Complete Pipeline Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete end-to-end deployment script\n",
    "deployment_script = '''\n",
    "#!/bin/bash\n",
    "# deploy_finetuned_model.sh\n",
    "# Complete pipeline to deploy a fine-tuned model to Ollama\n",
    "\n",
    "set -e  # Exit on error\n",
    "\n",
    "# Configuration\n",
    "BASE_MODEL=\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "ADAPTER_PATH=\"./llama3-8b-lora-adapter\"\n",
    "MERGED_PATH=\"./llama3-8b-merged\"\n",
    "GGUF_PATH=\"./llama3-8b-finetuned.gguf\"\n",
    "MODEL_NAME=\"my-finetuned-llama\"\n",
    "QUANTIZATION=\"Q5_K_M\"\n",
    "\n",
    "echo \"=====================================\"\n",
    "echo \"Fine-tuned Model Deployment Pipeline\"\n",
    "echo \"=====================================\"\n",
    "\n",
    "# Step 1: Merge LoRA weights\n",
    "echo \"\\nStep 1: Merging LoRA weights...\"\n",
    "python << 'EOF'\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\"$BASE_MODEL\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "model = PeftModel.from_pretrained(base, \"$ADAPTER_PATH\")\n",
    "model = model.merge_and_unload()\n",
    "model.save_pretrained(\"$MERGED_PATH\")\n",
    "AutoTokenizer.from_pretrained(\"$BASE_MODEL\").save_pretrained(\"$MERGED_PATH\")\n",
    "print(\"Merge complete!\")\n",
    "EOF\n",
    "\n",
    "# Step 2: Convert to GGUF\n",
    "echo \"\\nStep 2: Converting to GGUF...\"\n",
    "python llama.cpp/convert_hf_to_gguf.py $MERGED_PATH --outfile ${GGUF_PATH%.gguf}-f16.gguf --outtype f16\n",
    "./llama.cpp/llama-quantize ${GGUF_PATH%.gguf}-f16.gguf $GGUF_PATH $QUANTIZATION\n",
    "rm ${GGUF_PATH%.gguf}-f16.gguf  # Cleanup\n",
    "\n",
    "# Step 3: Create Modelfile\n",
    "echo \"\\nStep 3: Creating Modelfile...\"\n",
    "cat > Modelfile << EOF\n",
    "FROM $GGUF_PATH\n",
    "\n",
    "PARAMETER temperature 0.7\n",
    "PARAMETER num_ctx 4096\n",
    "\n",
    "SYSTEM \"You are a helpful AI assistant.\"\n",
    "EOF\n",
    "\n",
    "# Step 4: Import to Ollama\n",
    "echo \"\\nStep 4: Importing to Ollama...\"\n",
    "ollama create $MODEL_NAME -f Modelfile\n",
    "\n",
    "# Step 5: Verify\n",
    "echo \"\\nStep 5: Verifying...\"\n",
    "ollama list | grep $MODEL_NAME\n",
    "\n",
    "echo \"\\n=====================================\"\n",
    "echo \"Deployment complete!\"\n",
    "echo \"Test with: ollama run $MODEL_NAME\"\n",
    "echo \"=====================================\"\n",
    "'''\n",
    "\n",
    "print(deployment_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Checkpoint\n\nYou've learned:\n- ✅ How to merge LoRA weights with base model\n- ✅ How to convert to GGUF format with various quantization levels\n- ✅ How to create Modelfiles and import to Ollama\n- ✅ How to test and benchmark deployed models\n- ✅ Complete deployment pipeline automation\n\n---\n\n## Congratulations!\n\nYou've completed Module 3.1: Large Language Model Fine-Tuning!\n\n**Your achievements:**\n- Understood LoRA theory and implemented it from scratch\n- Fine-tuned an 8B model with LoRA\n- Fine-tuned a **70B model with QLoRA** on DGX Spark!\n- Created professional instruction datasets\n- Trained with DPO for preference optimization\n- Explored LLaMA Factory GUI\n- Deployed your model to Ollama\n\n---\n\n## Further Reading\n\n- [Ollama Documentation](https://github.com/ollama/ollama)\n- [llama.cpp](https://github.com/ggerganov/llama.cpp)\n- [GGUF Format Specification](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)\n\n---\n\n## Next Steps\n\nContinue to **[Module 3.2: Quantization & Optimization](../module-3.2-quantization/)** to learn advanced techniques for making your models faster and smaller!"
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Cleanup",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cleanup\nimport gc\n\n# Clear any loaded models from memory\nif 'model' in dir():\n    del model\nif 'tokenizer' in dir():\n    del tokenizer\n\ngc.collect()\n\n# Clear GPU cache if available\ntry:\n    import torch\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        print(\"GPU cache cleared\")\nexcept ImportError:\n    pass\n\nprint(\"Cleanup complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}