{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 3.1.10: Ollama Integration - Deploy Your Fine-Tuned Model Locally\n\n## The Finish Line üèÅ\n\nYou've fine-tuned your model with LoRA, DoRA, DPO, or KTO. Now what? You need to **run it**!\n\n**Ollama** makes running LLMs locally as simple as:\n```bash\nollama run my-custom-model\n```\n\n### ELI5: What is Ollama?\n\nThink of Ollama like **Docker for LLMs**:\n- Docker packages applications ‚Üí Ollama packages AI models\n- `docker run nginx` ‚Üí `ollama run llama3`\n- Dockerfile ‚Üí Modelfile\n- Docker Hub ‚Üí Ollama Library\n\n### Why Ollama for Your Fine-Tuned Models?\n\n| Benefit | Description |\n|---------|-------------|\n| **Simple Deployment** | No Python scripts, just run a command |\n| **API Ready** | Built-in REST API compatible with OpenAI format |\n| **Efficient** | GGUF format, Metal/CUDA optimized |\n| **Portable** | Works on any machine with Ollama installed |\n| **Team Sharing** | Push to registry for team access |\n| **Ollama Web UI** | Test models interactively via web interface |\n\n### What You'll Learn\n\n1. **Convert** - Transform your fine-tuned model to GGUF format\n2. **Package** - Create a Modelfile with your system prompt\n3. **Deploy** - Run locally with one command\n4. **Test** - Verify in Ollama Web UI at http://localhost:11434\n5. **Integrate** - Use the API in your applications\n\n### Prerequisites\n\n- Completed fine-tuning from previous labs (or use our pre-trained example)\n- Ollama installed (`curl -fsSL https://ollama.ai/install.sh | sh`)\n- llama.cpp for GGUF conversion"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# DGX SPARK NOTE: These packages are pre-installed in the NGC PyTorch container.\n# If running outside NGC container, install with: pip install transformers peft bitsandbytes accelerate requests\n# IMPORTANT: Do NOT run 'pip install torch' on DGX Spark - use the NGC container instead.\n\n# Verify required packages (should already be available in NGC container)\nimport importlib\nrequired_packages = ['transformers', 'peft', 'bitsandbytes', 'accelerate', 'requests']\nmissing = []\nfor pkg in required_packages:\n    try:\n        importlib.import_module(pkg.replace('-', '_'))\n    except ImportError:\n        missing.append(pkg)\n\nif missing:\n    print(f\"‚ö†Ô∏è Missing packages: {missing}\")\n    print(\"Install inside NGC container with: pip install \" + \" \".join(missing))\nelse:\n    print(\"‚úÖ All required packages are available\")\n\n# Check if Ollama is installed\nimport subprocess\nresult = subprocess.run([\"which\", \"ollama\"], capture_output=True, text=True)\nif result.returncode == 0:\n    print(f\"‚úÖ Ollama found: {result.stdout.strip()}\")\nelse:\n    print(\"‚ö†Ô∏è Ollama not found. Install with: curl -fsSL https://ollama.ai/install.sh | sh\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: The Deployment Pipeline\n",
    "\n",
    "### From Fine-Tuned Weights to Running Model\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                      FINE-TUNED MODEL DEPLOYMENT PIPELINE                    ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                              ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ\n",
    "‚îÇ  ‚îÇ  Fine-Tuned ‚îÇ    ‚îÇ   Merged    ‚îÇ    ‚îÇ    GGUF     ‚îÇ    ‚îÇ   Ollama    ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ   Weights   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Model     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Format    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Model     ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  (adapter)  ‚îÇ    ‚îÇ (full size) ‚îÇ    ‚îÇ (quantized) ‚îÇ    ‚îÇ  (ready!)   ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ\n",
    "‚îÇ        ‚îÇ                  ‚îÇ                  ‚îÇ                  ‚îÇ           ‚îÇ\n",
    "‚îÇ        ‚ñº                  ‚ñº                  ‚ñº                  ‚ñº           ‚îÇ\n",
    "‚îÇ   LoRA adapter      Base + LoRA        Compressed         ollama run       ‚îÇ\n",
    "‚îÇ    ~100-500MB        ~3-16GB           ~2-8GB           my-model           ‚îÇ\n",
    "‚îÇ                                                                              ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Two Deployment Paths\n",
    "\n",
    "| Path | Description | Use Case |\n",
    "|------|-------------|----------|\n",
    "| **Full Merge** | Merge LoRA ‚Üí Convert to GGUF ‚Üí Ollama | Maximum portability, any model |\n",
    "| **GGUF LoRA** | Apply LoRA directly to GGUF base | Faster, smaller files |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Merge LoRA Adapters into Base Model\n",
    "\n",
    "### Why Merge First?\n",
    "\n",
    "LoRA adapters are \"diffs\" from the base model. For Ollama, we need a complete model.\n",
    "\n",
    "```\n",
    "Base Model (7B params) + LoRA Adapter (20M params) = Merged Model (7B params)\n",
    "                                                     (with fine-tuned weights)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lora_model(\n",
    "    base_model_id: str,\n",
    "    adapter_path: str,\n",
    "    output_path: str,\n",
    "    push_to_hub: bool = False,\n",
    "    hub_model_id: Optional[str] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Merge LoRA adapter weights into base model.\n",
    "    \n",
    "    This creates a standalone model that doesn't need the adapter.\n",
    "    \"\"\"\n",
    "    print(f\"üì• Loading base model: {base_model_id}\")\n",
    "    \n",
    "    # Load base model in FP16 for merging\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "    \n",
    "    print(f\"üîå Loading adapter: {adapter_path}\")\n",
    "    \n",
    "    # Load and merge LoRA weights\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    \n",
    "    print(\"üîÄ Merging weights...\")\n",
    "    merged_model = model.merge_and_unload()\n",
    "    \n",
    "    print(f\"üíæ Saving merged model to: {output_path}\")\n",
    "    merged_model.save_pretrained(output_path)\n",
    "    tokenizer.save_pretrained(output_path)\n",
    "    \n",
    "    if push_to_hub and hub_model_id:\n",
    "        print(f\"üöÄ Pushing to Hub: {hub_model_id}\")\n",
    "        merged_model.push_to_hub(hub_model_id)\n",
    "        tokenizer.push_to_hub(hub_model_id)\n",
    "    \n",
    "    print(\"‚úÖ Merge complete!\")\n",
    "    \n",
    "    # Show size comparison\n",
    "    adapter_size = sum(\n",
    "        f.stat().st_size for f in Path(adapter_path).rglob('*') if f.is_file()\n",
    "    ) / (1024**3)\n",
    "    merged_size = sum(\n",
    "        f.stat().st_size for f in Path(output_path).rglob('*') if f.is_file()\n",
    "    ) / (1024**3)\n",
    "    \n",
    "    print(f\"\\nüìä Size comparison:\")\n",
    "    print(f\"   Adapter: {adapter_size:.2f} GB\")\n",
    "    print(f\"   Merged:  {merged_size:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Merge a fine-tuned model\n",
    "# (Uncomment and modify paths for your model)\n",
    "\n",
    "# merge_lora_model(\n",
    "#     base_model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "#     adapter_path=\"./fine-tuned-adapter\",\n",
    "#     output_path=\"./merged-model\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Convert to GGUF Format\n",
    "\n",
    "### What is GGUF?\n",
    "\n",
    "GGUF (GPT-Generated Unified Format) is the format Ollama uses:\n",
    "- **Single file** - All weights, config, and metadata in one `.gguf` file\n",
    "- **Quantized** - Compressed from FP16 to Q4/Q8 (2-8x smaller)\n",
    "- **Fast** - Optimized for CPU/GPU inference\n",
    "\n",
    "### Quantization Options\n",
    "\n",
    "| Format | Bits | Size (7B) | Quality | Speed | Use Case |\n",
    "|--------|------|-----------|---------|-------|----------|\n",
    "| Q2_K | 2-3 | ~3GB | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Extreme compression |\n",
    "| Q4_K_M | 4 | ~4GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | **Recommended** |\n",
    "| Q5_K_M | 5 | ~5GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Quality/size balance |\n",
    "| Q8_0 | 8 | ~7GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Near-original quality |\n",
    "| F16 | 16 | ~14GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | Full precision |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_llama_cpp():\n",
    "    \"\"\"\n",
    "    Clone and build llama.cpp for GGUF conversion.\n",
    "    \"\"\"\n",
    "    llama_cpp_path = Path(\"./llama.cpp\")\n",
    "    \n",
    "    if not llama_cpp_path.exists():\n",
    "        print(\"üì• Cloning llama.cpp...\")\n",
    "        subprocess.run([\n",
    "            \"git\", \"clone\", \n",
    "            \"https://github.com/ggerganov/llama.cpp.git\"\n",
    "        ], check=True)\n",
    "        \n",
    "        print(\"üî® Building llama.cpp...\")\n",
    "        subprocess.run([\"make\", \"-j\"], cwd=llama_cpp_path, check=True)\n",
    "    else:\n",
    "        print(\"‚úÖ llama.cpp already exists\")\n",
    "    \n",
    "    # Install Python requirements for conversion\n",
    "    subprocess.run([\n",
    "        \"pip\", \"install\", \"-q\", \"-r\", \n",
    "        str(llama_cpp_path / \"requirements.txt\")\n",
    "    ])\n",
    "    \n",
    "    return llama_cpp_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_gguf(\n",
    "    model_path: str,\n",
    "    output_path: str,\n",
    "    quantization: str = \"q4_k_m\",\n",
    "    llama_cpp_path: str = \"./llama.cpp\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Convert HuggingFace model to GGUF format.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to merged HuggingFace model\n",
    "        output_path: Directory for GGUF output\n",
    "        quantization: Quantization level (q4_k_m, q5_k_m, q8_0, f16)\n",
    "        llama_cpp_path: Path to llama.cpp directory\n",
    "    \n",
    "    Returns:\n",
    "        Path to the GGUF file\n",
    "    \"\"\"\n",
    "    llama_cpp = Path(llama_cpp_path)\n",
    "    output_dir = Path(output_path)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Step 1: Convert to FP16 GGUF\n",
    "    print(\"üîÑ Converting to GGUF format...\")\n",
    "    fp16_path = output_dir / \"model-f16.gguf\"\n",
    "    \n",
    "    subprocess.run([\n",
    "        \"python\", str(llama_cpp / \"convert_hf_to_gguf.py\"),\n",
    "        model_path,\n",
    "        \"--outfile\", str(fp16_path),\n",
    "        \"--outtype\", \"f16\"\n",
    "    ], check=True)\n",
    "    \n",
    "    # Step 2: Quantize (if not f16)\n",
    "    if quantization.lower() != \"f16\":\n",
    "        print(f\"üì¶ Quantizing to {quantization}...\")\n",
    "        quantized_path = output_dir / f\"model-{quantization}.gguf\"\n",
    "        \n",
    "        subprocess.run([\n",
    "            str(llama_cpp / \"llama-quantize\"),\n",
    "            str(fp16_path),\n",
    "            str(quantized_path),\n",
    "            quantization.upper()\n",
    "        ], check=True)\n",
    "        \n",
    "        # Remove FP16 to save space\n",
    "        fp16_path.unlink()\n",
    "        final_path = quantized_path\n",
    "    else:\n",
    "        final_path = fp16_path\n",
    "    \n",
    "    # Show result\n",
    "    size_gb = final_path.stat().st_size / (1024**3)\n",
    "    print(f\"\\n‚úÖ GGUF created: {final_path}\")\n",
    "    print(f\"üìè Size: {size_gb:.2f} GB\")\n",
    "    \n",
    "    return str(final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Convert merged model to GGUF\n",
    "# (Uncomment after merging)\n",
    "\n",
    "# llama_cpp_path = setup_llama_cpp()\n",
    "# gguf_path = convert_to_gguf(\n",
    "#     model_path=\"./merged-model\",\n",
    "#     output_path=\"./gguf-output\",\n",
    "#     quantization=\"q4_k_m\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Create Ollama Modelfile\n",
    "\n",
    "### What is a Modelfile?\n",
    "\n",
    "A Modelfile is like a Dockerfile for LLMs:\n",
    "\n",
    "```dockerfile\n",
    "# Dockerfile           # Modelfile\n",
    "FROM python:3.11       FROM ./model.gguf\n",
    "COPY . /app            SYSTEM \"You are a helpful assistant\"\n",
    "CMD [\"python\", \"app\"]  PARAMETER temperature 0.7\n",
    "```\n",
    "\n",
    "### Modelfile Components\n",
    "\n",
    "| Directive | Description | Example |\n",
    "|-----------|-------------|---------|\n",
    "| FROM | Base model or GGUF path | `FROM ./model.gguf` |\n",
    "| SYSTEM | System prompt | `SYSTEM \"You are a coding assistant\"` |\n",
    "| TEMPLATE | Chat template | `TEMPLATE \"{{ .System }}...\"` |\n",
    "| PARAMETER | Model parameters | `PARAMETER temperature 0.7` |\n",
    "| LICENSE | License info | `LICENSE \"MIT\"` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_modelfile(\n",
    "    gguf_path: str,\n",
    "    model_name: str,\n",
    "    system_prompt: str,\n",
    "    template: Optional[str] = None,\n",
    "    parameters: Optional[Dict[str, Any]] = None,\n",
    "    output_dir: str = \".\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create an Ollama Modelfile for your custom model.\n",
    "    \n",
    "    Args:\n",
    "        gguf_path: Path to the GGUF model file\n",
    "        model_name: Name for your model in Ollama\n",
    "        system_prompt: Default system prompt\n",
    "        template: Optional chat template (auto-detected if None)\n",
    "        parameters: Optional parameters (temperature, top_p, etc.)\n",
    "        output_dir: Where to save the Modelfile\n",
    "    \n",
    "    Returns:\n",
    "        Path to the Modelfile\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    \n",
    "    # Base model\n",
    "    lines.append(f\"FROM {gguf_path}\")\n",
    "    lines.append(\"\")\n",
    "    \n",
    "    # System prompt\n",
    "    lines.append(f'SYSTEM \"\"\"{system_prompt}\"\"\"')\n",
    "    lines.append(\"\")\n",
    "    \n",
    "    # Chat template (for Llama 3 style models)\n",
    "    if template:\n",
    "        lines.append(f'TEMPLATE \"\"\"{template}\"\"\"')\n",
    "        lines.append(\"\")\n",
    "    \n",
    "    # Parameters\n",
    "    if parameters:\n",
    "        for key, value in parameters.items():\n",
    "            lines.append(f\"PARAMETER {key} {value}\")\n",
    "        lines.append(\"\")\n",
    "    \n",
    "    # Default parameters if none specified\n",
    "    else:\n",
    "        lines.extend([\n",
    "            \"PARAMETER temperature 0.7\",\n",
    "            \"PARAMETER top_p 0.9\",\n",
    "            \"PARAMETER top_k 40\",\n",
    "            \"PARAMETER num_ctx 4096\",\n",
    "            \"\"\n",
    "        ])\n",
    "    \n",
    "    # Write Modelfile\n",
    "    modelfile_path = Path(output_dir) / \"Modelfile\"\n",
    "    modelfile_path.write_text(\"\\n\".join(lines))\n",
    "    \n",
    "    print(f\"üìù Created Modelfile: {modelfile_path}\")\n",
    "    print(\"\\nContents:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\\n\".join(lines))\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    return str(modelfile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Modelfiles for different use cases\n",
    "\n",
    "# 1. Code Assistant\n",
    "CODE_ASSISTANT_SYSTEM = \"\"\"You are an expert programming assistant specialized in Python, \n",
    "JavaScript, and system design. You write clean, efficient, well-documented code. \n",
    "You explain your reasoning step by step and consider edge cases.\"\"\"\n",
    "\n",
    "# 2. Customer Support Bot\n",
    "SUPPORT_BOT_SYSTEM = \"\"\"You are a friendly customer support agent for TechCorp. \n",
    "You help users troubleshoot issues, answer product questions, and escalate \n",
    "complex issues when needed. Always be polite and helpful.\"\"\"\n",
    "\n",
    "# 3. Technical Writer\n",
    "TECH_WRITER_SYSTEM = \"\"\"You are a technical documentation specialist. \n",
    "You write clear, concise documentation with examples. \n",
    "You structure content logically and use consistent formatting.\"\"\"\n",
    "\n",
    "# Llama 3 Chat Template\n",
    "LLAMA3_TEMPLATE = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{{ .System }}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{{ .Prompt }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{{ .Response }}<|eot_id|>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Modelfile (example)\n",
    "# Uncomment after converting to GGUF\n",
    "\n",
    "# modelfile_path = create_modelfile(\n",
    "#     gguf_path=\"./gguf-output/model-q4_k_m.gguf\",\n",
    "#     model_name=\"my-code-assistant\",\n",
    "#     system_prompt=CODE_ASSISTANT_SYSTEM,\n",
    "#     template=LLAMA3_TEMPLATE,\n",
    "#     parameters={\n",
    "#         \"temperature\": 0.7,\n",
    "#         \"top_p\": 0.95,\n",
    "#         \"num_ctx\": 8192\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Deploy with Ollama\n",
    "\n",
    "### Three Ways to Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaDeployer:\n",
    "    \"\"\"\n",
    "    Deploy and manage custom models with Ollama.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, host: str = \"http://localhost:11434\"):\n",
    "        self.host = host\n",
    "        self.api_url = f\"{host}/api\"\n",
    "    \n",
    "    def check_ollama_running(self) -> bool:\n",
    "        \"\"\"Check if Ollama server is running.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.host}/api/tags\", timeout=5)\n",
    "            return response.status_code == 200\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            return False\n",
    "    \n",
    "    def create_model(\n",
    "        self, \n",
    "        model_name: str, \n",
    "        modelfile_path: str\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Create a new model from a Modelfile.\n",
    "        \n",
    "        Equivalent to: ollama create <name> -f <Modelfile>\n",
    "        \"\"\"\n",
    "        if not self.check_ollama_running():\n",
    "            print(\"‚ùå Ollama is not running. Start with: ollama serve\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"üèóÔ∏è Creating model: {model_name}\")\n",
    "        \n",
    "        modelfile_content = Path(modelfile_path).read_text()\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{self.api_url}/create\",\n",
    "            json={\n",
    "                \"name\": model_name,\n",
    "                \"modelfile\": modelfile_content\n",
    "            },\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                status = json.loads(line)\n",
    "                if \"status\" in status:\n",
    "                    print(f\"   {status['status']}\")\n",
    "        \n",
    "        print(f\"‚úÖ Model created: {model_name}\")\n",
    "        return True\n",
    "    \n",
    "    def list_models(self) -> list:\n",
    "        \"\"\"List all available models.\"\"\"\n",
    "        response = requests.get(f\"{self.api_url}/tags\")\n",
    "        return response.json().get(\"models\", [])\n",
    "    \n",
    "    def delete_model(self, model_name: str) -> bool:\n",
    "        \"\"\"Delete a model.\"\"\"\n",
    "        response = requests.delete(\n",
    "            f\"{self.api_url}/delete\",\n",
    "            json={\"name\": model_name}\n",
    "        )\n",
    "        return response.status_code == 200\n",
    "    \n",
    "    def model_info(self, model_name: str) -> dict:\n",
    "        \"\"\"Get model information.\"\"\"\n",
    "        response = requests.post(\n",
    "            f\"{self.api_url}/show\",\n",
    "            json={\"name\": model_name}\n",
    "        )\n",
    "        return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy your model\n",
    "deployer = OllamaDeployer()\n",
    "\n",
    "# Check if Ollama is running\n",
    "if deployer.check_ollama_running():\n",
    "    print(\"‚úÖ Ollama is running\")\n",
    "    \n",
    "    # List current models\n",
    "    models = deployer.list_models()\n",
    "    print(f\"\\nüìã Available models ({len(models)}):\")\n",
    "    for model in models:\n",
    "        size_gb = model.get('size', 0) / (1024**3)\n",
    "        print(f\"   ‚Ä¢ {model['name']}: {size_gb:.2f}GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Ollama not running. Start with: ollama serve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your custom model (uncomment after creating Modelfile)\n",
    "\n",
    "# deployer.create_model(\n",
    "#     model_name=\"my-code-assistant\",\n",
    "#     modelfile_path=\"./Modelfile\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Command Line Alternative\n",
    "\n",
    "```bash\n",
    "# Create model from Modelfile\n",
    "ollama create my-code-assistant -f Modelfile\n",
    "\n",
    "# Run the model\n",
    "ollama run my-code-assistant\n",
    "\n",
    "# List models\n",
    "ollama list\n",
    "\n",
    "# Show model info\n",
    "ollama show my-code-assistant\n",
    "\n",
    "# Delete model\n",
    "ollama rm my-code-assistant\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Using the Ollama API\n",
    "\n",
    "Ollama provides a REST API compatible with OpenAI format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaClient:\n",
    "    \"\"\"\n",
    "    Simple client for Ollama API.\n",
    "    \n",
    "    Compatible with OpenAI API format for easy migration.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, host: str = \"http://localhost:11434\"):\n",
    "        self.host = host\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        model: str,\n",
    "        prompt: str,\n",
    "        system: Optional[str] = None,\n",
    "        stream: bool = False,\n",
    "        **kwargs\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Generate a completion.\n",
    "        \n",
    "        Args:\n",
    "            model: Model name\n",
    "            prompt: User prompt\n",
    "            system: Optional system prompt override\n",
    "            stream: Whether to stream response\n",
    "            **kwargs: Additional parameters (temperature, top_p, etc.)\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": stream,\n",
    "            **kwargs\n",
    "        }\n",
    "        \n",
    "        if system:\n",
    "            payload[\"system\"] = system\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{self.host}/api/generate\",\n",
    "            json=payload\n",
    "        )\n",
    "        \n",
    "        return response.json()[\"response\"]\n",
    "    \n",
    "    def chat(\n",
    "        self,\n",
    "        model: str,\n",
    "        messages: list,\n",
    "        stream: bool = False,\n",
    "        **kwargs\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Chat completion (OpenAI compatible).\n",
    "        \n",
    "        Args:\n",
    "            model: Model name\n",
    "            messages: List of {\"role\": \"user/assistant/system\", \"content\": \"...\"}\n",
    "            stream: Whether to stream response\n",
    "        \"\"\"\n",
    "        response = requests.post(\n",
    "            f\"{self.host}/api/chat\",\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"messages\": messages,\n",
    "                \"stream\": stream,\n",
    "                **kwargs\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return response.json()[\"message\"][\"content\"]\n",
    "    \n",
    "    def chat_stream(\n",
    "        self,\n",
    "        model: str,\n",
    "        messages: list,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Streaming chat completion.\n",
    "        \n",
    "        Yields tokens as they're generated.\n",
    "        \"\"\"\n",
    "        response = requests.post(\n",
    "            f\"{self.host}/api/chat\",\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"messages\": messages,\n",
    "                \"stream\": True,\n",
    "                **kwargs\n",
    "            },\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                chunk = json.loads(line)\n",
    "                if \"message\" in chunk:\n",
    "                    yield chunk[\"message\"].get(\"content\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a model (using llama3.2:1b for demo)\n",
    "client = OllamaClient()\n",
    "\n",
    "# Check if we have a model to test with\n",
    "deployer = OllamaDeployer()\n",
    "if deployer.check_ollama_running():\n",
    "    models = deployer.list_models()\n",
    "    if models:\n",
    "        test_model = models[0][\"name\"].split(\":\")[0]\n",
    "        print(f\"Testing with model: {test_model}\")\n",
    "        \n",
    "        # Simple generation\n",
    "        response = client.generate(\n",
    "            model=test_model,\n",
    "            prompt=\"What is 2+2? Reply with just the number.\",\n",
    "            options={\"temperature\": 0.1}\n",
    "        )\n",
    "        print(f\"\\nResponse: {response}\")\n",
    "    else:\n",
    "        print(\"No models available. Run: ollama pull llama3.2:1b\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Ollama not running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat example with streaming\n",
    "if deployer.check_ollama_running() and models:\n",
    "    print(\"Streaming response:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for token in client.chat_stream(\n",
    "        model=test_model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Write a haiku about machine learning.\"}\n",
    "        ],\n",
    "        options={\"temperature\": 0.7}\n",
    "    ):\n",
    "        print(token, end=\"\", flush=True)\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: OpenAI-Compatible API\n",
    "\n",
    "Ollama can serve as a drop-in replacement for OpenAI API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# DGX SPARK NOTE: Install openai package inside NGC container if not already available\n# The openai package is a pure Python package and works fine on ARM64\nimport importlib\ntry:\n    importlib.import_module('openai')\n    print(\"‚úÖ openai package is available\")\nexcept ImportError:\n    print(\"Installing openai package...\")\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"-q\", \"openai\"], check=True)\n    print(\"‚úÖ openai package installed\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Point OpenAI client to Ollama\n",
    "openai_client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\"  # Any string works, Ollama doesn't check\n",
    ")\n",
    "\n",
    "if deployer.check_ollama_running() and models:\n",
    "    # Use OpenAI API format with Ollama\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=test_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What's the capital of France?\"}\n",
    "        ],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(\"OpenAI-compatible response:\")\n",
    "    print(response.choices[0].message.content)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Start Ollama first: ollama serve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why This Matters\n",
    "\n",
    "```python\n",
    "# Your existing code:\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"sk-...\")\n",
    "\n",
    "# Now runs locally with ONE line change:\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\"\n",
    ")\n",
    "\n",
    "# Rest of your code stays exactly the same!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 9: Complete Deployment Pipeline\n",
    "\n",
    "Let's put it all together in one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_finetuned_model(\n",
    "    base_model_id: str,\n",
    "    adapter_path: str,\n",
    "    ollama_model_name: str,\n",
    "    system_prompt: str,\n",
    "    quantization: str = \"q4_k_m\",\n",
    "    work_dir: str = \"./deployment\",\n",
    "    cleanup: bool = True\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Complete pipeline: LoRA adapter ‚Üí Running Ollama model.\n",
    "    \n",
    "    Args:\n",
    "        base_model_id: HuggingFace model ID\n",
    "        adapter_path: Path to LoRA adapter\n",
    "        ollama_model_name: Name for the Ollama model\n",
    "        system_prompt: Default system prompt\n",
    "        quantization: GGUF quantization level\n",
    "        work_dir: Working directory for intermediate files\n",
    "        cleanup: Whether to clean up intermediate files\n",
    "    \n",
    "    Returns:\n",
    "        True if successful\n",
    "    \"\"\"\n",
    "    work_path = Path(work_dir)\n",
    "    work_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üöÄ DEPLOYMENT PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Merge LoRA\n",
    "    print(\"\\nüìç Step 1/4: Merging LoRA adapter...\")\n",
    "    merged_path = work_path / \"merged\"\n",
    "    merge_lora_model(\n",
    "        base_model_id=base_model_id,\n",
    "        adapter_path=adapter_path,\n",
    "        output_path=str(merged_path)\n",
    "    )\n",
    "    \n",
    "    # Step 2: Setup llama.cpp\n",
    "    print(\"\\nüìç Step 2/4: Setting up conversion tools...\")\n",
    "    llama_cpp_path = setup_llama_cpp()\n",
    "    \n",
    "    # Step 3: Convert to GGUF\n",
    "    print(\"\\nüìç Step 3/4: Converting to GGUF...\")\n",
    "    gguf_dir = work_path / \"gguf\"\n",
    "    gguf_path = convert_to_gguf(\n",
    "        model_path=str(merged_path),\n",
    "        output_path=str(gguf_dir),\n",
    "        quantization=quantization,\n",
    "        llama_cpp_path=str(llama_cpp_path)\n",
    "    )\n",
    "    \n",
    "    # Step 4: Create Modelfile and deploy\n",
    "    print(\"\\nüìç Step 4/4: Creating Ollama model...\")\n",
    "    modelfile_path = create_modelfile(\n",
    "        gguf_path=gguf_path,\n",
    "        model_name=ollama_model_name,\n",
    "        system_prompt=system_prompt,\n",
    "        output_dir=str(work_path)\n",
    "    )\n",
    "    \n",
    "    deployer = OllamaDeployer()\n",
    "    success = deployer.create_model(\n",
    "        model_name=ollama_model_name,\n",
    "        modelfile_path=modelfile_path\n",
    "    )\n",
    "    \n",
    "    # Cleanup\n",
    "    if cleanup and success:\n",
    "        print(\"\\nüßπ Cleaning up intermediate files...\")\n",
    "        import shutil\n",
    "        shutil.rmtree(merged_path)\n",
    "        # Keep GGUF for backup\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    if success:\n",
    "        print(f\"‚úÖ SUCCESS! Run your model with:\")\n",
    "        print(f\"   ollama run {ollama_model_name}\")\n",
    "    else:\n",
    "        print(\"‚ùå Deployment failed. Check error messages above.\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full deployment example\n",
    "# Uncomment with your actual paths\n",
    "\n",
    "# deploy_finetuned_model(\n",
    "#     base_model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "#     adapter_path=\"./my-lora-adapter\",\n",
    "#     ollama_model_name=\"my-assistant\",\n",
    "#     system_prompt=\"You are a helpful AI assistant specialized in...\",\n",
    "#     quantization=\"q4_k_m\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 10: Alternative - GGUF LoRA (Advanced)\n",
    "\n",
    "Instead of merging, you can apply LoRA to a GGUF base model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gguf_lora_modelfile(\n",
    "    base_model: str,  # Ollama model name or GGUF path\n",
    "    adapter_gguf: str,  # LoRA in GGUF format\n",
    "    model_name: str,\n",
    "    system_prompt: str,\n",
    "    output_dir: str = \".\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create Modelfile that applies LoRA at runtime.\n",
    "    \n",
    "    This avoids merging - the LoRA is applied on the fly.\n",
    "    Useful when you have multiple LoRAs for the same base.\n",
    "    \"\"\"\n",
    "    lines = [\n",
    "        f\"FROM {base_model}\",\n",
    "        f\"ADAPTER {adapter_gguf}\",\n",
    "        \"\",\n",
    "        f'SYSTEM \"\"\"{system_prompt}\"\"\"',\n",
    "        \"\",\n",
    "        \"PARAMETER temperature 0.7\",\n",
    "        \"PARAMETER top_p 0.9\"\n",
    "    ]\n",
    "    \n",
    "    modelfile_path = Path(output_dir) / \"Modelfile\"\n",
    "    modelfile_path.write_text(\"\\n\".join(lines))\n",
    "    \n",
    "    print(\"üìù Created LoRA Modelfile:\")\n",
    "    print(\"\\n\".join(lines))\n",
    "    \n",
    "    return str(modelfile_path)\n",
    "\n",
    "# This requires converting LoRA to GGUF format:\n",
    "# python convert_lora_to_gguf.py --input adapter --output adapter.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use GGUF LoRA vs Merged\n",
    "\n",
    "| Approach | Pros | Cons | Best For |\n",
    "|----------|------|------|----------|\n",
    "| **Merged** | Single file, simpler | Larger file, can't swap | Production deployment |\n",
    "| **GGUF LoRA** | Swappable, smaller files | Slightly slower, complex | Multiple adapters |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 11: Production Tips\n",
    "\n",
    "### 1. Model Registry Organization\n",
    "\n",
    "```bash\n",
    "# Name your models clearly\n",
    "ollama create company/model-v1.0-code -f Modelfile.code\n",
    "ollama create company/model-v1.0-support -f Modelfile.support\n",
    "ollama create company/model-v1.1-code -f Modelfile.code.v2\n",
    "```\n",
    "\n",
    "### 2. Running as a Service\n",
    "\n",
    "```bash\n",
    "# Systemd service (Linux)\n",
    "sudo systemctl enable ollama\n",
    "sudo systemctl start ollama\n",
    "\n",
    "# Docker\n",
    "docker run -d -p 11434:11434 -v ollama:/root/.ollama ollama/ollama\n",
    "```\n",
    "\n",
    "### 3. Performance Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama environment variables for DGX Spark\n",
    "OLLAMA_CONFIG = \"\"\"\n",
    "# Add to ~/.bashrc or ~/.zshrc\n",
    "\n",
    "# GPU memory allocation (for unified memory systems)\n",
    "export OLLAMA_GPU_OVERHEAD=\"500MB\"\n",
    "\n",
    "# Number of parallel requests\n",
    "export OLLAMA_NUM_PARALLEL=4\n",
    "\n",
    "# Keep models loaded longer\n",
    "export OLLAMA_KEEP_ALIVE=\"30m\"\n",
    "\n",
    "# Maximum loaded models\n",
    "export OLLAMA_MAX_LOADED_MODELS=2\n",
    "\n",
    "# Context window (for large context models)\n",
    "export OLLAMA_NUM_CTX=8192\n",
    "\"\"\"\n",
    "\n",
    "print(OLLAMA_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sharing with Your Team\n",
    "\n",
    "```bash\n",
    "# Push to Ollama registry (requires account)\n",
    "ollama push username/my-model:latest\n",
    "\n",
    "# Team member pulls it\n",
    "ollama pull username/my-model:latest\n",
    "ollama run username/my-model\n",
    "```\n",
    "\n",
    "### 5. Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_ollama():\n",
    "    \"\"\"Monitor Ollama server status and loaded models.\"\"\"\n",
    "    deployer = OllamaDeployer()\n",
    "    \n",
    "    if not deployer.check_ollama_running():\n",
    "        print(\"‚ùå Ollama not running\")\n",
    "        return\n",
    "    \n",
    "    # Get running models\n",
    "    response = requests.get(\"http://localhost:11434/api/ps\")\n",
    "    running = response.json().get(\"models\", [])\n",
    "    \n",
    "    print(\"üìä Ollama Status\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Running models: {len(running)}\")\n",
    "    \n",
    "    for model in running:\n",
    "        name = model.get(\"name\", \"unknown\")\n",
    "        size = model.get(\"size\", 0) / (1024**3)\n",
    "        expires = model.get(\"expires_at\", \"N/A\")\n",
    "        print(f\"  ‚Ä¢ {name}: {size:.2f}GB (expires: {expires})\")\n",
    "\n",
    "if deployer.check_ollama_running():\n",
    "    monitor_ollama()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 12: Troubleshooting Guide\n\n### Common Issues and Solutions\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| \"model not found\" | Model not created | `ollama create name -f Modelfile` |\n| Connection refused | Ollama not running | `ollama serve` |\n| Out of memory | Model too large | Use smaller quantization (Q4 vs Q8) |\n| Slow generation | Not using GPU | Check `ollama ps` shows GPU usage |\n| Wrong template | Mismatched chat format | Update TEMPLATE in Modelfile |\n| Can't access Web UI | Server not running | Start with `ollama serve`, access at http://localhost:11434 |\n\n**Testing in Ollama Web UI**: For interactive testing and benchmarking, access Ollama Web UI at http://localhost:11434 after starting the server with `ollama serve`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_ollama():\n",
    "    \"\"\"Diagnose common Ollama issues.\"\"\"\n",
    "    print(\"üîç Ollama Diagnostics\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Check if installed\n",
    "    result = subprocess.run([\"which\", \"ollama\"], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"‚úÖ Ollama installed: {result.stdout.strip()}\")\n",
    "    else:\n",
    "        print(\"‚ùå Ollama not installed\")\n",
    "        print(\"   Install: curl -fsSL https://ollama.ai/install.sh | sh\")\n",
    "        return\n",
    "    \n",
    "    # Check if running\n",
    "    deployer = OllamaDeployer()\n",
    "    if deployer.check_ollama_running():\n",
    "        print(\"‚úÖ Ollama server running\")\n",
    "    else:\n",
    "        print(\"‚ùå Ollama server not running\")\n",
    "        print(\"   Start: ollama serve\")\n",
    "        return\n",
    "    \n",
    "    # Check models\n",
    "    models = deployer.list_models()\n",
    "    print(f\"‚úÖ Models available: {len(models)}\")\n",
    "    \n",
    "    # Check GPU\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No CUDA GPU (CPU inference only)\")\n",
    "\n",
    "diagnose_ollama()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### 1. The Deployment Pipeline\n",
    "```\n",
    "LoRA Adapter ‚Üí Merge ‚Üí GGUF ‚Üí Modelfile ‚Üí Ollama Model\n",
    "```\n",
    "\n",
    "### 2. Essential Commands\n",
    "```bash\n",
    "ollama create mymodel -f Modelfile  # Create\n",
    "ollama run mymodel                   # Run\n",
    "ollama list                          # List\n",
    "ollama rm mymodel                    # Delete\n",
    "```\n",
    "\n",
    "### 3. OpenAI Compatibility\n",
    "```python\n",
    "# Just change base_url!\n",
    "client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"x\")\n",
    "```\n",
    "\n",
    "### 4. Quantization Guide\n",
    "- **Q4_K_M**: Best balance for most uses\n",
    "- **Q8_0**: When quality matters more than size\n",
    "- **Q2_K**: When you're really tight on memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Deploy a Pre-trained Model\n",
    "Create a custom Modelfile for `llama3.2:1b` with a specific system prompt for your use case.\n",
    "\n",
    "### Exercise 2: Full Pipeline\n",
    "Take a fine-tuned adapter from Lab 3.1.4 and deploy it to Ollama.\n",
    "\n",
    "### Exercise 3: Build an Application\n",
    "Create a simple chat application using the Ollama API with your custom model.\n",
    "\n",
    "### Exercise 4: Compare Quantizations\n",
    "Convert the same model to Q4, Q5, and Q8. Compare:\n",
    "- File sizes\n",
    "- Generation speed\n",
    "- Output quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Congratulations! üéâ\n",
    "\n",
    "You've completed the **LLM Fine-Tuning Module**!\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "| Lab | Topic | Key Skill |\n",
    "|-----|-------|----------|\n",
    "| 3.1.1 | LoRA Theory | Understand low-rank adaptation |\n",
    "| 3.1.2 | DoRA | Weight decomposition for +3.7 points |\n",
    "| 3.1.3 | NEFTune | 5-line trick for +29% quality |\n",
    "| 3.1.4 | 8B Fine-tuning | Complete training pipeline |\n",
    "| 3.1.5 | 70B QLoRA | Fine-tune massive models locally |\n",
    "| 3.1.6 | Dataset Prep | Format and clean training data |\n",
    "| 3.1.7 | DPO | Align models with preferences |\n",
    "| 3.1.8 | SimPO/ORPO | Modern preference optimization |\n",
    "| 3.1.9 | KTO | Train with binary feedback |\n",
    "| 3.1.10 | Ollama | Deploy models locally |\n",
    "\n",
    "### You Can Now\n",
    "\n",
    "‚úÖ Fine-tune any open-source LLM  \n",
    "‚úÖ Use the latest efficient techniques (DoRA, NEFTune)  \n",
    "‚úÖ Align models with human preferences (DPO, SimPO, ORPO, KTO)  \n",
    "‚úÖ Deploy your models for local inference  \n",
    "‚úÖ Run 70B models on your DGX Spark  \n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Module 3.2**: Quantization - Make models even smaller\n",
    "- **Module 3.3**: Deployment - Production serving at scale\n",
    "- **Module 3.4**: Test-Time Compute - Inference optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}