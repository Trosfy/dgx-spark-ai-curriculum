{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1.7: DPO Training - Direct Preference Optimization\n",
    "\n",
    "**Module:** 3.1 - Large Language Model Fine-Tuning  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐⭐⭐☆\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand how DPO aligns models to human preferences\n",
    "- [ ] Implement DPO training with TRL\n",
    "- [ ] Compare DPO vs SFT-only results\n",
    "- [ ] Choose appropriate hyperparameters (beta)\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "### The Alignment Problem\n",
    "\n",
    "After supervised fine-tuning (SFT), your model can follow instructions. But it might still:\n",
    "- Give overly verbose responses when you want concise ones\n",
    "- Be too formal when you want casual\n",
    "- Refuse things it shouldn't, or not refuse things it should\n",
    "\n",
    "**DPO teaches the model your PREFERENCES** - not just what to say, but HOW to say it.\n",
    "\n",
    "### The Traditional RLHF Pipeline (Complex)\n",
    "\n",
    "1. Collect human preferences (A is better than B)\n",
    "2. Train a reward model\n",
    "3. Use PPO to optimize policy against reward model\n",
    "4. Deal with training instability, reward hacking, etc.\n",
    "\n",
    "### DPO: A Simpler Alternative\n",
    "\n",
    "DPO **skips the reward model entirely** and directly optimizes the policy using preference data!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELI5: What is DPO?\n",
    "\n",
    "> **Imagine you're training a dog with treats.** Traditional RLHF is like:\n",
    "> 1. First, hire a judge to rate every trick (reward model)\n",
    "> 2. Then, give treats based on the judge's scores (PPO)\n",
    ">\n",
    "> **DPO is simpler:** You just show the dog two tricks and reward whichever one YOU prefer. No judge needed!\n",
    ">\n",
    "> **The math magic:** DPO proves that you can implicitly learn the reward function just from preferences. Instead of:\n",
    "> - Train reward model → Optimize policy with RL\n",
    ">\n",
    "> You get:\n",
    "> - Directly optimize policy with supervised learning on preferences\n",
    ">\n",
    "> **Result:** Same alignment quality, 10x simpler to implement!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The DPO Algorithm\n",
    "\n",
    "### The Math (Simplified)\n",
    "\n",
    "Given:\n",
    "- A prompt $x$\n",
    "- A chosen response $y_w$ (preferred)\n",
    "- A rejected response $y_l$ (not preferred)\n",
    "\n",
    "DPO minimizes:\n",
    "\n",
    "$$\\mathcal{L}_{DPO} = -\\log \\sigma\\left(\\beta \\left[ \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)} \\right]\\right)$$\n",
    "\n",
    "Where:\n",
    "- $\\pi_\\theta$ is your policy (the model being trained)\n",
    "- $\\pi_{ref}$ is the reference model (the SFT model you started from)\n",
    "- $\\beta$ controls how much to prefer chosen over rejected\n",
    "- $\\sigma$ is the sigmoid function\n",
    "\n",
    "### Intuition\n",
    "\n",
    "The loss encourages:\n",
    "- **Increase probability of chosen response** relative to reference\n",
    "- **Decrease probability of rejected response** relative to reference\n",
    "\n",
    "The reference model prevents \"reward hacking\" by penalizing outputs that drift too far from the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import gc\n",
    "from typing import Dict, List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from datasets import Dataset\n",
    "\n",
    "print(\"Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Prepare Preference Dataset\n",
    "\n",
    "DPO needs triplets: (prompt, chosen, rejected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample preference dataset\n",
    "# In practice, you'd use real preference data from human annotations\n",
    "\n",
    "preference_data = [\n",
    "    {\n",
    "        \"prompt\": \"Explain quantum computing in simple terms.\",\n",
    "        \"chosen\": \"Quantum computing uses quantum bits (qubits) that can be both 0 and 1 simultaneously, unlike regular bits. This allows quantum computers to solve certain problems much faster by exploring many possibilities at once. Think of it like checking all paths in a maze simultaneously instead of one at a time.\",\n",
    "        \"rejected\": \"Quantum computing is a type of computation that uses quantum mechanics phenomena such as superposition and entanglement to process information. It operates on quantum bits or qubits.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What's the best way to learn programming?\",\n",
    "        \"chosen\": \"Start with a beginner-friendly language like Python. Build small projects that interest you - a calculator, a todo app, or a simple game. Practice daily, even just 30 minutes. Join communities like Stack Overflow when you get stuck. Remember: every expert was once a beginner!\",\n",
    "        \"rejected\": \"There are many ways to learn programming. You could take online courses, read books, or watch tutorials. Practice is important.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"How do I make my code run faster?\",\n",
    "        \"chosen\": \"Here are key strategies: 1) Profile first - measure before optimizing. 2) Use appropriate data structures (sets for membership, dicts for lookups). 3) Avoid unnecessary loops - use vectorized operations. 4) Cache expensive computations. 5) Consider algorithm complexity (O(n) vs O(n²)).\",\n",
    "        \"rejected\": \"You should optimize your code by making it more efficient.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Write a haiku about programming.\",\n",
    "        \"chosen\": \"Bugs hide in the code\\nDebugging through midnight hours\\nStack trace reveals truth\",\n",
    "        \"rejected\": \"Here is a haiku: Code and programming, Computers do what we say, Technology works.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What are the benefits of exercise?\",\n",
    "        \"chosen\": \"Exercise benefits your whole life: physically (stronger heart, better sleep, more energy), mentally (reduced anxiety, sharper thinking, better mood), and socially (confidence, community). Even 20 minutes of walking daily can transform your health.\",\n",
    "        \"rejected\": \"Exercise is good for health. It makes you stronger and healthier. You should exercise regularly.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create more examples through variation\n",
    "extended_data = []\n",
    "for item in preference_data:\n",
    "    extended_data.append(item)\n",
    "    # Create variations with slight modifications\n",
    "    extended_data.append({\n",
    "        \"prompt\": \"Please \" + item[\"prompt\"].lower(),\n",
    "        \"chosen\": item[\"chosen\"],\n",
    "        \"rejected\": item[\"rejected\"],\n",
    "    })\n",
    "\n",
    "preference_dataset = Dataset.from_list(extended_data)\n",
    "print(f\"Created preference dataset with {len(preference_dataset)} examples\")\n",
    "print(f\"\\nSample:\")\n",
    "print(f\"  Prompt: {preference_dataset[0]['prompt'][:50]}...\")\n",
    "print(f\"  Chosen: {preference_dataset[0]['chosen'][:50]}...\")\n",
    "print(f\"  Rejected: {preference_dataset[0]['rejected'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Load Model for DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Small for demo\n",
    "# For production:\n",
    "# MODEL_NAME = \"Qwen/Qwen3-8B-Instruct\"\n",
    "\n",
    "# DPO hyperparameters\n",
    "DPO_BETA = 0.1  # Controls preference strength (0.1-0.5 typical)\n",
    "LEARNING_RATE = 5e-5\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 2\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "print(f\"DPO Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Beta: {DPO_BETA}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Tokenizer loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(f\"Loading model {MODEL_NAME}...\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(f\"Model loaded!\")\n",
    "print(f\"Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapters\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: DPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPO training configuration\n",
    "dpo_config = DPOConfig(\n",
    "    output_dir=\"./dpo_output\",\n",
    "    \n",
    "    # DPO-specific\n",
    "    beta=DPO_BETA,\n",
    "    \n",
    "    # Training\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    \n",
    "    # Sequence length\n",
    "    max_length=MAX_LENGTH,\n",
    "    max_prompt_length=256,\n",
    "    \n",
    "    # Optimization\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    bf16=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=5,\n",
    "    \n",
    "    # Saving\n",
    "    save_strategy=\"no\",  # Don't save for demo\n",
    "    \n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"DPO config created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DPO trainer\n",
    "# Note: DPO needs a reference model (the model before DPO training)\n",
    "# TRL handles this automatically by creating a copy\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,  # TRL creates reference automatically\n",
    "    args=dpo_config,\n",
    "    train_dataset=preference_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"DPO Trainer created!\")\n",
    "print(f\"\\nMemory after trainer setup: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with DPO!\n",
    "print(\"=\"*50)\n",
    "print(\"STARTING DPO TRAINING\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nThis will teach the model to prefer better responses!\")\n",
    "\n",
    "dpo_result = dpo_trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DPO TRAINING COMPLETE!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print metrics\n",
    "print(\"\\nTraining Metrics:\")\n",
    "for key, value in dpo_result.metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Evaluate DPO Results\n",
    "\n",
    "Let's compare the model's outputs before and after DPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_new_tokens=128):\n",
    "    \"\"\"Generate a response from the model.\"\"\"\n",
    "    # Format prompt\n",
    "    formatted = f\"<|user|>\\n{prompt}</s>\\n<|assistant|>\\n\"\n",
    "    \n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the response\n",
    "    if \"<|assistant|>\" in response:\n",
    "        response = response.split(\"<|assistant|>\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Explain machine learning simply.\",\n",
    "    \"What's the best programming language?\",\n",
    "    \"Give me tips for better sleep.\",\n",
    "]\n",
    "\n",
    "print(\"Testing DPO-trained model:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"-\"*40)\n",
    "    response = generate_response(model, tokenizer, prompt)\n",
    "    print(f\"Response: {response[:300]}...\" if len(response) > 300 else f\"Response: {response}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Understanding Beta\n",
    "\n",
    "The `beta` parameter controls how strongly the model prefers chosen over rejected responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta guidelines\n",
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════╗\n",
    "║                      DPO BETA GUIDELINES                          ║\n",
    "╠══════════════════════════════════════════════════════════════════╣\n",
    "║ Beta Value │ Effect                                              ║\n",
    "╠════════════╪═════════════════════════════════════════════════════╣\n",
    "║ 0.01-0.05  │ Weak preference. Model changes slowly.              ║\n",
    "║ 0.1        │ DEFAULT. Balanced preference learning.              ║\n",
    "║ 0.2-0.3    │ Strong preference. May overfit to style.            ║\n",
    "║ 0.5+       │ Very strong. Risk of mode collapse.                 ║\n",
    "╠══════════════════════════════════════════════════════════════════╣\n",
    "║ Recommendations:                                                 ║\n",
    "║   - Start with beta=0.1                                          ║\n",
    "║   - If model doesn't change enough, increase to 0.2              ║\n",
    "║   - If responses become repetitive, decrease to 0.05             ║\n",
    "║   - Watch for training loss divergence (too high beta)           ║\n",
    "╚══════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Poor Preference Data\n",
    "\n",
    "```python\n",
    "# Wrong: Chosen and rejected are too similar\n",
    "{\n",
    "    \"prompt\": \"What is Python?\",\n",
    "    \"chosen\": \"Python is a programming language.\",\n",
    "    \"rejected\": \"Python is a coding language.\"  # Too similar!\n",
    "}\n",
    "\n",
    "# Right: Clear quality difference\n",
    "{\n",
    "    \"prompt\": \"What is Python?\",\n",
    "    \"chosen\": \"Python is a high-level programming language known for readability. It's great for beginners and experts alike.\",\n",
    "    \"rejected\": \"Python is programming.\"  # Clearly worse\n",
    "}\n",
    "```\n",
    "\n",
    "### Mistake 2: Beta Too High\n",
    "\n",
    "```python\n",
    "# Wrong: Beta too aggressive\n",
    "dpo_config = DPOConfig(beta=1.0)  # Model may collapse!\n",
    "\n",
    "# Right: Start conservative\n",
    "dpo_config = DPOConfig(beta=0.1)  # Then tune if needed\n",
    "```\n",
    "\n",
    "### Mistake 3: Forgetting Reference Model\n",
    "\n",
    "```python\n",
    "# Wrong: Not using reference model\n",
    "# (Training without constraint leads to reward hacking)\n",
    "\n",
    "# Right: TRL handles this automatically with ref_model=None\n",
    "trainer = DPOTrainer(model=model, ref_model=None, ...)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ✅ How DPO aligns models to preferences without a reward model\n",
    "- ✅ How to create preference datasets\n",
    "- ✅ How to train with DPO using TRL\n",
    "- ✅ How to tune the beta parameter\n",
    "- ✅ Common mistakes to avoid\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [DPO Paper](https://arxiv.org/abs/2305.18290) - Direct Preference Optimization\n",
    "- [TRL DPO Documentation](https://huggingface.co/docs/trl/dpo_trainer)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory\n",
    "del model, dpo_trainer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to:\n",
    "\n",
    "**[Lab 3.1.8: SimPO vs ORPO](lab-3.1.8-simpo-vs-orpo.ipynb)** - Compare modern alternatives to DPO that are simpler and more memory-efficient!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
