{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1.9: KTO - Training with Binary Feedback\n",
    "\n",
    "**Module:** 3.1 - Large Language Model Fine-Tuning  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand KTO (Kahneman-Tversky Optimization)\n",
    "- [ ] Train with binary feedback (thumbs up/down)\n",
    "- [ ] Know when KTO is better than DPO\n",
    "- [ ] Implement KTO training with TRL\n",
    "\n",
    "---\n",
    "\n",
    "## The Problem: You Don't Always Have Pairs\n",
    "\n",
    "DPO, SimPO, and ORPO all need **preference pairs**: \"This response is better than that one.\"\n",
    "\n",
    "But often, you only have **binary feedback**:\n",
    "- üëç This response is good\n",
    "- üëé This response is bad\n",
    "\n",
    "Examples:\n",
    "- User ratings (helpful/not helpful)\n",
    "- Flagged content (appropriate/inappropriate)\n",
    "- Click-through data (engaged/bounced)\n",
    "\n",
    "**KTO lets you train directly on this simpler feedback!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELI5: What is KTO?\n",
    "\n",
    "> **Imagine you're training a dog.** \n",
    ">\n",
    "> **DPO** is like: \"Show two tricks, reward the better one.\" But what if you only saw one trick at a time?\n",
    ">\n",
    "> **KTO** is like: \"For each trick, give a thumbs up or thumbs down. The dog learns to do more thumbs-up things and fewer thumbs-down things.\"\n",
    ">\n",
    "> **The clever part:** KTO is based on **Prospect Theory** from behavioral economics (Kahneman & Tversky won a Nobel Prize for this!). It models how humans actually perceive gains and losses:\n",
    "> - Losses hurt more than equivalent gains feel good\n",
    "> - This asymmetry is built into the loss function\n",
    ">\n",
    "> **Result:** A more human-aligned training objective!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The KTO Algorithm\n",
    "\n",
    "### Key Insight: Prospect Theory\n",
    "\n",
    "Humans don't perceive gains and losses equally:\n",
    "- Losing $100 feels worse than winning $100 feels good\n",
    "- This is called **loss aversion**\n",
    "\n",
    "KTO incorporates this into training:\n",
    "\n",
    "$$\\mathcal{L}_{KTO} = \\mathbb{E}_{(x,y)\\sim\\mathcal{D}}\\left[w(y) \\cdot \\left(1 - v_{\\text{KTO}}(x, y; \\beta)\\right)\\right]$$\n",
    "\n",
    "Where:\n",
    "- $w(y)$ is a weighting that makes losses hurt more\n",
    "- $v_{\\text{KTO}}$ is the \"value function\" inspired by Prospect Theory\n",
    "- For desirable outputs: maximize probability relative to reference\n",
    "- For undesirable outputs: minimize probability relative to reference\n",
    "\n",
    "### No Pairs Needed!\n",
    "\n",
    "Unlike DPO which needs (prompt, chosen, rejected) triplets, KTO only needs:\n",
    "- (prompt, response, label) where label ‚àà {desirable, undesirable}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import gc\n",
    "from typing import Dict, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import KTOTrainer, KTOConfig\n",
    "from datasets import Dataset\n",
    "\n",
    "print(\"Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Binary Feedback Dataset\n",
    "\n",
    "KTO uses a simpler format than DPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KTO dataset format: (prompt, completion, label)\n",
    "# label: True = desirable (üëç), False = undesirable (üëé)\n",
    "\n",
    "binary_feedback_data = [\n",
    "    # Good responses (üëç)\n",
    "    {\n",
    "        \"prompt\": \"Explain Python decorators.\",\n",
    "        \"completion\": \"Decorators are functions that modify other functions. They wrap a function to add behavior before/after it runs. Use @decorator syntax above a function definition. Common uses: logging, timing, authentication.\",\n",
    "        \"label\": True\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"How do I handle errors in Python?\",\n",
    "        \"completion\": \"Use try/except blocks: wrap risky code in 'try', handle specific exceptions in 'except'. Always catch specific exceptions (not bare 'except'). Use 'finally' for cleanup. Example: try: file = open('data.txt') except FileNotFoundError: print('File missing')\",\n",
    "        \"label\": True\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What's the difference between a list and tuple?\",\n",
    "        \"completion\": \"Lists are mutable (can change), tuples are immutable (can't change). Lists use [], tuples use (). Tuples are faster and can be dictionary keys. Use lists when you need to modify data, tuples for fixed collections.\",\n",
    "        \"label\": True\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"How do I improve my code quality?\",\n",
    "        \"completion\": \"Write clear names, keep functions small, add tests, use type hints, run linters (like pylint), review your own code, get feedback from others, and refactor regularly. Code is read more than written!\",\n",
    "        \"label\": True\n",
    "    },\n",
    "    \n",
    "    # Bad responses (üëé)\n",
    "    {\n",
    "        \"prompt\": \"Explain Python decorators.\",\n",
    "        \"completion\": \"Decorators are a Python feature.\",\n",
    "        \"label\": False\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"How do I handle errors in Python?\",\n",
    "        \"completion\": \"Just use try except.\",\n",
    "        \"label\": False\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What's the difference between a list and tuple?\",\n",
    "        \"completion\": \"They're both data structures in Python.\",\n",
    "        \"label\": False\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"How do I improve my code quality?\",\n",
    "        \"completion\": \"Practice more.\",\n",
    "        \"label\": False\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What is machine learning?\",\n",
    "        \"completion\": \"It's when computers learn from data to make predictions. Like how Netflix learns your preferences to recommend shows you might like.\",\n",
    "        \"label\": True\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What is machine learning?\",\n",
    "        \"completion\": \"AI stuff.\",\n",
    "        \"label\": False\n",
    "    },\n",
    "]\n",
    "\n",
    "# Expand the dataset\n",
    "expanded_data = []\n",
    "for item in binary_feedback_data:\n",
    "    expanded_data.append(item)\n",
    "    # Add variation\n",
    "    expanded_data.append({\n",
    "        \"prompt\": \"Please \" + item[\"prompt\"].lower(),\n",
    "        \"completion\": item[\"completion\"],\n",
    "        \"label\": item[\"label\"],\n",
    "    })\n",
    "\n",
    "kto_dataset = Dataset.from_list(expanded_data)\n",
    "\n",
    "# Count labels\n",
    "n_positive = sum(1 for item in expanded_data if item[\"label\"])\n",
    "n_negative = len(expanded_data) - n_positive\n",
    "\n",
    "print(f\"KTO Dataset: {len(kto_dataset)} examples\")\n",
    "print(f\"  üëç Desirable: {n_positive}\")\n",
    "print(f\"  üëé Undesirable: {n_negative}\")\n",
    "print(f\"\\nNote: No preference pairs needed! Just binary labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Load Model for KTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# KTO-specific parameters\n",
    "KTO_BETA = 0.1  # Controls strength of preference\n",
    "DESIRABLE_WEIGHT = 1.0  # Weight for positive examples\n",
    "UNDESIRABLE_WEIGHT = 1.0  # Weight for negative examples\n",
    "\n",
    "print(f\"KTO Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Beta: {KTO_BETA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer and model\n",
    "print(f\"Loading model {MODEL_NAME}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Add LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(f\"\\nModel loaded!\")\n",
    "model.print_trainable_parameters()\n",
    "print(f\"Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: KTO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KTO Configuration\n",
    "kto_config = KTOConfig(\n",
    "    output_dir=\"./kto_output\",\n",
    "    \n",
    "    # KTO-specific\n",
    "    beta=KTO_BETA,\n",
    "    desirable_weight=DESIRABLE_WEIGHT,\n",
    "    undesirable_weight=UNDESIRABLE_WEIGHT,\n",
    "    \n",
    "    # Training\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-5,\n",
    "    \n",
    "    # Sequence\n",
    "    max_length=512,\n",
    "    max_prompt_length=256,\n",
    "    \n",
    "    # Optimization\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    bf16=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=5,\n",
    "    \n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"KTO configuration created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create KTO trainer\n",
    "kto_trainer = KTOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,  # TRL creates reference automatically\n",
    "    args=kto_config,\n",
    "    train_dataset=kto_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"KTO Trainer created!\")\n",
    "print(f\"Memory after setup: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with KTO!\n",
    "print(\"=\"*50)\n",
    "print(\"STARTING KTO TRAINING\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nTraining with binary feedback (thumbs up/down)!\")\n",
    "\n",
    "kto_result = kto_trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"KTO TRAINING COMPLETE!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print metrics\n",
    "print(\"\\nKTO Training Metrics:\")\n",
    "for key, value in kto_result.metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: When to Use KTO\n",
    "\n",
    "### Decision Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                           WHEN TO USE KTO                                     ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïë  ‚úÖ USE KTO when you have:                                                   ‚ïë\n",
    "‚ïë    ‚Ä¢ Binary feedback (helpful/not helpful)                                   ‚ïë\n",
    "‚ïë    ‚Ä¢ User ratings (thumbs up/down)                                           ‚ïë\n",
    "‚ïë    ‚Ä¢ Flagged content (appropriate/inappropriate)                             ‚ïë\n",
    "‚ïë    ‚Ä¢ Click-through data (engaged/bounced)                                    ‚ïë\n",
    "‚ïë    ‚Ä¢ Any label that's True/False, not comparative                            ‚ïë\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïë  ‚ùå USE DPO/SimPO/ORPO when you have:                                        ‚ïë\n",
    "‚ïë    ‚Ä¢ Side-by-side comparisons (A is better than B)                           ‚ïë\n",
    "‚ïë    ‚Ä¢ Ranked responses (best to worst)                                        ‚ïë\n",
    "‚ïë    ‚Ä¢ Elo ratings from comparisons                                            ‚ïë\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë  KTO Advantages:                                                             ‚ïë\n",
    "‚ïë    ‚Ä¢ Works with simpler data (no pairs needed)                               ‚ïë\n",
    "‚ïë    ‚Ä¢ Based on human behavioral economics (Prospect Theory)                   ‚ïë\n",
    "‚ïë    ‚Ä¢ Handles imbalanced positive/negative ratios well                        ‚ïë\n",
    "‚ïë    ‚Ä¢ Can mix data from different sources more easily                         ‚ïë\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïë  KTO Considerations:                                                         ‚ïë\n",
    "‚ïë    ‚Ä¢ May need more data than DPO for same quality                            ‚ïë\n",
    "‚ïë    ‚Ä¢ Still needs reference model (like DPO)                                  ‚ïë\n",
    "‚ïë    ‚Ä¢ Newer method, less battle-tested                                        ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Converting Between Formats\n",
    "\n",
    "You can convert preference pairs to binary format (but not vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preference_pairs_to_binary(preference_data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Convert DPO-style preference pairs to KTO binary format.\n",
    "    \n",
    "    Input: [{\"prompt\": str, \"chosen\": str, \"rejected\": str}, ...]\n",
    "    Output: [{\"prompt\": str, \"completion\": str, \"label\": bool}, ...]\n",
    "    \"\"\"\n",
    "    binary_data = []\n",
    "    \n",
    "    for item in preference_data:\n",
    "        # Chosen response = desirable\n",
    "        binary_data.append({\n",
    "            \"prompt\": item[\"prompt\"],\n",
    "            \"completion\": item[\"chosen\"],\n",
    "            \"label\": True,\n",
    "        })\n",
    "        \n",
    "        # Rejected response = undesirable\n",
    "        binary_data.append({\n",
    "            \"prompt\": item[\"prompt\"],\n",
    "            \"completion\": item[\"rejected\"],\n",
    "            \"label\": False,\n",
    "        })\n",
    "    \n",
    "    return binary_data\n",
    "\n",
    "\n",
    "# Example\n",
    "dpo_data = [\n",
    "    {\n",
    "        \"prompt\": \"What is Python?\",\n",
    "        \"chosen\": \"Python is a versatile programming language known for readability.\",\n",
    "        \"rejected\": \"It's a coding thing.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "kto_data = preference_pairs_to_binary(dpo_data)\n",
    "print(\"Converted DPO ‚Üí KTO:\")\n",
    "for item in kto_data:\n",
    "    label = \"üëç\" if item[\"label\"] else \"üëé\"\n",
    "    print(f\"  {label} {item['completion'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Imbalanced Labels\n",
    "\n",
    "```python\n",
    "# Wrong: 90% positive, 10% negative\n",
    "# Model may ignore negative examples\n",
    "\n",
    "# Right: Balance with weights or sampling\n",
    "kto_config = KTOConfig(\n",
    "    desirable_weight=1.0,\n",
    "    undesirable_weight=9.0,  # Boost negative importance\n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "### Mistake 2: Weak Negative Examples\n",
    "\n",
    "```python\n",
    "# Wrong: Negative is just shorter version of positive\n",
    "{\"prompt\": \"Explain X\", \"completion\": \"X is...\", \"label\": True}\n",
    "{\"prompt\": \"Explain X\", \"completion\": \"X.\", \"label\": False}\n",
    "# Model just learns \"longer = better\"\n",
    "\n",
    "# Right: Negative has clear quality issues\n",
    "{\"prompt\": \"Explain X\", \"completion\": \"X is... (helpful detail)\", \"label\": True}\n",
    "{\"prompt\": \"Explain X\", \"completion\": \"I don't know.\", \"label\": False}\n",
    "# Model learns actual quality differences\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ KTO trains with binary feedback (no pairs needed)\n",
    "- ‚úÖ It's based on Prospect Theory from behavioral economics\n",
    "- ‚úÖ When to choose KTO over DPO/SimPO\n",
    "- ‚úÖ How to implement KTO with TRL\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [KTO Paper](https://arxiv.org/abs/2402.01306) - Kahneman-Tversky Optimization\n",
    "- [Prospect Theory](https://en.wikipedia.org/wiki/Prospect_theory) - The psychology behind KTO\n",
    "- [TRL KTO Documentation](https://huggingface.co/docs/trl/kto_trainer)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory\n",
    "del model, kto_trainer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to:\n",
    "\n",
    "**[Lab 3.1.10: Ollama Integration](lab-3.1.10-ollama-integration.ipynb)** - Deploy your fine-tuned model with Ollama!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
