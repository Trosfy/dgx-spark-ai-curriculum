{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 10.3: Fine-Tuning Llama 3.1 70B with QLoRA\n",
    "\n",
    "**Module:** 10 - Large Language Model Fine-Tuning  \n",
    "**Time:** 4 hours  \n",
    "**Difficulty:** ⭐⭐⭐⭐☆\n",
    "\n",
    "## This is the DGX Spark Showcase!\n",
    "\n",
    "You're about to do something that would normally require:\n",
    "- **4x NVIDIA A100 80GB** (~$60,000+), or\n",
    "- **8x NVIDIA RTX 4090** (~$16,000+), or  \n",
    "- **Cloud GPU rental** ($20-50/hour)\n",
    "\n",
    "**On your DGX Spark**, you'll fine-tune a 70-billion parameter model on a single desktop machine. This is what makes DGX Spark special!\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Clear system cache before loading large models\n",
    "- [ ] Load Llama 3.1 70B with QLoRA configuration\n",
    "- [ ] Monitor and document memory usage throughout the process\n",
    "- [ ] Fine-tune the 70B model on a custom dataset\n",
    "- [ ] Understand what makes DGX Spark unique for this task\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Task 10.1 (LoRA Theory) and Task 10.2 (8B Fine-tuning)\n",
    "- Hardware: DGX Spark with 128GB unified memory\n",
    "- Model access: Meta Llama 3.1 70B (request at meta.ai)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELI5: What is QLoRA?\n",
    "\n",
    "> **Remember our chef analogy from the LoRA notebook?** QLoRA adds another trick.\n",
    ">\n",
    "> **LoRA:** Give the chef a small recipe card instead of retraining them.\n",
    ">\n",
    "> **QLoRA:** Also compress the chef's entire cookbook (base weights) into a tiny, highly-efficient format while keeping the recipe card (adapters) in high quality.\n",
    ">\n",
    "> **The technical trick:** \n",
    "> - Base model weights are stored in 4-bit precision (NF4 format)\n",
    "> - LoRA adapters stay in higher precision (bfloat16)\n",
    "> - Computations happen in bfloat16 for accuracy\n",
    "> - Double quantization further reduces memory overhead\n",
    ">\n",
    "> **Result:** A 70B model that normally needs 140GB (in float16) fits in ~35-45GB!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Budget Analysis\n",
    "\n",
    "Let's understand why this works on DGX Spark:\n",
    "\n",
    "| Component | FP16 Memory | QLoRA Memory |\n",
    "|-----------|------------|---------------|\n",
    "| 70B Model Weights | ~140 GB | **~35 GB** (4-bit) |\n",
    "| LoRA Parameters | - | ~0.5 GB |\n",
    "| Gradients (LoRA only) | - | ~1 GB |\n",
    "| Optimizer States | - | ~4 GB |\n",
    "| Activations (with gradient checkpointing) | - | ~5-10 GB |\n",
    "| **Total** | **~400+ GB** | **~45-55 GB** |\n",
    "\n",
    "**DGX Spark Unified Memory: 128GB** - plenty of room!\n",
    "\n",
    "### Why Consumer GPUs Can't Do This\n",
    "\n",
    "- RTX 4090: 24GB VRAM - can't fit the model\n",
    "- Multi-GPU setup: Requires complex tensor parallelism, PCIe bandwidth limitations\n",
    "- CPU offloading: Works but 10-100x slower\n",
    "\n",
    "**DGX Spark's unified memory** means the full model can be accessed efficiently without complex distribution strategies!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: System Preparation (CRITICAL)\n",
    "\n",
    "Before loading a 70B model, we **MUST** clear the system buffer cache. This is critical!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: Clear buffer cache before loading large models\n",
    "import subprocess\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "\n",
    "def clear_buffer_cache():\n",
    "    \"\"\"\n",
    "    Clear Linux buffer cache to free up memory for model loading.\n",
    "    This is CRITICAL for loading 70B models!\n",
    "    \n",
    "    Note: Requires sudo access. If you get permission errors,\n",
    "    run this command manually in terminal:\n",
    "    sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'\n",
    "    \"\"\"\n",
    "    print(\"Clearing buffer cache...\")\n",
    "    try:\n",
    "        # Sync filesystem\n",
    "        subprocess.run([\"sync\"], check=True)\n",
    "        # Drop caches\n",
    "        subprocess.run(\n",
    "            [\"sudo\", \"sh\", \"-c\", \"echo 3 > /proc/sys/vm/drop_caches\"],\n",
    "            check=True\n",
    "        )\n",
    "        print(\"Buffer cache cleared successfully!\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Warning: Could not clear cache automatically: {e}\")\n",
    "        print(\"Please run this command manually:\")\n",
    "        print(\"  sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'\")\n",
    "\n",
    "# Clear Python garbage collector\n",
    "gc.collect()\n",
    "\n",
    "# Clear buffer cache\n",
    "clear_buffer_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Memory monitoring utilities\nimport torch\nfrom datetime import datetime\n\n# Graceful import for psutil - may not be installed in all NGC container versions\ntry:\n    import psutil\n    PSUTIL_AVAILABLE = True\nexcept ImportError:\n    PSUTIL_AVAILABLE = False\n    print(\"Note: psutil not available. Install with 'pip install psutil' for system memory monitoring.\")\n    print(\"GPU memory monitoring will still work.\")\n\ndef get_memory_info():\n    \"\"\"\n    Get comprehensive memory information.\n    \"\"\"\n    info = {\n        'timestamp': datetime.now().strftime('%H:%M:%S'),\n    }\n    \n    # System memory (if psutil available)\n    if PSUTIL_AVAILABLE:\n        sys_mem = psutil.virtual_memory()\n        info['system_total_gb'] = sys_mem.total / 1e9\n        info['system_used_gb'] = sys_mem.used / 1e9\n        info['system_available_gb'] = sys_mem.available / 1e9\n    else:\n        # Fallback: estimate from /proc/meminfo on Linux\n        try:\n            with open('/proc/meminfo', 'r') as f:\n                meminfo = f.read()\n            mem_total = int([x for x in meminfo.split('\\n') if 'MemTotal' in x][0].split()[1]) * 1024\n            mem_avail = int([x for x in meminfo.split('\\n') if 'MemAvailable' in x][0].split()[1]) * 1024\n            info['system_total_gb'] = mem_total / 1e9\n            info['system_available_gb'] = mem_avail / 1e9\n            info['system_used_gb'] = (mem_total - mem_avail) / 1e9\n        except Exception:\n            info['system_total_gb'] = 0\n            info['system_used_gb'] = 0\n            info['system_available_gb'] = 0\n    \n    # GPU memory\n    if torch.cuda.is_available():\n        info['gpu_allocated_gb'] = torch.cuda.memory_allocated() / 1e9\n        info['gpu_reserved_gb'] = torch.cuda.memory_reserved() / 1e9\n        \n        # Get device properties for total memory\n        props = torch.cuda.get_device_properties(0)\n        info['gpu_total_gb'] = props.total_memory / 1e9\n    \n    return info\n\ndef print_memory_status(label: str = \"\"):\n    \"\"\"\n    Print formatted memory status.\n    \"\"\"\n    info = get_memory_info()\n    print(f\"\\n{'='*60}\")\n    print(f\"Memory Status: {label} ({info['timestamp']})\")\n    print(f\"{'='*60}\")\n    print(f\"System RAM: {info['system_used_gb']:.1f} / {info['system_total_gb']:.1f} GB used\")\n    print(f\"System Available: {info['system_available_gb']:.1f} GB\")\n    if 'gpu_allocated_gb' in info:\n        print(f\"GPU Allocated: {info['gpu_allocated_gb']:.1f} GB\")\n        print(f\"GPU Reserved: {info['gpu_reserved_gb']:.1f} GB\")\n        print(f\"GPU Total: {info['gpu_total_gb']:.1f} GB\")\n    print(f\"{'='*60}\\n\")\n    return info\n\n# Track memory throughout the notebook\nmemory_log = []\n\ndef log_memory(label: str):\n    \"\"\"Log memory status for later analysis.\"\"\"\n    info = print_memory_status(label)\n    info['label'] = label\n    memory_log.append(info)\n\n# Initial status\nlog_memory(\"Initial (after cache clear)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System information\n",
    "import torch\n",
    "\n",
    "print(\"System Information:\")\n",
    "print(f\"  PyTorch version: {torch.__version__}\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name()}\")\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"  Total GPU Memory: {props.total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"  CUDA Capability: {props.major}.{props.minor}\")\n",
    "    print(f\"  Multiprocessors: {props.multi_processor_count}\")\n",
    "\n",
    "# Set environment variables for optimal performance\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Loading the 70B Model with QLoRA\n",
    "\n",
    "This is the moment of truth! We'll load a 70-billion parameter model into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import warnings\n# Suppress verbose warnings from transformers/PEFT/bitsandbytes that clutter output\n# These warnings don't affect functionality - we filter them for cleaner notebooks\nwarnings.filterwarnings('ignore')\n\nimport time  # Required for timing model loading and training\n\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import Dataset\nfrom trl import SFTTrainer\n\n# Try Unsloth for faster training\ntry:\n    from unsloth import FastLanguageModel\n    USE_UNSLOTH = True\n    print(\"Unsloth available - will use for faster training!\")\nexcept ImportError:\n    USE_UNSLOTH = False\n    print(\"Using standard transformers (Unsloth not available)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration for 70B\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-70B-Instruct\"\n",
    "# Alternative: \"unsloth/Llama-3.1-70B-Instruct-bnb-4bit\" (pre-quantized)\n",
    "\n",
    "MAX_SEQ_LENGTH = 2048  # Can go up to 4096, but uses more memory\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(\"\\nThis is a 70B parameter model - loading will take several minutes...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # 4-bit quantization\n",
    "    bnb_4bit_quant_type=\"nf4\",            # NormalFloat4 - optimal for normally distributed weights\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Compute in bfloat16 for Blackwell\n",
    "    bnb_4bit_use_double_quant=True,       # Quantize the quantization constants too!\n",
    ")\n",
    "\n",
    "print(\"QLoRA Configuration:\")\n",
    "print(f\"  4-bit quantization: {bnb_config.load_in_4bit}\")\n",
    "print(f\"  Quantization type: {bnb_config.bnb_4bit_quant_type}\")\n",
    "print(f\"  Compute dtype: {bnb_config.bnb_4bit_compute_dtype}\")\n",
    "print(f\"  Double quantization: {bnb_config.bnb_4bit_use_double_quant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the 70B model - this is the big moment!\nprint(\"\\n\" + \"=\"*60)\nprint(\"LOADING 70B MODEL - This will take 5-15 minutes...\")\nprint(\"=\"*60 + \"\\n\")\n\nload_start = time.time()\nlog_memory(\"Before loading model\")\n\nif USE_UNSLOTH:\n    # Unsloth path - faster loading and training\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name=MODEL_NAME,\n        max_seq_length=MAX_SEQ_LENGTH,\n        dtype=torch.bfloat16,\n        load_in_4bit=True,\n    )\nelse:\n    # Standard transformers path\n    print(\"Loading tokenizer...\")\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"\n    \n    print(\"Loading model with QLoRA quantization...\")\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        torch_dtype=torch.bfloat16,\n        low_cpu_mem_usage=True,  # Important for large models\n        # Note: trust_remote_code not needed for official Llama models\n    )\n    \n    # Prepare for k-bit training\n    model = prepare_model_for_kbit_training(model)\n\nload_time = time.time() - load_start\nprint(f\"\\nModel loaded in {load_time/60:.1f} minutes!\")\n\nlog_memory(\"After loading model\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the model loaded correctly\n",
    "print(\"Model Information:\")\n",
    "print(f\"  Model type: {type(model).__name__}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Tokenizer vocabulary: {len(tokenizer):,}\")\n",
    "\n",
    "# Quick inference test\n",
    "print(\"\\nQuick inference test...\")\n",
    "test_input = tokenizer(\"The capital of France is\", return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**test_input, max_new_tokens=10, do_sample=False)\n",
    "print(f\"Test output: {tokenizer.decode(output[0], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Celebrate!\n",
    "\n",
    "You just loaded a **70 billion parameter model** on a single desktop GPU! This is remarkable - let's document the memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document the memory achievement\n",
    "mem_info = get_memory_info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DGX SPARK ACHIEVEMENT: 70B Model Loaded!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nGPU Memory Used: {mem_info.get('gpu_allocated_gb', 0):.1f} GB\")\n",
    "print(f\"GPU Memory Reserved: {mem_info.get('gpu_reserved_gb', 0):.1f} GB\")\n",
    "print(f\"System RAM Available: {mem_info.get('system_available_gb', 0):.1f} GB\")\n",
    "print(\"\\nThis would require:\")\n",
    "print(\"  - 4x A100 80GB GPUs ($60,000+)\")\n",
    "print(\"  - 8x RTX 4090 GPUs ($16,000+)\")\n",
    "print(\"  - Cloud rental at $20-50/hour\")\n",
    "print(\"\\nBut you're doing it on a single DGX Spark!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Adding LoRA Adapters\n",
    "\n",
    "Now we add the trainable LoRA adapters. For 70B, we'll use a conservative configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration for 70B - optimized for memory\n",
    "LORA_CONFIG_70B = {\n",
    "    \"r\": 16,                # Rank - 16 is good balance for 70B\n",
    "    \"lora_alpha\": 32,       # Scaling factor\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"target_modules\": [\n",
    "        \"q_proj\",           # Query projection\n",
    "        \"k_proj\",           # Key projection\n",
    "        \"v_proj\",           # Value projection\n",
    "        \"o_proj\",           # Output projection\n",
    "        # For 70B, we skip MLP layers to save memory\n",
    "        # Uncomment if you have memory headroom:\n",
    "        # \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "}\n",
    "\n",
    "print(\"LoRA Configuration for 70B:\")\n",
    "for k, v in LORA_CONFIG_70B.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to the model\n",
    "log_memory(\"Before adding LoRA\")\n",
    "\n",
    "if USE_UNSLOTH:\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=LORA_CONFIG_70B[\"r\"],\n",
    "        lora_alpha=LORA_CONFIG_70B[\"lora_alpha\"],\n",
    "        lora_dropout=LORA_CONFIG_70B[\"lora_dropout\"],\n",
    "        target_modules=LORA_CONFIG_70B[\"target_modules\"],\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=42,\n",
    "    )\n",
    "else:\n",
    "    lora_config = LoraConfig(**LORA_CONFIG_70B)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.enable_input_require_grads()\n",
    "\n",
    "log_memory(\"After adding LoRA\")\n",
    "\n",
    "# Print parameter counts\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"\\nParameter Summary:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Trainable %: {100 * trainable_params / total_params:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Dataset Preparation\n",
    "\n",
    "We'll use a small sample dataset for demonstration. In practice, use your domain-specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training data - replace with your own!\n",
    "training_examples = [\n",
    "    {\n",
    "        \"instruction\": \"Explain quantum computing in simple terms.\",\n",
    "        \"response\": \"Quantum computing uses quantum mechanics to process information in fundamentally different ways than classical computers. While regular computers use bits (0 or 1), quantum computers use qubits that can be 0, 1, or both at once (superposition). This allows them to explore many solutions simultaneously, making them potentially powerful for specific problems like cryptography and molecular simulation.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What are the key differences between GPT and BERT architectures?\",\n",
    "        \"response\": \"GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers) differ in three main ways: 1) Direction - GPT is unidirectional (left-to-right), while BERT is bidirectional, processing text in both directions. 2) Training objective - GPT predicts the next token, while BERT uses masked language modeling. 3) Use cases - GPT excels at text generation, while BERT is better for understanding and classification tasks.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"How does backpropagation work in neural networks?\",\n",
    "        \"response\": \"Backpropagation calculates how to adjust neural network weights to reduce prediction error. It works in two phases: 1) Forward pass - input flows through the network to produce output. 2) Backward pass - the error is propagated backwards, computing gradients using the chain rule. Each weight is updated proportionally to how much it contributed to the error, allowing the network to learn from mistakes.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is the attention mechanism and why is it important?\",\n",
    "        \"response\": \"The attention mechanism allows neural networks to focus on relevant parts of the input when producing output. Instead of compressing all input into a fixed-size vector, attention computes weighted sums where weights indicate importance. This enables models to handle long sequences, capture long-range dependencies, and provides interpretability by showing what the model 'attends to'. It's the foundation of transformers and modern NLP.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain the concept of transfer learning.\",\n",
    "        \"response\": \"Transfer learning reuses knowledge from one task to help with another related task. Instead of training from scratch, you take a model pre-trained on a large dataset (like ImageNet or text corpora) and adapt it to your specific task. This works because early layers learn general features applicable to many tasks. Benefits include: faster training, better performance with limited data, and reduced computational requirements.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is the difference between L1 and L2 regularization?\",\n",
    "        \"response\": \"L1 (Lasso) and L2 (Ridge) regularization both prevent overfitting by penalizing large weights, but differently: L1 adds the absolute value of weights to the loss (|w|), encouraging sparse models with some weights exactly zero - useful for feature selection. L2 adds squared weights (w²), encouraging small but non-zero weights - better when all features are relevant. L1 produces simpler models; L2 handles correlated features better.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"How do transformers handle positional information?\",\n",
    "        \"response\": \"Transformers use positional encoding because self-attention is permutation-invariant - it doesn't inherently know token order. Two main approaches: 1) Sinusoidal positional encoding adds sine/cosine functions of different frequencies to embeddings, allowing the model to learn relative positions. 2) Learned positional embeddings train position vectors directly. Modern variants include RoPE (rotary position embedding) and ALiBi, which handle longer sequences better.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What are the main challenges in training very large language models?\",\n",
    "        \"response\": \"Training large language models faces several challenges: 1) Memory - models with billions of parameters require distributed training across many GPUs. 2) Compute cost - training GPT-4 scale models costs millions of dollars in compute. 3) Data quality - need massive high-quality datasets. 4) Stability - larger models can have training instabilities requiring careful hyperparameter tuning. 5) Evaluation - it's hard to comprehensively evaluate capabilities and limitations.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Created {len(training_examples)} training examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format for Llama 3.1 Instruct\n",
    "def format_example_llama3(example: dict) -> str:\n",
    "    \"\"\"Format a single example in Llama 3.1 chat format.\"\"\"\n",
    "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are an expert AI assistant specializing in machine learning and deep learning. Provide clear, accurate, and educational explanations.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{example['instruction']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{example['response']}<|eot_id|>\"\"\"\n",
    "\n",
    "# Create dataset\n",
    "formatted_data = [{\"text\": format_example_llama3(ex)} for ex in training_examples]\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "# For 70B, we'll use all data for training (small dataset)\n",
    "train_dataset = dataset\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"\\nSample formatted example:\")\n",
    "print(\"=\"*50)\n",
    "print(formatted_data[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Training Configuration\n",
    "\n",
    "We need to be careful with training parameters for 70B to manage memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration optimized for 70B on DGX Spark\n",
    "OUTPUT_DIR = \"./llama3-70b-qlora-finetuned\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # Batch size - CRITICAL for memory!\n",
    "    per_device_train_batch_size=1,        # Keep at 1 for 70B\n",
    "    gradient_accumulation_steps=8,         # Effective batch = 8\n",
    "    \n",
    "    # Training duration\n",
    "    num_train_epochs=1,                    # Start with 1 epoch\n",
    "    max_steps=50,                          # Or limit steps for demo\n",
    "    \n",
    "    # Optimizer - use 8-bit Adam to save memory\n",
    "    learning_rate=1e-4,                    # Slightly lower for 70B\n",
    "    optim=\"adamw_8bit\",                    # 8-bit optimizer!\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # Memory optimization - CRITICAL\n",
    "    bf16=True,                             # Use bfloat16\n",
    "    fp16=False,\n",
    "    gradient_checkpointing=True,           # Trade compute for memory\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,\n",
    "    save_total_limit=1,                    # Save only 1 checkpoint\n",
    "    \n",
    "    # Other\n",
    "    seed=42,\n",
    "    report_to=\"none\",\n",
    "    dataloader_pin_memory=False,           # Reduce memory overhead\n",
    ")\n",
    "\n",
    "print(\"Training Configuration for 70B:\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Optimizer: {training_args.optim}\")\n",
    "print(f\"  Max steps: {training_args.max_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "log_memory(\"Before creating trainer\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    args=training_args,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "log_memory(\"After creating trainer\")\n",
    "print(\"Trainer created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Training!\n",
    "\n",
    "This is it - training a 70B model on your desktop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN THE 70B MODEL!\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING 70B MODEL - This is the DGX Spark showcase!\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "log_memory(\"Before training\")\n",
    "\n",
    "train_start = time.time()\n",
    "\n",
    "# Train!\n",
    "train_result = trainer.train()\n",
    "\n",
    "train_time = time.time() - train_start\n",
    "\n",
    "log_memory(\"After training\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"TRAINING COMPLETE!\")\n",
    "print(f\"Total time: {train_time/60:.1f} minutes\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "metrics = train_result.metrics\n",
    "\n",
    "print(\"\\nTraining Metrics:\")\n",
    "print(f\"  Total steps: {metrics.get('total_steps', 'N/A')}\")\n",
    "print(f\"  Training loss: {metrics.get('train_loss', 'N/A'):.4f}\")\n",
    "print(f\"  Runtime: {metrics.get('train_runtime', 'N/A'):.1f} seconds\")\n",
    "print(f\"  Samples/second: {metrics.get('train_samples_per_second', 'N/A'):.3f}\")\n",
    "print(f\"  Steps/second: {metrics.get('train_steps_per_second', 'N/A'):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Testing the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put model in inference mode\n",
    "if USE_UNSLOTH:\n",
    "    FastLanguageModel.for_inference(model)\n",
    "model.eval()\n",
    "\n",
    "def generate_70b_response(prompt: str, max_new_tokens: int = 200) -> str:\n",
    "    \"\"\"Generate response from the fine-tuned 70B model.\"\"\"\n",
    "    formatted = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are an expert AI assistant specializing in machine learning and deep learning.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model\n",
    "test_prompts = [\n",
    "    \"What is the difference between CNN and RNN architectures?\",\n",
    "    \"Explain how dropout works as regularization.\",\n",
    "    \"What makes LoRA an efficient fine-tuning method?\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Testing Fine-Tuned 70B Model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nQ: {prompt}\")\n",
    "    print(\"-\"*50)\n",
    "    response = generate_70b_response(prompt)\n",
    "    print(f\"A: {response}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Memory Analysis Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create memory usage visualization\nimport matplotlib.pyplot as plt\n\n# Extract memory data\nlabels = [m['label'] for m in memory_log]\ngpu_allocated = [m.get('gpu_allocated_gb', 0) for m in memory_log]\ngpu_reserved = [m.get('gpu_reserved_gb', 0) for m in memory_log]\n\nfig, ax = plt.subplots(figsize=(14, 6))\n\nx = range(len(labels))\nwidth = 0.35\n\nbars1 = ax.bar([i - width/2 for i in x], gpu_allocated, width, label='Allocated', color='steelblue')\nbars2 = ax.bar([i + width/2 for i in x], gpu_reserved, width, label='Reserved', color='lightsteelblue')\n\n# Add 128GB line for reference\nax.axhline(y=128, color='red', linestyle='--', label='DGX Spark Capacity (128GB)')\n\nax.set_ylabel('GPU Memory (GB)')\nax.set_title('70B Model QLoRA Training: Memory Usage Over Time')\nax.set_xticks(x)\nax.set_xticklabels(labels, rotation=45, ha='right')\nax.legend()\nax.set_ylim(0, 140)\n\n# Add value labels on bars\nfor bar in bars1:\n    height = bar.get_height()\n    ax.annotate(f'{height:.1f}',\n                xy=(bar.get_x() + bar.get_width() / 2, height),\n                xytext=(0, 3),\n                textcoords=\"offset points\",\n                ha='center', va='bottom', fontsize=8)\n\nplt.tight_layout()\nplt.savefig('70b_memory_analysis.png', dpi=150, bbox_inches='tight')\nplt.show()\nplt.close(fig)  # Release memory\n\nprint(\"\\nMemory usage saved to 70b_memory_analysis.png\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DGX SPARK 70B FINE-TUNING - FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "final_mem = get_memory_info()\n",
    "\n",
    "print(f\"\"\"\n",
    "Model: {MODEL_NAME}\n",
    "Total Parameters: ~70 billion\n",
    "Trainable Parameters: {trainable_params:,} ({100*trainable_params/total_params:.4f}%)\n",
    "\n",
    "Memory Usage:\n",
    "  Peak GPU Allocated: ~{max(m.get('gpu_allocated_gb', 0) for m in memory_log):.1f} GB\n",
    "  Peak GPU Reserved: ~{max(m.get('gpu_reserved_gb', 0) for m in memory_log):.1f} GB\n",
    "  DGX Spark Capacity: 128 GB\n",
    "  Headroom: ~{128 - max(m.get('gpu_reserved_gb', 0) for m in memory_log):.1f} GB\n",
    "\n",
    "Training:\n",
    "  Total Time: {train_time/60:.1f} minutes\n",
    "  Steps Completed: {metrics.get('total_steps', 'N/A')}\n",
    "  Final Loss: {metrics.get('train_loss', 'N/A'):.4f}\n",
    "\n",
    "What This Would Cost Elsewhere:\n",
    "  Cloud (4x A100): ~$30-50/hour\n",
    "  Hardware (4x A100): ~$60,000+\n",
    "  Consumer (8x 4090): Not possible (memory limitations)\n",
    "\n",
    "On DGX Spark: Your desktop. No cloud bills. No waiting.\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapter (small, ~100MB)\n",
    "ADAPTER_PATH = \"./llama3-70b-qlora-adapter\"\n",
    "\n",
    "model.save_pretrained(ADAPTER_PATH)\n",
    "tokenizer.save_pretrained(ADAPTER_PATH)\n",
    "\n",
    "print(f\"LoRA adapter saved to {ADAPTER_PATH}\")\n",
    "\n",
    "# Check size\n",
    "adapter_size = sum(\n",
    "    os.path.getsize(os.path.join(ADAPTER_PATH, f))\n",
    "    for f in os.listdir(ADAPTER_PATH)\n",
    "    if os.path.isfile(os.path.join(ADAPTER_PATH, f))\n",
    ")\n",
    "print(f\"Adapter size: {adapter_size / 1e6:.1f} MB\")\n",
    "print(f\"\\nTo load later:\")\n",
    "print(f\"  1. Load base model with QLoRA config\")\n",
    "print(f\"  2. Load adapter: PeftModel.from_pretrained(base_model, '{ADAPTER_PATH}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes with 70B Models\n",
    "\n",
    "### Mistake 1: Not Clearing Buffer Cache\n",
    "\n",
    "```python\n",
    "# ❌ Wrong: Loading without clearing cache\n",
    "model = AutoModelForCausalLM.from_pretrained(...)  # OOM!\n",
    "\n",
    "# ✅ Right: Clear cache first\n",
    "subprocess.run([\"sudo\", \"sh\", \"-c\", \"sync; echo 3 > /proc/sys/vm/drop_caches\"])\n",
    "model = AutoModelForCausalLM.from_pretrained(...)\n",
    "```\n",
    "\n",
    "### Mistake 2: Batch Size Too Large\n",
    "\n",
    "```python\n",
    "# ❌ Wrong: Using 8B settings\n",
    "per_device_train_batch_size = 4  # OOM on 70B!\n",
    "\n",
    "# ✅ Right: Minimal batch size with accumulation\n",
    "per_device_train_batch_size = 1\n",
    "gradient_accumulation_steps = 8\n",
    "```\n",
    "\n",
    "### Mistake 3: Not Using 8-bit Optimizer\n",
    "\n",
    "```python\n",
    "# ❌ Wrong: Standard optimizer\n",
    "optim = \"adamw_torch\"  # Uses too much memory for 70B\n",
    "\n",
    "# ✅ Right: 8-bit optimizer\n",
    "optim = \"adamw_8bit\"  # Significantly reduces memory\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've achieved:\n",
    "- ✅ Loaded a 70B parameter model with QLoRA quantization\n",
    "- ✅ Documented memory usage throughout the process\n",
    "- ✅ Fine-tuned the model on custom data\n",
    "- ✅ Saved the fine-tuned adapter for later use\n",
    "- ✅ Understood what makes DGX Spark special for this task\n",
    "\n",
    "**Congratulations!** You've done something that most ML practitioners never get to experience - fine-tuning a 70B model on desktop hardware!\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del model, tokenizer, trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print_memory_status(\"After cleanup\")\n",
    "print(\"Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you've mastered 70B fine-tuning:\n",
    "\n",
    "1. **[Task 10.4: Dataset Preparation](04-dataset-preparation.ipynb)** - Create professional instruction datasets\n",
    "2. **[Task 10.5: DPO Training](05-dpo-training.ipynb)** - Improve model quality with preference data\n",
    "3. **[Task 10.7: Ollama Integration](07-ollama-integration.ipynb)** - Deploy your fine-tuned model locally\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [QLoRA Paper](https://arxiv.org/abs/2305.14314)\n",
    "- [Llama 3.1 Technical Report](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/)\n",
    "- [bitsandbytes Documentation](https://github.com/TimDettmers/bitsandbytes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}