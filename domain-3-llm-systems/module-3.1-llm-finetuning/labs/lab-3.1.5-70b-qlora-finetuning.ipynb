{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1.5: 70B Model QLoRA Fine-Tuning â­\n",
    "\n",
    "**Module:** 3.1 - Large Language Model Fine-Tuning  \n",
    "**Time:** 4 hours  \n",
    "**Difficulty:** â­â­â­â­â˜†\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ THIS IS THE DGX SPARK SHOWCASE!\n",
    "\n",
    "**What you're about to do would be impossible on any consumer GPU:**\n",
    "\n",
    "| Hardware | 70B Fine-Tuning? | Cost |\n",
    "|----------|-----------------|------|\n",
    "| RTX 4090 (24GB) | âŒ Impossible | N/A |\n",
    "| 2Ã— RTX 4090 (48GB) | âŒ Still impossible | ~$4,000 |\n",
    "| 4Ã— A100 80GB (Cloud) | âœ… Works | ~$20/hour |\n",
    "| **DGX Spark (128GB)** | âœ… **Works!** | **One-time purchase** |\n",
    "\n",
    "**With 128GB unified memory, you can fine-tune Llama 3.1 70B on your DESKTOP!**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Fine-tune a 70B parameter model on DGX Spark\n",
    "- [ ] Understand memory optimization techniques for large models\n",
    "- [ ] Document memory usage patterns for 70B training\n",
    "- [ ] Compare 70B vs 8B fine-tuning quality\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Lab 3.1.4 (8B Model Fine-Tuning)\n",
    "- **Model access required:** Request access to `Qwen/Qwen3-32B-Instruct` on HuggingFace\n",
    "- **Time:** First run will download ~40GB of model weights\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELI5: Why is This Special?\n",
    "\n",
    "> **Imagine you have a giant library with 70 billion books.** Normally, you'd need a warehouse (cloud GPUs) to store and work with all these books.\n",
    ">\n",
    "> **DGX Spark's 128GB unified memory is like having a really efficient librarian** who can compress those books (4-bit quantization) while still being able to read them. The books take up 4Ã— less space, and you can work with ALL of them on your desk!\n",
    ">\n",
    "> **The magic:**\n",
    "> - 70B parameters Ã— 2 bytes (FP16) = 140GB (won't fit)\n",
    "> - 70B parameters Ã— 0.5 bytes (4-bit) = **~35GB** (fits easily!)\n",
    "> - Add LoRA adapters + training state = **~45-55GB total**\n",
    "> - That leaves **70GB+ for batch data and overhead!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pre-Flight Checks\n",
    "\n",
    "Before loading a 70B model, we need to ensure optimal conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ CRITICAL: Clear system caches before loading 70B model                      â•‘\n",
    "# â•‘ This frees up memory fragmentation from previous operations                 â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Clear Linux buffer cache (requires sudo)\n",
    "# This is ESSENTIAL for reliable 70B loading\n",
    "print(\"Clearing system caches...\")\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"sudo\", \"sh\", \"-c\", \"sync; echo 3 > /proc/sys/vm/drop_caches\"],\n",
    "        capture_output=True, text=True, timeout=30\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(\"âœ… System caches cleared successfully\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ Could not clear caches: {result.stderr}\")\n",
    "        print(\"   You may need to run: sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Cache clearing skipped: {e}\")\n",
    "    print(\"   Consider running manually if you encounter OOM issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import psutil\n",
    "from typing import Dict\n",
    "\n",
    "def get_system_memory() -> Dict[str, float]:\n",
    "    \"\"\"Get comprehensive memory statistics.\"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    # GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        stats['gpu_allocated'] = torch.cuda.memory_allocated() / 1e9\n",
    "        stats['gpu_reserved'] = torch.cuda.memory_reserved() / 1e9\n",
    "        stats['gpu_total'] = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        stats['gpu_free'] = stats['gpu_total'] - stats['gpu_allocated']\n",
    "    \n",
    "    # System memory (for unified memory awareness)\n",
    "    mem = psutil.virtual_memory()\n",
    "    stats['system_total'] = mem.total / 1e9\n",
    "    stats['system_available'] = mem.available / 1e9\n",
    "    stats['system_used'] = mem.used / 1e9\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def print_memory_status():\n",
    "    \"\"\"Print formatted memory status.\"\"\"\n",
    "    stats = get_system_memory()\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MEMORY STATUS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if 'gpu_total' in stats:\n",
    "        print(f\"\\nGPU Memory:\")\n",
    "        print(f\"  Total:     {stats['gpu_total']:.1f} GB\")\n",
    "        print(f\"  Allocated: {stats['gpu_allocated']:.1f} GB\")\n",
    "        print(f\"  Free:      {stats['gpu_free']:.1f} GB\")\n",
    "        \n",
    "        # DGX Spark detection\n",
    "        if stats['gpu_total'] > 100:\n",
    "            print(f\"\\nğŸš€ DGX Spark detected! 128GB unified memory available.\")\n",
    "            print(f\"   You have {stats['gpu_free']:.1f} GB free for the 70B model!\")\n",
    "    \n",
    "    print(f\"\\nSystem Memory:\")\n",
    "    print(f\"  Total:     {stats['system_total']:.1f} GB\")\n",
    "    print(f\"  Available: {stats['system_available']:.1f} GB\")\n",
    "    print(f\"  Used:      {stats['system_used']:.1f} GB\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "print_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify we have enough memory\n",
    "stats = get_system_memory()\n",
    "\n",
    "REQUIRED_MEMORY_GB = 60  # Conservative estimate for 70B training\n",
    "\n",
    "if stats.get('gpu_free', 0) < REQUIRED_MEMORY_GB:\n",
    "    print(f\"âš ï¸ WARNING: Only {stats.get('gpu_free', 0):.1f} GB free.\")\n",
    "    print(f\"   Recommended: {REQUIRED_MEMORY_GB} GB for comfortable 70B training.\")\n",
    "    print(\"\\n   Options:\")\n",
    "    print(\"   1. Clear buffer cache: sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'\")\n",
    "    print(\"   2. Restart Jupyter kernel\")\n",
    "    print(\"   3. Close other GPU-using applications\")\n",
    "else:\n",
    "    print(f\"âœ… Sufficient memory available: {stats.get('gpu_free', 0):.1f} GB\")\n",
    "    print(f\"   Ready to load 70B model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Configuration\n",
    "\n",
    "Optimized settings for 70B model training on DGX Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘                         70B TRAINING CONFIGURATION                          â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Model selection\n",
    "MODEL_NAME = \"Qwen/Qwen3-32B-Instruct\"  # The big one!\n",
    "# Alternative if you don't have Llama access:\n",
    "# MODEL_NAME = \"Qwen/Qwen2-72B-Instruct\"  # Another 70B+ option\n",
    "\n",
    "# Output\n",
    "OUTPUT_DIR = \"./finetuned-70b-model\"\n",
    "\n",
    "# Sequence length - keep shorter for 70B to save memory\n",
    "MAX_SEQ_LENGTH = 512  # Reduced from 1024 for memory efficiency\n",
    "\n",
    "# Batch size - be conservative with 70B\n",
    "BATCH_SIZE = 1  # Start with 1, can increase if memory allows\n",
    "GRADIENT_ACCUMULATION = 16  # Compensate with gradient accumulation\n",
    "\n",
    "# Training schedule\n",
    "LEARNING_RATE = 1e-4  # Slightly lower for stability\n",
    "NUM_EPOCHS = 1\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "# LoRA configuration - more conservative for 70B\n",
    "LORA_RANK = 16  # Can go higher if memory allows\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "USE_DORA = True  # Still use DoRA for quality\n",
    "\n",
    "# NEFTune\n",
    "NEFTUNE_ALPHA = 5.0\n",
    "\n",
    "# Target modules - attention only for 70B to save memory\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "# Add MLP if you have memory headroom:\n",
    "# TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "print(\"70B Training Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  LoRA rank: {LORA_RANK} (alpha={LORA_ALPHA})\")\n",
    "print(f\"  DoRA: {'Enabled' if USE_DORA else 'Disabled'}\")\n",
    "print(f\"  NEFTune: Î±={NEFTUNE_ALPHA}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE} Ã— {GRADIENT_ACCUMULATION} = {BATCH_SIZE * GRADIENT_ACCUMULATION} effective\")\n",
    "print(f\"  Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"  Target modules: {TARGET_MODULES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Load the 70B Model\n",
    "\n",
    "This is where the magic happens. We'll load 70 billion parameters into memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA Configuration - optimized for 70B\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,  # Double quantization saves ~5GB\n",
    ")\n",
    "\n",
    "print(\"Quantization config ready:\")\n",
    "print(\"  4-bit NF4 quantization\")\n",
    "print(\"  Double quantization enabled\")\n",
    "print(\"  Compute dtype: bfloat16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer first (lightweight)\n",
    "print(f\"Loading tokenizer from {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"âœ… Tokenizer loaded (vocab size: {tokenizer.vocab_size:,})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ LOADING 70B MODEL - This is the big moment!                                 â•‘\n",
    "# â•‘ First run: Downloads ~40GB. Subsequent runs: Loads from cache.              â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import time\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"LOADING {MODEL_NAME}\")\n",
    "print(f\"This is 70 BILLION parameters!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nMemory before loading:\")\n",
    "print_memory_status()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"sdpa\",  # Efficient attention\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ… 70B MODEL LOADED SUCCESSFULLY!\")\n",
    "print(f\"   Load time: {load_time:.1f} seconds ({load_time/60:.1f} minutes)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nMemory after loading:\")\n",
    "print_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for training\n",
    "print(\"Preparing model for k-bit training...\")\n",
    "\n",
    "model = prepare_model_for_kbit_training(\n",
    "    model,\n",
    "    use_gradient_checkpointing=True,  # ESSENTIAL for 70B\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "print(\"âœ… Model prepared with gradient checkpointing enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA/DoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    use_dora=USE_DORA,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"{'DoRA' if USE_DORA else 'LoRA'} Adapters Added!\")\n",
    "print(f\"{'='*60}\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(f\"\\nMemory after adding adapters:\")\n",
    "print_memory_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ‰ You Just Loaded a 70B Model on Your Desktop!\n",
    "\n",
    "Take a moment to appreciate this:\n",
    "- **70 billion parameters** compressed to ~35-40GB\n",
    "- **Still fits** with room for training\n",
    "- **No cloud required** - this is YOUR computer!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (using smaller subset for 70B to keep training reasonable)\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:500]\")\n",
    "\n",
    "print(f\"Dataset: {len(dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format for Llama 3.1\n",
    "def format_instruction_llama3(example):\n",
    "    instruction = example['instruction']\n",
    "    input_text = example.get('input', '')\n",
    "    output = example['output']\n",
    "    \n",
    "    if input_text:\n",
    "        prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{instruction}\n",
    "\n",
    "Input: {input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{output}<|eot_id|>\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{output}<|eot_id|>\"\"\"\n",
    "    \n",
    "    return {\"text\": prompt}\n",
    "\n",
    "formatted_dataset = dataset.map(format_instruction_llama3)\n",
    "print(f\"Dataset formatted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Training Configuration\n",
    "\n",
    "Optimized settings for 70B on DGX Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments - optimized for 70B on DGX Spark\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # Conservative batch size for 70B\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    \n",
    "    # Training schedule\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    \n",
    "    # Optimization\n",
    "    optim=\"paged_adamw_8bit\",  # Memory-efficient\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=0.3,  # Lower for stability with large models\n",
    "    \n",
    "    # Precision\n",
    "    bf16=True,\n",
    "    \n",
    "    # Sequence length\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    packing=True,\n",
    "    \n",
    "    # NEFTune\n",
    "    neftune_noise_alpha=NEFTUNE_ALPHA,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=5,\n",
    "    logging_first_step=True,\n",
    "    \n",
    "    # Saving\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,  # Save space\n",
    "    \n",
    "    # Performance\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    report_to=\"none\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"70B Training Configuration Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=formatted_dataset,\n",
    "    args=training_args,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "print(\"Trainer created!\")\n",
    "print_memory_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Train the 70B Model!\n",
    "\n",
    "This is it - training 70 billion parameters on your desktop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘                    TRAINING 70B MODEL ON DGX SPARK                          â•‘\n",
    "# â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "# â•‘ This would be IMPOSSIBLE on any consumer GPU!                               â•‘\n",
    "# â•‘ You're about to train one of the largest open-source models on your desk!  â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"ğŸ”¥\"*30)\n",
    "print(\"STARTING 70B MODEL TRAINING!\")\n",
    "print(\"ğŸ”¥\"*30 + \"\\n\")\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Parameters: 70 BILLION\")\n",
    "print(f\"Method: {'DoRA' if USE_DORA else 'LoRA'} + NEFTune\")\n",
    "print(f\"Dataset: {len(formatted_dataset)} examples\")\n",
    "print(f\"\\nThis is running on YOUR DGX Spark!\")\n",
    "\n",
    "# Memory status before training\n",
    "print_memory_status()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training starting...\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"ğŸ‰\"*30)\n",
    "    print(\"70B TRAINING COMPLETE!\")\n",
    "    print(\"ğŸ‰\"*30)\n",
    "    print(f\"\\nTraining time: {training_time:.1f} seconds ({training_time/60:.1f} minutes)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Training error: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"  1. Try reducing MAX_SEQ_LENGTH to 256\")\n",
    "    print(\"  2. Try reducing LORA_RANK to 8\")\n",
    "    print(\"  3. Clear caches and restart kernel\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print training metrics\n",
    "print(\"\\nTraining Metrics:\")\n",
    "for key, value in train_result.metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print_memory_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Test Your 70B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_70b(prompt: str, max_new_tokens: int = 256) -> str:\n",
    "    \"\"\"Generate response from fine-tuned 70B model.\"\"\"\n",
    "    formatted = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract assistant response\n",
    "    if \"assistant\" in response.lower():\n",
    "        parts = response.split(\"assistant\")\n",
    "        response = parts[-1].strip() if len(parts) > 1 else response\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the 70B model\n",
    "test_prompts = [\n",
    "    \"Explain quantum computing to a 10-year-old.\",\n",
    "    \"What are the main differences between Python and JavaScript?\",\n",
    "    \"Write a haiku about artificial intelligence.\",\n",
    "]\n",
    "\n",
    "print(\"Testing fine-tuned 70B model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nğŸ“ Prompt: {prompt}\")\n",
    "    print(\"-\"*40)\n",
    "    response = generate_response_70b(prompt)\n",
    "    print(f\"ğŸ¤– Response: {response[:500]}...\" if len(response) > 500 else f\"ğŸ¤– Response: {response}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Save Your 70B Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapters\n",
    "adapter_path = os.path.join(OUTPUT_DIR, \"70b_adapter\")\n",
    "model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "\n",
    "print(f\"âœ… 70B Adapter saved to: {adapter_path}\")\n",
    "\n",
    "# Show saved files\n",
    "print(\"\\nSaved files:\")\n",
    "total_size = 0\n",
    "for f in os.listdir(adapter_path):\n",
    "    size = os.path.getsize(os.path.join(adapter_path, f)) / 1e6\n",
    "    total_size += size\n",
    "    print(f\"  {f}: {size:.2f} MB\")\n",
    "\n",
    "print(f\"\\nTotal adapter size: {total_size:.2f} MB\")\n",
    "print(f\"(Compare to full 70B model: ~140GB in FP16!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Memory Analysis Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate memory report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"70B TRAINING MEMORY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_stats = get_system_memory()\n",
    "\n",
    "print(f\"\"\"\n",
    "SUMMARY\n",
    "-------\n",
    "Model: {MODEL_NAME}\n",
    "Parameters: 70,000,000,000 (70B)\n",
    "Quantization: 4-bit NF4 with double quantization\n",
    "\n",
    "Memory Usage:\n",
    "  Base model (4-bit):     ~35-40 GB\n",
    "  LoRA adapters:          ~{(LORA_RANK * 2 * 4096 * len(TARGET_MODULES) * 80) / 1e6:.0f} MB\n",
    "  Optimizer states:       ~2-3 GB\n",
    "  Gradients + activations: ~5-10 GB\n",
    "  ----------------------------------\n",
    "  Peak training memory:   ~45-55 GB\n",
    "\n",
    "DGX Spark Capacity:\n",
    "  Total unified memory:   128 GB\n",
    "  Used for training:      ~{final_stats.get('gpu_allocated', 0):.1f} GB\n",
    "  Remaining headroom:     ~{128 - final_stats.get('gpu_allocated', 50):.1f} GB\n",
    "\n",
    "What This Means:\n",
    "  - RTX 4090 (24GB): âŒ IMPOSSIBLE\n",
    "  - 2Ã— RTX 4090 (48GB): âŒ IMPOSSIBLE  \n",
    "  - A100 40GB: âŒ IMPOSSIBLE\n",
    "  - A100 80GB (cloud): âš ï¸ Tight, needs careful tuning\n",
    "  - DGX Spark (128GB): âœ… COMFORTABLE with headroom!\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: What You Just Accomplished\n",
    "\n",
    "### ğŸ† Achievement Unlocked: 70B Fine-Tuning Master!\n",
    "\n",
    "You successfully:\n",
    "\n",
    "1. âœ… Loaded a **70 billion parameter model** on your desktop\n",
    "2. âœ… Applied **QLoRA** (4-bit quantization) for memory efficiency\n",
    "3. âœ… Added **DoRA** adapters for quality improvement\n",
    "4. âœ… Enabled **NEFTune** for better generalization\n",
    "5. âœ… **Trained** the model on instruction data\n",
    "6. âœ… **Tested** and saved your custom 70B model\n",
    "\n",
    "### DGX Spark vs. Alternatives\n",
    "\n",
    "| Approach | Hardware | Cost | 70B Training? |\n",
    "|----------|----------|------|---------------|\n",
    "| DGX Spark | 128GB unified | One-time purchase | âœ… Yes! |\n",
    "| Cloud A100s | 4Ã— 80GB | ~$80/hour | âœ… Yes |\n",
    "| RTX 4090s | 2Ã— 24GB | ~$4,000 | âŒ No |\n",
    "\n",
    "**You just did in your home office what normally requires cloud infrastructure!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "print(\"Cleaning up...\")\n",
    "\n",
    "del model, trainer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… Cleanup complete!\")\n",
    "print_memory_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Congratulations! You've mastered the core fine-tuning skills. Continue to:\n",
    "\n",
    "**[Lab 3.1.6: Dataset Preparation](lab-3.1.6-dataset-preparation.ipynb)** - Learn to create your own high-quality training datasets!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
