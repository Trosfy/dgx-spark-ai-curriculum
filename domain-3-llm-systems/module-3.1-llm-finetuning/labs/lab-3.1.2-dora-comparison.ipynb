{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1.2: DoRA - Weight-Decomposed Low-Rank Adaptation\n",
    "\n",
    "**Module:** 3.1 - Large Language Model Fine-Tuning  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐⭐☆☆\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand how DoRA improves upon standard LoRA\n",
    "- [ ] Implement DoRA's weight decomposition from scratch\n",
    "- [ ] Compare LoRA vs DoRA on the same task\n",
    "- [ ] Measure the +3.7 point improvement on commonsense reasoning\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Lab 3.1.1 (LoRA Theory)\n",
    "- Knowledge of: LoRA mathematics, PyTorch\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "### The Problem with Standard LoRA\n",
    "\n",
    "LoRA works great, but researchers at NVIDIA discovered it has a subtle limitation: **it couples magnitude and direction updates together**.\n",
    "\n",
    "When you update weights with LoRA ($W = W_0 + BA$), you're changing both:\n",
    "- **How strongly** neurons fire (magnitude)\n",
    "- **What patterns** they respond to (direction)\n",
    "\n",
    "But what if the model already has the right \"strength\" and just needs to learn new \"patterns\"? LoRA can't separate these!\n",
    "\n",
    "**DoRA (Weight-Decomposed LoRA)** fixes this by explicitly separating magnitude and direction, giving you **+3.7 points on commonsense reasoning** benchmarks!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELI5: What is DoRA?\n",
    "\n",
    "> **Imagine you're learning to throw darts.** There are two things you need to get right:\n",
    ">\n",
    "> 1. **How hard you throw** (magnitude) - Too soft and it falls short, too hard and it goes past the board\n",
    "> 2. **Which direction you aim** (direction) - Left, right, up, down\n",
    ">\n",
    "> **Standard LoRA** is like learning both at the same time with the same practice throws. It works, but it's not optimal.\n",
    ">\n",
    "> **DoRA** separates these: First, it figures out the right \"strength\" (a simple number for each neuron), then separately learns the \"aim\" (the direction pattern). This separation makes learning more efficient!\n",
    ">\n",
    "> **In AI terms:** DoRA decomposes the weight matrix into a magnitude vector and a direction matrix, then applies LoRA only to the direction. This gives the model more flexibility in how it adapts.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Mathematics of DoRA\n",
    "\n",
    "### Standard LoRA Recap\n",
    "\n",
    "In LoRA, the adapted weight is:\n",
    "\n",
    "$$W = W_0 + \\Delta W = W_0 + BA$$\n",
    "\n",
    "### DoRA's Key Insight\n",
    "\n",
    "DoRA decomposes the pretrained weights into **magnitude** and **direction**:\n",
    "\n",
    "$$W_0 = m \\cdot \\frac{V}{\\|V\\|_c}$$\n",
    "\n",
    "Where:\n",
    "- $m \\in \\mathbb{R}^{1 \\times k}$ is the **magnitude vector** (one value per column)\n",
    "- $V \\in \\mathbb{R}^{d \\times k}$ is the **direction matrix**\n",
    "- $\\|V\\|_c$ denotes the column-wise norm\n",
    "\n",
    "### DoRA's Adaptation\n",
    "\n",
    "DoRA then adapts both components:\n",
    "\n",
    "$$W' = (m + \\Delta m) \\cdot \\frac{V + \\Delta V}{\\|V + \\Delta V\\|_c}$$\n",
    "\n",
    "Where:\n",
    "- $\\Delta m$ is a **trainable magnitude adjustment** (very few parameters!)\n",
    "- $\\Delta V = BA$ is the **LoRA update to direction**\n",
    "\n",
    "### Why This Works Better\n",
    "\n",
    "1. **Decoupled learning**: Magnitude and direction are optimized separately\n",
    "2. **Closer to full fine-tuning**: Research shows full FT naturally separates these\n",
    "3. **Better gradient flow**: Normalization provides stable gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, List, Tuple, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Implementing DoRA from Scratch\n",
    "\n",
    "Let's implement DoRA step by step to understand exactly how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    DoRA: Weight-Decomposed Low-Rank Adaptation\n",
    "    \n",
    "    Implements the DoRA method from \"DoRA: Weight-Decomposed Low-Rank Adaptation\"\n",
    "    (https://arxiv.org/abs/2402.09353)\n",
    "    \n",
    "    The key insight: Decompose W = m * (V / ||V||) where:\n",
    "    - m is magnitude (trainable scalar per column)\n",
    "    - V is direction (adapted with LoRA)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        original_layer: nn.Linear,\n",
    "        rank: int = 8,\n",
    "        alpha: float = 16.0,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.original_layer = original_layer\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        in_features = original_layer.in_features\n",
    "        out_features = original_layer.out_features\n",
    "        \n",
    "        # Freeze original weights\n",
    "        self.original_layer.weight.requires_grad = False\n",
    "        if self.original_layer.bias is not None:\n",
    "            self.original_layer.bias.requires_grad = False\n",
    "        \n",
    "        # === DoRA-specific: Magnitude vector ===\n",
    "        # Initialize from the column norms of W0\n",
    "        with torch.no_grad():\n",
    "            weight = original_layer.weight.data  # (out_features, in_features)\n",
    "            # Compute column-wise L2 norm\n",
    "            self.magnitude = nn.Parameter(\n",
    "                weight.norm(dim=0, keepdim=True)  # Shape: (1, in_features)\n",
    "            )\n",
    "        \n",
    "        # === LoRA matrices (for direction) ===\n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        \n",
    "        # Initialize A with Kaiming, B stays zero\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=np.sqrt(5))\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        \n",
    "        # Store original direction (normalized)\n",
    "        with torch.no_grad():\n",
    "            self.register_buffer(\n",
    "                'original_direction',\n",
    "                weight / (weight.norm(dim=0, keepdim=True) + 1e-8)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        DoRA forward pass:\n",
    "        W' = m * normalize(V + BA)\n",
    "        \"\"\"\n",
    "        # Get original weight\n",
    "        weight = self.original_layer.weight\n",
    "        \n",
    "        # Compute LoRA delta for direction\n",
    "        lora_delta = self.scaling * (self.lora_B @ self.lora_A)  # (out, in)\n",
    "        \n",
    "        # Update direction: V' = W0 + BA (before normalization)\n",
    "        updated_weight = weight + lora_delta\n",
    "        \n",
    "        # Normalize to get direction\n",
    "        direction_norm = updated_weight.norm(dim=0, keepdim=True) + 1e-8\n",
    "        normalized_direction = updated_weight / direction_norm\n",
    "        \n",
    "        # Apply magnitude\n",
    "        dora_weight = self.magnitude * normalized_direction\n",
    "        \n",
    "        # Forward pass\n",
    "        result = F.linear(self.dropout(x), dora_weight, self.original_layer.bias)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @property\n",
    "    def trainable_params(self) -> int:\n",
    "        \"\"\"LoRA params + magnitude vector.\"\"\"\n",
    "        return self.lora_A.numel() + self.lora_B.numel() + self.magnitude.numel()\n",
    "    \n",
    "    def get_magnitude_change(self) -> torch.Tensor:\n",
    "        \"\"\"Get how much magnitude has changed from original.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            original_mag = self.original_layer.weight.norm(dim=0)\n",
    "            return (self.magnitude.squeeze() - original_mag).abs()\n",
    "\n",
    "\n",
    "# Compare with standard LoRA\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"Standard LoRA for comparison.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        original_layer: nn.Linear,\n",
    "        rank: int = 8,\n",
    "        alpha: float = 16.0,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.original_layer = original_layer\n",
    "        self.rank = rank\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        in_features = original_layer.in_features\n",
    "        out_features = original_layer.out_features\n",
    "        \n",
    "        # Freeze original\n",
    "        self.original_layer.weight.requires_grad = False\n",
    "        if self.original_layer.bias is not None:\n",
    "            self.original_layer.bias.requires_grad = False\n",
    "        \n",
    "        # LoRA matrices\n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=np.sqrt(5))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        result = self.original_layer(x)\n",
    "        lora_output = self.dropout(x) @ self.lora_A.T @ self.lora_B.T\n",
    "        return result + self.scaling * lora_output\n",
    "    \n",
    "    @property\n",
    "    def trainable_params(self) -> int:\n",
    "        return self.lora_A.numel() + self.lora_B.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test our DoRA implementation\n",
    "print(\"=\" * 60)\n",
    "print(\"DoRA vs LoRA Parameter Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a sample layer (typical transformer projection)\n",
    "original = nn.Linear(4096, 4096, bias=False)\n",
    "\n",
    "# Create both adapters\n",
    "lora = LoRALayer(nn.Linear(4096, 4096, bias=False), rank=16, alpha=32)\n",
    "dora = DoRALayer(nn.Linear(4096, 4096, bias=False), rank=16, alpha=32)\n",
    "\n",
    "print(f\"\\nOriginal layer parameters: {original.weight.numel():,}\")\n",
    "print(f\"\\nLoRA trainable parameters: {lora.trainable_params:,}\")\n",
    "print(f\"  - lora_A: {lora.lora_A.numel():,}\")\n",
    "print(f\"  - lora_B: {lora.lora_B.numel():,}\")\n",
    "\n",
    "print(f\"\\nDoRA trainable parameters: {dora.trainable_params:,}\")\n",
    "print(f\"  - lora_A: {dora.lora_A.numel():,}\")\n",
    "print(f\"  - lora_B: {dora.lora_B.numel():,}\")\n",
    "print(f\"  - magnitude: {dora.magnitude.numel():,}\")\n",
    "\n",
    "print(f\"\\nDoRA overhead vs LoRA: +{dora.trainable_params - lora.trainable_params:,} params\")\n",
    "print(f\"  ({(dora.trainable_params / lora.trainable_params - 1) * 100:.2f}% more)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Happening?\n",
    "\n",
    "DoRA adds a **magnitude vector** with one value per input feature. For a 4096-dimensional layer:\n",
    "- LoRA: 2 × 16 × 4096 = 131,072 parameters\n",
    "- DoRA: 131,072 + 4,096 = 135,168 parameters (~3% more)\n",
    "\n",
    "This tiny overhead gives significant quality improvements!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 3: Visualizing the Difference\n\nLet's visualize how LoRA and DoRA update weights differently.\n\n### Key PyTorch Functions Used\n\n| Function | Description |\n|----------|-------------|\n| `F.cosine_similarity(a, b, dim)` | Computes cosine similarity between tensors along specified dimension. Returns values in [-1, 1] where 1 = identical direction. |\n| `F.scaled_dot_product_attention(q, k, v)` | PyTorch 2.0+ efficient attention with Flash Attention optimization. |\n| `torch.nn.utils.clip_grad_norm_(params, max_norm)` | Clips gradient norms to prevent exploding gradients during training. |\n| `module.register_buffer(name, tensor)` | Registers a tensor as a buffer (saved with model but not a parameter). Used for non-trainable state. |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_weight_decomposition():\n",
    "    \"\"\"\n",
    "    Visualize how DoRA decomposes weights into magnitude and direction.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Create a small weight matrix for visualization\n",
    "    d = 64\n",
    "    W = torch.randn(d, d) * 0.5\n",
    "    \n",
    "    # Decompose into magnitude and direction\n",
    "    magnitude = W.norm(dim=0, keepdim=True)  # (1, d)\n",
    "    direction = W / (magnitude + 1e-8)  # (d, d) - unit vectors\n",
    "    \n",
    "    # Verify: magnitude * direction = W\n",
    "    reconstructed = magnitude * direction\n",
    "    reconstruction_error = (W - reconstructed).abs().max().item()\n",
    "    print(f\"Reconstruction error: {reconstruction_error:.2e}\")\n",
    "    \n",
    "    # Row 1: Original decomposition\n",
    "    im0 = axes[0, 0].imshow(W.numpy(), cmap='RdBu', aspect='auto')\n",
    "    axes[0, 0].set_title('Original Weight W')\n",
    "    axes[0, 0].set_xlabel('Input features')\n",
    "    axes[0, 0].set_ylabel('Output features')\n",
    "    plt.colorbar(im0, ax=axes[0, 0])\n",
    "    \n",
    "    axes[0, 1].bar(range(d), magnitude.squeeze().numpy(), color='steelblue')\n",
    "    axes[0, 1].set_title('Magnitude (per column)')\n",
    "    axes[0, 1].set_xlabel('Column index')\n",
    "    axes[0, 1].set_ylabel('||W[:, i]||')\n",
    "    \n",
    "    im2 = axes[0, 2].imshow(direction.numpy(), cmap='RdBu', aspect='auto', vmin=-1, vmax=1)\n",
    "    axes[0, 2].set_title('Direction (normalized)')\n",
    "    axes[0, 2].set_xlabel('Input features')\n",
    "    axes[0, 2].set_ylabel('Output features')\n",
    "    plt.colorbar(im2, ax=axes[0, 2])\n",
    "    \n",
    "    # Row 2: Simulated updates\n",
    "    # LoRA update (random low-rank)\n",
    "    r = 8\n",
    "    B = torch.randn(d, r) * 0.1\n",
    "    A = torch.randn(r, d) * 0.1\n",
    "    lora_delta = B @ A\n",
    "    \n",
    "    # LoRA changes both magnitude and direction together\n",
    "    W_lora = W + lora_delta\n",
    "    lora_mag = W_lora.norm(dim=0, keepdim=True)\n",
    "    \n",
    "    # DoRA: Only changes direction, magnitude is separate\n",
    "    direction_updated = direction + lora_delta / (magnitude + 1e-8)\n",
    "    direction_updated = direction_updated / (direction_updated.norm(dim=0, keepdim=True) + 1e-8)\n",
    "    \n",
    "    im3 = axes[1, 0].imshow(lora_delta.numpy(), cmap='RdBu', aspect='auto')\n",
    "    axes[1, 0].set_title('LoRA Update (BA)')\n",
    "    axes[1, 0].set_xlabel('Input features')\n",
    "    axes[1, 0].set_ylabel('Output features')\n",
    "    plt.colorbar(im3, ax=axes[1, 0])\n",
    "    \n",
    "    # Magnitude change comparison\n",
    "    lora_mag_change = (lora_mag - magnitude).squeeze()\n",
    "    axes[1, 1].bar(range(d), lora_mag_change.numpy(), color='coral', alpha=0.7, label='LoRA')\n",
    "    axes[1, 1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    axes[1, 1].set_title('Magnitude Change (LoRA)')\n",
    "    axes[1, 1].set_xlabel('Column index')\n",
    "    axes[1, 1].set_ylabel('Change in magnitude')\n",
    "    \n",
    "    # Direction change (cosine similarity)\n",
    "    cos_sim_lora = F.cosine_similarity(\n",
    "        direction, W_lora / (W_lora.norm(dim=0, keepdim=True) + 1e-8), dim=0\n",
    "    )\n",
    "    axes[1, 2].bar(range(d), (1 - cos_sim_lora).numpy(), color='purple')\n",
    "    axes[1, 2].set_title('Direction Change (1 - cos_sim)')\n",
    "    axes[1, 2].set_xlabel('Column index')\n",
    "    axes[1, 2].set_ylabel('Direction divergence')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dora_weight_decomposition.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(\"\\nKey Insight:\")\n",
    "    print(\"- LoRA changes BOTH magnitude and direction in coupled way\")\n",
    "    print(\"- DoRA separates these, allowing independent optimization\")\n",
    "\n",
    "visualize_weight_decomposition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Head-to-Head Comparison\n",
    "\n",
    "Let's train both LoRA and DoRA on the same task and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformerBlock(nn.Module):\n",
    "    \"\"\"A simplified transformer block for comparison testing.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int = 256, n_heads: int = 8):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        # Attention projections\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.o_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        # MLP\n",
    "        self.mlp_up = nn.Linear(d_model, d_model * 4)\n",
    "        self.mlp_down = nn.Linear(d_model * 4, d_model)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Attention\n",
    "        normed = self.norm1(x)\n",
    "        q = self.q_proj(normed).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(normed).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(normed).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        attn = F.scaled_dot_product_attention(q, k, v)\n",
    "        attn = attn.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        x = x + self.o_proj(attn)\n",
    "        \n",
    "        # MLP\n",
    "        x = x + self.mlp_down(F.gelu(self.mlp_up(self.norm2(x))))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def add_adapters_to_model(model: nn.Module, adapter_type: str, rank: int = 8, alpha: float = 16) -> nn.Module:\n",
    "    \"\"\"Add LoRA or DoRA adapters to attention projections.\"\"\"\n",
    "    import copy\n",
    "    model = copy.deepcopy(model)\n",
    "    \n",
    "    AdapterClass = DoRALayer if adapter_type == 'dora' else LoRALayer\n",
    "    \n",
    "    for name in ['q_proj', 'k_proj', 'v_proj', 'o_proj']:\n",
    "        original_layer = getattr(model, name)\n",
    "        adapter_layer = AdapterClass(original_layer, rank=rank, alpha=alpha)\n",
    "        setattr(model, name, adapter_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "    adapter_type: str,\n",
    "    n_epochs: int = 100,\n",
    "    rank: int = 8,\n",
    "    verbose: bool = True\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Train a model with specified adapter type and return metrics.\n",
    "    \"\"\"\n",
    "    # Create model with adapters\n",
    "    base_model = SimpleTransformerBlock(d_model=256, n_heads=8).to(device)\n",
    "    model = add_adapters_to_model(base_model, adapter_type, rank=rank, alpha=rank*2)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Count trainable params\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    # Task: Learn a specific transformation pattern\n",
    "    torch.manual_seed(42)  # Same data for fair comparison\n",
    "    n_samples, seq_len, d_model = 500, 32, 256\n",
    "    \n",
    "    # Create a target transformation (the \"ground truth\" we're trying to learn)\n",
    "    target_transform = nn.Linear(d_model, d_model, bias=False).to(device)\n",
    "    with torch.no_grad():\n",
    "        target_transform.weight.normal_(0, 0.1)\n",
    "    \n",
    "    x = torch.randn(n_samples, seq_len, d_model, device=device)\n",
    "    y = target_transform(x) + torch.randn_like(x) * 0.01  # Slight noise\n",
    "    \n",
    "    # Training setup\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.AdamW(params, lr=1e-3, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)\n",
    "    \n",
    "    # Training loop\n",
    "    losses = []\n",
    "    batch_size = 32\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        # Shuffle data\n",
    "        perm = torch.randperm(n_samples)\n",
    "        \n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            batch_x, batch_y = x[idx], y[idx]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x)\n",
    "            loss = F.mse_loss(output, batch_y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(params, 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        scheduler.step()\n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if verbose and (epoch + 1) % 25 == 0:\n",
    "            print(f\"  Epoch {epoch+1}/{n_epochs}: Loss = {avg_loss:.6f}\")\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_x = torch.randn(100, seq_len, d_model, device=device)\n",
    "        test_y = target_transform(test_x)\n",
    "        test_pred = model(test_x)\n",
    "        test_loss = F.mse_loss(test_pred, test_y).item()\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, optimizer, x, y\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        'adapter_type': adapter_type,\n",
    "        'trainable_params': trainable,\n",
    "        'train_losses': losses,\n",
    "        'final_train_loss': np.mean(losses[-10:]),\n",
    "        'test_loss': test_loss,\n",
    "    }\n",
    "\n",
    "\n",
    "# Run comparison\n",
    "print(\"=\"*60)\n",
    "print(\"LoRA vs DoRA Training Comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nTraining with LoRA...\")\n",
    "lora_results = train_and_evaluate('lora', n_epochs=100, rank=8)\n",
    "\n",
    "print(\"\\nTraining with DoRA...\")\n",
    "dora_results = train_and_evaluate('dora', n_epochs=100, rank=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Training curves\n",
    "axes[0].plot(lora_results['train_losses'], label='LoRA', linewidth=2, color='blue')\n",
    "axes[0].plot(dora_results['train_losses'], label='DoRA', linewidth=2, color='red')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Loss')\n",
    "axes[0].set_title('Training Loss Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Final metrics comparison\n",
    "methods = ['LoRA', 'DoRA']\n",
    "train_losses = [lora_results['final_train_loss'], dora_results['final_train_loss']]\n",
    "test_losses = [lora_results['test_loss'], dora_results['test_loss']]\n",
    "\n",
    "x_pos = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].bar(x_pos - width/2, train_losses, width, label='Train Loss', color='steelblue')\n",
    "axes[1].bar(x_pos + width/2, test_losses, width, label='Test Loss', color='coral')\n",
    "axes[1].set_xlabel('Method')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Final Loss Comparison')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(methods)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter count\n",
    "params = [lora_results['trainable_params'], dora_results['trainable_params']]\n",
    "axes[2].bar(methods, params, color=['blue', 'red'])\n",
    "axes[2].set_xlabel('Method')\n",
    "axes[2].set_ylabel('Trainable Parameters')\n",
    "axes[2].set_title('Parameter Count Comparison')\n",
    "for i, v in enumerate(params):\n",
    "    axes[2].text(i, v + 100, f'{v:,}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lora_vs_dora_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "improvement = (lora_results['test_loss'] - dora_results['test_loss']) / lora_results['test_loss'] * 100\n",
    "param_overhead = (dora_results['trainable_params'] - lora_results['trainable_params']) / lora_results['trainable_params'] * 100\n",
    "\n",
    "print(f\"\\nLoRA:\")\n",
    "print(f\"  Trainable params: {lora_results['trainable_params']:,}\")\n",
    "print(f\"  Final train loss: {lora_results['final_train_loss']:.6f}\")\n",
    "print(f\"  Test loss: {lora_results['test_loss']:.6f}\")\n",
    "\n",
    "print(f\"\\nDoRA:\")\n",
    "print(f\"  Trainable params: {dora_results['trainable_params']:,}\")\n",
    "print(f\"  Final train loss: {dora_results['final_train_loss']:.6f}\")\n",
    "print(f\"  Test loss: {dora_results['test_loss']:.6f}\")\n",
    "\n",
    "print(f\"\\nDoRA Advantage:\")\n",
    "print(f\"  Test loss improvement: {improvement:.1f}%\")\n",
    "print(f\"  Parameter overhead: +{param_overhead:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Using DoRA with PEFT (Production)\n",
    "\n",
    "In practice, you'll use the PEFT library which has DoRA built-in. It's just **one flag**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production DoRA configuration with PEFT\n",
    "# (This is what you'll use in real fine-tuning)\n",
    "\n",
    "peft_config_example = \"\"\"\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Standard LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # use_dora=False  # Default: standard LoRA\n",
    ")\n",
    "\n",
    "# DoRA config - just add one flag!\n",
    "dora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    use_dora=True  # Enable DoRA!\n",
    ")\n",
    "\n",
    "# Apply to model\n",
    "model = get_peft_model(base_model, dora_config)\n",
    "model.print_trainable_parameters()\n",
    "\"\"\"\n",
    "\n",
    "print(\"Production DoRA Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "print(peft_config_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: When to Use DoRA vs LoRA\n",
    "\n",
    "| Scenario | Recommendation | Why |\n",
    "|----------|----------------|-----|\n",
    "| Quick experiments | LoRA | Simpler, faster |\n",
    "| Production fine-tuning | **DoRA** | Better quality, minimal overhead |\n",
    "| Commonsense reasoning | **DoRA** | +3.7 points improvement |\n",
    "| Code generation | DoRA | Benefits from directional adaptation |\n",
    "| Memory-critical | LoRA | Slightly fewer params |\n",
    "| 70B+ models | **DoRA** | Quality gains worth small overhead |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Exercises\n",
    "\n",
    "### Exercise 1: Rank Comparison\n",
    "\n",
    "Compare LoRA vs DoRA at different ranks (4, 8, 16, 32) and plot the improvement.\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Use a loop over ranks and call `train_and_evaluate` for both methods at each rank.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "def compare_across_ranks(ranks: List[int] = [4, 8, 16, 32]) -> Dict:\n",
    "    \"\"\"\n",
    "    Compare LoRA vs DoRA across different rank values.\n",
    "    \n",
    "    Args:\n",
    "        ranks: List of rank values to test\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results for each method and rank\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# results = compare_across_ranks([4, 8, 16])\n",
    "# Plot the results..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Magnitude Analysis\n",
    "\n",
    "Track how the magnitude vector changes during DoRA training. Does it change significantly?\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Add tracking inside the training loop to record `dora_layer.magnitude` at each epoch.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "def track_magnitude_changes(n_epochs: int = 50) -> Dict:\n",
    "    \"\"\"\n",
    "    Train DoRA and track magnitude vector changes.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with magnitude history and statistics\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting to Normalize\n",
    "\n",
    "```python\n",
    "# Wrong: Not normalizing after adding LoRA delta\n",
    "updated_weight = weight + lora_delta\n",
    "dora_weight = self.magnitude * updated_weight  # WRONG!\n",
    "\n",
    "# Right: Normalize to get direction first\n",
    "updated_weight = weight + lora_delta\n",
    "direction = updated_weight / (updated_weight.norm(dim=0, keepdim=True) + 1e-8)\n",
    "dora_weight = self.magnitude * direction  # Correct!\n",
    "```\n",
    "\n",
    "**Why:** DoRA explicitly separates magnitude and direction. Without normalization, you're not getting the benefits of decomposition.\n",
    "\n",
    "### Mistake 2: Wrong Magnitude Initialization\n",
    "\n",
    "```python\n",
    "# Wrong: Random magnitude\n",
    "self.magnitude = nn.Parameter(torch.randn(1, in_features))\n",
    "\n",
    "# Right: Initialize from original weight norms\n",
    "self.magnitude = nn.Parameter(weight.norm(dim=0, keepdim=True))\n",
    "```\n",
    "\n",
    "**Why:** Starting with the original magnitudes ensures the model behaves identically before training begins.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ✅ How DoRA decomposes weights into magnitude and direction\n",
    "- ✅ Why this separation improves learning (+3.7 points!)\n",
    "- ✅ How to implement DoRA from scratch\n",
    "- ✅ How to enable DoRA in PEFT (just `use_dora=True`!)\n",
    "- ✅ When to choose DoRA vs standard LoRA\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "### Implement rsLoRA (Rank-Stabilized LoRA)\n",
    "\n",
    "rsLoRA uses a different scaling: instead of $\\alpha/r$, it uses $\\alpha/\\sqrt{r}$. This provides better stability at higher ranks.\n",
    "\n",
    "1. Modify `DoRALayer` to support rsLoRA scaling\n",
    "2. Compare DoRA with rsLoRA vs regular DoRA at rank=64\n",
    "3. Measure training stability (gradient norms)\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [DoRA: Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/abs/2402.09353) - The original DoRA paper\n",
    "- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) - Original LoRA paper\n",
    "- [PEFT Documentation](https://huggingface.co/docs/peft) - Hugging Face PEFT library\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"Cleanup complete!\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you understand DoRA, continue to:\n",
    "\n",
    "**[Lab 3.1.3: NEFTune Magic](lab-3.1.3-neftune-magic.ipynb)** - Learn how adding noise to embeddings can boost performance by 35%!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}