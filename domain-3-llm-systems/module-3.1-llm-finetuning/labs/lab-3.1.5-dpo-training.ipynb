{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1.5: Direct Preference Optimization (DPO) Training\n",
    "\n",
    "**Module:** 3.1 - Large Language Model Fine-Tuning  \n",
    "**Time:** 3 hours  \n",
    "**Difficulty:** ⭐⭐⭐⭐☆\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand the theory behind DPO and how it differs from RLHF\n",
    "- [ ] Create preference pair datasets from raw data\n",
    "- [ ] Implement the DPO loss function from scratch\n",
    "- [ ] Train a model with DPO using TRL\n",
    "- [ ] Evaluate and compare DPO-trained vs SFT-only models\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Tasks 10.1-10.4\n",
    "- Knowledge of: Supervised fine-tuning, basic reinforcement learning concepts\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "### The Problem with SFT Alone\n",
    "\n",
    "Supervised Fine-Tuning (SFT) teaches models *what* to say, but not *how to choose* between alternatives. When there are multiple valid responses, how does the model know which one is *better*?\n",
    "\n",
    "**Real example:** \"Write a poem about the ocean\"\n",
    "- Response A: Short, simple, rhymes perfectly\n",
    "- Response B: Long, metaphorical, more creative\n",
    "\n",
    "Both are valid! But different users might prefer different styles. **Preference optimization** teaches the model which responses are *preferred* by humans.\n",
    "\n",
    "### The Traditional Approach: RLHF\n",
    "\n",
    "RLHF (Reinforcement Learning from Human Feedback) used by ChatGPT:\n",
    "1. Collect human preferences\n",
    "2. Train a reward model\n",
    "3. Use RL (PPO) to optimize against the reward model\n",
    "\n",
    "**Problem:** Complex, unstable, requires lots of compute.\n",
    "\n",
    "### DPO: The Elegant Solution\n",
    "\n",
    "DPO (Direct Preference Optimization) achieves similar results with:\n",
    "- No reward model needed\n",
    "- No RL training loop\n",
    "- Stable, simple supervised learning\n",
    "\n",
    "**That's why it's become so popular!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELI5: What is DPO?\n",
    "\n",
    "> **Imagine you're training a chef** (your model) to make better dishes.\n",
    ">\n",
    "> **SFT (Supervised Fine-Tuning):**  \n",
    "> You show the chef recipes and say \"Cook exactly like this.\" They learn to follow recipes, but can't improve on their own.\n",
    ">\n",
    "> **RLHF (Reinforcement Learning):**  \n",
    "> You hire a food critic (reward model), have the chef cook many dishes, and the critic rates each one. Complex: you need to train the critic, and the chef learns through trial and error.\n",
    ">\n",
    "> **DPO (Direct Preference Optimization):**  \n",
    "> You simply show the chef pairs of dishes: \"This one is better than that one.\" No food critic needed! The chef directly learns what makes one dish preferred over another.\n",
    ">\n",
    "> **In technical terms:** DPO uses pairs of (chosen, rejected) responses to directly teach the model to assign higher probability to preferred responses, without needing an explicit reward model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the DPO Loss\n",
    "\n",
    "### The Math Behind DPO\n",
    "\n",
    "The DPO loss is:\n",
    "\n",
    "$$\\mathcal{L}_{DPO}(\\pi_\\theta; \\pi_{ref}) = -\\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)} \\right) \\right]$$\n",
    "\n",
    "Where:\n",
    "- $\\pi_\\theta$ is our trainable policy (the model we're training)\n",
    "- $\\pi_{ref}$ is the reference policy (frozen copy of initial model)\n",
    "- $y_w$ is the winning/chosen response\n",
    "- $y_l$ is the losing/rejected response\n",
    "- $\\beta$ is a temperature parameter (typically 0.1)\n",
    "- $\\sigma$ is the sigmoid function\n",
    "\n",
    "### Intuition\n",
    "\n",
    "The loss encourages:\n",
    "- **Higher probability for chosen responses** relative to reference\n",
    "- **Lower probability for rejected responses** relative to reference\n",
    "- **Staying close to the reference model** (the $\\pi_{ref}$ terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nimport warnings\n# Suppress verbose warnings from transformers/PEFT that clutter notebook output\n# (e.g., deprecation warnings, tokenizer warnings that don't affect functionality)\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Dict, List, Tuple, Optional\nimport json\nimport gc\n\n# Set random seeds\ntorch.manual_seed(42)\nnp.random.seed(42)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpo_loss(\n",
    "    policy_chosen_logps: torch.Tensor,\n",
    "    policy_rejected_logps: torch.Tensor,\n",
    "    reference_chosen_logps: torch.Tensor,\n",
    "    reference_rejected_logps: torch.Tensor,\n",
    "    beta: float = 0.1,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute the DPO loss from scratch.\n",
    "    \n",
    "    Args:\n",
    "        policy_chosen_logps: Log probabilities of chosen responses under policy\n",
    "        policy_rejected_logps: Log probabilities of rejected responses under policy\n",
    "        reference_chosen_logps: Log probabilities of chosen responses under reference\n",
    "        reference_rejected_logps: Log probabilities of rejected responses under reference\n",
    "        beta: Temperature parameter controlling deviation from reference\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (loss, chosen_rewards, rejected_rewards)\n",
    "    \"\"\"\n",
    "    # Compute log ratios\n",
    "    # These represent how much more/less likely the response is under policy vs reference\n",
    "    chosen_logratios = policy_chosen_logps - reference_chosen_logps\n",
    "    rejected_logratios = policy_rejected_logps - reference_rejected_logps\n",
    "    \n",
    "    # Compute implicit rewards\n",
    "    # Higher is better for chosen, lower is better for rejected\n",
    "    chosen_rewards = beta * chosen_logratios\n",
    "    rejected_rewards = beta * rejected_logratios\n",
    "    \n",
    "    # DPO loss is negative log sigmoid of reward difference\n",
    "    # We want chosen_rewards > rejected_rewards\n",
    "    logits = chosen_rewards - rejected_rewards\n",
    "    loss = -F.logsigmoid(logits).mean()\n",
    "    \n",
    "    return loss, chosen_rewards.mean(), rejected_rewards.mean()\n",
    "\n",
    "\n",
    "# Demonstrate with simple example\n",
    "batch_size = 4\n",
    "\n",
    "# Simulated log probabilities (more negative = less likely)\n",
    "policy_chosen = torch.tensor([-2.0, -1.5, -2.5, -1.8])\n",
    "policy_rejected = torch.tensor([-3.0, -2.5, -3.5, -2.8])\n",
    "ref_chosen = torch.tensor([-2.5, -2.0, -2.5, -2.0])\n",
    "ref_rejected = torch.tensor([-2.5, -2.0, -3.0, -2.5])\n",
    "\n",
    "loss, chosen_rew, rejected_rew = dpo_loss(\n",
    "    policy_chosen, policy_rejected, ref_chosen, ref_rejected\n",
    ")\n",
    "\n",
    "print(\"DPO Loss Example:\")\n",
    "print(f\"  Loss: {loss.item():.4f}\")\n",
    "print(f\"  Mean chosen reward: {chosen_rew.item():.4f}\")\n",
    "print(f\"  Mean rejected reward: {rejected_rew.item():.4f}\")\n",
    "print(f\"  Reward margin: {(chosen_rew - rejected_rew).item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize DPO loss behavior\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot 1: Loss vs reward margin\nmargins = np.linspace(-5, 5, 100)\nlosses = -np.log(1 / (1 + np.exp(-margins)))\n\naxes[0].plot(margins, losses, 'b-', linewidth=2)\naxes[0].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\naxes[0].fill_between(margins, losses, alpha=0.3, where=(margins > 0))\naxes[0].set_xlabel('Reward Margin (chosen - rejected)')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('DPO Loss vs Reward Margin')\naxes[0].grid(True, alpha=0.3)\naxes[0].annotate('Good: chosen > rejected', xy=(2, 0.2), fontsize=10)\naxes[0].annotate('Bad: rejected > chosen', xy=(-4, 2), fontsize=10)\n\n# Plot 2: Effect of beta\nlog_ratios = np.linspace(-2, 2, 100)\nbetas = [0.05, 0.1, 0.2, 0.5]\n\nfor beta in betas:\n    rewards = beta * log_ratios\n    axes[1].plot(log_ratios, rewards, label=f'β={beta}')\n\naxes[1].set_xlabel('Log Ratio (log π_θ / π_ref)')\naxes[1].set_ylabel('Implicit Reward')\naxes[1].set_title('Effect of β on Reward Scaling')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('dpo_loss_visualization.png', dpi=150)\nplt.show()\nplt.close(fig)  # Release memory\n\nprint(\"\\nKey Insights:\")\nprint(\"1. Loss is low when chosen response has higher reward than rejected\")\nprint(\"2. Higher β makes the model deviate more from reference for the same log ratio\")\nprint(\"3. Lower β keeps the model closer to the reference policy\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Creating Preference Datasets\n",
    "\n",
    "DPO requires pairs of (chosen, rejected) responses for the same prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Sample preference dataset\npreference_data = [\n    {\n        \"prompt\": \"Explain what machine learning is.\",\n        \"chosen\": \"Machine learning is a subset of artificial intelligence that enables computers to learn patterns from data without being explicitly programmed. By analyzing examples, ML algorithms can identify patterns and make predictions or decisions. It's the technology behind recommendation systems, voice assistants, and many other applications we use daily.\",\n        \"rejected\": \"Machine learning is when computers learn stuff. It's like AI but different. You put in data and it learns patterns or whatever.\"\n    },\n    {\n        \"prompt\": \"How do I improve my Python programming skills?\",\n        \"chosen\": \"Here are effective ways to improve your Python skills:\\n\\n1. **Practice daily** - Solve problems on LeetCode, HackerRank, or Codewars\\n2. **Build projects** - Create real applications that solve problems you care about\\n3. **Read quality code** - Study open-source projects on GitHub\\n4. **Learn the standard library** - Python's batteries-included philosophy offers powerful built-in tools\\n5. **Write tests** - Test-driven development improves code quality\\n6. **Review others' code** - Learn different approaches and patterns\",\n        \"rejected\": \"Just practice a lot. Code every day and you'll get better eventually.\"\n    },\n    {\n        \"prompt\": \"What's the difference between a list and a tuple in Python?\",\n        \"chosen\": \"Lists and tuples are both sequence types in Python, but they have key differences:\\n\\n**Mutability:**\\n- Lists are mutable (can be modified after creation)\\n- Tuples are immutable (cannot be changed after creation)\\n\\n**Syntax:**\\n- Lists: `[1, 2, 3]`\\n- Tuples: `(1, 2, 3)`\\n\\n**Performance:**\\n- Tuples are slightly faster due to immutability\\n- Tuples use less memory\\n\\n**Use Cases:**\\n- Lists for collections that may change\\n- Tuples for fixed data like coordinates or function returns\",\n        \"rejected\": \"Lists use [] and tuples use (). Lists can change, tuples can't.\"\n    },\n    {\n        \"prompt\": \"Write a function to calculate factorial.\",\n        \"chosen\": \"\"\"Here's a well-documented factorial function with error handling:\n\n```python\ndef factorial(n: int) -> int:\n    \\\"\\\"\\\"Calculate the factorial of a non-negative integer.\n    \n    Args:\n        n: A non-negative integer\n    \n    Returns:\n        The factorial of n (n!)\n    \n    Raises:\n        ValueError: If n is negative\n    \n    Examples:\n        >>> factorial(5)\n        120\n        >>> factorial(0)\n        1\n    \\\"\\\"\\\" \n    if n < 0:\n        raise ValueError(\"Factorial is not defined for negative numbers\")\n    if n <= 1:\n        return 1\n    return n * factorial(n - 1)\n```\n\nThis handles edge cases and includes type hints and documentation.\"\"\",\n        \"rejected\": \"def f(n): return 1 if n<=1 else n*f(n-1)\"\n    },\n    {\n        \"prompt\": \"What is overfitting in machine learning?\",\n        \"chosen\": \"Overfitting occurs when a machine learning model learns the training data too well, including its noise and random fluctuations. The result is a model that performs excellently on training data but poorly on new, unseen data.\\n\\n**Signs of overfitting:**\\n- Very low training error but high validation error\\n- Large gap between training and validation performance\\n\\n**Solutions:**\\n- Regularization (L1, L2, dropout)\\n- More training data\\n- Simpler model architecture\\n- Cross-validation\\n- Early stopping\",\n        \"rejected\": \"It's when the model memorizes the data instead of learning. Not good.\"\n    },\n    # Edge cases: Similar quality responses where preference is subtle\n    {\n        \"prompt\": \"How do I center a div in CSS?\",\n        \"chosen\": \"There are several modern ways to center a div:\\n\\n**Flexbox (recommended):**\\n```css\\n.container {\\n  display: flex;\\n  justify-content: center;\\n  align-items: center;\\n}\\n```\\n\\n**Grid:**\\n```css\\n.container {\\n  display: grid;\\n  place-items: center;\\n}\\n```\\n\\nBoth methods work for horizontal and vertical centering.\",\n        \"rejected\": \"You can use flexbox with display: flex and then justify-content: center and align-items: center on the parent element.\"\n    },\n    # Edge case: Handling a request that could be misinterpreted\n    {\n        \"prompt\": \"Tell me a joke about programmers.\",\n        \"chosen\": \"Why do programmers prefer dark mode? Because light attracts bugs!\\n\\nHere's another: A programmer's wife tells them, 'Go to the store and buy milk. If they have eggs, get a dozen.' The programmer comes home with 12 cartons of milk. When asked why, they say, 'They had eggs!'\",\n        \"rejected\": \"Programmers are nerdy and don't go outside much. Ha ha.\"\n    },\n    # Edge case: Technical accuracy matters more than length\n    {\n        \"prompt\": \"What is the time complexity of binary search?\",\n        \"chosen\": \"Binary search has O(log n) time complexity, where n is the number of elements. This is because each comparison eliminates half of the remaining elements.\",\n        \"rejected\": \"Binary search is really fast because it keeps dividing the array in half. It's much faster than linear search which checks every element one by one. Binary search only works on sorted arrays though, which is important to remember. The algorithm compares the target with the middle element and decides which half to search next.\"\n    },\n]\n\nprint(f\"Created {len(preference_data)} preference pairs\")\nprint(f\"\\nIncludes {len(preference_data) - 5} edge case examples demonstrating:\")\nprint(\"  - Similar quality responses with subtle differences\")\nprint(\"  - Humor handling\")\nprint(\"  - Accuracy vs verbosity tradeoffs\")\nprint(f\"\\nSample pair:\")\nprint(f\"Prompt: {preference_data[0]['prompt']}\")\nprint(f\"\\nChosen (length: {len(preference_data[0]['chosen'])})\")\nprint(f\"Rejected (length: {len(preference_data[0]['rejected'])})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreferenceDataset:\n",
    "    \"\"\"\n",
    "    Dataset class for DPO training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data: List[Dict], tokenizer, max_length: int = 512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def format_prompt(self, prompt: str, response: str) -> str:\n",
    "        \"\"\"Format prompt and response for the model.\"\"\"\n",
    "        return f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{response}<|eot_id|>\"\"\"\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Format chosen and rejected\n",
    "        chosen_text = self.format_prompt(item['prompt'], item['chosen'])\n",
    "        rejected_text = self.format_prompt(item['prompt'], item['rejected'])\n",
    "        \n",
    "        # Tokenize\n",
    "        chosen_tokens = self.tokenizer(\n",
    "            chosen_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        rejected_tokens = self.tokenizer(\n",
    "            rejected_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'chosen_input_ids': chosen_tokens['input_ids'].squeeze(),\n",
    "            'chosen_attention_mask': chosen_tokens['attention_mask'].squeeze(),\n",
    "            'rejected_input_ids': rejected_tokens['input_ids'].squeeze(),\n",
    "            'rejected_attention_mask': rejected_tokens['attention_mask'].squeeze(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Preference Data from Existing Datasets\n",
    "\n",
    "Often you need to create preference pairs from existing data. Here are strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreferenceDataGenerator:\n",
    "    \"\"\"\n",
    "    Generate preference pairs from various sources.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_quality_scores(\n",
    "        responses: List[Dict],\n",
    "        min_score_diff: float = 0.5\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Create pairs from responses with quality scores.\n",
    "        \n",
    "        Input format: [{\"prompt\": str, \"response\": str, \"score\": float}, ...]\n",
    "        \"\"\"\n",
    "        # Group by prompt\n",
    "        by_prompt = {}\n",
    "        for r in responses:\n",
    "            prompt = r['prompt']\n",
    "            if prompt not in by_prompt:\n",
    "                by_prompt[prompt] = []\n",
    "            by_prompt[prompt].append(r)\n",
    "        \n",
    "        pairs = []\n",
    "        for prompt, responses in by_prompt.items():\n",
    "            # Sort by score\n",
    "            responses.sort(key=lambda x: x['score'], reverse=True)\n",
    "            \n",
    "            # Create pairs from high/low scores\n",
    "            for i, high in enumerate(responses):\n",
    "                for low in responses[i+1:]:\n",
    "                    if high['score'] - low['score'] >= min_score_diff:\n",
    "                        pairs.append({\n",
    "                            'prompt': prompt,\n",
    "                            'chosen': high['response'],\n",
    "                            'rejected': low['response']\n",
    "                        })\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_length_preference(\n",
    "        data: List[Dict],\n",
    "        prefer_longer: bool = True\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Create pairs based on response length.\n",
    "        Useful for teaching the model to give more/less detailed responses.\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        \n",
    "        for item in data:\n",
    "            if 'responses' not in item:\n",
    "                continue\n",
    "            \n",
    "            responses = sorted(item['responses'], key=len, reverse=True)\n",
    "            \n",
    "            if len(responses) >= 2:\n",
    "                if prefer_longer:\n",
    "                    chosen, rejected = responses[0], responses[-1]\n",
    "                else:\n",
    "                    chosen, rejected = responses[-1], responses[0]\n",
    "                \n",
    "                pairs.append({\n",
    "                    'prompt': item['prompt'],\n",
    "                    'chosen': chosen,\n",
    "                    'rejected': rejected\n",
    "                })\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_human_rankings(\n",
    "        rankings: List[Dict]\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Create pairs from human rankings.\n",
    "        \n",
    "        Input: [{\"prompt\": str, \"responses\": [str], \"ranking\": [int]}, ...]\n",
    "        ranking[i] = position of responses[i] (1 = best)\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        \n",
    "        for item in rankings:\n",
    "            responses = item['responses']\n",
    "            ranking = item['ranking']\n",
    "            \n",
    "            # Create all pairs where chosen is ranked higher\n",
    "            for i in range(len(responses)):\n",
    "                for j in range(len(responses)):\n",
    "                    if ranking[i] < ranking[j]:  # lower number = better\n",
    "                        pairs.append({\n",
    "                            'prompt': item['prompt'],\n",
    "                            'chosen': responses[i],\n",
    "                            'rejected': responses[j]\n",
    "                        })\n",
    "        \n",
    "        return pairs\n",
    "\n",
    "\n",
    "# Example with quality scores\n",
    "scored_data = [\n",
    "    {\"prompt\": \"What is AI?\", \"response\": \"AI is advanced computers.\", \"score\": 0.3},\n",
    "    {\"prompt\": \"What is AI?\", \"response\": \"Artificial Intelligence (AI) refers to computer systems designed to perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation.\", \"score\": 0.9},\n",
    "    {\"prompt\": \"What is AI?\", \"response\": \"AI means Artificial Intelligence. It's technology that mimics human thinking.\", \"score\": 0.6},\n",
    "]\n",
    "\n",
    "generated_pairs = PreferenceDataGenerator.from_quality_scores(scored_data, min_score_diff=0.2)\n",
    "print(f\"Generated {len(generated_pairs)} preference pairs from scored data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: DPO Training with TRL\n",
    "\n",
    "Let's use the TRL library to train with DPO on a real model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import DPO trainer with version compatibility\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import Dataset\n\n# Handle TRL API changes across versions\n# TRL >=0.8.0 uses DPOConfig, older versions use TrainingArguments\nfrom trl import DPOTrainer\n\n# Log TRL version for debugging\ntry:\n    import trl\n    print(f\"TRL version: {trl.__version__}\")\nexcept AttributeError:\n    print(\"TRL version: unknown (pre-0.5)\")\n\n# DPOConfig was introduced in TRL 0.8+ with different parameter names\n# We need to detect the TRL version to handle this correctly\nUSE_DPO_CONFIG = False\nDPO_BETA_IN_CONFIG = True  # Whether beta goes in config vs trainer\n\ntry:\n    from trl import DPOConfig\n    USE_DPO_CONFIG = True\n    DPO_BETA_IN_CONFIG = True\n    print(\"Using DPOConfig from TRL (recommended for TRL >=0.8)\")\nexcept ImportError:\n    try:\n        from trl import DPOTrainingArguments as DPOConfig\n        USE_DPO_CONFIG = True\n        DPO_BETA_IN_CONFIG = True\n        print(\"Using DPOTrainingArguments as DPOConfig (TRL 0.7.x)\")\n    except ImportError:\n        # Fallback: older TRL versions use TrainingArguments + pass beta to trainer\n        USE_DPO_CONFIG = False\n        DPO_BETA_IN_CONFIG = False\n        print(\"Note: Using TrainingArguments (older TRL <0.7 - beta passed to DPOTrainer directly)\")\n        print(\"  Consider upgrading TRL: pip install --upgrade trl\")\n\nprint(\"\\nTRL DPO imports successful!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for DPO training\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"  # Or smaller model for faster testing\n",
    "# Alternative: \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" for quick testing\n",
    "\n",
    "# Quantization config for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "print(\"Configuration ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset in the format expected by TRL's DPOTrainer\n",
    "def prepare_dpo_dataset(data: List[Dict]) -> Dataset:\n",
    "    \"\"\"\n",
    "    Prepare dataset for TRL DPOTrainer.\n",
    "    \n",
    "    Expected columns: 'prompt', 'chosen', 'rejected'\n",
    "    \"\"\"\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "# Prepare our preference data\n",
    "dpo_dataset = prepare_dpo_dataset(preference_data)\n",
    "print(f\"DPO Dataset: {len(dpo_dataset)} examples\")\n",
    "print(f\"Columns: {dpo_dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Function to load model and run DPO training\ndef train_with_dpo(\n    model_name: str,\n    dataset: Dataset,\n    output_dir: str = \"./dpo-finetuned\",\n    num_epochs: int = 1,\n    beta: float = 0.1,\n    learning_rate: float = 5e-5,\n):\n    \"\"\"\n    Complete DPO training pipeline.\n    \n    Handles API differences between TRL versions automatically.\n    \"\"\"\n    print(f\"Loading model: {model_name}\")\n    \n    # Load model\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        torch_dtype=torch.bfloat16,\n    )\n    model = prepare_model_for_kbit_training(model)\n    model = get_peft_model(model, lora_config)\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"left\"  # Important for generation\n    \n    # The reference model is a frozen copy\n    # DPOTrainer handles this automatically\n    \n    # Handle TRL API differences\n    if USE_DPO_CONFIG:\n        # Modern TRL with DPOConfig - beta goes in config\n        dpo_config = DPOConfig(\n            output_dir=output_dir,\n            num_train_epochs=num_epochs,\n            per_device_train_batch_size=1,\n            gradient_accumulation_steps=4,\n            learning_rate=learning_rate,\n            beta=beta,  # DPO beta parameter in config\n            max_length=512,\n            max_prompt_length=256,\n            bf16=True,\n            gradient_checkpointing=True,\n            logging_steps=1,\n            save_strategy=\"epoch\",\n            report_to=\"none\",\n        )\n        \n        # Try new API first (processing_class), fall back to tokenizer\n        try:\n            trainer = DPOTrainer(\n                model=model,\n                args=dpo_config,\n                train_dataset=dataset,\n                processing_class=tokenizer,\n            )\n        except TypeError:\n            # Older TRL version uses 'tokenizer' parameter\n            trainer = DPOTrainer(\n                model=model,\n                args=dpo_config,\n                train_dataset=dataset,\n                tokenizer=tokenizer,\n            )\n    else:\n        # Older TRL: use TrainingArguments and pass beta to trainer\n        training_args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=num_epochs,\n            per_device_train_batch_size=1,\n            gradient_accumulation_steps=4,\n            learning_rate=learning_rate,\n            bf16=True,\n            gradient_checkpointing=True,\n            logging_steps=1,\n            save_strategy=\"epoch\",\n            report_to=\"none\",\n        )\n        \n        trainer = DPOTrainer(\n            model=model,\n            args=training_args,\n            beta=beta,  # Pass beta directly to trainer in older TRL\n            train_dataset=dataset,\n            tokenizer=tokenizer,\n            max_length=512,\n            max_prompt_length=256,\n        )\n    \n    print(\"\\nStarting DPO training...\")\n    trainer.train()\n    \n    # Save model\n    trainer.save_model(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    \n    print(f\"\\nModel saved to {output_dir}\")\n    return model, tokenizer\n\nprint(\"DPO training function ready!\")\nprint(\"\\nTo train, run:\")\nprint(\"model, tokenizer = train_with_dpo(MODEL_NAME, dpo_dataset)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to actually run training (requires model access and significant compute)\n",
    "# model, tokenizer = train_with_dpo(MODEL_NAME, dpo_dataset)\n",
    "\n",
    "# For demonstration, we'll show the expected training output\n",
    "print(\"\"\"Expected training output:\n",
    "\n",
    "Loading model: meta-llama/Llama-3.1-8B-Instruct\n",
    "Starting DPO training...\n",
    "\n",
    "{'loss': 0.693, 'chosen_rewards': 0.012, 'rejected_rewards': -0.015, ...}\n",
    "{'loss': 0.651, 'chosen_rewards': 0.045, 'rejected_rewards': -0.032, ...}\n",
    "{'loss': 0.589, 'chosen_rewards': 0.089, 'rejected_rewards': -0.078, ...}\n",
    "...\n",
    "\n",
    "What to look for:\n",
    "- loss should decrease over time\n",
    "- chosen_rewards should increase (positive)\n",
    "- rejected_rewards should decrease (negative)\n",
    "- The gap between chosen and rejected rewards should grow\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Comparing SFT vs DPO\n",
    "\n",
    "Let's compare models trained with only SFT vs SFT + DPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelComparator:\n",
    "    \"\"\"\n",
    "    Compare responses from SFT-only vs DPO-trained models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sft_model, dpo_model, tokenizer):\n",
    "        self.sft_model = sft_model\n",
    "        self.dpo_model = dpo_model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def generate(self, model, prompt: str, max_tokens: int = 200) -> str:\n",
    "        \"\"\"Generate response from a model.\"\"\"\n",
    "        formatted = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        \n",
    "        inputs = self.tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        generated = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "        return self.tokenizer.decode(generated, skip_special_tokens=True)\n",
    "    \n",
    "    def compare(self, prompt: str) -> Dict:\n",
    "        \"\"\"Compare responses from both models.\"\"\"\n",
    "        return {\n",
    "            \"prompt\": prompt,\n",
    "            \"sft_response\": self.generate(self.sft_model, prompt),\n",
    "            \"dpo_response\": self.generate(self.dpo_model, prompt),\n",
    "        }\n",
    "    \n",
    "    def run_comparison(self, prompts: List[str]) -> List[Dict]:\n",
    "        \"\"\"Compare responses for multiple prompts.\"\"\"\n",
    "        results = []\n",
    "        for prompt in prompts:\n",
    "            result = self.compare(prompt)\n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Prompt: {prompt}\")\n",
    "            print(f\"\\n--- SFT Response ---\")\n",
    "            print(result['sft_response'][:500])\n",
    "            print(f\"\\n--- DPO Response ---\")\n",
    "            print(result['dpo_response'][:500])\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation criteria for manual comparison\n",
    "evaluation_criteria = \"\"\"\n",
    "EVALUATION CRITERIA FOR SFT vs DPO COMPARISON\n",
    "=============================================\n",
    "\n",
    "When comparing responses, evaluate on these dimensions:\n",
    "\n",
    "1. HELPFULNESS (0-5)\n",
    "   - Does it actually answer the question?\n",
    "   - Is the information accurate and complete?\n",
    "   - Does it provide actionable guidance?\n",
    "\n",
    "2. CLARITY (0-5)\n",
    "   - Is it well-organized and easy to follow?\n",
    "   - Does it use appropriate formatting?\n",
    "   - Is the language clear and precise?\n",
    "\n",
    "3. DEPTH (0-5)\n",
    "   - Does it provide sufficient detail?\n",
    "   - Does it cover edge cases or nuances?\n",
    "   - Does it offer examples where helpful?\n",
    "\n",
    "4. STYLE (0-5)\n",
    "   - Is the tone appropriate?\n",
    "   - Is it engaging without being overly casual?\n",
    "   - Does it match the complexity to the question?\n",
    "\n",
    "5. SAFETY (0-5)\n",
    "   - Does it avoid harmful content?\n",
    "   - Does it acknowledge uncertainty appropriately?\n",
    "   - Does it avoid hallucination?\n",
    "\n",
    "EXPECTED DIFFERENCES:\n",
    "- DPO should produce responses more aligned with human preferences\n",
    "- DPO often produces more detailed, better-structured responses\n",
    "- DPO may better match the preferred style from training data\n",
    "- SFT alone may produce valid but \"blander\" responses\n",
    "\"\"\"\n",
    "\n",
    "print(evaluation_criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: DPO Variants and Extensions\n",
    "\n",
    "Several improvements to DPO have been proposed. Let's explore the main ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPO Variants\n",
    "dpo_variants = \"\"\"\n",
    "DPO VARIANTS AND EXTENSIONS\n",
    "===========================\n",
    "\n",
    "1. IPO (Identity Preference Optimization)\n",
    "   - Uses a different loss formulation\n",
    "   - Less prone to overfitting on preference data\n",
    "   - Better for smaller datasets\n",
    "\n",
    "2. cDPO (Conservative DPO)\n",
    "   - Adds label smoothing to prevent overconfidence\n",
    "   - More robust when preference labels are noisy\n",
    "\n",
    "3. ORPO (Odds Ratio Preference Optimization)\n",
    "   - Combines SFT and DPO in a single training step\n",
    "   - More memory efficient (no reference model needed)\n",
    "   - Loss: L_SFT + λ * L_odds_ratio\n",
    "\n",
    "4. SimPO (Simple Preference Optimization)\n",
    "   - Removes need for reference model\n",
    "   - Uses length-normalized rewards\n",
    "   - Simpler to implement and train\n",
    "\n",
    "5. KTO (Kahneman-Tversky Optimization)\n",
    "   - Doesn't require paired preferences\n",
    "   - Only needs good/bad labels\n",
    "   - Based on prospect theory\n",
    "\n",
    "CHOOSING THE RIGHT VARIANT:\n",
    "- DPO: Standard choice, well-tested\n",
    "- ORPO: When memory is limited\n",
    "- SimPO: When you want simplicity\n",
    "- KTO: When you only have thumbs up/down data\n",
    "\"\"\"\n",
    "\n",
    "print(dpo_variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement ORPO loss for comparison\n",
    "def orpo_loss(\n",
    "    policy_chosen_logps: torch.Tensor,\n",
    "    policy_rejected_logps: torch.Tensor,\n",
    "    lambda_weight: float = 0.1,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute ORPO loss (no reference model needed!).\n",
    "    \n",
    "    ORPO computes odds ratio: P(chosen) / P(rejected)\n",
    "    and maximizes log odds ratio.\n",
    "    \"\"\"\n",
    "    # Convert log probs to probs\n",
    "    # In practice, we work in log space for numerical stability\n",
    "    \n",
    "    # Log odds ratio = log P(chosen) - log P(rejected)\n",
    "    # This is already what we have!\n",
    "    log_odds_ratio = policy_chosen_logps - policy_rejected_logps\n",
    "    \n",
    "    # ORPO loss: negative log sigmoid of log odds ratio\n",
    "    odds_loss = -F.logsigmoid(log_odds_ratio).mean()\n",
    "    \n",
    "    # SFT loss component (maximize chosen log prob)\n",
    "    sft_loss = -policy_chosen_logps.mean()\n",
    "    \n",
    "    # Combined loss\n",
    "    total_loss = sft_loss + lambda_weight * odds_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "# Test ORPO\n",
    "orpo_l = orpo_loss(policy_chosen, policy_rejected)\n",
    "print(f\"ORPO Loss: {orpo_l.item():.4f}\")\n",
    "print(\"\\nORPO advantage: No reference model needed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Exercises\n",
    "\n",
    "### Exercise 1: Create a Preference Dataset\n",
    "\n",
    "Create at least 20 preference pairs for a specific domain (e.g., customer support, medical Q&A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your preference dataset\n",
    "your_preferences = [\n",
    "    # Add your pairs here\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement SimPO Loss\n",
    "\n",
    "SimPO uses length-normalized rewards and no reference model. Implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: SimPO implementation\n",
    "def simpo_loss(\n",
    "    policy_chosen_logps: torch.Tensor,\n",
    "    policy_rejected_logps: torch.Tensor,\n",
    "    chosen_lengths: torch.Tensor,\n",
    "    rejected_lengths: torch.Tensor,\n",
    "    beta: float = 2.0,\n",
    "    gamma: float = 0.5,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implement SimPO loss.\n",
    "    \n",
    "    Hint: SimPO uses length-normalized log probs and adds a margin.\n",
    "    r_chosen = beta * (logp_chosen / len_chosen)\n",
    "    r_rejected = beta * (logp_rejected / len_rejected)\n",
    "    loss = -log_sigmoid(r_chosen - r_rejected - gamma)\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Imbalanced Preference Pairs\n",
    "\n",
    "```python\n",
    "# ❌ Wrong: Chosen always much longer\n",
    "pairs = [\n",
    "    {\"prompt\": \"...\", \"chosen\": \"Very long detailed response...\", \"rejected\": \"Short.\"},\n",
    "    # Model learns to just output longer responses!\n",
    "]\n",
    "\n",
    "# ✅ Right: Mix of lengths, focus on quality\n",
    "pairs = [\n",
    "    {\"prompt\": \"...\", \"chosen\": \"Concise but complete.\", \"rejected\": \"Verbose but unhelpful...\"},\n",
    "    {\"prompt\": \"...\", \"chosen\": \"Detailed when needed...\", \"rejected\": \"Too brief.\"},\n",
    "]\n",
    "```\n",
    "\n",
    "### Mistake 2: Wrong Beta Value\n",
    "\n",
    "```python\n",
    "# ❌ Wrong: Beta too high\n",
    "beta = 1.0  # Model diverges too far from reference, becomes unstable\n",
    "\n",
    "# ❌ Wrong: Beta too low\n",
    "beta = 0.01  # Model barely learns preferences\n",
    "\n",
    "# ✅ Right: Standard range\n",
    "beta = 0.1  # Good starting point\n",
    "```\n",
    "\n",
    "### Mistake 3: Noisy Preference Labels\n",
    "\n",
    "```python\n",
    "# ❌ Wrong: Inconsistent preferences\n",
    "pairs = [\n",
    "    {\"prompt\": \"...\", \"chosen\": \"Response A\", \"rejected\": \"Response B\"},\n",
    "    {\"prompt\": \"...\", \"chosen\": \"Response B\", \"rejected\": \"Response A\"},  # Contradicts!\n",
    "]\n",
    "\n",
    "# ✅ Right: Consistent, clear preferences\n",
    "# Use multiple annotators and filter disagreements\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ✅ The theory behind DPO and how it simplifies RLHF\n",
    "- ✅ How to create and structure preference datasets\n",
    "- ✅ How to implement DPO loss from scratch\n",
    "- ✅ How to use TRL's DPOTrainer\n",
    "- ✅ DPO variants (ORPO, SimPO, KTO)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [DPO Paper: Direct Preference Optimization](https://arxiv.org/abs/2305.18290)\n",
    "- [ORPO Paper](https://arxiv.org/abs/2403.07691)\n",
    "- [SimPO Paper](https://arxiv.org/abs/2405.14734)\n",
    "- [TRL Documentation](https://huggingface.co/docs/trl)\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "**[Lab 3.1.6: LLaMA Factory Exploration](06-llama-factory-exploration.ipynb)**\n",
    "\n",
    "Learn to use GUI-based fine-tuning with LLaMA Factory!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}