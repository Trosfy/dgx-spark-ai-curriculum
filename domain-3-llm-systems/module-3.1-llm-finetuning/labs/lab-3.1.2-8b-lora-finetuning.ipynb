{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 10.2: Fine-Tuning Llama 3.1 8B with LoRA\n",
    "\n",
    "**Module:** 10 - Large Language Model Fine-Tuning  \n",
    "**Time:** 3 hours  \n",
    "**Difficulty:** ⭐⭐⭐☆☆\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Load Llama 3.1 8B with 4-bit quantization on DGX Spark\n",
    "- [ ] Apply LoRA adapters to attention layers using PEFT\n",
    "- [ ] Prepare an instruction-following dataset\n",
    "- [ ] Fine-tune using Unsloth for 2x faster training\n",
    "- [ ] Evaluate the fine-tuned model on a held-out test set\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Task 10.1 (LoRA Theory)\n",
    "- Knowledge of: PyTorch training loops, Hugging Face Transformers\n",
    "- Required: Llama 3.1 model access (request at meta.ai)\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "### Why Fine-Tune an 8B Model?\n",
    "\n",
    "Imagine you're building a **customer support chatbot** for your company. The base Llama model is great at general conversation, but:\n",
    "\n",
    "- It doesn't know your product names or features\n",
    "- It doesn't follow your company's communication style\n",
    "- It might give incorrect information about your services\n",
    "\n",
    "**Fine-tuning** teaches the model your specific domain while keeping all its general knowledge intact!\n",
    "\n",
    "### What Makes 8B Special?\n",
    "\n",
    "| Model Size | Capability | DGX Spark Training |\n",
    "|------------|------------|--------------------|\n",
    "| 1-3B | Basic tasks | Full fine-tuning possible |\n",
    "| **8B** | **Production quality** | **LoRA is perfect** |\n",
    "| 70B | State-of-the-art | QLoRA required |\n",
    "\n",
    "8B is the **sweet spot** - smart enough for most tasks, small enough to iterate quickly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELI5: What is Fine-Tuning?\n",
    "\n",
    "> **Imagine you hired a brilliant new employee (the base model).** They went to the best schools and know a lot about everything.\n",
    ">\n",
    "> But on their first day, they don't know:\n",
    "> - Your company's product names\n",
    "> - How you talk to customers\n",
    "> - Your internal processes\n",
    ">\n",
    "> **Fine-tuning is like onboarding.** You show them examples of how things are done at YOUR company. After a few days, they combine their general brilliance with your specific knowledge.\n",
    ">\n",
    "> **With LoRA, it's even better:** Instead of retraining them from scratch (expensive!), you just give them a small \"cheat sheet\" of adjustments. They keep everything they learned before, plus your specific tweaks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup\n",
    "\n",
    "First, let's set up our environment. We'll use Unsloth for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup cell - run this first!\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "# Set environment variables for optimal performance\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory monitoring utility\n",
    "def print_gpu_memory(label: str = \"\"):\n",
    "    \"\"\"Print current GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        print(f\"[{label}] GPU Memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU memory.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "print_gpu_memory(\"Initial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Installing Required Libraries\n\nWe'll use Unsloth for faster training when available. Install inside your NGC container:\n\n```bash\n# IMPORTANT: Run inside NGC PyTorch container on DGX Spark\n# The NGC container already includes PyTorch, CUDA, and most dependencies\n\n# Install fine-tuning libraries (these work on ARM64)\npip install --no-deps trl peft accelerate\n\n# Unsloth provides 2x faster training (ARM64 support may vary)\n# If this fails, the notebook automatically falls back to standard transformers\npip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" || echo \"Unsloth not available, using standard transformers\"\n\n# bitsandbytes should already be in NGC container\n# If not: pip install bitsandbytes\n```\n\n**Note for DGX Spark users:**\n- The NGC PyTorch container is recommended (includes ARM64-optimized libraries)\n- If Unsloth installation fails, don't worry - the notebook automatically falls back to standard transformers\n- Standard transformers work perfectly on DGX Spark, just ~2x slower than Unsloth"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Note: warnings already suppressed in cell-3, no need to repeat here\n\nimport torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import Dataset\nfrom trl import SFTTrainer\nimport gc\n\n# Try Unsloth for faster training (optional - falls back gracefully)\ntry:\n    from unsloth import FastLanguageModel\n    USE_UNSLOTH = True\n    print(\"Unsloth available - will use for 2x faster training!\")\nexcept ImportError:\n    USE_UNSLOTH = False\n    print(\"Using standard transformers (Unsloth not available - training will be slower but works fine)\")\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Loading the Model with 4-bit Quantization\n",
    "\n",
    "### ELI5: What is 4-bit Quantization?\n",
    "\n",
    "> **Imagine you have a very detailed map (the model weights)** with every tiny road and path marked.\n",
    ">\n",
    "> **Normal precision (16-bit):** The map shows roads accurate to the centimeter. Very detailed, but takes up lots of space.\n",
    ">\n",
    "> **4-bit quantization:** The map shows roads accurate to the meter. You lose some tiny details, but the major roads are still clear, and the map fits in your pocket!\n",
    ">\n",
    "> **For LLMs:** An 8B model at 16-bit takes ~16GB. At 4-bit, it takes ~4-5GB. You can now fit it in memory with room for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model configuration\nMODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n# Alternatives if you don't have Llama access:\n# MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n# MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # For quick testing\n\nMAX_SEQ_LENGTH = 2048\nLOAD_IN_4BIT = True\n\nprint(f\"Model: {MODEL_NAME}\")\nprint(f\"Max sequence length: {MAX_SEQ_LENGTH}\")\nprint(f\"4-bit quantization: {LOAD_IN_4BIT}\")\n\n# Optional: Clear buffer cache for cleaner memory state\n# This is CRITICAL for 70B models, helpful for 8B to ensure stable loading\nimport subprocess\ntry:\n    subprocess.run([\"sudo\", \"sh\", \"-c\", \"sync; echo 3 > /proc/sys/vm/drop_caches\"], \n                   check=True, capture_output=True)\n    print(\"Buffer cache cleared for optimal memory state\")\nexcept (subprocess.CalledProcessError, FileNotFoundError):\n    print(\"Could not clear buffer cache (requires sudo) - continuing anyway\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load model with Unsloth (faster) or standard transformers\nprint_gpu_memory(\"Before loading\")\n\nif USE_UNSLOTH:\n    # Unsloth path - 2x faster, 50% less memory\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name=MODEL_NAME,\n        max_seq_length=MAX_SEQ_LENGTH,\n        dtype=torch.bfloat16,  # Use bfloat16 for Blackwell\n        load_in_4bit=LOAD_IN_4BIT,\n    )\n    print(\"Model loaded with Unsloth!\")\n    \nelse:\n    # Standard transformers path\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=LOAD_IN_4BIT,\n        bnb_4bit_quant_type=\"nf4\",  # Normal Float 4-bit\n        bnb_4bit_compute_dtype=torch.bfloat16,\n        bnb_4bit_use_double_quant=True,  # Double quantization saves more memory\n    )\n    \n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        torch_dtype=torch.bfloat16,\n        # Note: trust_remote_code not needed for official Llama models\n    )\n    \n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"\n    \n    # Prepare for k-bit training\n    model = prepare_model_for_kbit_training(model)\n    print(\"Model loaded with standard transformers!\")\n\nprint_gpu_memory(\"After loading\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify model is loaded correctly\n",
    "print(f\"Model type: {type(model).__name__}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "# Quick test\n",
    "test_prompt = \"Hello, my name is\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10, do_sample=False)\n",
    "print(f\"\\nTest generation: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Adding LoRA Adapters\n",
    "\n",
    "Now we'll add LoRA adapters to the model. We'll only train these small adapter weights, not the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Configuration\n",
    "LORA_CONFIG = {\n",
    "    \"r\": 16,  # Rank - higher = more capacity but more memory\n",
    "    \"lora_alpha\": 32,  # Scaling factor - usually 2*r\n",
    "    \"lora_dropout\": 0.05,  # Dropout for regularization\n",
    "    \"target_modules\": [\n",
    "        \"q_proj\",  # Query projection\n",
    "        \"k_proj\",  # Key projection  \n",
    "        \"v_proj\",  # Value projection\n",
    "        \"o_proj\",  # Output projection\n",
    "        # Uncomment for more capacity (uses more memory):\n",
    "        # \"gate_proj\",\n",
    "        # \"up_proj\", \n",
    "        # \"down_proj\",\n",
    "    ],\n",
    "    \"bias\": \"none\",  # Don't train biases\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "}\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "for key, value in LORA_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to the model\n",
    "if USE_UNSLOTH:\n",
    "    # Unsloth's optimized LoRA\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=LORA_CONFIG[\"r\"],\n",
    "        lora_alpha=LORA_CONFIG[\"lora_alpha\"],\n",
    "        lora_dropout=LORA_CONFIG[\"lora_dropout\"],\n",
    "        target_modules=LORA_CONFIG[\"target_modules\"],\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized checkpointing\n",
    "        random_state=42,\n",
    "    )\n",
    "else:\n",
    "    # Standard PEFT\n",
    "    lora_config = LoraConfig(**LORA_CONFIG)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.enable_input_require_grads()\n",
    "\n",
    "# Print trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"Trainable params: {trainable_params:,} || \"\n",
    "        f\"All params: {all_param:,} || \"\n",
    "        f\"Trainable%: {100 * trainable_params / all_param:.2f}%\"\n",
    "    )\n",
    "    return trainable_params, all_param\n",
    "\n",
    "trainable, total = print_trainable_parameters(model)\n",
    "print_gpu_memory(\"After LoRA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Happening?\n",
    "\n",
    "Notice that we're only training about **0.1-1%** of the total parameters! This is the magic of LoRA:\n",
    "\n",
    "- **8B parameters** in the base model\n",
    "- **~8-80M trainable LoRA parameters** (depending on config)\n",
    "- **99%+ of weights are frozen**\n",
    "\n",
    "This means:\n",
    "- Much less memory for gradients and optimizer states\n",
    "- Faster training iterations\n",
    "- No risk of \"catastrophic forgetting\" of base model knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Preparing the Training Dataset\n",
    "\n",
    "We'll create a sample instruction-following dataset. In practice, you'd use your own domain-specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample instruction dataset - replace with your own data!\n",
    "sample_data = [\n",
    "    {\n",
    "        \"instruction\": \"What is machine learning?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Machine learning is a subset of artificial intelligence that enables computers to learn patterns from data without being explicitly programmed. Instead of following hard-coded rules, ML algorithms improve their performance through experience, identifying patterns and making decisions based on the data they're trained on.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain the concept of overfitting.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Overfitting occurs when a machine learning model learns the training data too well, including its noise and outliers. This results in excellent performance on training data but poor generalization to new, unseen data. It's like memorizing answers instead of understanding concepts - you'll ace the practice test but struggle with new questions.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is the difference between supervised and unsupervised learning?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Supervised learning uses labeled data where we know the correct answers (like classifying emails as spam/not spam). The model learns to map inputs to known outputs. Unsupervised learning works with unlabeled data, finding hidden patterns or structures (like grouping similar customers). Think of it as learning with a teacher vs. exploring on your own.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Summarize the following text.\",\n",
    "        \"input\": \"The transformer architecture revolutionized natural language processing when it was introduced in 2017. Unlike previous models that processed text sequentially, transformers can process entire sequences in parallel using a mechanism called self-attention. This allows them to capture long-range dependencies in text more effectively and train much faster on modern hardware.\",\n",
    "        \"output\": \"Transformers, introduced in 2017, transformed NLP by processing text in parallel using self-attention, enabling better capture of long-range dependencies and faster training compared to sequential models.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Write a Python function to calculate factorial.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"```python\\ndef factorial(n):\\n    if n < 0:\\n        raise ValueError('Factorial is not defined for negative numbers')\\n    if n <= 1:\\n        return 1\\n    return n * factorial(n - 1)\\n\\n# Example usage:\\nprint(factorial(5))  # Output: 120\\n```\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What are the key hyperparameters in training neural networks?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Key hyperparameters include: 1) Learning rate - controls step size during optimization, 2) Batch size - number of samples processed before updating weights, 3) Number of epochs - full passes through the dataset, 4) Network architecture - layers, neurons, activation functions, 5) Regularization - dropout rate, weight decay, 6) Optimizer choice - Adam, SGD, etc.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain gradient descent in simple terms.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Imagine you're blindfolded on a hilly terrain, trying to find the lowest valley. Gradient descent is like feeling the slope with your feet and taking small steps downhill. The 'gradient' tells you which direction is steepest, and you keep stepping until you reach a valley (minimum loss). The step size is your 'learning rate'.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is transfer learning?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Transfer learning reuses knowledge from a model trained on one task to help with a different but related task. Like how learning to ride a bicycle helps you learn to ride a motorcycle - you don't start from scratch. In AI, we often take models pre-trained on large datasets and fine-tune them for specific tasks, saving time and data.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Compare batch normalization and layer normalization.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Batch normalization normalizes across the batch dimension - it computes statistics from all samples in a batch. Layer normalization normalizes across the feature dimension - each sample is normalized independently. BatchNorm works well for CNNs and large batches, while LayerNorm is preferred for transformers and RNNs where batch sizes may vary.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is LoRA and why is it useful?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"LoRA (Low-Rank Adaptation) is a technique for efficient fine-tuning of large language models. Instead of updating all model weights, LoRA adds small trainable matrices that capture the changes needed for your task. This reduces memory requirements by 10-100x and allows fine-tuning of models that wouldn't otherwise fit in GPU memory.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Created {len(sample_data)} training examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data for Llama 3.1 Instruct format\n",
    "def format_instruction_llama3(example: dict) -> str:\n",
    "    \"\"\"\n",
    "    Format a single example into Llama 3.1 Instruct format.\n",
    "    \n",
    "    Llama 3.1 uses this format:\n",
    "    <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    {system_message}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    {user_message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    {assistant_message}<|eot_id|>\n",
    "    \"\"\"\n",
    "    system_message = \"You are a helpful AI assistant specializing in machine learning and data science.\"\n",
    "    \n",
    "    # Combine instruction and input if input exists\n",
    "    if example.get(\"input\", \"\"):\n",
    "        user_message = f\"{example['instruction']}\\n\\n{example['input']}\"\n",
    "    else:\n",
    "        user_message = example['instruction']\n",
    "    \n",
    "    # Format in Llama 3.1 style\n",
    "    formatted = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_message}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{example['output']}<|eot_id|>\"\"\"\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "# Test formatting\n",
    "print(\"Sample formatted example:\")\n",
    "print(\"=\" * 50)\n",
    "print(format_instruction_llama3(sample_data[0]))\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "formatted_data = [{\"text\": format_instruction_llama3(ex)} for ex in sample_data]\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "# Split into train/eval\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Evaluation examples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Training with Unsloth/SFTTrainer\n",
    "\n",
    "Now the exciting part - actually training the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "OUTPUT_DIR = \"./llama3-8b-lora-finetuned\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,  # Adjust based on memory\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 2 * 4 = 8\n",
    "    \n",
    "    # Optimizer settings\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # Memory optimization\n",
    "    fp16=False,  # Use bf16 instead on Blackwell\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Other\n",
    "    seed=42,\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    args=training_args,\n",
    "    packing=False,  # Set to True for more efficient training with short examples\n",
    ")\n",
    "\n",
    "print(\"Trainer created successfully!\")\n",
    "print_gpu_memory(\"Before training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "start_time = datetime.now()\n",
    "train_result = trainer.train()\n",
    "end_time = datetime.now()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training completed in {end_time - start_time}\")\n",
    "print_gpu_memory(\"After training\")\n",
    "\n",
    "# Print training metrics\n",
    "metrics = train_result.metrics\n",
    "print(f\"\\nTraining Metrics:\")\n",
    "print(f\"  Total steps: {metrics.get('total_steps', 'N/A')}\")\n",
    "print(f\"  Training loss: {metrics.get('train_loss', 'N/A'):.4f}\")\n",
    "print(f\"  Training runtime: {metrics.get('train_runtime', 'N/A'):.2f}s\")\n",
    "print(f\"  Samples/second: {metrics.get('train_samples_per_second', 'N/A'):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Evaluating the Fine-Tuned Model\n",
    "\n",
    "Let's test our fine-tuned model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put model in inference mode\n",
    "if USE_UNSLOTH:\n",
    "    FastLanguageModel.for_inference(model)\n",
    "model.eval()\n",
    "\n",
    "def generate_response(prompt: str, max_new_tokens: int = 256) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response from the fine-tuned model.\n",
    "    \"\"\"\n",
    "    # Format prompt in Llama 3.1 style\n",
    "    system_message = \"You are a helpful AI assistant specializing in machine learning and data science.\"\n",
    "    formatted_prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_message}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated part\n",
    "    generated = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    response = tokenizer.decode(generated, skip_special_tokens=True)\n",
    "    \n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts - mix of in-domain and out-of-domain\n",
    "test_prompts = [\n",
    "    \"What is the difference between LoRA and full fine-tuning?\",\n",
    "    \"Explain attention mechanism in transformers.\",\n",
    "    \"Write a Python function to calculate the mean of a list.\",\n",
    "    \"What is regularization and why is it important?\",\n",
    "]\n",
    "\n",
    "print(\"Testing fine-tuned model:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    response = generate_response(prompt)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Saving the Model\n",
    "\n",
    "We have two options:\n",
    "1. **Save LoRA adapters only** (small, ~50MB)\n",
    "2. **Merge and save full model** (large, ~16GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Save LoRA adapters only (recommended for most use cases)\n",
    "ADAPTER_PATH = \"./llama3-8b-lora-adapter\"\n",
    "\n",
    "model.save_pretrained(ADAPTER_PATH)\n",
    "tokenizer.save_pretrained(ADAPTER_PATH)\n",
    "\n",
    "print(f\"LoRA adapter saved to {ADAPTER_PATH}\")\n",
    "\n",
    "# Check size\n",
    "import os\n",
    "adapter_size = sum(os.path.getsize(os.path.join(ADAPTER_PATH, f)) \n",
    "                   for f in os.listdir(ADAPTER_PATH) \n",
    "                   if os.path.isfile(os.path.join(ADAPTER_PATH, f)))\n",
    "print(f\"Adapter size: {adapter_size / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Merge LoRA into base model and save (for deployment)\n",
    "# Warning: This creates a ~16GB model!\n",
    "\n",
    "MERGED_MODEL_PATH = \"./llama3-8b-merged\"\n",
    "SAVE_MERGED = False  # Set to True to save merged model\n",
    "\n",
    "if SAVE_MERGED:\n",
    "    print(\"Merging LoRA weights into base model...\")\n",
    "    \n",
    "    if USE_UNSLOTH:\n",
    "        # Unsloth method\n",
    "        model.save_pretrained_merged(\n",
    "            MERGED_MODEL_PATH,\n",
    "            tokenizer,\n",
    "            save_method=\"merged_16bit\",\n",
    "        )\n",
    "    else:\n",
    "        # Standard PEFT method\n",
    "        merged_model = model.merge_and_unload()\n",
    "        merged_model.save_pretrained(MERGED_MODEL_PATH)\n",
    "        tokenizer.save_pretrained(MERGED_MODEL_PATH)\n",
    "    \n",
    "    print(f\"Merged model saved to {MERGED_MODEL_PATH}\")\n",
    "else:\n",
    "    print(\"Skipping merged model save (set SAVE_MERGED=True to enable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Loading the Fine-Tuned Model Later\n",
    "\n",
    "Here's how to load your fine-tuned model in a new session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Loading the LoRA adapter later\n",
    "def load_finetuned_model(base_model_name: str, adapter_path: str):\n",
    "    \"\"\"\n",
    "    Load a fine-tuned model by combining base model with LoRA adapter.\n",
    "    \"\"\"\n",
    "    from peft import PeftModel\n",
    "    \n",
    "    # Load base model\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    # Load adapter\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "print(\"To load your fine-tuned model later:\")\n",
    "print(f\"model, tokenizer = load_finetuned_model('{MODEL_NAME}', '{ADAPTER_PATH}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Exercises\n",
    "\n",
    "### Exercise 1: Custom Dataset\n",
    "\n",
    "Create a dataset for a specific domain (e.g., medical, legal, coding) and fine-tune the model.\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Use the same format as `sample_data` but with domain-specific examples. Aim for at least 50-100 examples for meaningful fine-tuning.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your custom dataset here\n",
    "custom_data = [\n",
    "    # Add your domain-specific examples\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Hyperparameter Tuning\n",
    "\n",
    "Experiment with different LoRA configurations:\n",
    "- Try ranks: 8, 16, 32, 64\n",
    "- Try different target modules\n",
    "- Compare training loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Hyperparameter experiments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Evaluation Metrics\n",
    "\n",
    "Implement proper evaluation using:\n",
    "- Perplexity on held-out data\n",
    "- ROUGE scores for summarization\n",
    "- Human evaluation criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Evaluation implementation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Wrong Chat Template\n",
    "\n",
    "```python\n",
    "# ❌ Wrong: Generic prompt format\n",
    "prompt = f\"User: {question}\\nAssistant:\"\n",
    "\n",
    "# ✅ Right: Use model's specific chat template\n",
    "prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Why:** Each model is trained with a specific format. Using the wrong format leads to poor results.\n",
    "\n",
    "### Mistake 2: Not Enabling Gradient Checkpointing\n",
    "\n",
    "```python\n",
    "# ❌ Wrong: No checkpointing\n",
    "training_args = TrainingArguments(\n",
    "    gradient_checkpointing=False,  # OOM error!\n",
    ")\n",
    "\n",
    "# ✅ Right: Enable checkpointing\n",
    "training_args = TrainingArguments(\n",
    "    gradient_checkpointing=True,  # Trades compute for memory\n",
    ")\n",
    "```\n",
    "\n",
    "**Why:** Gradient checkpointing saves memory by recomputing activations during backward pass.\n",
    "\n",
    "### Mistake 3: Too High Learning Rate\n",
    "\n",
    "```python\n",
    "# ❌ Wrong: Standard learning rate\n",
    "learning_rate = 1e-3  # Too high for LoRA\n",
    "\n",
    "# ✅ Right: Lower learning rate for LoRA\n",
    "learning_rate = 2e-4  # Works well for most LoRA fine-tuning\n",
    "```\n",
    "\n",
    "**Why:** LoRA parameters are initialized near zero, so updates are amplified. Lower LR prevents instability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ✅ How to load large models with 4-bit quantization\n",
    "- ✅ How to apply LoRA adapters using PEFT/Unsloth\n",
    "- ✅ How to prepare instruction-following datasets\n",
    "- ✅ How to fine-tune with SFTTrainer\n",
    "- ✅ How to save and load fine-tuned models\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge (Optional): Full Pipeline\n",
    "\n",
    "Build a complete fine-tuning pipeline that:\n",
    "1. Loads data from a CSV file\n",
    "2. Cleans and formats it automatically\n",
    "3. Splits into train/validation/test\n",
    "4. Trains with early stopping\n",
    "5. Evaluates on test set\n",
    "6. Saves the best checkpoint\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Unsloth Documentation](https://github.com/unslothai/unsloth)\n",
    "- [PEFT Library](https://huggingface.co/docs/peft)\n",
    "- [TRL Library](https://huggingface.co/docs/trl)\n",
    "- [Llama 3.1 Model Card](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del model, tokenizer, trainer\n",
    "clear_memory()\n",
    "\n",
    "print(\"Cleanup complete!\")\n",
    "print_gpu_memory(\"After cleanup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "**[Task 10.3: 70B Model QLoRA Fine-Tuning](03-70b-qlora-finetuning.ipynb)**\n",
    "\n",
    "Ready for the DGX Spark showcase? In the next notebook, you'll fine-tune a **70 billion parameter model** - something that would require multiple expensive GPUs elsewhere!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}