{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 3.1.1: LoRA Theory - Understanding Low-Rank Adaptation\n\n**Module:** 3.1 - Large Language Model Fine-Tuning  \n**Time:** 2 hours  \n**Difficulty:** ⭐⭐⭐☆☆\n\n---\n\n## Learning Objectives\n\nBy the end of this notebook, you will:\n- [ ] Understand the mathematical foundation of LoRA (Low-Rank Adaptation)\n- [ ] Implement a LoRA layer from scratch using NumPy and PyTorch\n- [ ] Visualize how LoRA updates work during training\n- [ ] Understand the relationship between LoRA and SVD\n- [ ] Choose appropriate rank values for different tasks\n\n---\n\n## Prerequisites\n\n- Completed: Module 1.4 (Mathematics for Deep Learning) - especially SVD concepts\n- Completed: Module 1.5 (Neural Network Fundamentals)\n- Knowledge of: Matrix operations, gradient descent, neural network layers\n\n---\n\n## Real-World Context\n\n### The Problem: Fine-Tuning is Expensive!\n\nImagine you work at a legal tech company. You want to make GPT-4-level quality responses for legal documents, but:\n- Full fine-tuning of a 70B model requires **140GB+ of GPU memory** (just for weights!)\n- Add gradients and optimizer states, and you need **400-500GB**\n- Training takes weeks and costs thousands of dollars\n\n**LoRA changes everything:** Instead of updating all 70 billion parameters, you update just **0.1-1%** of them through a clever mathematical trick. Your 70B fine-tuning now fits in **45-55GB** on your DGX Spark!\n\n### Who Uses LoRA?\n- **Stability AI** - For fine-tuning Stable Diffusion models\n- **Microsoft** - Built into Azure AI services\n- **Every major AI company** - It's become the standard approach\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELI5: What is LoRA?\n",
    "\n",
    "> **Imagine you have a master chef (the base model) who knows how to cook thousands of dishes.** You want them to specialize in making YOUR family's recipes taste just right.\n",
    ">\n",
    "> **Option 1: Re-train the chef from scratch** (Full Fine-Tuning)  \n",
    "> Send them back to culinary school for 4 years, teaching them everything again plus your recipes. Expensive, time-consuming, and they might forget some things!\n",
    ">\n",
    "> **Option 2: Give them a small recipe card** (LoRA)  \n",
    "> Just give them a tiny note card with adjustments: \"Add a bit more garlic\", \"Use grandma's secret spice\". The chef keeps all their knowledge but makes small adjustments when cooking for YOU.\n",
    ">\n",
    "> **The magic:** The recipe card (LoRA adapter) is tiny - maybe 100 recipes instead of 10,000. But combined with the chef's existing skills, it produces exactly what you want!\n",
    ">\n",
    "> **In AI terms:** Instead of updating a 4096×4096 weight matrix (16 million parameters), LoRA adds two small matrices: 4096×16 and 16×4096 (only 131K parameters - **99% reduction!**). These small matrices encode the \"adjustments\" to the original weights.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Math Behind LoRA\n",
    "\n",
    "### The Core Insight\n",
    "\n",
    "The key insight of LoRA is that **weight updates during fine-tuning have low intrinsic rank**.\n",
    "\n",
    "In plain English: When you fine-tune a model, you don't need to change everything - you only need to make small, targeted adjustments. These adjustments can be represented efficiently using low-rank matrices.\n",
    "\n",
    "### The Math\n",
    "\n",
    "For a pre-trained weight matrix $W_0 \\in \\mathbb{R}^{d \\times k}$, traditional fine-tuning updates it to:\n",
    "\n",
    "$$W = W_0 + \\Delta W$$\n",
    "\n",
    "Where $\\Delta W$ is also a $d \\times k$ matrix (same size as original!).\n",
    "\n",
    "**LoRA's trick:** Instead of storing the full $\\Delta W$, decompose it into two smaller matrices:\n",
    "\n",
    "$$\\Delta W = BA$$\n",
    "\n",
    "Where:\n",
    "- $B \\in \\mathbb{R}^{d \\times r}$ (tall and thin)\n",
    "- $A \\in \\mathbb{R}^{r \\times k}$ (short and wide)\n",
    "- $r \\ll \\min(d, k)$ is the **rank** (typically 4-64)\n",
    "\n",
    "So the adapted forward pass becomes:\n",
    "\n",
    "$$h = W_0 x + BAx$$\n",
    "\n",
    "Or equivalently:\n",
    "\n",
    "$$h = W_0 x + \\frac{\\alpha}{r} BAx$$\n",
    "\n",
    "Where $\\alpha$ is a scaling factor (often called `lora_alpha`).\n",
    "\n",
    "### Parameter Savings\n",
    "\n",
    "| Matrix | Dimensions | Parameters |\n",
    "|--------|------------|------------|\n",
    "| $\\Delta W$ (full) | 4096 × 4096 | 16,777,216 |\n",
    "| $B$ (LoRA) | 4096 × 16 | 65,536 |\n",
    "| $A$ (LoRA) | 16 × 4096 | 65,536 |\n",
    "| **Total LoRA** | - | **131,072** |\n",
    "| **Savings** | - | **99.2%** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing LoRA from Scratch\n",
    "\n",
    "Let's implement LoRA step by step, first with NumPy for clarity, then with PyTorch for GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Requirements: numpy, torch, matplotlib\n# These are all included in the NGC PyTorch container\n\n# Setup: Import required libraries\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom typing import Optional, List, Tuple\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# Check device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name()}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 NumPy Implementation (For Understanding)\n",
    "\n",
    "Let's first build a simple LoRA layer using NumPy to really understand what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayerNumPy:\n",
    "    \"\"\"\n",
    "    A simple LoRA layer implementation in NumPy for educational purposes.\n",
    "    \n",
    "    This implements: h = W_0 @ x + (alpha/r) * B @ A @ x\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int, rank: int = 4, alpha: float = 1.0):\n",
    "        \"\"\"\n",
    "        Initialize LoRA layer.\n",
    "        \n",
    "        Args:\n",
    "            in_features: Input dimension (k)\n",
    "            out_features: Output dimension (d)\n",
    "            rank: LoRA rank (r) - lower = fewer parameters, higher = more expressiveness\n",
    "            alpha: Scaling factor - controls how much LoRA affects the output\n",
    "        \"\"\"\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # Original frozen weights (pretrained) - initialized with Xavier\n",
    "        # In practice, these come from a pretrained model\n",
    "        self.W0 = np.random.randn(out_features, in_features) * np.sqrt(2.0 / (in_features + out_features))\n",
    "        \n",
    "        # LoRA matrices\n",
    "        # A is initialized with small random values (Kaiming/Gaussian)\n",
    "        self.A = np.random.randn(rank, in_features) * 0.01\n",
    "        # B is initialized to zeros - so initially LoRA has NO effect!\n",
    "        self.B = np.zeros((out_features, rank))\n",
    "        \n",
    "        print(f\"LoRA Layer Created:\")\n",
    "        print(f\"  Original W0: {out_features} × {in_features} = {out_features * in_features:,} params\")\n",
    "        print(f\"  LoRA A: {rank} × {in_features} = {rank * in_features:,} params\")\n",
    "        print(f\"  LoRA B: {out_features} × {rank} = {out_features * rank:,} params\")\n",
    "        print(f\"  Total LoRA params: {rank * in_features + out_features * rank:,}\")\n",
    "        print(f\"  Compression ratio: {(rank * in_features + out_features * rank) / (out_features * in_features) * 100:.2f}%\")\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass: h = W_0 @ x + scaling * B @ A @ x\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, in_features)\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, out_features)\n",
    "        \"\"\"\n",
    "        # Original path (frozen)\n",
    "        original_output = x @ self.W0.T\n",
    "        \n",
    "        # LoRA path (trainable)\n",
    "        # First: x @ A.T gives (batch, rank)\n",
    "        # Then: result @ B.T gives (batch, out_features)\n",
    "        lora_output = (x @ self.A.T) @ self.B.T\n",
    "        \n",
    "        # Combined output\n",
    "        return original_output + self.scaling * lora_output\n",
    "    \n",
    "    def get_merged_weights(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Merge LoRA weights with original weights.\n",
    "        After training, you can merge: W = W_0 + scaling * B @ A\n",
    "        This gives you a regular layer with no inference overhead!\n",
    "        \"\"\"\n",
    "        return self.W0 + self.scaling * (self.B @ self.A)\n",
    "\n",
    "\n",
    "# Example: Create a LoRA layer for a typical transformer attention dimension\n",
    "lora_layer = LoRALayerNumPy(in_features=4096, out_features=4096, rank=16, alpha=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's happening here?\n",
    "\n",
    "1. **Original weights (`W0`)**: These are the frozen pretrained weights - we don't update them\n",
    "2. **Matrix A**: Projects input to low-rank space (4096 → 16)\n",
    "3. **Matrix B**: Projects back to output space (16 → 4096)\n",
    "4. **Initialization trick**: B starts at zero, so initially LoRA has NO effect!\n",
    "\n",
    "The key insight: We're not learning a full 4096×4096 update matrix. We're learning a **factorized** version that goes through a 16-dimensional bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's verify the forward pass works\n",
    "batch_size = 8\n",
    "x = np.random.randn(batch_size, 4096)\n",
    "\n",
    "output = lora_layer.forward(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Since B is initialized to zeros, LoRA should have no effect initially\n",
    "original_only = x @ lora_layer.W0.T\n",
    "print(f\"\\nDifference from original (should be ~0): {np.abs(output - original_only).max():.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 PyTorch Implementation (Production Ready)\n",
    "\n",
    "Now let's create a proper PyTorch implementation that can be used for actual training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Production-ready LoRA layer implementation in PyTorch.\n",
    "    \n",
    "    This wraps an existing nn.Linear layer and adds LoRA adapters.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        original_layer: nn.Linear,\n",
    "        rank: int = 4,\n",
    "        alpha: float = 1.0,\n",
    "        dropout: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.original_layer = original_layer\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        in_features = original_layer.in_features\n",
    "        out_features = original_layer.out_features\n",
    "        \n",
    "        # Freeze original weights\n",
    "        self.original_layer.weight.requires_grad = False\n",
    "        if self.original_layer.bias is not None:\n",
    "            self.original_layer.bias.requires_grad = False\n",
    "        \n",
    "        # LoRA matrices\n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        \n",
    "        # Initialize A with Kaiming uniform\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=np.sqrt(5))\n",
    "        # B stays zero - important for training stability!\n",
    "        \n",
    "        # Optional dropout\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Original path\n",
    "        original_output = self.original_layer(x)\n",
    "        \n",
    "        # LoRA path with dropout\n",
    "        lora_output = self.dropout(x) @ self.lora_A.T @ self.lora_B.T\n",
    "        \n",
    "        return original_output + self.scaling * lora_output\n",
    "    \n",
    "    def merge_weights(self) -> None:\n",
    "        \"\"\"Merge LoRA weights into original layer for inference.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.original_layer.weight += self.scaling * (self.lora_B @ self.lora_A)\n",
    "    \n",
    "    def unmerge_weights(self) -> None:\n",
    "        \"\"\"Unmerge LoRA weights (for continuing training).\"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.original_layer.weight -= self.scaling * (self.lora_B @ self.lora_A)\n",
    "    \n",
    "    @property\n",
    "    def trainable_params(self) -> int:\n",
    "        return self.lora_A.numel() + self.lora_B.numel()\n",
    "    \n",
    "    @property\n",
    "    def total_params(self) -> int:\n",
    "        return self.original_layer.weight.numel() + self.trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample linear layer (simulating a transformer attention projection)\n",
    "original_linear = nn.Linear(4096, 4096, bias=False).to(device)\n",
    "\n",
    "# Wrap it with LoRA\n",
    "lora_linear = LoRALayer(original_linear, rank=16, alpha=32, dropout=0.05).to(device)\n",
    "\n",
    "print(f\"Original parameters: {original_linear.weight.numel():,}\")\n",
    "print(f\"LoRA trainable parameters: {lora_linear.trainable_params:,}\")\n",
    "print(f\"Compression: {lora_linear.trainable_params / original_linear.weight.numel() * 100:.2f}%\")\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(8, 4096, device=device)\n",
    "output = lora_linear(x)\n",
    "print(f\"\\nOutput shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 3: Visualizing LoRA Training Dynamics\n\nLet's actually train a simple model with LoRA and visualize what's happening to the weights.\n\n### Key PyTorch Functions Used in This Section\n\nBefore we proceed, here are the key PyTorch functions we'll use:\n\n| Function | Description |\n|----------|-------------|\n| `F.scaled_dot_product_attention(q, k, v)` | PyTorch 2.0+ efficient attention implementation. Computes $\\text{softmax}(QK^T/\\sqrt{d_k})V$ with Flash Attention optimization. |\n| `torch.roll(x, shifts, dims)` | Circular shift of tensor elements along specified dimension. Used for creating synthetic training targets. |\n| `nn.init.kaiming_uniform_(tensor)` | Initializes weights using He initialization, optimal for ReLU/GELU networks. |\n| `tensor.norm()` | Computes the Frobenius norm (default) of a tensor. |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformerBlock(nn.Module):\n",
    "    \"\"\"A simplified transformer block for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int = 512, n_heads: int = 8):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        # Attention projections\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.o_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        # MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Attention\n",
    "        normed = self.norm1(x)\n",
    "        q = self.q_proj(normed).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(normed).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(normed).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        attn = F.scaled_dot_product_attention(q, k, v)\n",
    "        attn = attn.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        x = x + self.o_proj(attn)\n",
    "        \n",
    "        # MLP\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def add_lora_to_model(model: nn.Module, rank: int = 8, alpha: float = 16) -> nn.Module:\n",
    "    \"\"\"Add LoRA adapters to attention projections in a model.\"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear) and any(proj in name for proj in ['q_proj', 'k_proj', 'v_proj', 'o_proj']):\n",
    "            parent_name = '.'.join(name.split('.')[:-1])\n",
    "            child_name = name.split('.')[-1]\n",
    "            \n",
    "            parent = model\n",
    "            for part in parent_name.split('.'):\n",
    "                if part:\n",
    "                    parent = getattr(parent, part)\n",
    "            \n",
    "            lora_layer = LoRALayer(module, rank=rank, alpha=alpha)\n",
    "            setattr(parent, child_name, lora_layer)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple model and add LoRA\n",
    "model = SimpleTransformerBlock(d_model=256, n_heads=8).to(device)\n",
    "\n",
    "# Count parameters before LoRA\n",
    "total_params_before = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters before LoRA: {total_params_before:,}\")\n",
    "\n",
    "# Add LoRA\n",
    "model = add_lora_to_model(model, rank=8, alpha=16)\n",
    "\n",
    "# Count trainable vs frozen parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "\n",
    "print(f\"\\nAfter adding LoRA:\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Frozen parameters: {frozen_params:,}\")\n",
    "print(f\"  Trainable ratio: {trainable_params / (trainable_params + frozen_params) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train this model on a simple task and track LoRA weight changes\n",
    "\n",
    "def generate_synthetic_data(n_samples: int, seq_len: int, d_model: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Generate synthetic sequence data for training.\"\"\"\n",
    "    # Input sequences\n",
    "    x = torch.randn(n_samples, seq_len, d_model)\n",
    "    # Target: shifted version with some transformation\n",
    "    y = torch.roll(x, shifts=1, dims=1) * 0.9 + torch.randn_like(x) * 0.1\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# Generate data\n",
    "x_train, y_train = generate_synthetic_data(1000, 32, 256)\n",
    "x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "\n",
    "# Only train LoRA parameters\n",
    "lora_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(lora_params, lr=1e-3)\n",
    "\n",
    "# Track weights during training\n",
    "weight_history = {'A_norm': [], 'B_norm': [], 'delta_W_norm': []}\n",
    "\n",
    "# Get first LoRA layer for tracking\n",
    "first_lora = model.q_proj\n",
    "\n",
    "print(\"Training with LoRA...\")\n",
    "losses = []\n",
    "n_epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    \n",
    "    # Track weight norms\n",
    "    with torch.no_grad():\n",
    "        weight_history['A_norm'].append(first_lora.lora_A.norm().item())\n",
    "        weight_history['B_norm'].append(first_lora.lora_B.norm().item())\n",
    "        delta_W = first_lora.lora_B @ first_lora.lora_A\n",
    "        weight_history['delta_W_norm'].append(delta_W.norm().item())\n",
    "    \n",
    "    for i in range(0, len(x_train), batch_size):\n",
    "        batch_x = x_train[i:i+batch_size]\n",
    "        batch_y = y_train[i:i+batch_size]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_x)\n",
    "        loss = F.mse_loss(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "    \n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize training dynamics\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Loss curve\naxes[0, 0].plot(losses, 'b-', linewidth=2)\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].set_title('Training Loss (LoRA only)')\naxes[0, 0].grid(True, alpha=0.3)\n\n# LoRA weight norms\naxes[0, 1].plot(weight_history['A_norm'], label='||A||', linewidth=2)\naxes[0, 1].plot(weight_history['B_norm'], label='||B||', linewidth=2)\naxes[0, 1].set_xlabel('Epoch')\naxes[0, 1].set_ylabel('Frobenius Norm')\naxes[0, 1].set_title('LoRA Matrix Norms During Training')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Delta W norm\naxes[1, 0].plot(weight_history['delta_W_norm'], 'g-', linewidth=2)\naxes[1, 0].set_xlabel('Epoch')\naxes[1, 0].set_ylabel('||B @ A||')\naxes[1, 0].set_title('Effective Weight Update Norm (ΔW = BA)')\naxes[1, 0].grid(True, alpha=0.3)\n\n# Visualize the learned ΔW matrix\nwith torch.no_grad():\n    delta_W = (first_lora.lora_B @ first_lora.lora_A).cpu().numpy()\n\nim = axes[1, 1].imshow(delta_W, cmap='RdBu', aspect='auto', vmin=-0.1, vmax=0.1)\naxes[1, 1].set_title('Learned Weight Update (ΔW = BA)')\naxes[1, 1].set_xlabel('Input dimension')\naxes[1, 1].set_ylabel('Output dimension')\nplt.colorbar(im, ax=axes[1, 1])\n\nplt.tight_layout()\nplt.savefig('lora_training_dynamics.png', dpi=150, bbox_inches='tight')\nplt.show()\nplt.close(fig)  # Release memory\n\nprint(\"\\nKey Observations:\")\nprint(\"1. B starts at zero and grows during training\")\nprint(\"2. The effective ΔW = BA has low rank structure\")\nprint(\"3. Weight updates are localized - not all entries change equally\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Happening?\n",
    "\n",
    "Look at the visualizations above:\n",
    "\n",
    "1. **Matrix B grows from zero**: This is the initialization trick - starting from zero means LoRA has no effect initially, ensuring stable training\n",
    "\n",
    "2. **ΔW has structure**: The learned weight update isn't random - it has patterns that reflect the task\n",
    "\n",
    "3. **Low-rank constraint**: The update can only express patterns that fit through the rank-r bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 4: The Connection to SVD\n\nLoRA is deeply connected to Singular Value Decomposition (SVD). Let's explore this connection.\n\n### ELI5: SVD and LoRA\n\n> **Imagine you have a big photograph.** SVD lets you compress it by finding the most important \"ingredients\" (singular values) that make up the image.\n>\n> If you keep only the top 10 ingredients instead of all 1000, you get a slightly blurry but recognizable image.\n>\n> **LoRA makes a similar bet:** The \"changes\" needed to adapt a model to a new task can be captured with just a few key ingredients, not millions of parameters.\n\n### NumPy Linear Algebra Functions Used\n\n| Function | Description |\n|----------|-------------|\n| `np.linalg.svd(A)` | Singular Value Decomposition: factors matrix A = U @ diag(S) @ V^T where U and V are orthogonal matrices and S contains singular values |\n| `np.linalg.norm(A, 'fro')` | Computes Frobenius norm: $\\sqrt{\\sum_{i,j} |a_{ij}|^2}$, measures matrix \"magnitude\" |\n| `np.diag(S)` | Creates diagonal matrix from vector S |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's demonstrate: Full weight update vs low-rank approximation\n",
    "\n",
    "# Simulate a \"full\" fine-tuning weight update\n",
    "np.random.seed(42)\n",
    "d = 512  # dimension\n",
    "\n",
    "# Create a simulated weight update matrix (what full fine-tuning would learn)\n",
    "# In practice, these updates tend to be low-rank!\n",
    "full_delta_W = np.random.randn(d, d) * 0.01\n",
    "\n",
    "# Perform SVD\n",
    "U, S, Vt = np.linalg.svd(full_delta_W, full_matrices=False)\n",
    "\n",
    "print(f\"Original ΔW shape: {full_delta_W.shape}\")\n",
    "print(f\"SVD shapes: U={U.shape}, S={S.shape}, V^T={Vt.shape}\")\n",
    "\n",
    "# Plot singular values\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(S, 'b-', linewidth=2)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Singular Value')\n",
    "plt.title('Singular Values of ΔW')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Compare reconstruction error for different ranks\n",
    "ranks = [4, 8, 16, 32, 64, 128, 256, 512]\n",
    "errors = []\n",
    "\n",
    "for r in ranks:\n",
    "    # Low-rank approximation using top-r singular values\n",
    "    approx = U[:, :r] @ np.diag(S[:r]) @ Vt[:r, :]\n",
    "    error = np.linalg.norm(full_delta_W - approx, 'fro') / np.linalg.norm(full_delta_W, 'fro')\n",
    "    errors.append(error * 100)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(len(ranks)), errors, tick_label=ranks)\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('Reconstruction Error (%)')\n",
    "plt.title('Error vs Rank (Lower = Better)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Parameter savings\n",
    "print(\"\\nParameter counts:\")\n",
    "full_params = d * d\n",
    "for r in [8, 16, 32, 64]:\n",
    "    lora_params = 2 * d * r\n",
    "    print(f\"  Rank {r}: {lora_params:,} params ({lora_params/full_params*100:.2f}% of full), Error: {errors[ranks.index(r)]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight from SVD Analysis\n",
    "\n",
    "The plot above shows that for a random matrix, you need many singular values to get good reconstruction. But in practice, **real fine-tuning updates are much more low-rank**!\n",
    "\n",
    "Research has shown that actual weight updates during LLM fine-tuning can be well-approximated with ranks as low as 4-16. This is the fundamental insight that makes LoRA work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Experimenting with Different Ranks\n",
    "\n",
    "Let's systematically explore how rank affects performance and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_rank(rank: int, n_epochs: int = 50) -> dict:\n",
    "    \"\"\"Train a model with specific LoRA rank and return metrics.\"\"\"\n",
    "    \n",
    "    # Create fresh model\n",
    "    model = SimpleTransformerBlock(d_model=256, n_heads=8).to(device)\n",
    "    model = add_lora_to_model(model, rank=rank, alpha=rank * 2)  # alpha = 2*rank is common\n",
    "    \n",
    "    # Count params\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    # Training setup\n",
    "    lora_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.AdamW(lora_params, lr=1e-3)\n",
    "    \n",
    "    # Train\n",
    "    model.train()\n",
    "    final_losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for i in range(0, len(x_train), 64):\n",
    "            batch_x = x_train[i:i+64]\n",
    "            batch_y = y_train[i:i+64]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x)\n",
    "            loss = F.mse_loss(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if epoch >= n_epochs - 10:  # Average last 10 epochs\n",
    "            final_losses.append(loss.item())\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, optimizer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        'rank': rank,\n",
    "        'trainable_params': trainable,\n",
    "        'final_loss': np.mean(final_losses)\n",
    "    }\n",
    "\n",
    "\n",
    "# Test different ranks\n",
    "ranks_to_test = [4, 8, 16, 32, 64, 128]\n",
    "results = []\n",
    "\n",
    "print(\"Testing different LoRA ranks...\\n\")\n",
    "for rank in ranks_to_test:\n",
    "    print(f\"Training with rank={rank}...\", end=' ')\n",
    "    result = train_with_rank(rank)\n",
    "    results.append(result)\n",
    "    print(f\"Loss: {result['final_loss']:.4f}, Params: {result['trainable_params']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize rank vs performance tradeoff\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nranks = [r['rank'] for r in results]\nparams = [r['trainable_params'] for r in results]\nlosses = [r['final_loss'] for r in results]\n\n# Parameters vs Rank\naxes[0].bar(range(len(ranks)), params, tick_label=ranks, color='steelblue')\naxes[0].set_xlabel('LoRA Rank')\naxes[0].set_ylabel('Trainable Parameters')\naxes[0].set_title('Parameters vs Rank')\naxes[0].grid(True, alpha=0.3)\n\n# Loss vs Rank\naxes[1].bar(range(len(ranks)), losses, tick_label=ranks, color='coral')\naxes[1].set_xlabel('LoRA Rank')\naxes[1].set_ylabel('Final Loss')\naxes[1].set_title('Performance vs Rank')\naxes[1].grid(True, alpha=0.3)\n\n# Efficiency: Loss per 1K params\nefficiency = [l / (p / 1000) for l, p in zip(losses, params)]\naxes[2].bar(range(len(ranks)), efficiency, tick_label=ranks, color='seagreen')\naxes[2].set_xlabel('LoRA Rank')\naxes[2].set_ylabel('Loss per 1K params')\naxes[2].set_title('Efficiency (Lower = Better)')\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('lora_rank_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()\nplt.close(fig)  # Release memory\n\n# Find sweet spot\nbest_efficiency_idx = np.argmin(efficiency)\nprint(f\"\\nSweet Spot Analysis:\")\nprint(f\"  Best efficiency at rank={ranks[best_efficiency_idx]}\")\nprint(f\"  Lowest loss at rank={ranks[np.argmin(losses)]}\")\nprint(f\"\\nRecommendation: For most tasks, rank=16 offers the best tradeoff.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: LoRA Best Practices\n",
    "\n",
    "Based on research and practical experience, here are the key guidelines for using LoRA effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive LoRA configuration guide\n",
    "\n",
    "lora_guidelines = \"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════════════╗\n",
    "║                         LoRA CONFIGURATION GUIDE                              ║\n",
    "╠══════════════════════════════════════════════════════════════════════════════╣\n",
    "║ RANK (r) SELECTION                                                           ║\n",
    "╠════════════════╦═════════════════════════════════════════════════════════════╣\n",
    "║ r = 4-8        ║ Quick experiments, simple adaptation tasks                  ║\n",
    "║ r = 16         ║ DEFAULT - Works well for most fine-tuning tasks             ║\n",
    "║ r = 32-64      ║ Complex tasks requiring more capacity                       ║\n",
    "║ r = 128+       ║ Approaching full fine-tuning, rarely needed                 ║\n",
    "╠══════════════════════════════════════════════════════════════════════════════╣\n",
    "║ ALPHA (α) SELECTION                                                          ║\n",
    "╠════════════════════════════════════════════════════════════════════════════════╣\n",
    "║ Common choices:                                                              ║\n",
    "║   α = r        → Scaling factor = 1.0                                        ║\n",
    "║   α = 2r       → Scaling factor = 2.0 (DEFAULT, more aggressive updates)     ║\n",
    "║   α = 32       → Fixed value, independent of rank                            ║\n",
    "╠══════════════════════════════════════════════════════════════════════════════╣\n",
    "║ TARGET MODULES                                                               ║\n",
    "╠════════════════════════════════════════════════════════════════════════════════╣\n",
    "║ Attention only (minimal):  q_proj, v_proj                                    ║\n",
    "║ Full attention (default):  q_proj, k_proj, v_proj, o_proj                    ║\n",
    "║ With MLP (maximum):        + gate_proj, up_proj, down_proj                   ║\n",
    "╠══════════════════════════════════════════════════════════════════════════════╣\n",
    "║ DROPOUT                                                                      ║\n",
    "╠════════════════════════════════════════════════════════════════════════════════╣\n",
    "║ 0.0            → For large datasets                                          ║\n",
    "║ 0.05 (default) → Standard regularization                                     ║\n",
    "║ 0.1+           → For small datasets or if overfitting                        ║\n",
    "╠══════════════════════════════════════════════════════════════════════════════╣\n",
    "║ DGX SPARK SPECIFIC                                                           ║\n",
    "╠════════════════════════════════════════════════════════════════════════════════╣\n",
    "║ 8B models:   r=16-32, full attention + MLP                                   ║\n",
    "║ 70B models:  r=16, attention only (to fit in 128GB)                          ║\n",
    "║ Always use: bfloat16 compute dtype                                           ║\n",
    "╚══════════════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\"\n",
    "\n",
    "print(lora_guidelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Exercises\n",
    "\n",
    "### Exercise 1: Implement LoRA with Bias\n",
    "\n",
    "The standard LoRA doesn't train biases. Modify the `LoRALayer` class to optionally train the bias term as well.\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Add a `train_bias` parameter to `__init__`. If True, make `self.original_layer.bias.requires_grad = True`.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 1: Your code here\nclass LoRALayerWithBias(nn.Module):\n    \"\"\"\n    TODO: Implement LoRA layer that optionally trains bias.\n    \n    Args:\n        original_layer: The nn.Linear layer to adapt\n        rank: Rank of the low-rank decomposition (r)\n        alpha: Scaling factor for LoRA updates\n        dropout: Dropout probability for LoRA path\n        train_bias: If True, also make bias trainable\n    \n    Example:\n        >>> linear = nn.Linear(512, 512)\n        >>> lora = LoRALayerWithBias(linear, rank=8, train_bias=True)\n        >>> output = lora(torch.randn(4, 512))\n    \"\"\"\n    \n    def __init__(\n        self,\n        original_layer: nn.Linear,\n        rank: int = 4,\n        alpha: float = 1.0,\n        dropout: float = 0.0,\n        train_bias: bool = False,\n    ) -> None:\n        super().__init__()\n        # Your implementation here\n        pass\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Forward pass with LoRA adaptation.\"\"\"\n        # Your implementation here\n        pass"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Compare Different Target Modules\n",
    "\n",
    "Compare training with LoRA on:\n",
    "1. Only Q and V projections\n",
    "2. All attention projections (Q, K, V, O)\n",
    "3. Attention + MLP\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Modify the `add_lora_to_model` function to accept a list of target module names.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 2: Your code here\nfrom typing import List, Dict\n\ndef add_lora_to_model_selective(\n    model: nn.Module,\n    rank: int = 8,\n    alpha: float = 16,\n    target_modules: List[str] = None,\n) -> nn.Module:\n    \"\"\"\n    Add LoRA adapters to specific modules in a model.\n    \n    Args:\n        model: The model to adapt\n        rank: LoRA rank\n        alpha: LoRA scaling factor\n        target_modules: List of module name patterns to target.\n                       e.g., ['q_proj', 'v_proj'] for attention only\n                       e.g., ['q_proj', 'k_proj', 'v_proj', 'o_proj'] for full attention\n                       If None, targets all attention projections.\n    \n    Returns:\n        Modified model with LoRA layers\n    \n    Example:\n        >>> model = SimpleTransformerBlock(d_model=256)\n        >>> # Only Q and V projections\n        >>> model_qv = add_lora_to_model_selective(model, target_modules=['q_proj', 'v_proj'])\n        >>> # All attention projections\n        >>> model_all = add_lora_to_model_selective(model, target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'])\n    \"\"\"\n    # Your implementation here\n    # Hint: Modify add_lora_to_model to use target_modules parameter\n    pass\n\n\ndef compare_target_modules(\n    target_configs: List[List[str]],\n    n_epochs: int = 50,\n) -> Dict[str, Dict]:\n    \"\"\"\n    Compare training with different target module configurations.\n    \n    Args:\n        target_configs: List of target module lists to compare\n                       e.g., [['q_proj', 'v_proj'], ['q_proj', 'k_proj', 'v_proj', 'o_proj']]\n        n_epochs: Number of training epochs\n    \n    Returns:\n        Dictionary mapping config name to results\n    \"\"\"\n    # Your implementation here\n    pass"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Memory Analysis\n",
    "\n",
    "Calculate and visualize the memory savings of LoRA for different model sizes.\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "For a model with `n` parameters, full fine-tuning needs `n * (weight + grad + optimizer_state)` = roughly `n * 16` bytes in float32.\n",
    "LoRA only needs gradient and optimizer state for the LoRA parameters.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 3: Your code here\nfrom typing import Dict\n\ndef calculate_memory_requirements(\n    model_params_billions: float,\n    lora_rank: int = 16,\n    lora_target_ratio: float = 0.1,\n    precision: str = \"float32\",\n) -> Dict[str, float]:\n    \"\"\"\n    Calculate memory requirements for full fine-tuning vs LoRA.\n    \n    Args:\n        model_params_billions: Model size in billions of parameters\n        lora_rank: LoRA rank (r parameter)\n        lora_target_ratio: Fraction of parameters that get LoRA adapters\n                          (e.g., 0.1 means 10% of layers get LoRA)\n        precision: One of \"float32\", \"float16\", \"bfloat16\", \"int8\", \"int4\"\n    \n    Returns:\n        Dictionary with memory estimates in GB:\n        {\n            'full_ft_weights': float,      # Memory for model weights (full FT)\n            'full_ft_gradients': float,    # Memory for gradients (full FT)\n            'full_ft_optimizer': float,    # Memory for optimizer states (full FT)\n            'full_ft_total': float,        # Total memory for full FT\n            'lora_weights': float,         # Memory for model weights (LoRA)\n            'lora_adapters': float,        # Memory for LoRA adapters\n            'lora_gradients': float,       # Memory for gradients (LoRA only)\n            'lora_optimizer': float,       # Memory for optimizer states (LoRA only)\n            'lora_total': float,           # Total memory for LoRA\n            'memory_savings_ratio': float, # LoRA total / Full FT total\n        }\n    \n    Example:\n        >>> mem = calculate_memory_requirements(7.0, lora_rank=16)\n        >>> print(f\"Full FT: {mem['full_ft_total']:.1f} GB\")\n        >>> print(f\"LoRA: {mem['lora_total']:.1f} GB\")\n        >>> print(f\"Savings: {(1 - mem['memory_savings_ratio'])*100:.1f}%\")\n    \n    Hints:\n        - float32: 4 bytes per param\n        - float16/bfloat16: 2 bytes per param\n        - int8: 1 byte per param\n        - int4: 0.5 bytes per param\n        - Adam optimizer stores 2 states per param (m and v)\n        - For LoRA, gradients and optimizer states only for adapter params\n    \"\"\"\n    # Your implementation here\n    pass\n\n\n# Test your implementation\n# mem = calculate_memory_requirements(7.0, lora_rank=16)\n# print(f\"7B model memory requirements:\")\n# print(f\"  Full Fine-tuning: {mem['full_ft_total']:.1f} GB\")\n# print(f\"  LoRA (r=16): {mem['lora_total']:.1f} GB\")\n# print(f\"  Memory savings: {(1 - mem['memory_savings_ratio'])*100:.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting to Freeze Base Weights\n",
    "\n",
    "```python\n",
    "# ❌ Wrong: Base weights still trainable\n",
    "lora_layer = LoRALayer(original_layer, rank=16)\n",
    "optimizer = torch.optim.Adam(model.parameters())  # Trains everything!\n",
    "\n",
    "# ✅ Right: Only train LoRA parameters\n",
    "lora_layer = LoRALayer(original_layer, rank=16)\n",
    "lora_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(lora_params)\n",
    "```\n",
    "\n",
    "**Why:** If you train base weights too, you lose the memory savings and might overwrite valuable pretrained knowledge.\n",
    "\n",
    "### Mistake 2: Wrong Alpha/Rank Ratio\n",
    "\n",
    "```python\n",
    "# ❌ Wrong: Alpha too low relative to rank\n",
    "config = LoraConfig(r=64, lora_alpha=1)  # Scaling = 1/64 = 0.015\n",
    "\n",
    "# ✅ Right: Alpha proportional to rank\n",
    "config = LoraConfig(r=64, lora_alpha=128)  # Scaling = 128/64 = 2.0\n",
    "```\n",
    "\n",
    "**Why:** Too low scaling means LoRA barely affects the output. Too high causes instability.\n",
    "\n",
    "### Mistake 3: Initializing B Non-Zero\n",
    "\n",
    "```python\n",
    "# ❌ Wrong: B initialized randomly\n",
    "self.lora_B = nn.Parameter(torch.randn(out_features, rank))\n",
    "\n",
    "# ✅ Right: B initialized to zeros\n",
    "self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "```\n",
    "\n",
    "**Why:** Zero initialization ensures the model starts identical to the base model. Random B would immediately change all outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ✅ The mathematical foundation of LoRA: $W = W_0 + \\frac{\\alpha}{r}BA$\n",
    "- ✅ How to implement LoRA from scratch in NumPy and PyTorch\n",
    "- ✅ The connection between LoRA and SVD\n",
    "- ✅ How to choose rank, alpha, and target modules\n",
    "- ✅ Best practices for LoRA configuration\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "### Implement QLoRA from Scratch\n",
    "\n",
    "QLoRA combines LoRA with 4-bit quantization. The key components are:\n",
    "\n",
    "1. **NF4 Quantization**: Quantize base weights to 4-bit normal float\n",
    "2. **Double Quantization**: Quantize the quantization constants too\n",
    "3. **LoRA in BFloat16**: Keep LoRA weights in higher precision\n",
    "\n",
    "Try implementing a simplified version of NF4 quantization:\n",
    "\n",
    "```python\n",
    "def quantize_nf4(tensor: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Quantize tensor to 4-bit normal float format.\n",
    "    Returns quantized values and scale factors.\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) - The original paper\n",
    "- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314) - Extends LoRA with quantization\n",
    "- [The PEFT Library](https://github.com/huggingface/peft) - Hugging Face's implementation\n",
    "- [DoRA: Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/abs/2402.09353) - Recent improvement to LoRA\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "\n",
    "# Delete any remaining tensors\n",
    "del x_train, y_train\n",
    "if 'model' in dir():\n",
    "    del model\n",
    "\n",
    "# Clear CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"Cleanup complete!\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Next Steps\n\nNow that you understand LoRA theory, you're ready for:\n\n**[Lab 3.1.2: DoRA Comparison](lab-3.1.2-dora-comparison.ipynb)** - Learn how DoRA improves on LoRA with weight decomposition for +3.7 points improvement!\n\nIn the next notebook, you'll compare LoRA vs DoRA and see how weight-decomposed adaptation leads to better fine-tuning results."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}