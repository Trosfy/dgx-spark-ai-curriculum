{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 10.7 Solution: Ollama Integration\n",
    "\n",
    "**Module:** 10 - Large Language Model Fine-Tuning  \n",
    "**Type:** Solution Notebook\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains complete solutions and working code for deploying a fine-tuned model to Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Overview\n",
    "\n",
    "The complete workflow for deploying a fine-tuned model to Ollama:\n",
    "\n",
    "1. **Merge LoRA weights** with the base model\n",
    "2. **Convert to GGUF** format with appropriate quantization\n",
    "3. **Create Modelfile** with correct chat template\n",
    "4. **Import to Ollama** and verify\n",
    "5. **Test and benchmark** the deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, List\n",
    "\n",
    "# Check for optional dependencies\n",
    "try:\n",
    "    import torch\n",
    "    HAS_TORCH = True\n",
    "except ImportError:\n",
    "    HAS_TORCH = False\n",
    "    print(\"PyTorch not available - merge functions will not work\")\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "    HAS_REQUESTS = True\n",
    "except ImportError:\n",
    "    HAS_REQUESTS = False\n",
    "    print(\"Requests not available - Ollama client will not work\")\n",
    "\n",
    "print(f\"PyTorch available: {HAS_TORCH}\")\n",
    "print(f\"Requests available: {HAS_REQUESTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1: Complete LoRA Weight Merging\n",
    "\n",
    "A production-ready function for merging LoRA adapters with base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lora_complete(\n",
    "    base_model_path: str,\n",
    "    adapter_path: str,\n",
    "    output_path: str,\n",
    "    torch_dtype: str = \"float16\",\n",
    "    device_map: str = \"auto\",\n",
    "    safe_serialization: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Complete solution for merging LoRA weights with base model.\n",
    "    \n",
    "    Args:\n",
    "        base_model_path: HuggingFace model ID or local path\n",
    "        adapter_path: Path to LoRA adapter directory\n",
    "        output_path: Where to save the merged model\n",
    "        torch_dtype: Data type for loading (float16, bfloat16, float32)\n",
    "        device_map: Device mapping strategy\n",
    "        safe_serialization: Use safetensors format\n",
    "    \n",
    "    Returns:\n",
    "        Path to merged model\n",
    "    \"\"\"\n",
    "    if not HAS_TORCH:\n",
    "        raise RuntimeError(\"PyTorch required for merging\")\n",
    "    \n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    from peft import PeftModel\n",
    "    \n",
    "    # Map string to torch dtype\n",
    "    dtype_map = {\n",
    "        \"float16\": torch.float16,\n",
    "        \"bfloat16\": torch.bfloat16,\n",
    "        \"float32\": torch.float32,\n",
    "    }\n",
    "    dtype = dtype_map.get(torch_dtype, torch.float16)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"LoRA Weight Merging\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Base model: {base_model_path}\")\n",
    "    print(f\"Adapter: {adapter_path}\")\n",
    "    print(f\"Output: {output_path}\")\n",
    "    print(f\"Dtype: {torch_dtype}\")\n",
    "    \n",
    "    # Step 1: Load base model\n",
    "    print(\"\\n[1/5] Loading base model...\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_path,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=device_map,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    param_count = sum(p.numel() for p in base_model.parameters())\n",
    "    print(f\"      Loaded {param_count:,} parameters\")\n",
    "    \n",
    "    # Step 2: Load tokenizer\n",
    "    print(\"\\n[2/5] Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "    print(f\"      Vocab size: {tokenizer.vocab_size:,}\")\n",
    "    \n",
    "    # Step 3: Load adapter\n",
    "    print(\"\\n[3/5] Loading LoRA adapter...\")\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        adapter_path,\n",
    "        torch_dtype=dtype,\n",
    "    )\n",
    "    \n",
    "    # Count adapter parameters\n",
    "    adapter_params = sum(\n",
    "        p.numel() for n, p in model.named_parameters() \n",
    "        if \"lora\" in n.lower()\n",
    "    )\n",
    "    print(f\"      Adapter parameters: {adapter_params:,}\")\n",
    "    print(f\"      Adapter ratio: {adapter_params/param_count*100:.2f}%\")\n",
    "    \n",
    "    # Step 4: Merge weights\n",
    "    print(\"\\n[4/5] Merging weights...\")\n",
    "    model = model.merge_and_unload()\n",
    "    print(\"      Merge complete\")\n",
    "    \n",
    "    # Step 5: Save\n",
    "    print(\"\\n[5/5] Saving merged model...\")\n",
    "    output_path = Path(output_path)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    model.save_pretrained(\n",
    "        output_path,\n",
    "        safe_serialization=safe_serialization,\n",
    "    )\n",
    "    tokenizer.save_pretrained(output_path)\n",
    "    \n",
    "    # Calculate total size\n",
    "    total_size = sum(\n",
    "        f.stat().st_size for f in output_path.glob('**/*') \n",
    "        if f.is_file()\n",
    "    )\n",
    "    print(f\"      Saved to: {output_path}\")\n",
    "    print(f\"      Total size: {total_size / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, base_model\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Merge complete!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return str(output_path)\n",
    "\n",
    "print(\"Merge function defined. Usage:\")\n",
    "print(\"  merged_path = merge_lora_complete(\")\n",
    "print(\"      'meta-llama/Llama-3.1-8B-Instruct',\")\n",
    "print(\"      './my-lora-adapter',\")\n",
    "print(\"      './merged-model'\")\n",
    "print(\"  )\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2: GGUF Conversion with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_gguf_complete(\n",
    "    model_path: str,\n",
    "    output_path: str,\n",
    "    quantization: str = \"Q5_K_M\",\n",
    "    llama_cpp_path: str = \"./llama.cpp\",\n",
    "    validate: bool = True,\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Complete GGUF conversion solution with validation.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to HuggingFace format model\n",
    "        output_path: Output path for GGUF file\n",
    "        quantization: Quantization level (F16, Q8_0, Q5_K_M, Q4_K_M, etc.)\n",
    "        llama_cpp_path: Path to llama.cpp repository\n",
    "        validate: Whether to validate the output file\n",
    "    \n",
    "    Returns:\n",
    "        Path to GGUF file or None on failure\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"GGUF Conversion\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Input: {model_path}\")\n",
    "    print(f\"Output: {output_path}\")\n",
    "    print(f\"Quantization: {quantization}\")\n",
    "    \n",
    "    # Validate llama.cpp installation\n",
    "    convert_script = Path(llama_cpp_path) / \"convert_hf_to_gguf.py\"\n",
    "    quantize_bin = Path(llama_cpp_path) / \"llama-quantize\"\n",
    "    \n",
    "    if not convert_script.exists():\n",
    "        print(f\"\\nError: Conversion script not found at {convert_script}\")\n",
    "        print(\"Clone llama.cpp: git clone https://github.com/ggerganov/llama.cpp\")\n",
    "        return None\n",
    "    \n",
    "    # Step 1: Convert to FP16 GGUF\n",
    "    print(\"\\n[1/3] Converting to FP16 GGUF...\")\n",
    "    fp16_path = output_path.replace('.gguf', '-f16.gguf')\n",
    "    \n",
    "    cmd = [\n",
    "        \"python\", str(convert_script),\n",
    "        model_path,\n",
    "        \"--outfile\", fp16_path,\n",
    "        \"--outtype\", \"f16\",\n",
    "    ]\n",
    "    \n",
    "    print(f\"      Running: {' '.join(cmd[:4])}...\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(f\"      Error: {result.stderr[:500]}\")\n",
    "        return None\n",
    "    \n",
    "    fp16_size = os.path.getsize(fp16_path) / 1e9\n",
    "    print(f\"      FP16 GGUF created: {fp16_size:.2f} GB\")\n",
    "    \n",
    "    # Step 2: Quantize (if not FP16)\n",
    "    if quantization.upper() != \"F16\":\n",
    "        print(f\"\\n[2/3] Quantizing to {quantization}...\")\n",
    "        \n",
    "        if not quantize_bin.exists():\n",
    "            print(f\"      Warning: Quantize binary not found at {quantize_bin}\")\n",
    "            print(f\"      Returning FP16 version instead\")\n",
    "            return fp16_path\n",
    "        \n",
    "        cmd = [str(quantize_bin), fp16_path, output_path, quantization]\n",
    "        print(f\"      Running: llama-quantize ...\")\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            print(f\"      Error: {result.stderr[:500]}\")\n",
    "            return fp16_path\n",
    "        \n",
    "        # Cleanup FP16\n",
    "        os.remove(fp16_path)\n",
    "        final_path = output_path\n",
    "    else:\n",
    "        final_path = fp16_path\n",
    "    \n",
    "    final_size = os.path.getsize(final_path) / 1e9\n",
    "    print(f\"      Final GGUF: {final_size:.2f} GB\")\n",
    "    \n",
    "    # Step 3: Validate\n",
    "    if validate:\n",
    "        print(\"\\n[3/3] Validating GGUF file...\")\n",
    "        \n",
    "        # Check file header\n",
    "        with open(final_path, 'rb') as f:\n",
    "            magic = f.read(4)\n",
    "            if magic == b'GGUF':\n",
    "                print(\"      ✓ Valid GGUF header\")\n",
    "            else:\n",
    "                print(f\"      ✗ Invalid header: {magic}\")\n",
    "                return None\n",
    "        \n",
    "        # Check file size is reasonable\n",
    "        if final_size < 0.1:  # Less than 100MB\n",
    "            print(f\"      ✗ File suspiciously small\")\n",
    "            return None\n",
    "        \n",
    "        print(\"      ✓ File size reasonable\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Conversion complete: {final_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return final_path\n",
    "\n",
    "print(\"GGUF conversion function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 3: Complete Modelfile Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_modelfile_complete(\n",
    "    gguf_path: str,\n",
    "    model_name: str,\n",
    "    output_path: str = \"./Modelfile\",\n",
    "    model_family: str = \"llama3\",\n",
    "    system_prompt: str = None,\n",
    "    temperature: float = 0.7,\n",
    "    context_length: int = 4096,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a complete Modelfile for Ollama import.\n",
    "    \n",
    "    Args:\n",
    "        gguf_path: Path to GGUF file\n",
    "        model_name: Name for the model in Ollama\n",
    "        output_path: Where to save Modelfile\n",
    "        model_family: Template family (llama3, mistral, etc.)\n",
    "        system_prompt: Custom system prompt\n",
    "        temperature: Default temperature\n",
    "        context_length: Context window size\n",
    "    \n",
    "    Returns:\n",
    "        Path to created Modelfile\n",
    "    \"\"\"\n",
    "    gguf_abs_path = os.path.abspath(gguf_path)\n",
    "    \n",
    "    # Default system prompts\n",
    "    default_prompts = {\n",
    "        \"llama3\": \"You are a helpful AI assistant.\",\n",
    "        \"mistral\": \"You are a helpful assistant.\",\n",
    "        \"code\": \"You are an expert programmer. Provide clear, working code with explanations.\",\n",
    "    }\n",
    "    \n",
    "    if system_prompt is None:\n",
    "        system_prompt = default_prompts.get(model_family, default_prompts[\"llama3\"])\n",
    "    \n",
    "    # Chat templates by model family\n",
    "    templates = {\n",
    "        \"llama3\": '''{{ if .System }}<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{{ .Response }}<|eot_id|>''',\n",
    "        \n",
    "        \"mistral\": '''[INST] {{ if .System }}{{ .System }} {{ end }}{{ .Prompt }} [/INST] {{ .Response }}''',\n",
    "    }\n",
    "    \n",
    "    template = templates.get(model_family, templates[\"llama3\"])\n",
    "    \n",
    "    # Build Modelfile\n",
    "    modelfile_content = f'''# Modelfile for {model_name}\n",
    "# Generated by Ollama Integration Solution\n",
    "# Model family: {model_family}\n",
    "\n",
    "# Base model from GGUF file\n",
    "FROM {gguf_abs_path}\n",
    "\n",
    "# Model parameters\n",
    "PARAMETER temperature {temperature}\n",
    "PARAMETER top_p 0.9\n",
    "PARAMETER top_k 40\n",
    "PARAMETER num_ctx {context_length}\n",
    "PARAMETER num_predict 512\n",
    "PARAMETER stop \"<|eot_id|>\"\n",
    "PARAMETER stop \"<|end_of_text|>\"\n",
    "\n",
    "# System prompt\n",
    "SYSTEM \"\"\"{system_prompt}\"\"\"\n",
    "\n",
    "# Chat template\n",
    "TEMPLATE \"\"\"{template}\"\"\"\n",
    "\n",
    "# License\n",
    "LICENSE \"\"\"This model is a fine-tuned version of {model_family.title()}.\n",
    "Please respect the original model's license terms.\"\"\"\n",
    "'''\n",
    "    \n",
    "    # Save\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(modelfile_content)\n",
    "    \n",
    "    print(f\"Modelfile created: {output_path}\")\n",
    "    print(f\"\\nTo import to Ollama:\")\n",
    "    print(f\"  ollama create {model_name} -f {output_path}\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Example\n",
    "print(\"Example Modelfile content:\")\n",
    "print(\"=\"*50)\n",
    "create_modelfile_complete(\n",
    "    \"./model.gguf\",\n",
    "    \"my-finetuned-model\",\n",
    "    \"./example-Modelfile\",\n",
    "    system_prompt=\"You are an expert AI educator.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 4: Complete Ollama Client with Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaClientComplete:\n",
    "    \"\"\"\n",
    "    Complete Ollama client with robust error handling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        base_url: str = \"http://localhost:11434\",\n",
    "        timeout: int = 120,\n",
    "    ):\n",
    "        self.base_url = base_url\n",
    "        self.timeout = timeout\n",
    "        self._verify_connection()\n",
    "    \n",
    "    def _verify_connection(self) -> bool:\n",
    "        \"\"\"Verify Ollama server is running.\"\"\"\n",
    "        if not HAS_REQUESTS:\n",
    "            print(\"Warning: requests library not available\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(\n",
    "                f\"{self.base_url}/api/tags\",\n",
    "                timeout=5\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                print(f\"Connected to Ollama at {self.base_url}\")\n",
    "                return True\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"Warning: Cannot connect to Ollama at {self.base_url}\")\n",
    "            print(\"Start Ollama with: ollama serve\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: {e}\")\n",
    "        return False\n",
    "    \n",
    "    def list_models(self) -> List[Dict]:\n",
    "        \"\"\"List all available models.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                f\"{self.base_url}/api/tags\",\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json().get('models', [])\n",
    "        except Exception as e:\n",
    "            print(f\"Error listing models: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        model: str,\n",
    "        prompt: str,\n",
    "        system: str = None,\n",
    "        **kwargs,\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate a response.\n",
    "        \n",
    "        Returns dict with 'response', 'tokens', 'duration' keys.\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            **kwargs,\n",
    "        }\n",
    "        if system:\n",
    "            payload[\"system\"] = system\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/api/generate\",\n",
    "                json=payload,\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            return {\n",
    "                \"response\": data.get('response', ''),\n",
    "                \"tokens\": data.get('eval_count', 0),\n",
    "                \"duration\": time.time() - start_time,\n",
    "                \"model\": model,\n",
    "                \"success\": True,\n",
    "            }\n",
    "        except requests.exceptions.Timeout:\n",
    "            return {\n",
    "                \"response\": \"\",\n",
    "                \"error\": f\"Timeout after {self.timeout}s\",\n",
    "                \"success\": False,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"response\": \"\",\n",
    "                \"error\": str(e),\n",
    "                \"success\": False,\n",
    "            }\n",
    "    \n",
    "    def chat(\n",
    "        self,\n",
    "        model: str,\n",
    "        messages: List[Dict],\n",
    "        **kwargs,\n",
    "    ) -> Dict:\n",
    "        \"\"\"Chat with conversation history.\"\"\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False,\n",
    "            **kwargs,\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/api/chat\",\n",
    "                json=payload,\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            return {\n",
    "                \"response\": data.get('message', {}).get('content', ''),\n",
    "                \"tokens\": data.get('eval_count', 0),\n",
    "                \"duration\": time.time() - start_time,\n",
    "                \"success\": True,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"response\": \"\",\n",
    "                \"error\": str(e),\n",
    "                \"success\": False,\n",
    "            }\n",
    "    \n",
    "    def import_model(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        modelfile_path: str,\n",
    "    ) -> bool:\n",
    "        \"\"\"Import a model from Modelfile.\"\"\"\n",
    "        cmd = [\"ollama\", \"create\", model_name, \"-f\", modelfile_path]\n",
    "        print(f\"Running: {' '.join(cmd)}\")\n",
    "        \n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"Successfully imported: {model_name}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Error: {result.stderr}\")\n",
    "            return False\n",
    "\n",
    "# Create client (will warn if Ollama not running)\n",
    "if HAS_REQUESTS:\n",
    "    client = OllamaClientComplete()\n",
    "    \n",
    "    # List models if connected\n",
    "    models = client.list_models()\n",
    "    if models:\n",
    "        print(f\"\\nAvailable models: {len(models)}\")\n",
    "        for m in models[:5]:\n",
    "            print(f\"  - {m['name']}\")\n",
    "else:\n",
    "    print(\"Ollama client requires 'requests' package\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 5: Complete Benchmarking Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model_complete(\n",
    "    model_name: str,\n",
    "    client: 'OllamaClientComplete',\n",
    "    prompts: List[str] = None,\n",
    "    num_runs: int = 3,\n",
    "    warmup: bool = True,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Complete benchmarking solution with statistics.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Model to benchmark\n",
    "        client: OllamaClient instance\n",
    "        prompts: Test prompts (uses defaults if None)\n",
    "        num_runs: Number of runs per prompt\n",
    "        warmup: Whether to do a warmup run first\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with benchmark results\n",
    "    \"\"\"\n",
    "    if prompts is None:\n",
    "        prompts = [\n",
    "            \"What is machine learning?\",\n",
    "            \"Explain the difference between AI and ML.\",\n",
    "            \"Write a Python function to sort a list.\",\n",
    "            \"What are the benefits of neural networks?\",\n",
    "        ]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Benchmarking: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Prompts: {len(prompts)}\")\n",
    "    print(f\"Runs per prompt: {num_runs}\")\n",
    "    \n",
    "    # Warmup\n",
    "    if warmup:\n",
    "        print(\"\\nWarmup run...\")\n",
    "        client.generate(model_name, \"Hello\")\n",
    "    \n",
    "    # Collect results\n",
    "    results = {\n",
    "        \"model\": model_name,\n",
    "        \"prompts\": len(prompts),\n",
    "        \"runs\": num_runs,\n",
    "        \"times\": [],\n",
    "        \"tokens\": [],\n",
    "        \"tps\": [],  # tokens per second\n",
    "    }\n",
    "    \n",
    "    print(\"\\nRunning benchmark...\")\n",
    "    for run in range(num_runs):\n",
    "        print(f\"  Run {run + 1}/{num_runs}\")\n",
    "        for prompt in prompts:\n",
    "            result = client.generate(model_name, prompt)\n",
    "            \n",
    "            if result[\"success\"]:\n",
    "                results[\"times\"].append(result[\"duration\"])\n",
    "                results[\"tokens\"].append(result[\"tokens\"])\n",
    "                if result[\"duration\"] > 0:\n",
    "                    results[\"tps\"].append(\n",
    "                        result[\"tokens\"] / result[\"duration\"]\n",
    "                    )\n",
    "    \n",
    "    # Calculate statistics\n",
    "    if results[\"times\"]:\n",
    "        import statistics\n",
    "        \n",
    "        results[\"stats\"] = {\n",
    "            \"avg_time\": statistics.mean(results[\"times\"]),\n",
    "            \"std_time\": statistics.stdev(results[\"times\"]) if len(results[\"times\"]) > 1 else 0,\n",
    "            \"min_time\": min(results[\"times\"]),\n",
    "            \"max_time\": max(results[\"times\"]),\n",
    "            \"avg_tokens\": statistics.mean(results[\"tokens\"]),\n",
    "            \"avg_tps\": statistics.mean(results[\"tps\"]) if results[\"tps\"] else 0,\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Results\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Average response time: {results['stats']['avg_time']:.2f}s\")\n",
    "        print(f\"Std deviation: {results['stats']['std_time']:.2f}s\")\n",
    "        print(f\"Range: {results['stats']['min_time']:.2f}s - {results['stats']['max_time']:.2f}s\")\n",
    "        print(f\"Average tokens: {results['stats']['avg_tokens']:.0f}\")\n",
    "        print(f\"Tokens/second: {results['stats']['avg_tps']:.1f}\")\n",
    "    else:\n",
    "        print(\"\\nNo successful runs - check model and connection\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Benchmark function defined.\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  results = benchmark_model_complete('my-model', client)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 6: Complete Deployment Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_to_ollama(\n",
    "    base_model: str,\n",
    "    adapter_path: str,\n",
    "    model_name: str,\n",
    "    quantization: str = \"Q5_K_M\",\n",
    "    work_dir: str = \"./ollama_deploy\",\n",
    "    llama_cpp_path: str = \"./llama.cpp\",\n",
    "    cleanup_intermediate: bool = True,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Complete end-to-end deployment pipeline.\n",
    "    \n",
    "    Args:\n",
    "        base_model: HuggingFace model ID\n",
    "        adapter_path: Path to LoRA adapter\n",
    "        model_name: Name for Ollama model\n",
    "        quantization: GGUF quantization level\n",
    "        work_dir: Working directory for intermediate files\n",
    "        llama_cpp_path: Path to llama.cpp\n",
    "        cleanup_intermediate: Delete intermediate files\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    work_dir = Path(work_dir)\n",
    "    work_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    merged_path = work_dir / \"merged\"\n",
    "    gguf_path = work_dir / f\"{model_name}.gguf\"\n",
    "    modelfile_path = work_dir / \"Modelfile\"\n",
    "    \n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"# Deploying {model_name} to Ollama\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Merge\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 1: Merge LoRA weights\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        merged = merge_lora_complete(\n",
    "            base_model,\n",
    "            adapter_path,\n",
    "            str(merged_path)\n",
    "        )\n",
    "        \n",
    "        # Step 2: Convert to GGUF\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 2: Convert to GGUF\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        gguf = convert_to_gguf_complete(\n",
    "            str(merged_path),\n",
    "            str(gguf_path),\n",
    "            quantization,\n",
    "            llama_cpp_path\n",
    "        )\n",
    "        \n",
    "        if gguf is None:\n",
    "            print(\"GGUF conversion failed\")\n",
    "            return False\n",
    "        \n",
    "        # Step 3: Create Modelfile\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 3: Create Modelfile\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        create_modelfile_complete(\n",
    "            gguf,\n",
    "            model_name,\n",
    "            str(modelfile_path)\n",
    "        )\n",
    "        \n",
    "        # Step 4: Import to Ollama\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 4: Import to Ollama\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        cmd = [\"ollama\", \"create\", model_name, \"-f\", str(modelfile_path)]\n",
    "        print(f\"Running: {' '.join(cmd)}\")\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            print(f\"Error: {result.stderr}\")\n",
    "            return False\n",
    "        \n",
    "        # Step 5: Verify\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 5: Verify deployment\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"list\"],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        if model_name in result.stdout:\n",
    "            print(f\"✓ Model '{model_name}' successfully deployed!\")\n",
    "        else:\n",
    "            print(f\"Warning: Model not found in ollama list\")\n",
    "        \n",
    "        # Cleanup\n",
    "        if cleanup_intermediate:\n",
    "            print(\"\\nCleaning up intermediate files...\")\n",
    "            import shutil\n",
    "            if merged_path.exists():\n",
    "                shutil.rmtree(merged_path)\n",
    "            print(\"Cleanup complete\")\n",
    "        \n",
    "        print(f\"\\n{'#'*60}\")\n",
    "        print(f\"# Deployment complete!\")\n",
    "        print(f\"# Test with: ollama run {model_name}\")\n",
    "        print(f\"{'#'*60}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nDeployment failed: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"Complete deployment function defined.\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  deploy_to_ollama(\")\n",
    "print(\"      'meta-llama/Llama-3.1-8B-Instruct',\")\n",
    "print(\"      './my-lora-adapter',\")\n",
    "print(\"      'my-finetuned-llama'\")\n",
    "print(\"  )\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This solution notebook provides:\n",
    "\n",
    "1. **Complete LoRA Merging** - Production-ready weight merging with progress reporting\n",
    "2. **GGUF Conversion** - With validation and error handling\n",
    "3. **Modelfile Generation** - Support for multiple model families\n",
    "4. **Ollama Client** - Robust API client with timeout handling\n",
    "5. **Benchmarking Suite** - Statistical analysis of model performance\n",
    "6. **End-to-End Pipeline** - Single function to deploy from adapter to Ollama\n",
    "\n",
    "All functions include proper error handling, progress reporting, and documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "gc.collect()\n",
    "\n",
    "if HAS_TORCH and torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Solution notebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
