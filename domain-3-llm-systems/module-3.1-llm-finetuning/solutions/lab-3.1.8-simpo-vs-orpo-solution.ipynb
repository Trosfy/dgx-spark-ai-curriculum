{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1.8: SimPO vs ORPO - Solutions\n",
    "\n",
    "Complete solutions for modern preference optimization exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: SimPO Loss Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def simpo_loss(\n",
    "    policy_chosen_logps: torch.Tensor,\n",
    "    policy_rejected_logps: torch.Tensor,\n",
    "    chosen_lengths: torch.Tensor,\n",
    "    rejected_lengths: torch.Tensor,\n",
    "    beta: float = 2.0,\n",
    "    gamma: float = 0.5\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    SimPO Loss: Simple Preference Optimization.\n",
    "    \n",
    "    Key differences from DPO:\n",
    "    1. No reference model needed\n",
    "    2. Length-normalized log probs\n",
    "    3. Target reward margin (gamma)\n",
    "    \n",
    "    L_SimPO = -log(σ(β * (avg_logp(y_w) - avg_logp(y_l)) - γ))\n",
    "    \n",
    "    Where avg_logp = log_prob / sequence_length\n",
    "    \"\"\"\n",
    "    # Length-normalized log probabilities\n",
    "    chosen_avg_logps = policy_chosen_logps / chosen_lengths\n",
    "    rejected_avg_logps = policy_rejected_logps / rejected_lengths\n",
    "    \n",
    "    # Compute logits with target margin\n",
    "    logits = beta * (chosen_avg_logps - rejected_avg_logps) - gamma\n",
    "    \n",
    "    # Loss\n",
    "    losses = -F.logsigmoid(logits)\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = (logits > 0).float().mean()\n",
    "    margin = (chosen_avg_logps - rejected_avg_logps).mean()\n",
    "    \n",
    "    return losses.mean(), {\n",
    "        \"accuracy\": accuracy.item(),\n",
    "        \"avg_margin\": margin.item(),\n",
    "        \"chosen_avg_logp\": chosen_avg_logps.mean().item(),\n",
    "        \"rejected_avg_logp\": rejected_avg_logps.mean().item()\n",
    "    }\n",
    "\n",
    "# Test\n",
    "torch.manual_seed(42)\n",
    "batch_size = 4\n",
    "\n",
    "# Simulate\n",
    "policy_chosen = torch.randn(batch_size) * 100 - 200  # Total log prob\n",
    "policy_rejected = torch.randn(batch_size) * 100 - 250\n",
    "chosen_lens = torch.randint(50, 200, (batch_size,)).float()\n",
    "rejected_lens = torch.randint(50, 200, (batch_size,)).float()\n",
    "\n",
    "loss, metrics = simpo_loss(\n",
    "    policy_chosen, policy_rejected,\n",
    "    chosen_lens, rejected_lens,\n",
    "    beta=2.0, gamma=0.5\n",
    ")\n",
    "\n",
    "print(f\"SimPO Loss: {loss:.4f}\")\n",
    "print(f\"Metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: ORPO Loss Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orpo_loss(\n",
    "    policy_chosen_logps: torch.Tensor,\n",
    "    policy_rejected_logps: torch.Tensor,\n",
    "    policy_chosen_logits: torch.Tensor,  # For SFT loss\n",
    "    chosen_labels: torch.Tensor,\n",
    "    lambda_weight: float = 0.1\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    ORPO Loss: Odds Ratio Preference Optimization.\n",
    "    \n",
    "    L_ORPO = L_SFT + λ * L_OR\n",
    "    \n",
    "    Where L_OR = -log(σ(log(odds(y_w)) - log(odds(y_l))))\n",
    "    And odds(y) = p(y) / (1 - p(y))\n",
    "    \n",
    "    This combines SFT and preference alignment in one step.\n",
    "    \"\"\"\n",
    "    # SFT loss (standard cross-entropy)\n",
    "    sft_loss = F.cross_entropy(\n",
    "        policy_chosen_logits.view(-1, policy_chosen_logits.size(-1)),\n",
    "        chosen_labels.view(-1),\n",
    "        ignore_index=-100\n",
    "    )\n",
    "    \n",
    "    # Odds ratio loss\n",
    "    # log_odds = log(p / (1-p)) = log(p) - log(1-p) ≈ logp - log(1-exp(logp))\n",
    "    # For numerical stability, we use a simpler approximation\n",
    "    log_odds_chosen = policy_chosen_logps - torch.log1p(-torch.exp(policy_chosen_logps).clamp(max=0.99))\n",
    "    log_odds_rejected = policy_rejected_logps - torch.log1p(-torch.exp(policy_rejected_logps).clamp(max=0.99))\n",
    "    \n",
    "    or_logits = log_odds_chosen - log_odds_rejected\n",
    "    or_loss = -F.logsigmoid(or_logits).mean()\n",
    "    \n",
    "    # Combined loss\n",
    "    total_loss = sft_loss + lambda_weight * or_loss\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = (or_logits > 0).float().mean()\n",
    "    \n",
    "    return total_loss, {\n",
    "        \"sft_loss\": sft_loss.item(),\n",
    "        \"or_loss\": or_loss.item(),\n",
    "        \"total_loss\": total_loss.item(),\n",
    "        \"accuracy\": accuracy.item()\n",
    "    }\n",
    "\n",
    "print(\"ORPO combines SFT + Odds Ratio in single training stage.\")\n",
    "print(\"Benefit: 50% less memory (no reference model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def compare_methods():\n",
    "    \"\"\"\n",
    "    Visual comparison of DPO, SimPO, and ORPO.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # 1. Memory comparison\n",
    "    methods = ['DPO', 'SimPO', 'ORPO']\n",
    "    memory = [100, 50, 50]  # Relative %\n",
    "    colors = ['#e74c3c', '#3498db', '#2ecc71']\n",
    "    \n",
    "    axes[0].bar(methods, memory, color=colors)\n",
    "    axes[0].set_ylabel('Relative Memory (%)')\n",
    "    axes[0].set_title('Memory Usage')\n",
    "    axes[0].set_ylim(0, 120)\n",
    "    for i, v in enumerate(memory):\n",
    "        axes[0].text(i, v + 3, f'{v}%', ha='center', fontweight='bold')\n",
    "    \n",
    "    # 2. Training stages\n",
    "    stages_dpo = ['SFT', 'DPO']\n",
    "    stages_simpo = ['SFT', 'SimPO']\n",
    "    stages_orpo = ['ORPO\\n(combined)']\n",
    "    \n",
    "    axes[1].barh(['DPO'], [2], color='#e74c3c', label='Steps')\n",
    "    axes[1].barh(['SimPO'], [2], color='#3498db')\n",
    "    axes[1].barh(['ORPO'], [1], color='#2ecc71')\n",
    "    axes[1].set_xlabel('Training Stages')\n",
    "    axes[1].set_title('Training Complexity')\n",
    "    \n",
    "    # Add annotations\n",
    "    axes[1].text(2.1, 0, 'SFT + DPO', va='center')\n",
    "    axes[1].text(2.1, 1, 'SFT + SimPO', va='center')\n",
    "    axes[1].text(1.1, 2, 'Single stage!', va='center', fontweight='bold')\n",
    "    \n",
    "    # 3. Performance comparison (based on benchmarks)\n",
    "    benchmarks = ['AlpacaEval', 'MT-Bench', 'Arena Hard']\n",
    "    dpo_scores = [40.2, 7.5, 35.0]\n",
    "    simpo_scores = [44.7, 7.8, 38.0]  # +6.4% on AlpacaEval\n",
    "    orpo_scores = [42.5, 7.6, 36.5]\n",
    "    \n",
    "    x = np.arange(len(benchmarks))\n",
    "    width = 0.25\n",
    "    \n",
    "    axes[2].bar(x - width, dpo_scores, width, label='DPO', color='#e74c3c')\n",
    "    axes[2].bar(x, simpo_scores, width, label='SimPO', color='#3498db')\n",
    "    axes[2].bar(x + width, orpo_scores, width, label='ORPO', color='#2ecc71')\n",
    "    \n",
    "    axes[2].set_ylabel('Score')\n",
    "    axes[2].set_title('Benchmark Performance')\n",
    "    axes[2].set_xticks(x)\n",
    "    axes[2].set_xticklabels(benchmarks)\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('method_comparison.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nMethod Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Method':<10} {'Ref Model':<12} {'Stages':<10} {'Best For':<30}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'DPO':<10} {'Yes':<12} {'2':<10} {'Standard preference alignment':<30}\")\n",
    "    print(f\"{'SimPO':<10} {'No':<12} {'2':<10} {'Best performance, length control':<30}\")\n",
    "    print(f\"{'ORPO':<10} {'No':<12} {'1':<10} {'Memory-constrained training':<30}\")\n",
    "\n",
    "compare_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Complete SimPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import CPOTrainer, CPOConfig\n",
    "from datasets import Dataset\n",
    "\n",
    "def create_simpo_training(\n",
    "    model_id: str = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    output_dir: str = \"./simpo-output\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete SimPO training setup.\n",
    "    \n",
    "    Note: SimPO is implemented in TRL as CPO with loss_type=\"simpo\"\n",
    "    \"\"\"\n",
    "    print(\"Setting up SimPO Training...\")\n",
    "    \n",
    "    # Quantization\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    # Load model (no reference model needed!)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Sample data\n",
    "    data = Dataset.from_list([\n",
    "        {\"prompt\": \"Explain AI\", \"chosen\": \"AI is...\", \"rejected\": \"dunno\"},\n",
    "    ])\n",
    "    \n",
    "    # SimPO config via CPO\n",
    "    training_args = CPOConfig(\n",
    "        output_dir=output_dir,\n",
    "        loss_type=\"simpo\",  # Key setting!\n",
    "        cpo_alpha=0.5,  # gamma in SimPO paper\n",
    "        per_device_train_batch_size=2,\n",
    "        bf16=True,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    trainer = CPOTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=data,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    print(\"\\nSimPO Key Settings:\")\n",
    "    print(f\"  loss_type: simpo\")\n",
    "    print(f\"  gamma (cpo_alpha): {training_args.cpo_alpha}\")\n",
    "    print(f\"  No reference model needed!\")\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "# Uncomment to run\n",
    "# trainer = create_simpo_training()\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Complete ORPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import ORPOTrainer, ORPOConfig\n",
    "\n",
    "def create_orpo_training(\n",
    "    model_id: str = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    output_dir: str = \"./orpo-output\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete ORPO training setup.\n",
    "    \n",
    "    ORPO = SFT + Odds Ratio in single stage!\n",
    "    \"\"\"\n",
    "    print(\"Setting up ORPO Training...\")\n",
    "    \n",
    "    # Quantization\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Sample data\n",
    "    data = Dataset.from_list([\n",
    "        {\"prompt\": \"Explain AI\", \"chosen\": \"AI is...\", \"rejected\": \"dunno\"},\n",
    "    ])\n",
    "    \n",
    "    # ORPO config\n",
    "    training_args = ORPOConfig(\n",
    "        output_dir=output_dir,\n",
    "        beta=0.1,  # Odds ratio weight (lambda in paper)\n",
    "        per_device_train_batch_size=2,\n",
    "        bf16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    trainer = ORPOTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=data,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    print(\"\\nORPO Key Settings:\")\n",
    "    print(f\"  beta (lambda): {training_args.beta}\")\n",
    "    print(f\"  Single training stage (SFT + preference combined)\")\n",
    "    print(f\"  No reference model needed!\")\n",
    "    print(f\"  50% memory savings vs DPO!\")\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "# Uncomment to run\n",
    "# trainer = create_orpo_training()\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Guide\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────┐\n",
    "│          Which method to use?           │\n",
    "└─────────────────────────────────────────┘\n",
    "                    │\n",
    "          ┌─────────┴─────────┐\n",
    "          │ Memory limited?   │\n",
    "          └─────────┬─────────┘\n",
    "           Yes      │      No\n",
    "            │       │       │\n",
    "     ┌──────┴───┐   │   ┌───┴──────┐\n",
    "     │  ORPO    │   │   │ Need best│\n",
    "     │(single   │   │   │ quality? │\n",
    "     │ stage!)  │   │   └────┬─────┘\n",
    "     └──────────┘   │     Yes│    No\n",
    "                    │        │     │\n",
    "              ┌─────┴────┐ ┌─┴───┐ ┌┴────┐\n",
    "              │  SimPO   │ │SimPO│ │ DPO │\n",
    "              │  (+6.4%  │ │     │ │     │\n",
    "              │AlpacaEval)│ └─────┘ └─────┘\n",
    "              └──────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **SimPO**: No ref model + length normalization = +6.4% on AlpacaEval\n",
    "2. **ORPO**: Single stage (SFT + preference) = 50% memory savings\n",
    "3. **Both beat DPO** while being more memory efficient\n",
    "4. **Use SimPO** for best quality, **ORPO** for memory constraints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
