{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 3.1.2 Solutions: 8B Model LoRA Fine-Tuning\n\n**Module:** 3.1 - Large Language Model Fine-Tuning  \n**Difficulty:** ⭐⭐⭐☆☆ (Intermediate)  \n**Exercises:** 4 (Custom LoRA Config, Hyperparameter Tuning, Custom Training Loop, Evaluation Metrics)\n\nThis notebook contains solutions for all exercises in the 8B Model LoRA Fine-Tuning notebook.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1 Solution: Custom LoRA Configuration\n",
    "\n",
    "**Task:** Create a LoRA configuration optimized for code generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "# Code generation benefits from higher rank (more capacity for patterns)\n",
    "# and targeting more modules for comprehensive adaptation\n",
    "\n",
    "code_lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=32,  # Higher rank for complex code patterns\n",
    "    lora_alpha=64,  # alpha = 2 * r is common\n",
    "    lora_dropout=0.1,  # Slight dropout for regularization\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",  # Include K for better attention\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",  # MLP modules for code logic\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    modules_to_save=[\"lm_head\"],  # Important for specialized vocabulary\n",
    ")\n",
    "\n",
    "print(\"Code Generation LoRA Configuration:\")\n",
    "print(f\"  Rank: {code_lora_config.r}\")\n",
    "print(f\"  Alpha: {code_lora_config.lora_alpha}\")\n",
    "print(f\"  Scaling: {code_lora_config.lora_alpha / code_lora_config.r}\")\n",
    "print(f\"  Target modules: {code_lora_config.target_modules}\")\n",
    "print(f\"  Dropout: {code_lora_config.lora_dropout}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation of choices:\n",
    "\n",
    "explanation = \"\"\"\n",
    "Why these settings for code generation?\n",
    "\n",
    "1. HIGHER RANK (r=32):\n",
    "   - Code has strict syntax rules and patterns\n",
    "   - Higher rank captures more complex relationships\n",
    "   - Trade-off: More parameters, but code tasks benefit\n",
    "\n",
    "2. MORE TARGET MODULES:\n",
    "   - gate_proj, up_proj, down_proj: MLP layers handle logic/reasoning\n",
    "   - k_proj: Better key matching for code patterns\n",
    "   - Code generation needs both attention AND reasoning adaptation\n",
    "\n",
    "3. MODULES_TO_SAVE=[\"lm_head\"]:\n",
    "   - Fully fine-tunes the output layer\n",
    "   - Important when dealing with specialized tokens (code syntax)\n",
    "   - Helps model learn language-specific outputs better\n",
    "\n",
    "4. DROPOUT=0.1:\n",
    "   - Code datasets often have patterns that can be memorized\n",
    "   - Slight dropout prevents overfitting to specific code snippets\n",
    "\n",
    "Parameter count estimate for 8B model:\n",
    "- 7 target modules per layer × 32 layers = 224 adapted modules\n",
    "- Each module: 2 × rank × dimension ≈ 2 × 32 × 4096 = 262K params\n",
    "- Total LoRA params: ~60M (still <1% of 8B base!)\n",
    "\"\"\"\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2 Solution: Training Hyperparameter Tuning\n",
    "\n",
    "**Task:** Implement a hyperparameter search comparing different learning rates and batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class HyperparameterConfig:\n",
    "    \"\"\"Configuration for a single hyperparameter experiment.\"\"\"\n",
    "    learning_rate: float\n",
    "    batch_size: int\n",
    "    gradient_accumulation_steps: int\n",
    "    warmup_ratio: float\n",
    "    weight_decay: float\n",
    "    \n",
    "    @property\n",
    "    def effective_batch_size(self) -> int:\n",
    "        return self.batch_size * self.gradient_accumulation_steps\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"gradient_accumulation_steps\": self.gradient_accumulation_steps,\n",
    "            \"effective_batch_size\": self.effective_batch_size,\n",
    "            \"warmup_ratio\": self.warmup_ratio,\n",
    "            \"weight_decay\": self.weight_decay,\n",
    "        }\n",
    "\n",
    "\n",
    "def generate_search_space() -> List[HyperparameterConfig]:\n",
    "    \"\"\"\n",
    "    Generate hyperparameter configurations for grid search.\n",
    "    \n",
    "    Returns:\n",
    "        List of configurations to try\n",
    "    \"\"\"\n",
    "    configs = []\n",
    "    \n",
    "    # Learning rates to try (log scale)\n",
    "    learning_rates = [1e-5, 2e-5, 5e-5, 1e-4, 2e-4]\n",
    "    \n",
    "    # Batch size configurations (batch_size, grad_accum)\n",
    "    # All result in different effective batch sizes\n",
    "    batch_configs = [\n",
    "        (1, 8),   # Effective: 8\n",
    "        (2, 8),   # Effective: 16\n",
    "        (4, 8),   # Effective: 32\n",
    "        (4, 16),  # Effective: 64\n",
    "    ]\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        for batch_size, grad_accum in batch_configs:\n",
    "            configs.append(HyperparameterConfig(\n",
    "                learning_rate=lr,\n",
    "                batch_size=batch_size,\n",
    "                gradient_accumulation_steps=grad_accum,\n",
    "                warmup_ratio=0.1,\n",
    "                weight_decay=0.01,\n",
    "            ))\n",
    "    \n",
    "    return configs\n",
    "\n",
    "\n",
    "# Generate and display search space\n",
    "search_space = generate_search_space()\n",
    "print(f\"Total configurations: {len(search_space)}\")\n",
    "print(\"\\nSample configurations:\")\n",
    "for i, config in enumerate(search_space[:5]):\n",
    "    print(f\"  {i+1}. LR={config.learning_rate:.0e}, \"\n",
    "          f\"Batch={config.batch_size}×{config.gradient_accumulation_steps}=\"\n",
    "          f\"{config.effective_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "import os\n",
    "\n",
    "def create_training_args(\n",
    "    config: HyperparameterConfig,\n",
    "    output_dir: str,\n",
    "    num_train_epochs: int = 3,\n",
    "    max_steps: int = -1,\n",
    ") -> TrainingArguments:\n",
    "    \"\"\"\n",
    "    Create TrainingArguments from a hyperparameter config.\n",
    "    \n",
    "    Args:\n",
    "        config: Hyperparameter configuration\n",
    "        output_dir: Directory for outputs\n",
    "        num_train_epochs: Number of training epochs\n",
    "        max_steps: Max steps (-1 for epoch-based)\n",
    "    \n",
    "    Returns:\n",
    "        TrainingArguments object\n",
    "    \"\"\"\n",
    "    run_name = (f\"lr{config.learning_rate:.0e}_\"\n",
    "                f\"bs{config.effective_batch_size}\")\n",
    "    \n",
    "    return TrainingArguments(\n",
    "        output_dir=os.path.join(output_dir, run_name),\n",
    "        \n",
    "        # From config\n",
    "        learning_rate=config.learning_rate,\n",
    "        per_device_train_batch_size=config.batch_size,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        warmup_ratio=config.warmup_ratio,\n",
    "        weight_decay=config.weight_decay,\n",
    "        \n",
    "        # Training duration\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        max_steps=max_steps,\n",
    "        \n",
    "        # DGX Spark optimizations\n",
    "        bf16=True,\n",
    "        optim=\"adamw_8bit\",\n",
    "        gradient_checkpointing=True,\n",
    "        \n",
    "        # Logging\n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        \n",
    "        # Misc\n",
    "        run_name=run_name,\n",
    "        report_to=\"none\",  # Or \"wandb\" for tracking\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Example usage\n",
    "example_config = search_space[0]\n",
    "example_args = create_training_args(example_config, \"./hp_search\")\n",
    "print(\"Example TrainingArguments created:\")\n",
    "print(f\"  Output dir: {example_args.output_dir}\")\n",
    "print(f\"  Learning rate: {example_args.learning_rate}\")\n",
    "print(f\"  Effective batch size: {example_args.per_device_train_batch_size * example_args.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated hyperparameter search results (for demonstration)\n",
    "# In practice, you would run actual training\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Simulated results (realistic patterns)\n",
    "np.random.seed(42)\n",
    "\n",
    "results = []\n",
    "for config in search_space:\n",
    "    # Simulate loss based on hyperparameters\n",
    "    # Lower is better\n",
    "    base_loss = 2.0\n",
    "    \n",
    "    # Optimal LR around 2e-5\n",
    "    lr_factor = abs(np.log10(config.learning_rate) - np.log10(2e-5)) * 0.3\n",
    "    \n",
    "    # Larger batch sizes generally help (up to a point)\n",
    "    batch_factor = 0.1 * (1 - min(config.effective_batch_size, 32) / 32)\n",
    "    \n",
    "    # Too high LR causes instability\n",
    "    instability = 0.5 if config.learning_rate > 1e-4 else 0\n",
    "    \n",
    "    final_loss = base_loss - 0.5 + lr_factor + batch_factor + instability\n",
    "    final_loss += np.random.normal(0, 0.05)  # Add noise\n",
    "    \n",
    "    results.append({\n",
    "        \"config\": config.to_dict(),\n",
    "        \"final_loss\": max(0.5, final_loss),\n",
    "        \"best_step\": np.random.randint(100, 500),\n",
    "    })\n",
    "\n",
    "# Sort by loss\n",
    "results.sort(key=lambda x: x[\"final_loss\"])\n",
    "\n",
    "print(\"Top 5 Configurations:\")\n",
    "print(\"=\" * 60)\n",
    "for i, r in enumerate(results[:5]):\n",
    "    c = r[\"config\"]\n",
    "    print(f\"{i+1}. Loss: {r['final_loss']:.4f}\")\n",
    "    print(f\"   LR: {c['learning_rate']:.0e}, \"\n",
    "          f\"Batch: {c['effective_batch_size']}, \"\n",
    "          f\"Best step: {r['best_step']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hyperparameter search results\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Extract data for plotting\n",
    "lrs = [r[\"config\"][\"learning_rate\"] for r in results]\n",
    "batch_sizes = [r[\"config\"][\"effective_batch_size\"] for r in results]\n",
    "losses = [r[\"final_loss\"] for r in results]\n",
    "\n",
    "# Plot 1: Learning rate vs Loss\n",
    "ax1 = axes[0]\n",
    "scatter1 = ax1.scatter(lrs, losses, c=batch_sizes, cmap='viridis', \n",
    "                       s=100, alpha=0.7)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('Learning Rate')\n",
    "ax1.set_ylabel('Final Loss')\n",
    "ax1.set_title('Learning Rate vs Loss\\n(color = batch size)')\n",
    "plt.colorbar(scatter1, ax=ax1, label='Effective Batch Size')\n",
    "ax1.axvline(x=2e-5, color='red', linestyle='--', alpha=0.5, label='Optimal LR region')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Batch size vs Loss\n",
    "ax2 = axes[1]\n",
    "scatter2 = ax2.scatter(batch_sizes, losses, c=np.log10(lrs), cmap='plasma',\n",
    "                       s=100, alpha=0.7)\n",
    "ax2.set_xlabel('Effective Batch Size')\n",
    "ax2.set_ylabel('Final Loss')\n",
    "ax2.set_title('Batch Size vs Loss\\n(color = log10(LR))')\n",
    "plt.colorbar(scatter2, ax=ax2, label='log10(Learning Rate)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hp_search_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"- Optimal learning rate: ~2e-5 (sweet spot for LoRA)\")\n",
    "print(\"- Larger batch sizes help stability but diminishing returns past 32\")\n",
    "print(\"- High LR (>1e-4) causes training instability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3 Solution: Custom Training Loop with Memory Monitoring\n",
    "\n",
    "**Task:** Implement a training loop that tracks memory usage and supports gradient accumulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Optional, Callable, Dict, Any\n",
    "from dataclasses import dataclass, field\n",
    "import time\n",
    "import gc\n",
    "\n",
    "@dataclass\n",
    "class TrainingMetrics:\n",
    "    \"\"\"Container for training metrics.\"\"\"\n",
    "    step: int = 0\n",
    "    epoch: int = 0\n",
    "    loss: float = 0.0\n",
    "    learning_rate: float = 0.0\n",
    "    gpu_memory_gb: float = 0.0\n",
    "    tokens_per_second: float = 0.0\n",
    "    gradient_norm: float = 0.0\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"step\": self.step,\n",
    "            \"epoch\": self.epoch,\n",
    "            \"loss\": self.loss,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"gpu_memory_gb\": self.gpu_memory_gb,\n",
    "            \"tokens_per_second\": self.tokens_per_second,\n",
    "            \"gradient_norm\": self.gradient_norm,\n",
    "        }\n",
    "\n",
    "\n",
    "class MemoryEfficientTrainer:\n",
    "    \"\"\"\n",
    "    Custom trainer with memory monitoring and gradient accumulation.\n",
    "    \n",
    "    Optimized for DGX Spark's unified memory architecture.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,\n",
    "        gradient_accumulation_steps: int = 1,\n",
    "        max_grad_norm: float = 1.0,\n",
    "        use_amp: bool = True,\n",
    "        log_interval: int = 10,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.use_amp = use_amp\n",
    "        self.log_interval = log_interval\n",
    "        \n",
    "        # Mixed precision scaler (for non-BF16 systems)\n",
    "        self.scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "        \n",
    "        # Metrics history\n",
    "        self.history: list = []\n",
    "        self.global_step = 0\n",
    "        \n",
    "    def get_memory_stats(self) -> Dict[str, float]:\n",
    "        \"\"\"Get current GPU memory statistics.\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return {\"allocated\": 0, \"reserved\": 0, \"max_allocated\": 0}\n",
    "        \n",
    "        return {\n",
    "            \"allocated\": torch.cuda.memory_allocated() / 1e9,\n",
    "            \"reserved\": torch.cuda.memory_reserved() / 1e9,\n",
    "            \"max_allocated\": torch.cuda.max_memory_allocated() / 1e9,\n",
    "        }\n",
    "    \n",
    "    def compute_gradient_norm(self) -> float:\n",
    "        \"\"\"Compute total gradient norm across all parameters.\"\"\"\n",
    "        total_norm = 0.0\n",
    "        for p in self.model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        return total_norm ** 0.5\n",
    "    \n",
    "    def train_epoch(\n",
    "        self,\n",
    "        train_loader: DataLoader,\n",
    "        epoch: int,\n",
    "        callback: Optional[Callable[[TrainingMetrics], None]] = None,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Train for one epoch with gradient accumulation and memory monitoring.\n",
    "        \n",
    "        Args:\n",
    "            train_loader: DataLoader for training data\n",
    "            epoch: Current epoch number\n",
    "            callback: Optional callback for each logging step\n",
    "        \n",
    "        Returns:\n",
    "            Average loss for the epoch\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        accumulated_loss = 0.0\n",
    "        \n",
    "        # Reset memory stats\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        tokens_processed = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # Move batch to device\n",
    "            if isinstance(batch, dict):\n",
    "                batch = {k: v.cuda() if torch.is_tensor(v) else v \n",
    "                        for k, v in batch.items()}\n",
    "                input_ids = batch.get(\"input_ids\")\n",
    "            else:\n",
    "                batch = batch.cuda()\n",
    "                input_ids = batch\n",
    "            \n",
    "            # Track tokens\n",
    "            if input_ids is not None:\n",
    "                tokens_processed += input_ids.numel()\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=self.use_amp):\n",
    "                if isinstance(batch, dict):\n",
    "                    outputs = self.model(**batch)\n",
    "                    loss = outputs.loss if hasattr(outputs, 'loss') else outputs[0]\n",
    "                else:\n",
    "                    outputs = self.model(batch)\n",
    "                    loss = outputs\n",
    "                \n",
    "                # Scale loss for gradient accumulation\n",
    "                loss = loss / self.gradient_accumulation_steps\n",
    "            \n",
    "            # Backward pass\n",
    "            if self.scaler:\n",
    "                self.scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            \n",
    "            accumulated_loss += loss.item()\n",
    "            \n",
    "            # Optimizer step (after accumulation)\n",
    "            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:\n",
    "                # Gradient clipping\n",
    "                if self.scaler:\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                \n",
    "                grad_norm = self.compute_gradient_norm()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.model.parameters(), \n",
    "                    self.max_grad_norm\n",
    "                )\n",
    "                \n",
    "                # Optimizer step\n",
    "                if self.scaler:\n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    self.scaler.update()\n",
    "                else:\n",
    "                    self.optimizer.step()\n",
    "                \n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                self.global_step += 1\n",
    "                \n",
    "                # Record metrics\n",
    "                total_loss += accumulated_loss\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Log at intervals\n",
    "                if self.global_step % self.log_interval == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    memory_stats = self.get_memory_stats()\n",
    "                    \n",
    "                    metrics = TrainingMetrics(\n",
    "                        step=self.global_step,\n",
    "                        epoch=epoch,\n",
    "                        loss=accumulated_loss,\n",
    "                        learning_rate=self.optimizer.param_groups[0][\"lr\"],\n",
    "                        gpu_memory_gb=memory_stats[\"allocated\"],\n",
    "                        tokens_per_second=tokens_processed / elapsed,\n",
    "                        gradient_norm=grad_norm,\n",
    "                    )\n",
    "                    \n",
    "                    self.history.append(metrics.to_dict())\n",
    "                    \n",
    "                    if callback:\n",
    "                        callback(metrics)\n",
    "                    else:\n",
    "                        print(f\"Step {self.global_step}: \"\n",
    "                              f\"loss={accumulated_loss:.4f}, \"\n",
    "                              f\"lr={metrics.learning_rate:.2e}, \"\n",
    "                              f\"mem={metrics.gpu_memory_gb:.1f}GB, \"\n",
    "                              f\"tok/s={metrics.tokens_per_second:.0f}\")\n",
    "                \n",
    "                accumulated_loss = 0.0\n",
    "        \n",
    "        avg_loss = total_loss / max(num_batches, 1)\n",
    "        return avg_loss\n",
    "    \n",
    "    def get_memory_summary(self) -> str:\n",
    "        \"\"\"Get a summary of memory usage during training.\"\"\"\n",
    "        if not self.history:\n",
    "            return \"No training history available.\"\n",
    "        \n",
    "        memory_values = [h[\"gpu_memory_gb\"] for h in self.history]\n",
    "        return (\n",
    "            f\"Memory Usage Summary:\\n\"\n",
    "            f\"  Min: {min(memory_values):.2f} GB\\n\"\n",
    "            f\"  Max: {max(memory_values):.2f} GB\\n\"\n",
    "            f\"  Avg: {sum(memory_values)/len(memory_values):.2f} GB\"\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"MemoryEfficientTrainer class defined!\")\n",
    "print(\"\\nFeatures:\")\n",
    "print(\"  - Gradient accumulation\")\n",
    "print(\"  - Mixed precision training (BF16)\")\n",
    "print(\"  - Memory monitoring\")\n",
    "print(\"  - Gradient norm tracking\")\n",
    "print(\"  - Tokens/second throughput\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with a simple model\n",
    "\n",
    "# Create a dummy model for demonstration\n",
    "class DummyLM(nn.Module):\n",
    "    def __init__(self, vocab_size=1000, hidden_size=256):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(hidden_size, 4, batch_first=True),\n",
    "            num_layers=2\n",
    "        )\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, input_ids, labels=None, **kwargs):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.transformer(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.functional.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                labels.view(-1)\n",
    "            )\n",
    "        \n",
    "        return type('Output', (), {'loss': loss, 'logits': logits})()\n",
    "\n",
    "# Setup\n",
    "model = DummyLM().cuda() if torch.cuda.is_available() else DummyLM()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "trainer = MemoryEfficientTrainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_grad_norm=1.0,\n",
    "    use_amp=torch.cuda.is_available(),\n",
    "    log_interval=2,\n",
    ")\n",
    "\n",
    "# Create dummy data\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "dummy_input_ids = torch.randint(0, 1000, (32, 64))\n",
    "dummy_labels = torch.randint(0, 1000, (32, 64))\n",
    "dummy_dataset = TensorDataset(dummy_input_ids, dummy_labels)\n",
    "\n",
    "class DictDataLoader:\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for input_ids, labels in self.loader:\n",
    "            yield {\"input_ids\": input_ids, \"labels\": labels}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "\n",
    "train_loader = DictDataLoader(dummy_dataset, batch_size=4)\n",
    "\n",
    "# Train for one epoch\n",
    "print(\"\\nStarting training...\\n\")\n",
    "avg_loss = trainer.train_epoch(train_loader, epoch=1)\n",
    "print(f\"\\nEpoch complete! Average loss: {avg_loss:.4f}\")\n",
    "print(f\"\\n{trainer.get_memory_summary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4 Solution: Evaluation Metrics Implementation\n",
    "\n",
    "**Task:** Implement perplexity calculation and generation quality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "class LLMEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive evaluator for language model fine-tuning.\n",
    "    \n",
    "    Includes:\n",
    "    - Perplexity calculation\n",
    "    - BLEU score (for translation/paraphrase)\n",
    "    - ROUGE scores (for summarization)\n",
    "    - Distinct-N (for diversity)\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_perplexity(\n",
    "        model: torch.nn.Module,\n",
    "        dataloader: torch.utils.data.DataLoader,\n",
    "        device: str = \"cuda\",\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Calculate perplexity on a dataset.\n",
    "        \n",
    "        Perplexity = exp(average cross-entropy loss)\n",
    "        Lower is better.\n",
    "        \n",
    "        Args:\n",
    "            model: The language model\n",
    "            dataloader: DataLoader with input_ids and labels\n",
    "            device: Device to use\n",
    "        \n",
    "        Returns:\n",
    "            Perplexity score\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                if isinstance(batch, dict):\n",
    "                    input_ids = batch[\"input_ids\"].to(device)\n",
    "                    labels = batch.get(\"labels\", input_ids).to(device)\n",
    "                else:\n",
    "                    input_ids = batch[0].to(device)\n",
    "                    labels = batch[1].to(device) if len(batch) > 1 else input_ids\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                # Count non-padding tokens\n",
    "                num_tokens = (labels != -100).sum().item()\n",
    "                total_loss += loss.item() * num_tokens\n",
    "                total_tokens += num_tokens\n",
    "        \n",
    "        avg_loss = total_loss / total_tokens\n",
    "        perplexity = math.exp(avg_loss)\n",
    "        \n",
    "        return perplexity\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_ngrams(tokens: List[str], n: int) -> List[Tuple[str, ...]]:\n",
    "        \"\"\"Extract n-grams from a token list.\"\"\"\n",
    "        return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_bleu(\n",
    "        references: List[str],\n",
    "        hypotheses: List[str],\n",
    "        max_n: int = 4,\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate BLEU scores.\n",
    "        \n",
    "        Args:\n",
    "            references: List of reference texts\n",
    "            hypotheses: List of generated texts\n",
    "            max_n: Maximum n-gram to consider\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with BLEU-1, BLEU-2, ..., BLEU-N scores\n",
    "        \"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        for n in range(1, max_n + 1):\n",
    "            total_matches = 0\n",
    "            total_count = 0\n",
    "            \n",
    "            for ref, hyp in zip(references, hypotheses):\n",
    "                ref_tokens = ref.lower().split()\n",
    "                hyp_tokens = hyp.lower().split()\n",
    "                \n",
    "                ref_ngrams = Counter(LLMEvaluator.get_ngrams(ref_tokens, n))\n",
    "                hyp_ngrams = Counter(LLMEvaluator.get_ngrams(hyp_tokens, n))\n",
    "                \n",
    "                # Clipped count\n",
    "                for ngram, count in hyp_ngrams.items():\n",
    "                    total_matches += min(count, ref_ngrams.get(ngram, 0))\n",
    "                \n",
    "                total_count += sum(hyp_ngrams.values())\n",
    "            \n",
    "            precision = total_matches / max(total_count, 1)\n",
    "            scores[f\"BLEU-{n}\"] = precision\n",
    "        \n",
    "        # Geometric mean for overall BLEU\n",
    "        if all(scores[f\"BLEU-{n}\"] > 0 for n in range(1, max_n + 1)):\n",
    "            log_sum = sum(math.log(scores[f\"BLEU-{n}\"]) for n in range(1, max_n + 1))\n",
    "            scores[\"BLEU\"] = math.exp(log_sum / max_n)\n",
    "        else:\n",
    "            scores[\"BLEU\"] = 0.0\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_rouge_l(\n",
    "        references: List[str],\n",
    "        hypotheses: List[str],\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate ROUGE-L (Longest Common Subsequence).\n",
    "        \n",
    "        Args:\n",
    "            references: List of reference texts\n",
    "            hypotheses: List of generated texts\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with precision, recall, and F1\n",
    "        \"\"\"\n",
    "        def lcs_length(x: List[str], y: List[str]) -> int:\n",
    "            \"\"\"Compute length of longest common subsequence.\"\"\"\n",
    "            m, n = len(x), len(y)\n",
    "            dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "            \n",
    "            for i in range(1, m + 1):\n",
    "                for j in range(1, n + 1):\n",
    "                    if x[i-1] == y[j-1]:\n",
    "                        dp[i][j] = dp[i-1][j-1] + 1\n",
    "                    else:\n",
    "                        dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "            \n",
    "            return dp[m][n]\n",
    "        \n",
    "        total_precision = 0.0\n",
    "        total_recall = 0.0\n",
    "        \n",
    "        for ref, hyp in zip(references, hypotheses):\n",
    "            ref_tokens = ref.lower().split()\n",
    "            hyp_tokens = hyp.lower().split()\n",
    "            \n",
    "            lcs_len = lcs_length(ref_tokens, hyp_tokens)\n",
    "            \n",
    "            precision = lcs_len / max(len(hyp_tokens), 1)\n",
    "            recall = lcs_len / max(len(ref_tokens), 1)\n",
    "            \n",
    "            total_precision += precision\n",
    "            total_recall += recall\n",
    "        \n",
    "        n = len(references)\n",
    "        avg_precision = total_precision / n\n",
    "        avg_recall = total_recall / n\n",
    "        \n",
    "        f1 = (2 * avg_precision * avg_recall / \n",
    "              max(avg_precision + avg_recall, 1e-8))\n",
    "        \n",
    "        return {\n",
    "            \"ROUGE-L-P\": avg_precision,\n",
    "            \"ROUGE-L-R\": avg_recall,\n",
    "            \"ROUGE-L-F1\": f1,\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_distinct_n(\n",
    "        texts: List[str],\n",
    "        max_n: int = 2,\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate Distinct-N scores (lexical diversity).\n",
    "        \n",
    "        Higher = more diverse generation.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of generated texts\n",
    "            max_n: Maximum n-gram size\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with Distinct-1, Distinct-2, etc.\n",
    "        \"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        for n in range(1, max_n + 1):\n",
    "            all_ngrams = []\n",
    "            \n",
    "            for text in texts:\n",
    "                tokens = text.lower().split()\n",
    "                ngrams = LLMEvaluator.get_ngrams(tokens, n)\n",
    "                all_ngrams.extend(ngrams)\n",
    "            \n",
    "            if all_ngrams:\n",
    "                unique_ngrams = len(set(all_ngrams))\n",
    "                total_ngrams = len(all_ngrams)\n",
    "                scores[f\"Distinct-{n}\"] = unique_ngrams / total_ngrams\n",
    "            else:\n",
    "                scores[f\"Distinct-{n}\"] = 0.0\n",
    "        \n",
    "        return scores\n",
    "\n",
    "\n",
    "print(\"LLMEvaluator class defined!\")\n",
    "print(\"\\nAvailable metrics:\")\n",
    "print(\"  - calculate_perplexity()\")\n",
    "print(\"  - calculate_bleu()\")\n",
    "print(\"  - calculate_rouge_l()\")\n",
    "print(\"  - calculate_distinct_n()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo the evaluator\n",
    "\n",
    "# Sample reference and generated texts\n",
    "references = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Machine learning is a subset of artificial intelligence\",\n",
    "    \"Python is a popular programming language for data science\",\n",
    "]\n",
    "\n",
    "# Good generations (similar to reference)\n",
    "good_hypotheses = [\n",
    "    \"The quick brown fox leaps over the lazy dog\",\n",
    "    \"Machine learning is part of artificial intelligence\",\n",
    "    \"Python is a widely used language for data science\",\n",
    "]\n",
    "\n",
    "# Bad generations (very different)\n",
    "bad_hypotheses = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"Cooking recipes are available online\",\n",
    "    \"The weather is nice today\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GOOD GENERATIONS:\")\n",
    "print(\"=\" * 60)\n",
    "bleu_good = LLMEvaluator.calculate_bleu(references, good_hypotheses)\n",
    "rouge_good = LLMEvaluator.calculate_rouge_l(references, good_hypotheses)\n",
    "distinct_good = LLMEvaluator.calculate_distinct_n(good_hypotheses)\n",
    "\n",
    "print(f\"BLEU scores: {bleu_good}\")\n",
    "print(f\"ROUGE-L: {rouge_good}\")\n",
    "print(f\"Distinct-N: {distinct_good}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BAD GENERATIONS:\")\n",
    "print(\"=\" * 60)\n",
    "bleu_bad = LLMEvaluator.calculate_bleu(references, bad_hypotheses)\n",
    "rouge_bad = LLMEvaluator.calculate_rouge_l(references, bad_hypotheses)\n",
    "distinct_bad = LLMEvaluator.calculate_distinct_n(bad_hypotheses)\n",
    "\n",
    "print(f\"BLEU scores: {bleu_bad}\")\n",
    "print(f\"ROUGE-L: {rouge_bad}\")\n",
    "print(f\"Distinct-N: {distinct_bad}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Good vs Bad BLEU-2: {bleu_good['BLEU-2']:.3f} vs {bleu_bad['BLEU-2']:.3f}\")\n",
    "print(f\"Good vs Bad ROUGE-L-F1: {rouge_good['ROUGE-L-F1']:.3f} vs {rouge_bad['ROUGE-L-F1']:.3f}\")\n",
    "print(\"\\nHigher BLEU/ROUGE = closer to reference (better for many tasks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "These solutions demonstrate:\n",
    "\n",
    "1. **Custom LoRA Configuration**: How to tune LoRA parameters for specific tasks (code generation)\n",
    "\n",
    "2. **Hyperparameter Search**: Systematic exploration of learning rates and batch sizes with visualization\n",
    "\n",
    "3. **Custom Training Loop**: Memory-efficient training with gradient accumulation and monitoring\n",
    "\n",
    "4. **Evaluation Metrics**: Perplexity, BLEU, ROUGE-L, and Distinct-N for comprehensive model evaluation\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **LoRA rank 8-32** works well for most tasks; higher for complex tasks like code\n",
    "- **Learning rate 2e-5** is a good starting point for LoRA fine-tuning\n",
    "- **Gradient accumulation** allows large effective batch sizes with limited memory\n",
    "- **Multiple metrics** give a fuller picture than any single metric"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Cleanup\nimport gc\ngc.collect()\n\ntry:\n    import torch\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        print(\"GPU cache cleared\")\nexcept ImportError:\n    pass\n\nprint(\"Cleanup complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}