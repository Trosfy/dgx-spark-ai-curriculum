{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 3.1.3 Solutions: 70B Model QLoRA Fine-Tuning (DGX Spark Showcase)\n\n**Module:** 3.1 - Large Language Model Fine-Tuning  \n**Difficulty:** ⭐⭐⭐⭐☆ (Advanced)  \n**Exercises:** 3 (Memory Estimation, Memory Profiler, Config Optimizer)\n\nThis notebook contains solutions for exercises in the 70B QLoRA fine-tuning notebook.\n\n**Note:** This is the DGX Spark showcase task - fine-tuning a 70B model on a single desktop!\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1 Solution: Memory Estimation Calculator\n",
    "\n",
    "**Task:** Create a function that estimates memory requirements for different model sizes and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Optional\n",
    "import math\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Model architecture configuration.\"\"\"\n",
    "    name: str\n",
    "    params_billions: float\n",
    "    hidden_size: int\n",
    "    num_layers: int\n",
    "    num_attention_heads: int\n",
    "    intermediate_size: int\n",
    "    vocab_size: int = 128256  # Llama 3 default\n",
    "\n",
    "\n",
    "# Common model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    \"llama-3.1-8b\": ModelConfig(\n",
    "        name=\"Llama 3.1 8B\",\n",
    "        params_billions=8.0,\n",
    "        hidden_size=4096,\n",
    "        num_layers=32,\n",
    "        num_attention_heads=32,\n",
    "        intermediate_size=14336,\n",
    "    ),\n",
    "    \"llama-3.1-70b\": ModelConfig(\n",
    "        name=\"Llama 3.1 70B\",\n",
    "        params_billions=70.0,\n",
    "        hidden_size=8192,\n",
    "        num_layers=80,\n",
    "        num_attention_heads=64,\n",
    "        intermediate_size=28672,\n",
    "    ),\n",
    "    \"llama-3.1-405b\": ModelConfig(\n",
    "        name=\"Llama 3.1 405B\",\n",
    "        params_billions=405.0,\n",
    "        hidden_size=16384,\n",
    "        num_layers=126,\n",
    "        num_attention_heads=128,\n",
    "        intermediate_size=53248,\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "def estimate_memory_requirements(\n",
    "    model_config: ModelConfig,\n",
    "    batch_size: int = 1,\n",
    "    sequence_length: int = 2048,\n",
    "    lora_rank: int = 16,\n",
    "    num_lora_modules: int = 4,  # e.g., q, k, v, o projections\n",
    "    quantization_bits: int = 4,  # 4 for QLoRA, 16 for full precision\n",
    "    gradient_checkpointing: bool = True,\n",
    "    optimizer_states_per_param: int = 2,  # AdamW: 2 (momentum + variance)\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Estimate memory requirements for model training/inference.\n",
    "    \n",
    "    Args:\n",
    "        model_config: Model configuration\n",
    "        batch_size: Training batch size\n",
    "        sequence_length: Maximum sequence length\n",
    "        lora_rank: LoRA rank (r)\n",
    "        num_lora_modules: Number of modules with LoRA adapters per layer\n",
    "        quantization_bits: Bits for base model weights (4 for QLoRA)\n",
    "        gradient_checkpointing: Whether gradient checkpointing is enabled\n",
    "        optimizer_states_per_param: Optimizer state multiplier\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with memory estimates in GB\n",
    "    \"\"\"\n",
    "    params = model_config.params_billions * 1e9\n",
    "    \n",
    "    # 1. Base model weights\n",
    "    bytes_per_param = quantization_bits / 8\n",
    "    model_memory_gb = (params * bytes_per_param) / 1e9\n",
    "    \n",
    "    # 2. LoRA parameters (always in full precision)\n",
    "    lora_params_per_layer = (\n",
    "        num_lora_modules * 2 *  # A and B matrices\n",
    "        lora_rank * model_config.hidden_size\n",
    "    )\n",
    "    total_lora_params = lora_params_per_layer * model_config.num_layers\n",
    "    lora_memory_gb = (total_lora_params * 4) / 1e9  # FP32\n",
    "    \n",
    "    # 3. Optimizer states (only for LoRA params)\n",
    "    optimizer_memory_gb = (\n",
    "        total_lora_params * optimizer_states_per_param * 4\n",
    "    ) / 1e9  # FP32\n",
    "    \n",
    "    # 4. Gradients (only for LoRA params)\n",
    "    gradient_memory_gb = (total_lora_params * 4) / 1e9  # FP32\n",
    "    \n",
    "    # 5. Activations (this is approximate)\n",
    "    # Per layer: attention + MLP activations\n",
    "    # With gradient checkpointing, we only store sqrt(L) checkpoints\n",
    "    bytes_per_activation = 2  # BF16\n",
    "    \n",
    "    # Attention activations: QKV, attention weights, attention output\n",
    "    attention_activations = (\n",
    "        3 * batch_size * sequence_length * model_config.hidden_size +  # Q, K, V\n",
    "        batch_size * model_config.num_attention_heads * sequence_length ** 2  # Attention weights\n",
    "    )\n",
    "    \n",
    "    # MLP activations\n",
    "    mlp_activations = (\n",
    "        batch_size * sequence_length * model_config.intermediate_size * 2\n",
    "    )\n",
    "    \n",
    "    activations_per_layer = (attention_activations + mlp_activations) * bytes_per_activation\n",
    "    \n",
    "    if gradient_checkpointing:\n",
    "        # Only store sqrt(L) checkpoints\n",
    "        num_stored_layers = int(math.sqrt(model_config.num_layers))\n",
    "        activation_memory_gb = (activations_per_layer * num_stored_layers) / 1e9\n",
    "    else:\n",
    "        activation_memory_gb = (activations_per_layer * model_config.num_layers) / 1e9\n",
    "    \n",
    "    # 6. KV Cache (for inference/generation)\n",
    "    kv_cache_per_layer = (\n",
    "        2 * batch_size * sequence_length * model_config.hidden_size * 2  # K and V, BF16\n",
    "    )\n",
    "    kv_cache_memory_gb = (kv_cache_per_layer * model_config.num_layers) / 1e9\n",
    "    \n",
    "    # 7. Buffer and overhead (typically 10-20%)\n",
    "    overhead_factor = 0.15\n",
    "    \n",
    "    # Total for training\n",
    "    training_total = (\n",
    "        model_memory_gb +\n",
    "        lora_memory_gb +\n",
    "        optimizer_memory_gb +\n",
    "        gradient_memory_gb +\n",
    "        activation_memory_gb\n",
    "    )\n",
    "    training_with_overhead = training_total * (1 + overhead_factor)\n",
    "    \n",
    "    # Total for inference\n",
    "    inference_total = model_memory_gb + lora_memory_gb + kv_cache_memory_gb\n",
    "    inference_with_overhead = inference_total * (1 + overhead_factor)\n",
    "    \n",
    "    return {\n",
    "        \"model_name\": model_config.name,\n",
    "        \"base_model_weights_gb\": model_memory_gb,\n",
    "        \"lora_params\": total_lora_params,\n",
    "        \"lora_params_gb\": lora_memory_gb,\n",
    "        \"optimizer_states_gb\": optimizer_memory_gb,\n",
    "        \"gradients_gb\": gradient_memory_gb,\n",
    "        \"activations_gb\": activation_memory_gb,\n",
    "        \"kv_cache_gb\": kv_cache_memory_gb,\n",
    "        \"training_total_gb\": training_with_overhead,\n",
    "        \"inference_total_gb\": inference_with_overhead,\n",
    "        \"fits_dgx_spark_128gb\": training_with_overhead < 128,\n",
    "        \"quantization\": f\"{quantization_bits}-bit\",\n",
    "        \"gradient_checkpointing\": gradient_checkpointing,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example: Estimate for 70B model\n",
    "estimates_70b = estimate_memory_requirements(\n",
    "    MODEL_CONFIGS[\"llama-3.1-70b\"],\n",
    "    batch_size=1,\n",
    "    sequence_length=2048,\n",
    "    lora_rank=16,\n",
    "    quantization_bits=4,\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "print(\"Memory Estimation: Llama 3.1 70B with QLoRA\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Base model weights: {estimates_70b['base_model_weights_gb']:.1f} GB\")\n",
    "print(f\"LoRA parameters: {estimates_70b['lora_params']:,} ({estimates_70b['lora_params_gb']:.3f} GB)\")\n",
    "print(f\"Optimizer states: {estimates_70b['optimizer_states_gb']:.3f} GB\")\n",
    "print(f\"Gradients: {estimates_70b['gradients_gb']:.3f} GB\")\n",
    "print(f\"Activations: {estimates_70b['activations_gb']:.1f} GB\")\n",
    "print(f\"\\nTotal Training Memory: {estimates_70b['training_total_gb']:.1f} GB\")\n",
    "print(f\"Total Inference Memory: {estimates_70b['inference_total_gb']:.1f} GB\")\n",
    "print(f\"\\nFits on DGX Spark (128GB): {estimates_70b['fits_dgx_spark_128gb']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different configurations\n",
    "import pandas as pd\n",
    "\n",
    "configurations = [\n",
    "    # (model_key, batch_size, seq_len, lora_rank, quant_bits, grad_ckpt)\n",
    "    (\"llama-3.1-8b\", 4, 2048, 16, 16, True),   # Full precision 8B\n",
    "    (\"llama-3.1-8b\", 4, 2048, 16, 4, True),    # QLoRA 8B\n",
    "    (\"llama-3.1-70b\", 1, 2048, 16, 16, True),  # Full precision 70B (won't fit!)\n",
    "    (\"llama-3.1-70b\", 1, 2048, 16, 4, True),   # QLoRA 70B\n",
    "    (\"llama-3.1-70b\", 2, 4096, 32, 4, True),   # QLoRA 70B larger batch\n",
    "    (\"llama-3.1-405b\", 1, 2048, 8, 4, True),   # QLoRA 405B\n",
    "]\n",
    "\n",
    "results = []\n",
    "for model_key, bs, seq_len, rank, bits, grad_ckpt in configurations:\n",
    "    est = estimate_memory_requirements(\n",
    "        MODEL_CONFIGS[model_key],\n",
    "        batch_size=bs,\n",
    "        sequence_length=seq_len,\n",
    "        lora_rank=rank,\n",
    "        quantization_bits=bits,\n",
    "        gradient_checkpointing=grad_ckpt,\n",
    "    )\n",
    "    results.append({\n",
    "        \"Model\": est[\"model_name\"],\n",
    "        \"Batch\": bs,\n",
    "        \"SeqLen\": seq_len,\n",
    "        \"Rank\": rank,\n",
    "        \"Quant\": est[\"quantization\"],\n",
    "        \"Training GB\": f\"{est['training_total_gb']:.1f}\",\n",
    "        \"Fits 128GB\": \"Yes\" if est['fits_dgx_spark_128gb'] else \"No\",\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\nMemory Comparison Across Configurations:\")\n",
    "print(df.to_string(index=False))\n",
    "print(\"\\nKey Insight: QLoRA makes 70B training possible on DGX Spark!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2 Solution: Memory Profiler for Training\n",
    "\n",
    "**Task:** Create a memory profiler that tracks GPU memory during training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from typing import List, Dict, Callable, Any\n",
    "from dataclasses import dataclass, field\n",
    "from contextlib import contextmanager\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@dataclass\n",
    "class MemorySnapshot:\n",
    "    \"\"\"A single memory measurement.\"\"\"\n",
    "    timestamp: float\n",
    "    phase: str\n",
    "    allocated_gb: float\n",
    "    reserved_gb: float\n",
    "    cached_gb: float\n",
    "\n",
    "\n",
    "class GPUMemoryProfiler:\n",
    "    \"\"\"\n",
    "    Profile GPU memory usage during model training.\n",
    "    \n",
    "    Works with DGX Spark's unified memory architecture.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, device: int = 0):\n",
    "        self.device = device\n",
    "        self.snapshots: List[MemorySnapshot] = []\n",
    "        self.start_time = None\n",
    "        self.phase_starts: Dict[str, float] = {}\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Clear all snapshots and reset.\"\"\"\n",
    "        self.snapshots = []\n",
    "        self.start_time = None\n",
    "        self.phase_starts = {}\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats(self.device)\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    def _get_memory_gb(self) -> Dict[str, float]:\n",
    "        \"\"\"Get current memory statistics in GB.\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return {\"allocated\": 0, \"reserved\": 0, \"cached\": 0}\n",
    "        \n",
    "        return {\n",
    "            \"allocated\": torch.cuda.memory_allocated(self.device) / 1e9,\n",
    "            \"reserved\": torch.cuda.memory_reserved(self.device) / 1e9,\n",
    "            \"cached\": torch.cuda.memory_reserved(self.device) / 1e9,  # Same as reserved in PyTorch\n",
    "        }\n",
    "    \n",
    "    def snapshot(self, phase: str = \"unknown\"):\n",
    "        \"\"\"Take a memory snapshot.\"\"\"\n",
    "        if self.start_time is None:\n",
    "            self.start_time = time.time()\n",
    "        \n",
    "        mem = self._get_memory_gb()\n",
    "        self.snapshots.append(MemorySnapshot(\n",
    "            timestamp=time.time() - self.start_time,\n",
    "            phase=phase,\n",
    "            allocated_gb=mem[\"allocated\"],\n",
    "            reserved_gb=mem[\"reserved\"],\n",
    "            cached_gb=mem[\"cached\"],\n",
    "        ))\n",
    "    \n",
    "    @contextmanager\n",
    "    def track_phase(self, phase: str):\n",
    "        \"\"\"\n",
    "        Context manager to track a training phase.\n",
    "        \n",
    "        Usage:\n",
    "            with profiler.track_phase(\"forward\"):\n",
    "                outputs = model(inputs)\n",
    "        \"\"\"\n",
    "        self.snapshot(f\"{phase}_start\")\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            self.snapshot(f\"{phase}_end\")\n",
    "    \n",
    "    def get_peak_memory(self) -> float:\n",
    "        \"\"\"Get peak allocated memory in GB.\"\"\"\n",
    "        if not self.snapshots:\n",
    "            return 0.0\n",
    "        return max(s.allocated_gb for s in self.snapshots)\n",
    "    \n",
    "    def get_phase_memory_delta(self, phase: str) -> float:\n",
    "        \"\"\"Get memory increase during a phase.\"\"\"\n",
    "        start_snap = None\n",
    "        end_snap = None\n",
    "        \n",
    "        for s in self.snapshots:\n",
    "            if s.phase == f\"{phase}_start\":\n",
    "                start_snap = s\n",
    "            elif s.phase == f\"{phase}_end\":\n",
    "                end_snap = s\n",
    "        \n",
    "        if start_snap and end_snap:\n",
    "            return end_snap.allocated_gb - start_snap.allocated_gb\n",
    "        return 0.0\n",
    "    \n",
    "    def summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get a summary of memory usage.\"\"\"\n",
    "        if not self.snapshots:\n",
    "            return {\"error\": \"No snapshots recorded\"}\n",
    "        \n",
    "        allocated_values = [s.allocated_gb for s in self.snapshots]\n",
    "        \n",
    "        # Extract unique phases\n",
    "        phases = set()\n",
    "        for s in self.snapshots:\n",
    "            if s.phase.endswith(\"_start\"):\n",
    "                phases.add(s.phase.replace(\"_start\", \"\"))\n",
    "        \n",
    "        phase_deltas = {p: self.get_phase_memory_delta(p) for p in phases}\n",
    "        \n",
    "        return {\n",
    "            \"num_snapshots\": len(self.snapshots),\n",
    "            \"peak_allocated_gb\": max(allocated_values),\n",
    "            \"min_allocated_gb\": min(allocated_values),\n",
    "            \"final_allocated_gb\": allocated_values[-1],\n",
    "            \"phase_deltas_gb\": phase_deltas,\n",
    "            \"total_duration_s\": self.snapshots[-1].timestamp,\n",
    "        }\n",
    "    \n",
    "    def plot_memory_timeline(self, save_path: str = None):\n",
    "        \"\"\"Plot memory usage over time.\"\"\"\n",
    "        if not self.snapshots:\n",
    "            print(\"No snapshots to plot\")\n",
    "            return\n",
    "        \n",
    "        timestamps = [s.timestamp for s in self.snapshots]\n",
    "        allocated = [s.allocated_gb for s in self.snapshots]\n",
    "        reserved = [s.reserved_gb for s in self.snapshots]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        ax.plot(timestamps, allocated, 'b-', label='Allocated', linewidth=2)\n",
    "        ax.plot(timestamps, reserved, 'r--', label='Reserved', linewidth=2, alpha=0.7)\n",
    "        \n",
    "        # Mark phase boundaries\n",
    "        colors = plt.cm.Set3(range(10))\n",
    "        phase_idx = 0\n",
    "        for i, s in enumerate(self.snapshots):\n",
    "            if s.phase.endswith(\"_start\"):\n",
    "                phase_name = s.phase.replace(\"_start\", \"\")\n",
    "                ax.axvline(s.timestamp, color=colors[phase_idx % 10], \n",
    "                          linestyle=':', alpha=0.7)\n",
    "                ax.annotate(phase_name, (s.timestamp, ax.get_ylim()[1] * 0.95),\n",
    "                           rotation=90, fontsize=8)\n",
    "                phase_idx += 1\n",
    "        \n",
    "        ax.set_xlabel('Time (seconds)')\n",
    "        ax.set_ylabel('Memory (GB)')\n",
    "        ax.set_title('GPU Memory Usage During Training Step')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add DGX Spark reference line\n",
    "        ax.axhline(y=128, color='green', linestyle='--', alpha=0.5,\n",
    "                  label='DGX Spark 128GB')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "\n",
    "print(\"GPUMemoryProfiler class defined!\")\n",
    "print(\"\\nUsage example:\")\n",
    "print(\"  profiler = GPUMemoryProfiler()\")\n",
    "print(\"  with profiler.track_phase('forward'):\")\n",
    "print(\"      outputs = model(inputs)\")\n",
    "print(\"  print(profiler.summary())\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo the profiler with a simulated training step\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a simple model\n",
    "class DemoModel(nn.Module):\n",
    "    def __init__(self, hidden_size=1024, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.activation = nn.GELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = self.activation(layer(x))\n",
    "        return x\n",
    "\n",
    "# Initialize\n",
    "if torch.cuda.is_available():\n",
    "    profiler = GPUMemoryProfiler()\n",
    "    model = DemoModel(hidden_size=2048, num_layers=8).cuda()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    profiler.reset()\n",
    "    \n",
    "    # Profile a training step\n",
    "    batch = torch.randn(32, 512, 2048).cuda()\n",
    "    target = torch.randn(32, 512, 2048).cuda()\n",
    "    \n",
    "    with profiler.track_phase(\"forward\"):\n",
    "        output = model(batch)\n",
    "    \n",
    "    with profiler.track_phase(\"loss\"):\n",
    "        loss = nn.functional.mse_loss(output, target)\n",
    "    \n",
    "    with profiler.track_phase(\"backward\"):\n",
    "        loss.backward()\n",
    "    \n",
    "    with profiler.track_phase(\"optimizer\"):\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # Print summary\n",
    "    summary = profiler.summary()\n",
    "    print(\"Memory Profiling Summary:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Peak memory: {summary['peak_allocated_gb']:.2f} GB\")\n",
    "    print(f\"Final memory: {summary['final_allocated_gb']:.2f} GB\")\n",
    "    print(f\"Duration: {summary['total_duration_s']:.3f} s\")\n",
    "    print(\"\\nPhase memory deltas:\")\n",
    "    for phase, delta in summary['phase_deltas_gb'].items():\n",
    "        print(f\"  {phase}: {delta:+.3f} GB\")\n",
    "    \n",
    "    # Plot\n",
    "    profiler.plot_memory_timeline()\n",
    "else:\n",
    "    print(\"CUDA not available - skipping GPU profiling demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3 Solution: QLoRA Configuration Optimizer\n",
    "\n",
    "**Task:** Create a function that suggests optimal QLoRA configurations based on available memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class QLoRAConfig:\n",
    "    \"\"\"QLoRA training configuration.\"\"\"\n",
    "    batch_size: int\n",
    "    gradient_accumulation: int\n",
    "    sequence_length: int\n",
    "    lora_rank: int\n",
    "    lora_alpha: int\n",
    "    target_modules: List[str]\n",
    "    estimated_memory_gb: float\n",
    "    effective_batch_size: int\n",
    "    \n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"QLoRA Config:\\n\"\n",
    "            f\"  Batch size: {self.batch_size} × {self.gradient_accumulation} = \"\n",
    "            f\"{self.effective_batch_size}\\n\"\n",
    "            f\"  Sequence length: {self.sequence_length}\\n\"\n",
    "            f\"  LoRA rank: {self.lora_rank}\\n\"\n",
    "            f\"  LoRA alpha: {self.lora_alpha}\\n\"\n",
    "            f\"  Target modules: {self.target_modules}\\n\"\n",
    "            f\"  Estimated memory: {self.estimated_memory_gb:.1f} GB\"\n",
    "        )\n",
    "\n",
    "\n",
    "def optimize_qlora_config(\n",
    "    model_params_billions: float,\n",
    "    available_memory_gb: float = 128.0,  # DGX Spark default\n",
    "    target_batch_size: int = 16,\n",
    "    max_sequence_length: int = 4096,\n",
    "    safety_margin: float = 0.85,  # Use 85% of available memory\n",
    ") -> QLoRAConfig:\n",
    "    \"\"\"\n",
    "    Optimize QLoRA configuration for given memory constraints.\n",
    "    \n",
    "    Args:\n",
    "        model_params_billions: Model size in billions of parameters\n",
    "        available_memory_gb: Available GPU memory in GB\n",
    "        target_batch_size: Desired effective batch size\n",
    "        max_sequence_length: Maximum sequence length to consider\n",
    "        safety_margin: Fraction of memory to actually use\n",
    "    \n",
    "    Returns:\n",
    "        Optimized QLoRAConfig\n",
    "    \"\"\"\n",
    "    usable_memory = available_memory_gb * safety_margin\n",
    "    \n",
    "    # Base model memory (4-bit quantized)\n",
    "    base_model_memory = model_params_billions * 0.5  # ~0.5 GB per billion params in 4-bit\n",
    "    \n",
    "    # Memory available for training overhead\n",
    "    training_budget = usable_memory - base_model_memory\n",
    "    \n",
    "    if training_budget <= 0:\n",
    "        raise ValueError(f\"Model ({model_params_billions}B) too large for {available_memory_gb}GB\")\n",
    "    \n",
    "    # Try different configurations\n",
    "    best_config = None\n",
    "    best_score = -1\n",
    "    \n",
    "    # Configuration options to try\n",
    "    rank_options = [8, 16, 32, 64]\n",
    "    seq_len_options = [512, 1024, 2048, 4096]\n",
    "    batch_options = [1, 2, 4, 8]\n",
    "    grad_accum_options = [1, 2, 4, 8, 16, 32]\n",
    "    \n",
    "    # Target module configurations\n",
    "    module_configs = [\n",
    "        [\"q_proj\", \"v_proj\"],  # Minimal\n",
    "        [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Attention only\n",
    "        [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],  # All\n",
    "    ]\n",
    "    \n",
    "    for rank in rank_options:\n",
    "        for seq_len in seq_len_options:\n",
    "            if seq_len > max_sequence_length:\n",
    "                continue\n",
    "            \n",
    "            for batch_size in batch_options:\n",
    "                for grad_accum in grad_accum_options:\n",
    "                    effective_batch = batch_size * grad_accum\n",
    "                    \n",
    "                    for modules in module_configs:\n",
    "                        # Estimate memory for this config\n",
    "                        num_modules = len(modules)\n",
    "                        \n",
    "                        # LoRA params (rough estimate)\n",
    "                        hidden_size = int(model_params_billions * 500)  # Rough approximation\n",
    "                        hidden_size = min(hidden_size, 8192)  # Cap at 70B hidden size\n",
    "                        lora_memory = (rank * hidden_size * num_modules * 80 * 4) / 1e9  # ~80 layers\n",
    "                        \n",
    "                        # Activation memory (rough estimate)\n",
    "                        activation_memory = (\n",
    "                            batch_size * seq_len * hidden_size * 4 * 0.001  # Simplified\n",
    "                        )\n",
    "                        \n",
    "                        # Optimizer states\n",
    "                        optimizer_memory = lora_memory * 2  # AdamW\n",
    "                        \n",
    "                        total_training_memory = (\n",
    "                            lora_memory + activation_memory + optimizer_memory\n",
    "                        )\n",
    "                        \n",
    "                        total_memory = base_model_memory + total_training_memory\n",
    "                        \n",
    "                        # Check if it fits\n",
    "                        if total_memory > usable_memory:\n",
    "                            continue\n",
    "                        \n",
    "                        # Score this configuration\n",
    "                        # Prioritize: effective batch size, sequence length, rank\n",
    "                        score = (\n",
    "                            min(effective_batch, target_batch_size) / target_batch_size * 40 +\n",
    "                            seq_len / max_sequence_length * 30 +\n",
    "                            rank / 64 * 20 +\n",
    "                            num_modules / 7 * 10\n",
    "                        )\n",
    "                        \n",
    "                        if score > best_score:\n",
    "                            best_score = score\n",
    "                            best_config = QLoRAConfig(\n",
    "                                batch_size=batch_size,\n",
    "                                gradient_accumulation=grad_accum,\n",
    "                                sequence_length=seq_len,\n",
    "                                lora_rank=rank,\n",
    "                                lora_alpha=rank * 2,  # Common practice\n",
    "                                target_modules=modules,\n",
    "                                estimated_memory_gb=total_memory,\n",
    "                                effective_batch_size=effective_batch,\n",
    "                            )\n",
    "    \n",
    "    if best_config is None:\n",
    "        raise ValueError(\"Could not find valid configuration\")\n",
    "    \n",
    "    return best_config\n",
    "\n",
    "\n",
    "# Demo: Optimize for different model sizes on DGX Spark\n",
    "print(\"QLoRA Configuration Optimization for DGX Spark (128GB)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_size in [8, 70, 100]:\n",
    "    try:\n",
    "        config = optimize_qlora_config(\n",
    "            model_params_billions=model_size,\n",
    "            available_memory_gb=128.0,\n",
    "            target_batch_size=16,\n",
    "        )\n",
    "        print(f\"\\n{model_size}B Model:\")\n",
    "        print(config)\n",
    "    except ValueError as e:\n",
    "        print(f\"\\n{model_size}B Model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "These solutions demonstrate:\n",
    "\n",
    "1. **Memory Estimation**: Calculate expected memory usage before training\n",
    "\n",
    "2. **Memory Profiling**: Track actual memory usage during training\n",
    "\n",
    "3. **Configuration Optimization**: Automatically find the best QLoRA config for your hardware\n",
    "\n",
    "### Key Takeaways for DGX Spark\n",
    "\n",
    "- **128GB unified memory** allows 70B models with QLoRA\n",
    "- **4-bit quantization** reduces base model from 140GB to ~35GB\n",
    "- **Gradient checkpointing** is essential for large models\n",
    "- **Batch size 1-2** with gradient accumulation is typical for 70B\n",
    "- Always leave **15-20% memory headroom** for safety"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Cleanup\nimport gc\ngc.collect()\n\ntry:\n    import torch\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        print(\"GPU cache cleared\")\nexcept ImportError:\n    pass\n\nprint(\"Cleanup complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}