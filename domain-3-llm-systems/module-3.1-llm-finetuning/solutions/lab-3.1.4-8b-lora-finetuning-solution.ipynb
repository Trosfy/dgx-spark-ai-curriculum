{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1.4: 8B Model LoRA Fine-Tuning - Solutions\n",
    "\n",
    "Complete solutions for the 8B fine-tuning exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Complete Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "def create_complete_training_pipeline(\n",
    "    model_id: str = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    dataset_name: str = \"tatsu-lab/alpaca\",\n",
    "    output_dir: str = \"./lora-finetuned\",\n",
    "    use_dora: bool = True,\n",
    "    neftune_alpha: float = 5.0,\n",
    "    lora_r: int = 16,\n",
    "    lora_alpha: int = 32,\n",
    "    epochs: int = 1,\n",
    "    max_samples: int = 1000\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete production-ready fine-tuning pipeline.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"COMPLETE FINE-TUNING PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Quantization config for memory efficiency\n",
    "    print(\"\\n1. Setting up quantization...\")\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    \n",
    "    # 2. Load model\n",
    "    print(\"2. Loading model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"flash_attention_2\" if torch.cuda.is_available() else \"eager\"\n",
    "    )\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    # 3. LoRA configuration with DoRA\n",
    "    print(\"3. Configuring LoRA/DoRA...\")\n",
    "    lora_config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        use_dora=use_dora  # Enable DoRA for better performance\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # 4. Load and format dataset\n",
    "    print(\"4. Loading dataset...\")\n",
    "    dataset = load_dataset(dataset_name, split=f\"train[:{max_samples}]\")\n",
    "    \n",
    "    def format_instruction(example):\n",
    "        if example.get(\"input\", \"\").strip():\n",
    "            text = f\"\"\"### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Input:\n",
    "{example['input']}\n",
    "\n",
    "### Response:\n",
    "{example['output']}\"\"\"\n",
    "        else:\n",
    "            text = f\"\"\"### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Response:\n",
    "{example['output']}\"\"\"\n",
    "        return {\"text\": text}\n",
    "    \n",
    "    dataset = dataset.map(format_instruction)\n",
    "    \n",
    "    # 5. Training configuration with NEFTune\n",
    "    print(\"5. Setting up training...\")\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.1,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        bf16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        max_seq_length=512,\n",
    "        \n",
    "        # NEFTune for better generalization\n",
    "        neftune_noise_alpha=neftune_alpha,\n",
    "        \n",
    "        # Disable unnecessary features\n",
    "        report_to=\"none\",\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "    \n",
    "    # 6. Create trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        dataset_text_field=\"text\",\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Configuration Summary:\")\n",
    "    print(f\"  Model: {model_id}\")\n",
    "    print(f\"  LoRA rank: {lora_r}\")\n",
    "    print(f\"  DoRA enabled: {use_dora}\")\n",
    "    print(f\"  NEFTune alpha: {neftune_alpha}\")\n",
    "    print(f\"  Training samples: {len(dataset)}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return trainer, model, tokenizer\n",
    "\n",
    "# Create pipeline\n",
    "# trainer, model, tokenizer = create_complete_training_pipeline()\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Memory Optimization Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_memory_usage():\n",
    "    \"\"\"\n",
    "    Analyze memory usage for different configurations.\n",
    "    \"\"\"\n",
    "    import gc\n",
    "    \n",
    "    configs = [\n",
    "        {\"name\": \"FP16 Full\", \"bits\": 16, \"lora\": False, \"grad_ckpt\": False},\n",
    "        {\"name\": \"FP16 + LoRA\", \"bits\": 16, \"lora\": True, \"grad_ckpt\": False},\n",
    "        {\"name\": \"4-bit QLoRA\", \"bits\": 4, \"lora\": True, \"grad_ckpt\": False},\n",
    "        {\"name\": \"4-bit QLoRA + GC\", \"bits\": 4, \"lora\": True, \"grad_ckpt\": True},\n",
    "    ]\n",
    "    \n",
    "    # Theoretical memory for 8B model\n",
    "    params_b = 8  # 8 billion params\n",
    "    \n",
    "    results = []\n",
    "    for config in configs:\n",
    "        # Base model memory\n",
    "        bytes_per_param = config[\"bits\"] / 8\n",
    "        model_mem = params_b * bytes_per_param\n",
    "        \n",
    "        # Optimizer states (only for trainable params)\n",
    "        if config[\"lora\"]:\n",
    "            trainable_ratio = 0.01  # ~1% trainable\n",
    "        else:\n",
    "            trainable_ratio = 1.0\n",
    "        \n",
    "        # AdamW: 2 states per param (m and v)\n",
    "        optimizer_mem = params_b * trainable_ratio * 4 * 2  # FP32 states\n",
    "        \n",
    "        # Gradients\n",
    "        gradient_mem = params_b * trainable_ratio * 4  # FP32\n",
    "        \n",
    "        # Activations (rough estimate)\n",
    "        batch_size = 4\n",
    "        seq_len = 512\n",
    "        hidden = 4096\n",
    "        layers = 32\n",
    "        \n",
    "        if config[\"grad_ckpt\"]:\n",
    "            activation_mem = batch_size * seq_len * hidden * 2 / 1e9  # Only keep essential\n",
    "        else:\n",
    "            activation_mem = batch_size * seq_len * hidden * layers * 2 / 1e9\n",
    "        \n",
    "        total = model_mem + optimizer_mem + gradient_mem + activation_mem\n",
    "        \n",
    "        results.append({\n",
    "            \"config\": config[\"name\"],\n",
    "            \"model_gb\": model_mem,\n",
    "            \"optimizer_gb\": optimizer_mem,\n",
    "            \"gradient_gb\": gradient_mem,\n",
    "            \"activation_gb\": activation_mem,\n",
    "            \"total_gb\": total\n",
    "        })\n",
    "    \n",
    "    # Display\n",
    "    print(\"Memory Analysis for 8B Model:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Config':<20} {'Model':>10} {'Optimizer':>10} {'Gradients':>10} {'Activations':>12} {'Total':>10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for r in results:\n",
    "        print(f\"{r['config']:<20} {r['model_gb']:>9.1f}G {r['optimizer_gb']:>9.1f}G {r['gradient_gb']:>9.1f}G {r['activation_gb']:>11.1f}G {r['total_gb']:>9.1f}G\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Key Insight: 4-bit QLoRA + Gradient Checkpointing reduces memory by ~10x!\")\n",
    "    return results\n",
    "\n",
    "memory_results = analyze_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def hyperparameter_search_space():\n",
    "    \"\"\"\n",
    "    Define hyperparameter search space for LoRA fine-tuning.\n",
    "    \"\"\"\n",
    "    search_space = {\n",
    "        # LoRA parameters\n",
    "        \"lora_r\": [8, 16, 32, 64],\n",
    "        \"lora_alpha\": [16, 32, 64],\n",
    "        \"lora_dropout\": [0.0, 0.05, 0.1],\n",
    "        \n",
    "        # Training parameters\n",
    "        \"learning_rate\": [1e-4, 2e-4, 5e-4],\n",
    "        \"batch_size\": [4, 8],\n",
    "        \"warmup_ratio\": [0.03, 0.1],\n",
    "        \n",
    "        # Regularization\n",
    "        \"neftune_alpha\": [0, 5, 10],\n",
    "        \"use_dora\": [False, True],\n",
    "    }\n",
    "    \n",
    "    # Recommended configurations based on research\n",
    "    recommended_configs = [\n",
    "        {\n",
    "            \"name\": \"Baseline LoRA\",\n",
    "            \"lora_r\": 16, \"lora_alpha\": 32, \"lora_dropout\": 0.05,\n",
    "            \"learning_rate\": 2e-4, \"batch_size\": 4, \"warmup_ratio\": 0.1,\n",
    "            \"neftune_alpha\": 0, \"use_dora\": False\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"DoRA + NEFTune (Recommended)\",\n",
    "            \"lora_r\": 16, \"lora_alpha\": 32, \"lora_dropout\": 0.05,\n",
    "            \"learning_rate\": 2e-4, \"batch_size\": 4, \"warmup_ratio\": 0.1,\n",
    "            \"neftune_alpha\": 5, \"use_dora\": True\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"High Capacity\",\n",
    "            \"lora_r\": 64, \"lora_alpha\": 128, \"lora_dropout\": 0.1,\n",
    "            \"learning_rate\": 1e-4, \"batch_size\": 4, \"warmup_ratio\": 0.1,\n",
    "            \"neftune_alpha\": 5, \"use_dora\": True\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Fast Training\",\n",
    "            \"lora_r\": 8, \"lora_alpha\": 16, \"lora_dropout\": 0.0,\n",
    "            \"learning_rate\": 5e-4, \"batch_size\": 8, \"warmup_ratio\": 0.03,\n",
    "            \"neftune_alpha\": 0, \"use_dora\": False\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    print(\"Recommended Configurations:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for config in recommended_configs:\n",
    "        print(f\"\\n{config['name']}:\")\n",
    "        for k, v in config.items():\n",
    "            if k != \"name\":\n",
    "                print(f\"  {k}: {v}\")\n",
    "    \n",
    "    # Calculate total combinations\n",
    "    total = 1\n",
    "    for values in search_space.values():\n",
    "        total *= len(values)\n",
    "    print(f\"\\nTotal possible combinations: {total:,}\")\n",
    "    print(\"Consider using Optuna or Ray Tune for efficient search.\")\n",
    "    \n",
    "    return search_space, recommended_configs\n",
    "\n",
    "search_space, configs = hyperparameter_search_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_finetuned_model(model, tokenizer, prompts: list):\n",
    "    \"\"\"\n",
    "    Evaluate fine-tuned model on test prompts.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        formatted = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
    "        \n",
    "        inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = response.split(\"### Response:\\n\")[-1].strip()\n",
    "        \n",
    "        results.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"-\"*60)\n",
    "        print(f\"Response: {response}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"Write a Python function to reverse a string.\",\n",
    "    \"What are the benefits of exercise?\",\n",
    "    \"Summarize the plot of Romeo and Juliet in one paragraph.\"\n",
    "]\n",
    "\n",
    "# Uncomment after training\n",
    "# results = evaluate_finetuned_model(model, tokenizer, test_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Save and Load Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "def save_and_load_adapter(model, tokenizer, save_path: str):\n",
    "    \"\"\"\n",
    "    Save LoRA adapter and demonstrate loading.\n",
    "    \"\"\"\n",
    "    # Save adapter only (not full model)\n",
    "    print(f\"Saving adapter to {save_path}...\")\n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    \n",
    "    # Check saved files\n",
    "    import os\n",
    "    files = os.listdir(save_path)\n",
    "    print(f\"\\nSaved files: {files}\")\n",
    "    \n",
    "    # Calculate size\n",
    "    total_size = sum(\n",
    "        os.path.getsize(os.path.join(save_path, f)) \n",
    "        for f in files if os.path.isfile(os.path.join(save_path, f))\n",
    "    )\n",
    "    print(f\"Total size: {total_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "def load_adapter(base_model_id: str, adapter_path: str):\n",
    "    \"\"\"\n",
    "    Load adapter onto base model.\n",
    "    \"\"\"\n",
    "    print(f\"Loading base model: {base_model_id}\")\n",
    "    \n",
    "    # Quantization config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Loading adapter: {adapter_path}\")\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "    \n",
    "    print(\"Model loaded successfully!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# Example usage:\n",
    "# save_and_load_adapter(model, tokenizer, \"./my-adapter\")\n",
    "# loaded_model, loaded_tokenizer = load_adapter(\n",
    "#     \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "#     \"./my-adapter\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **QLoRA + DoRA + NEFTune**: The winning combination for efficient fine-tuning\n",
    "2. **Memory**: 4-bit quantization + gradient checkpointing enables 8B training on consumer GPUs\n",
    "3. **Hyperparameters**: r=16, alpha=32, lr=2e-4 is a solid starting point\n",
    "4. **Evaluation**: Always test on diverse prompts before deployment\n",
    "5. **Saving**: Adapters are tiny (~100MB) vs full model (~16GB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
