{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1.5: 70B Model QLoRA Fine-Tuning - Solutions\n",
    "\n",
    "Complete solutions for fine-tuning 70B models on DGX Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Memory Planning Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_70b_memory(\n",
    "    model_params_b: float = 70,\n",
    "    bits: int = 4,\n",
    "    lora_r: int = 16,\n",
    "    batch_size: int = 1,\n",
    "    seq_len: int = 2048,\n",
    "    hidden_dim: int = 8192,\n",
    "    num_layers: int = 80,\n",
    "    gradient_checkpointing: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate memory requirements for 70B QLoRA training.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"70B MODEL MEMORY CALCULATOR\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Base model memory (quantized)\n",
    "    model_mem = model_params_b * (bits / 8)\n",
    "    print(f\"\\n1. Base Model ({bits}-bit): {model_mem:.1f} GB\")\n",
    "    \n",
    "    # LoRA adapter parameters\n",
    "    # Targeting: q, k, v, o projections + gate, up, down\n",
    "    lora_targets = 7  # per layer\n",
    "    lora_params = num_layers * lora_targets * 2 * hidden_dim * lora_r\n",
    "    lora_mem = lora_params * 2 / 1e9  # FP16\n",
    "    print(f\"2. LoRA Adapters (r={lora_r}): {lora_mem:.2f} GB\")\n",
    "    \n",
    "    # Optimizer states (AdamW: m and v)\n",
    "    optimizer_mem = lora_params * 4 * 2 / 1e9  # FP32 states\n",
    "    print(f\"3. Optimizer States: {optimizer_mem:.2f} GB\")\n",
    "    \n",
    "    # Gradients for LoRA params\n",
    "    gradient_mem = lora_params * 4 / 1e9  # FP32\n",
    "    print(f\"4. Gradients: {gradient_mem:.2f} GB\")\n",
    "    \n",
    "    # Activation memory\n",
    "    if gradient_checkpointing:\n",
    "        # Only keep sqrt(L) checkpoints\n",
    "        ckpt_layers = int(num_layers ** 0.5)\n",
    "        activation_mem = batch_size * seq_len * hidden_dim * ckpt_layers * 2 / 1e9\n",
    "    else:\n",
    "        activation_mem = batch_size * seq_len * hidden_dim * num_layers * 2 / 1e9\n",
    "    print(f\"5. Activations {'(with GC)' if gradient_checkpointing else ''}: {activation_mem:.2f} GB\")\n",
    "    \n",
    "    # KV cache for inference\n",
    "    kv_cache = 2 * batch_size * seq_len * hidden_dim * num_layers * 2 / 1e9\n",
    "    print(f\"6. KV Cache: {kv_cache:.2f} GB\")\n",
    "    \n",
    "    # CUDA overhead\n",
    "    cuda_overhead = 2.0\n",
    "    print(f\"7. CUDA Overhead: {cuda_overhead:.1f} GB\")\n",
    "    \n",
    "    # Total\n",
    "    total = model_mem + lora_mem + optimizer_mem + gradient_mem + activation_mem + kv_cache + cuda_overhead\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"TOTAL ESTIMATED: {total:.1f} GB\")\n",
    "    print(f\"DGX Spark Available: 128 GB\")\n",
    "    print(f\"Headroom: {128 - total:.1f} GB\")\n",
    "    \n",
    "    if total > 128:\n",
    "        print(\"\\n‚ö†Ô∏è WARNING: May exceed memory. Consider:\")\n",
    "        print(\"  - Reduce batch size\")\n",
    "        print(\"  - Reduce sequence length\")\n",
    "        print(\"  - Use lower LoRA rank\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ Should fit in DGX Spark memory!\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return {\n",
    "        \"model\": model_mem,\n",
    "        \"lora\": lora_mem,\n",
    "        \"optimizer\": optimizer_mem,\n",
    "        \"gradients\": gradient_mem,\n",
    "        \"activations\": activation_mem,\n",
    "        \"kv_cache\": kv_cache,\n",
    "        \"overhead\": cuda_overhead,\n",
    "        \"total\": total\n",
    "    }\n",
    "\n",
    "# Calculate for default settings\n",
    "mem = calculate_70b_memory()\n",
    "\n",
    "# Try with larger batch\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WITH BATCH SIZE 2:\")\n",
    "mem_batch2 = calculate_70b_memory(batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Optimal Configuration Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_config(memory_budget_gb: float = 120):\n",
    "    \"\"\"\n",
    "    Find optimal training configuration for given memory budget.\n",
    "    \"\"\"\n",
    "    configs = []\n",
    "    \n",
    "    for batch_size in [1, 2, 4]:\n",
    "        for seq_len in [512, 1024, 2048, 4096]:\n",
    "            for lora_r in [8, 16, 32, 64]:\n",
    "                for grad_accum in [1, 4, 8, 16]:\n",
    "                    mem = calculate_70b_memory(\n",
    "                        batch_size=batch_size,\n",
    "                        seq_len=seq_len,\n",
    "                        lora_r=lora_r,\n",
    "                        gradient_checkpointing=True\n",
    "                    )\n",
    "                    \n",
    "                    if mem[\"total\"] <= memory_budget_gb:\n",
    "                        effective_batch = batch_size * grad_accum\n",
    "                        throughput_score = effective_batch * seq_len / mem[\"total\"]\n",
    "                        \n",
    "                        configs.append({\n",
    "                            \"batch_size\": batch_size,\n",
    "                            \"seq_len\": seq_len,\n",
    "                            \"lora_r\": lora_r,\n",
    "                            \"grad_accum\": grad_accum,\n",
    "                            \"effective_batch\": effective_batch,\n",
    "                            \"memory_gb\": mem[\"total\"],\n",
    "                            \"throughput_score\": throughput_score\n",
    "                        })\n",
    "    \n",
    "    # Sort by throughput score\n",
    "    configs.sort(key=lambda x: x[\"throughput_score\"], reverse=True)\n",
    "    \n",
    "    print(f\"\\nTop 5 Configurations (budget: {memory_budget_gb}GB):\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Batch':<8} {'SeqLen':<8} {'LoRA r':<8} {'Accum':<8} {'Eff.Batch':<10} {'Memory':<10} {'Score':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for config in configs[:5]:\n",
    "        print(f\"{config['batch_size']:<8} {config['seq_len']:<8} {config['lora_r']:<8} \"\n",
    "              f\"{config['grad_accum']:<8} {config['effective_batch']:<10} \"\n",
    "              f\"{config['memory_gb']:<10.1f} {config['throughput_score']:<10.2f}\")\n",
    "    \n",
    "    return configs[:5]\n",
    "\n",
    "# Suppress individual calculations for this search\n",
    "import sys\n",
    "from io import StringIO\n",
    "\n",
    "# Redirect output\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = StringIO()\n",
    "\n",
    "optimal_configs = find_optimal_config(memory_budget_gb=120)\n",
    "\n",
    "# Restore output\n",
    "sys.stdout = old_stdout\n",
    "\n",
    "# Show results\n",
    "print(\"\\nTop 5 Configurations (budget: 120GB):\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Batch':<8} {'SeqLen':<8} {'LoRA r':<8} {'Accum':<8} {'Eff.Batch':<10} {'Memory':<10}\")\n",
    "print(\"-\" * 80)\n",
    "for config in optimal_configs:\n",
    "    print(f\"{config['batch_size']:<8} {config['seq_len']:<8} {config['lora_r']:<8} \"\n",
    "          f\"{config['grad_accum']:<8} {config['effective_batch']:<10} \"\n",
    "          f\"{config['memory_gb']:<10.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Complete 70B Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset\n",
    "import gc\n",
    "\n",
    "def train_70b_qlora(\n",
    "    model_id: str = \"meta-llama/Llama-3.1-70B\",\n",
    "    dataset_name: str = \"tatsu-lab/alpaca\",\n",
    "    output_dir: str = \"./llama-70b-qlora\",\n",
    "    max_samples: int = 500,\n",
    "    epochs: int = 1\n",
    "):\n",
    "    \"\"\"\n",
    "    Production-ready 70B QLoRA training script for DGX Spark.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"70B MODEL QLORA TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Clear memory\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # 1. Quantization - Critical for 70B\n",
    "    print(\"\\n1. Setting up 4-bit quantization...\")\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True  # Nested quantization\n",
    "    )\n",
    "    \n",
    "    # 2. Load model with memory optimizations\n",
    "    print(\"2. Loading 70B model (this takes a few minutes)...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        # Memory optimization\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    \n",
    "    # Prepare for training\n",
    "    model = prepare_model_for_kbit_training(\n",
    "        model,\n",
    "        use_gradient_checkpointing=True\n",
    "    )\n",
    "    \n",
    "    # Enable gradient checkpointing\n",
    "    model.gradient_checkpointing_enable()\n",
    "    \n",
    "    print(f\"   Model loaded. Memory: {torch.cuda.memory_allocated()/1e9:.1f}GB\")\n",
    "    \n",
    "    # 3. Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    # 4. Conservative LoRA config for 70B\n",
    "    print(\"3. Configuring LoRA (conservative for 70B)...\")\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,  # Conservative rank\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "        ],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        use_dora=True,  # DoRA for better performance\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # 5. Load dataset\n",
    "    print(\"4. Loading dataset...\")\n",
    "    dataset = load_dataset(dataset_name, split=f\"train[:{max_samples}]\")\n",
    "    \n",
    "    def format_alpaca(example):\n",
    "        if example.get(\"input\", \"\").strip():\n",
    "            return {\"text\": f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{example['instruction']}\n",
    "\n",
    "Input: {example['input']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{example['output']}<|eot_id|>\"\"\"}\n",
    "        else:\n",
    "            return {\"text\": f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{example['instruction']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{example['output']}<|eot_id|}\"\"\"}\n",
    "    \n",
    "    dataset = dataset.map(format_alpaca)\n",
    "    \n",
    "    # 6. Memory-optimized training config\n",
    "    print(\"5. Setting up training...\")\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        \n",
    "        # Memory-optimized batch settings\n",
    "        per_device_train_batch_size=1,  # Single sample per step\n",
    "        gradient_accumulation_steps=16,  # Effective batch of 16\n",
    "        \n",
    "        # Learning rate\n",
    "        learning_rate=1e-4,  # Conservative for large models\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.1,\n",
    "        \n",
    "        # Precision\n",
    "        bf16=True,\n",
    "        \n",
    "        # Memory optimizations\n",
    "        gradient_checkpointing=True,\n",
    "        optim=\"paged_adamw_8bit\",  # 8-bit optimizer\n",
    "        \n",
    "        # Sequence length (conservative)\n",
    "        max_seq_length=1024,\n",
    "        \n",
    "        # NEFTune\n",
    "        neftune_noise_alpha=5.0,\n",
    "        \n",
    "        # Logging\n",
    "        logging_steps=5,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        save_total_limit=2,\n",
    "        \n",
    "        # Disable\n",
    "        report_to=\"none\",\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "    \n",
    "    # 7. Create trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        dataset_text_field=\"text\",\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nMemory before training: {torch.cuda.memory_allocated()/1e9:.1f}GB\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Ready to train! Call trainer.train()\")\n",
    "    \n",
    "    return trainer, model, tokenizer\n",
    "\n",
    "# Uncomment to run (requires access to Llama 70B)\n",
    "# trainer, model, tokenizer = train_70b_qlora()\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Memory Monitoring During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MemoryMonitor:\n",
    "    \"\"\"\n",
    "    Monitor GPU memory during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, interval: float = 1.0):\n",
    "        self.interval = interval\n",
    "        self.memory_log = []\n",
    "        self.timestamps = []\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "    \n",
    "    def _monitor(self):\n",
    "        start_time = time.time()\n",
    "        while self.running:\n",
    "            if torch.cuda.is_available():\n",
    "                allocated = torch.cuda.memory_allocated() / 1e9\n",
    "                reserved = torch.cuda.memory_reserved() / 1e9\n",
    "                self.memory_log.append((allocated, reserved))\n",
    "                self.timestamps.append(time.time() - start_time)\n",
    "            time.sleep(self.interval)\n",
    "    \n",
    "    def start(self):\n",
    "        self.running = True\n",
    "        self.memory_log = []\n",
    "        self.timestamps = []\n",
    "        self.thread = threading.Thread(target=self._monitor)\n",
    "        self.thread.start()\n",
    "        print(\"Memory monitoring started...\")\n",
    "    \n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        if self.thread:\n",
    "            self.thread.join()\n",
    "        print(\"Memory monitoring stopped.\")\n",
    "    \n",
    "    def plot(self, save_path: str = None):\n",
    "        if not self.memory_log:\n",
    "            print(\"No data to plot.\")\n",
    "            return\n",
    "        \n",
    "        allocated = [m[0] for m in self.memory_log]\n",
    "        reserved = [m[1] for m in self.memory_log]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        ax.plot(self.timestamps, allocated, label='Allocated', linewidth=2)\n",
    "        ax.plot(self.timestamps, reserved, label='Reserved', linewidth=2, alpha=0.7)\n",
    "        ax.axhline(y=128, color='r', linestyle='--', label='DGX Spark Limit')\n",
    "        \n",
    "        ax.set_xlabel('Time (seconds)')\n",
    "        ax.set_ylabel('Memory (GB)')\n",
    "        ax.set_title('GPU Memory Usage During Training')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=150)\n",
    "        plt.show()\n",
    "        \n",
    "        # Statistics\n",
    "        print(f\"\\nMemory Statistics:\")\n",
    "        print(f\"  Peak Allocated: {max(allocated):.1f} GB\")\n",
    "        print(f\"  Peak Reserved: {max(reserved):.1f} GB\")\n",
    "        print(f\"  Average Allocated: {sum(allocated)/len(allocated):.1f} GB\")\n",
    "\n",
    "# Usage example:\n",
    "# monitor = MemoryMonitor(interval=0.5)\n",
    "# monitor.start()\n",
    "# trainer.train()\n",
    "# monitor.stop()\n",
    "# monitor.plot('memory_usage.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Troubleshooting OOM Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_oom():\n",
    "    \"\"\"\n",
    "    Diagnose and fix out-of-memory issues.\n",
    "    \"\"\"\n",
    "    print(\"OOM TROUBLESHOOTING GUIDE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    fixes = [\n",
    "        {\n",
    "            \"issue\": \"OOM during model loading\",\n",
    "            \"solutions\": [\n",
    "                \"Clear system cache: sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'\",\n",
    "                \"Use load_in_4bit=True in BitsAndBytesConfig\",\n",
    "                \"Add low_cpu_mem_usage=True to from_pretrained()\",\n",
    "                \"Kill other GPU processes: nvidia-smi then kill PID\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"issue\": \"OOM during forward pass\",\n",
    "            \"solutions\": [\n",
    "                \"Reduce batch_size to 1\",\n",
    "                \"Reduce max_seq_length (try 512 first)\",\n",
    "                \"Enable gradient_checkpointing=True\",\n",
    "                \"Use attn_implementation='flash_attention_2'\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"issue\": \"OOM during backward pass\",\n",
    "            \"solutions\": [\n",
    "                \"Use gradient_checkpointing=True\",\n",
    "                \"Reduce LoRA rank (r=8 instead of r=16)\",\n",
    "                \"Use paged_adamw_8bit optimizer\",\n",
    "                \"Increase gradient_accumulation_steps instead of batch_size\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"issue\": \"Memory keeps growing during training\",\n",
    "            \"solutions\": [\n",
    "                \"Clear cache periodically: torch.cuda.empty_cache()\",\n",
    "                \"Check for memory leaks in callbacks\",\n",
    "                \"Disable evaluation during training\",\n",
    "                \"Save checkpoints less frequently\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for fix in fixes:\n",
    "        print(f\"\\nüìç {fix['issue']}\")\n",
    "        for i, solution in enumerate(fix['solutions'], 1):\n",
    "            print(f\"   {i}. {solution}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EMERGENCY MEMORY RECOVERY:\")\n",
    "    print(\"\"\"\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Delete model and clear references\n",
    "del model\n",
    "del trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Check memory\n",
    "print(f\"Memory freed: {torch.cuda.memory_allocated()/1e9:.1f}GB used\")\n",
    "    \"\"\")\n",
    "\n",
    "diagnose_oom()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Memory Planning**: Calculate before loading - 70B needs careful planning\n",
    "2. **Essential Optimizations**: 4-bit + gradient checkpointing + paged optimizer\n",
    "3. **Conservative Settings**: batch_size=1, seq_len=1024, r=16 for safety\n",
    "4. **Monitoring**: Track memory throughout training\n",
    "5. **Recovery**: Know how to free memory when things go wrong"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
