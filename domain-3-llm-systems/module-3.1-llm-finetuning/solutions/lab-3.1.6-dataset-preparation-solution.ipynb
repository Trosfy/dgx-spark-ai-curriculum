{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1.6: Dataset Preparation - Solutions\n",
    "\n",
    "Complete solutions for dataset preparation exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Multi-Format Dataset Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class UniversalConverter:\n",
    "    \"\"\"\n",
    "    Universal dataset format converter supporting multiple formats.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def detect_format(data: List[Dict]) -> str:\n",
    "        \"\"\"Auto-detect dataset format.\"\"\"\n",
    "        sample = data[0]\n",
    "        \n",
    "        if \"conversations\" in sample:\n",
    "            return \"sharegpt\"\n",
    "        elif \"messages\" in sample:\n",
    "            return \"openai\"\n",
    "        elif \"instruction\" in sample:\n",
    "            return \"alpaca\"\n",
    "        elif \"prompt\" in sample and \"completion\" in sample:\n",
    "            return \"completion\"\n",
    "        elif \"chosen\" in sample and \"rejected\" in sample:\n",
    "            return \"preference\"\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_sharegpt(data: List[Dict], source_format: str) -> List[Dict]:\n",
    "        \"\"\"Convert any format to ShareGPT.\"\"\"\n",
    "        converted = []\n",
    "        \n",
    "        for item in data:\n",
    "            if source_format == \"alpaca\":\n",
    "                conversations = [\n",
    "                    {\"from\": \"human\", \"value\": item.get(\"instruction\", \"\") + \n",
    "                     (f\"\\n\\nInput: {item['input']}\" if item.get(\"input\") else \"\")},\n",
    "                    {\"from\": \"gpt\", \"value\": item.get(\"output\", \"\")}\n",
    "                ]\n",
    "            elif source_format == \"openai\":\n",
    "                conversations = []\n",
    "                for msg in item.get(\"messages\", []):\n",
    "                    role = \"human\" if msg[\"role\"] == \"user\" else \"gpt\"\n",
    "                    conversations.append({\"from\": role, \"value\": msg[\"content\"]})\n",
    "            elif source_format == \"completion\":\n",
    "                conversations = [\n",
    "                    {\"from\": \"human\", \"value\": item.get(\"prompt\", \"\")},\n",
    "                    {\"from\": \"gpt\", \"value\": item.get(\"completion\", \"\")}\n",
    "                ]\n",
    "            else:\n",
    "                conversations = item.get(\"conversations\", [])\n",
    "            \n",
    "            converted.append({\"conversations\": conversations})\n",
    "        \n",
    "        return converted\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_alpaca(data: List[Dict], source_format: str) -> List[Dict]:\n",
    "        \"\"\"Convert any format to Alpaca.\"\"\"\n",
    "        converted = []\n",
    "        \n",
    "        for item in data:\n",
    "            if source_format == \"sharegpt\":\n",
    "                convs = item.get(\"conversations\", [])\n",
    "                human_msgs = [c[\"value\"] for c in convs if c[\"from\"] == \"human\"]\n",
    "                gpt_msgs = [c[\"value\"] for c in convs if c[\"from\"] == \"gpt\"]\n",
    "                \n",
    "                converted.append({\n",
    "                    \"instruction\": human_msgs[0] if human_msgs else \"\",\n",
    "                    \"input\": \"\",\n",
    "                    \"output\": gpt_msgs[0] if gpt_msgs else \"\"\n",
    "                })\n",
    "            elif source_format == \"openai\":\n",
    "                msgs = item.get(\"messages\", [])\n",
    "                user_msgs = [m[\"content\"] for m in msgs if m[\"role\"] == \"user\"]\n",
    "                asst_msgs = [m[\"content\"] for m in msgs if m[\"role\"] == \"assistant\"]\n",
    "                \n",
    "                converted.append({\n",
    "                    \"instruction\": user_msgs[0] if user_msgs else \"\",\n",
    "                    \"input\": \"\",\n",
    "                    \"output\": asst_msgs[0] if asst_msgs else \"\"\n",
    "                })\n",
    "            elif source_format == \"completion\":\n",
    "                converted.append({\n",
    "                    \"instruction\": item.get(\"prompt\", \"\"),\n",
    "                    \"input\": \"\",\n",
    "                    \"output\": item.get(\"completion\", \"\")\n",
    "                })\n",
    "            else:\n",
    "                converted.append(item)\n",
    "        \n",
    "        return converted\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_chatml(data: List[Dict], source_format: str, system_prompt: str = \"\") -> List[Dict]:\n",
    "        \"\"\"Convert any format to ChatML.\"\"\"\n",
    "        converted = []\n",
    "        \n",
    "        for item in data:\n",
    "            messages = []\n",
    "            \n",
    "            if system_prompt:\n",
    "                messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "            \n",
    "            if source_format == \"alpaca\":\n",
    "                user_content = item.get(\"instruction\", \"\")\n",
    "                if item.get(\"input\"):\n",
    "                    user_content += f\"\\n\\nInput: {item['input']}\"\n",
    "                messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "                messages.append({\"role\": \"assistant\", \"content\": item.get(\"output\", \"\")})\n",
    "            \n",
    "            elif source_format == \"sharegpt\":\n",
    "                for conv in item.get(\"conversations\", []):\n",
    "                    role = \"user\" if conv[\"from\"] == \"human\" else \"assistant\"\n",
    "                    messages.append({\"role\": role, \"content\": conv[\"value\"]})\n",
    "            \n",
    "            elif source_format == \"completion\":\n",
    "                messages.append({\"role\": \"user\", \"content\": item.get(\"prompt\", \"\")})\n",
    "                messages.append({\"role\": \"assistant\", \"content\": item.get(\"completion\", \"\")})\n",
    "            \n",
    "            converted.append({\"messages\": messages})\n",
    "        \n",
    "        return converted\n",
    "\n",
    "# Test\n",
    "converter = UniversalConverter()\n",
    "\n",
    "# Sample Alpaca data\n",
    "alpaca_data = [\n",
    "    {\"instruction\": \"Explain gravity\", \"input\": \"\", \"output\": \"Gravity is a force...\"},\n",
    "    {\"instruction\": \"Translate\", \"input\": \"Hello world\", \"output\": \"Hola mundo\"}\n",
    "]\n",
    "\n",
    "print(\"Alpaca → ShareGPT:\")\n",
    "print(json.dumps(converter.to_sharegpt(alpaca_data, \"alpaca\")[0], indent=2))\n",
    "\n",
    "print(\"\\nAlpaca → ChatML:\")\n",
    "print(json.dumps(converter.to_chatml(alpaca_data, \"alpaca\", \"You are helpful.\")[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Quality Filter Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "class DataQualityPipeline:\n",
    "    \"\"\"\n",
    "    Complete data quality filtering pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stats = {\n",
    "            \"total\": 0,\n",
    "            \"passed\": 0,\n",
    "            \"filtered\": Counter()\n",
    "        }\n",
    "    \n",
    "    def filter_length(\n",
    "        self,\n",
    "        text: str,\n",
    "        min_chars: int = 50,\n",
    "        max_chars: int = 10000,\n",
    "        min_words: int = 10,\n",
    "        max_words: int = 2000\n",
    "    ) -> tuple:\n",
    "        \"\"\"Filter by length.\"\"\"\n",
    "        if len(text) < min_chars:\n",
    "            return False, \"too_short_chars\"\n",
    "        if len(text) > max_chars:\n",
    "            return False, \"too_long_chars\"\n",
    "        \n",
    "        words = text.split()\n",
    "        if len(words) < min_words:\n",
    "            return False, \"too_few_words\"\n",
    "        if len(words) > max_words:\n",
    "            return False, \"too_many_words\"\n",
    "        \n",
    "        return True, None\n",
    "    \n",
    "    def filter_quality(self, text: str) -> tuple:\n",
    "        \"\"\"Filter by content quality.\"\"\"\n",
    "        # Check for repetition\n",
    "        words = text.lower().split()\n",
    "        if len(words) > 10:\n",
    "            unique_ratio = len(set(words)) / len(words)\n",
    "            if unique_ratio < 0.3:\n",
    "                return False, \"too_repetitive\"\n",
    "        \n",
    "        # Check for special character ratio\n",
    "        alpha_chars = sum(c.isalpha() for c in text)\n",
    "        if len(text) > 0 and alpha_chars / len(text) < 0.5:\n",
    "            return False, \"too_many_special_chars\"\n",
    "        \n",
    "        # Check for excessive capitalization\n",
    "        upper_chars = sum(c.isupper() for c in text)\n",
    "        if len(text) > 0 and upper_chars / len(text) > 0.5:\n",
    "            return False, \"excessive_caps\"\n",
    "        \n",
    "        return True, None\n",
    "    \n",
    "    def filter_content(self, text: str) -> tuple:\n",
    "        \"\"\"Filter inappropriate content.\"\"\"\n",
    "        # Placeholder patterns (expand as needed)\n",
    "        placeholder_patterns = [\n",
    "            r\"\\[.*?\\]\",  # [placeholder]\n",
    "            r\"\\{.*?\\}\",  # {placeholder}\n",
    "            r\"<.*?>\",    # <placeholder>\n",
    "            r\"___+\",     # ____\n",
    "        ]\n",
    "        \n",
    "        for pattern in placeholder_patterns:\n",
    "            if len(re.findall(pattern, text)) > 3:\n",
    "                return False, \"too_many_placeholders\"\n",
    "        \n",
    "        # Check for \"I cannot\" / \"I'm sorry\" patterns\n",
    "        refusal_patterns = [\n",
    "            r\"i cannot\",\n",
    "            r\"i'm sorry\",\n",
    "            r\"i am unable\",\n",
    "            r\"as an ai\",\n",
    "        ]\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        for pattern in refusal_patterns:\n",
    "            if pattern in text_lower:\n",
    "                return False, \"contains_refusal\"\n",
    "        \n",
    "        return True, None\n",
    "    \n",
    "    def filter_language(\n",
    "        self,\n",
    "        text: str,\n",
    "        min_english_ratio: float = 0.8\n",
    "    ) -> tuple:\n",
    "        \"\"\"Basic language filter (English).\"\"\"\n",
    "        # Simple heuristic: check ASCII ratio\n",
    "        ascii_chars = sum(ord(c) < 128 for c in text)\n",
    "        if len(text) > 0:\n",
    "            ascii_ratio = ascii_chars / len(text)\n",
    "            if ascii_ratio < min_english_ratio:\n",
    "                return False, \"non_english\"\n",
    "        \n",
    "        return True, None\n",
    "    \n",
    "    def process(\n",
    "        self,\n",
    "        data: List[Dict],\n",
    "        text_field: str = \"text\"\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Process dataset through all filters.\"\"\"\n",
    "        filtered_data = []\n",
    "        self.stats = {\"total\": len(data), \"passed\": 0, \"filtered\": Counter()}\n",
    "        \n",
    "        for item in data:\n",
    "            text = item.get(text_field, \"\")\n",
    "            if isinstance(text, list):\n",
    "                # Handle conversation format\n",
    "                text = \" \".join(str(t) for t in text)\n",
    "            \n",
    "            # Run all filters\n",
    "            filters = [\n",
    "                self.filter_length,\n",
    "                self.filter_quality,\n",
    "                self.filter_content,\n",
    "                self.filter_language,\n",
    "            ]\n",
    "            \n",
    "            passed = True\n",
    "            for filter_func in filters:\n",
    "                result, reason = filter_func(text)\n",
    "                if not result:\n",
    "                    self.stats[\"filtered\"][reason] += 1\n",
    "                    passed = False\n",
    "                    break\n",
    "            \n",
    "            if passed:\n",
    "                filtered_data.append(item)\n",
    "                self.stats[\"passed\"] += 1\n",
    "        \n",
    "        return filtered_data\n",
    "    \n",
    "    def report(self):\n",
    "        \"\"\"Print filtering report.\"\"\"\n",
    "        print(\"\\nData Quality Report\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"Total samples: {self.stats['total']}\")\n",
    "        print(f\"Passed: {self.stats['passed']} ({100*self.stats['passed']/self.stats['total']:.1f}%)\")\n",
    "        print(f\"Filtered: {self.stats['total'] - self.stats['passed']}\")\n",
    "        print(\"\\nFilter breakdown:\")\n",
    "        for reason, count in self.stats[\"filtered\"].most_common():\n",
    "            print(f\"  {reason}: {count}\")\n",
    "\n",
    "# Test\n",
    "test_data = [\n",
    "    {\"text\": \"This is a good quality response about machine learning and its applications.\"},\n",
    "    {\"text\": \"Hi\"},  # Too short\n",
    "    {\"text\": \"word \" * 500},  # Repetitive\n",
    "    {\"text\": \"I cannot help with that request. As an AI...\"},  # Refusal\n",
    "    {\"text\": \"!!!! @@@@ #### $$$$ %%%%\"},  # Special chars\n",
    "    {\"text\": \"This is another valid response with sufficient length and quality content.\"},\n",
    "]\n",
    "\n",
    "pipeline = DataQualityPipeline()\n",
    "filtered = pipeline.process(test_data)\n",
    "pipeline.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Preference Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreferenceDataGenerator:\n",
    "    \"\"\"\n",
    "    Generate preference pairs for DPO/ORPO training.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_ratings(\n",
    "        data: List[Dict],\n",
    "        prompt_field: str = \"prompt\",\n",
    "        response_field: str = \"response\",\n",
    "        rating_field: str = \"rating\"\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Create pairs from rated responses.\n",
    "        \n",
    "        Higher rating = chosen, lower rating = rejected.\n",
    "        \"\"\"\n",
    "        from itertools import groupby\n",
    "        \n",
    "        # Group by prompt\n",
    "        sorted_data = sorted(data, key=lambda x: x[prompt_field])\n",
    "        pairs = []\n",
    "        \n",
    "        for prompt, group in groupby(sorted_data, key=lambda x: x[prompt_field]):\n",
    "            responses = list(group)\n",
    "            if len(responses) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Sort by rating\n",
    "            responses.sort(key=lambda x: x[rating_field], reverse=True)\n",
    "            \n",
    "            # Create pairs: best vs each worse\n",
    "            best = responses[0]\n",
    "            for worse in responses[1:]:\n",
    "                if best[rating_field] > worse[rating_field]:\n",
    "                    pairs.append({\n",
    "                        \"prompt\": prompt,\n",
    "                        \"chosen\": best[response_field],\n",
    "                        \"rejected\": worse[response_field]\n",
    "                    })\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_comparison(\n",
    "        data: List[Dict],\n",
    "        prompt_field: str = \"prompt\",\n",
    "        response_a_field: str = \"response_a\",\n",
    "        response_b_field: str = \"response_b\",\n",
    "        preference_field: str = \"preference\"  # \"a\" or \"b\"\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Create pairs from A/B comparisons.\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        \n",
    "        for item in data:\n",
    "            if item[preference_field].lower() == \"a\":\n",
    "                chosen = item[response_a_field]\n",
    "                rejected = item[response_b_field]\n",
    "            else:\n",
    "                chosen = item[response_b_field]\n",
    "                rejected = item[response_a_field]\n",
    "            \n",
    "            pairs.append({\n",
    "                \"prompt\": item[prompt_field],\n",
    "                \"chosen\": chosen,\n",
    "                \"rejected\": rejected\n",
    "            })\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_binary_feedback(\n",
    "        data: List[Dict],\n",
    "        prompt_field: str = \"prompt\",\n",
    "        response_field: str = \"response\",\n",
    "        feedback_field: str = \"feedback\"  # \"good\" or \"bad\"\n",
    "    ) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"\n",
    "        Organize data for KTO training (binary feedback).\n",
    "        \"\"\"\n",
    "        desirable = []\n",
    "        undesirable = []\n",
    "        \n",
    "        for item in data:\n",
    "            entry = {\n",
    "                \"prompt\": item[prompt_field],\n",
    "                \"completion\": item[response_field]\n",
    "            }\n",
    "            \n",
    "            if item[feedback_field].lower() in [\"good\", \"positive\", \"1\", \"true\"]:\n",
    "                entry[\"label\"] = True\n",
    "                desirable.append(entry)\n",
    "            else:\n",
    "                entry[\"label\"] = False\n",
    "                undesirable.append(entry)\n",
    "        \n",
    "        return {\n",
    "            \"desirable\": desirable,\n",
    "            \"undesirable\": undesirable\n",
    "        }\n",
    "\n",
    "# Test\n",
    "generator = PreferenceDataGenerator()\n",
    "\n",
    "# From ratings\n",
    "rated_data = [\n",
    "    {\"prompt\": \"What is AI?\", \"response\": \"AI is artificial intelligence.\", \"rating\": 3},\n",
    "    {\"prompt\": \"What is AI?\", \"response\": \"AI refers to machine learning systems.\", \"rating\": 4},\n",
    "    {\"prompt\": \"What is AI?\", \"response\": \"Comprehensive explanation...\", \"rating\": 5},\n",
    "]\n",
    "\n",
    "pairs = generator.from_ratings(rated_data)\n",
    "print(\"From Ratings:\")\n",
    "print(json.dumps(pairs, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "class DataDeduplicator:\n",
    "    \"\"\"\n",
    "    Remove duplicate and near-duplicate samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def exact_dedup(data: List[Dict], text_field: str = \"text\") -> List[Dict]:\n",
    "        \"\"\"Remove exact duplicates using hash.\"\"\"\n",
    "        seen = set()\n",
    "        unique = []\n",
    "        \n",
    "        for item in data:\n",
    "            text = str(item.get(text_field, \"\"))\n",
    "            text_hash = hashlib.md5(text.encode()).hexdigest()\n",
    "            \n",
    "            if text_hash not in seen:\n",
    "                seen.add(text_hash)\n",
    "                unique.append(item)\n",
    "        \n",
    "        print(f\"Exact dedup: {len(data)} → {len(unique)} ({len(data)-len(unique)} removed)\")\n",
    "        return unique\n",
    "    \n",
    "    @staticmethod\n",
    "    def fuzzy_dedup(\n",
    "        data: List[Dict],\n",
    "        text_field: str = \"text\",\n",
    "        threshold: float = 0.9\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Remove near-duplicates using similarity threshold.\n",
    "        \n",
    "        Note: O(n²) complexity - use MinHash for large datasets.\n",
    "        \"\"\"\n",
    "        unique = []\n",
    "        \n",
    "        for item in data:\n",
    "            text = str(item.get(text_field, \"\"))\n",
    "            is_duplicate = False\n",
    "            \n",
    "            for existing in unique:\n",
    "                existing_text = str(existing.get(text_field, \"\"))\n",
    "                similarity = SequenceMatcher(None, text, existing_text).ratio()\n",
    "                \n",
    "                if similarity >= threshold:\n",
    "                    is_duplicate = True\n",
    "                    break\n",
    "            \n",
    "            if not is_duplicate:\n",
    "                unique.append(item)\n",
    "        \n",
    "        print(f\"Fuzzy dedup: {len(data)} → {len(unique)} ({len(data)-len(unique)} removed)\")\n",
    "        return unique\n",
    "    \n",
    "    @staticmethod\n",
    "    def minhash_dedup(\n",
    "        data: List[Dict],\n",
    "        text_field: str = \"text\",\n",
    "        threshold: float = 0.8,\n",
    "        num_perm: int = 128\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Efficient near-duplicate detection using MinHash LSH.\n",
    "        \n",
    "        Requires: pip install datasketch\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from datasketch import MinHash, MinHashLSH\n",
    "        except ImportError:\n",
    "            print(\"Install datasketch: pip install datasketch\")\n",
    "            return data\n",
    "        \n",
    "        lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "        unique = []\n",
    "        \n",
    "        for i, item in enumerate(data):\n",
    "            text = str(item.get(text_field, \"\"))\n",
    "            \n",
    "            # Create MinHash\n",
    "            mh = MinHash(num_perm=num_perm)\n",
    "            for word in text.split():\n",
    "                mh.update(word.encode('utf-8'))\n",
    "            \n",
    "            # Check for duplicates\n",
    "            if not lsh.query(mh):\n",
    "                lsh.insert(f\"doc_{i}\", mh)\n",
    "                unique.append(item)\n",
    "        \n",
    "        print(f\"MinHash dedup: {len(data)} → {len(unique)} ({len(data)-len(unique)} removed)\")\n",
    "        return unique\n",
    "\n",
    "# Test\n",
    "deduplicator = DataDeduplicator()\n",
    "\n",
    "test_data = [\n",
    "    {\"text\": \"The quick brown fox jumps over the lazy dog.\"},\n",
    "    {\"text\": \"The quick brown fox jumps over the lazy dog.\"},  # Exact dup\n",
    "    {\"text\": \"The quick brown fox jumped over the lazy dog.\"},  # Near dup\n",
    "    {\"text\": \"A completely different sentence about something else.\"},\n",
    "]\n",
    "\n",
    "deduped = deduplicator.exact_dedup(test_data)\n",
    "deduped = deduplicator.fuzzy_dedup(deduped, threshold=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Format Conversion**: Support multiple formats (Alpaca, ShareGPT, ChatML)\n",
    "2. **Quality Filtering**: Length, quality, content, and language checks\n",
    "3. **Preference Pairs**: Generate from ratings, comparisons, or binary feedback\n",
    "4. **Deduplication**: Exact hash + fuzzy matching + MinHash for scale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
