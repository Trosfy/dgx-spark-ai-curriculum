{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1.7: DPO Training - Solutions\n",
    "\n",
    "Complete solutions for Direct Preference Optimization exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: DPO Loss Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def dpo_loss(\n",
    "    policy_chosen_logps: torch.Tensor,\n",
    "    policy_rejected_logps: torch.Tensor,\n",
    "    reference_chosen_logps: torch.Tensor,\n",
    "    reference_rejected_logps: torch.Tensor,\n",
    "    beta: float = 0.1\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Compute DPO loss from scratch.\n",
    "    \n",
    "    L_DPO = -log(σ(β * (log(π(y_w|x)/π_ref(y_w|x)) - log(π(y_l|x)/π_ref(y_l|x)))))\n",
    "    \n",
    "    Where:\n",
    "    - π = policy model\n",
    "    - π_ref = reference (frozen) model\n",
    "    - y_w = chosen (winning) response\n",
    "    - y_l = rejected (losing) response\n",
    "    - β = temperature parameter\n",
    "    \"\"\"\n",
    "    # Compute log ratios\n",
    "    chosen_logratios = policy_chosen_logps - reference_chosen_logps\n",
    "    rejected_logratios = policy_rejected_logps - reference_rejected_logps\n",
    "    \n",
    "    # Compute DPO loss\n",
    "    # logits = β * (chosen_logratio - rejected_logratio)\n",
    "    logits = beta * (chosen_logratios - rejected_logratios)\n",
    "    \n",
    "    # Binary cross entropy with logits (more stable than sigmoid + log)\n",
    "    # Target is 1 (chosen should be preferred)\n",
    "    losses = -F.logsigmoid(logits)\n",
    "    \n",
    "    # Metrics\n",
    "    chosen_rewards = beta * chosen_logratios.detach()\n",
    "    rejected_rewards = beta * rejected_logratios.detach()\n",
    "    reward_margins = (chosen_rewards - rejected_rewards).mean()\n",
    "    accuracy = (logits > 0).float().mean()\n",
    "    \n",
    "    return losses.mean(), {\n",
    "        \"chosen_rewards\": chosen_rewards.mean().item(),\n",
    "        \"rejected_rewards\": rejected_rewards.mean().item(),\n",
    "        \"reward_margin\": reward_margins.item(),\n",
    "        \"accuracy\": accuracy.item()\n",
    "    }\n",
    "\n",
    "# Test\n",
    "torch.manual_seed(42)\n",
    "batch_size = 4\n",
    "\n",
    "# Simulate log probabilities\n",
    "policy_chosen = torch.randn(batch_size) - 1  # Higher for chosen\n",
    "policy_rejected = torch.randn(batch_size) - 2  # Lower for rejected\n",
    "ref_chosen = torch.randn(batch_size) - 1.5\n",
    "ref_rejected = torch.randn(batch_size) - 1.5\n",
    "\n",
    "loss, metrics = dpo_loss(\n",
    "    policy_chosen, policy_rejected,\n",
    "    ref_chosen, ref_rejected,\n",
    "    beta=0.1\n",
    ")\n",
    "\n",
    "print(f\"DPO Loss: {loss:.4f}\")\n",
    "print(f\"Metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Complete DPO Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from datasets import Dataset\n",
    "\n",
    "def create_dpo_pipeline(\n",
    "    model_id: str = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    output_dir: str = \"./dpo-output\",\n",
    "    beta: float = 0.1,\n",
    "    epochs: int = 1\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete DPO training pipeline.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DPO TRAINING PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Quantization\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    # 2. Load model\n",
    "    print(\"\\n1. Loading model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # 3. LoRA for DPO\n",
    "    print(\"2. Configuring LoRA...\")\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # 4. Reference model (frozen copy)\n",
    "    print(\"3. Creating reference model...\")\n",
    "    ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    # 5. Sample preference data\n",
    "    print(\"4. Preparing dataset...\")\n",
    "    preference_data = [\n",
    "        {\n",
    "            \"prompt\": \"Explain quantum computing in simple terms.\",\n",
    "            \"chosen\": \"Quantum computing uses quantum bits that can be 0, 1, or both at once, \"\n",
    "                      \"allowing it to process many possibilities simultaneously.\",\n",
    "            \"rejected\": \"Quantum computing is about computers using quantum stuff.\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"What are the benefits of exercise?\",\n",
    "            \"chosen\": \"Regular exercise improves cardiovascular health, boosts mood through \"\n",
    "                      \"endorphin release, enhances sleep quality, and increases energy levels.\",\n",
    "            \"rejected\": \"Exercise is good for you.\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"How do I make a good first impression?\",\n",
    "            \"chosen\": \"Make eye contact, smile genuinely, use a firm handshake, listen actively, \"\n",
    "                      \"and show interest in others by asking thoughtful questions.\",\n",
    "            \"rejected\": \"Just be yourself I guess.\"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    dataset = Dataset.from_list(preference_data)\n",
    "    \n",
    "    # 6. DPO training config\n",
    "    print(\"5. Setting up DPO training...\")\n",
    "    training_args = DPOConfig(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=5e-5,\n",
    "        \n",
    "        # DPO specific\n",
    "        beta=beta,\n",
    "        loss_type=\"sigmoid\",  # or \"hinge\", \"ipo\"\n",
    "        \n",
    "        # Memory\n",
    "        bf16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        \n",
    "        # Logging\n",
    "        logging_steps=1,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    # 7. Create DPO trainer\n",
    "    trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        ref_model=ref_model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  β (beta): {beta}\")\n",
    "    print(f\"  Loss type: {training_args.loss_type}\")\n",
    "    print(f\"  Samples: {len(dataset)}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return trainer, model, tokenizer\n",
    "\n",
    "# Uncomment to run\n",
    "# trainer, model, tokenizer = create_dpo_pipeline()\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Beta Parameter Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def beta_sensitivity_analysis():\n",
    "    \"\"\"\n",
    "    Visualize how beta affects DPO behavior.\n",
    "    \"\"\"\n",
    "    # Simulate log probability differences\n",
    "    log_ratio_diff = np.linspace(-5, 5, 100)  # chosen_logratio - rejected_logratio\n",
    "    \n",
    "    betas = [0.01, 0.1, 0.5, 1.0]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Loss curves\n",
    "    for beta in betas:\n",
    "        logits = beta * log_ratio_diff\n",
    "        loss = -np.log(1 / (1 + np.exp(-logits)))\n",
    "        axes[0].plot(log_ratio_diff, loss, label=f'β={beta}', linewidth=2)\n",
    "    \n",
    "    axes[0].set_xlabel('Log Ratio Difference (chosen - rejected)')\n",
    "    axes[0].set_ylabel('DPO Loss')\n",
    "    axes[0].set_title('DPO Loss vs Log Ratio Difference')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Plot 2: Gradient magnitude\n",
    "    for beta in betas:\n",
    "        logits = beta * log_ratio_diff\n",
    "        sigmoid = 1 / (1 + np.exp(-logits))\n",
    "        gradient = -beta * (1 - sigmoid)  # d/d(log_ratio) of -log(sigmoid)\n",
    "        axes[1].plot(log_ratio_diff, np.abs(gradient), label=f'β={beta}', linewidth=2)\n",
    "    \n",
    "    axes[1].set_xlabel('Log Ratio Difference')\n",
    "    axes[1].set_ylabel('|Gradient|')\n",
    "    axes[1].set_title('Gradient Magnitude (Learning Signal)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('beta_analysis.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nBeta Guidelines:\")\n",
    "    print(\"  β = 0.01-0.05: Very conservative, slow alignment\")\n",
    "    print(\"  β = 0.1:       Default, good starting point\")\n",
    "    print(\"  β = 0.2-0.5:   Stronger preference enforcement\")\n",
    "    print(\"  β > 0.5:       Aggressive, risk of overfitting\")\n",
    "\n",
    "beta_sensitivity_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dpo_model(model, tokenizer, ref_model, eval_data: list):\n",
    "    \"\"\"\n",
    "    Evaluate DPO model performance.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    ref_model.eval()\n",
    "    \n",
    "    metrics = {\n",
    "        \"preference_accuracy\": [],\n",
    "        \"reward_margin\": [],\n",
    "        \"kl_divergence\": []\n",
    "    }\n",
    "    \n",
    "    for item in eval_data:\n",
    "        prompt = item[\"prompt\"]\n",
    "        chosen = item[\"chosen\"]\n",
    "        rejected = item[\"rejected\"]\n",
    "        \n",
    "        # Tokenize\n",
    "        chosen_input = tokenizer(prompt + chosen, return_tensors=\"pt\").to(model.device)\n",
    "        rejected_input = tokenizer(prompt + rejected, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Policy log probs\n",
    "            policy_chosen_logp = model(**chosen_input).logits.log_softmax(-1).mean()\n",
    "            policy_rejected_logp = model(**rejected_input).logits.log_softmax(-1).mean()\n",
    "            \n",
    "            # Reference log probs\n",
    "            ref_chosen_logp = ref_model(**chosen_input).logits.log_softmax(-1).mean()\n",
    "            ref_rejected_logp = ref_model(**rejected_input).logits.log_softmax(-1).mean()\n",
    "        \n",
    "        # Compute metrics\n",
    "        chosen_reward = policy_chosen_logp - ref_chosen_logp\n",
    "        rejected_reward = policy_rejected_logp - ref_rejected_logp\n",
    "        \n",
    "        correct = (chosen_reward > rejected_reward).float().item()\n",
    "        margin = (chosen_reward - rejected_reward).item()\n",
    "        kl = (policy_chosen_logp - ref_chosen_logp).item()\n",
    "        \n",
    "        metrics[\"preference_accuracy\"].append(correct)\n",
    "        metrics[\"reward_margin\"].append(margin)\n",
    "        metrics[\"kl_divergence\"].append(kl)\n",
    "    \n",
    "    # Aggregate\n",
    "    results = {\n",
    "        \"preference_accuracy\": np.mean(metrics[\"preference_accuracy\"]),\n",
    "        \"avg_reward_margin\": np.mean(metrics[\"reward_margin\"]),\n",
    "        \"avg_kl_divergence\": np.mean(metrics[\"kl_divergence\"])\n",
    "    }\n",
    "    \n",
    "    print(\"\\nDPO Evaluation Results\")\n",
    "    print(\"=\" * 40)\n",
    "    for key, value in results.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Uncomment after training\n",
    "# eval_results = evaluate_dpo_model(model, tokenizer, ref_model, eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **DPO Loss**: -log(σ(β * (chosen_logratio - rejected_logratio)))\n",
    "2. **Reference Model**: Frozen copy to prevent mode collapse\n",
    "3. **Beta Tuning**: 0.1 is good default, increase for stronger alignment\n",
    "4. **Evaluation**: Track preference accuracy and KL divergence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
