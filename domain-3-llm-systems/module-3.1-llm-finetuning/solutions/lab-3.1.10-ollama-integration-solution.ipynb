{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1.10: Ollama Integration - Solutions\n",
    "\n",
    "Complete solutions for deploying fine-tuned models with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Complete Deployment Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "def complete_deployment_pipeline(\n",
    "    base_model_id: str,\n",
    "    adapter_path: str,\n",
    "    ollama_model_name: str,\n",
    "    system_prompt: str,\n",
    "    quantization: str = \"q4_k_m\",\n",
    "    work_dir: str = \"./deployment\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete pipeline from LoRA adapter to running Ollama model.\n",
    "    \n",
    "    Steps:\n",
    "    1. Merge LoRA adapter into base model\n",
    "    2. Convert to GGUF format\n",
    "    3. Create Modelfile\n",
    "    4. Deploy with Ollama\n",
    "    \"\"\"\n",
    "    work_path = Path(work_dir)\n",
    "    work_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"COMPLETE DEPLOYMENT PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Merge LoRA\n",
    "    print(\"\\n[1/4] Merging LoRA adapter...\")\n",
    "    merged_path = work_path / \"merged\"\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "    \n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    merged_model = model.merge_and_unload()\n",
    "    \n",
    "    merged_model.save_pretrained(str(merged_path))\n",
    "    tokenizer.save_pretrained(str(merged_path))\n",
    "    print(f\"   Merged model saved to: {merged_path}\")\n",
    "    \n",
    "    # Step 2: Convert to GGUF\n",
    "    print(\"\\n[2/4] Converting to GGUF...\")\n",
    "    llama_cpp_path = Path(\"./llama.cpp\")\n",
    "    \n",
    "    if not llama_cpp_path.exists():\n",
    "        subprocess.run([\"git\", \"clone\", \"https://github.com/ggerganov/llama.cpp.git\"], check=True)\n",
    "        subprocess.run([\"make\", \"-j\"], cwd=llama_cpp_path, check=True)\n",
    "    \n",
    "    gguf_path = work_path / \"gguf\"\n",
    "    gguf_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Convert to FP16\n",
    "    fp16_file = gguf_path / \"model-f16.gguf\"\n",
    "    subprocess.run([\n",
    "        \"python\", str(llama_cpp_path / \"convert_hf_to_gguf.py\"),\n",
    "        str(merged_path),\n",
    "        \"--outfile\", str(fp16_file),\n",
    "        \"--outtype\", \"f16\"\n",
    "    ], check=True)\n",
    "    \n",
    "    # Quantize\n",
    "    quantized_file = gguf_path / f\"model-{quantization}.gguf\"\n",
    "    subprocess.run([\n",
    "        str(llama_cpp_path / \"llama-quantize\"),\n",
    "        str(fp16_file),\n",
    "        str(quantized_file),\n",
    "        quantization.upper()\n",
    "    ], check=True)\n",
    "    \n",
    "    fp16_file.unlink()  # Remove FP16 to save space\n",
    "    print(f\"   GGUF saved to: {quantized_file}\")\n",
    "    \n",
    "    # Step 3: Create Modelfile\n",
    "    print(\"\\n[3/4] Creating Modelfile...\")\n",
    "    modelfile_content = f'''FROM {quantized_file}\n",
    "\n",
    "SYSTEM \"\"\"{system_prompt}\"\"\"\n",
    "\n",
    "PARAMETER temperature 0.7\n",
    "PARAMETER top_p 0.9\n",
    "PARAMETER top_k 40\n",
    "PARAMETER num_ctx 4096\n",
    "'''\n",
    "    \n",
    "    modelfile_path = work_path / \"Modelfile\"\n",
    "    modelfile_path.write_text(modelfile_content)\n",
    "    print(f\"   Modelfile saved to: {modelfile_path}\")\n",
    "    \n",
    "    # Step 4: Deploy with Ollama\n",
    "    print(\"\\n[4/4] Creating Ollama model...\")\n",
    "    result = subprocess.run(\n",
    "        [\"ollama\", \"create\", ollama_model_name, \"-f\", str(modelfile_path)],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"   Model created: {ollama_model_name}\")\n",
    "    else:\n",
    "        print(f\"   Error: {result.stderr}\")\n",
    "        return False\n",
    "    \n",
    "    # Cleanup merged model (keep GGUF for backup)\n",
    "    shutil.rmtree(merged_path)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SUCCESS! Run your model with:\")\n",
    "    print(f\"   ollama run {ollama_model_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Example usage:\n",
    "# complete_deployment_pipeline(\n",
    "#     base_model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "#     adapter_path=\"./my-lora-adapter\",\n",
    "#     ollama_model_name=\"my-assistant\",\n",
    "#     system_prompt=\"You are a helpful AI assistant.\",\n",
    "#     quantization=\"q4_k_m\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Custom Modelfile Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "@dataclass\n",
    "class ModelfileBuilder:\n",
    "    \"\"\"\n",
    "    Build Ollama Modelfiles with proper configuration.\n",
    "    \"\"\"\n",
    "    base_model: str  # Path to GGUF or Ollama model name\n",
    "    system_prompt: str = \"You are a helpful AI assistant.\"\n",
    "    template: Optional[str] = None\n",
    "    parameters: Dict[str, Any] = field(default_factory=dict)\n",
    "    license_text: Optional[str] = None\n",
    "    \n",
    "    # Common templates\n",
    "    TEMPLATES = {\n",
    "        \"llama3\": \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{{ .System }}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{{ .Prompt }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{{ .Response }}<|eot_id|>\"\"\",\n",
    "        \"chatml\": \"\"\"<|im_start|>system\n",
    "{{ .System }}<|im_end|>\n",
    "<|im_start|>user\n",
    "{{ .Prompt }}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{{ .Response }}<|im_end|>\"\"\",\n",
    "        \"alpaca\": \"\"\"### Instruction:\n",
    "{{ .System }}\n",
    "\n",
    "{{ .Prompt }}\n",
    "\n",
    "### Response:\n",
    "{{ .Response }}\"\"\"\n",
    "    }\n",
    "    \n",
    "    # Default parameters by use case\n",
    "    PRESETS = {\n",
    "        \"creative\": {\n",
    "            \"temperature\": 0.9,\n",
    "            \"top_p\": 0.95,\n",
    "            \"top_k\": 50,\n",
    "            \"num_ctx\": 4096\n",
    "        },\n",
    "        \"precise\": {\n",
    "            \"temperature\": 0.3,\n",
    "            \"top_p\": 0.7,\n",
    "            \"top_k\": 20,\n",
    "            \"num_ctx\": 4096\n",
    "        },\n",
    "        \"balanced\": {\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 40,\n",
    "            \"num_ctx\": 4096\n",
    "        },\n",
    "        \"code\": {\n",
    "            \"temperature\": 0.2,\n",
    "            \"top_p\": 0.6,\n",
    "            \"top_k\": 10,\n",
    "            \"num_ctx\": 8192,\n",
    "            \"stop\": [\"```\\n\", \"</code>\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def use_template(self, name: str) -> 'ModelfileBuilder':\n",
    "        \"\"\"Use a predefined template.\"\"\"\n",
    "        if name in self.TEMPLATES:\n",
    "            self.template = self.TEMPLATES[name]\n",
    "        return self\n",
    "    \n",
    "    def use_preset(self, name: str) -> 'ModelfileBuilder':\n",
    "        \"\"\"Use a parameter preset.\"\"\"\n",
    "        if name in self.PRESETS:\n",
    "            self.parameters.update(self.PRESETS[name])\n",
    "        return self\n",
    "    \n",
    "    def set_parameter(self, key: str, value: Any) -> 'ModelfileBuilder':\n",
    "        \"\"\"Set a single parameter.\"\"\"\n",
    "        self.parameters[key] = value\n",
    "        return self\n",
    "    \n",
    "    def build(self) -> str:\n",
    "        \"\"\"Generate the Modelfile content.\"\"\"\n",
    "        lines = [f\"FROM {self.base_model}\", \"\"]\n",
    "        \n",
    "        # System prompt\n",
    "        lines.append(f'SYSTEM \"\"\"{self.system_prompt}\"\"\"')\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # Template\n",
    "        if self.template:\n",
    "            lines.append(f'TEMPLATE \"\"\"{self.template}\"\"\"')\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        # Parameters\n",
    "        for key, value in self.parameters.items():\n",
    "            if isinstance(value, list):\n",
    "                for v in value:\n",
    "                    lines.append(f'PARAMETER {key} \"{v}\"')\n",
    "            else:\n",
    "                lines.append(f\"PARAMETER {key} {value}\")\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # License\n",
    "        if self.license_text:\n",
    "            lines.append(f'LICENSE \"\"\"{self.license_text}\"\"\"')\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save Modelfile to disk.\"\"\"\n",
    "        Path(path).write_text(self.build())\n",
    "        print(f\"Saved Modelfile to: {path}\")\n",
    "\n",
    "# Examples\n",
    "print(\"Code Assistant Modelfile:\")\n",
    "print(\"-\" * 40)\n",
    "modelfile = ModelfileBuilder(\n",
    "    base_model=\"./model-q4_k_m.gguf\",\n",
    "    system_prompt=\"You are an expert Python programmer. Write clean, efficient code with comments.\"\n",
    ").use_template(\"llama3\").use_preset(\"code\").build()\n",
    "print(modelfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Ollama API Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from typing import Iterator, List, Dict, Optional\n",
    "\n",
    "class OllamaClient:\n",
    "    \"\"\"\n",
    "    Full-featured Ollama API client.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, host: str = \"http://localhost:11434\"):\n",
    "        self.host = host\n",
    "        self.api_url = f\"{host}/api\"\n",
    "    \n",
    "    def _check_running(self) -> bool:\n",
    "        try:\n",
    "            requests.get(f\"{self.host}/api/tags\", timeout=2)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def list_models(self) -> List[Dict]:\n",
    "        \"\"\"List all available models.\"\"\"\n",
    "        response = requests.get(f\"{self.api_url}/tags\")\n",
    "        return response.json().get(\"models\", [])\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        model: str,\n",
    "        prompt: str,\n",
    "        system: Optional[str] = None,\n",
    "        **kwargs\n",
    "    ) -> str:\n",
    "        \"\"\"Generate a completion.\"\"\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            **kwargs\n",
    "        }\n",
    "        if system:\n",
    "            payload[\"system\"] = system\n",
    "        \n",
    "        response = requests.post(f\"{self.api_url}/generate\", json=payload)\n",
    "        return response.json()[\"response\"]\n",
    "    \n",
    "    def generate_stream(\n",
    "        self,\n",
    "        model: str,\n",
    "        prompt: str,\n",
    "        **kwargs\n",
    "    ) -> Iterator[str]:\n",
    "        \"\"\"Stream a completion token by token.\"\"\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": True,\n",
    "            **kwargs\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{self.api_url}/generate\",\n",
    "            json=payload,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                chunk = json.loads(line)\n",
    "                yield chunk.get(\"response\", \"\")\n",
    "    \n",
    "    def chat(\n",
    "        self,\n",
    "        model: str,\n",
    "        messages: List[Dict[str, str]],\n",
    "        **kwargs\n",
    "    ) -> str:\n",
    "        \"\"\"Chat completion (OpenAI-compatible format).\"\"\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False,\n",
    "            **kwargs\n",
    "        }\n",
    "        \n",
    "        response = requests.post(f\"{self.api_url}/chat\", json=payload)\n",
    "        return response.json()[\"message\"][\"content\"]\n",
    "    \n",
    "    def embeddings(\n",
    "        self,\n",
    "        model: str,\n",
    "        text: str\n",
    "    ) -> List[float]:\n",
    "        \"\"\"Get embeddings for text.\"\"\"\n",
    "        response = requests.post(\n",
    "            f\"{self.api_url}/embeddings\",\n",
    "            json={\"model\": model, \"prompt\": text}\n",
    "        )\n",
    "        return response.json()[\"embedding\"]\n",
    "    \n",
    "    def create_model(\n",
    "        self,\n",
    "        name: str,\n",
    "        modelfile: str\n",
    "    ) -> bool:\n",
    "        \"\"\"Create a model from Modelfile content.\"\"\"\n",
    "        response = requests.post(\n",
    "            f\"{self.api_url}/create\",\n",
    "            json={\"name\": name, \"modelfile\": modelfile},\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                status = json.loads(line)\n",
    "                print(f\"   {status.get('status', '')}\")\n",
    "        \n",
    "        return response.status_code == 200\n",
    "    \n",
    "    def delete_model(self, name: str) -> bool:\n",
    "        \"\"\"Delete a model.\"\"\"\n",
    "        response = requests.delete(\n",
    "            f\"{self.api_url}/delete\",\n",
    "            json={\"name\": name}\n",
    "        )\n",
    "        return response.status_code == 200\n",
    "\n",
    "# Demo\n",
    "client = OllamaClient()\n",
    "\n",
    "if client._check_running():\n",
    "    models = client.list_models()\n",
    "    print(f\"Available models: {len(models)}\")\n",
    "    for m in models[:5]:\n",
    "        print(f\"  - {m['name']}\")\n",
    "else:\n",
    "    print(\"Ollama not running. Start with: ollama serve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: OpenAI Drop-in Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def demonstrate_openai_compatibility():\n",
    "    \"\"\"\n",
    "    Show how Ollama can replace OpenAI with one line change.\n",
    "    \"\"\"\n",
    "    print(\"OpenAI API Compatibility Demo\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Original OpenAI code:\n",
    "    # client = OpenAI(api_key=\"sk-...\")\n",
    "    \n",
    "    # With Ollama - just change base_url!\n",
    "    client = OpenAI(\n",
    "        base_url=\"http://localhost:11434/v1\",\n",
    "        api_key=\"ollama\"  # Ollama ignores this, but OpenAI client requires it\n",
    "    )\n",
    "    \n",
    "    # Now use exactly like OpenAI!\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama3.2:1b\",  # or your custom model\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nModel: {response.model}\")\n",
    "        print(f\"Response: {response.choices[0].message.content}\")\n",
    "        print(f\"Tokens: {response.usage}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"\\nMake sure Ollama is running and has a model loaded.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Key advantage: Your existing OpenAI code works with Ollama!\")\n",
    "\n",
    "# Run demo\n",
    "# demonstrate_openai_compatibility()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Simple Chat Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleChat:\n",
    "    \"\"\"\n",
    "    Simple chat application using Ollama.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        system_prompt: str = \"You are a helpful AI assistant.\",\n",
    "        host: str = \"http://localhost:11434\"\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.system_prompt = system_prompt\n",
    "        self.host = host\n",
    "        self.history = []\n",
    "    \n",
    "    def chat(self, user_message: str) -> str:\n",
    "        \"\"\"Send a message and get a response.\"\"\"\n",
    "        # Add to history\n",
    "        self.history.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        # Build messages with system prompt\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt}\n",
    "        ] + self.history\n",
    "        \n",
    "        # Call API\n",
    "        response = requests.post(\n",
    "            f\"{self.host}/api/chat\",\n",
    "            json={\n",
    "                \"model\": self.model,\n",
    "                \"messages\": messages,\n",
    "                \"stream\": False\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        assistant_message = response.json()[\"message\"][\"content\"]\n",
    "        \n",
    "        # Add to history\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "        \n",
    "        return assistant_message\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history.\"\"\"\n",
    "        self.history = []\n",
    "        print(\"History cleared.\")\n",
    "    \n",
    "    def run_interactive(self):\n",
    "        \"\"\"Run interactive chat session.\"\"\"\n",
    "        print(f\"Chat with {self.model}\")\n",
    "        print(\"Type 'quit' to exit, 'clear' to reset history\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"You: \").strip()\n",
    "                \n",
    "                if user_input.lower() == 'quit':\n",
    "                    print(\"Goodbye!\")\n",
    "                    break\n",
    "                elif user_input.lower() == 'clear':\n",
    "                    self.clear_history()\n",
    "                    continue\n",
    "                elif not user_input:\n",
    "                    continue\n",
    "                \n",
    "                response = self.chat(user_input)\n",
    "                print(f\"AI: {response}\\n\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nGoodbye!\")\n",
    "                break\n",
    "\n",
    "# Usage:\n",
    "# chat = SimpleChat(\"my-assistant\", \"You are a coding expert.\")\n",
    "# chat.run_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Deployment Pipeline**: Merge → GGUF → Modelfile → Ollama\n",
    "2. **Modelfile**: Configure system prompt, template, and parameters\n",
    "3. **API Client**: REST API for generate, chat, and embeddings\n",
    "4. **OpenAI Compatible**: Change `base_url` to use Ollama\n",
    "5. **Chat Apps**: Build interactive apps with conversation history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
