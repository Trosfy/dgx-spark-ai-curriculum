{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1.6 Solution: LLaMA Factory Exploration\n",
    "\n",
    "**Module:** 3.1 - Large Language Model Fine-Tuning  \n",
    "**Type:** Solution Notebook\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains solutions to the exercises from the LLaMA Factory exploration task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Setup and Explore\n",
    "\n",
    "**Task:** Install LLaMA Factory, launch the web UI, and document 5 interesting features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 Solution: Document your findings\n",
    "# Here are 5 interesting features of LLaMA Factory:\n",
    "\n",
    "llama_factory_features = [\n",
    "    {\n",
    "        \"feature\": \"Multi-Method Training Support\",\n",
    "        \"description\": \"LLaMA Factory supports SFT, DPO, RLHF, PPO, and full fine-tuning all from the same interface.\",\n",
    "        \"benefit\": \"No need to switch tools when trying different training methods.\"\n",
    "    },\n",
    "    {\n",
    "        \"feature\": \"Real-Time Training Monitoring\",\n",
    "        \"description\": \"The UI shows live loss curves, GPU memory usage, and training progress.\",\n",
    "        \"benefit\": \"Easy to catch problems early without checking terminal logs.\"\n",
    "    },\n",
    "    {\n",
    "        \"feature\": \"Built-in Chat Testing\",\n",
    "        \"description\": \"Test trained models directly in the Chat tab without any additional setup.\",\n",
    "        \"benefit\": \"Immediate feedback on model quality after training.\"\n",
    "    },\n",
    "    {\n",
    "        \"feature\": \"One-Click GGUF Export\",\n",
    "        \"description\": \"Export merged LoRA models directly to GGUF format for Ollama.\",\n",
    "        \"benefit\": \"Streamlines the deployment pipeline significantly.\"\n",
    "    },\n",
    "    {\n",
    "        \"feature\": \"Dataset Preview\",\n",
    "        \"description\": \"Preview dataset samples before training to verify formatting.\",\n",
    "        \"benefit\": \"Catches dataset issues before wasting time on training.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Print features\n",
    "print(\"5 Interesting LLaMA Factory Features\")\n",
    "print(\"=\" * 50)\n",
    "for i, feature in enumerate(llama_factory_features, 1):\n",
    "    print(f\"\\n{i}. {feature['feature']}\")\n",
    "    print(f\"   Description: {feature['description']}\")\n",
    "    print(f\"   Benefit: {feature['benefit']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Train with GUI\n",
    "\n",
    "**Task:** Create a custom dataset (at least 20 examples), upload it, configure LoRA training, monitor training, and test the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Exercise 2 Solution: Create a custom dataset with 25 examples\n",
    "# Theme: AI and Machine Learning concepts explained simply\n",
    "\n",
    "custom_dataset = [\n",
    "    # Basic concepts\n",
    "    {\n",
    "        \"instruction\": \"Explain what artificial intelligence is.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Artificial intelligence (AI) is the simulation of human intelligence by machines. It involves creating computer systems that can perform tasks typically requiring human intelligence, such as visual perception, speech recognition, decision-making, and language understanding.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is the difference between AI and machine learning?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"AI is the broader concept of machines being able to carry out tasks in a smart way. Machine learning is a subset of AI that focuses on giving machines the ability to learn from data without being explicitly programmed. Think of AI as the goal and ML as one way to achieve it.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain deep learning in simple terms.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Deep learning is a type of machine learning that uses neural networks with many layers (hence 'deep'). These networks learn to recognize patterns in data, similar to how our brains work. It's especially good at tasks like image recognition and natural language processing.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is a neural network?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"A neural network is a computing system inspired by the human brain. It consists of interconnected nodes (neurons) organized in layers. Data flows through these layers, with each layer learning to recognize increasingly complex patterns. The network learns by adjusting the connections between neurons based on examples.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Define supervised learning.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Supervised learning is when you train a model using labeled data - meaning each example has both the input and the correct answer. The model learns to predict the right answer by comparing its predictions to the known correct answers and adjusting accordingly.\"\n",
    "    },\n",
    "    # Training concepts\n",
    "    {\n",
    "        \"instruction\": \"What is a loss function?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"A loss function measures how wrong a model's predictions are compared to the actual values. It produces a single number that represents the error. The goal of training is to minimize this loss. Common examples include Mean Squared Error for regression and Cross-Entropy for classification.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain what backpropagation does.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Backpropagation is how neural networks learn. After making a prediction and calculating the error (loss), backpropagation figures out how much each weight in the network contributed to that error. It then adjusts the weights to reduce the error. This process repeats thousands of times during training.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is gradient descent?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Gradient descent is an optimization algorithm used to minimize the loss function. Imagine you're blindfolded on a hill and want to reach the lowest point - you'd feel which direction goes downward and take a step that way. Gradient descent does this mathematically, repeatedly moving in the direction that reduces the loss.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is the learning rate?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"The learning rate controls how big the steps are during gradient descent. Too large and you might overshoot the optimal solution. Too small and training takes forever. It's typically a small number like 0.001 or 0.0001 and is one of the most important hyperparameters to tune.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Define overfitting.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Overfitting happens when a model learns the training data too well, including its noise and peculiarities, rather than the underlying pattern. The model performs great on training data but poorly on new data. It's like memorizing test answers instead of understanding the subject.\"\n",
    "    },\n",
    "    # LLM concepts\n",
    "    {\n",
    "        \"instruction\": \"What is a large language model (LLM)?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"A large language model is a type of AI trained on vast amounts of text data. It learns patterns in language and can generate human-like text, answer questions, translate languages, and perform various language tasks. Examples include GPT-4, Claude, and LLaMA.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain what fine-tuning means.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Fine-tuning is the process of taking a pre-trained model and training it further on a specific dataset or task. Instead of training from scratch, you start with a model that already understands language and adapt it to your needs. This is faster and requires less data than training from scratch.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is LoRA and why is it useful?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"LoRA (Low-Rank Adaptation) is a technique for efficiently fine-tuning large models. Instead of updating all the model's parameters, LoRA adds small trainable adapters. This reduces memory requirements by 10-100x and training time significantly, making it possible to fine-tune large models on consumer hardware.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain QLoRA.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"QLoRA combines quantization with LoRA. The base model is quantized to 4-bit precision (reducing memory by 4x), while the LoRA adapters train in higher precision. This allows fine-tuning of 70B+ parameter models on a single GPU while maintaining quality close to full fine-tuning.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is a transformer architecture?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"The transformer is the architecture behind modern LLMs. Its key innovation is the attention mechanism, which allows the model to focus on relevant parts of the input when generating each output. Unlike previous models, transformers process all tokens in parallel, making them faster to train.\"\n",
    "    },\n",
    "    # Practical concepts\n",
    "    {\n",
    "        \"instruction\": \"What is tokenization?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Tokenization is breaking text into smaller pieces called tokens that a model can process. These could be words, subwords, or characters. For example, 'unhappiness' might become ['un', 'happiness'] or ['un', 'happ', 'iness']. The tokenizer maps these tokens to numbers the model can work with.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain what an embedding is.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"An embedding is a way to represent words or tokens as lists of numbers (vectors). Similar words have similar vectors. For example, 'king' and 'queen' would have similar embeddings. These vector representations allow models to understand relationships between words mathematically.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is the attention mechanism?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"The attention mechanism lets a model focus on different parts of the input when producing output. When translating 'the cat sat on the mat', the model pays attention to different input words when generating each output word. This allows it to handle long-range dependencies better than previous methods.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is inference in machine learning?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Inference is using a trained model to make predictions on new data. During training, the model learns patterns. During inference, it applies what it learned to generate outputs. For LLMs, inference means generating text in response to prompts.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Define batch size.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Batch size is the number of training examples processed together before updating model weights. Larger batches are faster but need more memory and can lead to poorer generalization. Smaller batches are slower but often train better. A common range is 8-64 examples per batch.\"\n",
    "    },\n",
    "    # DGX Spark specific\n",
    "    {\n",
    "        \"instruction\": \"What is unified memory in the context of DGX Spark?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"DGX Spark has 128GB of unified memory shared between the CPU and GPU. This means the GPU can access system memory when needed, allowing you to work with larger models than the GPU memory alone would allow. It's particularly useful for fine-tuning large language models.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Why should you use NGC containers on DGX Spark?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"DGX Spark uses ARM64 architecture, which is different from standard x86 computers. Most pip packages like PyTorch are built for x86. NGC containers from NVIDIA contain ARM64-optimized versions of AI frameworks, ensuring everything works correctly with the DGX Spark's hardware.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is bfloat16 and why use it?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"bfloat16 is a 16-bit floating point format optimized for deep learning. It has the same range as float32 but less precision. Modern GPUs like the Blackwell in DGX Spark have dedicated bfloat16 hardware, making training 2x faster than float32 while maintaining quality.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is model quantization?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Quantization reduces the precision of model weights from float32 (32 bits) to lower precision like int8 (8 bits) or int4 (4 bits). This reduces memory usage and increases inference speed with minimal quality loss. A 7B model that needs 14GB in float16 might only need 4GB in 4-bit.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain DPO (Direct Preference Optimization).\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"DPO is a training technique that teaches models to prefer certain responses over others without needing a separate reward model. You provide pairs of responses - one preferred, one rejected - and the model learns to generate more of the preferred style. It's simpler than RLHF but often equally effective.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Created dataset with {len(custom_dataset)} examples\")\n",
    "print(\"\\nSample entry:\")\n",
    "print(json.dumps(custom_dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset for LLaMA Factory\n",
    "output_dir = Path(\"../data\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "dataset_path = output_dir / \"ai_concepts_dataset.json\"\n",
    "with open(dataset_path, 'w') as f:\n",
    "    json.dump(custom_dataset, f, indent=2)\n",
    "\n",
    "print(f\"Dataset saved to: {dataset_path}\")\n",
    "\n",
    "# Create the dataset_info.json entry\n",
    "dataset_info_entry = {\n",
    "    \"ai_concepts\": {\n",
    "        \"file_name\": \"ai_concepts_dataset.json\",\n",
    "        \"columns\": {\n",
    "            \"prompt\": \"instruction\",\n",
    "            \"query\": \"input\",\n",
    "            \"response\": \"output\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nAdd this to LLaMA Factory's data/dataset_info.json:\")\n",
    "print(json.dumps(dataset_info_entry, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configuration Used\n",
    "\n",
    "Here's the configuration that works well for this dataset on DGX Spark:\n",
    "\n",
    "| Setting | Value | Reason |\n",
    "|---------|-------|--------|\n",
    "| Model | Llama-3.1-8B-Instruct | Good balance of quality and speed |\n",
    "| Finetuning Type | LoRA | Efficient, preserves base knowledge |\n",
    "| Quantization | 4-bit | Reduces memory for faster iteration |\n",
    "| LoRA Rank | 16 | Good for small dataset |\n",
    "| LoRA Alpha | 32 | Standard 2x rank |\n",
    "| Learning Rate | 2e-4 | Standard for LoRA |\n",
    "| Epochs | 3 | Small dataset, don't overtrain |\n",
    "| Batch Size | 4 | DGX Spark can handle more |\n",
    "| Max Length | 512 | Our responses are short |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration as YAML (for CLI usage)\n",
    "training_config = \"\"\"\n",
    "# ai_concepts_training.yaml\n",
    "# Training config for AI concepts dataset\n",
    "\n",
    "# Model\n",
    "model_name_or_path: meta-llama/Llama-3.1-8B-Instruct\n",
    "trust_remote_code: true\n",
    "\n",
    "# Training method\n",
    "stage: sft\n",
    "finetuning_type: lora\n",
    "\n",
    "# Quantization\n",
    "quantization_bit: 4\n",
    "\n",
    "# Dataset\n",
    "dataset: ai_concepts\n",
    "template: llama3\n",
    "cutoff_len: 512\n",
    "\n",
    "# LoRA settings\n",
    "lora_rank: 16\n",
    "lora_alpha: 32\n",
    "lora_dropout: 0.05\n",
    "lora_target: q_proj,k_proj,v_proj,o_proj\n",
    "\n",
    "# Training\n",
    "output_dir: ./output/ai-concepts-lora\n",
    "per_device_train_batch_size: 4\n",
    "gradient_accumulation_steps: 4\n",
    "learning_rate: 2e-4\n",
    "num_train_epochs: 3\n",
    "lr_scheduler_type: cosine\n",
    "warmup_ratio: 0.1\n",
    "\n",
    "# Optimization\n",
    "bf16: true\n",
    "gradient_checkpointing: true\n",
    "\n",
    "# Logging\n",
    "logging_steps: 5\n",
    "save_steps: 50\n",
    "\"\"\"\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(training_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Results\n",
    "\n",
    "After training, test the model in the Chat tab with these prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts and expected behavior\n",
    "test_cases = [\n",
    "    {\n",
    "        \"prompt\": \"What is machine learning?\",\n",
    "        \"expected_behavior\": \"Should give a clear, simple explanation similar to training data style\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Explain reinforcement learning.\",\n",
    "        \"expected_behavior\": \"Should generalize from training examples to explain a related but not-trained concept\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What are the benefits of using LoRA for fine-tuning?\",\n",
    "        \"expected_behavior\": \"Should combine knowledge from LoRA and QLoRA examples\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Why would I use DGX Spark for AI work?\",\n",
    "        \"expected_behavior\": \"Should reference unified memory and NGC containers from training\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Test Cases for Evaluating Fine-Tuned Model\")\n",
    "print(\"=\" * 50)\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(f\"  Prompt: {test['prompt']}\")\n",
    "    print(f\"  Expected: {test['expected_behavior']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this solution, we:\n",
    "\n",
    "1. **Explored LLaMA Factory** and documented 5 key features:\n",
    "   - Multi-method training support\n",
    "   - Real-time training monitoring\n",
    "   - Built-in chat testing\n",
    "   - One-click GGUF export\n",
    "   - Dataset preview\n",
    "\n",
    "2. **Created a custom dataset** with 25 examples covering:\n",
    "   - Basic AI/ML concepts\n",
    "   - Training concepts\n",
    "   - LLM concepts\n",
    "   - DGX Spark specifics\n",
    "\n",
    "3. **Configured training** with appropriate settings for DGX Spark\n",
    "\n",
    "4. **Defined test cases** for evaluating the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"Solution notebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}