{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 3.1.1: LoRA Theory - Solutions\n\n**Module:** 3.1 - Large Language Model Fine-Tuning  \n**Difficulty:** ⭐⭐⭐☆☆ (Intermediate)  \n**Exercises:** 3 (LoRA with Bias, Target Module Comparison, Memory Analysis)\n\nThis notebook contains solutions to the exercises in the LoRA Theory notebook.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Implement LoRA with Bias\n",
    "\n",
    "Modify the `LoRALayer` class to optionally train the bias term as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class LoRALayerWithBias(nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA layer that optionally trains bias.\n",
    "    \n",
    "    Extends the standard LoRA to allow training the bias term,\n",
    "    which can improve performance for some tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        original_layer: nn.Linear,\n",
    "        rank: int = 8,\n",
    "        alpha: float = 16.0,\n",
    "        dropout: float = 0.0,\n",
    "        train_bias: bool = False,  # NEW: option to train bias\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.original_layer = original_layer\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        self.train_bias = train_bias\n",
    "        \n",
    "        in_features = original_layer.in_features\n",
    "        out_features = original_layer.out_features\n",
    "        \n",
    "        # Freeze original weights\n",
    "        self.original_layer.weight.requires_grad = False\n",
    "        \n",
    "        # Handle bias - optionally train it\n",
    "        if self.original_layer.bias is not None:\n",
    "            if train_bias:\n",
    "                self.original_layer.bias.requires_grad = True  # Train bias!\n",
    "            else:\n",
    "                self.original_layer.bias.requires_grad = False\n",
    "        \n",
    "        # LoRA matrices\n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        \n",
    "        # Initialize A with Kaiming uniform\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=np.sqrt(5))\n",
    "        \n",
    "        # Optional dropout\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Original path\n",
    "        result = self.original_layer(x)\n",
    "        \n",
    "        # LoRA path\n",
    "        lora_x = self.dropout(x)\n",
    "        lora_output = lora_x @ self.lora_A.T @ self.lora_B.T\n",
    "        \n",
    "        return result + self.scaling * lora_output\n",
    "    \n",
    "    @property\n",
    "    def trainable_params(self) -> int:\n",
    "        params = self.lora_A.numel() + self.lora_B.numel()\n",
    "        if self.train_bias and self.original_layer.bias is not None:\n",
    "            params += self.original_layer.bias.numel()\n",
    "        return params\n",
    "\n",
    "# Test the implementation\n",
    "linear = nn.Linear(512, 512, bias=True)\n",
    "\n",
    "# Without training bias\n",
    "lora_no_bias = LoRALayerWithBias(linear, rank=16, train_bias=False)\n",
    "print(f\"Without bias training: {lora_no_bias.trainable_params:,} trainable params\")\n",
    "\n",
    "# With training bias\n",
    "linear2 = nn.Linear(512, 512, bias=True)\n",
    "lora_with_bias = LoRALayerWithBias(linear2, rank=16, train_bias=True)\n",
    "print(f\"With bias training: {lora_with_bias.trainable_params:,} trainable params\")\n",
    "\n",
    "# Verify bias is trainable\n",
    "print(f\"\\nBias requires_grad: {lora_with_bias.original_layer.bias.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Compare Different Target Modules\n",
    "\n",
    "Compare training with LoRA on different sets of target modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"Standard LoRA layer for comparison.\"\"\"\n",
    "    def __init__(self, original_layer, rank=8, alpha=16.0):\n",
    "        super().__init__()\n",
    "        self.original_layer = original_layer\n",
    "        self.rank = rank\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        in_f = original_layer.in_features\n",
    "        out_f = original_layer.out_features\n",
    "        \n",
    "        original_layer.weight.requires_grad = False\n",
    "        if original_layer.bias is not None:\n",
    "            original_layer.bias.requires_grad = False\n",
    "        \n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_f))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_f, rank))\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=np.sqrt(5))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        result = self.original_layer(x)\n",
    "        lora_out = x @ self.lora_A.T @ self.lora_B.T\n",
    "        return result + self.scaling * lora_out\n",
    "\n",
    "\n",
    "class SimpleTransformerBlock(nn.Module):\n",
    "    \"\"\"Simple transformer block for testing.\"\"\"\n",
    "    def __init__(self, d_model=256, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        # Attention projections\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.o_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        # MLP\n",
    "        self.gate_proj = nn.Linear(d_model, d_model * 4, bias=False)\n",
    "        self.up_proj = nn.Linear(d_model, d_model * 4, bias=False)\n",
    "        self.down_proj = nn.Linear(d_model * 4, d_model, bias=False)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Attention\n",
    "        normed = self.norm1(x)\n",
    "        q = self.q_proj(normed).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(normed).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(normed).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        attn = F.scaled_dot_product_attention(q, k, v)\n",
    "        attn = attn.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        x = x + self.o_proj(attn)\n",
    "        \n",
    "        # MLP (SwiGLU style)\n",
    "        normed = self.norm2(x)\n",
    "        x = x + self.down_proj(F.silu(self.gate_proj(normed)) * self.up_proj(normed))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def add_lora_to_model(model, rank=8, alpha=16, target_modules=None):\n",
    "    \"\"\"Add LoRA to specific modules.\"\"\"\n",
    "    if target_modules is None:\n",
    "        target_modules = []\n",
    "    \n",
    "    for name, module in list(model.named_modules()):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if target_modules and not any(t in name for t in target_modules):\n",
    "                continue\n",
    "            \n",
    "            parts = name.split('.')\n",
    "            parent = model\n",
    "            for part in parts[:-1]:\n",
    "                parent = getattr(parent, part)\n",
    "            \n",
    "            setattr(parent, parts[-1], LoRALayer(module, rank, alpha))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_evaluate(target_modules, n_epochs=50):\n",
    "    \"\"\"Train model with specific target modules and return final loss.\"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Create model\n",
    "    model = SimpleTransformerBlock().to(device)\n",
    "    model = add_lora_to_model(model, rank=8, target_modules=target_modules)\n",
    "    \n",
    "    # Count params\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    # Create data\n",
    "    x_train = torch.randn(100, 32, 256, device=device)\n",
    "    y_train = torch.randn(100, 32, 256, device=device)\n",
    "    \n",
    "    # Train\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad], \n",
    "        lr=1e-3\n",
    "    )\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for i in range(0, 100, 10):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_train[i:i+10])\n",
    "            loss = F.mse_loss(output, y_train[i:i+10])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return trainable, losses[-1]\n",
    "\n",
    "\n",
    "# Compare different target module configurations\n",
    "configs = [\n",
    "    (\"Q and V only\", [\"q_proj\", \"v_proj\"]),\n",
    "    (\"All attention\", [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]),\n",
    "    (\"Attention + MLP\", [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]),\n",
    "]\n",
    "\n",
    "print(\"Comparing different target module configurations:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, targets in configs:\n",
    "    params, loss = train_and_evaluate(targets)\n",
    "    print(f\"{name:20s}: {params:,} params, Final Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Memory Analysis\n",
    "\n",
    "Calculate and visualize memory savings of LoRA for different model sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_memory_requirements(\n",
    "    model_params_billions: float,\n",
    "    lora_rank: int = 16,\n",
    "    dtype_bytes: int = 2,  # float16\n",
    "    lora_target_ratio: float = 0.3,  # ~30% of params are in target modules\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Calculate memory requirements for full fine-tuning vs LoRA.\n",
    "    \n",
    "    Memory components:\n",
    "    - Model weights\n",
    "    - Gradients (same size as trainable params)\n",
    "    - Optimizer states (Adam: 2x trainable params for momentum + variance)\n",
    "    - Activations (varies, but typically 2-4x model size during training)\n",
    "    \n",
    "    Args:\n",
    "        model_params_billions: Number of parameters in billions\n",
    "        lora_rank: LoRA rank\n",
    "        dtype_bytes: Bytes per parameter (2 for float16, 4 for float32)\n",
    "        lora_target_ratio: Fraction of parameters in LoRA target modules\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with memory estimates\n",
    "    \"\"\"\n",
    "    total_params = model_params_billions * 1e9\n",
    "    \n",
    "    # Estimate dimensions (rough approximation)\n",
    "    # Assuming d_model proportional to sqrt(params)\n",
    "    d_model = int((total_params / 100) ** 0.5)  # Rough estimate\n",
    "    \n",
    "    # LoRA parameters per adapted layer\n",
    "    # Each layer adds: d_model * rank + rank * d_model = 2 * d_model * rank\n",
    "    lora_params_per_layer = 2 * d_model * lora_rank\n",
    "    \n",
    "    # Number of adapted layers (estimate)\n",
    "    n_layers = int(total_params / (12 * d_model * d_model))  # Rough transformer formula\n",
    "    n_adapted = int(n_layers * 4 * lora_target_ratio)  # ~4 projections per layer * target ratio\n",
    "    \n",
    "    total_lora_params = n_adapted * lora_params_per_layer\n",
    "    \n",
    "    # Full fine-tuning memory\n",
    "    full_weights = total_params * dtype_bytes / 1e9  # GB\n",
    "    full_gradients = total_params * dtype_bytes / 1e9\n",
    "    full_optimizer = total_params * 2 * dtype_bytes / 1e9  # Adam states\n",
    "    full_activations = full_weights * 2  # Rough estimate\n",
    "    full_total = full_weights + full_gradients + full_optimizer + full_activations\n",
    "    \n",
    "    # LoRA memory\n",
    "    lora_weights = total_params * dtype_bytes / 1e9  # Still need base model\n",
    "    lora_trainable = total_lora_params * dtype_bytes / 1e9\n",
    "    lora_gradients = total_lora_params * dtype_bytes / 1e9  # Only for LoRA params\n",
    "    lora_optimizer = total_lora_params * 2 * dtype_bytes / 1e9\n",
    "    lora_activations = lora_weights * 0.5  # Less activation memory with gradient checkpointing\n",
    "    lora_total = lora_weights + lora_trainable + lora_gradients + lora_optimizer + lora_activations\n",
    "    \n",
    "    return {\n",
    "        'model_params_B': model_params_billions,\n",
    "        'lora_rank': lora_rank,\n",
    "        'lora_params': total_lora_params,\n",
    "        'lora_params_ratio': total_lora_params / total_params,\n",
    "        'full_finetuning_GB': full_total,\n",
    "        'lora_finetuning_GB': lora_total,\n",
    "        'memory_savings': 1 - (lora_total / full_total),\n",
    "        'breakdown_full': {\n",
    "            'weights': full_weights,\n",
    "            'gradients': full_gradients,\n",
    "            'optimizer': full_optimizer,\n",
    "            'activations': full_activations,\n",
    "        },\n",
    "        'breakdown_lora': {\n",
    "            'weights': lora_weights,\n",
    "            'lora_params': lora_trainable,\n",
    "            'gradients': lora_gradients,\n",
    "            'optimizer': lora_optimizer,\n",
    "            'activations': lora_activations,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "# Analyze different model sizes\n",
    "model_sizes = [1, 3, 7, 13, 30, 70, 120]\n",
    "results = []\n",
    "\n",
    "print(\"Memory Requirements Analysis\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Model Size':>12} | {'Full FT (GB)':>12} | {'LoRA (GB)':>12} | {'Savings':>10} | {'LoRA Params':>15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for size in model_sizes:\n",
    "    result = calculate_memory_requirements(size, lora_rank=16)\n",
    "    results.append(result)\n",
    "    print(f\"{size:>10}B | {result['full_finetuning_GB']:>10.1f}GB | {result['lora_finetuning_GB']:>10.1f}GB | {result['memory_savings']:>9.1%} | {result['lora_params']/1e6:>12.1f}M\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Memory comparison\n",
    "full_mem = [r['full_finetuning_GB'] for r in results]\n",
    "lora_mem = [r['lora_finetuning_GB'] for r in results]\n",
    "\n",
    "x = range(len(model_sizes))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar([i - width/2 for i in x], full_mem, width, label='Full Fine-tuning', color='coral')\n",
    "axes[0].bar([i + width/2 for i in x], lora_mem, width, label='LoRA', color='steelblue')\n",
    "axes[0].axhline(y=128, color='green', linestyle='--', label='DGX Spark (128GB)')\n",
    "axes[0].axhline(y=24, color='red', linestyle='--', label='RTX 4090 (24GB)')\n",
    "axes[0].set_xlabel('Model Size')\n",
    "axes[0].set_ylabel('Memory (GB)')\n",
    "axes[0].set_title('Memory Requirements')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels([f'{s}B' for s in model_sizes])\n",
    "axes[0].legend()\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Savings\n",
    "savings = [r['memory_savings'] * 100 for r in results]\n",
    "axes[1].bar(x, savings, color='seagreen')\n",
    "axes[1].set_xlabel('Model Size')\n",
    "axes[1].set_ylabel('Memory Savings (%)')\n",
    "axes[1].set_title('LoRA Memory Savings')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels([f'{s}B' for s in model_sizes])\n",
    "\n",
    "# LoRA params\n",
    "lora_params = [r['lora_params'] / 1e6 for r in results]\n",
    "axes[2].bar(x, lora_params, color='purple')\n",
    "axes[2].set_xlabel('Model Size')\n",
    "axes[2].set_ylabel('LoRA Parameters (M)')\n",
    "axes[2].set_title('LoRA Trainable Parameters')\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels([f'{s}B' for s in model_sizes])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('memory_analysis_solution.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\nKey Insights:\")\n",
    "print(\"1. LoRA reduces memory by 60-80% across all model sizes\")\n",
    "print(\"2. DGX Spark (128GB) can handle 70B+ with LoRA, but not full fine-tuning\")\n",
    "print(\"3. RTX 4090 (24GB) can only handle ~7B with LoRA\")\n",
    "print(\"4. LoRA parameters scale sublinearly with model size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "These solutions demonstrate:\n",
    "\n",
    "1. **Exercise 1**: How to extend LoRA to optionally train bias terms\n",
    "2. **Exercise 2**: Comparing different target module configurations (Q/V only vs full attention vs attention + MLP)\n",
    "3. **Exercise 3**: Comprehensive memory analysis showing LoRA's efficiency across model sizes\n",
    "\n",
    "Key takeaways:\n",
    "- LoRA with bias can provide marginal improvements for some tasks\n",
    "- More target modules = more capacity but more memory\n",
    "- LoRA enables training of 70B+ models on DGX Spark (impossible with full fine-tuning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}