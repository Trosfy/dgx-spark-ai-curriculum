{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 3.1.4 Solutions: Dataset Preparation for Fine-Tuning\n\n**Module:** 3.1 - Large Language Model Fine-Tuning  \n**Difficulty:** ⭐⭐☆☆☆ (Beginner-Intermediate)  \n**Exercises:** 3 (Multi-Format Converter, Chat Template Formatter, Data Quality Pipeline)\n\nThis notebook contains solutions for the dataset preparation exercises.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1 Solution: Multi-Format Dataset Converter\n",
    "\n",
    "**Task:** Create a converter that handles Alpaca, ShareGPT, and custom formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any, Optional, Callable\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "class DatasetFormat(Enum):\n",
    "    \"\"\"Supported dataset formats.\"\"\"\n",
    "    ALPACA = \"alpaca\"          # instruction, input, output\n",
    "    SHAREGPT = \"sharegpt\"      # conversations with roles\n",
    "    OASST = \"oasst\"            # Open Assistant format\n",
    "    DOLLY = \"dolly\"            # Databricks Dolly format\n",
    "    CUSTOM = \"custom\"          # User-defined format\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ConversationTurn:\n",
    "    \"\"\"A single turn in a conversation.\"\"\"\n",
    "    role: str  # 'system', 'user', 'assistant'\n",
    "    content: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Conversation:\n",
    "    \"\"\"A full conversation with multiple turns.\"\"\"\n",
    "    turns: List[ConversationTurn]\n",
    "    metadata: Optional[Dict[str, Any]] = None\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"conversations\": [\n",
    "                {\"role\": t.role, \"content\": t.content}\n",
    "                for t in self.turns\n",
    "            ],\n",
    "            \"metadata\": self.metadata or {}\n",
    "        }\n",
    "\n",
    "\n",
    "class UniversalDatasetConverter:\n",
    "    \"\"\"\n",
    "    Convert between different fine-tuning dataset formats.\n",
    "    \n",
    "    Supports:\n",
    "    - Alpaca (instruction/input/output)\n",
    "    - ShareGPT (conversations)\n",
    "    - Open Assistant (trees)\n",
    "    - Dolly (context/instruction/response)\n",
    "    - Custom formats via user-defined parsers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.custom_parsers: Dict[str, Callable] = {}\n",
    "    \n",
    "    def register_parser(\n",
    "        self, \n",
    "        name: str, \n",
    "        parser: Callable[[Dict], Conversation]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Register a custom parser for a new format.\n",
    "        \n",
    "        Args:\n",
    "            name: Name of the format\n",
    "            parser: Function that converts a dict to Conversation\n",
    "        \"\"\"\n",
    "        self.custom_parsers[name] = parser\n",
    "    \n",
    "    def parse_alpaca(self, item: Dict) -> Conversation:\n",
    "        \"\"\"Parse Alpaca format (instruction, input, output).\"\"\"\n",
    "        turns = []\n",
    "        \n",
    "        # Build user message\n",
    "        instruction = item.get(\"instruction\", \"\")\n",
    "        input_text = item.get(\"input\", \"\")\n",
    "        \n",
    "        if input_text:\n",
    "            user_content = f\"{instruction}\\n\\nInput: {input_text}\"\n",
    "        else:\n",
    "            user_content = instruction\n",
    "        \n",
    "        turns.append(ConversationTurn(role=\"user\", content=user_content))\n",
    "        turns.append(ConversationTurn(\n",
    "            role=\"assistant\", \n",
    "            content=item.get(\"output\", \"\")\n",
    "        ))\n",
    "        \n",
    "        return Conversation(turns=turns)\n",
    "    \n",
    "    def parse_sharegpt(self, item: Dict) -> Conversation:\n",
    "        \"\"\"Parse ShareGPT format (conversations list).\"\"\"\n",
    "        turns = []\n",
    "        \n",
    "        conversations = item.get(\"conversations\", [])\n",
    "        \n",
    "        role_mapping = {\n",
    "            \"human\": \"user\",\n",
    "            \"gpt\": \"assistant\",\n",
    "            \"system\": \"system\",\n",
    "            \"user\": \"user\",\n",
    "            \"assistant\": \"assistant\",\n",
    "        }\n",
    "        \n",
    "        for conv in conversations:\n",
    "            role = conv.get(\"from\", conv.get(\"role\", \"user\")).lower()\n",
    "            role = role_mapping.get(role, role)\n",
    "            content = conv.get(\"value\", conv.get(\"content\", \"\"))\n",
    "            turns.append(ConversationTurn(role=role, content=content))\n",
    "        \n",
    "        return Conversation(turns=turns)\n",
    "    \n",
    "    def parse_dolly(self, item: Dict) -> Conversation:\n",
    "        \"\"\"Parse Dolly format (context, instruction, response).\"\"\"\n",
    "        turns = []\n",
    "        \n",
    "        context = item.get(\"context\", \"\")\n",
    "        instruction = item.get(\"instruction\", \"\")\n",
    "        \n",
    "        # Build user message with context\n",
    "        if context:\n",
    "            user_content = f\"Context: {context}\\n\\n{instruction}\"\n",
    "        else:\n",
    "            user_content = instruction\n",
    "        \n",
    "        turns.append(ConversationTurn(role=\"user\", content=user_content))\n",
    "        turns.append(ConversationTurn(\n",
    "            role=\"assistant\",\n",
    "            content=item.get(\"response\", \"\")\n",
    "        ))\n",
    "        \n",
    "        return Conversation(\n",
    "            turns=turns,\n",
    "            metadata={\"category\": item.get(\"category\", \"unknown\")}\n",
    "        )\n",
    "    \n",
    "    def parse_oasst(self, item: Dict) -> Conversation:\n",
    "        \"\"\"Parse Open Assistant format (message trees).\"\"\"\n",
    "        turns = []\n",
    "        \n",
    "        # OASST uses a tree structure; we take the main branch\n",
    "        def extract_branch(node: Dict, turns_list: List):\n",
    "            role = \"user\" if node.get(\"role\") == \"prompter\" else \"assistant\"\n",
    "            turns_list.append(ConversationTurn(\n",
    "                role=role,\n",
    "                content=node.get(\"text\", \"\")\n",
    "            ))\n",
    "            \n",
    "            # Get first reply (main branch)\n",
    "            replies = node.get(\"replies\", [])\n",
    "            if replies:\n",
    "                extract_branch(replies[0], turns_list)\n",
    "        \n",
    "        if \"prompt\" in item:\n",
    "            # Flattened format\n",
    "            turns.append(ConversationTurn(role=\"user\", content=item[\"prompt\"]))\n",
    "            turns.append(ConversationTurn(role=\"assistant\", content=item.get(\"response\", \"\")))\n",
    "        else:\n",
    "            # Tree format\n",
    "            extract_branch(item, turns)\n",
    "        \n",
    "        return Conversation(turns=turns)\n",
    "    \n",
    "    def detect_format(self, item: Dict) -> DatasetFormat:\n",
    "        \"\"\"Auto-detect the format of a dataset item.\"\"\"\n",
    "        if \"conversations\" in item:\n",
    "            return DatasetFormat.SHAREGPT\n",
    "        elif \"instruction\" in item and \"output\" in item:\n",
    "            return DatasetFormat.ALPACA\n",
    "        elif \"instruction\" in item and \"response\" in item:\n",
    "            return DatasetFormat.DOLLY\n",
    "        elif \"prompt\" in item or \"text\" in item:\n",
    "            return DatasetFormat.OASST\n",
    "        else:\n",
    "            return DatasetFormat.CUSTOM\n",
    "    \n",
    "    def convert(\n",
    "        self,\n",
    "        item: Dict,\n",
    "        source_format: Optional[DatasetFormat] = None,\n",
    "    ) -> Conversation:\n",
    "        \"\"\"\n",
    "        Convert a single item to unified Conversation format.\n",
    "        \n",
    "        Args:\n",
    "            item: Dataset item dictionary\n",
    "            source_format: Format of the item (auto-detected if None)\n",
    "        \n",
    "        Returns:\n",
    "            Conversation object\n",
    "        \"\"\"\n",
    "        if source_format is None:\n",
    "            source_format = self.detect_format(item)\n",
    "        \n",
    "        if source_format == DatasetFormat.ALPACA:\n",
    "            return self.parse_alpaca(item)\n",
    "        elif source_format == DatasetFormat.SHAREGPT:\n",
    "            return self.parse_sharegpt(item)\n",
    "        elif source_format == DatasetFormat.DOLLY:\n",
    "            return self.parse_dolly(item)\n",
    "        elif source_format == DatasetFormat.OASST:\n",
    "            return self.parse_oasst(item)\n",
    "        elif source_format == DatasetFormat.CUSTOM:\n",
    "            # Try custom parsers\n",
    "            for name, parser in self.custom_parsers.items():\n",
    "                try:\n",
    "                    return parser(item)\n",
    "                except:\n",
    "                    continue\n",
    "            raise ValueError(f\"Unknown format and no custom parser matched\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown format: {source_format}\")\n",
    "    \n",
    "    def convert_dataset(\n",
    "        self,\n",
    "        items: List[Dict],\n",
    "        source_format: Optional[DatasetFormat] = None,\n",
    "    ) -> List[Conversation]:\n",
    "        \"\"\"Convert a full dataset to Conversation format.\"\"\"\n",
    "        return [self.convert(item, source_format) for item in items]\n",
    "\n",
    "\n",
    "# Demo\n",
    "converter = UniversalDatasetConverter()\n",
    "\n",
    "# Test with different formats\n",
    "alpaca_item = {\n",
    "    \"instruction\": \"Explain what machine learning is.\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"Machine learning is a branch of AI that enables computers to learn from data.\"\n",
    "}\n",
    "\n",
    "sharegpt_item = {\n",
    "    \"conversations\": [\n",
    "        {\"from\": \"human\", \"value\": \"What is Python?\"},\n",
    "        {\"from\": \"gpt\", \"value\": \"Python is a popular programming language.\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "dolly_item = {\n",
    "    \"instruction\": \"Summarize the context.\",\n",
    "    \"context\": \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"response\": \"A fox jumps over a dog.\",\n",
    "    \"category\": \"summarization\"\n",
    "}\n",
    "\n",
    "print(\"Format Detection and Conversion:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, item in [(\"Alpaca\", alpaca_item), (\"ShareGPT\", sharegpt_item), (\"Dolly\", dolly_item)]:\n",
    "    detected = converter.detect_format(item)\n",
    "    conv = converter.convert(item)\n",
    "    print(f\"\\n{name} -> {detected.value}:\")\n",
    "    for turn in conv.turns:\n",
    "        print(f\"  [{turn.role}]: {turn.content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2 Solution: Chat Template Formatter\n",
    "\n",
    "**Task:** Create formatters for all major chat template formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class ChatTemplateFormatter(ABC):\n",
    "    \"\"\"Base class for chat template formatters.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def format(self, conversation: Conversation) -> str:\n",
    "        \"\"\"Format a conversation into a string.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def name(self) -> str:\n",
    "        \"\"\"Name of the template.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class ChatMLFormatter(ChatTemplateFormatter):\n",
    "    \"\"\"ChatML format (OpenAI style).\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"ChatML\"\n",
    "    \n",
    "    def format(self, conversation: Conversation) -> str:\n",
    "        parts = []\n",
    "        for turn in conversation.turns:\n",
    "            parts.append(f\"<|im_start|>{turn.role}\\n{turn.content}<|im_end|>\")\n",
    "        parts.append(\"<|im_start|>assistant\\n\")  # For generation\n",
    "        return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "class Llama3Formatter(ChatTemplateFormatter):\n",
    "    \"\"\"Llama 3.1 chat format.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"Llama 3.1\"\n",
    "    \n",
    "    def format(self, conversation: Conversation) -> str:\n",
    "        parts = [\"<|begin_of_text|>\"]\n",
    "        \n",
    "        for turn in conversation.turns:\n",
    "            role = turn.role\n",
    "            parts.append(f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n{turn.content}<|eot_id|>\")\n",
    "        \n",
    "        # Add assistant header for generation\n",
    "        parts.append(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")\n",
    "        return \"\".join(parts)\n",
    "\n",
    "\n",
    "class Llama2Formatter(ChatTemplateFormatter):\n",
    "    \"\"\"Llama 2 chat format.\"\"\"\n",
    "    \n",
    "    def __init__(self, default_system: str = \"You are a helpful assistant.\"):\n",
    "        self.default_system = default_system\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"Llama 2\"\n",
    "    \n",
    "    def format(self, conversation: Conversation) -> str:\n",
    "        parts = [\"<s>\"]\n",
    "        \n",
    "        system_msg = self.default_system\n",
    "        start_idx = 0\n",
    "        \n",
    "        # Check for system message\n",
    "        if conversation.turns and conversation.turns[0].role == \"system\":\n",
    "            system_msg = conversation.turns[0].content\n",
    "            start_idx = 1\n",
    "        \n",
    "        # First turn includes system\n",
    "        first_user = True\n",
    "        for i, turn in enumerate(conversation.turns[start_idx:]):\n",
    "            if turn.role == \"user\":\n",
    "                if first_user:\n",
    "                    parts.append(f\"[INST] <<SYS>>\\n{system_msg}\\n<</SYS>>\\n\\n{turn.content} [/INST]\")\n",
    "                    first_user = False\n",
    "                else:\n",
    "                    parts.append(f\"<s>[INST] {turn.content} [/INST]\")\n",
    "            elif turn.role == \"assistant\":\n",
    "                parts.append(f\" {turn.content} </s>\")\n",
    "        \n",
    "        return \"\".join(parts)\n",
    "\n",
    "\n",
    "class MistralFormatter(ChatTemplateFormatter):\n",
    "    \"\"\"Mistral/Mixtral chat format.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"Mistral\"\n",
    "    \n",
    "    def format(self, conversation: Conversation) -> str:\n",
    "        parts = [\"<s>\"]\n",
    "        \n",
    "        for turn in conversation.turns:\n",
    "            if turn.role == \"user\":\n",
    "                parts.append(f\"[INST] {turn.content} [/INST]\")\n",
    "            elif turn.role == \"assistant\":\n",
    "                parts.append(f\"{turn.content}</s> \")\n",
    "        \n",
    "        return \"\".join(parts)\n",
    "\n",
    "\n",
    "class VicunaFormatter(ChatTemplateFormatter):\n",
    "    \"\"\"Vicuna chat format.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"Vicuna\"\n",
    "    \n",
    "    def format(self, conversation: Conversation) -> str:\n",
    "        parts = []\n",
    "        \n",
    "        for turn in conversation.turns:\n",
    "            if turn.role == \"system\":\n",
    "                parts.append(f\"{turn.content}\\n\\n\")\n",
    "            elif turn.role == \"user\":\n",
    "                parts.append(f\"USER: {turn.content}\\n\")\n",
    "            elif turn.role == \"assistant\":\n",
    "                parts.append(f\"ASSISTANT: {turn.content}</s>\\n\")\n",
    "        \n",
    "        parts.append(\"ASSISTANT:\")\n",
    "        return \"\".join(parts)\n",
    "\n",
    "\n",
    "class Phi3Formatter(ChatTemplateFormatter):\n",
    "    \"\"\"Phi-3 chat format.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"Phi-3\"\n",
    "    \n",
    "    def format(self, conversation: Conversation) -> str:\n",
    "        parts = []\n",
    "        \n",
    "        for turn in conversation.turns:\n",
    "            if turn.role == \"system\":\n",
    "                parts.append(f\"<|system|>\\n{turn.content}<|end|>\\n\")\n",
    "            elif turn.role == \"user\":\n",
    "                parts.append(f\"<|user|>\\n{turn.content}<|end|>\\n\")\n",
    "            elif turn.role == \"assistant\":\n",
    "                parts.append(f\"<|assistant|>\\n{turn.content}<|end|>\\n\")\n",
    "        \n",
    "        parts.append(\"<|assistant|>\\n\")\n",
    "        return \"\".join(parts)\n",
    "\n",
    "\n",
    "# Demo all formatters\n",
    "formatters = [\n",
    "    ChatMLFormatter(),\n",
    "    Llama3Formatter(),\n",
    "    Llama2Formatter(),\n",
    "    MistralFormatter(),\n",
    "    VicunaFormatter(),\n",
    "    Phi3Formatter(),\n",
    "]\n",
    "\n",
    "# Sample conversation\n",
    "sample_conv = Conversation(turns=[\n",
    "    ConversationTurn(role=\"system\", content=\"You are a helpful AI assistant.\"),\n",
    "    ConversationTurn(role=\"user\", content=\"What is Python?\"),\n",
    "    ConversationTurn(role=\"assistant\", content=\"Python is a programming language.\"),\n",
    "])\n",
    "\n",
    "print(\"Chat Template Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for formatter in formatters:\n",
    "    print(f\"\\n--- {formatter.name} ---\")\n",
    "    formatted = formatter.format(sample_conv)\n",
    "    # Show first 200 chars\n",
    "    print(formatted[:300] + \"...\" if len(formatted) > 300 else formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3 Solution: Data Quality Pipeline\n",
    "\n",
    "**Task:** Build a complete data quality pipeline with filtering, deduplication, and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import re\n",
    "from typing import List, Dict, Set, Tuple, Callable\n",
    "from dataclasses import dataclass, field\n",
    "from collections import Counter\n",
    "\n",
    "@dataclass\n",
    "class QualityReport:\n",
    "    \"\"\"Report of data quality processing.\"\"\"\n",
    "    original_count: int\n",
    "    final_count: int\n",
    "    removed_by_filter: Dict[str, int] = field(default_factory=dict)\n",
    "    duplicates_removed: int = 0\n",
    "    warnings: List[str] = field(default_factory=list)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"Quality Report:\\n\"\n",
    "            f\"  Original: {self.original_count}\\n\"\n",
    "            f\"  Final: {self.final_count}\\n\"\n",
    "            f\"  Removed: {self.original_count - self.final_count}\\n\"\n",
    "            f\"  - By filter: {self.removed_by_filter}\\n\"\n",
    "            f\"  - Duplicates: {self.duplicates_removed}\\n\"\n",
    "            f\"  Warnings: {len(self.warnings)}\"\n",
    "        )\n",
    "\n",
    "\n",
    "class DataQualityPipeline:\n",
    "    \"\"\"\n",
    "    Complete data quality pipeline for fine-tuning datasets.\n",
    "    \n",
    "    Features:\n",
    "    - Content filtering (length, language, toxicity)\n",
    "    - Deduplication (exact and near-duplicate)\n",
    "    - Format validation\n",
    "    - Statistics and reporting\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.filters: List[Tuple[str, Callable]] = []\n",
    "        self.seen_hashes: Set[str] = set()\n",
    "    \n",
    "    def add_filter(\n",
    "        self, \n",
    "        name: str, \n",
    "        filter_fn: Callable[[Conversation], bool]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Add a filter to the pipeline.\n",
    "        \n",
    "        Args:\n",
    "            name: Name of the filter\n",
    "            filter_fn: Function that returns True to KEEP the item\n",
    "        \"\"\"\n",
    "        self.filters.append((name, filter_fn))\n",
    "    \n",
    "    def compute_hash(self, conversation: Conversation) -> str:\n",
    "        \"\"\"Compute hash for deduplication.\"\"\"\n",
    "        content = \"\".join(t.content for t in conversation.turns)\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n",
    "    \n",
    "    def compute_minhash(self, text: str, num_hashes: int = 100) -> List[int]:\n",
    "        \"\"\"Compute MinHash signature for near-duplicate detection.\"\"\"\n",
    "        # Simple word-level shingling\n",
    "        words = text.lower().split()\n",
    "        shingles = set()\n",
    "        for i in range(len(words) - 2):\n",
    "            shingles.add(\" \".join(words[i:i+3]))\n",
    "        \n",
    "        if not shingles:\n",
    "            return [0] * num_hashes\n",
    "        \n",
    "        # Generate hash values\n",
    "        signature = []\n",
    "        for seed in range(num_hashes):\n",
    "            min_hash = min(\n",
    "                hash((shingle, seed)) & 0xFFFFFFFF \n",
    "                for shingle in shingles\n",
    "            )\n",
    "            signature.append(min_hash)\n",
    "        \n",
    "        return signature\n",
    "    \n",
    "    def estimate_jaccard(self, sig1: List[int], sig2: List[int]) -> float:\n",
    "        \"\"\"Estimate Jaccard similarity from MinHash signatures.\"\"\"\n",
    "        if len(sig1) != len(sig2):\n",
    "            return 0.0\n",
    "        matches = sum(1 for a, b in zip(sig1, sig2) if a == b)\n",
    "        return matches / len(sig1)\n",
    "    \n",
    "    def process(\n",
    "        self,\n",
    "        conversations: List[Conversation],\n",
    "        deduplicate: bool = True,\n",
    "        near_duplicate_threshold: float = 0.8,\n",
    "    ) -> Tuple[List[Conversation], QualityReport]:\n",
    "        \"\"\"\n",
    "        Process dataset through quality pipeline.\n",
    "        \n",
    "        Args:\n",
    "            conversations: Input conversations\n",
    "            deduplicate: Whether to remove duplicates\n",
    "            near_duplicate_threshold: Jaccard threshold for near-duplicates\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (filtered conversations, quality report)\n",
    "        \"\"\"\n",
    "        report = QualityReport(\n",
    "            original_count=len(conversations),\n",
    "            final_count=0,\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        self.seen_hashes = set()\n",
    "        signatures = []  # For near-duplicate detection\n",
    "        \n",
    "        for conv in conversations:\n",
    "            # Apply filters\n",
    "            passed = True\n",
    "            for filter_name, filter_fn in self.filters:\n",
    "                if not filter_fn(conv):\n",
    "                    report.removed_by_filter[filter_name] = (\n",
    "                        report.removed_by_filter.get(filter_name, 0) + 1\n",
    "                    )\n",
    "                    passed = False\n",
    "                    break\n",
    "            \n",
    "            if not passed:\n",
    "                continue\n",
    "            \n",
    "            # Deduplication\n",
    "            if deduplicate:\n",
    "                # Exact dedup\n",
    "                conv_hash = self.compute_hash(conv)\n",
    "                if conv_hash in self.seen_hashes:\n",
    "                    report.duplicates_removed += 1\n",
    "                    continue\n",
    "                self.seen_hashes.add(conv_hash)\n",
    "                \n",
    "                # Near-duplicate detection\n",
    "                content = \" \".join(t.content for t in conv.turns)\n",
    "                sig = self.compute_minhash(content)\n",
    "                \n",
    "                is_near_dup = False\n",
    "                for existing_sig in signatures:\n",
    "                    if self.estimate_jaccard(sig, existing_sig) > near_duplicate_threshold:\n",
    "                        is_near_dup = True\n",
    "                        break\n",
    "                \n",
    "                if is_near_dup:\n",
    "                    report.duplicates_removed += 1\n",
    "                    continue\n",
    "                \n",
    "                signatures.append(sig)\n",
    "            \n",
    "            results.append(conv)\n",
    "        \n",
    "        report.final_count = len(results)\n",
    "        return results, report\n",
    "\n",
    "\n",
    "# Build a complete quality pipeline\n",
    "pipeline = DataQualityPipeline()\n",
    "\n",
    "# Filter: Minimum length\n",
    "pipeline.add_filter(\n",
    "    \"min_length\",\n",
    "    lambda conv: sum(len(t.content) for t in conv.turns) >= 50\n",
    ")\n",
    "\n",
    "# Filter: Maximum length\n",
    "pipeline.add_filter(\n",
    "    \"max_length\",\n",
    "    lambda conv: sum(len(t.content) for t in conv.turns) <= 10000\n",
    ")\n",
    "\n",
    "# Filter: Has both user and assistant turns\n",
    "pipeline.add_filter(\n",
    "    \"has_response\",\n",
    "    lambda conv: any(t.role == \"assistant\" for t in conv.turns)\n",
    ")\n",
    "\n",
    "# Filter: No excessive repetition\n",
    "def check_repetition(conv: Conversation) -> bool:\n",
    "    for turn in conv.turns:\n",
    "        words = turn.content.lower().split()\n",
    "        if len(words) > 10:\n",
    "            word_counts = Counter(words)\n",
    "            most_common_ratio = word_counts.most_common(1)[0][1] / len(words)\n",
    "            if most_common_ratio > 0.5:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "pipeline.add_filter(\"no_repetition\", check_repetition)\n",
    "\n",
    "# Filter: No placeholder text\n",
    "pipeline.add_filter(\n",
    "    \"no_placeholders\",\n",
    "    lambda conv: not any(\n",
    "        re.search(r'\\[.*?\\]|\\{.*?\\}|<.*?>', t.content) \n",
    "        for t in conv.turns\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Quality Pipeline configured with filters:\")\n",
    "for name, _ in pipeline.filters:\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the pipeline\n",
    "\n",
    "# Create test data with various quality issues\n",
    "test_conversations = [\n",
    "    # Good conversation\n",
    "    Conversation(turns=[\n",
    "        ConversationTurn(role=\"user\", content=\"What is machine learning?\"),\n",
    "        ConversationTurn(role=\"assistant\", content=\"Machine learning is a branch of AI that enables computers to learn from data without being explicitly programmed.\")\n",
    "    ]),\n",
    "    # Too short\n",
    "    Conversation(turns=[\n",
    "        ConversationTurn(role=\"user\", content=\"Hi\"),\n",
    "        ConversationTurn(role=\"assistant\", content=\"Hello!\")\n",
    "    ]),\n",
    "    # Missing response\n",
    "    Conversation(turns=[\n",
    "        ConversationTurn(role=\"user\", content=\"This is a question without an answer.\")\n",
    "    ]),\n",
    "    # Duplicate of first\n",
    "    Conversation(turns=[\n",
    "        ConversationTurn(role=\"user\", content=\"What is machine learning?\"),\n",
    "        ConversationTurn(role=\"assistant\", content=\"Machine learning is a branch of AI that enables computers to learn from data without being explicitly programmed.\")\n",
    "    ]),\n",
    "    # Has placeholders\n",
    "    Conversation(turns=[\n",
    "        ConversationTurn(role=\"user\", content=\"Tell me about [TOPIC].\"),\n",
    "        ConversationTurn(role=\"assistant\", content=\"[RESPONSE HERE]\")\n",
    "    ]),\n",
    "    # Excessive repetition\n",
    "    Conversation(turns=[\n",
    "        ConversationTurn(role=\"user\", content=\"good good good good good good good good good good\"),\n",
    "        ConversationTurn(role=\"assistant\", content=\"That's great!\")\n",
    "    ]),\n",
    "    # Good conversation 2\n",
    "    Conversation(turns=[\n",
    "        ConversationTurn(role=\"user\", content=\"Explain deep learning.\"),\n",
    "        ConversationTurn(role=\"assistant\", content=\"Deep learning is a subset of machine learning that uses neural networks with many layers to learn complex patterns from data.\")\n",
    "    ]),\n",
    "]\n",
    "\n",
    "# Process\n",
    "filtered, report = pipeline.process(test_conversations, deduplicate=True)\n",
    "\n",
    "print(\"\\nQuality Pipeline Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(report)\n",
    "\n",
    "print(f\"\\n\\nKept {len(filtered)} conversations:\")\n",
    "for i, conv in enumerate(filtered):\n",
    "    user_msg = conv.turns[0].content[:50]\n",
    "    print(f\"  {i+1}. {user_msg}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "These solutions demonstrate:\n",
    "\n",
    "1. **Universal Converter**: Handle any dataset format with auto-detection\n",
    "\n",
    "2. **Chat Templates**: Format conversations for any model (Llama, Mistral, etc.)\n",
    "\n",
    "3. **Quality Pipeline**: Clean data with filtering and deduplication\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Always validate** your data before fine-tuning\n",
    "- **Deduplication** prevents overfitting on repeated examples\n",
    "- **Chat templates** must match the model's training format\n",
    "- **Quality > Quantity** for fine-tuning datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}