{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1.9: KTO Binary Feedback - Solutions\n",
    "\n",
    "Complete solutions for Kahneman-Tversky Optimization exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: KTO Loss Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def kto_loss(\n",
    "    policy_logps: torch.Tensor,\n",
    "    reference_logps: torch.Tensor,\n",
    "    labels: torch.Tensor,  # 1 for desirable, 0 for undesirable\n",
    "    beta: float = 0.1,\n",
    "    desirable_weight: float = 1.0,\n",
    "    undesirable_weight: float = 1.0\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    KTO Loss based on Prospect Theory.\n",
    "    \n",
    "    Key insight: Humans are loss-averse (losses hurt more than gains help).\n",
    "    \n",
    "    For desirable (label=1): Maximize log ratio above KL baseline\n",
    "    For undesirable (label=0): Minimize log ratio below KL baseline\n",
    "    \"\"\"\n",
    "    # Log ratios\n",
    "    log_ratios = policy_logps - reference_logps\n",
    "    \n",
    "    # KL divergence as baseline (estimated from batch)\n",
    "    kl_baseline = log_ratios.mean().detach()\n",
    "    \n",
    "    # Separate desirable and undesirable\n",
    "    desirable_mask = labels == 1\n",
    "    undesirable_mask = labels == 0\n",
    "    \n",
    "    # Desirable loss: want log_ratio > kl_baseline\n",
    "    # L_desirable = -log(σ(β * (log_ratio - kl_baseline)))\n",
    "    desirable_logits = beta * (log_ratios[desirable_mask] - kl_baseline)\n",
    "    desirable_loss = -F.logsigmoid(desirable_logits).mean() if desirable_mask.any() else torch.tensor(0.0)\n",
    "    \n",
    "    # Undesirable loss: want log_ratio < kl_baseline (flip the sign)\n",
    "    # L_undesirable = -log(σ(-β * (log_ratio - kl_baseline)))\n",
    "    undesirable_logits = -beta * (log_ratios[undesirable_mask] - kl_baseline)\n",
    "    undesirable_loss = -F.logsigmoid(undesirable_logits).mean() if undesirable_mask.any() else torch.tensor(0.0)\n",
    "    \n",
    "    # Weighted combination\n",
    "    total_loss = desirable_weight * desirable_loss + undesirable_weight * undesirable_loss\n",
    "    \n",
    "    # Metrics\n",
    "    desirable_accuracy = (desirable_logits > 0).float().mean().item() if desirable_mask.any() else 0\n",
    "    undesirable_accuracy = (undesirable_logits > 0).float().mean().item() if undesirable_mask.any() else 0\n",
    "    \n",
    "    return total_loss, {\n",
    "        \"desirable_loss\": desirable_loss.item() if isinstance(desirable_loss, torch.Tensor) else 0,\n",
    "        \"undesirable_loss\": undesirable_loss.item() if isinstance(undesirable_loss, torch.Tensor) else 0,\n",
    "        \"kl_baseline\": kl_baseline.item(),\n",
    "        \"desirable_accuracy\": desirable_accuracy,\n",
    "        \"undesirable_accuracy\": undesirable_accuracy\n",
    "    }\n",
    "\n",
    "# Test\n",
    "torch.manual_seed(42)\n",
    "batch_size = 8\n",
    "\n",
    "policy_logps = torch.randn(batch_size) - 1\n",
    "reference_logps = torch.randn(batch_size) - 1.2\n",
    "labels = torch.tensor([1, 1, 1, 1, 0, 0, 0, 0])  # Half desirable, half undesirable\n",
    "\n",
    "loss, metrics = kto_loss(policy_logps, reference_logps, labels)\n",
    "print(f\"KTO Loss: {loss:.4f}\")\n",
    "print(f\"Metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Prospect Theory Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_prospect_theory():\n",
    "    \"\"\"\n",
    "    Visualize Prospect Theory's value function.\n",
    "    \n",
    "    Key insight: Losses loom larger than gains.\n",
    "    \"\"\"\n",
    "    x = np.linspace(-3, 3, 100)\n",
    "    \n",
    "    # Standard value function from Prospect Theory\n",
    "    # v(x) = x^α for gains (x >= 0)\n",
    "    # v(x) = -λ(-x)^β for losses (x < 0)\n",
    "    alpha = 0.88\n",
    "    beta_pt = 0.88\n",
    "    lambda_loss = 2.25  # Loss aversion coefficient\n",
    "    \n",
    "    def prospect_value(x):\n",
    "        if x >= 0:\n",
    "            return x ** alpha\n",
    "        else:\n",
    "            return -lambda_loss * ((-x) ** beta_pt)\n",
    "    \n",
    "    y = np.array([prospect_value(xi) for xi in x])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Prospect Theory Value Function\n",
    "    axes[0].plot(x, y, 'b-', linewidth=2, label='Prospect Theory')\n",
    "    axes[0].plot(x, x, 'k--', alpha=0.5, label='Linear (rational)')\n",
    "    axes[0].axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "    axes[0].axvline(x=0, color='gray', linestyle='-', alpha=0.3)\n",
    "    axes[0].fill_between(x[x<0], y[x<0], 0, alpha=0.2, color='red', label='Losses')\n",
    "    axes[0].fill_between(x[x>=0], 0, y[x>=0], alpha=0.2, color='green', label='Gains')\n",
    "    \n",
    "    axes[0].set_xlabel('Outcome (gains/losses)')\n",
    "    axes[0].set_ylabel('Perceived Value')\n",
    "    axes[0].set_title('Prospect Theory: Losses Loom Larger')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotate\n",
    "    axes[0].annotate('Loss aversion:\\nλ = 2.25', xy=(-2, prospect_value(-2)),\n",
    "                     xytext=(-1, -3), fontsize=10,\n",
    "                     arrowprops=dict(arrowstyle='->', color='red'))\n",
    "    \n",
    "    # Plot 2: KTO Loss curves\n",
    "    log_ratio_diff = np.linspace(-2, 2, 100)\n",
    "    beta_kto = 0.1\n",
    "    \n",
    "    desirable_loss = -np.log(1 / (1 + np.exp(-beta_kto * log_ratio_diff)))\n",
    "    undesirable_loss = -np.log(1 / (1 + np.exp(beta_kto * log_ratio_diff)))\n",
    "    \n",
    "    axes[1].plot(log_ratio_diff, desirable_loss, 'g-', linewidth=2, label='Desirable (thumbs up)')\n",
    "    axes[1].plot(log_ratio_diff, undesirable_loss, 'r-', linewidth=2, label='Undesirable (thumbs down)')\n",
    "    axes[1].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    axes[1].set_xlabel('Log Ratio - KL Baseline')\n",
    "    axes[1].set_ylabel('KTO Loss')\n",
    "    axes[1].set_title('KTO Loss Functions')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prospect_theory.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nKey Insight:\")\n",
    "    print(\"Humans feel losses ~2.25x more strongly than equivalent gains.\")\n",
    "    print(\"KTO leverages this: heavily penalize bad outputs, gently reward good ones.\")\n",
    "\n",
    "visualize_prospect_theory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Complete KTO Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import KTOTrainer, KTOConfig\n",
    "from datasets import Dataset\n",
    "\n",
    "def create_kto_pipeline(\n",
    "    model_id: str = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    output_dir: str = \"./kto-output\",\n",
    "    beta: float = 0.1,\n",
    "    desirable_weight: float = 1.0,\n",
    "    undesirable_weight: float = 1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete KTO training pipeline for binary feedback data.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"KTO TRAINING PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Quantization\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    # 2. Load model\n",
    "    print(\"\\n1. Loading model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # 3. LoRA\n",
    "    print(\"2. Configuring LoRA...\")\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # 4. Binary feedback dataset\n",
    "    print(\"3. Preparing binary feedback data...\")\n",
    "    kto_data = [\n",
    "        # Desirable examples (thumbs up)\n",
    "        {\"prompt\": \"Explain machine learning.\",\n",
    "         \"completion\": \"Machine learning is a subset of AI where computers learn patterns from data without explicit programming.\",\n",
    "         \"label\": True},\n",
    "        {\"prompt\": \"What's the capital of France?\",\n",
    "         \"completion\": \"Paris is the capital of France.\",\n",
    "         \"label\": True},\n",
    "        {\"prompt\": \"How do I stay healthy?\",\n",
    "         \"completion\": \"Eat balanced meals, exercise regularly, get enough sleep, and manage stress.\",\n",
    "         \"label\": True},\n",
    "        \n",
    "        # Undesirable examples (thumbs down)\n",
    "        {\"prompt\": \"Explain machine learning.\",\n",
    "         \"completion\": \"Its computers doing stuff.\",\n",
    "         \"label\": False},\n",
    "        {\"prompt\": \"What's the capital of France?\",\n",
    "         \"completion\": \"I don't know.\",\n",
    "         \"label\": False},\n",
    "        {\"prompt\": \"How do I stay healthy?\",\n",
    "         \"completion\": \"Just don't get sick lol\",\n",
    "         \"label\": False},\n",
    "    ]\n",
    "    \n",
    "    dataset = Dataset.from_list(kto_data)\n",
    "    \n",
    "    desirable_count = sum(1 for x in kto_data if x[\"label\"])\n",
    "    undesirable_count = len(kto_data) - desirable_count\n",
    "    print(f\"   Desirable: {desirable_count}, Undesirable: {undesirable_count}\")\n",
    "    \n",
    "    # 5. Reference model\n",
    "    print(\"4. Creating reference model...\")\n",
    "    ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    # 6. KTO training config\n",
    "    print(\"5. Setting up KTO training...\")\n",
    "    training_args = KTOConfig(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=5e-5,\n",
    "        \n",
    "        # KTO specific\n",
    "        beta=beta,\n",
    "        desirable_weight=desirable_weight,\n",
    "        undesirable_weight=undesirable_weight,\n",
    "        \n",
    "        # Memory\n",
    "        bf16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        \n",
    "        # Logging\n",
    "        logging_steps=1,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    # 7. Create trainer\n",
    "    trainer = KTOTrainer(\n",
    "        model=model,\n",
    "        ref_model=ref_model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nKTO Configuration:\")\n",
    "    print(f\"  β (beta): {beta}\")\n",
    "    print(f\"  Desirable weight: {desirable_weight}\")\n",
    "    print(f\"  Undesirable weight: {undesirable_weight}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return trainer, model, tokenizer\n",
    "\n",
    "# Uncomment to run\n",
    "# trainer, model, tokenizer = create_kto_pipeline()\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Collecting Binary Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "class BinaryFeedbackCollector:\n",
    "    \"\"\"\n",
    "    Simple system for collecting thumbs up/down feedback.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_file: str = \"feedback_data.jsonl\"):\n",
    "        self.output_file = output_file\n",
    "        self.collected = []\n",
    "    \n",
    "    def record_feedback(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        completion: str,\n",
    "        is_good: bool,\n",
    "        metadata: dict = None\n",
    "    ):\n",
    "        \"\"\"Record a single feedback instance.\"\"\"\n",
    "        entry = {\n",
    "            \"prompt\": prompt,\n",
    "            \"completion\": completion,\n",
    "            \"label\": is_good,\n",
    "            \"metadata\": metadata or {}\n",
    "        }\n",
    "        self.collected.append(entry)\n",
    "        \n",
    "        # Append to file\n",
    "        with open(self.output_file, \"a\") as f:\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get feedback statistics.\"\"\"\n",
    "        desirable = sum(1 for x in self.collected if x[\"label\"])\n",
    "        undesirable = len(self.collected) - desirable\n",
    "        \n",
    "        return {\n",
    "            \"total\": len(self.collected),\n",
    "            \"desirable\": desirable,\n",
    "            \"undesirable\": undesirable,\n",
    "            \"ratio\": desirable / max(undesirable, 1)\n",
    "        }\n",
    "    \n",
    "    def load_existing(self) -> List[Dict]:\n",
    "        \"\"\"Load existing feedback from file.\"\"\"\n",
    "        try:\n",
    "            with open(self.output_file, \"r\") as f:\n",
    "                self.collected = [json.loads(line) for line in f]\n",
    "        except FileNotFoundError:\n",
    "            self.collected = []\n",
    "        return self.collected\n",
    "    \n",
    "    def to_kto_format(self) -> List[Dict]:\n",
    "        \"\"\"Convert to KTO training format.\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"prompt\": x[\"prompt\"],\n",
    "                \"completion\": x[\"completion\"],\n",
    "                \"label\": x[\"label\"]\n",
    "            }\n",
    "            for x in self.collected\n",
    "        ]\n",
    "\n",
    "# Demo usage\n",
    "collector = BinaryFeedbackCollector()\n",
    "\n",
    "# Simulate collecting feedback\n",
    "collector.record_feedback(\n",
    "    prompt=\"What is Python?\",\n",
    "    completion=\"Python is a high-level programming language known for its readability.\",\n",
    "    is_good=True,\n",
    "    metadata={\"user_id\": \"demo\", \"timestamp\": \"2024-01-01\"}\n",
    ")\n",
    "\n",
    "collector.record_feedback(\n",
    "    prompt=\"What is Python?\",\n",
    "    completion=\"A snake.\",\n",
    "    is_good=False,\n",
    "    metadata={\"user_id\": \"demo\", \"timestamp\": \"2024-01-01\"}\n",
    ")\n",
    "\n",
    "print(\"Feedback Stats:\", collector.get_stats())\n",
    "print(\"\\nKTO Format:\")\n",
    "print(json.dumps(collector.to_kto_format(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: When to Use KTO vs DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_guide():\n",
    "    \"\"\"\n",
    "    Decision guide for choosing between DPO and KTO.\n",
    "    \"\"\"\n",
    "    print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════╗\n",
    "║                     DPO vs KTO Decision Guide                     ║\n",
    "╠══════════════════════════════════════════════════════════════════╣\n",
    "║                                                                   ║\n",
    "║   Do you have PAIRED preference data?                            ║\n",
    "║   (same prompt, chosen vs rejected)                              ║\n",
    "║                                                                   ║\n",
    "║   YES ──────────────────────┐                                    ║\n",
    "║                             │                                    ║\n",
    "║                             ▼                                    ║\n",
    "║                    ┌─────────────────┐                           ║\n",
    "║                    │  Use DPO/SimPO  │                           ║\n",
    "║                    │  (pair-based)   │                           ║\n",
    "║                    └─────────────────┘                           ║\n",
    "║                                                                   ║\n",
    "║   NO (only thumbs up/down) ─┐                                    ║\n",
    "║                             │                                    ║\n",
    "║                             ▼                                    ║\n",
    "║                    ┌─────────────────┐                           ║\n",
    "║                    │    Use KTO!     │                           ║\n",
    "║                    │ (binary signal) │                           ║\n",
    "║                    └─────────────────┘                           ║\n",
    "║                                                                   ║\n",
    "╠══════════════════════════════════════════════════════════════════╣\n",
    "║                                                                   ║\n",
    "║   KTO is ideal when:                                             ║\n",
    "║   ✓ You only have thumbs up/down feedback                        ║\n",
    "║   ✓ Feedback comes from production (not curated)                 ║\n",
    "║   ✓ You can't pair responses to same prompt                      ║\n",
    "║   ✓ Data is highly imbalanced (mostly good or mostly bad)        ║\n",
    "║                                                                   ║\n",
    "║   DPO is better when:                                            ║\n",
    "║   ✓ You have A/B comparison data                                 ║\n",
    "║   ✓ Each prompt has multiple rated responses                     ║\n",
    "║   ✓ You need strong preference learning                          ║\n",
    "║                                                                   ║\n",
    "╚══════════════════════════════════════════════════════════════════╝\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\nData format examples:\")\n",
    "    print(\"\\nDPO format (paired):\")\n",
    "    print(json.dumps({\n",
    "        \"prompt\": \"What is AI?\",\n",
    "        \"chosen\": \"AI is artificial intelligence...\",\n",
    "        \"rejected\": \"dunno\"\n",
    "    }, indent=2))\n",
    "    \n",
    "    print(\"\\nKTO format (binary):\")\n",
    "    print(json.dumps({\n",
    "        \"prompt\": \"What is AI?\",\n",
    "        \"completion\": \"AI is artificial intelligence...\",\n",
    "        \"label\": True  # or False\n",
    "    }, indent=2))\n",
    "\n",
    "decision_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Prospect Theory**: Losses hurt ~2.25x more than gains help\n",
    "2. **KTO Loss**: Uses KL baseline to separate desirable/undesirable\n",
    "3. **Binary Feedback**: Just thumbs up/down, no paired comparisons needed\n",
    "4. **Use Case**: Production feedback, imbalanced data, simple annotation\n",
    "5. **vs DPO**: KTO for binary, DPO for paired comparisons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
