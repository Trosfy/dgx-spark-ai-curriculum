{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 3.1.5 Solutions: Direct Preference Optimization (DPO)\n\n**Module:** 3.1 - Large Language Model Fine-Tuning  \n**Difficulty:** ⭐⭐⭐⭐☆ (Advanced)  \n**Exercises:** 3 (DPO Loss from Scratch, DPO Variants Comparison, Preference Dataset Generator)\n\nThis notebook contains solutions for the DPO training exercises.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1 Solution: DPO Loss Implementation from Scratch\n",
    "\n",
    "**Task:** Implement the DPO loss function with detailed comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple, Dict, Optional\n",
    "\n",
    "class DPOLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Direct Preference Optimization (DPO) Loss.\n",
    "    \n",
    "    DPO directly optimizes the policy to maximize the log probability ratio\n",
    "    between chosen and rejected responses, weighted by a reference model.\n",
    "    \n",
    "    Loss = -log(sigmoid(β * (log π(y_w|x) - log π(y_l|x) - log π_ref(y_w|x) + log π_ref(y_l|x))))\n",
    "    \n",
    "    Where:\n",
    "    - π is the policy model being trained\n",
    "    - π_ref is the frozen reference model\n",
    "    - y_w is the chosen (winner) response\n",
    "    - y_l is the rejected (loser) response\n",
    "    - x is the prompt\n",
    "    - β is the temperature controlling preference strength\n",
    "    \n",
    "    Reference: https://arxiv.org/abs/2305.18290\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        beta: float = 0.1,\n",
    "        label_smoothing: float = 0.0,\n",
    "        reference_free: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize DPO loss.\n",
    "        \n",
    "        Args:\n",
    "            beta: Temperature parameter (higher = stronger preferences)\n",
    "            label_smoothing: Label smoothing for soft targets\n",
    "            reference_free: If True, skip reference model (SimPO-like)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.reference_free = reference_free\n",
    "    \n",
    "    def compute_log_probs(\n",
    "        self,\n",
    "        logits: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute per-sequence log probabilities.\n",
    "        \n",
    "        Args:\n",
    "            logits: Model output logits (batch, seq_len, vocab_size)\n",
    "            labels: Target token IDs (batch, seq_len)\n",
    "            attention_mask: Mask for valid tokens (batch, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            Per-sequence sum of log probabilities (batch,)\n",
    "        \"\"\"\n",
    "        # Shift logits and labels for causal LM\n",
    "        # logits[t] predicts labels[t+1]\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = labels[:, 1:].contiguous()\n",
    "        shift_mask = attention_mask[:, 1:].contiguous()\n",
    "        \n",
    "        # Compute log softmax\n",
    "        log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "        \n",
    "        # Gather the log probs for the actual tokens\n",
    "        # Shape: (batch, seq_len-1)\n",
    "        per_token_log_probs = torch.gather(\n",
    "            log_probs,\n",
    "            dim=-1,\n",
    "            index=shift_labels.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "        \n",
    "        # Mask out padding and sum per sequence\n",
    "        per_token_log_probs = per_token_log_probs * shift_mask\n",
    "        sequence_log_probs = per_token_log_probs.sum(dim=-1)\n",
    "        \n",
    "        # Optional: normalize by length\n",
    "        # sequence_log_probs = sequence_log_probs / shift_mask.sum(dim=-1).clamp(min=1)\n",
    "        \n",
    "        return sequence_log_probs\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        policy_chosen_logits: torch.Tensor,\n",
    "        policy_rejected_logits: torch.Tensor,\n",
    "        chosen_labels: torch.Tensor,\n",
    "        rejected_labels: torch.Tensor,\n",
    "        chosen_mask: torch.Tensor,\n",
    "        rejected_mask: torch.Tensor,\n",
    "        reference_chosen_logits: Optional[torch.Tensor] = None,\n",
    "        reference_rejected_logits: Optional[torch.Tensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Compute DPO loss.\n",
    "        \n",
    "        Args:\n",
    "            policy_chosen_logits: Policy model logits for chosen responses\n",
    "            policy_rejected_logits: Policy model logits for rejected responses\n",
    "            chosen_labels: Token IDs for chosen responses\n",
    "            rejected_labels: Token IDs for rejected responses\n",
    "            chosen_mask: Attention mask for chosen responses\n",
    "            rejected_mask: Attention mask for rejected responses\n",
    "            reference_chosen_logits: Reference model logits for chosen (optional)\n",
    "            reference_rejected_logits: Reference model logits for rejected (optional)\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (loss, metrics_dict)\n",
    "        \"\"\"\n",
    "        # Step 1: Compute policy log probabilities\n",
    "        policy_chosen_log_probs = self.compute_log_probs(\n",
    "            policy_chosen_logits, chosen_labels, chosen_mask\n",
    "        )\n",
    "        policy_rejected_log_probs = self.compute_log_probs(\n",
    "            policy_rejected_logits, rejected_labels, rejected_mask\n",
    "        )\n",
    "        \n",
    "        # Step 2: Compute reference log probabilities (if not reference-free)\n",
    "        if self.reference_free or reference_chosen_logits is None:\n",
    "            ref_chosen_log_probs = torch.zeros_like(policy_chosen_log_probs)\n",
    "            ref_rejected_log_probs = torch.zeros_like(policy_rejected_log_probs)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                ref_chosen_log_probs = self.compute_log_probs(\n",
    "                    reference_chosen_logits, chosen_labels, chosen_mask\n",
    "                )\n",
    "                ref_rejected_log_probs = self.compute_log_probs(\n",
    "                    reference_rejected_logits, rejected_labels, rejected_mask\n",
    "                )\n",
    "        \n",
    "        # Step 3: Compute log ratios\n",
    "        # π_chosen = log π(y_w|x) - log π_ref(y_w|x)\n",
    "        # π_rejected = log π(y_l|x) - log π_ref(y_l|x)\n",
    "        chosen_log_ratio = policy_chosen_log_probs - ref_chosen_log_probs\n",
    "        rejected_log_ratio = policy_rejected_log_probs - ref_rejected_log_probs\n",
    "        \n",
    "        # Step 4: Compute the DPO loss\n",
    "        # logits = β * (π_chosen - π_rejected)\n",
    "        logits = self.beta * (chosen_log_ratio - rejected_log_ratio)\n",
    "        \n",
    "        # Apply label smoothing if needed\n",
    "        if self.label_smoothing > 0:\n",
    "            # Soft labels: (1 - ε, ε) instead of (1, 0)\n",
    "            loss = (\n",
    "                (1 - self.label_smoothing) * F.logsigmoid(logits) +\n",
    "                self.label_smoothing * F.logsigmoid(-logits)\n",
    "            )\n",
    "            loss = -loss.mean()\n",
    "        else:\n",
    "            # Standard DPO loss: -log(sigmoid(logits))\n",
    "            loss = -F.logsigmoid(logits).mean()\n",
    "        \n",
    "        # Step 5: Compute metrics\n",
    "        with torch.no_grad():\n",
    "            # Accuracy: how often does the model prefer chosen over rejected?\n",
    "            accuracy = (logits > 0).float().mean()\n",
    "            \n",
    "            # Reward margins\n",
    "            chosen_rewards = self.beta * chosen_log_ratio\n",
    "            rejected_rewards = self.beta * rejected_log_ratio\n",
    "            reward_margin = (chosen_rewards - rejected_rewards).mean()\n",
    "        \n",
    "        metrics = {\n",
    "            \"loss\": loss.detach(),\n",
    "            \"accuracy\": accuracy,\n",
    "            \"chosen_reward\": chosen_rewards.mean(),\n",
    "            \"rejected_reward\": rejected_rewards.mean(),\n",
    "            \"reward_margin\": reward_margin,\n",
    "            \"chosen_log_prob\": policy_chosen_log_probs.mean(),\n",
    "            \"rejected_log_prob\": policy_rejected_log_probs.mean(),\n",
    "        }\n",
    "        \n",
    "        return loss, metrics\n",
    "\n",
    "\n",
    "# Demo\n",
    "print(\"DPO Loss Implementation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create dummy data\n",
    "batch_size = 4\n",
    "seq_len = 32\n",
    "vocab_size = 1000\n",
    "\n",
    "policy_chosen_logits = torch.randn(batch_size, seq_len, vocab_size)\n",
    "policy_rejected_logits = torch.randn(batch_size, seq_len, vocab_size)\n",
    "chosen_labels = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "rejected_labels = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "chosen_mask = torch.ones(batch_size, seq_len)\n",
    "rejected_mask = torch.ones(batch_size, seq_len)\n",
    "\n",
    "# Compute loss\n",
    "dpo_loss = DPOLoss(beta=0.1)\n",
    "loss, metrics = dpo_loss(\n",
    "    policy_chosen_logits,\n",
    "    policy_rejected_logits,\n",
    "    chosen_labels,\n",
    "    rejected_labels,\n",
    "    chosen_mask,\n",
    "    rejected_mask,\n",
    ")\n",
    "\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "print(f\"Accuracy: {metrics['accuracy'].item():.2%}\")\n",
    "print(f\"Reward margin: {metrics['reward_margin'].item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2 Solution: DPO Variants Comparison\n",
    "\n",
    "**Task:** Implement IPO, KTO, and ORPO loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IPOLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Identity Preference Optimization (IPO) Loss.\n",
    "    \n",
    "    IPO avoids overfitting by using a squared hinge loss instead of sigmoid.\n",
    "    This makes the optimization more robust to noisy preferences.\n",
    "    \n",
    "    Loss = ((log π(y_w|x)/π_ref(y_w|x) - log π(y_l|x)/π_ref(y_l|x)) - 1/2β)²\n",
    "    \n",
    "    Reference: https://arxiv.org/abs/2310.12036\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, beta: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.target = 1.0 / (2 * beta)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        chosen_log_ratio: torch.Tensor,\n",
    "        rejected_log_ratio: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute IPO loss.\n",
    "        \n",
    "        Args:\n",
    "            chosen_log_ratio: log π(y_w|x) - log π_ref(y_w|x)\n",
    "            rejected_log_ratio: log π(y_l|x) - log π_ref(y_l|x)\n",
    "        \n",
    "        Returns:\n",
    "            IPO loss\n",
    "        \"\"\"\n",
    "        log_ratio_diff = chosen_log_ratio - rejected_log_ratio\n",
    "        loss = (log_ratio_diff - self.target) ** 2\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class KTOLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Kahneman-Tversky Optimization (KTO) Loss.\n",
    "    \n",
    "    KTO doesn't require paired preferences - it works with just\n",
    "    positive/negative labels for individual responses.\n",
    "    Based on prospect theory's asymmetric value function.\n",
    "    \n",
    "    Reference: https://arxiv.org/abs/2402.01306\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        beta: float = 0.1,\n",
    "        desirable_weight: float = 1.0,\n",
    "        undesirable_weight: float = 1.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.desirable_weight = desirable_weight\n",
    "        self.undesirable_weight = undesirable_weight\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        log_ratios: torch.Tensor,\n",
    "        is_desirable: torch.Tensor,\n",
    "        kl_penalty: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute KTO loss.\n",
    "        \n",
    "        Args:\n",
    "            log_ratios: log π(y|x) - log π_ref(y|x) for each response\n",
    "            is_desirable: Binary mask (1 for chosen, 0 for rejected)\n",
    "            kl_penalty: KL divergence term for regularization\n",
    "        \n",
    "        Returns:\n",
    "            KTO loss\n",
    "        \"\"\"\n",
    "        # Compute rewards\n",
    "        rewards = self.beta * log_ratios - kl_penalty\n",
    "        \n",
    "        # Desirable: maximize log sigmoid(reward)\n",
    "        # Undesirable: maximize log(1 - sigmoid(reward))\n",
    "        desirable_loss = -F.logsigmoid(rewards)\n",
    "        undesirable_loss = -F.logsigmoid(-rewards)\n",
    "        \n",
    "        # Weight and combine\n",
    "        loss = (\n",
    "            is_desirable * self.desirable_weight * desirable_loss +\n",
    "            (1 - is_desirable) * self.undesirable_weight * undesirable_loss\n",
    "        )\n",
    "        \n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class ORPOLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Odds Ratio Preference Optimization (ORPO) Loss.\n",
    "    \n",
    "    ORPO combines SFT and preference optimization into a single loss,\n",
    "    eliminating the need for a separate reference model.\n",
    "    \n",
    "    Loss = SFT_loss - λ * log(sigmoid(log(odds_w / odds_l)))\n",
    "    \n",
    "    Where odds = p / (1 - p) for each response.\n",
    "    \n",
    "    Reference: https://arxiv.org/abs/2403.07691\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lambda_orpo: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.lambda_orpo = lambda_orpo\n",
    "    \n",
    "    def compute_odds(self, log_probs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute odds from log probabilities.\n",
    "        \n",
    "        odds = p / (1 - p) = exp(log_p) / (1 - exp(log_p))\n",
    "        log_odds = log_p - log(1 - exp(log_p))\n",
    "        \"\"\"\n",
    "        # Numerically stable computation\n",
    "        log_odds = log_probs - torch.log1p(-torch.exp(log_probs.clamp(max=-1e-6)))\n",
    "        return log_odds\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        chosen_logits: torch.Tensor,\n",
    "        rejected_logits: torch.Tensor,\n",
    "        chosen_labels: torch.Tensor,\n",
    "        rejected_labels: torch.Tensor,\n",
    "        chosen_mask: torch.Tensor,\n",
    "        rejected_mask: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Compute ORPO loss.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (total_loss, metrics_dict)\n",
    "        \"\"\"\n",
    "        # SFT loss on chosen responses\n",
    "        shift_logits = chosen_logits[:, :-1, :].contiguous()\n",
    "        shift_labels = chosen_labels[:, 1:].contiguous()\n",
    "        shift_mask = chosen_mask[:, 1:].contiguous()\n",
    "        \n",
    "        sft_loss = F.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "            reduction='none'\n",
    "        ).view(shift_labels.shape)\n",
    "        sft_loss = (sft_loss * shift_mask).sum() / shift_mask.sum()\n",
    "        \n",
    "        # Compute log probabilities\n",
    "        chosen_log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "        chosen_per_token = torch.gather(\n",
    "            chosen_log_probs, -1, shift_labels.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "        chosen_avg_log_prob = (chosen_per_token * shift_mask).sum(-1) / shift_mask.sum(-1)\n",
    "        \n",
    "        # Same for rejected\n",
    "        shift_rej_logits = rejected_logits[:, :-1, :].contiguous()\n",
    "        shift_rej_labels = rejected_labels[:, 1:].contiguous()\n",
    "        shift_rej_mask = rejected_mask[:, 1:].contiguous()\n",
    "        \n",
    "        rejected_log_probs = F.log_softmax(shift_rej_logits, dim=-1)\n",
    "        rejected_per_token = torch.gather(\n",
    "            rejected_log_probs, -1, shift_rej_labels.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "        rejected_avg_log_prob = (rejected_per_token * shift_rej_mask).sum(-1) / shift_rej_mask.sum(-1)\n",
    "        \n",
    "        # Compute log odds ratio\n",
    "        chosen_log_odds = self.compute_odds(chosen_avg_log_prob)\n",
    "        rejected_log_odds = self.compute_odds(rejected_avg_log_prob)\n",
    "        log_odds_ratio = chosen_log_odds - rejected_log_odds\n",
    "        \n",
    "        # ORPO preference loss\n",
    "        orpo_loss = -F.logsigmoid(log_odds_ratio).mean()\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = sft_loss + self.lambda_orpo * orpo_loss\n",
    "        \n",
    "        metrics = {\n",
    "            \"sft_loss\": sft_loss.detach(),\n",
    "            \"orpo_loss\": orpo_loss.detach(),\n",
    "            \"total_loss\": total_loss.detach(),\n",
    "            \"log_odds_ratio\": log_odds_ratio.mean().detach(),\n",
    "            \"accuracy\": (log_odds_ratio > 0).float().mean(),\n",
    "        }\n",
    "        \n",
    "        return total_loss, metrics\n",
    "\n",
    "\n",
    "# Compare the loss functions\n",
    "print(\"DPO Variants Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate synthetic data\n",
    "batch_size = 8\n",
    "chosen_log_ratio = torch.randn(batch_size) + 0.5  # Slightly positive\n",
    "rejected_log_ratio = torch.randn(batch_size) - 0.5  # Slightly negative\n",
    "\n",
    "# Standard DPO\n",
    "dpo_logits = 0.1 * (chosen_log_ratio - rejected_log_ratio)\n",
    "dpo_loss = -F.logsigmoid(dpo_logits).mean()\n",
    "\n",
    "# IPO\n",
    "ipo = IPOLoss(beta=0.1)\n",
    "ipo_loss = ipo(chosen_log_ratio, rejected_log_ratio)\n",
    "\n",
    "# KTO (simplified)\n",
    "kto = KTOLoss(beta=0.1)\n",
    "log_ratios = torch.cat([chosen_log_ratio, rejected_log_ratio])\n",
    "is_desirable = torch.cat([torch.ones(batch_size), torch.zeros(batch_size)])\n",
    "kl_penalty = torch.zeros_like(log_ratios)\n",
    "kto_loss = kto(log_ratios, is_desirable, kl_penalty)\n",
    "\n",
    "print(f\"DPO Loss: {dpo_loss.item():.4f}\")\n",
    "print(f\"IPO Loss: {ipo_loss.item():.4f}\")\n",
    "print(f\"KTO Loss: {kto_loss.item():.4f}\")\n",
    "\n",
    "print(\"\\nKey Differences:\")\n",
    "print(\"- DPO: Uses sigmoid, requires paired preferences\")\n",
    "print(\"- IPO: Uses squared loss, more robust to noise\")\n",
    "print(\"- KTO: Works with unpaired data, asymmetric weighting\")\n",
    "print(\"- ORPO: Combines SFT + preference, no reference model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3 Solution: Preference Dataset Generator\n",
    "\n",
    "**Task:** Create a utility to generate preference pairs from raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Callable, Optional\n",
    "import random\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class PreferencePair:\n",
    "    \"\"\"A single preference comparison.\"\"\"\n",
    "    prompt: str\n",
    "    chosen: str\n",
    "    rejected: str\n",
    "    chosen_score: Optional[float] = None\n",
    "    rejected_score: Optional[float] = None\n",
    "    metadata: Optional[Dict] = None\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"prompt\": self.prompt,\n",
    "            \"chosen\": self.chosen,\n",
    "            \"rejected\": self.rejected,\n",
    "            \"chosen_score\": self.chosen_score,\n",
    "            \"rejected_score\": self.rejected_score,\n",
    "            \"metadata\": self.metadata or {},\n",
    "        }\n",
    "\n",
    "\n",
    "class PreferenceDatasetGenerator:\n",
    "    \"\"\"\n",
    "    Generate preference pairs for DPO training.\n",
    "    \n",
    "    Supports multiple strategies:\n",
    "    - Score-based pairing (if responses have scores)\n",
    "    - Length-based heuristics\n",
    "    - Quality rubric-based\n",
    "    - LLM-as-judge\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_score_diff: float = 0.5):\n",
    "        \"\"\"\n",
    "        Initialize generator.\n",
    "        \n",
    "        Args:\n",
    "            min_score_diff: Minimum score difference for valid pairs\n",
    "        \"\"\"\n",
    "        self.min_score_diff = min_score_diff\n",
    "        self.quality_scorers: List[Callable[[str, str], float]] = []\n",
    "    \n",
    "    def add_scorer(self, scorer: Callable[[str, str], float]):\n",
    "        \"\"\"\n",
    "        Add a quality scoring function.\n",
    "        \n",
    "        Args:\n",
    "            scorer: Function(prompt, response) -> score\n",
    "        \"\"\"\n",
    "        self.quality_scorers.append(scorer)\n",
    "    \n",
    "    def compute_quality_score(\n",
    "        self, \n",
    "        prompt: str, \n",
    "        response: str\n",
    "    ) -> float:\n",
    "        \"\"\"Compute aggregate quality score using all scorers.\"\"\"\n",
    "        if not self.quality_scorers:\n",
    "            return 0.0\n",
    "        scores = [scorer(prompt, response) for scorer in self.quality_scorers]\n",
    "        return sum(scores) / len(scores)\n",
    "    \n",
    "    def from_scored_responses(\n",
    "        self,\n",
    "        data: List[Dict],\n",
    "        prompt_key: str = \"prompt\",\n",
    "        response_key: str = \"response\",\n",
    "        score_key: str = \"score\",\n",
    "    ) -> List[PreferencePair]:\n",
    "        \"\"\"\n",
    "        Generate pairs from responses with existing scores.\n",
    "        \n",
    "        Groups responses by prompt, then pairs high vs low scoring.\n",
    "        \"\"\"\n",
    "        # Group by prompt\n",
    "        by_prompt: Dict[str, List[Dict]] = {}\n",
    "        for item in data:\n",
    "            prompt = item[prompt_key]\n",
    "            if prompt not in by_prompt:\n",
    "                by_prompt[prompt] = []\n",
    "            by_prompt[prompt].append(item)\n",
    "        \n",
    "        # Generate pairs\n",
    "        pairs = []\n",
    "        for prompt, responses in by_prompt.items():\n",
    "            if len(responses) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Sort by score\n",
    "            sorted_resp = sorted(\n",
    "                responses, \n",
    "                key=lambda x: x.get(score_key, 0), \n",
    "                reverse=True\n",
    "            )\n",
    "            \n",
    "            # Pair best with worst (and potentially others)\n",
    "            for i in range(len(sorted_resp) // 2):\n",
    "                chosen = sorted_resp[i]\n",
    "                rejected = sorted_resp[-(i+1)]\n",
    "                \n",
    "                score_diff = chosen.get(score_key, 0) - rejected.get(score_key, 0)\n",
    "                if score_diff >= self.min_score_diff:\n",
    "                    pairs.append(PreferencePair(\n",
    "                        prompt=prompt,\n",
    "                        chosen=chosen[response_key],\n",
    "                        rejected=rejected[response_key],\n",
    "                        chosen_score=chosen.get(score_key),\n",
    "                        rejected_score=rejected.get(score_key),\n",
    "                    ))\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def from_single_responses(\n",
    "        self,\n",
    "        data: List[Dict],\n",
    "        prompt_key: str = \"prompt\",\n",
    "        response_key: str = \"response\",\n",
    "        rejection_strategy: str = \"perturb\",\n",
    "    ) -> List[PreferencePair]:\n",
    "        \"\"\"\n",
    "        Generate pairs from single good responses by creating rejections.\n",
    "        \n",
    "        Args:\n",
    "            data: List of {prompt, response} items\n",
    "            rejection_strategy: How to create rejected responses\n",
    "                - \"perturb\": Add noise/errors to good response\n",
    "                - \"truncate\": Cut the response short\n",
    "                - \"shuffle\": Randomly shuffle sentences\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        \n",
    "        for item in data:\n",
    "            prompt = item[prompt_key]\n",
    "            chosen = item[response_key]\n",
    "            \n",
    "            # Generate rejected response\n",
    "            if rejection_strategy == \"truncate\":\n",
    "                # Cut to 30% of original length\n",
    "                words = chosen.split()\n",
    "                cut_point = max(1, len(words) // 3)\n",
    "                rejected = \" \".join(words[:cut_point])\n",
    "                \n",
    "            elif rejection_strategy == \"shuffle\":\n",
    "                # Shuffle sentences\n",
    "                sentences = chosen.split(\". \")\n",
    "                if len(sentences) > 1:\n",
    "                    random.shuffle(sentences)\n",
    "                    rejected = \". \".join(sentences)\n",
    "                else:\n",
    "                    rejected = chosen[::-1]  # Reverse if single sentence\n",
    "                    \n",
    "            elif rejection_strategy == \"perturb\":\n",
    "                # Add filler words and make it less helpful\n",
    "                fillers = [\"um\", \"like\", \"basically\", \"I guess\", \"maybe\"]\n",
    "                words = chosen.split()\n",
    "                rejected_words = []\n",
    "                for i, word in enumerate(words):\n",
    "                    rejected_words.append(word)\n",
    "                    if i % 5 == 0 and random.random() > 0.5:\n",
    "                        rejected_words.append(random.choice(fillers))\n",
    "                rejected = \" \".join(rejected_words)\n",
    "            else:\n",
    "                rejected = \"I don't know.\"\n",
    "            \n",
    "            pairs.append(PreferencePair(\n",
    "                prompt=prompt,\n",
    "                chosen=chosen,\n",
    "                rejected=rejected,\n",
    "                metadata={\"strategy\": rejection_strategy},\n",
    "            ))\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def validate_pairs(self, pairs: List[PreferencePair]) -> List[PreferencePair]:\n",
    "        \"\"\"Filter out invalid pairs.\"\"\"\n",
    "        valid = []\n",
    "        for pair in pairs:\n",
    "            # Check minimum lengths\n",
    "            if len(pair.chosen) < 10 or len(pair.rejected) < 10:\n",
    "                continue\n",
    "            \n",
    "            # Check they're not identical\n",
    "            if pair.chosen.strip() == pair.rejected.strip():\n",
    "                continue\n",
    "            \n",
    "            valid.append(pair)\n",
    "        \n",
    "        return valid\n",
    "    \n",
    "    def to_huggingface_format(\n",
    "        self, \n",
    "        pairs: List[PreferencePair]\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Convert to HuggingFace TRL format.\n",
    "        \n",
    "        Returns list of {prompt, chosen, rejected} dicts.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"prompt\": p.prompt,\n",
    "                \"chosen\": p.chosen,\n",
    "                \"rejected\": p.rejected,\n",
    "            }\n",
    "            for p in pairs\n",
    "        ]\n",
    "\n",
    "\n",
    "# Demo the generator\n",
    "generator = PreferenceDatasetGenerator(min_score_diff=0.3)\n",
    "\n",
    "# Add some quality scorers\n",
    "def length_scorer(prompt: str, response: str) -> float:\n",
    "    \"\"\"Score based on response length (prefer longer, up to a point).\"\"\"\n",
    "    words = len(response.split())\n",
    "    if words < 10:\n",
    "        return 0.2\n",
    "    elif words < 50:\n",
    "        return 0.5 + words * 0.01\n",
    "    elif words < 200:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.8  # Too long\n",
    "\n",
    "def specificity_scorer(prompt: str, response: str) -> float:\n",
    "    \"\"\"Score based on specificity (prefer concrete details).\"\"\"\n",
    "    vague_words = [\"thing\", \"stuff\", \"something\", \"basically\", \"just\"]\n",
    "    response_lower = response.lower()\n",
    "    vague_count = sum(1 for w in vague_words if w in response_lower)\n",
    "    return max(0.1, 1.0 - vague_count * 0.2)\n",
    "\n",
    "generator.add_scorer(length_scorer)\n",
    "generator.add_scorer(specificity_scorer)\n",
    "\n",
    "# Test with sample data\n",
    "sample_data = [\n",
    "    {\n",
    "        \"prompt\": \"What is machine learning?\",\n",
    "        \"response\": \"Machine learning is a comprehensive field of artificial intelligence that enables computer systems to automatically learn and improve from experience without being explicitly programmed. It focuses on developing algorithms that can access data, learn from it, and make predictions or decisions.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Explain Python.\",\n",
    "        \"response\": \"Python is a high-level, interpreted programming language known for its clear syntax and readability. It supports multiple programming paradigms including procedural, object-oriented, and functional programming.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Generate pairs using truncation strategy\n",
    "pairs = generator.from_single_responses(\n",
    "    sample_data,\n",
    "    rejection_strategy=\"truncate\"\n",
    ")\n",
    "\n",
    "print(\"Generated Preference Pairs:\")\n",
    "print(\"=\" * 60)\n",
    "for i, pair in enumerate(pairs):\n",
    "    print(f\"\\nPair {i+1}:\")\n",
    "    print(f\"Prompt: {pair.prompt}\")\n",
    "    print(f\"Chosen ({len(pair.chosen)} chars): {pair.chosen[:100]}...\")\n",
    "    print(f\"Rejected ({len(pair.rejected)} chars): {pair.rejected[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "These solutions demonstrate:\n",
    "\n",
    "1. **DPO Loss**: Core implementation with log probability computation\n",
    "\n",
    "2. **DPO Variants**: IPO, KTO, and ORPO for different scenarios\n",
    "\n",
    "3. **Dataset Generation**: Creating preference pairs from various data sources\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **DPO** is simpler than RLHF but equally effective\n",
    "- **β (beta)** controls preference strength - start with 0.1\n",
    "- **ORPO** eliminates the reference model requirement\n",
    "- **KTO** works without paired preferences\n",
    "- **Data quality** is crucial for preference learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}