{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1.2: DoRA Comparison - Solutions\n",
    "\n",
    "This notebook contains complete solutions for all exercises in the DoRA lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Understanding Weight Decomposition\n",
    "\n",
    "**Task**: Implement magnitude and direction decomposition from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def decompose_weight(W: torch.Tensor) -> tuple:\n",
    "    \"\"\"\n",
    "    Decompose weight matrix into magnitude and direction.\n",
    "    \n",
    "    W = m * (W / ||W||_c) = m * d\n",
    "    \n",
    "    where:\n",
    "    - m = magnitude (learned scalar per column)\n",
    "    - d = direction (unit-normalized columns)\n",
    "    \"\"\"\n",
    "    # Compute column-wise L2 norms (magnitude)\n",
    "    magnitude = torch.norm(W, p=2, dim=0, keepdim=True)\n",
    "    \n",
    "    # Compute direction (normalized weight)\n",
    "    direction = W / (magnitude + 1e-8)  # Add epsilon for numerical stability\n",
    "    \n",
    "    return magnitude, direction\n",
    "\n",
    "# Test\n",
    "W = torch.randn(768, 768)\n",
    "m, d = decompose_weight(W)\n",
    "\n",
    "# Verify: W ≈ m * d\n",
    "W_reconstructed = m * d\n",
    "print(f\"Reconstruction error: {torch.mean(torch.abs(W - W_reconstructed)):.8f}\")\n",
    "\n",
    "# Verify: d has unit norm columns\n",
    "column_norms = torch.norm(d, p=2, dim=0)\n",
    "print(f\"Direction column norms (should be ~1): {column_norms[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Full DoRA Layer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete DoRA implementation with proper initialization.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        rank: int = 8,\n",
    "        alpha: float = 16.0,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # Frozen base weight (simulated - in practice this comes from pretrained model)\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features) * 0.02, requires_grad=False)\n",
    "        \n",
    "        # Compute initial magnitude from base weight\n",
    "        with torch.no_grad():\n",
    "            initial_magnitude = torch.norm(self.weight, p=2, dim=0, keepdim=True)\n",
    "        \n",
    "        # Trainable magnitude parameter (initialized from base weight)\n",
    "        self.magnitude = nn.Parameter(initial_magnitude.clone())\n",
    "        \n",
    "        # LoRA components for direction update\n",
    "        self.lora_A = nn.Parameter(torch.randn(rank, in_features) * 0.01)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Compute direction update from LoRA\n",
    "        lora_delta = (self.lora_B @ self.lora_A) * self.scaling\n",
    "        \n",
    "        # Updated weight direction\n",
    "        W_updated = self.weight + lora_delta\n",
    "        \n",
    "        # Normalize to get direction (column-wise)\n",
    "        direction = W_updated / (torch.norm(W_updated, p=2, dim=0, keepdim=True) + 1e-8)\n",
    "        \n",
    "        # Apply learned magnitude to direction\n",
    "        W_final = self.magnitude * direction\n",
    "        \n",
    "        # Apply dropout and linear transformation\n",
    "        x = self.dropout(x)\n",
    "        return x @ W_final.T\n",
    "    \n",
    "    def get_trainable_params(self):\n",
    "        \"\"\"Count trainable parameters.\"\"\"\n",
    "        return sum(p.numel() for p in [self.magnitude, self.lora_A, self.lora_B])\n",
    "\n",
    "# Test\n",
    "dora = DoRALinear(768, 768, rank=8)\n",
    "x = torch.randn(4, 768)\n",
    "out = dora(x)\n",
    "print(f\"Input: {x.shape}, Output: {out.shape}\")\n",
    "print(f\"Trainable params: {dora.get_trainable_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Gradient Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_gradient_flow():\n",
    "    \"\"\"\n",
    "    Compare gradient magnitudes in LoRA vs DoRA.\n",
    "    \"\"\"\n",
    "    # Standard LoRA\n",
    "    class LoRALinear(nn.Module):\n",
    "        def __init__(self, in_f, out_f, rank=8):\n",
    "            super().__init__()\n",
    "            self.weight = nn.Parameter(torch.randn(out_f, in_f) * 0.02, requires_grad=False)\n",
    "            self.lora_A = nn.Parameter(torch.randn(rank, in_f) * 0.01)\n",
    "            self.lora_B = nn.Parameter(torch.zeros(out_f, rank))\n",
    "            \n",
    "        def forward(self, x):\n",
    "            base = x @ self.weight.T\n",
    "            lora = x @ self.lora_A.T @ self.lora_B.T\n",
    "            return base + lora\n",
    "    \n",
    "    # Create both\n",
    "    lora = LoRALinear(768, 768, rank=8)\n",
    "    dora = DoRALinear(768, 768, rank=8)\n",
    "    \n",
    "    # Forward and backward\n",
    "    x = torch.randn(4, 768)\n",
    "    target = torch.randn(4, 768)\n",
    "    \n",
    "    # LoRA gradients\n",
    "    lora_out = lora(x)\n",
    "    lora_loss = ((lora_out - target) ** 2).mean()\n",
    "    lora_loss.backward()\n",
    "    \n",
    "    lora_A_grad = lora.lora_A.grad.norm().item()\n",
    "    lora_B_grad = lora.lora_B.grad.norm().item()\n",
    "    \n",
    "    # DoRA gradients\n",
    "    dora_out = dora(x)\n",
    "    dora_loss = ((dora_out - target) ** 2).mean()\n",
    "    dora_loss.backward()\n",
    "    \n",
    "    dora_A_grad = dora.lora_A.grad.norm().item()\n",
    "    dora_B_grad = dora.lora_B.grad.norm().item()\n",
    "    dora_m_grad = dora.magnitude.grad.norm().item()\n",
    "    \n",
    "    print(\"Gradient Norms:\")\n",
    "    print(f\"  LoRA  A: {lora_A_grad:.6f}, B: {lora_B_grad:.6f}\")\n",
    "    print(f\"  DoRA  A: {dora_A_grad:.6f}, B: {dora_B_grad:.6f}, M: {dora_m_grad:.6f}\")\n",
    "    print(f\"\\nDoRA magnitude gradient provides additional learning signal!\")\n",
    "\n",
    "compare_gradient_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Training Comparison on GLUE Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset\n",
    "import copy\n",
    "\n",
    "def train_and_compare(base_model_id=\"google/bert_uncased_L-2_H-128_A-2\", epochs=3):\n",
    "    \"\"\"\n",
    "    Train LoRA vs DoRA on SST-2 sentiment classification.\n",
    "    \"\"\"\n",
    "    # Load tiny BERT for quick demo\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "    \n",
    "    # LoRA config\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"query\", \"value\"],\n",
    "        use_dora=False  # Standard LoRA\n",
    "    )\n",
    "    \n",
    "    # DoRA config (same but with use_dora=True)\n",
    "    dora_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"query\", \"value\"],\n",
    "        use_dora=True  # Enable DoRA\n",
    "    )\n",
    "    \n",
    "    # Create models\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model_id, num_labels=2\n",
    "    )\n",
    "    \n",
    "    lora_model = get_peft_model(copy.deepcopy(base_model), lora_config)\n",
    "    dora_model = get_peft_model(copy.deepcopy(base_model), dora_config)\n",
    "    \n",
    "    print(\"LoRA trainable params:\", lora_model.print_trainable_parameters())\n",
    "    print(\"DoRA trainable params:\", dora_model.print_trainable_parameters())\n",
    "    \n",
    "    # Load SST-2 subset\n",
    "    dataset = load_dataset(\"glue\", \"sst2\", split=\"train[:500]\")\n",
    "    \n",
    "    def tokenize(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"sentence\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        )\n",
    "    \n",
    "    dataset = dataset.map(tokenize, batched=True)\n",
    "    dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    \n",
    "    # Simple training loop\n",
    "    from torch.utils.data import DataLoader\n",
    "    from torch.optim import AdamW\n",
    "    \n",
    "    def train_model(model, name):\n",
    "        model.train()\n",
    "        optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "        loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "        \n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for batch in loader:\n",
    "                outputs = model(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    labels=batch[\"label\"]\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            avg_loss = epoch_loss / len(loader)\n",
    "            losses.append(avg_loss)\n",
    "            print(f\"{name} Epoch {epoch+1}: Loss = {avg_loss:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    print(\"\\n=== Training LoRA ===\")\n",
    "    lora_losses = train_model(lora_model, \"LoRA\")\n",
    "    \n",
    "    print(\"\\n=== Training DoRA ===\")\n",
    "    dora_losses = train_model(dora_model, \"DoRA\")\n",
    "    \n",
    "    return lora_losses, dora_losses\n",
    "\n",
    "# Run comparison\n",
    "# lora_losses, dora_losses = train_and_compare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Visualize Weight Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_weight_decomposition():\n",
    "    \"\"\"\n",
    "    Visualize how DoRA separates magnitude and direction updates.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    \n",
    "    # Create sample weights\n",
    "    torch.manual_seed(42)\n",
    "    W_original = torch.randn(64, 64) * 0.5\n",
    "    \n",
    "    # Simulate LoRA update\n",
    "    lora_delta = torch.randn(64, 64) * 0.1\n",
    "    W_lora = W_original + lora_delta\n",
    "    \n",
    "    # Simulate DoRA update\n",
    "    m_original = torch.norm(W_original, p=2, dim=0, keepdim=True)\n",
    "    m_updated = m_original * (1 + torch.randn(1, 64) * 0.1)  # Magnitude change\n",
    "    d_original = W_original / (m_original + 1e-8)\n",
    "    d_updated = d_original + torch.randn(64, 64) * 0.05  # Direction change\n",
    "    d_updated = d_updated / (torch.norm(d_updated, p=2, dim=0, keepdim=True) + 1e-8)\n",
    "    W_dora = m_updated * d_updated\n",
    "    \n",
    "    # Plot\n",
    "    axes[0, 0].imshow(W_original.numpy(), cmap='coolwarm', aspect='auto')\n",
    "    axes[0, 0].set_title('Original W')\n",
    "    \n",
    "    axes[0, 1].imshow(W_lora.numpy(), cmap='coolwarm', aspect='auto')\n",
    "    axes[0, 1].set_title('W after LoRA')\n",
    "    \n",
    "    axes[0, 2].imshow(W_dora.numpy(), cmap='coolwarm', aspect='auto')\n",
    "    axes[0, 2].set_title('W after DoRA')\n",
    "    \n",
    "    # Show magnitude changes\n",
    "    axes[1, 0].bar(range(64), m_original.squeeze().numpy(), alpha=0.7, label='Original')\n",
    "    axes[1, 0].set_title('Column Magnitudes (Original)')\n",
    "    axes[1, 0].set_xlabel('Column')\n",
    "    \n",
    "    # LoRA magnitude change\n",
    "    m_lora = torch.norm(W_lora, p=2, dim=0)\n",
    "    axes[1, 1].bar(range(64), m_lora.numpy(), alpha=0.7, color='orange')\n",
    "    axes[1, 1].set_title('Magnitudes after LoRA (uncontrolled)')\n",
    "    axes[1, 1].set_xlabel('Column')\n",
    "    \n",
    "    # DoRA magnitude change\n",
    "    axes[1, 2].bar(range(64), m_updated.squeeze().numpy(), alpha=0.7, color='green')\n",
    "    axes[1, 2].set_title('Magnitudes after DoRA (explicitly learned)')\n",
    "    axes[1, 2].set_xlabel('Column')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dora_visualization.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nKey insight: DoRA gives explicit control over magnitude!\")\n",
    "\n",
    "visualize_weight_decomposition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights from Solutions\n",
    "\n",
    "1. **Weight Decomposition**: W = m × d where m is magnitude and d is direction\n",
    "2. **DoRA Advantage**: Separate learning of magnitude provides additional gradient signal\n",
    "3. **Training Stability**: DoRA often shows smoother loss curves\n",
    "4. **Performance**: ~3.7 point improvement on commonsense reasoning tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
