# Model Quantization: Shrinking AI Models Without Losing Intelligence

## Introduction

Quantization is the process of reducing the precision of neural network weights and activations from higher-precision formats (like FP32 or FP16) to lower-precision formats (INT8, INT4, or even FP4). This dramatically reduces memory usage and can significantly speed up inference.

## Why Quantization Matters

### Memory Reduction
A 70B parameter model in different precisions:
- FP32 (32-bit): 280 GB
- FP16 (16-bit): 140 GB
- INT8 (8-bit): 70 GB
- INT4 (4-bit): 35 GB

### Speed Improvements
Lower precision means:
- More data fits in cache
- Faster memory bandwidth utilization
- Specialized hardware instructions (Tensor Cores)

## Quantization Types

### Post-Training Quantization (PTQ)
Quantize a pre-trained model without additional training:
- Fastest to implement
- May have accuracy loss
- Requires calibration data

### Quantization-Aware Training (QAT)
Simulate quantization during training:
- Best accuracy preservation
- Requires training infrastructure
- More time-consuming

## Common Quantization Formats

### INT8 Quantization
Symmetric quantization:
```
q = round(x / scale)
x_reconstructed = q * scale
```

Asymmetric quantization:
```
q = round(x / scale) + zero_point
x_reconstructed = (q - zero_point) * scale
```

### INT4/NF4 Quantization
NormalFloat4 (NF4) - optimal for normally distributed weights:
- Used in QLoRA
- Better preserves model quality than uniform INT4
- Introduced by QLoRA paper

### GPTQ
Post-training quantization for transformer models:
- Layer-by-layer quantization
- Uses calibration data
- Popular for deployment

```python
from transformers import GPTQConfig

gptq_config = GPTQConfig(
    bits=4,
    dataset="c4",
    tokenizer=tokenizer
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=gptq_config,
    device_map="auto"
)
```

### AWQ (Activation-aware Weight Quantization)
Weights important for activations are preserved:
- Better than GPTQ for many models
- Faster quantization process
- Good deployment format

```python
from transformers import AwqConfig

awq_config = AwqConfig(
    bits=4,
    fuse_max_seq_len=512,
    do_fuse=True
)
```

## GGUF Format

### What is GGUF?
GGUF (GPT-Generated Unified Format) is a file format for storing quantized models:
- Successor to GGML
- Used by llama.cpp, Ollama
- Supports various quantization levels

### GGUF Quantization Levels
| Name | Bits | Description |
|------|------|-------------|
| Q2_K | 2.5 | Smallest, lowest quality |
| Q3_K_S | 3.0 | Very small |
| Q4_K_M | 4.5 | Good balance |
| Q5_K_M | 5.5 | Better quality |
| Q6_K | 6.5 | Near original quality |
| Q8_0 | 8.0 | Best quantized quality |

### Using with Ollama
```bash
# Pull quantized model
ollama pull llama3.1:70b-instruct-q4_K_M

# Model uses ~35GB with Q4 quantization
```

## Blackwell FP4: The Next Generation

### What is FP4?
DGX Spark's Blackwell architecture introduces FP4:
- 4-bit floating point (not integer!)
- Maintains dynamic range unlike INT4
- Native hardware support

### FP4 vs INT4
```
INT4:  Fixed-point, 16 discrete values
FP4:   Floating-point, preserves magnitude structure
```

### Performance on DGX Spark
- 1 PFLOP at FP4 precision
- 2x compute vs FP8
- 4x memory efficiency vs FP16

## Quantization in Practice

### Using bitsandbytes
```python
from transformers import BitsAndBytesConfig

# 8-bit quantization
config_8bit = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0
)

# 4-bit quantization (NF4)
config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True
)

model = AutoModelForCausalLM.from_pretrained(
    "model_name",
    quantization_config=config_4bit,
    device_map="auto"
)
```

### Calibration Data
Good calibration data is crucial for PTQ:
```python
# Use representative samples
calibration_data = [
    "The capital of France is Paris.",
    "Machine learning is a subset of AI.",
    # ... more diverse samples
]

# GPTQ uses this for optimal quantization
```

## Quality Preservation Techniques

### Mixed-Precision Quantization
Keep sensitive layers at higher precision:
- First and last layers
- Attention layers
- Layer norm parameters

### Outlier-Aware Quantization
Handle activation outliers specially:
- LLM.int8() approach
- Keep outliers in FP16
- Quantize majority in INT8

## Measuring Quantization Quality

### Perplexity Comparison
```python
# Lower is better
perplexity_fp16 = compute_perplexity(model_fp16, eval_data)
perplexity_int4 = compute_perplexity(model_int4, eval_data)

print(f"FP16 Perplexity: {perplexity_fp16:.2f}")
print(f"INT4 Perplexity: {perplexity_int4:.2f}")
print(f"Degradation: {(perplexity_int4 - perplexity_fp16) / perplexity_fp16 * 100:.1f}%")
```

### Task-Specific Evaluation
Always evaluate on your actual use case:
- Accuracy on classification
- ROUGE/BLEU for generation
- Human evaluation for quality

## DGX Spark Quantization Strategy

### Memory-Performance Tradeoffs
With 128GB unified memory on DGX Spark:
- 7B models: No quantization needed
- 13B models: BF16 works great
- 70B models: BF16 fits! Or quantize for faster inference
- 70B+ models: Quantization recommended

### Recommended Approach
```python
# DGX Spark optimized loading
def load_model_for_dgx_spark(model_name, size):
    if size <= 13:  # Billion parameters
        return AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,  # Native Blackwell
            device_map="cuda"
        )
    elif size <= 70:
        # Can fit in BF16, but quantize for speed
        return AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            device_map="cuda"
        )
    else:
        # Quantize for larger models
        return AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=BitsAndBytesConfig(load_in_4bit=True),
            device_map="cuda"
        )
```

## Common Mistakes

### Mistake 1: Wrong Compute Dtype
```python
# Wrong: Using float16 on Blackwell
bnb_4bit_compute_dtype=torch.float16

# Right: Use bfloat16 (native support)
bnb_4bit_compute_dtype=torch.bfloat16
```

### Mistake 2: Quantizing Everything
Not all layers should be quantized:
- Keep embeddings in higher precision
- Keep layer norms unquantized
- Be careful with output layers

### Mistake 3: Insufficient Calibration
Use diverse, representative calibration data:
- Multiple domains
- Various lengths
- Different tasks

## Conclusion

Quantization is essential for deploying large language models efficiently. DGX Spark's 128GB unified memory reduces the need for aggressive quantization, but understanding these techniques helps optimize inference speed and enables working with even larger models. The Blackwell FP4 capability is particularly exciting for future model deployments.
