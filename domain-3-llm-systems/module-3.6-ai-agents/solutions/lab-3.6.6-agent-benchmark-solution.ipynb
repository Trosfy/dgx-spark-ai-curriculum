{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.6.6: Agent Benchmark - SOLUTIONS\n",
    "\n",
    "**Complete solutions with explanations and alternative approaches**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List, Optional, Callable, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "import json\n",
    "import time\n",
    "import statistics\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1 Solution: Comprehensive Benchmark Framework\n",
    "\n",
    "**Task**: Build a complete agent evaluation framework with multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCategory(Enum):\n",
    "    \"\"\"Categories of benchmark tests.\"\"\"\n",
    "    FACTUAL = \"factual\"\n",
    "    REASONING = \"reasoning\"\n",
    "    CALCULATION = \"calculation\"\n",
    "    TOOL_USE = \"tool_use\"\n",
    "    MULTI_STEP = \"multi_step\"\n",
    "    CREATIVE = \"creative\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkTest:\n",
    "    \"\"\"A single benchmark test case.\"\"\"\n",
    "    id: str\n",
    "    question: str\n",
    "    expected_answer: str\n",
    "    category: TestCategory\n",
    "    keywords: List[str] = field(default_factory=list)\n",
    "    required_tools: List[str] = field(default_factory=list)\n",
    "    difficulty: str = \"medium\"\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TestResult:\n",
    "    \"\"\"Result of running a benchmark test.\"\"\"\n",
    "    test_id: str\n",
    "    agent_response: str\n",
    "    expected_answer: str\n",
    "    scores: Dict[str, float]\n",
    "    latency_ms: float\n",
    "    tools_used: List[str]\n",
    "    passed: bool\n",
    "    error: Optional[str] = None\n",
    "\n",
    "\n",
    "class MetricCalculator:\n",
    "    \"\"\"\n",
    "    Collection of evaluation metrics for agent responses.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def keyword_match(response: str, keywords: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate keyword match score.\n",
    "        \n",
    "        Score = (matched keywords) / (total keywords)\n",
    "        \"\"\"\n",
    "        if not keywords:\n",
    "            return 1.0\n",
    "        \n",
    "        response_lower = response.lower()\n",
    "        matched = sum(1 for kw in keywords if kw.lower() in response_lower)\n",
    "        return matched / len(keywords)\n",
    "    \n",
    "    @staticmethod\n",
    "    def exact_match(response: str, expected: str) -> float:\n",
    "        \"\"\"\n",
    "        Check for exact match (case-insensitive, whitespace-normalized).\n",
    "        \"\"\"\n",
    "        norm_response = \" \".join(response.lower().split())\n",
    "        norm_expected = \" \".join(expected.lower().split())\n",
    "        return 1.0 if norm_expected in norm_response else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def token_f1(response: str, expected: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate token-level F1 score.\n",
    "        \n",
    "        Measures overlap between response and expected tokens.\n",
    "        \"\"\"\n",
    "        response_tokens = set(response.lower().split())\n",
    "        expected_tokens = set(expected.lower().split())\n",
    "        \n",
    "        if not expected_tokens:\n",
    "            return 1.0 if not response_tokens else 0.0\n",
    "        \n",
    "        intersection = response_tokens & expected_tokens\n",
    "        \n",
    "        if not intersection:\n",
    "            return 0.0\n",
    "        \n",
    "        precision = len(intersection) / len(response_tokens) if response_tokens else 0\n",
    "        recall = len(intersection) / len(expected_tokens)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        return f1\n",
    "    \n",
    "    @staticmethod\n",
    "    def semantic_similarity(response: str, expected: str) -> float:\n",
    "        \"\"\"\n",
    "        Approximate semantic similarity using character n-gram overlap.\n",
    "        \n",
    "        Note: For production, use proper embedding-based similarity.\n",
    "        \"\"\"\n",
    "        def get_ngrams(text: str, n: int = 3) -> set:\n",
    "            text = text.lower().replace(\" \", \"\")\n",
    "            return set(text[i:i+n] for i in range(len(text)-n+1))\n",
    "        \n",
    "        response_ngrams = get_ngrams(response)\n",
    "        expected_ngrams = get_ngrams(expected)\n",
    "        \n",
    "        if not expected_ngrams or not response_ngrams:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = response_ngrams & expected_ngrams\n",
    "        union = response_ngrams | expected_ngrams\n",
    "        \n",
    "        return len(intersection) / len(union) if union else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def tool_usage_score(tools_used: List[str], required_tools: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Score based on correct tool usage.\n",
    "        \"\"\"\n",
    "        if not required_tools:\n",
    "            return 1.0\n",
    "        \n",
    "        tools_used_set = set(t.lower() for t in tools_used)\n",
    "        required_set = set(t.lower() for t in required_tools)\n",
    "        \n",
    "        # Precision: used correct tools\n",
    "        correct_used = tools_used_set & required_set\n",
    "        precision = len(correct_used) / len(tools_used_set) if tools_used_set else 0\n",
    "        \n",
    "        # Recall: used all required tools\n",
    "        recall = len(correct_used) / len(required_set)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    @staticmethod\n",
    "    def response_length_penalty(response: str, min_len: int = 10, max_len: int = 1000) -> float:\n",
    "        \"\"\"\n",
    "        Penalize responses that are too short or too long.\n",
    "        \"\"\"\n",
    "        length = len(response.split())\n",
    "        \n",
    "        if length < min_len:\n",
    "            return length / min_len\n",
    "        elif length > max_len:\n",
    "            return max_len / length\n",
    "        else:\n",
    "            return 1.0\n",
    "\n",
    "\n",
    "class AgentBenchmark:\n",
    "    \"\"\"\n",
    "    Comprehensive agent benchmarking framework.\n",
    "    \n",
    "    Features:\n",
    "    - Multiple evaluation metrics\n",
    "    - Category-based testing\n",
    "    - Performance tracking\n",
    "    - Comparative analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str = \"AgentBenchmark\", pass_threshold: float = 0.6):\n",
    "        self.name = name\n",
    "        self.pass_threshold = pass_threshold\n",
    "        self.tests: List[BenchmarkTest] = []\n",
    "        self.results: List[TestResult] = []\n",
    "        self.metrics = MetricCalculator()\n",
    "    \n",
    "    def add_test(self, test: BenchmarkTest) -> None:\n",
    "        \"\"\"Add a benchmark test.\"\"\"\n",
    "        self.tests.append(test)\n",
    "    \n",
    "    def add_tests_from_dict(self, tests_data: List[Dict]) -> None:\n",
    "        \"\"\"Add multiple tests from dictionary format.\"\"\"\n",
    "        for data in tests_data:\n",
    "            test = BenchmarkTest(\n",
    "                id=data[\"id\"],\n",
    "                question=data[\"question\"],\n",
    "                expected_answer=data[\"expected_answer\"],\n",
    "                category=TestCategory(data.get(\"category\", \"factual\")),\n",
    "                keywords=data.get(\"keywords\", []),\n",
    "                required_tools=data.get(\"required_tools\", []),\n",
    "                difficulty=data.get(\"difficulty\", \"medium\"),\n",
    "            )\n",
    "            self.add_test(test)\n",
    "    \n",
    "    def evaluate_response(self, test: BenchmarkTest, response: str, \n",
    "                          tools_used: List[str], latency_ms: float) -> TestResult:\n",
    "        \"\"\"Evaluate a single response against a test.\"\"\"\n",
    "        scores = {\n",
    "            \"keyword_match\": self.metrics.keyword_match(response, test.keywords),\n",
    "            \"exact_match\": self.metrics.exact_match(response, test.expected_answer),\n",
    "            \"token_f1\": self.metrics.token_f1(response, test.expected_answer),\n",
    "            \"semantic\": self.metrics.semantic_similarity(response, test.expected_answer),\n",
    "            \"tool_usage\": self.metrics.tool_usage_score(tools_used, test.required_tools),\n",
    "            \"length_quality\": self.metrics.response_length_penalty(response),\n",
    "        }\n",
    "        \n",
    "        # Weighted overall score\n",
    "        weights = {\n",
    "            \"keyword_match\": 0.25,\n",
    "            \"exact_match\": 0.15,\n",
    "            \"token_f1\": 0.20,\n",
    "            \"semantic\": 0.15,\n",
    "            \"tool_usage\": 0.15,\n",
    "            \"length_quality\": 0.10,\n",
    "        }\n",
    "        \n",
    "        overall = sum(scores[k] * weights[k] for k in scores)\n",
    "        scores[\"overall\"] = overall\n",
    "        \n",
    "        return TestResult(\n",
    "            test_id=test.id,\n",
    "            agent_response=response,\n",
    "            expected_answer=test.expected_answer,\n",
    "            scores=scores,\n",
    "            latency_ms=latency_ms,\n",
    "            tools_used=tools_used,\n",
    "            passed=overall >= self.pass_threshold,\n",
    "        )\n",
    "    \n",
    "    def run_benchmark(self, agent_fn: Callable[[str], Tuple[str, List[str]]],\n",
    "                      categories: Optional[List[TestCategory]] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run benchmark against an agent.\n",
    "        \n",
    "        Args:\n",
    "            agent_fn: Function that takes question and returns (response, tools_used)\n",
    "            categories: Filter to specific categories (None = all)\n",
    "            \n",
    "        Returns:\n",
    "            Benchmark results summary\n",
    "        \"\"\"\n",
    "        tests = self.tests\n",
    "        if categories:\n",
    "            tests = [t for t in tests if t.category in categories]\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"RUNNING BENCHMARK: {self.name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Tests: {len(tests)}\")\n",
    "        \n",
    "        self.results = []\n",
    "        \n",
    "        for i, test in enumerate(tests, 1):\n",
    "            print(f\"\\n[{i}/{len(tests)}] {test.id}: {test.question[:50]}...\")\n",
    "            \n",
    "            start_time = time.perf_counter()\n",
    "            try:\n",
    "                response, tools_used = agent_fn(test.question)\n",
    "                latency_ms = (time.perf_counter() - start_time) * 1000\n",
    "                \n",
    "                result = self.evaluate_response(test, response, tools_used, latency_ms)\n",
    "                \n",
    "            except Exception as e:\n",
    "                latency_ms = (time.perf_counter() - start_time) * 1000\n",
    "                result = TestResult(\n",
    "                    test_id=test.id,\n",
    "                    agent_response=\"\",\n",
    "                    expected_answer=test.expected_answer,\n",
    "                    scores={\"overall\": 0.0},\n",
    "                    latency_ms=latency_ms,\n",
    "                    tools_used=[],\n",
    "                    passed=False,\n",
    "                    error=str(e),\n",
    "                )\n",
    "            \n",
    "            self.results.append(result)\n",
    "            status = \"PASS\" if result.passed else \"FAIL\"\n",
    "            print(f\"  Status: {status} (score: {result.scores.get('overall', 0):.2%}, latency: {result.latency_ms:.1f}ms)\")\n",
    "        \n",
    "        return self.get_summary()\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate benchmark summary.\"\"\"\n",
    "        if not self.results:\n",
    "            return {\"error\": \"No results\"}\n",
    "        \n",
    "        passed = sum(1 for r in self.results if r.passed)\n",
    "        overall_scores = [r.scores.get(\"overall\", 0) for r in self.results]\n",
    "        latencies = [r.latency_ms for r in self.results]\n",
    "        \n",
    "        # Category breakdown\n",
    "        by_category = defaultdict(list)\n",
    "        for result, test in zip(self.results, self.tests):\n",
    "            by_category[test.category.value].append(result.scores.get(\"overall\", 0))\n",
    "        \n",
    "        category_scores = {\n",
    "            cat: statistics.mean(scores) if scores else 0\n",
    "            for cat, scores in by_category.items()\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            \"benchmark\": self.name,\n",
    "            \"total_tests\": len(self.results),\n",
    "            \"passed\": passed,\n",
    "            \"failed\": len(self.results) - passed,\n",
    "            \"pass_rate\": passed / len(self.results),\n",
    "            \"scores\": {\n",
    "                \"mean\": statistics.mean(overall_scores),\n",
    "                \"median\": statistics.median(overall_scores),\n",
    "                \"std\": statistics.stdev(overall_scores) if len(overall_scores) > 1 else 0,\n",
    "                \"min\": min(overall_scores),\n",
    "                \"max\": max(overall_scores),\n",
    "            },\n",
    "            \"latency_ms\": {\n",
    "                \"mean\": statistics.mean(latencies),\n",
    "                \"median\": statistics.median(latencies),\n",
    "                \"p95\": sorted(latencies)[int(len(latencies) * 0.95)] if latencies else 0,\n",
    "            },\n",
    "            \"by_category\": category_scores,\n",
    "        }\n",
    "\n",
    "\n",
    "# Create benchmark with test cases\n",
    "print(\"=\" * 60)\n",
    "print(\"AGENT BENCHMARK FRAMEWORK SOLUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "benchmark = AgentBenchmark(name=\"AI Agent Evaluation\", pass_threshold=0.5)\n",
    "\n",
    "# Add test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"id\": \"FACT001\",\n",
    "        \"question\": \"What is the capital of France?\",\n",
    "        \"expected_answer\": \"The capital of France is Paris.\",\n",
    "        \"category\": \"factual\",\n",
    "        \"keywords\": [\"Paris\", \"capital\", \"France\"],\n",
    "        \"difficulty\": \"easy\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"CALC001\",\n",
    "        \"question\": \"What is 25 multiplied by 4?\",\n",
    "        \"expected_answer\": \"25 multiplied by 4 equals 100.\",\n",
    "        \"category\": \"calculation\",\n",
    "        \"keywords\": [\"100\"],\n",
    "        \"required_tools\": [\"calculator\"],\n",
    "        \"difficulty\": \"easy\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"REASON001\",\n",
    "        \"question\": \"If it takes 5 machines 5 minutes to make 5 widgets, how long would it take 100 machines to make 100 widgets?\",\n",
    "        \"expected_answer\": \"It would take 5 minutes. Each machine makes 1 widget in 5 minutes, so 100 machines make 100 widgets in 5 minutes.\",\n",
    "        \"category\": \"reasoning\",\n",
    "        \"keywords\": [\"5 minutes\", \"each machine\"],\n",
    "        \"difficulty\": \"medium\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TOOL001\",\n",
    "        \"question\": \"Search for information about machine learning and summarize the key concepts.\",\n",
    "        \"expected_answer\": \"Machine learning is a subset of AI that enables systems to learn from data. Key concepts include supervised learning, unsupervised learning, and neural networks.\",\n",
    "        \"category\": \"tool_use\",\n",
    "        \"keywords\": [\"machine learning\", \"AI\", \"learning\", \"data\"],\n",
    "        \"required_tools\": [\"search\"],\n",
    "        \"difficulty\": \"medium\",\n",
    "    },\n",
    "]\n",
    "\n",
    "benchmark.add_tests_from_dict(test_cases)\n",
    "\n",
    "# Simulate agent responses\n",
    "def simulated_agent(question: str) -> Tuple[str, List[str]]:\n",
    "    \"\"\"Simulated agent for testing.\"\"\"\n",
    "    tools_used = []\n",
    "    \n",
    "    if \"capital\" in question.lower():\n",
    "        return \"Paris is the capital of France.\", []\n",
    "    elif \"multipl\" in question.lower() or \"25\" in question:\n",
    "        tools_used = [\"calculator\"]\n",
    "        return \"The result of 25 multiplied by 4 is 100.\", tools_used\n",
    "    elif \"machine\" in question.lower() and \"widget\" in question.lower():\n",
    "        return \"It takes 5 minutes. Each machine independently produces one widget in 5 minutes.\", []\n",
    "    elif \"search\" in question.lower():\n",
    "        tools_used = [\"search\"]\n",
    "        return \"Machine learning is an AI technology that allows systems to learn patterns from data. Key concepts include supervised learning, unsupervised learning, and deep neural networks.\", tools_used\n",
    "    else:\n",
    "        return \"I don't have enough information to answer this question.\", []\n",
    "\n",
    "\n",
    "# Run benchmark\n",
    "results = benchmark.run_benchmark(simulated_agent)\n",
    "\n",
    "print(f\"\\n--- Benchmark Summary ---\")\n",
    "print(f\"Pass rate: {results['pass_rate']:.1%}\")\n",
    "print(f\"Mean score: {results['scores']['mean']:.2%}\")\n",
    "print(f\"Mean latency: {results['latency_ms']['mean']:.1f}ms\")\n",
    "print(f\"\\nBy category:\")\n",
    "for cat, score in results['by_category'].items():\n",
    "    print(f\"  {cat}: {score:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2 Solution: Comparative Benchmarking\n",
    "\n",
    "**Task**: Build a framework for comparing multiple agents on the same benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComparativeBenchmark:\n",
    "    \"\"\"\n",
    "    Framework for comparing multiple agents.\n",
    "    \n",
    "    Features:\n",
    "    - Run same tests across agents\n",
    "    - Statistical comparison\n",
    "    - Ranking and visualization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, benchmark: AgentBenchmark):\n",
    "        self.benchmark = benchmark\n",
    "        self.agent_results: Dict[str, Dict] = {}\n",
    "    \n",
    "    def evaluate_agent(self, name: str, \n",
    "                       agent_fn: Callable[[str], Tuple[str, List[str]]]) -> Dict:\n",
    "        \"\"\"Evaluate a single agent.\"\"\"\n",
    "        print(f\"\\n>>> Evaluating: {name}\")\n",
    "        results = self.benchmark.run_benchmark(agent_fn)\n",
    "        self.agent_results[name] = results\n",
    "        return results\n",
    "    \n",
    "    def compare(self) -> Dict[str, Any]:\n",
    "        \"\"\"Compare all evaluated agents.\"\"\"\n",
    "        if not self.agent_results:\n",
    "            return {\"error\": \"No agents evaluated\"}\n",
    "        \n",
    "        # Rank by mean score\n",
    "        rankings = sorted(\n",
    "            self.agent_results.items(),\n",
    "            key=lambda x: x[1][\"scores\"][\"mean\"],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        comparison = {\n",
    "            \"agents_compared\": len(self.agent_results),\n",
    "            \"rankings\": [\n",
    "                {\n",
    "                    \"rank\": i + 1,\n",
    "                    \"agent\": name,\n",
    "                    \"mean_score\": results[\"scores\"][\"mean\"],\n",
    "                    \"pass_rate\": results[\"pass_rate\"],\n",
    "                    \"mean_latency_ms\": results[\"latency_ms\"][\"mean\"],\n",
    "                }\n",
    "                for i, (name, results) in enumerate(rankings)\n",
    "            ],\n",
    "            \"best_agent\": rankings[0][0] if rankings else None,\n",
    "            \"score_spread\": {\n",
    "                \"best\": rankings[0][1][\"scores\"][\"mean\"] if rankings else 0,\n",
    "                \"worst\": rankings[-1][1][\"scores\"][\"mean\"] if rankings else 0,\n",
    "                \"difference\": (\n",
    "                    rankings[0][1][\"scores\"][\"mean\"] - rankings[-1][1][\"scores\"][\"mean\"]\n",
    "                ) if rankings else 0,\n",
    "            },\n",
    "            \"category_winners\": self._get_category_winners(),\n",
    "        }\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def _get_category_winners(self) -> Dict[str, str]:\n",
    "        \"\"\"Find best agent for each category.\"\"\"\n",
    "        categories = set()\n",
    "        for results in self.agent_results.values():\n",
    "            categories.update(results.get(\"by_category\", {}).keys())\n",
    "        \n",
    "        winners = {}\n",
    "        for cat in categories:\n",
    "            best_agent = None\n",
    "            best_score = -1\n",
    "            for name, results in self.agent_results.items():\n",
    "                score = results.get(\"by_category\", {}).get(cat, 0)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_agent = name\n",
    "            winners[cat] = best_agent\n",
    "        \n",
    "        return winners\n",
    "    \n",
    "    def generate_report(self) -> str:\n",
    "        \"\"\"Generate comparison report.\"\"\"\n",
    "        comparison = self.compare()\n",
    "        \n",
    "        lines = [\n",
    "            \"=\" * 60,\n",
    "            \"AGENT COMPARISON REPORT\",\n",
    "            \"=\" * 60,\n",
    "            f\"\\nAgents compared: {comparison['agents_compared']}\",\n",
    "            f\"Best overall: {comparison['best_agent']}\",\n",
    "            f\"\\n--- Rankings ---\",\n",
    "        ]\n",
    "        \n",
    "        for entry in comparison[\"rankings\"]:\n",
    "            lines.append(\n",
    "                f\"{entry['rank']}. {entry['agent']}: \"\n",
    "                f\"score={entry['mean_score']:.2%}, \"\n",
    "                f\"pass={entry['pass_rate']:.0%}, \"\n",
    "                f\"latency={entry['mean_latency_ms']:.1f}ms\"\n",
    "            )\n",
    "        \n",
    "        lines.append(f\"\\n--- Category Winners ---\")\n",
    "        for cat, winner in comparison[\"category_winners\"].items():\n",
    "            lines.append(f\"  {cat}: {winner}\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# Define different agent variants\n",
    "def agent_v1(question: str) -> Tuple[str, List[str]]:\n",
    "    \"\"\"Basic agent.\"\"\"\n",
    "    time.sleep(0.01)  # Simulate processing\n",
    "    if \"capital\" in question.lower():\n",
    "        return \"Paris is the capital of France.\", []\n",
    "    elif \"25\" in question:\n",
    "        return \"100\", [\"calculator\"]\n",
    "    elif \"widget\" in question.lower():\n",
    "        return \"5 minutes\", []\n",
    "    elif \"search\" in question.lower():\n",
    "        return \"Machine learning is AI that learns from data.\", [\"search\"]\n",
    "    return \"Unknown\", []\n",
    "\n",
    "def agent_v2(question: str) -> Tuple[str, List[str]]:\n",
    "    \"\"\"Improved agent with better responses.\"\"\"\n",
    "    time.sleep(0.015)  # Slightly slower but more thorough\n",
    "    if \"capital\" in question.lower():\n",
    "        return \"The capital of France is Paris, known as the City of Light.\", []\n",
    "    elif \"25\" in question:\n",
    "        return \"25 multiplied by 4 equals 100. I used the calculator to verify.\", [\"calculator\"]\n",
    "    elif \"widget\" in question.lower():\n",
    "        return \"It takes 5 minutes. Each machine produces one widget in 5 minutes, so 100 machines can make 100 widgets simultaneously in 5 minutes.\", []\n",
    "    elif \"search\" in question.lower():\n",
    "        return \"Machine learning is a subset of AI that enables systems to learn from data. Key concepts include supervised learning, unsupervised learning, and neural networks for deep learning.\", [\"search\"]\n",
    "    return \"I don't have information about that.\", []\n",
    "\n",
    "def agent_v3(question: str) -> Tuple[str, List[str]]:\n",
    "    \"\"\"Fast but less accurate agent.\"\"\"\n",
    "    time.sleep(0.005)  # Fast\n",
    "    if \"capital\" in question.lower():\n",
    "        return \"Paris\", []  # Too brief\n",
    "    elif \"25\" in question:\n",
    "        return \"100\", []  # Forgot to use calculator\n",
    "    elif \"widget\" in question.lower():\n",
    "        return \"100 minutes\", []  # Wrong answer!\n",
    "    elif \"search\" in question.lower():\n",
    "        return \"ML is about data.\", []  # Didn't use search\n",
    "    return \"?\", []\n",
    "\n",
    "\n",
    "# Run comparative benchmark\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARATIVE BENCHMARKING SOLUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Reset benchmark\n",
    "benchmark = AgentBenchmark(name=\"Comparison Test\", pass_threshold=0.5)\n",
    "benchmark.add_tests_from_dict(test_cases)\n",
    "\n",
    "comparator = ComparativeBenchmark(benchmark)\n",
    "\n",
    "# Evaluate agents\n",
    "comparator.evaluate_agent(\"Agent-v1-Basic\", agent_v1)\n",
    "\n",
    "# Reset benchmark for next agent\n",
    "benchmark.results = []\n",
    "comparator.evaluate_agent(\"Agent-v2-Improved\", agent_v2)\n",
    "\n",
    "benchmark.results = []\n",
    "comparator.evaluate_agent(\"Agent-v3-Fast\", agent_v3)\n",
    "\n",
    "# Generate report\n",
    "print(comparator.generate_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3 Solution: Stress Testing\n",
    "\n",
    "**Task**: Build a stress testing framework for agent performance under load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StressTestResult:\n",
    "    \"\"\"Result from stress testing.\"\"\"\n",
    "    total_requests: int\n",
    "    successful: int\n",
    "    failed: int\n",
    "    total_time_s: float\n",
    "    latencies_ms: List[float]\n",
    "    errors: List[str]\n",
    "    \n",
    "    @property\n",
    "    def success_rate(self) -> float:\n",
    "        return self.successful / self.total_requests if self.total_requests else 0\n",
    "    \n",
    "    @property\n",
    "    def throughput(self) -> float:\n",
    "        return self.successful / self.total_time_s if self.total_time_s else 0\n",
    "    \n",
    "    @property\n",
    "    def avg_latency(self) -> float:\n",
    "        return statistics.mean(self.latencies_ms) if self.latencies_ms else 0\n",
    "    \n",
    "    @property\n",
    "    def p99_latency(self) -> float:\n",
    "        if not self.latencies_ms:\n",
    "            return 0\n",
    "        return sorted(self.latencies_ms)[int(len(self.latencies_ms) * 0.99)]\n",
    "\n",
    "\n",
    "class AgentStressTest:\n",
    "    \"\"\"\n",
    "    Stress testing framework for agents.\n",
    "    \n",
    "    Features:\n",
    "    - Concurrent request simulation\n",
    "    - Throughput measurement\n",
    "    - Latency profiling\n",
    "    - Error tracking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, agent_fn: Callable[[str], Tuple[str, List[str]]]):\n",
    "        self.agent_fn = agent_fn\n",
    "        self.lock = threading.Lock()\n",
    "    \n",
    "    def _run_single_request(self, question: str) -> Tuple[bool, float, Optional[str]]:\n",
    "        \"\"\"Run a single request.\"\"\"\n",
    "        start = time.perf_counter()\n",
    "        try:\n",
    "            response, _ = self.agent_fn(question)\n",
    "            latency = (time.perf_counter() - start) * 1000\n",
    "            return True, latency, None\n",
    "        except Exception as e:\n",
    "            latency = (time.perf_counter() - start) * 1000\n",
    "            return False, latency, str(e)\n",
    "    \n",
    "    def run_load_test(self, questions: List[str], \n",
    "                      concurrent_users: int = 10,\n",
    "                      requests_per_user: int = 5) -> StressTestResult:\n",
    "        \"\"\"\n",
    "        Run load test with concurrent users.\n",
    "        \n",
    "        Args:\n",
    "            questions: Pool of questions to sample from\n",
    "            concurrent_users: Number of concurrent simulated users\n",
    "            requests_per_user: Requests each user makes\n",
    "        \"\"\"\n",
    "        total_requests = concurrent_users * requests_per_user\n",
    "        print(f\"\\nRunning load test: {concurrent_users} users x {requests_per_user} requests = {total_requests} total\")\n",
    "        \n",
    "        latencies = []\n",
    "        errors = []\n",
    "        successful = 0\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=concurrent_users) as executor:\n",
    "            # Create all request tasks\n",
    "            import random\n",
    "            tasks = [\n",
    "                random.choice(questions)\n",
    "                for _ in range(total_requests)\n",
    "            ]\n",
    "            \n",
    "            futures = [executor.submit(self._run_single_request, q) for q in tasks]\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                success, latency, error = future.result()\n",
    "                with self.lock:\n",
    "                    latencies.append(latency)\n",
    "                    if success:\n",
    "                        successful += 1\n",
    "                    elif error:\n",
    "                        errors.append(error)\n",
    "        \n",
    "        total_time = time.perf_counter() - start_time\n",
    "        \n",
    "        return StressTestResult(\n",
    "            total_requests=total_requests,\n",
    "            successful=successful,\n",
    "            failed=total_requests - successful,\n",
    "            total_time_s=total_time,\n",
    "            latencies_ms=latencies,\n",
    "            errors=errors[:10],  # Keep first 10 errors\n",
    "        )\n",
    "    \n",
    "    def run_ramp_test(self, questions: List[str],\n",
    "                      user_counts: List[int] = [1, 5, 10, 20]) -> List[StressTestResult]:\n",
    "        \"\"\"\n",
    "        Run tests with increasing user counts.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for users in user_counts:\n",
    "            result = self.run_load_test(questions, concurrent_users=users, requests_per_user=3)\n",
    "            results.append(result)\n",
    "            print(f\"  {users} users: {result.throughput:.1f} req/s, {result.avg_latency:.1f}ms avg latency\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# Run stress test\n",
    "print(\"=\" * 60)\n",
    "print(\"STRESS TESTING SOLUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Calculate 100 * 5\",\n",
    "    \"Explain machine learning\",\n",
    "    \"What is 2 + 2?\",\n",
    "]\n",
    "\n",
    "stress_tester = AgentStressTest(agent_v2)\n",
    "\n",
    "# Single load test\n",
    "print(\"\\n--- Single Load Test ---\")\n",
    "result = stress_tester.run_load_test(test_questions, concurrent_users=5, requests_per_user=4)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Success rate: {result.success_rate:.1%}\")\n",
    "print(f\"  Throughput: {result.throughput:.1f} requests/second\")\n",
    "print(f\"  Avg latency: {result.avg_latency:.1f}ms\")\n",
    "print(f\"  P99 latency: {result.p99_latency:.1f}ms\")\n",
    "\n",
    "# Ramp test\n",
    "print(\"\\n--- Ramp Test ---\")\n",
    "ramp_results = stress_tester.run_ramp_test(test_questions, user_counts=[1, 2, 5, 10])\n",
    "\n",
    "print(\"\\n--- Scaling Analysis ---\")\n",
    "for i, result in enumerate(ramp_results):\n",
    "    users = [1, 2, 5, 10][i]\n",
    "    print(f\"{users} users: throughput={result.throughput:.1f}/s, latency={result.avg_latency:.1f}ms, success={result.success_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Solution: Automated Regression Testing\n",
    "\n",
    "**Task**: Build a regression testing system that tracks agent performance over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BenchmarkRun:\n",
    "    \"\"\"A single benchmark run with timestamp.\"\"\"\n",
    "    run_id: str\n",
    "    timestamp: float\n",
    "    version: str\n",
    "    results: Dict[str, Any]\n",
    "\n",
    "\n",
    "class RegressionTracker:\n",
    "    \"\"\"\n",
    "    Tracks agent performance over time.\n",
    "    \n",
    "    Features:\n",
    "    - Historical tracking\n",
    "    - Regression detection\n",
    "    - Trend analysis\n",
    "    - Alerting\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, regression_threshold: float = 0.05):\n",
    "        self.regression_threshold = regression_threshold\n",
    "        self.history: List[BenchmarkRun] = []\n",
    "        self.baseline: Optional[BenchmarkRun] = None\n",
    "    \n",
    "    def record_run(self, version: str, results: Dict[str, Any]) -> BenchmarkRun:\n",
    "        \"\"\"Record a benchmark run.\"\"\"\n",
    "        run = BenchmarkRun(\n",
    "            run_id=f\"run_{len(self.history) + 1:03d}\",\n",
    "            timestamp=time.time(),\n",
    "            version=version,\n",
    "            results=results,\n",
    "        )\n",
    "        \n",
    "        self.history.append(run)\n",
    "        \n",
    "        # Set first run as baseline if none exists\n",
    "        if self.baseline is None:\n",
    "            self.baseline = run\n",
    "        \n",
    "        return run\n",
    "    \n",
    "    def set_baseline(self, run_id: str) -> None:\n",
    "        \"\"\"Set a specific run as baseline.\"\"\"\n",
    "        for run in self.history:\n",
    "            if run.run_id == run_id:\n",
    "                self.baseline = run\n",
    "                return\n",
    "        raise ValueError(f\"Run not found: {run_id}\")\n",
    "    \n",
    "    def check_regression(self, current: BenchmarkRun) -> Dict[str, Any]:\n",
    "        \"\"\"Check for regressions against baseline.\"\"\"\n",
    "        if not self.baseline:\n",
    "            return {\"status\": \"no_baseline\"}\n",
    "        \n",
    "        baseline_score = self.baseline.results.get(\"scores\", {}).get(\"mean\", 0)\n",
    "        current_score = current.results.get(\"scores\", {}).get(\"mean\", 0)\n",
    "        \n",
    "        baseline_latency = self.baseline.results.get(\"latency_ms\", {}).get(\"mean\", 0)\n",
    "        current_latency = current.results.get(\"latency_ms\", {}).get(\"mean\", 0)\n",
    "        \n",
    "        score_change = current_score - baseline_score\n",
    "        latency_change = current_latency - baseline_latency\n",
    "        \n",
    "        regressions = []\n",
    "        \n",
    "        if score_change < -self.regression_threshold:\n",
    "            regressions.append({\n",
    "                \"metric\": \"score\",\n",
    "                \"baseline\": baseline_score,\n",
    "                \"current\": current_score,\n",
    "                \"change\": score_change,\n",
    "                \"severity\": \"high\" if score_change < -0.1 else \"medium\",\n",
    "            })\n",
    "        \n",
    "        latency_threshold = baseline_latency * 0.2  # 20% slowdown\n",
    "        if latency_change > latency_threshold:\n",
    "            regressions.append({\n",
    "                \"metric\": \"latency\",\n",
    "                \"baseline\": baseline_latency,\n",
    "                \"current\": current_latency,\n",
    "                \"change\": latency_change,\n",
    "                \"severity\": \"high\" if latency_change > latency_threshold * 2 else \"medium\",\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"regression_detected\" if regressions else \"ok\",\n",
    "            \"baseline_version\": self.baseline.version,\n",
    "            \"current_version\": current.version,\n",
    "            \"score_change\": score_change,\n",
    "            \"latency_change\": latency_change,\n",
    "            \"regressions\": regressions,\n",
    "        }\n",
    "    \n",
    "    def get_trend(self, metric: str = \"score\", window: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze trend over recent runs.\"\"\"\n",
    "        if len(self.history) < 2:\n",
    "            return {\"status\": \"insufficient_data\"}\n",
    "        \n",
    "        recent = self.history[-window:]\n",
    "        \n",
    "        if metric == \"score\":\n",
    "            values = [r.results.get(\"scores\", {}).get(\"mean\", 0) for r in recent]\n",
    "        elif metric == \"latency\":\n",
    "            values = [r.results.get(\"latency_ms\", {}).get(\"mean\", 0) for r in recent]\n",
    "        elif metric == \"pass_rate\":\n",
    "            values = [r.results.get(\"pass_rate\", 0) for r in recent]\n",
    "        else:\n",
    "            return {\"error\": f\"Unknown metric: {metric}\"}\n",
    "        \n",
    "        # Simple linear trend\n",
    "        if len(values) >= 2:\n",
    "            slope = (values[-1] - values[0]) / len(values)\n",
    "            if slope > 0.01:\n",
    "                direction = \"improving\"\n",
    "            elif slope < -0.01:\n",
    "                direction = \"degrading\"\n",
    "            else:\n",
    "                direction = \"stable\"\n",
    "        else:\n",
    "            direction = \"unknown\"\n",
    "        \n",
    "        return {\n",
    "            \"metric\": metric,\n",
    "            \"direction\": direction,\n",
    "            \"values\": values,\n",
    "            \"latest\": values[-1] if values else None,\n",
    "            \"average\": statistics.mean(values) if values else None,\n",
    "        }\n",
    "    \n",
    "    def generate_report(self) -> str:\n",
    "        \"\"\"Generate regression tracking report.\"\"\"\n",
    "        if not self.history:\n",
    "            return \"No benchmark history\"\n",
    "        \n",
    "        latest = self.history[-1]\n",
    "        regression_check = self.check_regression(latest)\n",
    "        score_trend = self.get_trend(\"score\")\n",
    "        \n",
    "        lines = [\n",
    "            \"=\" * 60,\n",
    "            \"REGRESSION TRACKING REPORT\",\n",
    "            \"=\" * 60,\n",
    "            f\"\\nTotal runs: {len(self.history)}\",\n",
    "            f\"Latest version: {latest.version}\",\n",
    "            f\"Baseline version: {self.baseline.version if self.baseline else 'None'}\",\n",
    "            f\"\\n--- Regression Check ---\",\n",
    "            f\"Status: {regression_check['status'].upper()}\",\n",
    "        ]\n",
    "        \n",
    "        if regression_check.get(\"regressions\"):\n",
    "            lines.append(\"\\nRegressions detected:\")\n",
    "            for reg in regression_check[\"regressions\"]:\n",
    "                lines.append(\n",
    "                    f\"  [{reg['severity'].upper()}] {reg['metric']}: \"\n",
    "                    f\"{reg['baseline']:.3f} -> {reg['current']:.3f} ({reg['change']:+.3f})\"\n",
    "                )\n",
    "        \n",
    "        lines.extend([\n",
    "            f\"\\n--- Trend Analysis ---\",\n",
    "            f\"Score trend: {score_trend.get('direction', 'unknown')}\",\n",
    "            f\"Latest score: {score_trend.get('latest', 0):.2%}\",\n",
    "            f\"Average score: {score_trend.get('average', 0):.2%}\",\n",
    "        ])\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# Demonstrate regression tracking\n",
    "print(\"=\" * 60)\n",
    "print(\"REGRESSION TRACKING SOLUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tracker = RegressionTracker(regression_threshold=0.05)\n",
    "\n",
    "# Simulate multiple versions\n",
    "versions = [\n",
    "    (\"v1.0.0\", {\"scores\": {\"mean\": 0.75}, \"latency_ms\": {\"mean\": 50}, \"pass_rate\": 0.8}),\n",
    "    (\"v1.1.0\", {\"scores\": {\"mean\": 0.78}, \"latency_ms\": {\"mean\": 48}, \"pass_rate\": 0.82}),\n",
    "    (\"v1.2.0\", {\"scores\": {\"mean\": 0.80}, \"latency_ms\": {\"mean\": 52}, \"pass_rate\": 0.85}),\n",
    "    (\"v1.3.0\", {\"scores\": {\"mean\": 0.72}, \"latency_ms\": {\"mean\": 75}, \"pass_rate\": 0.78}),  # Regression!\n",
    "    (\"v1.3.1\", {\"scores\": {\"mean\": 0.79}, \"latency_ms\": {\"mean\": 55}, \"pass_rate\": 0.83}),  # Fixed\n",
    "]\n",
    "\n",
    "print(\"\\n--- Recording Runs ---\")\n",
    "for version, results in versions:\n",
    "    run = tracker.record_run(version, results)\n",
    "    regression = tracker.check_regression(run)\n",
    "    status = \"REGRESSION\" if regression[\"status\"] == \"regression_detected\" else \"OK\"\n",
    "    print(f\"{version}: score={results['scores']['mean']:.2f} - {status}\")\n",
    "\n",
    "# Generate report\n",
    "print(tracker.generate_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Multiple Metrics**: Use diverse metrics (exact match, F1, semantic) for robust evaluation\n",
    "2. **Comparative Testing**: Compare agents systematically on the same benchmark\n",
    "3. **Stress Testing**: Understand performance under concurrent load\n",
    "4. **Regression Tracking**: Monitor performance over time to catch degradations\n",
    "5. **Category Analysis**: Different agents excel at different task types"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
