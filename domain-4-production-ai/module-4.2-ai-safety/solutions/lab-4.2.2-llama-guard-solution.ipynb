{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Lab 4.2.2: Llama Guard Integration - SOLUTIONS\n",
    "\n",
    "This notebook contains complete solutions for the exercises in Lab 4.2.2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise1-header",
   "metadata": {},
   "source": [
    "## Exercise 1: Create a Content Moderation Pipeline\n",
    "\n",
    "**Task**: Create a pipeline that classifies input AND output, with caching for efficiency.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "try:\n",
    "    import ollama\n",
    "    OLLAMA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OLLAMA_AVAILABLE = False\n",
    "    print(\"Ollama not available - using simulated responses\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModerationResult:\n",
    "    \"\"\"Result of a content moderation check.\"\"\"\n",
    "    is_safe: bool\n",
    "    category: Optional[str] = None\n",
    "    category_name: Optional[str] = None\n",
    "    confidence: float = 1.0\n",
    "    cached: bool = False\n",
    "    latency_ms: float = 0.0\n",
    "\n",
    "\n",
    "# Llama Guard 3 safety categories\n",
    "SAFETY_CATEGORIES = {\n",
    "    \"S1\": \"Violent Crimes\",\n",
    "    \"S2\": \"Non-Violent Crimes\",\n",
    "    \"S3\": \"Sex-Related Crimes\",\n",
    "    \"S4\": \"Child Sexual Exploitation\",\n",
    "    \"S5\": \"Defamation\",\n",
    "    \"S6\": \"Specialized Advice\",\n",
    "    \"S7\": \"Privacy\",\n",
    "    \"S8\": \"Intellectual Property\",\n",
    "    \"S9\": \"Indiscriminate Weapons\",\n",
    "    \"S10\": \"Hate\",\n",
    "    \"S11\": \"Suicide & Self-Harm\",\n",
    "    \"S12\": \"Sexual Content\",\n",
    "    \"S13\": \"Elections\",\n",
    "    \"S14\": \"Code Interpreter Abuse\",\n",
    "}\n",
    "\n",
    "\n",
    "class ContentModerationPipeline:\n",
    "    \"\"\"\n",
    "    A complete content moderation pipeline using Llama Guard.\n",
    "    \n",
    "    Features:\n",
    "    - Input and output moderation\n",
    "    - Result caching for efficiency\n",
    "    - Statistics tracking\n",
    "    - Batch processing\n",
    "    \n",
    "    Example:\n",
    "        >>> pipeline = ContentModerationPipeline()\n",
    "        >>> \n",
    "        >>> # Check input\n",
    "        >>> input_result = pipeline.moderate_input(\"How do I bake a cake?\")\n",
    "        >>> print(f\"Input safe: {input_result.is_safe}\")\n",
    "        >>>\n",
    "        >>> # Check output\n",
    "        >>> output_result = pipeline.moderate_output(\n",
    "        ...     user_message=\"How do I bake a cake?\",\n",
    "        ...     assistant_message=\"Here's a recipe...\"\n",
    "        ... )\n",
    "        >>> print(f\"Output safe: {output_result.is_safe}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"llama-guard3:8b\",\n",
    "        cache_ttl_seconds: int = 3600,\n",
    "        fail_closed: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the moderation pipeline.\n",
    "        \n",
    "        Args:\n",
    "            model: Llama Guard model to use\n",
    "            cache_ttl_seconds: How long to cache results\n",
    "            fail_closed: If True, errors result in 'unsafe'\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.cache_ttl = cache_ttl_seconds\n",
    "        self.fail_closed = fail_closed\n",
    "        \n",
    "        # Cache: hash -> (result, timestamp)\n",
    "        self._cache: Dict[str, Tuple[ModerationResult, float]] = {}\n",
    "        \n",
    "        # Statistics\n",
    "        self._stats = {\n",
    "            \"total_checks\": 0,\n",
    "            \"input_checks\": 0,\n",
    "            \"output_checks\": 0,\n",
    "            \"cache_hits\": 0,\n",
    "            \"blocked\": 0,\n",
    "            \"errors\": 0,\n",
    "            \"total_latency_ms\": 0\n",
    "        }\n",
    "    \n",
    "    def _get_cache_key(self, content: str) -> str:\n",
    "        \"\"\"Generate a cache key for content.\"\"\"\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n",
    "    \n",
    "    def _check_cache(self, key: str) -> Optional[ModerationResult]:\n",
    "        \"\"\"Check if we have a cached result.\"\"\"\n",
    "        if key in self._cache:\n",
    "            result, timestamp = self._cache[key]\n",
    "            if time.time() - timestamp < self.cache_ttl:\n",
    "                self._stats[\"cache_hits\"] += 1\n",
    "                cached_result = ModerationResult(\n",
    "                    is_safe=result.is_safe,\n",
    "                    category=result.category,\n",
    "                    category_name=result.category_name,\n",
    "                    confidence=result.confidence,\n",
    "                    cached=True,\n",
    "                    latency_ms=0\n",
    "                )\n",
    "                return cached_result\n",
    "            else:\n",
    "                # Expired\n",
    "                del self._cache[key]\n",
    "        return None\n",
    "    \n",
    "    def _cache_result(self, key: str, result: ModerationResult):\n",
    "        \"\"\"Cache a moderation result.\"\"\"\n",
    "        self._cache[key] = (result, time.time())\n",
    "    \n",
    "    def _classify(self, prompt: str) -> ModerationResult:\n",
    "        \"\"\"Run Llama Guard classification.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if not OLLAMA_AVAILABLE:\n",
    "            # Simulated response for demo\n",
    "            time.sleep(0.1)\n",
    "            is_safe = \"hack\" not in prompt.lower() and \"malware\" not in prompt.lower()\n",
    "            return ModerationResult(\n",
    "                is_safe=is_safe,\n",
    "                category=\"S2\" if not is_safe else None,\n",
    "                category_name=\"Non-Violent Crimes\" if not is_safe else None,\n",
    "                latency_ms=100\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            response = ollama.chat(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            latency_ms = (time.time() - start_time) * 1000\n",
    "            \n",
    "            result_text = response[\"message\"][\"content\"].strip().lower()\n",
    "            is_safe = result_text.startswith(\"safe\")\n",
    "            \n",
    "            category = None\n",
    "            category_name = None\n",
    "            \n",
    "            if not is_safe:\n",
    "                for code, name in SAFETY_CATEGORIES.items():\n",
    "                    if code.lower() in result_text:\n",
    "                        category = code\n",
    "                        category_name = name\n",
    "                        break\n",
    "            \n",
    "            return ModerationResult(\n",
    "                is_safe=is_safe,\n",
    "                category=category,\n",
    "                category_name=category_name,\n",
    "                latency_ms=latency_ms\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            self._stats[\"errors\"] += 1\n",
    "            return ModerationResult(\n",
    "                is_safe=not self.fail_closed,\n",
    "                category=\"ERROR\",\n",
    "                category_name=str(e),\n",
    "                latency_ms=(time.time() - start_time) * 1000\n",
    "            )\n",
    "    \n",
    "    def moderate_input(self, user_message: str) -> ModerationResult:\n",
    "        \"\"\"\n",
    "        Moderate user input.\n",
    "        \n",
    "        Args:\n",
    "            user_message: The user's message to check\n",
    "            \n",
    "        Returns:\n",
    "            ModerationResult with safety classification\n",
    "        \"\"\"\n",
    "        self._stats[\"total_checks\"] += 1\n",
    "        self._stats[\"input_checks\"] += 1\n",
    "        \n",
    "        # Check cache\n",
    "        cache_key = self._get_cache_key(f\"input:{user_message}\")\n",
    "        cached = self._check_cache(cache_key)\n",
    "        if cached:\n",
    "            return cached\n",
    "        \n",
    "        # Build classification prompt\n",
    "        prompt = f\"\"\"Task: Check if there is unsafe content in 'User' messages.\n",
    "\n",
    "<BEGIN CONVERSATION>\n",
    "User: {user_message}\n",
    "<END CONVERSATION>\n",
    "\n",
    "Provide your safety assessment. First line: 'safe' or 'unsafe'. If unsafe, second line: violated category codes.\"\"\"\n",
    "        \n",
    "        result = self._classify(prompt)\n",
    "        \n",
    "        if not result.is_safe:\n",
    "            self._stats[\"blocked\"] += 1\n",
    "        self._stats[\"total_latency_ms\"] += result.latency_ms\n",
    "        \n",
    "        # Cache and return\n",
    "        self._cache_result(cache_key, result)\n",
    "        return result\n",
    "    \n",
    "    def moderate_output(\n",
    "        self,\n",
    "        user_message: str,\n",
    "        assistant_message: str\n",
    "    ) -> ModerationResult:\n",
    "        \"\"\"\n",
    "        Moderate assistant output in context.\n",
    "        \n",
    "        Args:\n",
    "            user_message: The original user message\n",
    "            assistant_message: The assistant's response to check\n",
    "            \n",
    "        Returns:\n",
    "            ModerationResult with safety classification\n",
    "        \"\"\"\n",
    "        self._stats[\"total_checks\"] += 1\n",
    "        self._stats[\"output_checks\"] += 1\n",
    "        \n",
    "        # Check cache\n",
    "        cache_key = self._get_cache_key(f\"output:{user_message}:{assistant_message}\")\n",
    "        cached = self._check_cache(cache_key)\n",
    "        if cached:\n",
    "            return cached\n",
    "        \n",
    "        # Build classification prompt\n",
    "        prompt = f\"\"\"Task: Check if there is unsafe content in 'Assistant' messages.\n",
    "\n",
    "<BEGIN CONVERSATION>\n",
    "User: {user_message}\n",
    "Assistant: {assistant_message}\n",
    "<END CONVERSATION>\n",
    "\n",
    "Provide your safety assessment. First line: 'safe' or 'unsafe'. If unsafe, second line: violated category codes.\"\"\"\n",
    "        \n",
    "        result = self._classify(prompt)\n",
    "        \n",
    "        if not result.is_safe:\n",
    "            self._stats[\"blocked\"] += 1\n",
    "        self._stats[\"total_latency_ms\"] += result.latency_ms\n",
    "        \n",
    "        # Cache and return\n",
    "        self._cache_result(cache_key, result)\n",
    "        return result\n",
    "    \n",
    "    def moderate_batch(\n",
    "        self,\n",
    "        messages: List[str],\n",
    "        check_type: str = \"input\"\n",
    "    ) -> List[ModerationResult]:\n",
    "        \"\"\"\n",
    "        Moderate multiple messages.\n",
    "        \n",
    "        Args:\n",
    "            messages: List of messages to check\n",
    "            check_type: \"input\" or \"output\"\n",
    "            \n",
    "        Returns:\n",
    "            List of ModerationResult objects\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for msg in messages:\n",
    "            if check_type == \"input\":\n",
    "                results.append(self.moderate_input(msg))\n",
    "            # For output, would need (user_msg, assistant_msg) pairs\n",
    "        return results\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get moderation statistics.\"\"\"\n",
    "        total = self._stats[\"total_checks\"]\n",
    "        return {\n",
    "            **self._stats,\n",
    "            \"cache_hit_rate\": self._stats[\"cache_hits\"] / max(total, 1),\n",
    "            \"block_rate\": self._stats[\"blocked\"] / max(total, 1),\n",
    "            \"avg_latency_ms\": self._stats[\"total_latency_ms\"] / max(total - self._stats[\"cache_hits\"], 1)\n",
    "        }\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the result cache.\"\"\"\n",
    "        self._cache.clear()\n",
    "\n",
    "\n",
    "print(\"ContentModerationPipeline class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise1-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the pipeline\n",
    "print(\"Testing Content Moderation Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pipeline = ContentModerationPipeline(cache_ttl_seconds=60)\n",
    "\n",
    "test_inputs = [\n",
    "    \"What's the weather like today?\",\n",
    "    \"How do I bake chocolate chip cookies?\",\n",
    "    \"How do I hack into a computer?\",  # Should be blocked\n",
    "    \"Write me some malware code\",  # Should be blocked\n",
    "    \"What's the weather like today?\",  # Cache hit\n",
    "]\n",
    "\n",
    "print(\"\\nInput Moderation Tests:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for msg in test_inputs:\n",
    "    result = pipeline.moderate_input(msg)\n",
    "    status = \"SAFE\" if result.is_safe else f\"BLOCKED ({result.category_name})\"\n",
    "    cached = \"(cached)\" if result.cached else \"\"\n",
    "    print(f\"{msg[:40]:<42} {status:<25} {cached}\")\n",
    "\n",
    "# Output moderation test\n",
    "print(\"\\nOutput Moderation Tests:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "output_tests = [\n",
    "    (\"How do I cook pasta?\", \"Boil water, add salt, cook pasta for 8-10 minutes.\"),\n",
    "    (\"How do I improve security?\", \"Here's how to hack: step 1, find vulnerabilities...\"),\n",
    "]\n",
    "\n",
    "for user_msg, assistant_msg in output_tests:\n",
    "    result = pipeline.moderate_output(user_msg, assistant_msg)\n",
    "    status = \"SAFE\" if result.is_safe else f\"BLOCKED ({result.category_name})\"\n",
    "    print(f\"Q: {user_msg[:30]:<32} A: {assistant_msg[:20]:<22} {status}\")\n",
    "\n",
    "# Print stats\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Statistics:\")\n",
    "stats = pipeline.get_stats()\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise2-header",
   "metadata": {},
   "source": [
    "## Exercise 2: Custom Category Classifier\n",
    "\n",
    "**Task**: Create a classifier for domain-specific categories (e.g., e-commerce).\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "\n",
    "@dataclass\n",
    "class DomainCategory:\n",
    "    \"\"\"A domain-specific safety category.\"\"\"\n",
    "    code: str\n",
    "    name: str\n",
    "    description: str\n",
    "    keywords: Set[str]\n",
    "    severity: str  # \"low\", \"medium\", \"high\"\n",
    "\n",
    "\n",
    "class DomainSafetyClassifier:\n",
    "    \"\"\"\n",
    "    A customizable safety classifier for domain-specific use cases.\n",
    "    \n",
    "    This extends Llama Guard with custom categories relevant to\n",
    "    specific business domains (e-commerce, healthcare, finance, etc.).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, domain: str = \"general\"):\n",
    "        self.domain = domain\n",
    "        self.categories: Dict[str, DomainCategory] = {}\n",
    "        \n",
    "        # Initialize with domain-specific categories\n",
    "        self._init_domain_categories()\n",
    "    \n",
    "    def _init_domain_categories(self):\n",
    "        \"\"\"Initialize domain-specific categories.\"\"\"\n",
    "        \n",
    "        if self.domain == \"ecommerce\":\n",
    "            self.categories = {\n",
    "                \"EC1\": DomainCategory(\n",
    "                    code=\"EC1\",\n",
    "                    name=\"Price Manipulation\",\n",
    "                    description=\"Attempts to manipulate prices or get unauthorized discounts\",\n",
    "                    keywords={\"discount code\", \"price override\", \"free shipping hack\", \"coupon exploit\"},\n",
    "                    severity=\"high\"\n",
    "                ),\n",
    "                \"EC2\": DomainCategory(\n",
    "                    code=\"EC2\",\n",
    "                    name=\"Return Fraud\",\n",
    "                    description=\"Attempts to abuse return policies\",\n",
    "                    keywords={\"fake return\", \"wardrobing\", \"return scam\", \"refund trick\"},\n",
    "                    severity=\"high\"\n",
    "                ),\n",
    "                \"EC3\": DomainCategory(\n",
    "                    code=\"EC3\",\n",
    "                    name=\"Competitor Pricing\",\n",
    "                    description=\"Requests for competitor pricing information\",\n",
    "                    keywords={\"competitor price\", \"amazon price\", \"cheaper on\", \"price match\"},\n",
    "                    severity=\"low\"\n",
    "                ),\n",
    "                \"EC4\": DomainCategory(\n",
    "                    code=\"EC4\",\n",
    "                    name=\"Stock Manipulation\",\n",
    "                    description=\"Attempts to manipulate inventory or availability\",\n",
    "                    keywords={\"hold stock\", \"reserve all\", \"buy out inventory\"},\n",
    "                    severity=\"medium\"\n",
    "                ),\n",
    "            }\n",
    "        \n",
    "        elif self.domain == \"healthcare\":\n",
    "            self.categories = {\n",
    "                \"HC1\": DomainCategory(\n",
    "                    code=\"HC1\",\n",
    "                    name=\"Medical Diagnosis Request\",\n",
    "                    description=\"Requests for medical diagnosis\",\n",
    "                    keywords={\"diagnose\", \"what condition\", \"do i have\", \"am i sick\"},\n",
    "                    severity=\"high\"\n",
    "                ),\n",
    "                \"HC2\": DomainCategory(\n",
    "                    code=\"HC2\",\n",
    "                    name=\"Medication Recommendation\",\n",
    "                    description=\"Requests for medication recommendations\",\n",
    "                    keywords={\"what medicine\", \"should i take\", \"prescription for\", \"drug for\"},\n",
    "                    severity=\"high\"\n",
    "                ),\n",
    "                \"HC3\": DomainCategory(\n",
    "                    code=\"HC3\",\n",
    "                    name=\"Emergency Situation\",\n",
    "                    description=\"Medical emergency indications\",\n",
    "                    keywords={\"chest pain\", \"can't breathe\", \"overdose\", \"bleeding heavily\"},\n",
    "                    severity=\"critical\"\n",
    "                ),\n",
    "            }\n",
    "        \n",
    "        elif self.domain == \"finance\":\n",
    "            self.categories = {\n",
    "                \"FN1\": DomainCategory(\n",
    "                    code=\"FN1\",\n",
    "                    name=\"Investment Advice\",\n",
    "                    description=\"Requests for specific investment recommendations\",\n",
    "                    keywords={\"should i invest\", \"buy stock\", \"best crypto\", \"guaranteed returns\"},\n",
    "                    severity=\"high\"\n",
    "                ),\n",
    "                \"FN2\": DomainCategory(\n",
    "                    code=\"FN2\",\n",
    "                    name=\"Tax Evasion\",\n",
    "                    description=\"Requests for tax evasion strategies\",\n",
    "                    keywords={\"avoid taxes\", \"hide income\", \"offshore account\", \"tax shelter\"},\n",
    "                    severity=\"critical\"\n",
    "                ),\n",
    "                \"FN3\": DomainCategory(\n",
    "                    code=\"FN3\",\n",
    "                    name=\"Account Fraud\",\n",
    "                    description=\"Attempts at account fraud\",\n",
    "                    keywords={\"fake account\", \"identity theft\", \"stolen card\", \"account takeover\"},\n",
    "                    severity=\"critical\"\n",
    "                ),\n",
    "            }\n",
    "        \n",
    "        else:  # general\n",
    "            self.categories = {}\n",
    "    \n",
    "    def add_category(self, category: DomainCategory):\n",
    "        \"\"\"Add a custom category.\"\"\"\n",
    "        self.categories[category.code] = category\n",
    "    \n",
    "    def classify(self, text: str) -> List[Tuple[DomainCategory, float]]:\n",
    "        \"\"\"\n",
    "        Classify text against domain categories.\n",
    "        \n",
    "        Returns:\n",
    "            List of (category, confidence) tuples for matched categories\n",
    "        \"\"\"\n",
    "        text_lower = text.lower()\n",
    "        matches = []\n",
    "        \n",
    "        for code, category in self.categories.items():\n",
    "            # Check keyword matches\n",
    "            keyword_matches = 0\n",
    "            for keyword in category.keywords:\n",
    "                if keyword in text_lower:\n",
    "                    keyword_matches += 1\n",
    "            \n",
    "            if keyword_matches > 0:\n",
    "                # Simple confidence based on keyword match ratio\n",
    "                confidence = min(keyword_matches / 2, 1.0)\n",
    "                matches.append((category, confidence))\n",
    "        \n",
    "        # Sort by confidence\n",
    "        return sorted(matches, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    def should_block(self, text: str) -> Tuple[bool, Optional[DomainCategory]]:\n",
    "        \"\"\"\n",
    "        Determine if text should be blocked.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (should_block, matched_category)\n",
    "        \"\"\"\n",
    "        matches = self.classify(text)\n",
    "        \n",
    "        for category, confidence in matches:\n",
    "            if category.severity in [\"high\", \"critical\"] and confidence > 0.5:\n",
    "                return True, category\n",
    "        \n",
    "        return False, None\n",
    "\n",
    "\n",
    "print(\"DomainSafetyClassifier class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise2-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test domain classifiers\n",
    "print(\"Testing Domain Safety Classifiers\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# E-commerce domain\n",
    "print(\"\\n E-COMMERCE DOMAIN\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "ecommerce_classifier = DomainSafetyClassifier(domain=\"ecommerce\")\n",
    "\n",
    "ecommerce_tests = [\n",
    "    \"What's your return policy?\",\n",
    "    \"Do you have a discount code I can use?\",\n",
    "    \"How do I do a fake return to get my money back?\",\n",
    "    \"Is this cheaper on Amazon?\",\n",
    "    \"Can I reserve all your stock?\",\n",
    "]\n",
    "\n",
    "for text in ecommerce_tests:\n",
    "    block, category = ecommerce_classifier.should_block(text)\n",
    "    if block:\n",
    "        print(f\"BLOCK: {text[:45]:<47} [{category.code}: {category.name}]\")\n",
    "    else:\n",
    "        print(f\"ALLOW: {text[:45]}\")\n",
    "\n",
    "# Healthcare domain\n",
    "print(\"\\n HEALTHCARE DOMAIN\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "healthcare_classifier = DomainSafetyClassifier(domain=\"healthcare\")\n",
    "\n",
    "healthcare_tests = [\n",
    "    \"What are your visiting hours?\",\n",
    "    \"Can you diagnose my symptoms?\",\n",
    "    \"What medicine should I take for my headache?\",\n",
    "    \"I'm having chest pain and can't breathe\",\n",
    "    \"Where is the cafeteria?\",\n",
    "]\n",
    "\n",
    "for text in healthcare_tests:\n",
    "    block, category = healthcare_classifier.should_block(text)\n",
    "    if block:\n",
    "        print(f\"BLOCK: {text[:45]:<47} [{category.code}: {category.name}]\")\n",
    "    else:\n",
    "        print(f\"ALLOW: {text[:45]}\")\n",
    "\n",
    "# Finance domain\n",
    "print(\"\\n FINANCE DOMAIN\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "finance_classifier = DomainSafetyClassifier(domain=\"finance\")\n",
    "\n",
    "finance_tests = [\n",
    "    \"What's my account balance?\",\n",
    "    \"Should I invest in Bitcoin?\",\n",
    "    \"How can I avoid taxes legally?\",\n",
    "    \"I found a stolen card, can I use it?\",\n",
    "    \"What are your branch hours?\",\n",
    "]\n",
    "\n",
    "for text in finance_tests:\n",
    "    block, category = finance_classifier.should_block(text)\n",
    "    if block:\n",
    "        print(f\"BLOCK: {text[:45]:<47} [{category.code}: {category.name}]\")\n",
    "    else:\n",
    "        print(f\"ALLOW: {text[:45]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
