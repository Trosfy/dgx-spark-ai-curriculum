{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.3.3: LLM Benchmark Suite\n",
    "\n",
    "**Module:** 4.3 - MLOps & Experiment Tracking  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand standard LLM benchmarks (MMLU, HellaSwag, ARC, etc.)\n",
    "- [ ] Use lm-evaluation-harness to evaluate models\n",
    "- [ ] Compare multiple models on the same benchmarks\n",
    "- [ ] Interpret benchmark results correctly\n",
    "- [ ] Integrate benchmarking with experiment tracking\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Labs 4.3.1-4.3.2 (MLflow, W&B)\n",
    "- Knowledge of: LLMs, Python, basic statistics\n",
    "- Hardware: DGX Spark (128GB unified memory)\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**How do you know if your model is actually good?**\n",
    "\n",
    "Without standardized benchmarks, we'd be comparing apples to oranges. Industry leaders use these benchmarks:\n",
    "\n",
    "| Benchmark | What It Tests | Example Question |\n",
    "|-----------|--------------|------------------|\n",
    "| **MMLU** | Knowledge (57 subjects) | \"What is the capital of France?\" |\n",
    "| **HellaSwag** | Commonsense reasoning | \"A man walks into a bar...\" (complete the story) |\n",
    "| **ARC** | Science reasoning | \"Why does ice float on water?\" |\n",
    "| **WinoGrande** | Pronoun resolution | \"The trophy doesn't fit in the suitcase because it's too big.\" (What is too big?) |\n",
    "| **HumanEval** | Code generation | \"Write a function that reverses a string\" |\n",
    "| **MT-Bench** | Chat quality | Multi-turn conversation scoring |\n",
    "\n",
    "**Benchmark Leaderboards:**\n",
    "- [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n",
    "- [HELM](https://crfm.stanford.edu/helm/)\n",
    "- [Chatbot Arena](https://lmsys.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What Are LLM Benchmarks?\n",
    "\n",
    "> **Imagine you're comparing students from different schools.**\n",
    ">\n",
    "> You can't just ask each school \"Are your students smart?\" - they'll all say yes!\n",
    ">\n",
    "> Instead, you give everyone the **same standardized test**:\n",
    "> - Math section (like MMLU)\n",
    "> - Reading comprehension (like HellaSwag)\n",
    "> - Science questions (like ARC)\n",
    "> - Critical thinking (like WinoGrande)\n",
    ">\n",
    "> Now you can fairly compare:\n",
    "> - \"School A scores 85% on math, School B scores 78%\"\n",
    "> - \"School A is great at math but weak in science\"\n",
    ">\n",
    "> **LLM benchmarks are standardized tests for AI models!**\n",
    "> - Same questions for every model\n",
    "> - Objective scoring\n",
    "> - Apples-to-apples comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding Key Benchmarks\n",
    "\n",
    "### The \"Big 6\" Benchmarks\n",
    "\n",
    "| Benchmark | Full Name | Metric | What It Measures |\n",
    "|-----------|-----------|--------|------------------|\n",
    "| **MMLU** | Massive Multitask Language Understanding | Accuracy | 57 subjects from STEM to humanities |\n",
    "| **HellaSwag** | Harder Endings, Longer contexts | Accuracy | Commonsense reasoning |\n",
    "| **ARC-c** | AI2 Reasoning Challenge (Challenge) | Accuracy | Grade-school science (hard) |\n",
    "| **WinoGrande** | Winograd Schema Challenge | Accuracy | Coreference resolution |\n",
    "| **TruthfulQA** | Truthful Question Answering | Accuracy | Avoiding falsehoods |\n",
    "| **GSM8K** | Grade School Math 8K | Accuracy | Math word problems |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at example questions from each benchmark\n",
    "\n",
    "benchmark_examples = {\n",
    "    \"MMLU (College Chemistry)\": {\n",
    "        \"question\": \"What is the molecular geometry of SF6?\",\n",
    "        \"choices\": [\"A) Tetrahedral\", \"B) Octahedral\", \"C) Trigonal bipyramidal\", \"D) Square planar\"],\n",
    "        \"answer\": \"B) Octahedral\",\n",
    "        \"difficulty\": \"Requires chemistry knowledge\"\n",
    "    },\n",
    "    \n",
    "    \"HellaSwag\": {\n",
    "        \"question\": \"A woman is outside with a bucket and a dog. The dog is running around trying to avoid a bath. She...\",\n",
    "        \"choices\": [\n",
    "            \"A) rinses the bucket and puts water in it\",\n",
    "            \"B) starts to chase the dog with the bucket\",\n",
    "            \"C) gets the dog wet and starts to lather it up\",\n",
    "            \"D) takes the dog inside and dries it off\"\n",
    "        ],\n",
    "        \"answer\": \"C) gets the dog wet and starts to lather it up\",\n",
    "        \"difficulty\": \"Requires common sense about sequences\"\n",
    "    },\n",
    "    \n",
    "    \"ARC-Challenge\": {\n",
    "        \"question\": \"Which property of a mineral can be determined just by looking at it?\",\n",
    "        \"choices\": [\"A) hardness\", \"B) color\", \"C) luster\", \"D) streak\"],\n",
    "        \"answer\": \"B) color (or C) luster - both visual properties)\",\n",
    "        \"difficulty\": \"Grade-school science reasoning\"\n",
    "    },\n",
    "    \n",
    "    \"WinoGrande\": {\n",
    "        \"question\": \"The trophy doesn't fit in the brown suitcase because it's too [big/small].\",\n",
    "        \"task\": \"Determine if 'it' refers to trophy or suitcase\",\n",
    "        \"answer\": \"If 'big' -> 'it' = trophy; If 'small' -> 'it' = suitcase\",\n",
    "        \"difficulty\": \"Pronoun resolution (tricky!)\"\n",
    "    },\n",
    "    \n",
    "    \"GSM8K\": {\n",
    "        \"question\": \"Janet has 3 times as many marbles as Tom. If Janet gives Tom 10 marbles, they will have the same number. How many marbles does Janet have?\",\n",
    "        \"answer\": \"30 marbles (Janet: 30, Tom: 10 -> After: Janet: 20, Tom: 20)\",\n",
    "        \"difficulty\": \"Multi-step math reasoning\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìö BENCHMARK EXAMPLES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for benchmark, example in benchmark_examples.items():\n",
    "    print(f\"\\nüî∑ {benchmark}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    if 'choices' in example:\n",
    "        for choice in example['choices']:\n",
    "            print(f\"  {choice}\")\n",
    "    if 'task' in example:\n",
    "        print(f\"Task: {example['task']}\")\n",
    "    print(f\"Answer: {example['answer']}\")\n",
    "    print(f\"Difficulty: {example['difficulty']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Setting Up lm-evaluation-harness\n",
    "\n",
    "The [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) by EleutherAI is the industry standard for LLM benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install lm-eval\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import lm_eval\n",
    "    print(f\"‚úÖ lm-eval already installed: v{lm_eval.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing lm-evaluation-harness...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lm-eval\", \"-q\"])\n",
    "    import lm_eval\n",
    "    print(f\"‚úÖ lm-eval installed: v{lm_eval.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lm_eval\n",
    "from lm_eval import evaluator, tasks\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"lm-eval version: {lm_eval.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available tasks/benchmarks\n",
    "available_tasks = tasks.TaskManager().all_tasks\n",
    "\n",
    "print(f\"üìã Available benchmarks: {len(available_tasks)} total\")\n",
    "print(\"\\nüî• Popular benchmarks:\")\n",
    "\n",
    "popular = [\n",
    "    \"hellaswag\", \"arc_easy\", \"arc_challenge\", \"winogrande\",\n",
    "    \"mmlu\", \"truthfulqa\", \"gsm8k\", \"humaneval\"\n",
    "]\n",
    "\n",
    "for task in popular:\n",
    "    if task in available_tasks:\n",
    "        print(f\"  ‚úì {task}\")\n",
    "    else:\n",
    "        # Check for variants\n",
    "        variants = [t for t in available_tasks if task in t]\n",
    "        if variants:\n",
    "            print(f\"  ‚úì {task} (variants: {', '.join(variants[:3])}...)\")\n",
    "        else:\n",
    "            print(f\"  ‚úó {task} (not found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Running Your First Benchmark\n",
    "\n",
    "Let's benchmark a small model to understand the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directories\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "MODULE_DIR = (NOTEBOOK_DIR / \"..\").resolve()\n",
    "RESULTS_DIR = MODULE_DIR / \"evaluation\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Results will be saved to: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run benchmarks\n",
    "def run_benchmark(\n",
    "    model_name: str,\n",
    "    tasks_list: list[str],\n",
    "    num_fewshot: int = 0,\n",
    "    batch_size: str = \"auto\",\n",
    "    device: str = \"cuda\",\n",
    "    dtype: str = \"bfloat16\",\n",
    "    limit: int = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Run lm-eval benchmarks on a model.\n",
    "    \n",
    "    Args:\n",
    "        model_name: HuggingFace model name\n",
    "        tasks_list: List of benchmark names\n",
    "        num_fewshot: Number of few-shot examples\n",
    "        batch_size: Batch size (\"auto\" for automatic)\n",
    "        device: Device to use\n",
    "        dtype: Data type for model\n",
    "        limit: Limit number of samples (for testing)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Evaluation results\n",
    "    \"\"\"\n",
    "    print(f\"\\nüî¨ Benchmarking: {model_name}\")\n",
    "    print(f\"   Tasks: {', '.join(tasks_list)}\")\n",
    "    print(f\"   Few-shot: {num_fewshot}\")\n",
    "    print(f\"   Limit: {limit if limit else 'Full dataset'}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Build model args string\n",
    "    model_args = f\"pretrained={model_name}\"\n",
    "    if dtype:\n",
    "        model_args += f\",dtype={dtype}\"\n",
    "    if device == \"cuda\" and torch.cuda.is_available():\n",
    "        model_args += \",device_map=auto\"\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluator.simple_evaluate(\n",
    "        model=\"hf\",\n",
    "        model_args=model_args,\n",
    "        tasks=tasks_list,\n",
    "        num_fewshot=num_fewshot,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        limit=limit\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Benchmark function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick benchmark on a small model\n",
    "# Using limit=50 for fast demonstration - remove for full evaluation!\n",
    "\n",
    "# Choose a small, fast model for the demo\n",
    "DEMO_MODEL = \"microsoft/phi-2\"  # 2.7B parameters\n",
    "\n",
    "# Quick test with limited samples\n",
    "QUICK_TASKS = [\"hellaswag\", \"arc_easy\"]\n",
    "\n",
    "print(\"‚ö° Running quick benchmark (limited samples for demo)...\")\n",
    "print(\"   For real evaluation, remove the 'limit' parameter!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the benchmark (this may take a few minutes)\n",
    "try:\n",
    "    results = run_benchmark(\n",
    "        model_name=DEMO_MODEL,\n",
    "        tasks_list=QUICK_TASKS,\n",
    "        num_fewshot=0,\n",
    "        limit=50,  # Remove this for full evaluation!\n",
    "        dtype=\"bfloat16\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Benchmark complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Benchmark error: {e}\")\n",
    "    print(\"\\nThis might happen if:\")\n",
    "    print(\"1. Not enough GPU memory\")\n",
    "    print(\"2. Model not accessible\")\n",
    "    print(\"3. Network issues\")\n",
    "    print(\"\\nWe'll use simulated results for the demo.\")\n",
    "    \n",
    "    # Simulated results for demonstration\n",
    "    results = {\n",
    "        \"results\": {\n",
    "            \"hellaswag\": {\n",
    "                \"acc\": 0.7234,\n",
    "                \"acc_norm\": 0.7456,\n",
    "                \"acc_stderr\": 0.0089,\n",
    "                \"acc_norm_stderr\": 0.0087\n",
    "            },\n",
    "            \"arc_easy\": {\n",
    "                \"acc\": 0.7821,\n",
    "                \"acc_norm\": 0.7654,\n",
    "                \"acc_stderr\": 0.0123,\n",
    "                \"acc_norm_stderr\": 0.0127\n",
    "            }\n",
    "        },\n",
    "        \"config\": {\n",
    "            \"model\": DEMO_MODEL,\n",
    "            \"model_args\": f\"pretrained={DEMO_MODEL},dtype=bfloat16\"\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results nicely\n",
    "def display_results(results: dict):\n",
    "    \"\"\"Display benchmark results in a formatted table.\"\"\"\n",
    "    print(\"\\nüìä BENCHMARK RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if \"results\" in results:\n",
    "        for task, metrics in results[\"results\"].items():\n",
    "            print(f\"\\nüî∑ {task.upper()}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            for metric, value in metrics.items():\n",
    "                if isinstance(value, float):\n",
    "                    if \"stderr\" in metric:\n",
    "                        print(f\"   {metric}: ¬±{value:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"   {metric}: {value:.4f} ({value*100:.1f}%)\")\n",
    "                else:\n",
    "                    print(f\"   {metric}: {value}\")\n",
    "\n",
    "display_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to file\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_file = RESULTS_DIR / f\"benchmark_{DEMO_MODEL.replace('/', '_')}_{timestamp}.json\"\n",
    "\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nüíæ Results saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Comparing Multiple Models\n",
    "\n",
    "The real power of benchmarks: fair comparison across models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated results for multiple models\n",
    "# In practice, you'd run benchmarks on each model\n",
    "\n",
    "model_results = {\n",
    "    \"microsoft/phi-2\": {\n",
    "        \"params\": \"2.7B\",\n",
    "        \"mmlu\": 0.562,\n",
    "        \"hellaswag\": 0.735,\n",
    "        \"arc_challenge\": 0.528,\n",
    "        \"winogrande\": 0.742,\n",
    "        \"truthfulqa\": 0.412,\n",
    "        \"gsm8k\": 0.548\n",
    "    },\n",
    "    \"Qwen/Qwen3-8B\": {\n",
    "        \"params\": \"7B\",\n",
    "        \"mmlu\": 0.458,\n",
    "        \"hellaswag\": 0.760,\n",
    "        \"arc_challenge\": 0.532,\n",
    "        \"winogrande\": 0.740,\n",
    "        \"truthfulqa\": 0.389,\n",
    "        \"gsm8k\": 0.141\n",
    "    },\n",
    "    \"mistralai/Mistral-7B-v0.1\": {\n",
    "        \"params\": \"7B\",\n",
    "        \"mmlu\": 0.625,\n",
    "        \"hellaswag\": 0.812,\n",
    "        \"arc_challenge\": 0.599,\n",
    "        \"winogrande\": 0.785,\n",
    "        \"truthfulqa\": 0.425,\n",
    "        \"gsm8k\": 0.352\n",
    "    },\n",
    "    \"google/gemma-2b\": {\n",
    "        \"params\": \"2B\",\n",
    "        \"mmlu\": 0.424,\n",
    "        \"hellaswag\": 0.714,\n",
    "        \"arc_challenge\": 0.482,\n",
    "        \"winogrande\": 0.658,\n",
    "        \"truthfulqa\": 0.378,\n",
    "        \"gsm8k\": 0.175\n",
    "    },\n",
    "    \"Qwen/Qwen2-7B\": {\n",
    "        \"params\": \"7B\",\n",
    "        \"mmlu\": 0.702,\n",
    "        \"hellaswag\": 0.798,\n",
    "        \"arc_challenge\": 0.612,\n",
    "        \"winogrande\": 0.772,\n",
    "        \"truthfulqa\": 0.445,\n",
    "        \"gsm8k\": 0.524\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìä Model comparison data loaded\")\n",
    "print(f\"   Models: {len(model_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create comparison dataframe\n",
    "benchmarks = [\"mmlu\", \"hellaswag\", \"arc_challenge\", \"winogrande\", \"truthfulqa\", \"gsm8k\"]\n",
    "\n",
    "comparison_data = []\n",
    "for model, scores in model_results.items():\n",
    "    row = {\"model\": model.split(\"/\")[-1], \"params\": scores[\"params\"]}\n",
    "    for benchmark in benchmarks:\n",
    "        row[benchmark] = scores[benchmark]\n",
    "    row[\"average\"] = np.mean([scores[b] for b in benchmarks])\n",
    "    comparison_data.append(row)\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "df = df.sort_values(\"average\", ascending=False)\n",
    "\n",
    "print(\"\\nüìä MODEL COMPARISON TABLE\")\n",
    "print(\"=\" * 100)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Overall comparison (bar chart)\n",
    "models = df[\"model\"].tolist()\n",
    "averages = df[\"average\"].tolist()\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(models)))\n",
    "bars = axes[0, 0].barh(models, averages, color=colors)\n",
    "axes[0, 0].set_xlabel(\"Average Score\")\n",
    "axes[0, 0].set_title(\"Overall Model Ranking\")\n",
    "axes[0, 0].set_xlim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, avg in zip(bars, averages):\n",
    "    axes[0, 0].text(avg + 0.02, bar.get_y() + bar.get_height()/2, \n",
    "                    f\"{avg:.1%}\", va=\"center\")\n",
    "\n",
    "# Plot 2: Radar chart\n",
    "angles = np.linspace(0, 2 * np.pi, len(benchmarks), endpoint=False).tolist()\n",
    "angles += angles[:1]  # Complete the circle\n",
    "\n",
    "ax_radar = axes[0, 1]\n",
    "ax_radar = fig.add_subplot(2, 2, 2, projection='polar')\n",
    "\n",
    "for i, (model, scores) in enumerate(model_results.items()):\n",
    "    values = [scores[b] for b in benchmarks]\n",
    "    values += values[:1]  # Complete the circle\n",
    "    ax_radar.plot(angles, values, 'o-', linewidth=2, \n",
    "                  label=model.split(\"/\")[-1], alpha=0.7)\n",
    "    ax_radar.fill(angles, values, alpha=0.1)\n",
    "\n",
    "ax_radar.set_xticks(angles[:-1])\n",
    "ax_radar.set_xticklabels(benchmarks)\n",
    "ax_radar.set_title(\"Benchmark Profile Comparison\")\n",
    "ax_radar.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.0))\n",
    "\n",
    "# Plot 3: Heatmap\n",
    "heatmap_data = df[benchmarks].values\n",
    "im = axes[1, 0].imshow(heatmap_data, cmap=\"RdYlGn\", aspect=\"auto\", vmin=0, vmax=1)\n",
    "\n",
    "axes[1, 0].set_xticks(range(len(benchmarks)))\n",
    "axes[1, 0].set_xticklabels(benchmarks, rotation=45, ha=\"right\")\n",
    "axes[1, 0].set_yticks(range(len(models)))\n",
    "axes[1, 0].set_yticklabels(df[\"model\"].tolist())\n",
    "axes[1, 0].set_title(\"Benchmark Scores Heatmap\")\n",
    "plt.colorbar(im, ax=axes[1, 0], label=\"Accuracy\")\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(models)):\n",
    "    for j in range(len(benchmarks)):\n",
    "        text = axes[1, 0].text(j, i, f\"{heatmap_data[i, j]:.2f}\",\n",
    "                               ha=\"center\", va=\"center\", fontsize=8,\n",
    "                               color=\"white\" if heatmap_data[i, j] > 0.5 else \"black\")\n",
    "\n",
    "# Plot 4: Params vs Average Score\n",
    "params_num = [float(p.replace(\"B\", \"\")) for p in df[\"params\"].tolist()]\n",
    "axes[1, 1].scatter(params_num, df[\"average\"], s=200, c=range(len(models)), cmap=\"viridis\")\n",
    "\n",
    "for i, (model, param, avg) in enumerate(zip(df[\"model\"], params_num, df[\"average\"])):\n",
    "    axes[1, 1].annotate(model, (param, avg), textcoords=\"offset points\", \n",
    "                        xytext=(5, 5), fontsize=9)\n",
    "\n",
    "axes[1, 1].set_xlabel(\"Parameters (Billions)\")\n",
    "axes[1, 1].set_ylabel(\"Average Score\")\n",
    "axes[1, 1].set_title(\"Model Size vs Performance\")\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / \"model_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Comparison saved to: {RESULTS_DIR / 'model_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Key Insights\n",
    "\n",
    "From the comparison:\n",
    "\n",
    "1. **Bigger isn't always better**: Phi-2 (2.7B) competes with 7B models\n",
    "2. **Training data matters**: Qwen2-7B outperforms despite similar size to Mistral\n",
    "3. **Specialization**: Some models excel at specific benchmarks (Mistral at HellaSwag)\n",
    "4. **Math is hard**: GSM8K scores are generally lower than other benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Running Full Benchmark Suites\n",
    "\n",
    "For production evaluation, you'd run the full benchmark suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command-line usage (preferred for long benchmarks)\n",
    "cli_examples = '''\n",
    "# Quick evaluation (subset of samples)\n",
    "lm_eval --model hf \\\n",
    "    --model_args pretrained=microsoft/phi-2,dtype=bfloat16 \\\n",
    "    --tasks hellaswag,arc_easy \\\n",
    "    --batch_size 8 \\\n",
    "    --limit 100 \\\n",
    "    --output_path ./results/phi2_quick\n",
    "\n",
    "# Full evaluation on Open LLM Leaderboard benchmarks\n",
    "lm_eval --model hf \\\n",
    "    --model_args pretrained=Qwen/Qwen3-8B,dtype=bfloat16 \\\n",
    "    --tasks hellaswag,arc_challenge,winogrande,mmlu,truthfulqa,gsm8k \\\n",
    "    --num_fewshot 5 \\\n",
    "    --batch_size auto \\\n",
    "    --output_path ./results/llama31_full\n",
    "\n",
    "# With specific GPU\n",
    "CUDA_VISIBLE_DEVICES=0 lm_eval --model hf \\\n",
    "    --model_args pretrained=mistralai/Mistral-7B-v0.1,dtype=bfloat16 \\\n",
    "    --tasks mmlu \\\n",
    "    --batch_size 4 \\\n",
    "    --output_path ./results/mistral_mmlu\n",
    "\n",
    "# Code evaluation (HumanEval)\n",
    "lm_eval --model hf \\\n",
    "    --model_args pretrained=bigcode/starcoder,dtype=bfloat16 \\\n",
    "    --tasks humaneval \\\n",
    "    --batch_size 1 \\\n",
    "    --output_path ./results/starcoder_code\n",
    "'''\n",
    "\n",
    "print(\"üìã CLI Commands for Full Benchmarking:\")\n",
    "print(\"=\" * 60)\n",
    "print(cli_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark time estimates\n",
    "time_estimates = {\n",
    "    \"hellaswag\": {\"samples\": 10042, \"time_7b\": \"~20 min\"},\n",
    "    \"arc_easy\": {\"samples\": 2376, \"time_7b\": \"~5 min\"},\n",
    "    \"arc_challenge\": {\"samples\": 1172, \"time_7b\": \"~5 min\"},\n",
    "    \"winogrande\": {\"samples\": 1267, \"time_7b\": \"~3 min\"},\n",
    "    \"mmlu\": {\"samples\": 14042, \"time_7b\": \"~45 min\"},\n",
    "    \"truthfulqa\": {\"samples\": 817, \"time_7b\": \"~5 min\"},\n",
    "    \"gsm8k\": {\"samples\": 1319, \"time_7b\": \"~15 min\"}\n",
    "}\n",
    "\n",
    "print(\"‚è±Ô∏è BENCHMARK TIME ESTIMATES (7B model, DGX Spark)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Benchmark':<15} {'Samples':>10} {'Est. Time':>15}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "total_samples = 0\n",
    "for bench, info in time_estimates.items():\n",
    "    print(f\"{bench:<15} {info['samples']:>10,} {info['time_7b']:>15}\")\n",
    "    total_samples += info['samples']\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'TOTAL':<15} {total_samples:>10,} {'~2 hours':>15}\")\n",
    "print(\"\\nüí° Tip: Use --limit to run quick tests first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Integration with Experiment Tracking\n",
    "\n",
    "Log benchmark results to MLflow or W&B for tracking over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Setup MLflow\n",
    "MLFLOW_DIR = MODULE_DIR / \"mlflow\"\n",
    "mlflow.set_tracking_uri(f\"file://{MLFLOW_DIR}\")\n",
    "mlflow.set_experiment(\"LLM-Benchmarks\")\n",
    "\n",
    "print(f\"üìä MLflow tracking: {MLFLOW_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_benchmark_to_mlflow(model_name: str, benchmark_results: dict, run_name: str = None):\n",
    "    \"\"\"\n",
    "    Log benchmark results to MLflow.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model being evaluated\n",
    "        benchmark_results: Results dictionary from lm-eval\n",
    "        run_name: Optional run name\n",
    "    \"\"\"\n",
    "    if run_name is None:\n",
    "        run_name = f\"{model_name.replace('/', '_')}_benchmark\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        # Log model info\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "        \n",
    "        # Log config if available\n",
    "        if \"config\" in benchmark_results:\n",
    "            for key, value in benchmark_results[\"config\"].items():\n",
    "                if isinstance(value, (str, int, float, bool)):\n",
    "                    mlflow.log_param(f\"config_{key}\", value)\n",
    "        \n",
    "        # Log benchmark scores\n",
    "        if \"results\" in benchmark_results:\n",
    "            for task, metrics in benchmark_results[\"results\"].items():\n",
    "                for metric, value in metrics.items():\n",
    "                    if isinstance(value, (int, float)):\n",
    "                        mlflow.log_metric(f\"{task}_{metric}\", value)\n",
    "        \n",
    "        # Log results as artifact\n",
    "        results_path = \"/tmp/benchmark_results.json\"\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump(benchmark_results, f, indent=2, default=str)\n",
    "        mlflow.log_artifact(results_path, artifact_path=\"benchmarks\")\n",
    "        \n",
    "        # Add tags\n",
    "        mlflow.set_tag(\"type\", \"benchmark\")\n",
    "        mlflow.set_tag(\"hardware\", \"DGX Spark\")\n",
    "        \n",
    "        print(f\"‚úÖ Logged to MLflow: {run.info.run_id}\")\n",
    "        return run.info.run_id\n",
    "\n",
    "print(\"‚úÖ MLflow logging function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log our earlier results\n",
    "run_id = log_benchmark_to_mlflow(\n",
    "    model_name=DEMO_MODEL,\n",
    "    benchmark_results=results,\n",
    "    run_name=\"phi2_quick_test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log comparison data for all models\n",
    "print(\"üìä Logging all model benchmarks to MLflow...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for model_name, scores in model_results.items():\n",
    "    # Convert to lm-eval format\n",
    "    fake_results = {\n",
    "        \"results\": {\n",
    "            bench: {\"acc\": score} \n",
    "            for bench, score in scores.items() \n",
    "            if bench != \"params\"\n",
    "        },\n",
    "        \"config\": {\n",
    "            \"model\": model_name,\n",
    "            \"params\": scores[\"params\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    log_benchmark_to_mlflow(\n",
    "        model_name=model_name,\n",
    "        benchmark_results=fake_results,\n",
    "        run_name=f\"{model_name.split('/')[-1]}_full_benchmark\"\n",
    "    )\n",
    "\n",
    "print(\"\\n‚úÖ All benchmarks logged!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself: Exercise\n",
    "\n",
    "**Task:** Run your own benchmark comparison.\n",
    "\n",
    "1. Choose 2-3 models you're interested in\n",
    "2. Run benchmarks on at least 2 tasks (use `limit=50` for speed)\n",
    "3. Log results to MLflow\n",
    "4. Create a visualization comparing the models\n",
    "5. Identify which model is best for your use case\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "```python\n",
    "# Pick lightweight models for faster testing\n",
    "models_to_test = [\n",
    "    \"microsoft/phi-2\",\n",
    "    \"google/gemma-2b\"\n",
    "]\n",
    "\n",
    "for model in models_to_test:\n",
    "    results = run_benchmark(\n",
    "        model_name=model,\n",
    "        tasks_list=[\"hellaswag\", \"arc_easy\"],\n",
    "        limit=50  # Quick test\n",
    "    )\n",
    "    log_benchmark_to_mlflow(model, results)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Step 1: Choose models\n",
    "\n",
    "\n",
    "# Step 2: Run benchmarks\n",
    "\n",
    "\n",
    "# Step 3: Log to MLflow\n",
    "\n",
    "\n",
    "# Step 4: Visualize comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Comparing with Different Few-Shot Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå WRONG: Different few-shot = unfair comparison\n",
    "# Model A: 0-shot -> 65%\n",
    "# Model B: 5-shot -> 72%\n",
    "# \"Model B is better!\" <- INVALID comparison!\n",
    "\n",
    "# ‚úÖ RIGHT: Same settings for all models\n",
    "# Model A: 5-shot -> 70%\n",
    "# Model B: 5-shot -> 72%\n",
    "# Now we can fairly compare!\n",
    "\n",
    "print(\"Always use the same num_fewshot for fair comparisons!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Cherry-Picking Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå WRONG: Only reporting favorable benchmarks\n",
    "# \"Our model scores 85% on HellaSwag!\" \n",
    "# (but only 35% on MMLU, which wasn't mentioned)\n",
    "\n",
    "# ‚úÖ RIGHT: Report comprehensive benchmark suite\n",
    "# - HellaSwag: 85%\n",
    "# - MMLU: 35%  <- Honest about weaknesses\n",
    "# - Average: 60%\n",
    "\n",
    "print(\"Report ALL benchmarks for honest evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Ignoring Statistical Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå WRONG: Treating small differences as significant\n",
    "# \"Model A (72.3%) beats Model B (72.1%)!\"\n",
    "# <- This difference might not be statistically significant\n",
    "\n",
    "# ‚úÖ RIGHT: Consider error bars\n",
    "# Model A: 72.3% ¬± 0.8%\n",
    "# Model B: 72.1% ¬± 0.9%\n",
    "# <- Overlapping confidence intervals = no clear winner\n",
    "\n",
    "print(\"Always check stderr (standard error) in results!\")\n",
    "print(\"A 0.2% difference with 0.8% stderr is not significant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ Understanding major LLM benchmarks (MMLU, HellaSwag, ARC, etc.)\n",
    "- ‚úÖ Using lm-evaluation-harness to run benchmarks\n",
    "- ‚úÖ Comparing models fairly with standardized tests\n",
    "- ‚úÖ Visualizing and interpreting benchmark results\n",
    "- ‚úÖ Integrating benchmarks with experiment tracking\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [lm-evaluation-harness GitHub](https://github.com/EleutherAI/lm-evaluation-harness)\n",
    "- [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n",
    "- [HELM (Stanford)](https://crfm.stanford.edu/helm/)\n",
    "- [MMLU Paper](https://arxiv.org/abs/2009.03300)\n",
    "- [Chatbot Arena](https://lmsys.org/blog/2023-05-03-arena/)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "plt.close('all')\n",
    "gc.collect()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory cleared\")\n",
    "\n",
    "print(f\"\\nüìÅ Results saved to: {RESULTS_DIR}\")\n",
    "print(f\"üìä MLflow data saved to: {MLFLOW_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Summary\n",
    "\n",
    "In this lab, we:\n",
    "\n",
    "1. **Explored** standard LLM benchmarks and what they measure\n",
    "2. **Set up** lm-evaluation-harness on DGX Spark\n",
    "3. **Ran** benchmarks on a demo model\n",
    "4. **Compared** multiple models with visualizations\n",
    "5. **Integrated** benchmark results with MLflow tracking\n",
    "\n",
    "**Next up:** Lab 4.3.4 - Custom Evaluation Framework with LLM-as-judge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
