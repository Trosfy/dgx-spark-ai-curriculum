{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.3.1: MLflow Experiment Tracking\n",
    "\n",
    "**Module:** 4.3 - MLOps & Experiment Tracking  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand why experiment tracking is essential for ML development\n",
    "- [ ] Set up MLflow tracking server on DGX Spark\n",
    "- [ ] Log parameters, metrics, and artifacts systematically\n",
    "- [ ] Compare experiments and find optimal hyperparameters\n",
    "- [ ] Use the MLflow UI for experiment visualization\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Module 4.2 (AI Safety)\n",
    "- Knowledge of: Python, basic ML training loops, PyTorch fundamentals\n",
    "- Hardware: DGX Spark (or any CUDA-capable GPU)\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The Nightmare Scenario:**\n",
    "\n",
    "You train a model, achieve 95% accuracy... and then:\n",
    "- \"Wait, what learning rate did I use?\"\n",
    "- \"Which version of the dataset was this?\"\n",
    "- \"Did I use dropout or not?\"\n",
    "\n",
    "**Every ML engineer has been there.** That's why industry leaders use experiment tracking:\n",
    "\n",
    "| Company | Use Case | Scale |\n",
    "|---------|----------|-------|\n",
    "| **Netflix** | Recommendation model A/B tests | 10,000+ experiments/month |\n",
    "| **Airbnb** | Search ranking experiments | Every model version tracked |\n",
    "| **Tesla** | Autonomous driving model iterations | Petabytes of run data |\n",
    "| **OpenAI** | GPT training runs | Every hyperparameter logged |\n",
    "\n",
    "**MLflow** is the most popular open-source tool for this, with 4 core components:\n",
    "\n",
    "1. **Tracking** - Log everything about your experiments\n",
    "2. **Projects** - Package code for reproducibility  \n",
    "3. **Models** - Manage and deploy models\n",
    "4. **Registry** - Version and stage models for production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What is Experiment Tracking?\n",
    "\n",
    "> **Imagine you're a chef creating a new chocolate chip cookie recipe.**\n",
    ">\n",
    "> Every time you bake a batch, you'd want to write down:\n",
    "> - **Ingredients** (parameters): 2 cups flour, 1/2 cup sugar, 1 tsp vanilla\n",
    "> - **How it turned out** (metrics): Taste 8/10, texture 7/10, looks 9/10\n",
    "> - **A photo** (artifacts): What the cookies actually looked like\n",
    ">\n",
    "> After 50 batches, you flip through your notebook and see:\n",
    "> - \"Aha! More brown butter = chewier cookies!\"\n",
    "> - \"Batch #37 was the best - those exact ingredients!\"\n",
    "> - \"Overbaking always drops the texture score\"\n",
    ">\n",
    "> **Experiment tracking is your ML notebook!**\n",
    "> - **Parameters:** learning_rate=0.001, batch_size=32, epochs=10\n",
    "> - **Metrics:** loss=0.15, accuracy=92%, F1=0.89\n",
    "> - **Artifacts:** model weights, confusion matrix, training curves\n",
    ">\n",
    "> Without it, you're baking cookies with no notes - hoping to remember what worked!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setting Up MLflow on DGX Spark\n",
    "\n",
    "### Why MLflow Works Great on DGX Spark\n",
    "\n",
    "- **Pure Python**: No special ARM64 compilation needed\n",
    "- **Lightweight**: Minimal overhead even for large experiments\n",
    "- **Local-first**: Works without internet, perfect for secure environments\n",
    "- **Scalable**: Same API whether tracking 10 or 10,000 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MLflow if needed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import mlflow\n",
    "    print(f\"‚úÖ MLflow already installed: v{mlflow.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing MLflow...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"mlflow\", \"-q\"])\n",
    "    import mlflow\n",
    "    print(f\"‚úÖ MLflow installed: v{mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Additional imports we'll need\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"MLflow version: {mlflow.__version__}\")\n",
    "print(f\"Python version: {sys.version.split()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure MLflow storage location\n",
    "# We'll use a local directory - in production you'd use a server\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "MODULE_DIR = (NOTEBOOK_DIR / \"..\").resolve()\n",
    "MLFLOW_DIR = MODULE_DIR / \"mlflow\"\n",
    "MLFLOW_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Set tracking URI - this is where all experiment data is stored\n",
    "tracking_uri = f\"file://{MLFLOW_DIR}\"\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "\n",
    "print(f\"üìÅ MLflow storage: {MLFLOW_DIR}\")\n",
    "print(f\"üîó Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print()\n",
    "print(\"üí° Storage Options:\")\n",
    "print(\"   file:///path/to/dir    - Local development\")\n",
    "print(\"   http://localhost:5000  - Local server (team sharing)\")\n",
    "print(\"   http://mlflow.corp.com - Production server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "We configured MLflow to store experiment data locally. The `tracking_uri` tells MLflow:\n",
    "- **Where to save** parameters, metrics, and artifacts\n",
    "- **How to connect** (file system vs. HTTP server)\n",
    "\n",
    "For DGX Spark development, local storage is perfect. For team collaboration, you'd run an MLflow server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Your First Experiment\n",
    "\n",
    "### üßí ELI5: Experiments vs Runs\n",
    "\n",
    "> **Experiment** = A recipe book (\"Chocolate Chip Cookies\")\n",
    ">\n",
    "> **Run** = One attempt at the recipe (\"Batch #5: with brown butter\")\n",
    ">\n",
    "> You might have:\n",
    "> - Experiment: \"LLM Fine-tuning\"\n",
    ">   - Run 1: Phi-2, lr=1e-4, epochs=3\n",
    ">   - Run 2: Phi-2, lr=1e-5, epochs=5\n",
    ">   - Run 3: Phi-2, lr=1e-4, with LoRA rank=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or get an experiment\n",
    "EXPERIMENT_NAME = \"LLM-Finetuning-Demo\"\n",
    "\n",
    "# Check if experiment exists\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "\n",
    "if experiment is None:\n",
    "    # Create new experiment with metadata tags\n",
    "    experiment_id = mlflow.create_experiment(\n",
    "        EXPERIMENT_NAME,\n",
    "        tags={\n",
    "            \"project\": \"dgx-spark-curriculum\",\n",
    "            \"module\": \"4.3\",\n",
    "            \"hardware\": \"DGX Spark\",\n",
    "            \"created_by\": \"student\"\n",
    "        }\n",
    "    )\n",
    "    print(f\"‚ú® Created new experiment: '{EXPERIMENT_NAME}'\")\n",
    "    print(f\"   Experiment ID: {experiment_id}\")\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id\n",
    "    print(f\"üìÇ Using existing experiment: '{EXPERIMENT_NAME}'\")\n",
    "    print(f\"   Experiment ID: {experiment_id}\")\n",
    "\n",
    "# Set as active experiment (all future runs go here)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start your first run!\n",
    "# The context manager ensures the run is properly closed even if errors occur\n",
    "\n",
    "with mlflow.start_run(run_name=\"my-first-run\") as run:\n",
    "    \n",
    "    # ========== LOG PARAMETERS ==========\n",
    "    # Parameters are the INPUTS to your experiment\n",
    "    # Things you set BEFORE training starts\n",
    "    \n",
    "    mlflow.log_param(\"model_name\", \"microsoft/phi-2\")\n",
    "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
    "    mlflow.log_param(\"batch_size\", 8)\n",
    "    mlflow.log_param(\"epochs\", 3)\n",
    "    mlflow.log_param(\"lora_rank\", 16)\n",
    "    mlflow.log_param(\"lora_alpha\", 32)\n",
    "    mlflow.log_param(\"optimizer\", \"AdamW\")\n",
    "    \n",
    "    # You can also log multiple params at once\n",
    "    mlflow.log_params({\n",
    "        \"warmup_steps\": 100,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"gradient_accumulation\": 4\n",
    "    })\n",
    "    \n",
    "    # ========== LOG METRICS ==========\n",
    "    # Metrics are the OUTPUTS/RESULTS of your experiment\n",
    "    # Things measured DURING or AFTER training\n",
    "    \n",
    "    # Final metrics (single values)\n",
    "    mlflow.log_metric(\"final_train_loss\", 0.45)\n",
    "    mlflow.log_metric(\"final_eval_loss\", 0.52)\n",
    "    mlflow.log_metric(\"final_accuracy\", 0.87)\n",
    "    mlflow.log_metric(\"training_time_minutes\", 45)\n",
    "    mlflow.log_metric(\"gpu_memory_gb\", 24.5)\n",
    "    \n",
    "    # Metrics over time (for charts) - use 'step' parameter\n",
    "    for epoch in range(3):\n",
    "        # Simulate decreasing loss, increasing accuracy\n",
    "        train_loss = 0.8 - epoch * 0.15\n",
    "        eval_loss = 0.9 - epoch * 0.12\n",
    "        accuracy = 0.65 + epoch * 0.08\n",
    "        \n",
    "        mlflow.log_metrics({\n",
    "            \"epoch_train_loss\": train_loss,\n",
    "            \"epoch_eval_loss\": eval_loss,\n",
    "            \"epoch_accuracy\": accuracy\n",
    "        }, step=epoch)\n",
    "    \n",
    "    # ========== LOG TAGS ==========\n",
    "    # Tags are metadata for organization and filtering\n",
    "    \n",
    "    mlflow.set_tag(\"hardware\", \"DGX Spark\")\n",
    "    mlflow.set_tag(\"framework\", \"pytorch\")\n",
    "    mlflow.set_tag(\"status\", \"completed\")\n",
    "    mlflow.set_tag(\"dataset\", \"alpaca-cleaned\")\n",
    "    mlflow.set_tag(\"notes\", \"First successful run with LoRA\")\n",
    "    \n",
    "    # Save run info for later\n",
    "    run_id = run.info.run_id\n",
    "    artifact_uri = run.info.artifact_uri\n",
    "\n",
    "print(f\"\\nüéâ Run completed!\")\n",
    "print(f\"   Run ID: {run_id}\")\n",
    "print(f\"   Artifact URI: {artifact_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View what we logged\n",
    "run_data = mlflow.get_run(run_id)\n",
    "\n",
    "print(\"üìä LOGGED DATA SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìå Parameters (inputs):\")\n",
    "for key, value in sorted(run_data.data.params.items()):\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\nüìà Metrics (outputs):\")\n",
    "for key, value in sorted(run_data.data.metrics.items()):\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\nüè∑Ô∏è Tags (metadata):\")\n",
    "for key, value in sorted(run_data.data.tags.items()):\n",
    "    if not key.startswith(\"mlflow.\"):  # Skip internal tags\n",
    "        print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Understanding Parameters vs Metrics vs Tags\n",
    "\n",
    "| Type | When Set | Example | Use Case |\n",
    "|------|----------|---------|----------|\n",
    "| **Parameters** | Before training | `learning_rate=0.001` | Hyperparameter search |\n",
    "| **Metrics** | During/after training | `accuracy=0.95` | Performance comparison |\n",
    "| **Tags** | Anytime | `status=completed` | Organization & filtering |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Logging Artifacts\n",
    "\n",
    "Artifacts are **files** associated with a run: models, plots, configs, predictions, etc.\n",
    "\n",
    "### üßí ELI5: What Are Artifacts?\n",
    "\n",
    "> Back to our cookie recipe:\n",
    "> - Parameters = ingredient list\n",
    "> - Metrics = taste scores\n",
    "> - **Artifacts = photos of the actual cookies!**\n",
    ">\n",
    "> In ML:\n",
    "> - Model weights (the trained model itself)\n",
    "> - Training curves (loss over time plots)\n",
    "> - Config files (exact settings used)\n",
    "> - Predictions (sample outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_curves():\n",
    "    \"\"\"Generate realistic-looking training curves.\"\"\"\n",
    "    epochs = np.arange(1, 11)\n",
    "    \n",
    "    # Realistic training dynamics\n",
    "    train_loss = 1.2 * np.exp(-epochs * 0.35) + 0.08 + np.random.normal(0, 0.02, len(epochs))\n",
    "    val_loss = 1.3 * np.exp(-epochs * 0.28) + 0.12 + np.random.normal(0, 0.03, len(epochs))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(epochs, train_loss, 'b-o', label='Training Loss', linewidth=2, markersize=6)\n",
    "    axes[0].plot(epochs, val_loss, 'r-s', label='Validation Loss', linewidth=2, markersize=6)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].set_title('Training Progress', fontsize=14)\n",
    "    axes[0].legend(fontsize=11)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_ylim(0, 1.5)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    train_acc = 1 - train_loss * 0.5\n",
    "    val_acc = 1 - val_loss * 0.5\n",
    "    axes[1].plot(epochs, train_acc, 'b-o', label='Training Accuracy', linewidth=2, markersize=6)\n",
    "    axes[1].plot(epochs, val_acc, 'r-s', label='Validation Accuracy', linewidth=2, markersize=6)\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[1].set_title('Accuracy Progress', fontsize=14)\n",
    "    axes[1].legend(fontsize=11)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].set_ylim(0.3, 1.0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Preview the plot\n",
    "fig = create_training_curves()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a run with artifacts\n",
    "with mlflow.start_run(run_name=\"run-with-artifacts\") as run:\n",
    "    \n",
    "    # Log parameters and metrics as before\n",
    "    mlflow.log_params({\n",
    "        \"model_name\": \"microsoft/phi-2\",\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"batch_size\": 16,\n",
    "        \"epochs\": 10\n",
    "    })\n",
    "    \n",
    "    mlflow.log_metrics({\n",
    "        \"final_train_loss\": 0.12,\n",
    "        \"final_val_loss\": 0.18,\n",
    "        \"final_accuracy\": 0.91\n",
    "    })\n",
    "    \n",
    "    # ========== ARTIFACT 1: Training Plot ==========\n",
    "    fig = create_training_curves()\n",
    "    plot_path = \"/tmp/training_curves.png\"\n",
    "    fig.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Log to 'plots' subdirectory in artifacts\n",
    "    mlflow.log_artifact(plot_path, artifact_path=\"plots\")\n",
    "    print(f\"üìä Logged plot: {plot_path}\")\n",
    "    \n",
    "    # ========== ARTIFACT 2: Config File ==========\n",
    "    config = {\n",
    "        \"model\": {\n",
    "            \"name\": \"microsoft/phi-2\",\n",
    "            \"dtype\": \"bfloat16\",\n",
    "            \"max_length\": 2048\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"learning_rate\": 2e-4,\n",
    "            \"batch_size\": 16,\n",
    "            \"epochs\": 10,\n",
    "            \"warmup_ratio\": 0.1,\n",
    "            \"weight_decay\": 0.01\n",
    "        },\n",
    "        \"lora\": {\n",
    "            \"rank\": 16,\n",
    "            \"alpha\": 32,\n",
    "            \"dropout\": 0.05,\n",
    "            \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "        },\n",
    "        \"hardware\": {\n",
    "            \"device\": \"DGX Spark\",\n",
    "            \"memory_gb\": 128,\n",
    "            \"precision\": \"bfloat16\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config_path = \"/tmp/training_config.json\"\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    mlflow.log_artifact(config_path, artifact_path=\"configs\")\n",
    "    print(f\"‚öôÔ∏è Logged config: {config_path}\")\n",
    "    \n",
    "    # ========== ARTIFACT 3: Training Notes ==========\n",
    "    notes = \"\"\"# Training Notes\n",
    "\n",
    "## Setup\n",
    "- Hardware: DGX Spark with 128GB unified memory\n",
    "- Framework: PyTorch 2.x with bfloat16\n",
    "- Method: LoRA fine-tuning (rank=16, alpha=32)\n",
    "\n",
    "## Observations\n",
    "- Learning rate 2e-4 worked well (1e-4 was too slow)\n",
    "- Batch size 16 maximized GPU utilization\n",
    "- Warmup helped stabilize early training\n",
    "\n",
    "## Issues\n",
    "- Initial runs had gradient overflow - fixed with gradient clipping\n",
    "- Had to reduce context length from 4096 to 2048 for memory\n",
    "\n",
    "## Next Steps\n",
    "- Try rank=32 for better quality\n",
    "- Experiment with learning rate scheduling\n",
    "\"\"\"\n",
    "    \n",
    "    notes_path = \"/tmp/training_notes.md\"\n",
    "    with open(notes_path, 'w') as f:\n",
    "        f.write(notes)\n",
    "    \n",
    "    mlflow.log_artifact(notes_path)  # Root of artifacts\n",
    "    print(f\"üìù Logged notes: {notes_path}\")\n",
    "    \n",
    "    # ========== ARTIFACT 4: Sample Predictions ==========\n",
    "    predictions = [\n",
    "        {\"input\": \"What is machine learning?\", \n",
    "         \"output\": \"Machine learning is a subset of AI that enables systems to learn from data...\"},\n",
    "        {\"input\": \"Explain gradient descent\",\n",
    "         \"output\": \"Gradient descent is an optimization algorithm that minimizes loss by...\"},\n",
    "        {\"input\": \"What is a transformer?\",\n",
    "         \"output\": \"A transformer is a neural network architecture that uses self-attention...\"}\n",
    "    ]\n",
    "    \n",
    "    predictions_path = \"/tmp/sample_predictions.json\"\n",
    "    with open(predictions_path, 'w') as f:\n",
    "        json.dump(predictions, f, indent=2)\n",
    "    \n",
    "    mlflow.log_artifact(predictions_path, artifact_path=\"predictions\")\n",
    "    print(f\"üîÆ Logged predictions: {predictions_path}\")\n",
    "    \n",
    "    artifact_run_id = run.info.run_id\n",
    "\n",
    "print(f\"\\n‚úÖ Run completed with artifacts!\")\n",
    "print(f\"   Run ID: {artifact_run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all artifacts for this run\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "print(\"üìÅ ARTIFACTS STRUCTURE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def list_artifacts_recursive(run_id, path=\"\"):\n",
    "    \"\"\"Recursively list all artifacts.\"\"\"\n",
    "    artifacts = client.list_artifacts(run_id, path)\n",
    "    for artifact in artifacts:\n",
    "        indent = \"  \" * path.count(\"/\")\n",
    "        if artifact.is_dir:\n",
    "            print(f\"{indent}üìÇ {artifact.path}/\")\n",
    "            list_artifacts_recursive(run_id, artifact.path)\n",
    "        else:\n",
    "            size = artifact.file_size if artifact.file_size else 0\n",
    "            print(f\"{indent}üìÑ {artifact.path} ({size:,} bytes)\")\n",
    "\n",
    "list_artifacts_recursive(artifact_run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Logging PyTorch Models\n",
    "\n",
    "MLflow has native support for PyTorch models. This is incredibly powerful for:\n",
    "- **Reproducibility**: Load the exact model from any run\n",
    "- **Deployment**: MLflow models can be served as REST APIs\n",
    "- **Versioning**: Track model evolution over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple sentiment classifier for demonstration.\n",
    "    In practice, you'd use a transformer-based model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 30000, embed_dim: int = 256, \n",
    "                 hidden_dim: int = 512, num_classes: int = 3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)\n",
    "        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        _, (hidden, _) = self.lstm(embedded)  # hidden: (2, batch, hidden_dim)\n",
    "        \n",
    "        # Concatenate forward and backward hidden states\n",
    "        hidden = torch.cat([hidden[0], hidden[1]], dim=1)  # (batch, hidden_dim * 2)\n",
    "        \n",
    "        return self.classifier(hidden)\n",
    "\n",
    "# Create model\n",
    "model = SentimentClassifier()\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Model size: ~{total_params * 4 / 1e6:.1f} MB (FP32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the model to MLflow\n",
    "with mlflow.start_run(run_name=\"sentiment-model-v1\") as run:\n",
    "    \n",
    "    # Log model architecture parameters\n",
    "    mlflow.log_params({\n",
    "        \"vocab_size\": 30000,\n",
    "        \"embed_dim\": 256,\n",
    "        \"hidden_dim\": 512,\n",
    "        \"num_classes\": 3,\n",
    "        \"total_params\": total_params,\n",
    "        \"architecture\": \"BiLSTM\"\n",
    "    })\n",
    "    \n",
    "    # Simulate training metrics\n",
    "    for epoch in range(5):\n",
    "        train_loss = 0.8 * (0.7 ** epoch) + np.random.normal(0, 0.02)\n",
    "        val_loss = 0.9 * (0.75 ** epoch) + np.random.normal(0, 0.03)\n",
    "        accuracy = 0.6 + epoch * 0.08 + np.random.normal(0, 0.01)\n",
    "        \n",
    "        mlflow.log_metrics({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"accuracy\": min(accuracy, 0.98)\n",
    "        }, step=epoch)\n",
    "    \n",
    "    # Create sample input for signature\n",
    "    sample_input = torch.randint(0, 30000, (1, 128)).to(device)\n",
    "    \n",
    "    # Log the model!\n",
    "    # This saves: weights, architecture info, requirements, input example\n",
    "    mlflow.pytorch.log_model(\n",
    "        model,\n",
    "        artifact_path=\"model\",\n",
    "        input_example=sample_input.cpu().numpy(),\n",
    "        registered_model_name=None  # Don't register yet (we'll do this in Lab 4.3.6)\n",
    "    )\n",
    "    \n",
    "    model_run_id = run.info.run_id\n",
    "\n",
    "print(f\"\\n‚úÖ Model logged successfully!\")\n",
    "print(f\"   Run ID: {model_run_id}\")\n",
    "print(f\"   Model URI: runs:/{model_run_id}/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model back from MLflow\n",
    "model_uri = f\"runs:/{model_run_id}/model\"\n",
    "\n",
    "print(f\"Loading model from: {model_uri}\")\n",
    "loaded_model = mlflow.pytorch.load_model(model_uri)\n",
    "loaded_model = loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "print(f\"\\n‚úÖ Model loaded!\")\n",
    "print(f\"   Type: {type(loaded_model).__name__}\")\n",
    "\n",
    "# Test inference\n",
    "test_input = torch.randint(0, 30000, (2, 128)).to(device)\n",
    "with torch.no_grad():\n",
    "    output = loaded_model(test_input)\n",
    "\n",
    "print(f\"\\nüîÆ Inference test:\")\n",
    "print(f\"   Input shape: {test_input.shape}\")\n",
    "print(f\"   Output shape: {output.shape}\")\n",
    "print(f\"   Predictions: {torch.argmax(output, dim=1).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Running a Hyperparameter Sweep\n",
    "\n",
    "The real power of experiment tracking: running many experiments and finding the best configuration.\n",
    "\n",
    "### üßí ELI5: Hyperparameter Sweeps\n",
    "\n",
    "> Imagine trying every possible cookie recipe variation:\n",
    "> - Sugar: 1/4 cup, 1/2 cup, 3/4 cup\n",
    "> - Butter: 1/2 cup, 1 cup\n",
    "> - Baking time: 10 min, 12 min, 15 min\n",
    ">\n",
    "> That's 3 √ó 2 √ó 3 = 18 batches of cookies!\n",
    ">\n",
    "> With good notes (MLflow), you can easily find:\n",
    "> - \"1/2 cup sugar + 1 cup butter + 12 min = BEST cookies!\"\n",
    ">\n",
    "> In ML, we call this a **hyperparameter sweep** or **grid search**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    \"learning_rate\": [1e-5, 5e-5, 1e-4, 5e-4],\n",
    "    \"batch_size\": [8, 16, 32],\n",
    "    \"lora_rank\": [8, 16, 32]\n",
    "}\n",
    "\n",
    "# Calculate total combinations\n",
    "total_combinations = 1\n",
    "for values in param_grid.values():\n",
    "    total_combinations *= len(values)\n",
    "\n",
    "print(f\"üî¨ HYPERPARAMETER SWEEP\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Parameters to search:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"   {param}: {values}\")\n",
    "print(f\"\\nTotal combinations: {total_combinations}\")\n",
    "print(\"\\nRunning sweep...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new experiment for the sweep\n",
    "SWEEP_EXPERIMENT = \"Hyperparameter-Sweep-Demo\"\n",
    "mlflow.set_experiment(SWEEP_EXPERIMENT)\n",
    "\n",
    "# Run all combinations\n",
    "results = []\n",
    "\n",
    "for lr, bs, rank in itertools.product(\n",
    "    param_grid[\"learning_rate\"],\n",
    "    param_grid[\"batch_size\"],\n",
    "    param_grid[\"lora_rank\"]\n",
    "):\n",
    "    run_name = f\"lr={lr}_bs={bs}_r={rank}\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # Log parameters\n",
    "        mlflow.log_params({\n",
    "            \"learning_rate\": lr,\n",
    "            \"batch_size\": bs,\n",
    "            \"lora_rank\": rank,\n",
    "            \"model\": \"phi-2\",\n",
    "            \"epochs\": 5\n",
    "        })\n",
    "        \n",
    "        # Simulate training with realistic patterns\n",
    "        # (In real life, you'd actually train here!)\n",
    "        \n",
    "        # Higher LR = faster convergence but more noise\n",
    "        lr_factor = np.log10(lr) + 5  # Normalized: 0-4\n",
    "        \n",
    "        # Larger batch = more stable but slower\n",
    "        bs_factor = np.log2(bs) / 5  # Normalized: ~0.6-1.0\n",
    "        \n",
    "        # Higher rank = better capacity\n",
    "        rank_factor = rank / 32  # Normalized: 0.25-1.0\n",
    "        \n",
    "        # Simulate final metrics\n",
    "        base_accuracy = 0.75 + 0.1 * rank_factor + 0.05 * lr_factor\n",
    "        noise = random.gauss(0, 0.02)\n",
    "        final_accuracy = min(0.98, max(0.5, base_accuracy + noise))\n",
    "        \n",
    "        final_loss = (1 - final_accuracy) * 2 + random.gauss(0, 0.05)\n",
    "        final_loss = max(0.05, final_loss)\n",
    "        \n",
    "        # Memory usage increases with batch size and rank\n",
    "        memory_gb = 10 + bs * 0.5 + rank * 0.3\n",
    "        \n",
    "        # Training time inversely related to batch size\n",
    "        training_time = 120 / (bs / 8) * (rank / 16)\n",
    "        \n",
    "        # Log step-wise metrics\n",
    "        for epoch in range(5):\n",
    "            epoch_loss = final_loss * (2 - epoch * 0.2) + random.gauss(0, 0.03)\n",
    "            epoch_acc = final_accuracy * (0.7 + epoch * 0.06) + random.gauss(0, 0.02)\n",
    "            \n",
    "            mlflow.log_metrics({\n",
    "                \"train_loss\": epoch_loss,\n",
    "                \"accuracy\": min(0.98, epoch_acc)\n",
    "            }, step=epoch)\n",
    "        \n",
    "        # Log final metrics\n",
    "        mlflow.log_metrics({\n",
    "            \"final_loss\": final_loss,\n",
    "            \"final_accuracy\": final_accuracy,\n",
    "            \"memory_gb\": memory_gb,\n",
    "            \"training_time_min\": training_time\n",
    "        })\n",
    "        \n",
    "        results.append({\n",
    "            \"lr\": lr, \"bs\": bs, \"rank\": rank,\n",
    "            \"accuracy\": final_accuracy, \"loss\": final_loss\n",
    "        })\n",
    "\n",
    "print(f\"\\n‚úÖ Sweep complete! {len(results)} runs logged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query and analyze results\n",
    "import pandas as pd\n",
    "\n",
    "# Get experiment ID\n",
    "sweep_exp = mlflow.get_experiment_by_name(SWEEP_EXPERIMENT)\n",
    "\n",
    "# Search for all runs, sorted by accuracy\n",
    "runs_df = mlflow.search_runs(\n",
    "    experiment_ids=[sweep_exp.experiment_id],\n",
    "    filter_string=\"\",\n",
    "    order_by=[\"metrics.final_accuracy DESC\"]\n",
    ")\n",
    "\n",
    "# Display top results\n",
    "display_cols = [\n",
    "    \"params.learning_rate\",\n",
    "    \"params.batch_size\", \n",
    "    \"params.lora_rank\",\n",
    "    \"metrics.final_accuracy\",\n",
    "    \"metrics.final_loss\",\n",
    "    \"metrics.memory_gb\"\n",
    "]\n",
    "\n",
    "print(\"üèÜ TOP 10 CONFIGURATIONS (by accuracy)\")\n",
    "print(\"=\" * 80)\n",
    "print(runs_df[display_cols].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best run\n",
    "best_run = runs_df.iloc[0]\n",
    "\n",
    "print(\"\\nü•á BEST CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Run ID: {best_run['run_id'][:8]}...\")\n",
    "print(f\"\\nHyperparameters:\")\n",
    "print(f\"   Learning Rate: {best_run['params.learning_rate']}\")\n",
    "print(f\"   Batch Size: {best_run['params.batch_size']}\")\n",
    "print(f\"   LoRA Rank: {best_run['params.lora_rank']}\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"   Accuracy: {best_run['metrics.final_accuracy']:.4f}\")\n",
    "print(f\"   Loss: {best_run['metrics.final_loss']:.4f}\")\n",
    "print(f\"   Memory: {best_run['metrics.memory_gb']:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sweep results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Prepare data\n",
    "sweep_data = runs_df.copy()\n",
    "sweep_data['lr'] = sweep_data['params.learning_rate'].astype(float)\n",
    "sweep_data['bs'] = sweep_data['params.batch_size'].astype(int)\n",
    "sweep_data['rank'] = sweep_data['params.lora_rank'].astype(int)\n",
    "\n",
    "# Plot 1: Learning Rate vs Accuracy (colored by rank)\n",
    "for rank in sorted(sweep_data['rank'].unique()):\n",
    "    subset = sweep_data[sweep_data['rank'] == rank]\n",
    "    axes[0, 0].scatter(\n",
    "        subset['lr'], \n",
    "        subset['metrics.final_accuracy'],\n",
    "        label=f'rank={rank}',\n",
    "        s=100, alpha=0.7\n",
    "    )\n",
    "axes[0, 0].set_xscale('log')\n",
    "axes[0, 0].set_xlabel('Learning Rate')\n",
    "axes[0, 0].set_ylabel('Final Accuracy')\n",
    "axes[0, 0].set_title('Learning Rate vs Accuracy')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Batch Size vs Loss (colored by rank)\n",
    "for rank in sorted(sweep_data['rank'].unique()):\n",
    "    subset = sweep_data[sweep_data['rank'] == rank]\n",
    "    axes[0, 1].scatter(\n",
    "        subset['bs'],\n",
    "        subset['metrics.final_loss'],\n",
    "        label=f'rank={rank}',\n",
    "        s=100, alpha=0.7\n",
    "    )\n",
    "axes[0, 1].set_xlabel('Batch Size')\n",
    "axes[0, 1].set_ylabel('Final Loss')\n",
    "axes[0, 1].set_title('Batch Size vs Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Accuracy vs Memory (Pareto frontier)\n",
    "sc = axes[1, 0].scatter(\n",
    "    sweep_data['metrics.memory_gb'],\n",
    "    sweep_data['metrics.final_accuracy'],\n",
    "    c=sweep_data['rank'],\n",
    "    cmap='viridis',\n",
    "    s=100, alpha=0.7\n",
    ")\n",
    "axes[1, 0].set_xlabel('Memory Usage (GB)')\n",
    "axes[1, 0].set_ylabel('Final Accuracy')\n",
    "axes[1, 0].set_title('Accuracy vs Memory Trade-off')\n",
    "plt.colorbar(sc, ax=axes[1, 0], label='LoRA Rank')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Heatmap of best accuracy per LR x Rank\n",
    "pivot = sweep_data.pivot_table(\n",
    "    values='metrics.final_accuracy',\n",
    "    index='lr',\n",
    "    columns='rank',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "im = axes[1, 1].imshow(pivot.values, cmap='RdYlGn', aspect='auto')\n",
    "axes[1, 1].set_xticks(range(len(pivot.columns)))\n",
    "axes[1, 1].set_xticklabels(pivot.columns)\n",
    "axes[1, 1].set_yticks(range(len(pivot.index)))\n",
    "axes[1, 1].set_yticklabels([f'{x:.0e}' for x in pivot.index])\n",
    "axes[1, 1].set_xlabel('LoRA Rank')\n",
    "axes[1, 1].set_ylabel('Learning Rate')\n",
    "axes[1, 1].set_title('Mean Accuracy Heatmap')\n",
    "plt.colorbar(im, ax=axes[1, 1], label='Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/sweep_analysis.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Saved sweep analysis to /tmp/sweep_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: The MLflow UI\n",
    "\n",
    "MLflow includes a beautiful web interface for exploring experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "üñ•Ô∏è  STARTING THE MLFLOW UI\n",
    "{'=' * 60}\n",
    "\n",
    "To view your experiments in a web browser:\n",
    "\n",
    "1. Open a terminal and run:\n",
    "   \n",
    "   mlflow ui --backend-store-uri {MLFLOW_DIR} --host 0.0.0.0 --port 5000\n",
    "\n",
    "2. Open in browser: http://localhost:5000\n",
    "\n",
    "{'=' * 60}\n",
    "\n",
    "üìä UI Features:\n",
    "   ‚Ä¢ Compare runs side-by-side\n",
    "   ‚Ä¢ View metric charts over time\n",
    "   ‚Ä¢ Download artifacts\n",
    "   ‚Ä¢ Filter and search runs\n",
    "   ‚Ä¢ Export to CSV\n",
    "\n",
    "{'=' * 60}\n",
    "\n",
    "üê≥ For DGX Spark with Docker, start your container with port exposed:\n",
    "\n",
    "   docker run --gpus all -it --rm \\\\\n",
    "       -v $HOME/workspace:/workspace \\\\\n",
    "       -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\\\n",
    "       --ipc=host \\\\\n",
    "       -p 5000:5000 \\\\\n",
    "       nvcr.io/nvidia/pytorch:25.11-py3\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Autologging\n",
    "\n",
    "MLflow can automatically capture metrics from popular frameworks - no manual logging needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autologging for PyTorch\n",
    "mlflow.pytorch.autolog(\n",
    "    log_models=True,        # Automatically log model artifacts\n",
    "    log_every_n_epoch=1,    # Log metrics every epoch\n",
    "    log_every_n_step=None,  # Don't log every step (too much data)\n",
    "    registered_model_name=None  # Don't auto-register\n",
    ")\n",
    "\n",
    "print(\"‚úÖ PyTorch autologging enabled!\")\n",
    "print(\"\"\"\n",
    "With autologging, MLflow automatically captures:\n",
    "‚Ä¢ Training loss and metrics\n",
    "‚Ä¢ Model architecture\n",
    "‚Ä¢ Optimizer parameters\n",
    "‚Ä¢ Model artifacts\n",
    "‚Ä¢ Hardware info (GPU, memory)\n",
    "\n",
    "Just run your normal training code - MLflow handles the rest!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of autologging with HuggingFace Trainer\n",
    "example_code = '''\n",
    "# Autologging works seamlessly with HuggingFace!\n",
    "\n",
    "import mlflow\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Enable autologging for transformers\n",
    "mlflow.transformers.autolog()\n",
    "\n",
    "# Your normal training code - MLflow captures everything!\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# This automatically logs to MLflow!\n",
    "trainer.train()\n",
    "\n",
    "# Check the MLflow UI - you'll see:\n",
    "# - All training arguments\n",
    "# - Loss curves\n",
    "# - Evaluation metrics\n",
    "# - Model checkpoint\n",
    "'''\n",
    "\n",
    "print(\"üìù Example: Autologging with HuggingFace Trainer\")\n",
    "print(\"=\" * 50)\n",
    "print(example_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself: Exercise\n",
    "\n",
    "**Task:** Create your own experiment tracking workflow.\n",
    "\n",
    "1. Create a new experiment called `\"my-first-experiment\"`\n",
    "2. Run at least 5 training simulations with different hyperparameters\n",
    "3. Log:\n",
    "   - At least 4 parameters\n",
    "   - Metrics over time (multiple epochs)\n",
    "   - One artifact (plot or config file)\n",
    "4. Query to find the best run\n",
    "5. Create a visualization comparing runs\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "```python\n",
    "# Step 1: Create experiment\n",
    "mlflow.set_experiment(\"my-first-experiment\")\n",
    "\n",
    "# Step 2: Loop through configurations\n",
    "for lr in [1e-5, 1e-4, 1e-3]:\n",
    "    for dropout in [0.1, 0.3]:\n",
    "        with mlflow.start_run(run_name=f\"lr={lr}_drop={dropout}\"):\n",
    "            # Log params, simulate training, log metrics\n",
    "            ...\n",
    "\n",
    "# Step 3: Query results\n",
    "runs = mlflow.search_runs(order_by=[\"metrics.accuracy DESC\"])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Step 1: Create experiment\n",
    "\n",
    "\n",
    "# Step 2: Run training simulations\n",
    "\n",
    "\n",
    "# Step 3: Log artifacts\n",
    "\n",
    "\n",
    "# Step 4: Query for best run\n",
    "\n",
    "\n",
    "# Step 5: Visualize results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Using Context Managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå WRONG: Run never ends if code crashes\n",
    "# mlflow.start_run()\n",
    "# ... training code that might crash ...\n",
    "# mlflow.end_run()  # Never reached!\n",
    "\n",
    "# ‚úÖ RIGHT: Use context manager - run closes automatically\n",
    "# with mlflow.start_run():\n",
    "#     ... training code ...\n",
    "#     # Run ends automatically, even on error\n",
    "\n",
    "print(\"Always use 'with mlflow.start_run():' for automatic cleanup!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Logging Too Frequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå WRONG: Logging every step creates huge databases\n",
    "# for step in range(1_000_000):\n",
    "#     mlflow.log_metric(\"loss\", loss, step=step)  # 1M entries!\n",
    "\n",
    "# ‚úÖ RIGHT: Log at reasonable intervals\n",
    "# for step in range(1_000_000):\n",
    "#     if step % 1000 == 0:  # Every 1000 steps\n",
    "#         mlflow.log_metric(\"loss\", loss, step=step)  # Only 1000 entries\n",
    "\n",
    "print(\"Log metrics at reasonable intervals (every N steps/epochs).\")\n",
    "print(\"For long training: every 100-1000 steps is usually enough.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Not Setting Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå WRONG: All runs go to \"Default\" experiment\n",
    "# with mlflow.start_run():\n",
    "#     ...  # Which project is this for??\n",
    "\n",
    "# ‚úÖ RIGHT: Always set experiment first\n",
    "# mlflow.set_experiment(\"my-project-name\")\n",
    "# with mlflow.start_run():\n",
    "#     ...  # Clearly organized!\n",
    "\n",
    "print(\"Always call mlflow.set_experiment() before starting runs!\")\n",
    "print(\"This keeps your experiments organized and findable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 4: Nested Runs Without Explicit Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå WRONG: Accidental nested runs\n",
    "# with mlflow.start_run():  # Parent run\n",
    "#     for i in range(3):\n",
    "#         with mlflow.start_run():  # Creates NESTED runs!\n",
    "#             ...\n",
    "\n",
    "# ‚úÖ RIGHT: Explicit nested runs OR separate runs\n",
    "# Option A: Separate runs\n",
    "# for i in range(3):\n",
    "#     with mlflow.start_run(run_name=f\"run-{i}\"):\n",
    "#         ...\n",
    "\n",
    "# Option B: Explicit nested runs\n",
    "# with mlflow.start_run(run_name=\"parent\") as parent:\n",
    "#     for i in range(3):\n",
    "#         with mlflow.start_run(run_name=f\"child-{i}\", nested=True):\n",
    "#             ...\n",
    "\n",
    "print(\"Use nested=True when you intentionally want nested runs.\")\n",
    "print(\"For hyperparameter sweeps, separate runs are usually better.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ Setting up MLflow tracking on DGX Spark\n",
    "- ‚úÖ Logging parameters, metrics, and artifacts\n",
    "- ‚úÖ Running hyperparameter sweeps and finding optimal configs\n",
    "- ‚úÖ Using the MLflow UI for visualization\n",
    "- ‚úÖ Autologging with PyTorch and Transformers\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Set up production-grade MLflow:**\n",
    "\n",
    "1. Run MLflow server with SQLite/PostgreSQL backend\n",
    "2. Use MinIO or S3 for artifact storage\n",
    "3. Set up authentication with nginx reverse proxy\n",
    "4. Create a CI/CD pipeline that logs training runs automatically\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [MLflow Documentation](https://mlflow.org/docs/latest/)\n",
    "- [MLflow Tracking Guide](https://mlflow.org/docs/latest/tracking.html)\n",
    "- [MLflow with PyTorch](https://mlflow.org/docs/latest/python_api/mlflow.pytorch.html)\n",
    "- [MLflow with Transformers](https://mlflow.org/docs/latest/llms/transformers/index.html)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources\n",
    "import gc\n",
    "\n",
    "# Clear matplotlib figures\n",
    "plt.close('all')\n",
    "\n",
    "# Clear Python garbage\n",
    "gc.collect()\n",
    "\n",
    "# Clear GPU memory if available\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory freed. Current usage: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "print(f\"\\nüìÅ MLflow data saved to: {MLFLOW_DIR}\")\n",
    "print(f\"\\nüñ•Ô∏è  To view results, run:\")\n",
    "print(f\"   mlflow ui --backend-store-uri {MLFLOW_DIR} --port 5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Summary\n",
    "\n",
    "In this lab, we:\n",
    "\n",
    "1. **Set up** MLflow for local experiment tracking on DGX Spark\n",
    "2. **Created** experiments and logged training runs with parameters, metrics, and artifacts\n",
    "3. **Logged** PyTorch models for later retrieval\n",
    "4. **Ran** a hyperparameter sweep and analyzed results\n",
    "5. **Learned** best practices for production experiment tracking\n",
    "\n",
    "**Next up:** Lab 4.3.2 - Weights & Biases Integration for team collaboration and advanced visualizations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
