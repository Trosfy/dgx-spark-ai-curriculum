{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.2.1: LLM Benchmark Suite\n",
    "\n",
    "**Module:** 4.2 - Benchmarking, Evaluation & MLOps  \n",
    "**Time:** 3 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand what LLM benchmarks measure and why they matter\n",
    "- [ ] Run the LM Evaluation Harness on multiple models\n",
    "- [ ] Interpret benchmark results correctly\n",
    "- [ ] Compare model performance across different tasks\n",
    "- [ ] Know the limitations of standard benchmarks\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Module 3.1 -14 (LLM fundamentals)\n",
    "- Knowledge of: Model loading, inference basics\n",
    "- Hardware: DGX Spark with 128GB unified memory\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**Imagine you're a chef opening a new restaurant.** Before the grand opening, you'd want to:\n",
    "- Taste-test every dish (does it taste good?)\n",
    "- Check presentation (does it look appetizing?)\n",
    "- Time the kitchen (can we serve quickly?)\n",
    "- Get outside opinions (what do critics say?)\n",
    "\n",
    "**LLM benchmarks are the same idea for AI models!** They provide standardized tests that let you:\n",
    "- Compare your model against others on a level playing field\n",
    "- Identify strengths and weaknesses\n",
    "- Make informed decisions about deployment\n",
    "- Track improvements over time\n",
    "\n",
    "**Companies like OpenAI, Anthropic, Meta, and Google all use these benchmarks** to demonstrate their models' capabilities. When you see \"GPT-4 scores 86.4% on MMLU,\" that's a benchmark result!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßí ELI5: What Are LLM Benchmarks?\n",
    "\n",
    "> **Imagine you're in school, and there's a big test coming up.** But it's not just any test‚Äîit's a test that EVERY student in EVERY school takes, with the exact same questions.\n",
    ">\n",
    "> When you get your score, you can compare yourself to:\n",
    "> - Your classmates\n",
    "> - Students from other schools\n",
    "> - Students from other countries!\n",
    ">\n",
    "> **LLM benchmarks are like standardized tests for AI.** They ask the same questions to every AI model, so we can fairly compare them.\n",
    ">\n",
    "> Different benchmarks test different \"subjects\":\n",
    "> - **MMLU** = A test covering 57 subjects (like SATs for AI)\n",
    "> - **HellaSwag** = A test for common sense (\"What happens next?\")\n",
    "> - **HumanEval** = A coding test (\"Can you write this program?\")\n",
    "> - **MT-Bench** = A conversation test (\"Can you chat naturally?\")\n",
    ">\n",
    "> **In AI terms:** Benchmarks are standardized evaluation datasets with known correct answers, allowing reproducible comparison across models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding the Major Benchmarks\n",
    "\n",
    "Before we run any code, let's understand what each benchmark actually measures:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Benchmark Landscape\n",
    "\n",
    "| Benchmark | What It Tests | # Questions | Example Task |\n",
    "|-----------|--------------|-------------|---------------|\n",
    "| **MMLU** | World knowledge | 14,042 | Multiple choice across 57 subjects |\n",
    "| **HellaSwag** | Common sense | 10,042 | Predict sentence completion |\n",
    "| **ARC** | Science reasoning | 7,787 | Grade-school science questions |\n",
    "| **WinoGrande** | Pronoun resolution | 1,767 | \"The trophy doesn't fit in the suitcase because it's too [big/small]\" |\n",
    "| **TruthfulQA** | Factual accuracy | 817 | Avoid common misconceptions |\n",
    "| **GSM8K** | Math reasoning | 8,500 | Grade-school math word problems |\n",
    "| **HumanEval** | Code generation | 164 | Write Python functions |\n",
    "| **MT-Bench** | Multi-turn chat | 80 | Conversational quality |\n",
    "\n",
    "### üéØ The \"Open LLM Leaderboard\" Suite\n",
    "\n",
    "The Hugging Face Open LLM Leaderboard uses a standard set of benchmarks:\n",
    "1. **ARC** (AI2 Reasoning Challenge) - 25-shot\n",
    "2. **HellaSwag** - 10-shot\n",
    "3. **MMLU** - 5-shot\n",
    "4. **TruthfulQA** - 0-shot\n",
    "5. **Winogrande** - 5-shot\n",
    "6. **GSM8K** - 5-shot (chain-of-thought)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Does \"N-shot\" Mean?\n",
    "\n",
    "> **ELI5:** Imagine you're taking a test, but before each question, the teacher shows you some example questions WITH their answers.\n",
    "> \n",
    "> - **0-shot** = No examples, just the question\n",
    "> - **5-shot** = 5 example Q&A pairs before your question\n",
    "> - **25-shot** = 25 examples!\n",
    ">\n",
    "> More examples usually = better performance, but uses more memory and time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Setting Up the Environment\n",
    "\n",
    "Let's install and configure the LM Evaluation Harness, the gold-standard tool for benchmarking LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check our environment\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected! Benchmarks will be very slow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install lm-evaluation-harness\n# Note: On DGX Spark (ARM64), the NGC container should have PyTorch pre-installed\n# We install lm-eval which is mostly pure Python\n\nimport subprocess\nimport sys\n\ntry:\n    import lm_eval\n    print(f\"lm-eval already installed: {lm_eval.__version__}\")\nexcept ImportError:\n    print(\"Installing lm-eval...\")\n    # Install lm-eval - works on ARM64 as it's mostly pure Python\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lm-eval\", \"-q\"])\n    import lm_eval\n    print(f\"lm-eval installed: {lm_eval.__version__}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation and check available tasks\n",
    "!lm_eval --tasks list | head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üîç What Just Happened?\n\nWe installed `lm-eval`, which provides:\n- A standardized framework for running evaluations\n- Pre-configured benchmark tasks (MMLU, HellaSwag, etc.)\n- Support for various model backends (HuggingFace, vLLM, OpenAI API)\n- Reproducible evaluation with consistent prompting\n\n---\n\n### ‚ö†Ô∏è Docker Configuration Note (DGX Spark)\n\nIf running in a Docker container, ensure you started it with:\n\n```bash\ndocker run --gpus all -it --rm \\\n    -v $HOME/workspace:/workspace \\\n    -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\n    --ipc=host \\\n    nvcr.io/nvidia/pytorch:25.11-py3\n```\n\n| Flag | Purpose |\n|------|---------|\n| `--gpus all` | Enable GPU access |\n| `-it` | Interactive terminal |\n| `--rm` | Remove container on exit |\n| `-v workspace` | Mount workspace directory |\n| `-v hf_cache` | Mount HuggingFace cache |\n| `--ipc=host` | **Required** for PyTorch DataLoader with multiple workers |\n\nWithout `--ipc=host`, you may see errors like:\n> \"unable to open shared memory object\"\n\nThis is especially important for lm-eval which uses parallel data loading."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Running Your First Benchmark\n",
    "\n",
    "Let's start with a small model and a quick benchmark to understand the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clear GPU memory before loading models\nimport gc\nimport os\nimport subprocess\nimport torch\n\ndef clear_memory(clear_cache: bool = False):\n    \"\"\"\n    Clear GPU memory to ensure clean state.\n    \n    Args:\n        clear_cache: If True, also clear system buffer cache (requires sudo).\n                    Recommended before loading large models (>10GB).\n    \"\"\"\n    gc.collect()\n    \n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n    \n    # Clear system buffer cache for large model loading on DGX Spark\n    if clear_cache:\n        try:\n            subprocess.run(\n                ['sudo', 'sh', '-c', 'sync; echo 3 > /proc/sys/vm/drop_caches'],\n                check=True, capture_output=True, timeout=10\n            )\n            print(\"System buffer cache cleared\")\n        except (subprocess.CalledProcessError, subprocess.TimeoutExpired, FileNotFoundError):\n            print(\"Note: Could not clear buffer cache (requires sudo)\")\n    \n    if torch.cuda.is_available():\n        allocated = torch.cuda.memory_allocated() / 1e9\n        print(f\"GPU Memory: {allocated:.2f} GB allocated\")\n\nclear_memory()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nfrom pathlib import Path\n\n# Create output directory for results\n# Use absolute path for reliability\nNOTEBOOK_DIR = Path(os.getcwd())\nRESULTS_DIR = str((NOTEBOOK_DIR / \"../data/benchmark_results\").resolve())\nos.makedirs(RESULTS_DIR, exist_ok=True)\n\nprint(f\"Results will be saved to: {RESULTS_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a Quick Benchmark with lm-eval\n",
    "\n",
    "We'll start with a small, fast benchmark to understand the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run a quick benchmark on a small model\n# Using HellaSwag with a small subset for speed\n\n# For DGX Spark, we can use larger models!\n# Start with a smaller one to learn the process\n\nimport subprocess\n\n# Build command with proper path interpolation\n# Note: Using subprocess.run() instead of ! shell command for reliable path handling\ncmd = [\n    \"lm_eval\",\n    \"--model\", \"hf\",\n    \"--model_args\", \"pretrained=microsoft/phi-2,dtype=bfloat16\",\n    \"--tasks\", \"hellaswag\",\n    \"--num_fewshot\", \"0\",\n    \"--batch_size\", \"8\",\n    \"--limit\", \"100\",\n    \"--output_path\", f\"{RESULTS_DIR}/phi2_quick_test\"\n]\n\nprint(f\"Running command: {' '.join(cmd)}\")\nresult = subprocess.run(cmd, capture_output=False, text=True)\n\nif result.returncode != 0:\n    print(f\"‚ö†Ô∏è Benchmark may have encountered issues. Check output above.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Understanding the Command\n",
    "\n",
    "Let's break down what each argument does:\n",
    "\n",
    "| Argument | Purpose |\n",
    "|----------|----------|\n",
    "| `--model hf` | Use HuggingFace backend |\n",
    "| `--model_args pretrained=...` | Specify the model to evaluate |\n",
    "| `dtype=bfloat16` | Use bfloat16 for memory efficiency (native on Blackwell!) |\n",
    "| `--tasks hellaswag` | Which benchmark(s) to run |\n",
    "| `--num_fewshot 0` | How many examples to show (0-shot) |\n",
    "| `--batch_size 8` | Process 8 examples at once |\n",
    "| `--limit 100` | Only run 100 samples (for testing) |\n",
    "| `--output_path` | Where to save results |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display the results\n",
    "import glob\n",
    "\n",
    "result_files = glob.glob(f\"{RESULTS_DIR}/phi2_quick_test/*/results.json\")\n",
    "if result_files:\n",
    "    with open(result_files[0], 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìä BENCHMARK RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Display results nicely\n",
    "    for task_name, task_results in results.get('results', {}).items():\n",
    "        print(f\"\\nüìù Task: {task_name}\")\n",
    "        for metric, value in task_results.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"   {metric}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"   {metric}: {value}\")\n",
    "else:\n",
    "    print(\"No results found. Check the benchmark output above for errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Comprehensive Model Evaluation\n",
    "\n",
    "Now let's run a full benchmark suite on a model. With DGX Spark's 128GB memory, we can evaluate larger models locally!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßí ELI5: Why Multiple Benchmarks?\n",
    "\n",
    "> **Imagine testing a new car.** You wouldn't just check the speed‚Äîyou'd also check:\n",
    "> - Fuel efficiency (how far can it go?)\n",
    "> - Safety (what happens in a crash?)\n",
    "> - Comfort (is the ride smooth?)\n",
    "> - Reliability (will it break down?)\n",
    ">\n",
    "> **One benchmark = one aspect.** A model might ace MMLU (knowledge) but fail at GSM8K (math). Running multiple benchmarks gives you the full picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the benchmark suite we'll use\n",
    "# This is similar to the Open LLM Leaderboard evaluation\n",
    "\n",
    "BENCHMARK_SUITE = {\n",
    "    \"arc_easy\": {\n",
    "        \"description\": \"Easy science reasoning questions\",\n",
    "        \"num_fewshot\": 0,\n",
    "        \"metric\": \"acc_norm\"\n",
    "    },\n",
    "    \"hellaswag\": {\n",
    "        \"description\": \"Common sense sentence completion\",\n",
    "        \"num_fewshot\": 0, \n",
    "        \"metric\": \"acc_norm\"\n",
    "    },\n",
    "    \"truthfulqa_mc2\": {\n",
    "        \"description\": \"Avoiding false beliefs and misconceptions\",\n",
    "        \"num_fewshot\": 0,\n",
    "        \"metric\": \"acc\"\n",
    "    },\n",
    "    \"winogrande\": {\n",
    "        \"description\": \"Pronoun resolution/coreference\",\n",
    "        \"num_fewshot\": 0,\n",
    "        \"metric\": \"acc\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìã Benchmark Suite Overview:\")\n",
    "print(\"-\" * 60)\n",
    "for name, config in BENCHMARK_SUITE.items():\n",
    "    print(f\"  {name}: {config['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run benchmarks programmatically\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def run_benchmark(model_name: str, tasks: list, output_name: str, \n",
    "                  batch_size: int = 8, limit: int = None,\n",
    "                  dtype: str = \"bfloat16\") -> dict:\n",
    "    \"\"\"\n",
    "    Run lm-eval benchmark on a model.\n",
    "    \n",
    "    Args:\n",
    "        model_name: HuggingFace model path\n",
    "        tasks: List of benchmark tasks\n",
    "        output_name: Name for output directory\n",
    "        batch_size: Batch size for evaluation\n",
    "        limit: Optional limit on number of samples\n",
    "        dtype: Data type (bfloat16 recommended for DGX Spark)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of results\n",
    "    \"\"\"\n",
    "    clear_memory()\n",
    "    \n",
    "    output_path = f\"{RESULTS_DIR}/{output_name}\"\n",
    "    tasks_str = \",\".join(tasks)\n",
    "    \n",
    "    cmd = [\n",
    "        \"lm_eval\",\n",
    "        \"--model\", \"hf\",\n",
    "        \"--model_args\", f\"pretrained={model_name},dtype={dtype}\",\n",
    "        \"--tasks\", tasks_str,\n",
    "        \"--batch_size\", str(batch_size),\n",
    "        \"--output_path\", output_path\n",
    "    ]\n",
    "    \n",
    "    if limit:\n",
    "        cmd.extend([\"--limit\", str(limit)])\n",
    "    \n",
    "    print(f\"\\nüöÄ Starting evaluation of {model_name}\")\n",
    "    print(f\"   Tasks: {tasks_str}\")\n",
    "    print(f\"   Limit: {limit if limit else 'Full evaluation'}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è  Completed in {elapsed/60:.1f} minutes\")\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(f\"\\n‚ùå Error: {result.stderr}\")\n",
    "        return None\n",
    "    \n",
    "    # Load and return results\n",
    "    result_files = glob.glob(f\"{output_path}/*/results.json\")\n",
    "    if result_files:\n",
    "        with open(result_files[0], 'r') as f:\n",
    "            return json.load(f)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmarks on a model\n",
    "# Using phi-2 as an example (2.7B params - fast to evaluate)\n",
    "\n",
    "tasks = list(BENCHMARK_SUITE.keys())\n",
    "\n",
    "# For learning, use limit=100. Remove for full evaluation.\n",
    "phi2_results = run_benchmark(\n",
    "    model_name=\"microsoft/phi-2\",\n",
    "    tasks=tasks,\n",
    "    output_name=\"phi2_benchmark\",\n",
    "    batch_size=8,\n",
    "    limit=100  # Remove this line for full benchmark\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display results in a nice table\ndef display_results(results: dict, model_name: str) -> None:\n    \"\"\"\n    Display benchmark results in a formatted table.\n    \n    Args:\n        results: Dictionary of benchmark results from lm-eval\n        model_name: Name of the model being evaluated\n    \"\"\"\n    if not results:\n        print(\"No results to display.\")\n        return\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"üìä Results for {model_name}\")\n    print(f\"{'='*60}\")\n    \n    task_results = results.get('results', {})\n    \n    print(f\"\\n{'Task':<20} {'Metric':<15} {'Score':<10}\")\n    print(\"-\" * 45)\n    \n    total_score = 0\n    num_tasks = 0\n    \n    for task_name, metrics in task_results.items():\n        # Find the main metric\n        main_metric = BENCHMARK_SUITE.get(task_name, {}).get('metric', 'acc')\n        score = metrics.get(main_metric, metrics.get('acc', 0))\n        \n        if isinstance(score, (int, float)):\n            print(f\"{task_name:<20} {main_metric:<15} {score*100:>6.2f}%\")\n            total_score += score\n            num_tasks += 1\n    \n    if num_tasks > 0:\n        avg_score = (total_score / num_tasks) * 100\n        print(\"-\" * 45)\n        print(f\"{'AVERAGE':<20} {'':<15} {avg_score:>6.2f}%\")\n\nif phi2_results:\n    display_results(phi2_results, \"microsoft/phi-2\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Comparing Multiple Models\n",
    "\n",
    "The real power of benchmarks comes from comparison. Let's evaluate multiple models and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to compare\n",
    "# Adjust based on your memory constraints and time\n",
    "\n",
    "MODELS_TO_COMPARE = [\n",
    "    {\n",
    "        \"name\": \"microsoft/phi-2\",\n",
    "        \"size\": \"2.7B\",\n",
    "        \"description\": \"Microsoft's compact powerhouse\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "        \"size\": \"1.1B\", \n",
    "        \"description\": \"Compact chat model\"\n",
    "    },\n",
    "    # Uncomment for larger models (DGX Spark can handle these!)\n",
    "    # {\n",
    "    #     \"name\": \"Qwen/Qwen3-4B\",\n",
    "    #     \"size\": \"3B\",\n",
    "    #     \"description\": \"Meta's efficient Llama\"\n",
    "    # },\n",
    "]\n",
    "\n",
    "print(\"üìã Models to Evaluate:\")\n",
    "for m in MODELS_TO_COMPARE:\n",
    "    print(f\"  ‚Ä¢ {m['name']} ({m['size']}) - {m['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmarks on all models\n",
    "# This will take some time - perfect for a coffee break!\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for model_info in MODELS_TO_COMPARE:\n",
    "    model_name = model_info['name']\n",
    "    safe_name = model_name.replace('/', '_').replace('-', '_')\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating: {model_name} ({model_info['size']})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    results = run_benchmark(\n",
    "        model_name=model_name,\n",
    "        tasks=tasks,\n",
    "        output_name=f\"{safe_name}_benchmark\",\n",
    "        batch_size=8,\n",
    "        limit=100  # Remove for full evaluation\n",
    "    )\n",
    "    \n",
    "    if results:\n",
    "        all_results[model_name] = results\n",
    "        display_results(results, model_name)\n",
    "    \n",
    "    # Clear memory between models\n",
    "    clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison table\n",
    "import pandas as pd\n",
    "\n",
    "def create_comparison_table(results_dict: dict) -> pd.DataFrame:\n",
    "    \"\"\"Create a comparison table from multiple model results.\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for model_name, results in results_dict.items():\n",
    "        row = {'Model': model_name.split('/')[-1]}\n",
    "        \n",
    "        task_results = results.get('results', {})\n",
    "        \n",
    "        for task_name, metrics in task_results.items():\n",
    "            main_metric = BENCHMARK_SUITE.get(task_name, {}).get('metric', 'acc')\n",
    "            score = metrics.get(main_metric, metrics.get('acc', 0))\n",
    "            if isinstance(score, (int, float)):\n",
    "                row[task_name] = score * 100\n",
    "        \n",
    "        # Calculate average\n",
    "        scores = [v for k, v in row.items() if k != 'Model']\n",
    "        row['Average'] = sum(scores) / len(scores) if scores else 0\n",
    "        \n",
    "        data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.set_index('Model')\n",
    "    return df.round(2)\n",
    "\n",
    "if all_results:\n",
    "    comparison_df = create_comparison_table(all_results)\n",
    "    print(\"\\nüìä Model Comparison Table:\")\n",
    "    print(comparison_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if all_results and len(all_results) > 1:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    x = np.arange(len(comparison_df.columns) - 1)  # Exclude 'Average'\n",
    "    width = 0.35\n",
    "    \n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, len(comparison_df)))\n",
    "    \n",
    "    for i, (model, row) in enumerate(comparison_df.iterrows()):\n",
    "        values = [row[col] for col in comparison_df.columns if col != 'Average']\n",
    "        offset = width * (i - len(comparison_df)/2 + 0.5)\n",
    "        ax.bar(x + offset, values, width, label=model, color=colors[i])\n",
    "    \n",
    "    ax.set_ylabel('Score (%)')\n",
    "    ax.set_title('LLM Benchmark Comparison')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([col for col in comparison_df.columns if col != 'Average'], \n",
    "                       rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/benchmark_comparison.png\", dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìÅ Chart saved to {RESULTS_DIR}/benchmark_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Understanding MMLU In-Depth\n",
    "\n",
    "MMLU (Massive Multitask Language Understanding) is one of the most comprehensive benchmarks. Let's explore it in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßí ELI5: What is MMLU?\n",
    "\n",
    "> **Imagine a genius who claims to know EVERYTHING.** How would you test them?\n",
    ">\n",
    "> You'd ask questions from:\n",
    "> - üß¨ Biology: \"What is the powerhouse of the cell?\"\n",
    "> - ‚öñÔ∏è Law: \"What is the Fifth Amendment about?\"\n",
    "> - üî¨ Physics: \"What is E=mc¬≤?\"\n",
    "> - üìú History: \"When did WW2 end?\"\n",
    "> - üñ•Ô∏è Computer Science: \"What is a binary tree?\"\n",
    ">\n",
    "> **MMLU does exactly this!** It has 14,042 questions across 57 subjects, testing if the AI truly understands diverse topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMLU subject categories\n",
    "MMLU_SUBJECTS = {\n",
    "    \"STEM\": [\n",
    "        \"abstract_algebra\", \"anatomy\", \"astronomy\", \"college_biology\",\n",
    "        \"college_chemistry\", \"college_computer_science\", \"college_mathematics\",\n",
    "        \"college_physics\", \"computer_security\", \"conceptual_physics\",\n",
    "        \"electrical_engineering\", \"elementary_mathematics\", \"high_school_biology\",\n",
    "        \"high_school_chemistry\", \"high_school_computer_science\", \n",
    "        \"high_school_mathematics\", \"high_school_physics\", \"high_school_statistics\",\n",
    "        \"machine_learning\"\n",
    "    ],\n",
    "    \"Humanities\": [\n",
    "        \"formal_logic\", \"high_school_european_history\", \"high_school_us_history\",\n",
    "        \"high_school_world_history\", \"international_law\", \"jurisprudence\",\n",
    "        \"logical_fallacies\", \"moral_disputes\", \"moral_scenarios\", \"philosophy\",\n",
    "        \"prehistory\", \"professional_law\", \"world_religions\"\n",
    "    ],\n",
    "    \"Social_Sciences\": [\n",
    "        \"econometrics\", \"high_school_geography\", \"high_school_government_and_politics\",\n",
    "        \"high_school_macroeconomics\", \"high_school_microeconomics\", \"high_school_psychology\",\n",
    "        \"human_sexuality\", \"professional_psychology\", \"public_relations\", \"security_studies\",\n",
    "        \"sociology\", \"us_foreign_policy\"\n",
    "    ],\n",
    "    \"Other\": [\n",
    "        \"business_ethics\", \"clinical_knowledge\", \"college_medicine\", \"global_facts\",\n",
    "        \"human_aging\", \"management\", \"marketing\", \"medical_genetics\", \"miscellaneous\",\n",
    "        \"nutrition\", \"professional_accounting\", \"professional_medicine\", \"virology\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"üìö MMLU Subject Categories:\")\n",
    "for category, subjects in MMLU_SUBJECTS.items():\n",
    "    print(f\"\\n{category} ({len(subjects)} subjects):\")\n",
    "    print(f\"  {', '.join(subjects[:5])}...\" if len(subjects) > 5 else f\"  {', '.join(subjects)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MMLU on specific categories (much faster than full MMLU)\n",
    "# Let's test on a few representative subjects\n",
    "\n",
    "mmlu_sample_tasks = [\n",
    "    \"mmlu_high_school_computer_science\",\n",
    "    \"mmlu_college_physics\",\n",
    "    \"mmlu_philosophy\",\n",
    "    \"mmlu_high_school_psychology\"\n",
    "]\n",
    "\n",
    "print(\"Running MMLU sample evaluation...\")\n",
    "print(\"Tasks:\", mmlu_sample_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MMLU sample (uncomment to execute - takes ~10 minutes)\n",
    "# mmlu_results = run_benchmark(\n",
    "#     model_name=\"microsoft/phi-2\",\n",
    "#     tasks=mmlu_sample_tasks,\n",
    "#     output_name=\"phi2_mmlu_sample\",\n",
    "#     batch_size=8,\n",
    "#     limit=50  # 50 samples per subject\n",
    "# )\n",
    "# \n",
    "# if mmlu_results:\n",
    "#     display_results(mmlu_results, \"phi-2 MMLU Sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Advanced Evaluation Techniques\n",
    "\n",
    "### Using Different Backends\n",
    "\n",
    "The LM Evaluation Harness supports multiple backends for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different model backends available\n",
    "BACKENDS = {\n",
    "    \"hf\": {\n",
    "        \"name\": \"HuggingFace Transformers\",\n",
    "        \"use_case\": \"Standard evaluation, most compatible\",\n",
    "        \"example\": \"--model hf --model_args pretrained=Qwen/Qwen3-8B\"\n",
    "    },\n",
    "    \"hf-auto\": {\n",
    "        \"name\": \"HuggingFace Auto\",\n",
    "        \"use_case\": \"Automatic device mapping for large models\",\n",
    "        \"example\": \"--model hf-auto --model_args pretrained=Qwen/Qwen3-32B,parallelize=True\"\n",
    "    },\n",
    "    \"vllm\": {\n",
    "        \"name\": \"vLLM\",\n",
    "        \"use_case\": \"Fastest inference, PagedAttention\",\n",
    "        \"example\": \"--model vllm --model_args pretrained=Qwen/Qwen3-8B,tensor_parallel_size=1\"\n",
    "    },\n",
    "    \"openai-completions\": {\n",
    "        \"name\": \"OpenAI API\",\n",
    "        \"use_case\": \"Evaluate API-based models\",\n",
    "        \"example\": \"--model openai-completions --model_args model=gpt-4\"\n",
    "    },\n",
    "    \"local-completions\": {\n",
    "        \"name\": \"Local OpenAI-compatible API\",\n",
    "        \"use_case\": \"Ollama, vLLM server, etc.\",\n",
    "        \"example\": \"--model local-completions --model_args base_url=http://localhost:11434/v1\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üîß Available Model Backends:\")\n",
    "print(\"=\"*60)\n",
    "for backend, info in BACKENDS.items():\n",
    "    print(f\"\\n{backend}:\")\n",
    "    print(f\"  Name: {info['name']}\")\n",
    "    print(f\"  Use: {info['use_case']}\")\n",
    "    print(f\"  Example: {info['example']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Evaluate an Ollama model (if Ollama is running)\n",
    "# First, start Ollama: ollama serve\n",
    "# Then pull a model: ollama pull qwen3:4b\n",
    "\n",
    "ollama_cmd = \"\"\"\n",
    "# Uncomment and run if you have Ollama set up:\n",
    "# lm_eval --model local-completions \\\\\n",
    "#     --model_args model=qwen3:4b,base_url=http://localhost:11434/v1 \\\\\n",
    "#     --tasks hellaswag,arc_easy \\\\\n",
    "#     --batch_size 1 \\\\\n",
    "#     --limit 50 \\\\\n",
    "#     --output_path ./results/ollama_llama32\n",
    "\"\"\"\n",
    "print(ollama_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself: Exercise 1\n",
    "\n",
    "**Task:** Run a full benchmark comparison on two models of your choice.\n",
    "\n",
    "Requirements:\n",
    "1. Choose two models (can be different sizes)\n",
    "2. Run at least 3 benchmarks on each\n",
    "3. Create a comparison visualization\n",
    "4. Write a brief analysis of which model is \"better\" and why\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "Try comparing:\n",
    "- Different sizes of the same family (Llama 3B vs 8B)\n",
    "- Same size, different families (phi-2 vs TinyLlama)\n",
    "- Base vs instruction-tuned versions\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Step 1: Define your models\n",
    "my_models = [\n",
    "    # Add your model choices here\n",
    "]\n",
    "\n",
    "# Step 2: Define benchmarks\n",
    "my_benchmarks = [\n",
    "    # Add your benchmark choices here\n",
    "]\n",
    "\n",
    "# Step 3: Run evaluations\n",
    "# Use the run_benchmark function\n",
    "\n",
    "# Step 4: Create visualization\n",
    "# Use matplotlib to plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Comparing Models with Different Few-shot Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Different few-shot settings make comparison unfair\n",
    "# Model A: 0-shot evaluation\n",
    "# lm_eval --model hf --model_args pretrained=model_a --tasks mmlu --num_fewshot 0\n",
    "\n",
    "# Model B: 5-shot evaluation  \n",
    "# lm_eval --model hf --model_args pretrained=model_b --tasks mmlu --num_fewshot 5\n",
    "\n",
    "# ‚úÖ Right: Use the same settings for fair comparison\n",
    "# Both models: 5-shot\n",
    "# lm_eval --model hf --model_args pretrained=model_a --tasks mmlu --num_fewshot 5\n",
    "# lm_eval --model hf --model_args pretrained=model_b --tasks mmlu --num_fewshot 5\n",
    "\n",
    "print(\"‚úÖ Always use identical evaluation settings when comparing models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Running Out of Memory on Large Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Loading a 70B model without proper settings\n",
    "# lm_eval --model hf --model_args pretrained=Qwen/Qwen3-32B --tasks mmlu\n",
    "# This will crash with OOM even on DGX Spark!\n",
    "\n",
    "# ‚úÖ Right: Use model parallelization and smaller batch size\n",
    "correct_cmd = \"\"\"\n",
    "lm_eval --model hf \\\\\n",
    "    --model_args pretrained=Qwen/Qwen3-32B,dtype=bfloat16,parallelize=True \\\\\n",
    "    --tasks mmlu \\\\\n",
    "    --batch_size 1 \\\\\n",
    "    --output_path ./results/llama70b\n",
    "\"\"\"\n",
    "\n",
    "print(\"For 70B models on DGX Spark:\")\n",
    "print(\"1. Use parallelize=True to shard across memory\")\n",
    "print(\"2. Use batch_size=1 to minimize peak memory\")\n",
    "print(\"3. Use dtype=bfloat16 for memory efficiency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Misinterpreting Normalized vs Raw Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different metrics mean different things!\n",
    "\n",
    "print(\"\"\"\n",
    "üìä Understanding Metrics:\n",
    "\n",
    "acc (Accuracy)\n",
    "  - Raw correct / total\n",
    "  - Simple percentage of correct answers\n",
    "\n",
    "acc_norm (Normalized Accuracy)  \n",
    "  - Accounts for answer length bias\n",
    "  - Preferred for multiple choice tasks\n",
    "  - Used in: HellaSwag, ARC\n",
    "\n",
    "acc_stderr\n",
    "  - Standard error of the accuracy\n",
    "  - Shows confidence in the result\n",
    "  - Lower = more reliable\n",
    "\n",
    "‚ö†Ô∏è Always report which metric you're using!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 4: Testing on Training Data (Contamination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "üö® Data Contamination Warning\n",
    "\n",
    "Some models may have been trained on benchmark data!\n",
    "This leads to inflated scores that don't reflect true capability.\n",
    "\n",
    "Signs of contamination:\n",
    "- Unusually high scores on specific benchmarks\n",
    "- Model performance doesn't match real-world use\n",
    "- Perfect recall of exact benchmark questions\n",
    "\n",
    "Mitigation strategies:\n",
    "1. Use newer benchmarks not in training data\n",
    "2. Create custom evaluation sets\n",
    "3. Test on held-out data\n",
    "4. Use human evaluation for sanity checks\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ What LLM benchmarks are and why they matter\n",
    "- ‚úÖ How to use the LM Evaluation Harness\n",
    "- ‚úÖ Running benchmarks on multiple models\n",
    "- ‚úÖ Comparing and visualizing results\n",
    "- ‚úÖ Common pitfalls to avoid\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Advanced Challenge: Build a Benchmark Dashboard**\n",
    "\n",
    "Create a Streamlit or Gradio app that:\n",
    "1. Lets users select models and benchmarks\n",
    "2. Runs evaluations in the background\n",
    "3. Displays real-time progress\n",
    "4. Shows interactive comparison charts\n",
    "5. Exports results to CSV/JSON\n",
    "\n",
    "This is how companies like HuggingFace built the Open LLM Leaderboard!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [LM Evaluation Harness Documentation](https://github.com/EleutherAI/lm-evaluation-harness)\n",
    "- [MMLU Paper](https://arxiv.org/abs/2009.03300)\n",
    "- [HellaSwag Paper](https://arxiv.org/abs/1905.07830)\n",
    "- [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n",
    "- [HELM: Holistic Evaluation of Language Models](https://crfm.stanford.edu/helm/)\n",
    "- [AlpacaEval](https://tatsu-lab.github.io/alpaca_eval/)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory freed. Current allocation: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. **Explored** the landscape of LLM benchmarks (MMLU, HellaSwag, ARC, etc.)\n",
    "2. **Installed** and configured the LM Evaluation Harness\n",
    "3. **Ran** benchmarks on multiple models\n",
    "4. **Compared** results across models\n",
    "5. **Visualized** benchmark results\n",
    "6. **Learned** common mistakes to avoid\n",
    "\n",
    "**Next up:** In notebook 02, we'll learn how to create custom evaluation frameworks when standard benchmarks don't fit your use case!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}