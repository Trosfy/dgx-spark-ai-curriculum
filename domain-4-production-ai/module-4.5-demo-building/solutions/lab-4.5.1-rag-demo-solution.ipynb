{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.5.1: RAG Demo - Solutions\n",
    "\n",
    "Complete solutions for the RAG demo exercises.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1: Greeting with Time\n",
    "\n",
    "Add current time to the greeting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from datetime import datetime\n",
    "\n",
    "with gr.Blocks() as time_demo:\n",
    "    gr.Markdown(\"# Greeting with Time\")\n",
    "    \n",
    "    name_input = gr.Textbox(label=\"Your Name\")\n",
    "    greeting_output = gr.Textbox(label=\"Greeting\")\n",
    "    greet_button = gr.Button(\"Greet Me!\")\n",
    "    \n",
    "    def greet_with_time(name):\n",
    "        current_time = datetime.now().strftime(\"%H:%M\")\n",
    "        return f\"Hello, {name}! It's currently {current_time}. Welcome!\"\n",
    "    \n",
    "    greet_button.click(\n",
    "        fn=greet_with_time,\n",
    "        inputs=[name_input],\n",
    "        outputs=[greeting_output]\n",
    "    )\n",
    "\n",
    "# time_demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2: Three-Column Layout\n",
    "\n",
    "Create a 3-column layout with file upload, main content, and settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as three_column_demo:\n",
    "    gr.Markdown(\"# Three-Column Layout Demo\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        # Left column - File upload (scale=1)\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"### üìÅ Files\")\n",
    "            file_upload = gr.File(\n",
    "                label=\"Upload Documents\",\n",
    "                file_count=\"multiple\"\n",
    "            )\n",
    "            upload_btn = gr.Button(\"Process Files\", variant=\"secondary\")\n",
    "            file_status = gr.Textbox(label=\"Status\", lines=3)\n",
    "        \n",
    "        # Middle column - Main content (scale=2)\n",
    "        with gr.Column(scale=2):\n",
    "            gr.Markdown(\"### üí¨ Main Content\")\n",
    "            chat_input = gr.Textbox(\n",
    "                label=\"Your Message\",\n",
    "                placeholder=\"Type here...\",\n",
    "                lines=3\n",
    "            )\n",
    "            chat_output = gr.Textbox(\n",
    "                label=\"Response\",\n",
    "                lines=8\n",
    "            )\n",
    "            with gr.Row():\n",
    "                send_btn = gr.Button(\"Send\", variant=\"primary\")\n",
    "                clear_btn = gr.Button(\"Clear\")\n",
    "        \n",
    "        # Right column - Settings (scale=1)\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"### ‚öôÔ∏è Settings\")\n",
    "            model_select = gr.Dropdown(\n",
    "                choices=[\"llama3.2:3b\", \"llama3.1:8b\", \"mistral:7b\"],\n",
    "                value=\"llama3.2:3b\",\n",
    "                label=\"Model\"\n",
    "            )\n",
    "            temperature = gr.Slider(\n",
    "                minimum=0, maximum=1, value=0.7,\n",
    "                label=\"Temperature\"\n",
    "            )\n",
    "            max_tokens = gr.Number(\n",
    "                value=512,\n",
    "                label=\"Max Tokens\"\n",
    "            )\n",
    "            \n",
    "            with gr.Accordion(\"Advanced\", open=False):\n",
    "                top_p = gr.Slider(0, 1, 0.9, label=\"Top-P\")\n",
    "                repeat_penalty = gr.Slider(1, 2, 1.1, label=\"Repeat Penalty\")\n",
    "    \n",
    "    # Event handlers\n",
    "    def process_files(files):\n",
    "        if not files:\n",
    "            return \"No files uploaded\"\n",
    "        return f\"Processed {len(files)} files\"\n",
    "    \n",
    "    def respond(message, model, temp):\n",
    "        return f\"[{model}] Response to: {message}\\n(temp={temp})\"\n",
    "    \n",
    "    upload_btn.click(process_files, [file_upload], [file_status])\n",
    "    send_btn.click(respond, [chat_input, model_select, temperature], [chat_output])\n",
    "    clear_btn.click(lambda: (\"\", \"\"), outputs=[chat_input, chat_output])\n",
    "\n",
    "# three_column_demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 3: Complete RAG Demo with All Features\n",
    "\n",
    "Full-featured RAG demo with streaming, theme toggle, and export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import chromadb\n",
    "import ollama\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "\n",
    "class EnhancedRAGBackend:\n",
    "    \"\"\"\n",
    "    Enhanced RAG backend with additional features:\n",
    "    - Streaming responses\n",
    "    - Conversation export\n",
    "    - Analytics tracking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = chromadb.Client()\n",
    "        self.collection = self.client.get_or_create_collection(\"docs\")\n",
    "        self.llm_model = \"llama3.2:3b\"\n",
    "        self.embed_model = \"nomic-embed-text\"\n",
    "        self.n_results = 3\n",
    "        self.temperature = 0.7\n",
    "        \n",
    "        # Analytics\n",
    "        self.query_count = 0\n",
    "        self.avg_response_time = 0\n",
    "    \n",
    "    def chunk_text(self, text: str, chunk_size: int = 500) -> List[str]:\n",
    "        chunks = []\n",
    "        for i in range(0, len(text), chunk_size - 50):\n",
    "            chunk = text[i:i+chunk_size]\n",
    "            if len(chunk) > 50:\n",
    "                chunks.append(chunk.strip())\n",
    "        return chunks\n",
    "    \n",
    "    def index_document(self, text: str, filename: str) -> Tuple[int, str]:\n",
    "        try:\n",
    "            chunks = self.chunk_text(text)\n",
    "            if not chunks:\n",
    "                return 0, \"Document too short\"\n",
    "            \n",
    "            embeddings = []\n",
    "            for chunk in chunks:\n",
    "                resp = ollama.embeddings(model=self.embed_model, prompt=chunk)\n",
    "                embeddings.append(resp[\"embedding\"])\n",
    "            \n",
    "            base_id = filename.replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "            ids = [f\"{base_id}_{i}\" for i in range(len(chunks))]\n",
    "            \n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings,\n",
    "                documents=chunks,\n",
    "                metadatas=[{\"source\": filename}] * len(chunks)\n",
    "            )\n",
    "            \n",
    "            return len(chunks), f\"Indexed {len(chunks)} chunks\"\n",
    "        except Exception as e:\n",
    "            return 0, f\"Error: {str(e)}\"\n",
    "    \n",
    "    def search(self, query: str) -> List[Dict]:\n",
    "        resp = ollama.embeddings(model=self.embed_model, prompt=query)\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[resp[\"embedding\"]],\n",
    "            n_results=self.n_results\n",
    "        )\n",
    "        \n",
    "        chunks = []\n",
    "        for i in range(len(results[\"documents\"][0])):\n",
    "            chunks.append({\n",
    "                \"text\": results[\"documents\"][0][i],\n",
    "                \"source\": results[\"metadatas\"][0][i][\"source\"],\n",
    "                \"distance\": results[\"distances\"][0][i] if \"distances\" in results else 0\n",
    "            })\n",
    "        return chunks\n",
    "    \n",
    "    def chat_stream(self, query: str, history: List[Tuple[str, str]]):\n",
    "        \"\"\"Streaming chat response.\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        context_chunks = self.search(query)\n",
    "        if not context_chunks:\n",
    "            yield \"No documents indexed. Please upload some first!\"\n",
    "            return\n",
    "        \n",
    "        context = \"\\n\\n\".join([c[\"text\"] for c in context_chunks])\n",
    "        \n",
    "        system = \"\"\"Answer based on the context. Cite sources.\"\"\"\n",
    "        \n",
    "        messages = [{\"role\": \"system\", \"content\": system}]\n",
    "        for u, a in history[-5:]:\n",
    "            messages.append({\"role\": \"user\", \"content\": u})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": a})\n",
    "        \n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\"})\n",
    "        \n",
    "        response_text = \"\"\n",
    "        for chunk in ollama.chat(\n",
    "            model=self.llm_model,\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "            options={\"temperature\": self.temperature}\n",
    "        ):\n",
    "            token = chunk[\"message\"][\"content\"]\n",
    "            response_text += token\n",
    "            yield response_text\n",
    "        \n",
    "        # Update analytics\n",
    "        elapsed = time.time() - start_time\n",
    "        self.query_count += 1\n",
    "        self.avg_response_time = (\n",
    "            (self.avg_response_time * (self.query_count - 1) + elapsed) / \n",
    "            self.query_count\n",
    "        )\n",
    "    \n",
    "    def get_sources_markdown(self, query: str) -> str:\n",
    "        chunks = self.search(query)\n",
    "        sources_md = \"**Sources:**\\n\"\n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            confidence = 1 - chunk[\"distance\"]\n",
    "            sources_md += f\"\\n{i}. **{chunk['source']}** ({confidence:.0%})\\n\"\n",
    "            sources_md += f\"   > {chunk['text'][:100]}...\\n\"\n",
    "        return sources_md\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        return {\n",
    "            \"total_chunks\": self.collection.count(),\n",
    "            \"queries_processed\": self.query_count,\n",
    "            \"avg_response_time\": f\"{self.avg_response_time:.2f}s\",\n",
    "            \"model\": self.llm_model\n",
    "        }\n",
    "    \n",
    "    def export_history(self, history: List[Tuple[str, str]]) -> str:\n",
    "        \"\"\"Export chat history as JSON.\"\"\"\n",
    "        export_data = {\n",
    "            \"exported_at\": datetime.now().isoformat(),\n",
    "            \"model\": self.llm_model,\n",
    "            \"messages\": [\n",
    "                {\"user\": u, \"assistant\": a}\n",
    "                for u, a in history\n",
    "            ]\n",
    "        }\n",
    "        return json.dumps(export_data, indent=2)\n",
    "\n",
    "\n",
    "def create_enhanced_rag_demo():\n",
    "    \"\"\"Create an enhanced RAG demo with streaming and export.\"\"\"\n",
    "    \n",
    "    rag = EnhancedRAGBackend()\n",
    "    \n",
    "    # Theme options\n",
    "    light_theme = gr.themes.Soft()\n",
    "    dark_theme = gr.themes.Default()\n",
    "    \n",
    "    with gr.Blocks(theme=light_theme, title=\"Enhanced RAG Demo\") as demo:\n",
    "        gr.Markdown(\"\"\"\n",
    "        # üöÄ Enhanced RAG Demo\n",
    "        \n",
    "        Features: Streaming responses, conversation export, analytics\n",
    "        \"\"\")\n",
    "        \n",
    "        with gr.Tabs():\n",
    "            # Documents Tab\n",
    "            with gr.TabItem(\"üìÅ Documents\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=2):\n",
    "                        files = gr.File(\n",
    "                            label=\"Upload Documents\",\n",
    "                            file_count=\"multiple\",\n",
    "                            file_types=[\".txt\", \".md\", \".pdf\"]\n",
    "                        )\n",
    "                        with gr.Row():\n",
    "                            index_btn = gr.Button(\"üì• Index\", variant=\"primary\")\n",
    "                            clear_btn = gr.Button(\"üóëÔ∏è Clear\")\n",
    "                        status = gr.Textbox(label=\"Status\", lines=5)\n",
    "                    \n",
    "                    with gr.Column(scale=1):\n",
    "                        doc_count = gr.Number(label=\"Chunks Indexed\", value=0)\n",
    "                        stats_display = gr.JSON(label=\"Statistics\")\n",
    "                        refresh_stats_btn = gr.Button(\"üîÑ Refresh Stats\")\n",
    "            \n",
    "            # Chat Tab with Streaming\n",
    "            with gr.TabItem(\"üí¨ Chat\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=3):\n",
    "                        chatbot = gr.Chatbot(height=450, show_copy_button=True)\n",
    "                        \n",
    "                        with gr.Row():\n",
    "                            msg = gr.Textbox(\n",
    "                                label=\"Message\",\n",
    "                                placeholder=\"Ask about your documents...\",\n",
    "                                scale=4,\n",
    "                                show_label=False\n",
    "                            )\n",
    "                            send = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
    "                        \n",
    "                        with gr.Row():\n",
    "                            clear_chat = gr.Button(\"üóëÔ∏è Clear\")\n",
    "                            export_btn = gr.Button(\"üì§ Export\")\n",
    "                            export_file = gr.File(label=\"Download\", visible=False)\n",
    "                    \n",
    "                    with gr.Column(scale=1):\n",
    "                        gr.Markdown(\"### üìö Sources\")\n",
    "                        sources = gr.Markdown(\"*Ask a question to see sources*\")\n",
    "            \n",
    "            # Settings Tab\n",
    "            with gr.TabItem(\"‚öôÔ∏è Settings\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        model_select = gr.Dropdown(\n",
    "                            choices=[\"llama3.2:3b\", \"llama3.1:8b\"],\n",
    "                            value=\"llama3.2:3b\",\n",
    "                            label=\"LLM Model\"\n",
    "                        )\n",
    "                        chunks_slider = gr.Slider(1, 10, 3, step=1, label=\"Chunks\")\n",
    "                        temp_slider = gr.Slider(0, 1, 0.7, step=0.1, label=\"Temperature\")\n",
    "                        save_btn = gr.Button(\"üíæ Save Settings\", variant=\"primary\")\n",
    "                        settings_status = gr.Textbox(label=\"Status\")\n",
    "        \n",
    "        # Event Handlers\n",
    "        def process_files(files):\n",
    "            if not files:\n",
    "                return \"No files\", 0\n",
    "            results = []\n",
    "            for f in files:\n",
    "                with open(f.name, 'r', errors='ignore') as file:\n",
    "                    text = file.read()\n",
    "                n, msg = rag.index_document(text, Path(f.name).name)\n",
    "                results.append(f\"‚úÖ {Path(f.name).name}: {msg}\")\n",
    "            return \"\\n\".join(results), rag.collection.count()\n",
    "        \n",
    "        def clear_docs():\n",
    "            rag.client.delete_collection(\"docs\")\n",
    "            rag.collection = rag.client.create_collection(\"docs\")\n",
    "            return \"Cleared!\", 0\n",
    "        \n",
    "        def chat_stream(message, history):\n",
    "            if not message:\n",
    "                return history, \"\"\n",
    "            \n",
    "            history = history + [[message, \"\"]]\n",
    "            \n",
    "            for response in rag.chat_stream(message, history[:-1]):\n",
    "                history[-1][1] = response\n",
    "                yield history, rag.get_sources_markdown(message)\n",
    "        \n",
    "        def export_chat(history):\n",
    "            json_content = rag.export_history(history)\n",
    "            filename = f\"chat_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "            with open(filename, 'w') as f:\n",
    "                f.write(json_content)\n",
    "            return gr.update(visible=True, value=filename)\n",
    "        \n",
    "        def save_settings(model, chunks, temp):\n",
    "            rag.llm_model = model\n",
    "            rag.n_results = int(chunks)\n",
    "            rag.temperature = temp\n",
    "            return f\"Saved: model={model}, chunks={chunks}, temp={temp}\"\n",
    "        \n",
    "        # Wire events\n",
    "        index_btn.click(process_files, [files], [status, doc_count])\n",
    "        clear_btn.click(clear_docs, outputs=[status, doc_count])\n",
    "        refresh_stats_btn.click(lambda: rag.get_stats(), outputs=[stats_display])\n",
    "        \n",
    "        send.click(chat_stream, [msg, chatbot], [chatbot, sources])\n",
    "        msg.submit(chat_stream, [msg, chatbot], [chatbot, sources])\n",
    "        clear_chat.click(lambda: ([], \"*Ask a question*\"), outputs=[chatbot, sources])\n",
    "        export_btn.click(export_chat, [chatbot], [export_file])\n",
    "        \n",
    "        save_btn.click(save_settings, [model_select, chunks_slider, temp_slider], [settings_status])\n",
    "    \n",
    "    return demo\n",
    "\n",
    "\n",
    "# Create and test\n",
    "print(\"Enhanced RAG Demo created!\")\n",
    "# enhanced_demo = create_enhanced_rag_demo()\n",
    "# enhanced_demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 4: Challenge - Multi-language Support\n",
    "\n",
    "Add language selector and translate UI elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Translation dictionary\n",
    "TRANSLATIONS = {\n",
    "    \"en\": {\n",
    "        \"title\": \"RAG Chat Demo\",\n",
    "        \"upload\": \"Upload Documents\",\n",
    "        \"index\": \"Index Documents\",\n",
    "        \"clear\": \"Clear\",\n",
    "        \"message\": \"Type your message...\",\n",
    "        \"send\": \"Send\",\n",
    "        \"settings\": \"Settings\",\n",
    "        \"model\": \"Model\",\n",
    "        \"temperature\": \"Temperature\"\n",
    "    },\n",
    "    \"es\": {\n",
    "        \"title\": \"Demo de Chat RAG\",\n",
    "        \"upload\": \"Subir Documentos\",\n",
    "        \"index\": \"Indexar Documentos\",\n",
    "        \"clear\": \"Limpiar\",\n",
    "        \"message\": \"Escribe tu mensaje...\",\n",
    "        \"send\": \"Enviar\",\n",
    "        \"settings\": \"Configuraci√≥n\",\n",
    "        \"model\": \"Modelo\",\n",
    "        \"temperature\": \"Temperatura\"\n",
    "    },\n",
    "    \"fr\": {\n",
    "        \"title\": \"D√©mo Chat RAG\",\n",
    "        \"upload\": \"T√©l√©charger des Documents\",\n",
    "        \"index\": \"Indexer les Documents\",\n",
    "        \"clear\": \"Effacer\",\n",
    "        \"message\": \"Tapez votre message...\",\n",
    "        \"send\": \"Envoyer\",\n",
    "        \"settings\": \"Param√®tres\",\n",
    "        \"model\": \"Mod√®le\",\n",
    "        \"temperature\": \"Temp√©rature\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def create_multilang_demo():\n",
    "    \"\"\"Create a multi-language demo.\"\"\"\n",
    "    \n",
    "    with gr.Blocks() as demo:\n",
    "        # Language selector at top\n",
    "        with gr.Row():\n",
    "            lang = gr.Dropdown(\n",
    "                choices=[\"en\", \"es\", \"fr\"],\n",
    "                value=\"en\",\n",
    "                label=\"üåê Language\",\n",
    "                scale=1\n",
    "            )\n",
    "            title = gr.Markdown(\"# RAG Chat Demo\")\n",
    "        \n",
    "        # Dynamic components\n",
    "        upload = gr.File(label=\"Upload Documents\")\n",
    "        index_btn = gr.Button(\"Index Documents\", variant=\"primary\")\n",
    "        msg = gr.Textbox(label=\"Message\", placeholder=\"Type your message...\")\n",
    "        send_btn = gr.Button(\"Send\", variant=\"primary\")\n",
    "        \n",
    "        def update_language(language):\n",
    "            t = TRANSLATIONS.get(language, TRANSLATIONS[\"en\"])\n",
    "            return (\n",
    "                f\"# {t['title']}\",\n",
    "                gr.update(label=t[\"upload\"]),\n",
    "                gr.update(value=t[\"index\"]),\n",
    "                gr.update(placeholder=t[\"message\"]),\n",
    "                gr.update(value=t[\"send\"])\n",
    "            )\n",
    "        \n",
    "        lang.change(\n",
    "            update_language,\n",
    "            [lang],\n",
    "            [title, upload, index_btn, msg, send_btn]\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "print(\"Multi-language demo created!\")\n",
    "# multilang_demo = create_multilang_demo()\n",
    "# multilang_demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Gradio Blocks** provides full control over layout and styling\n",
    "2. **Streaming** makes chat interfaces feel much more responsive\n",
    "3. **Analytics** help you understand how users interact with your demo\n",
    "4. **Export** functionality adds professional polish\n",
    "5. **Internationalization** expands your demo's reach\n",
    "\n",
    "These solutions demonstrate production-quality patterns you can adapt for your own projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
