{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.5.3: Portfolio Demo - Solutions\n",
    "\n",
    "Complete example portfolio demos for reference.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Image Classification Demo\n",
    "\n",
    "A polished image classification demo with examples and explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classifier_demo = '''\n",
    "\"\"\"\n",
    "Image Classification Demo\n",
    "\n",
    "A polished portfolio demo for an image classifier.\n",
    "\"\"\"\n",
    "\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Mock classifier (replace with your model)\n",
    "CLASSES = [\"cat\", \"dog\", \"bird\", \"fish\", \"rabbit\"]\n",
    "\n",
    "def classify_image(image):\n",
    "    \"\"\"Classify an image.\"\"\"\n",
    "    if image is None:\n",
    "        return {c: 0 for c in CLASSES}\n",
    "    \n",
    "    # Mock prediction (replace with your model)\n",
    "    np.random.seed(hash(str(image.shape)) % 2**32)\n",
    "    probs = np.random.dirichlet(np.ones(len(CLASSES)))\n",
    "    \n",
    "    return {CLASSES[i]: float(probs[i]) for i in range(len(CLASSES))}\n",
    "\n",
    "# Build the demo\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    # Header\n",
    "    gr.Markdown(\"\"\"\n",
    "    # üêæ Pet Classifier\n",
    "    \n",
    "    **Upload an image of a pet and I\\'ll tell you what it is!**\n",
    "    \n",
    "    This demo uses a CNN trained on 10,000 pet images with 95% accuracy.\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            image_input = gr.Image(\n",
    "                label=\"Upload Pet Image\",\n",
    "                type=\"numpy\"\n",
    "            )\n",
    "            classify_btn = gr.Button(\"üîç Classify\", variant=\"primary\")\n",
    "        \n",
    "        with gr.Column():\n",
    "            label_output = gr.Label(\n",
    "                label=\"Predictions\",\n",
    "                num_top_classes=5\n",
    "            )\n",
    "    \n",
    "    # Examples\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            [\"examples/cat.jpg\"],\n",
    "            [\"examples/dog.jpg\"],\n",
    "            [\"examples/bird.jpg\"],\n",
    "        ],\n",
    "        inputs=[image_input],\n",
    "        outputs=[label_output],\n",
    "        fn=classify_image,\n",
    "        cache_examples=True,\n",
    "        label=\"Try these examples\"\n",
    "    )\n",
    "    \n",
    "    # Technical details\n",
    "    with gr.Accordion(\"üìä Technical Details\", open=False):\n",
    "        gr.Markdown(\"\"\"\n",
    "        ### Model Architecture\n",
    "        - Base: ResNet-50 pretrained on ImageNet\n",
    "        - Fine-tuned on custom pet dataset (10,000 images)\n",
    "        - Training: 50 epochs, AdamW optimizer, cosine LR schedule\n",
    "        \n",
    "        ### Performance\n",
    "        - Accuracy: 95.2%\n",
    "        - Inference time: ~50ms on GPU\n",
    "        \n",
    "        ### Limitations\n",
    "        - Works best with clear, centered images\n",
    "        - May struggle with unusual breeds\n",
    "        \"\"\")\n",
    "    \n",
    "    # Footer\n",
    "    gr.Markdown(\"\"\"\n",
    "    ---\n",
    "    **Built by** [Your Name](https://github.com/yourname) | \n",
    "    [GitHub](https://github.com/yourname/pet-classifier) | \n",
    "    [Paper](https://arxiv.org/abs/...)\n",
    "    \"\"\")\n",
    "    \n",
    "    # Events\n",
    "    classify_btn.click(classify_image, [image_input], [label_output])\n",
    "    image_input.change(classify_image, [image_input], [label_output])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n",
    "'''\n",
    "\n",
    "print(\"Image classifier demo:\")\n",
    "print(image_classifier_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Text Analysis Dashboard\n",
    "\n",
    "A Streamlit dashboard for text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_analysis_demo = '''\n",
    "\"\"\"\n",
    "Text Analysis Dashboard\n",
    "\n",
    "A multi-feature text analysis tool.\n",
    "\"\"\"\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"Text Analyzer\",\n",
    "    page_icon=\"üìù\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "st.title(\"üìù Text Analyzer\")\n",
    "st.markdown(\"Analyze your text with multiple AI-powered tools.\")\n",
    "\n",
    "# Sidebar configuration\n",
    "with st.sidebar:\n",
    "    st.header(\"‚öôÔ∏è Settings\")\n",
    "    analysis_type = st.multiselect(\n",
    "        \"Select analyses\",\n",
    "        [\"Statistics\", \"Sentiment\", \"Keywords\", \"Summary\"],\n",
    "        default=[\"Statistics\", \"Keywords\"]\n",
    "    )\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"### About\")\n",
    "    st.markdown(\"\"\"\n",
    "    This tool analyzes text using:\n",
    "    - NLP for sentiment\n",
    "    - TF-IDF for keywords\n",
    "    - LLM for summarization\n",
    "    \"\"\")\n",
    "\n",
    "# Main input\n",
    "text = st.text_area(\n",
    "    \"Enter your text\",\n",
    "    height=200,\n",
    "    placeholder=\"Paste or type your text here...\"\n",
    ")\n",
    "\n",
    "if st.button(\"üîç Analyze\", type=\"primary\") or text:\n",
    "    if not text:\n",
    "        st.warning(\"Please enter some text to analyze.\")\n",
    "    else:\n",
    "        # Create columns for results\n",
    "        cols = st.columns(len(analysis_type) if analysis_type else 1)\n",
    "        \n",
    "        # Statistics\n",
    "        if \"Statistics\" in analysis_type:\n",
    "            with cols[analysis_type.index(\"Statistics\")]:\n",
    "                st.subheader(\"üìä Statistics\")\n",
    "                \n",
    "                words = text.split()\n",
    "                sentences = re.split(r\"[.!?]+\", text)\n",
    "                \n",
    "                stats_df = pd.DataFrame({\n",
    "                    \"Metric\": [\"Characters\", \"Words\", \"Sentences\", \"Avg Word Length\"],\n",
    "                    \"Value\": [\n",
    "                        len(text),\n",
    "                        len(words),\n",
    "                        len([s for s in sentences if s.strip()]),\n",
    "                        f\"{sum(len(w) for w in words) / len(words):.1f}\" if words else \"0\"\n",
    "                    ]\n",
    "                })\n",
    "                \n",
    "                st.dataframe(stats_df, hide_index=True)\n",
    "        \n",
    "        # Sentiment\n",
    "        if \"Sentiment\" in analysis_type:\n",
    "            with cols[analysis_type.index(\"Sentiment\")]:\n",
    "                st.subheader(\"üòä Sentiment\")\n",
    "                \n",
    "                # Mock sentiment (replace with actual model)\n",
    "                positive_words = {\"good\", \"great\", \"excellent\", \"happy\", \"love\"}\n",
    "                negative_words = {\"bad\", \"terrible\", \"hate\", \"sad\", \"awful\"}\n",
    "                \n",
    "                words_lower = set(text.lower().split())\n",
    "                pos_count = len(words_lower & positive_words)\n",
    "                neg_count = len(words_lower & negative_words)\n",
    "                \n",
    "                if pos_count > neg_count:\n",
    "                    st.success(\"Positive üòä\")\n",
    "                elif neg_count > pos_count:\n",
    "                    st.error(\"Negative üòû\")\n",
    "                else:\n",
    "                    st.info(\"Neutral üòê\")\n",
    "                \n",
    "                st.metric(\"Positive signals\", pos_count)\n",
    "                st.metric(\"Negative signals\", neg_count)\n",
    "        \n",
    "        # Keywords\n",
    "        if \"Keywords\" in analysis_type:\n",
    "            with cols[analysis_type.index(\"Keywords\")]:\n",
    "                st.subheader(\"üîë Keywords\")\n",
    "                \n",
    "                # Simple word frequency (replace with TF-IDF)\n",
    "                words = re.findall(r\"\\\\b\\\\w{4,}\\\\b\", text.lower())\n",
    "                common = Counter(words).most_common(5)\n",
    "                \n",
    "                for word, count in common:\n",
    "                    st.markdown(f\"- **{word}** ({count}x)\")\n",
    "        \n",
    "        # Summary\n",
    "        if \"Summary\" in analysis_type:\n",
    "            with cols[analysis_type.index(\"Summary\")]:\n",
    "                st.subheader(\"üìã Summary\")\n",
    "                \n",
    "                # Mock summary (replace with LLM)\n",
    "                sentences = [s.strip() for s in re.split(r\"[.!?]+\", text) if s.strip()]\n",
    "                summary = \" \".join(sentences[:2]) + \"...\" if len(sentences) > 2 else text\n",
    "                \n",
    "                st.write(summary)\n",
    "\n",
    "# Footer\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\n",
    "    \"Built with ‚ù§Ô∏è using Streamlit | \"\n",
    "    \"[GitHub](https://github.com/yourname/text-analyzer)\"\n",
    ")\n",
    "'''\n",
    "\n",
    "print(\"Text analysis dashboard:\")\n",
    "print(text_analysis_demo[:2000] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Complete RAG Portfolio Demo\n",
    "\n",
    "A production-ready RAG demo with all best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_portfolio_demo = '''\n",
    "\"\"\"\n",
    "DocChat - RAG Portfolio Demo\n",
    "\n",
    "A complete, production-ready RAG demo showcasing:\n",
    "- Document upload and indexing\n",
    "- Conversational Q&A with citations\n",
    "- Settings and analytics\n",
    "\"\"\"\n",
    "\n",
    "import gradio as gr\n",
    "import chromadb\n",
    "import ollama\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "APP_TITLE = \"DocChat\"\n",
    "APP_DESCRIPTION = \"Chat with your documents using AI\"\n",
    "AUTHOR = \"Your Name\"\n",
    "GITHUB_URL = \"https://github.com/yourname/docchat\"\n",
    "\n",
    "# Theme\n",
    "THEME = gr.themes.Soft(\n",
    "    primary_hue=\"blue\",\n",
    "    secondary_hue=\"slate\",\n",
    "    font=gr.themes.GoogleFont(\"Inter\"),\n",
    ")\n",
    "\n",
    "CSS = \"\"\"\n",
    ".gradio-container { max-width: 1200px !important; margin: auto; }\n",
    ".source-box { background: #f0f7ff; padding: 12px; border-radius: 8px; margin: 8px 0; }\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ===== RAG BACKEND =====\n",
    "class DocChatRAG:\n",
    "    \"\"\"RAG backend with best practices.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = chromadb.Client()\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            \"documents\",\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        \n",
    "        # Settings\n",
    "        self.llm = \"llama3.2:3b\"\n",
    "        self.embed = \"nomic-embed-text\"\n",
    "        self.n_results = 3\n",
    "        self.temp = 0.7\n",
    "        \n",
    "        # Analytics\n",
    "        self.queries = 0\n",
    "        self.docs_indexed = 0\n",
    "    \n",
    "    def index(self, text: str, filename: str) -> str:\n",
    "        \"\"\"Index a document.\"\"\"\n",
    "        try:\n",
    "            # Chunk\n",
    "            chunks = [text[i:i+500] for i in range(0, len(text), 450)]\n",
    "            chunks = [c.strip() for c in chunks if len(c.strip()) > 50]\n",
    "            \n",
    "            if not chunks:\n",
    "                return \"Document too short\"\n",
    "            \n",
    "            # Embed\n",
    "            embeddings = [\n",
    "                ollama.embeddings(model=self.embed, prompt=c)[\"embedding\"]\n",
    "                for c in chunks\n",
    "            ]\n",
    "            \n",
    "            # Store\n",
    "            ids = [f\"{filename}_{i}\" for i in range(len(chunks))]\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings,\n",
    "                documents=chunks,\n",
    "                metadatas=[{\"source\": filename}] * len(chunks)\n",
    "            )\n",
    "            \n",
    "            self.docs_indexed += 1\n",
    "            return f\"Indexed {len(chunks)} chunks\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "    \n",
    "    def query(self, question: str, history: list) -> Tuple[str, str]:\n",
    "        \"\"\"Answer a question.\"\"\"\n",
    "        self.queries += 1\n",
    "        \n",
    "        # Search\n",
    "        emb = ollama.embeddings(model=self.embed, prompt=question)[\"embedding\"]\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[emb],\n",
    "            n_results=self.n_results\n",
    "        )\n",
    "        \n",
    "        if not results[\"documents\"][0]:\n",
    "            return \"No documents found. Please upload some first!\", \"\"\n",
    "        \n",
    "        # Build context\n",
    "        context = \"\\n\\n\".join(results[\"documents\"][0])\n",
    "        \n",
    "        # Generate\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Answer based on context. Cite sources.\"},\n",
    "            *[{\"role\": r, \"content\": c} for h in history for r, c in [(\"user\", h[0]), (\"assistant\", h[1])]],\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"}\n",
    "        ]\n",
    "        \n",
    "        response = ollama.chat(model=self.llm, messages=messages)\n",
    "        answer = response[\"message\"][\"content\"]\n",
    "        \n",
    "        # Format sources\n",
    "        sources = \"**Sources:**\\n\"\n",
    "        for i, (doc, meta) in enumerate(zip(results[\"documents\"][0], results[\"metadatas\"][0])):\n",
    "            sources += f\"\\n{i+1}. **{meta[\\'source\\']}**\\n> {doc[:100]}...\\n\"\n",
    "        \n",
    "        return answer, sources\n",
    "\n",
    "\n",
    "# ===== BUILD DEMO =====\n",
    "def create_demo():\n",
    "    rag = DocChatRAG()\n",
    "    \n",
    "    with gr.Blocks(theme=THEME, css=CSS, title=APP_TITLE) as demo:\n",
    "        # Header\n",
    "        gr.Markdown(f\"\"\"\n",
    "        # üìö {APP_TITLE}\n",
    "        \n",
    "        {APP_DESCRIPTION}. Upload PDFs or text files, then ask questions!\n",
    "        \"\"\")\n",
    "        \n",
    "        with gr.Tabs():\n",
    "            # Documents tab\n",
    "            with gr.TabItem(\"üìÅ Documents\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        files = gr.File(\n",
    "                            label=\"Upload (PDF, TXT, MD)\",\n",
    "                            file_count=\"multiple\",\n",
    "                            file_types=[\".pdf\", \".txt\", \".md\"]\n",
    "                        )\n",
    "                        with gr.Row():\n",
    "                            index_btn = gr.Button(\"üì• Index\", variant=\"primary\")\n",
    "                            clear_btn = gr.Button(\"üóëÔ∏è Clear\")\n",
    "                        status = gr.Textbox(label=\"Status\", lines=5)\n",
    "                    \n",
    "                    with gr.Column():\n",
    "                        count = gr.Number(label=\"Chunks\", value=0)\n",
    "            \n",
    "            # Chat tab\n",
    "            with gr.TabItem(\"üí¨ Chat\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=3):\n",
    "                        chatbot = gr.Chatbot(height=400)\n",
    "                        with gr.Row():\n",
    "                            msg = gr.Textbox(placeholder=\"Ask...\", show_label=False, scale=4)\n",
    "                            send = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
    "                    with gr.Column(scale=1):\n",
    "                        sources = gr.Markdown(\"*Sources appear here*\")\n",
    "            \n",
    "            # About tab\n",
    "            with gr.TabItem(\"‚ÑπÔ∏è About\"):\n",
    "                gr.Markdown(f\"\"\"\n",
    "                ### About {APP_TITLE}\n",
    "                \n",
    "                This demo showcases a RAG (Retrieval-Augmented Generation) system.\n",
    "                \n",
    "                **Features:**\n",
    "                - Multi-document support\n",
    "                - Source citations\n",
    "                - Conversation memory\n",
    "                \n",
    "                **Tech Stack:**\n",
    "                - ChromaDB for vector storage\n",
    "                - Ollama for LLM inference\n",
    "                - Gradio for UI\n",
    "                \n",
    "                **Author:** {AUTHOR}  \n",
    "                **GitHub:** [{GITHUB_URL}]({GITHUB_URL})\n",
    "                \"\"\")\n",
    "        \n",
    "        # Events\n",
    "        def process(files):\n",
    "            if not files: return \"No files\", 0\n",
    "            results = []\n",
    "            for f in files:\n",
    "                with open(f.name, \"r\", errors=\"ignore\") as file:\n",
    "                    msg = rag.index(file.read(), Path(f.name).name)\n",
    "                results.append(f\"‚úÖ {Path(f.name).name}: {msg}\")\n",
    "            return \"\\n\".join(results), rag.collection.count()\n",
    "        \n",
    "        def chat(message, history):\n",
    "            answer, srcs = rag.query(message, history)\n",
    "            return history + [[message, answer]], srcs, \"\"\n",
    "        \n",
    "        index_btn.click(process, [files], [status, count])\n",
    "        send.click(chat, [msg, chatbot], [chatbot, sources, msg])\n",
    "        msg.submit(chat, [msg, chatbot], [chatbot, sources, msg])\n",
    "    \n",
    "    return demo\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo = create_demo()\n",
    "    demo.queue()\n",
    "    demo.launch()\n",
    "'''\n",
    "\n",
    "print(\"RAG portfolio demo:\")\n",
    "print(rag_portfolio_demo[:3000] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Script Template (Filled)\n",
    "\n",
    "Example script for a 2-minute demo video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_script_example = '''\n",
    "===========================================\n",
    "DocChat - Demo Video Script\n",
    "===========================================\n",
    "\n",
    "[HOOK - 10 seconds]\n",
    "\"Ever wished you could just *ask* your documents questions \n",
    "instead of searching through them? That\\'s exactly what \n",
    "DocChat does.\"\n",
    "\n",
    "[DEMO - 70 seconds]\n",
    "\"Let me show you how it works.\n",
    "\n",
    "First, I\\'ll upload a few documents - here\\'s a product spec,\n",
    "a meeting transcript, and a research paper.\n",
    "[Drag files, click Index, show progress]\n",
    "\n",
    "DocChat chunks these into pieces and creates semantic embeddings.\n",
    "You can see it indexed 47 chunks from our 3 documents.\n",
    "\n",
    "Now, let\\'s ask a question. \\'What were the key decisions \n",
    "from the last meeting?\\'.\n",
    "[Type, send, wait for response]\n",
    "\n",
    "Look at that - it found the relevant sections and gave me\n",
    "a clear summary. And see these citations? I can verify\n",
    "exactly where this info came from.\n",
    "\n",
    "Let me try something harder: \\'How does the product spec\n",
    "relate to the research findings?\\'.\n",
    "[Show cross-document reasoning]\n",
    "\n",
    "It\\'s connecting information across multiple documents -\n",
    "something that would take me 20 minutes of searching.\"\n",
    "\n",
    "[BEHIND THE SCENES - 30 seconds]\n",
    "\"Under the hood, DocChat uses ChromaDB for vector storage\n",
    "and Llama 3.2 running locally via Ollama - no cloud APIs,\n",
    "your data stays on your machine.\n",
    "\n",
    "The interesting part is the chunking strategy - I use\n",
    "overlapping 500-character chunks to preserve context\n",
    "across chunk boundaries.\n",
    "\n",
    "On my DGX Spark, it processes queries in under 2 seconds.\"\n",
    "\n",
    "[CALL TO ACTION - 10 seconds]\n",
    "\"Try it yourself at huggingface.co/spaces/yourname/docchat.\n",
    "Check out the code on GitHub - link in the description.\n",
    "If you found this useful, give it a star!\n",
    "\n",
    "Thanks for watching!\"\n",
    "\n",
    "===========================================\n",
    "Total: ~2 minutes\n",
    "===========================================\n",
    "'''\n",
    "\n",
    "print(video_script_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Checklist\n",
    "\n",
    "Final checklist before deploying your portfolio demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_checklist = '''\n",
    "## Portfolio Demo Deployment Checklist\n",
    "\n",
    "### Before Deployment\n",
    "- [ ] All example inputs work correctly\n",
    "- [ ] Error handling shows friendly messages\n",
    "- [ ] Loading states present for slow operations\n",
    "- [ ] Mobile layout tested (use share=True)\n",
    "- [ ] No hardcoded file paths or secrets\n",
    "- [ ] requirements.txt is complete and pinned\n",
    "- [ ] README.md has proper metadata for Spaces\n",
    "\n",
    "### Content Quality\n",
    "- [ ] Title is clear and memorable\n",
    "- [ ] One-line description explains value\n",
    "- [ ] 3+ working examples provided\n",
    "- [ ] Technical details in collapsible section\n",
    "- [ ] Author/contact info visible\n",
    "- [ ] GitHub link included\n",
    "\n",
    "### Technical Quality\n",
    "- [ ] No debug print statements\n",
    "- [ ] No commented-out code\n",
    "- [ ] Type hints on public functions\n",
    "- [ ] Docstrings on complex functions\n",
    "- [ ] Memory usage is reasonable\n",
    "\n",
    "### After Deployment\n",
    "- [ ] Demo loads in under 30 seconds\n",
    "- [ ] All features work in deployed version\n",
    "- [ ] Share link works for others\n",
    "- [ ] Added demo link to GitHub README\n",
    "- [ ] Created video walkthrough\n",
    "- [ ] Posted to LinkedIn/Twitter\n",
    "'''\n",
    "\n",
    "print(deployment_checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Polish matters** - A well-designed demo creates a strong first impression\n",
    "2. **Examples are essential** - Pre-populated examples reduce friction\n",
    "3. **Tell the story** - Help users understand not just what, but why\n",
    "4. **Technical depth** - Show your expertise in expandable sections\n",
    "5. **Call to action** - Make it easy to find your GitHub/contact\n",
    "\n",
    "Your portfolio demo is often the first thing people see. Make it count!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
