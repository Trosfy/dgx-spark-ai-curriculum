{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.5.1: Complete RAG Demo with Gradio\n",
    "\n",
    "**Module:** 4.5 - Demo Building & Prototyping  \n",
    "**Time:** 3 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Build complex multi-tab interfaces with Gradio Blocks API\n",
    "- [ ] Implement document upload with indexing progress indicators\n",
    "- [ ] Create a polished chat interface with source citations\n",
    "- [ ] Add settings management with persistent state\n",
    "- [ ] Deploy your demo to Hugging Face Spaces\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Module 3.5 (RAG Systems)\n",
    "- Knowledge of: Python, basic Gradio, vector databases\n",
    "- Installed: `gradio`, `chromadb`, `ollama`\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "You've built an amazing RAG system that can answer questions from documents. But how do you:\n",
    "- Show it to your boss who doesn't know Python?\n",
    "- Demo it to potential investors?\n",
    "- Let beta testers try it without installing anything?\n",
    "\n",
    "**The answer: A polished web demo!**\n",
    "\n",
    "Companies like [ChatPDF](https://www.chatpdf.com/), [Consensus](https://consensus.app/), and [Humata](https://www.humata.ai/) built billion-dollar businesses on essentially what we're building today - a nice interface around RAG.\n",
    "\n",
    "---\n",
    "\n",
    "## üßí ELI5: Why Do We Need Demos?\n",
    "\n",
    "> **Imagine you baked the most delicious cake in the world** üéÇ\n",
    ">\n",
    "> But you only show people the recipe and a photo. They can't taste it, smell it, or see how moist it is when you cut it. Would they believe it's amazing?\n",
    ">\n",
    "> A demo is like inviting people to actually taste your cake. They click a button, ask a question, and *experience* your AI. That's 100x more convincing than any slide deck!\n",
    ">\n",
    "> **In AI terms:** Your Jupyter notebooks are the recipe. The trained model is the cake. Gradio/Streamlit is serving it on a beautiful plate so everyone can taste it.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the Gradio Blocks API\n",
    "\n",
    "### The Evolution of Gradio Interfaces\n",
    "\n",
    "Gradio has two main APIs:\n",
    "\n",
    "1. **`gr.Interface`** - Quick and simple, one function, one input, one output\n",
    "2. **`gr.Blocks`** - Full control, multiple components, custom layouts\n",
    "\n",
    "Think of `Interface` as a microwave meal and `Blocks` as a full kitchen. Today we're cooking from scratch!\n",
    "\n",
    "### üßí ELI5: Interface vs Blocks\n",
    "\n",
    "> **Interface is like a vending machine** - Put money in, get snack out. Simple!\n",
    ">\n",
    "> **Blocks is like a restaurant kitchen** - You decide where the stove goes, which pans to use, and exactly how to plate the food."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's install and import what we need\n",
    "# Run this once to install dependencies\n",
    "# !pip install gradio>=4.0.0 chromadb>=0.4.0 ollama>=0.1.0 pypdf>=4.0.0\n",
    "\n",
    "import gradio as gr\n",
    "import chromadb\n",
    "import ollama\n",
    "import os\n",
    "import time\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Gradio version: {gr.__version__}\")\n",
    "print(f\"ChromaDB version: {chromadb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your First Blocks Interface\n",
    "\n",
    "Let's start with the simplest Blocks example to understand the pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simplest Blocks example\n",
    "with gr.Blocks() as simple_demo:\n",
    "    # Everything inside the 'with' block becomes part of the UI\n",
    "    gr.Markdown(\"# Hello Gradio Blocks!\")\n",
    "    \n",
    "    # Create components\n",
    "    name_input = gr.Textbox(label=\"Your Name\")\n",
    "    greeting_output = gr.Textbox(label=\"Greeting\")\n",
    "    greet_button = gr.Button(\"Greet Me!\")\n",
    "    \n",
    "    # Define the function\n",
    "    def greet(name):\n",
    "        return f\"Hello, {name}! Welcome to Gradio Blocks!\"\n",
    "    \n",
    "    # Connect button click to function\n",
    "    greet_button.click(\n",
    "        fn=greet,            # Function to run\n",
    "        inputs=[name_input],  # Input components\n",
    "        outputs=[greeting_output]  # Output components\n",
    "    )\n",
    "\n",
    "# Launch (in Jupyter, this creates an embedded interface)\n",
    "simple_demo.launch(share=False, inline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "1. `with gr.Blocks() as demo:` - Creates a blank canvas\n",
    "2. Components are added in order (top to bottom)\n",
    "3. `.click()` connects a button to a function\n",
    "4. Inputs/outputs are lists of components\n",
    "\n",
    "### ‚úã Try It Yourself #1\n",
    "\n",
    "Modify the greeting to include the current time.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Import `datetime` and add `datetime.now().strftime(\"%H:%M\")` to the greeting string.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - add time to the greeting\n",
    "from datetime import datetime\n",
    "\n",
    "# TODO: Create a Blocks interface that shows \"Hello, {name}! It's {time}.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Layout with Rows, Columns, and Tabs\n",
    "\n",
    "Real apps need structured layouts. Gradio provides:\n",
    "- `gr.Row()` - Components side by side\n",
    "- `gr.Column()` - Components stacked vertically (with width control)\n",
    "- `gr.Tabs()` / `gr.TabItem()` - Tabbed interfaces\n",
    "\n",
    "### üßí ELI5: Layout Components\n",
    "\n",
    "> Think of building with LEGO:\n",
    "> - **Row** = A flat LEGO baseplate where you line things up horizontally\n",
    "> - **Column** = A tower of bricks going up\n",
    "> - **Tabs** = A LEGO house with different rooms you can visit one at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layout demonstration\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as layout_demo:\n",
    "    gr.Markdown(\"# Layout Examples\")\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        # Tab 1: Rows and Columns\n",
    "        with gr.TabItem(\"üìä Row & Column Demo\"):\n",
    "            with gr.Row():\n",
    "                # Column with scale=2 is twice as wide as scale=1\n",
    "                with gr.Column(scale=2):\n",
    "                    gr.Markdown(\"### Large Column (scale=2)\")\n",
    "                    large_text = gr.Textbox(label=\"Wide input\", lines=3)\n",
    "                \n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(\"### Small Column (scale=1)\")\n",
    "                    small_text = gr.Textbox(label=\"Narrow input\")\n",
    "                    go_button = gr.Button(\"Go!\", variant=\"primary\")\n",
    "        \n",
    "        # Tab 2: Nested layouts\n",
    "        with gr.TabItem(\"üî≤ Nested Layout\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    gr.Markdown(\"**Left Panel**\")\n",
    "                    with gr.Row():  # Nested row!\n",
    "                        btn1 = gr.Button(\"A\")\n",
    "                        btn2 = gr.Button(\"B\")\n",
    "                    with gr.Row():\n",
    "                        btn3 = gr.Button(\"C\")\n",
    "                        btn4 = gr.Button(\"D\")\n",
    "                \n",
    "                with gr.Column():\n",
    "                    gr.Markdown(\"**Right Panel**\")\n",
    "                    output = gr.Textbox(label=\"Output\", lines=4)\n",
    "        \n",
    "        # Tab 3: Accordion (collapsible)\n",
    "        with gr.TabItem(\"üìÅ Accordion Demo\"):\n",
    "            gr.Markdown(\"Accordions hide complexity until needed.\")\n",
    "            \n",
    "            with gr.Accordion(\"‚öôÔ∏è Advanced Settings\", open=False):\n",
    "                temperature = gr.Slider(0, 1, 0.7, label=\"Temperature\")\n",
    "                top_p = gr.Slider(0, 1, 0.9, label=\"Top-P\")\n",
    "                max_tokens = gr.Number(value=512, label=\"Max Tokens\")\n",
    "            \n",
    "            with gr.Accordion(\"üìñ Help & Documentation\", open=False):\n",
    "                gr.Markdown(\"\"\"\n",
    "                **Temperature**: Controls randomness. Lower = more deterministic.\n",
    "                \n",
    "                **Top-P**: Nucleus sampling threshold.\n",
    "                \n",
    "                **Max Tokens**: Maximum response length.\n",
    "                \"\"\")\n",
    "\n",
    "layout_demo.launch(inline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself #2\n",
    "\n",
    "Create a 3-column layout where:\n",
    "- Left column (scale=1): File upload\n",
    "- Middle column (scale=2): Main content area\n",
    "- Right column (scale=1): Settings\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Use `gr.Row()` with three `gr.Column(scale=...)` blocks inside.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - create a 3-column layout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Building the RAG Backend\n",
    "\n",
    "Before we build the UI, we need the RAG logic. Let's create a simple but functional backend.\n",
    "\n",
    "### üßí ELI5: RAG Backend\n",
    "\n",
    "> Imagine a librarian who:\n",
    "> 1. **Indexes books** - Reads every book and remembers what topics are where\n",
    "> 2. **Finds relevant pages** - When you ask a question, finds the right pages\n",
    "> 3. **Answers questions** - Reads those pages and gives you an answer\n",
    ">\n",
    "> That's exactly what our RAG system does with your documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGBackend:\n",
    "    \"\"\"\n",
    "    A simple RAG backend for the demo.\n",
    "    \n",
    "    This is intentionally simple - in production, you'd use\n",
    "    more sophisticated chunking, embedding models, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"demo_docs\"):\n",
    "        \"\"\"Initialize the RAG backend with ChromaDB.\"\"\"\n",
    "        # Use persistent storage so documents survive restarts\n",
    "        self.client = chromadb.Client()  # In-memory for demo, use PersistentClient for real apps\n",
    "        \n",
    "        # Get or create collection\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        \n",
    "        # Model settings (can be changed in UI)\n",
    "        self.llm_model = \"llama3.2:3b\"  # Default to smaller model\n",
    "        self.embed_model = \"qwen3-embedding:8b\"\n",
    "        self.n_results = 3\n",
    "        self.temperature = 0.7\n",
    "        \n",
    "        print(f\"RAG Backend initialized with collection: {collection_name}\")\n",
    "    \n",
    "    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into overlapping chunks.\n",
    "        \n",
    "        Args:\n",
    "            text: The full text to chunk\n",
    "            chunk_size: Target size of each chunk in characters\n",
    "            overlap: Overlap between consecutive chunks\n",
    "            \n",
    "        Returns:\n",
    "            List of text chunks\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = start + chunk_size\n",
    "            chunk = text[start:end]\n",
    "            \n",
    "            # Try to break at sentence boundary\n",
    "            if end < len(text):\n",
    "                last_period = chunk.rfind('.')\n",
    "                if last_period > chunk_size // 2:\n",
    "                    chunk = chunk[:last_period + 1]\n",
    "                    end = start + last_period + 1\n",
    "            \n",
    "            chunks.append(chunk.strip())\n",
    "            start = end - overlap\n",
    "        \n",
    "        return [c for c in chunks if len(c) > 50]  # Filter tiny chunks\n",
    "    \n",
    "    def index_document(self, text: str, filename: str) -> Tuple[int, str]:\n",
    "        \"\"\"\n",
    "        Index a document into the vector store.\n",
    "        \n",
    "        Args:\n",
    "            text: Document text content\n",
    "            filename: Name of the source file\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (number of chunks indexed, status message)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Chunk the document\n",
    "            chunks = self.chunk_text(text)\n",
    "            \n",
    "            if not chunks:\n",
    "                return 0, \"Document too short to index\"\n",
    "            \n",
    "            # Generate embeddings using Ollama\n",
    "            embeddings = []\n",
    "            for chunk in chunks:\n",
    "                response = ollama.embeddings(\n",
    "                    model=self.embed_model,\n",
    "                    prompt=chunk\n",
    "                )\n",
    "                embeddings.append(response[\"embedding\"])\n",
    "            \n",
    "            # Create unique IDs\n",
    "            base_id = filename.replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "            ids = [f\"{base_id}_chunk_{i}\" for i in range(len(chunks))]\n",
    "            \n",
    "            # Store in ChromaDB\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings,\n",
    "                documents=chunks,\n",
    "                metadatas=[{\"source\": filename, \"chunk_id\": i} for i in range(len(chunks))]\n",
    "            )\n",
    "            \n",
    "            return len(chunks), f\"Successfully indexed {len(chunks)} chunks from {filename}\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return 0, f\"Error indexing {filename}: {str(e)}\"\n",
    "    \n",
    "    def search(self, query: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search for relevant chunks.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            \n",
    "        Returns:\n",
    "            List of relevant chunks with metadata\n",
    "        \"\"\"\n",
    "        # Get query embedding\n",
    "        response = ollama.embeddings(\n",
    "            model=self.embed_model,\n",
    "            prompt=query\n",
    "        )\n",
    "        query_embedding = response[\"embedding\"]\n",
    "        \n",
    "        # Search ChromaDB\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=self.n_results\n",
    "        )\n",
    "        \n",
    "        # Format results\n",
    "        chunks = []\n",
    "        for i in range(len(results[\"documents\"][0])):\n",
    "            chunks.append({\n",
    "                \"text\": results[\"documents\"][0][i],\n",
    "                \"source\": results[\"metadatas\"][0][i][\"source\"],\n",
    "                \"distance\": results[\"distances\"][0][i] if \"distances\" in results else None\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def chat(self, query: str, history: List[Tuple[str, str]]) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Generate a response using RAG.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            history: Conversation history as list of (user, assistant) tuples\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (response text, sources markdown)\n",
    "        \"\"\"\n",
    "        # Search for relevant context\n",
    "        context_chunks = self.search(query)\n",
    "        \n",
    "        if not context_chunks:\n",
    "            return \"I don't have any documents to reference. Please upload some documents first!\", \"\"\n",
    "        \n",
    "        # Build context string\n",
    "        context = \"\\n\\n---\\n\\n\".join([c[\"text\"] for c in context_chunks])\n",
    "        \n",
    "        # Build the prompt\n",
    "        system_prompt = \"\"\"You are a helpful assistant that answers questions based on the provided context.\n",
    "Always cite which document your information comes from.\n",
    "If the context doesn't contain relevant information, say so honestly.\n",
    "Keep responses concise but complete.\"\"\"\n",
    "        \n",
    "        # Build messages with history\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        \n",
    "        # Add history (last 5 turns)\n",
    "        for user_msg, assistant_msg in history[-5:]:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        \n",
    "        # Add current query with context\n",
    "        user_message = f\"\"\"Context from documents:\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Please answer based on the context above.\"\"\"\n",
    "        \n",
    "        messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        # Generate response\n",
    "        response = ollama.chat(\n",
    "            model=self.llm_model,\n",
    "            messages=messages,\n",
    "            options={\"temperature\": self.temperature}\n",
    "        )\n",
    "        \n",
    "        answer = response[\"message\"][\"content\"]\n",
    "        \n",
    "        # Format sources\n",
    "        sources_md = \"**Sources:**\\n\"\n",
    "        for i, chunk in enumerate(context_chunks, 1):\n",
    "            confidence = 1 - (chunk[\"distance\"] or 0)  # Convert distance to similarity\n",
    "            sources_md += f\"\\n{i}. **{chunk['source']}** (relevance: {confidence:.0%})\\n\"\n",
    "            sources_md += f\"   > {chunk['text'][:150]}...\\n\"\n",
    "        \n",
    "        return answer, sources_md\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get collection statistics.\"\"\"\n",
    "        count = self.collection.count()\n",
    "        return {\n",
    "            \"total_chunks\": count,\n",
    "            \"llm_model\": self.llm_model,\n",
    "            \"embed_model\": self.embed_model\n",
    "        }\n",
    "    \n",
    "    def clear_collection(self):\n",
    "        \"\"\"Clear all documents from the collection.\"\"\"\n",
    "        # Delete and recreate\n",
    "        self.client.delete_collection(self.collection.name)\n",
    "        self.collection = self.client.create_collection(\n",
    "            name=self.collection.name,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        return \"Collection cleared!\"\n",
    "\n",
    "\n",
    "# Test the backend\n",
    "print(\"Testing RAG Backend...\")\n",
    "rag = RAGBackend()\n",
    "\n",
    "# Index a test document\n",
    "test_doc = \"\"\"\n",
    "The DGX Spark is NVIDIA's first desktop AI supercomputer designed for individual developers.\n",
    "It features the Blackwell GB10 Superchip with 128GB of unified memory shared between CPU and GPU.\n",
    "This unified memory architecture eliminates the need for data transfers between CPU and GPU,\n",
    "enabling efficient processing of large AI models.\n",
    "\n",
    "The system includes 192 fifth-generation Tensor Cores and 6,144 CUDA cores,\n",
    "delivering up to 1 petaflop of FP4 AI performance. This makes it capable of running\n",
    "models with up to 200 billion parameters locally using NVFP4 quantization.\n",
    "\"\"\"\n",
    "\n",
    "chunks, msg = rag.index_document(test_doc, \"dgx_spark_overview.txt\")\n",
    "print(f\"Indexed: {msg}\")\n",
    "print(f\"Stats: {rag.get_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "We created a `RAGBackend` class that:\n",
    "\n",
    "1. **Chunks documents** - Splits text into overlapping pieces\n",
    "2. **Creates embeddings** - Uses Ollama to vectorize each chunk\n",
    "3. **Stores in ChromaDB** - Vector database for fast similarity search\n",
    "4. **Searches and answers** - Finds relevant chunks and generates responses\n",
    "\n",
    "This is the \"kitchen\" - now let's build the \"restaurant front\"!\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4: Building the Complete RAG Demo\n",
    "\n",
    "Now let's build a polished three-tab interface:\n",
    "\n",
    "1. **üìÅ Documents** - Upload and manage files\n",
    "2. **üí¨ Chat** - Interact with the RAG system\n",
    "3. **‚öôÔ∏è Settings** - Configure models and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CSS for a polished look\n",
    "custom_css = \"\"\"\n",
    ".gradio-container {\n",
    "    max-width: 1200px !important;\n",
    "    margin: auto !important;\n",
    "}\n",
    "\n",
    ".source-box {\n",
    "    background-color: #f8f9fa;\n",
    "    border-left: 4px solid #007bff;\n",
    "    padding: 10px;\n",
    "    margin: 10px 0;\n",
    "    border-radius: 4px;\n",
    "}\n",
    "\n",
    ".stats-card {\n",
    "    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "    color: white;\n",
    "    padding: 20px;\n",
    "    border-radius: 10px;\n",
    "    text-align: center;\n",
    "}\n",
    "\n",
    ".warning-box {\n",
    "    background-color: #fff3cd;\n",
    "    border: 1px solid #ffc107;\n",
    "    padding: 10px;\n",
    "    border-radius: 4px;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Custom theme\n",
    "theme = gr.themes.Soft(\n",
    "    primary_hue=\"blue\",\n",
    "    secondary_hue=\"slate\",\n",
    "    font=gr.themes.GoogleFont(\"Inter\"),\n",
    ").set(\n",
    "    button_primary_background_fill=\"*primary_500\",\n",
    "    button_primary_text_color=\"white\",\n",
    "    block_title_text_weight=\"600\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_demo():\n",
    "    \"\"\"\n",
    "    Create the complete RAG demo interface.\n",
    "    \n",
    "    This function creates a polished Gradio Blocks interface with\n",
    "    document management, chat, and settings tabs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the RAG backend\n",
    "    rag = RAGBackend()\n",
    "    \n",
    "    # =========================================\n",
    "    # Helper Functions for UI\n",
    "    # =========================================\n",
    "    \n",
    "    def process_uploaded_files(files, progress=gr.Progress()):\n",
    "        \"\"\"\n",
    "        Process uploaded files and add to the index.\n",
    "        \n",
    "        Supports: .txt, .md, .pdf files\n",
    "        \"\"\"\n",
    "        if not files:\n",
    "            return \"No files uploaded\", 0, []\n",
    "        \n",
    "        results = []\n",
    "        total_chunks = 0\n",
    "        \n",
    "        for i, file in enumerate(progress.tqdm(files, desc=\"Indexing documents\")):\n",
    "            filename = Path(file.name).name\n",
    "            \n",
    "            try:\n",
    "                # Read file content\n",
    "                if filename.endswith('.pdf'):\n",
    "                    # Handle PDF\n",
    "                    try:\n",
    "                        from pypdf import PdfReader\n",
    "                        reader = PdfReader(file.name)\n",
    "                        text = \"\\n\".join([page.extract_text() for page in reader.pages])\n",
    "                    except ImportError:\n",
    "                        results.append(f\"‚ùå {filename}: pypdf not installed\")\n",
    "                        continue\n",
    "                else:\n",
    "                    # Handle text files\n",
    "                    with open(file.name, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        text = f.read()\n",
    "                \n",
    "                # Index the document\n",
    "                chunks, msg = rag.index_document(text, filename)\n",
    "                total_chunks += chunks\n",
    "                \n",
    "                if chunks > 0:\n",
    "                    results.append(f\"‚úÖ {filename}: {chunks} chunks indexed\")\n",
    "                else:\n",
    "                    results.append(f\"‚ö†Ô∏è {filename}: {msg}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                results.append(f\"‚ùå {filename}: {str(e)}\")\n",
    "        \n",
    "        status = \"\\n\".join(results)\n",
    "        doc_count = rag.get_stats()[\"total_chunks\"]\n",
    "        \n",
    "        return status, doc_count, files\n",
    "    \n",
    "    def chat_respond(message, history):\n",
    "        \"\"\"\n",
    "        Handle chat messages.\n",
    "        \"\"\"\n",
    "        if not message:\n",
    "            return history, \"\", \"\"\n",
    "        \n",
    "        if rag.get_stats()[\"total_chunks\"] == 0:\n",
    "            return history + [[message, \"üìö No documents indexed yet! Please upload some documents in the Documents tab first.\"]], \"\", \"\"\n",
    "        \n",
    "        # Get response from RAG\n",
    "        response, sources = rag.chat(message, history)\n",
    "        \n",
    "        # Update history\n",
    "        history = history + [[message, response]]\n",
    "        \n",
    "        return history, sources, \"\"\n",
    "    \n",
    "    def clear_history():\n",
    "        \"\"\"Clear chat history.\"\"\"\n",
    "        return [], \"\"\n",
    "    \n",
    "    def update_settings(model, chunks, temp):\n",
    "        \"\"\"Update RAG settings.\"\"\"\n",
    "        rag.llm_model = model\n",
    "        rag.n_results = int(chunks)\n",
    "        rag.temperature = temp\n",
    "        return f\"Settings updated: Model={model}, Chunks={chunks}, Temp={temp}\"\n",
    "    \n",
    "    def clear_documents():\n",
    "        \"\"\"Clear all indexed documents.\"\"\"\n",
    "        rag.clear_collection()\n",
    "        return \"All documents cleared!\", 0\n",
    "    \n",
    "    def get_doc_count():\n",
    "        \"\"\"Get current document count.\"\"\"\n",
    "        return rag.get_stats()[\"total_chunks\"]\n",
    "    \n",
    "    # =========================================\n",
    "    # Build the Interface\n",
    "    # =========================================\n",
    "    \n",
    "    with gr.Blocks(theme=theme, css=custom_css, title=\"RAG Chat Demo\") as demo:\n",
    "        # Header\n",
    "        gr.Markdown(\"\"\"\n",
    "        # ü§ñ RAG Chat Demo\n",
    "        \n",
    "        Upload your documents, then chat with them! Powered by local LLMs via Ollama.\n",
    "        \"\"\")\n",
    "        \n",
    "        with gr.Tabs() as tabs:\n",
    "            # ===== TAB 1: DOCUMENTS =====\n",
    "            with gr.TabItem(\"üìÅ Documents\", id=1):\n",
    "                gr.Markdown(\"### Upload & Manage Documents\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=2):\n",
    "                        # File upload\n",
    "                        file_upload = gr.File(\n",
    "                            label=\"Upload Documents\",\n",
    "                            file_count=\"multiple\",\n",
    "                            file_types=[\".txt\", \".md\", \".pdf\"],\n",
    "                            type=\"filepath\"\n",
    "                        )\n",
    "                        \n",
    "                        with gr.Row():\n",
    "                            index_btn = gr.Button(\"üì• Index Documents\", variant=\"primary\")\n",
    "                            clear_docs_btn = gr.Button(\"üóëÔ∏è Clear All\", variant=\"secondary\")\n",
    "                        \n",
    "                        # Status display\n",
    "                        status_box = gr.Textbox(\n",
    "                            label=\"Indexing Status\",\n",
    "                            lines=8,\n",
    "                            interactive=False\n",
    "                        )\n",
    "                    \n",
    "                    with gr.Column(scale=1):\n",
    "                        # Stats\n",
    "                        gr.Markdown(\"### üìä Statistics\")\n",
    "                        doc_count = gr.Number(\n",
    "                            label=\"Total Chunks Indexed\",\n",
    "                            value=0,\n",
    "                            interactive=False\n",
    "                        )\n",
    "                        \n",
    "                        gr.Markdown(\"\"\"\n",
    "                        ---\n",
    "                        **Supported formats:**\n",
    "                        - üìÑ Plain text (.txt)\n",
    "                        - üìù Markdown (.md)\n",
    "                        - üìï PDF (.pdf)\n",
    "                        \n",
    "                        **Tips:**\n",
    "                        - Smaller documents index faster\n",
    "                        - PDFs may take longer to process\n",
    "                        - Clear and re-index if results seem off\n",
    "                        \"\"\")\n",
    "                \n",
    "                # Wire up events\n",
    "                index_btn.click(\n",
    "                    fn=process_uploaded_files,\n",
    "                    inputs=[file_upload],\n",
    "                    outputs=[status_box, doc_count, file_upload]\n",
    "                )\n",
    "                \n",
    "                clear_docs_btn.click(\n",
    "                    fn=clear_documents,\n",
    "                    outputs=[status_box, doc_count]\n",
    "                )\n",
    "            \n",
    "            # ===== TAB 2: CHAT =====\n",
    "            with gr.TabItem(\"üí¨ Chat\", id=2):\n",
    "                gr.Markdown(\"### Chat with Your Documents\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=3):\n",
    "                        # Chat interface\n",
    "                        chatbot = gr.Chatbot(\n",
    "                            height=450,\n",
    "                            show_copy_button=True,\n",
    "                            bubble_full_width=False,\n",
    "                            avatar_images=(None, \"https://em-content.zobj.net/source/twitter/376/robot_1f916.png\")\n",
    "                        )\n",
    "                        \n",
    "                        with gr.Row():\n",
    "                            msg_input = gr.Textbox(\n",
    "                                label=\"Message\",\n",
    "                                placeholder=\"Ask about your documents...\",\n",
    "                                scale=4,\n",
    "                                show_label=False\n",
    "                            )\n",
    "                            send_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
    "                        \n",
    "                        clear_chat_btn = gr.Button(\"üóëÔ∏è Clear Chat\", variant=\"secondary\")\n",
    "                    \n",
    "                    with gr.Column(scale=1):\n",
    "                        gr.Markdown(\"### üìö Sources\")\n",
    "                        sources_display = gr.Markdown(\n",
    "                            value=\"*Sources will appear here after you ask a question*\"\n",
    "                        )\n",
    "                \n",
    "                # Example questions\n",
    "                with gr.Accordion(\"üí° Example Questions\", open=False):\n",
    "                    gr.Markdown(\"\"\"\n",
    "                    Try these example questions:\n",
    "                    - \"What is the main topic of the documents?\"\n",
    "                    - \"Summarize the key points\"\n",
    "                    - \"What are the most important facts?\"\n",
    "                    \"\"\")\n",
    "                \n",
    "                # Wire up events\n",
    "                send_btn.click(\n",
    "                    fn=chat_respond,\n",
    "                    inputs=[msg_input, chatbot],\n",
    "                    outputs=[chatbot, sources_display, msg_input]\n",
    "                )\n",
    "                \n",
    "                msg_input.submit(\n",
    "                    fn=chat_respond,\n",
    "                    inputs=[msg_input, chatbot],\n",
    "                    outputs=[chatbot, sources_display, msg_input]\n",
    "                )\n",
    "                \n",
    "                clear_chat_btn.click(\n",
    "                    fn=clear_history,\n",
    "                    outputs=[chatbot, sources_display]\n",
    "                )\n",
    "            \n",
    "            # ===== TAB 3: SETTINGS =====\n",
    "            with gr.TabItem(\"‚öôÔ∏è Settings\", id=3):\n",
    "                gr.Markdown(\"### Configure RAG Settings\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        gr.Markdown(\"#### Model Settings\")\n",
    "                        \n",
    "                        model_select = gr.Dropdown(\n",
    "                            choices=[\n",
    "                                \"llama3.2:3b\",\n",
    "                                \"llama3.2:1b\",\n",
    "                                \"qwen3:8b\",\n",
    "                                \"qwen3:32b\",\n",
    "                                \"mistral:7b\",\n",
    "                                \"qwen2:7b\"\n",
    "                            ],\n",
    "                            value=\"llama3.2:3b\",\n",
    "                            label=\"LLM Model\",\n",
    "                            info=\"Larger models are more capable but slower\"\n",
    "                        )\n",
    "                        \n",
    "                        chunks_slider = gr.Slider(\n",
    "                            minimum=1,\n",
    "                            maximum=10,\n",
    "                            value=3,\n",
    "                            step=1,\n",
    "                            label=\"Retrieved Chunks\",\n",
    "                            info=\"More chunks = more context, but slower\"\n",
    "                        )\n",
    "                        \n",
    "                        temp_slider = gr.Slider(\n",
    "                            minimum=0,\n",
    "                            maximum=1,\n",
    "                            value=0.7,\n",
    "                            step=0.1,\n",
    "                            label=\"Temperature\",\n",
    "                            info=\"Higher = more creative, Lower = more focused\"\n",
    "                        )\n",
    "                        \n",
    "                        save_settings_btn = gr.Button(\"üíæ Save Settings\", variant=\"primary\")\n",
    "                        settings_status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "                    \n",
    "                    with gr.Column():\n",
    "                        gr.Markdown(\"#### Setting Explanations\")\n",
    "                        gr.Markdown(\"\"\"\n",
    "                        **LLM Model:**\n",
    "                        - `llama3.2:3b` - Fast, good for quick tests\n",
    "                        - `qwen3:8b` - Balanced performance\n",
    "                        - `qwen3:32b` - Best quality (needs more RAM)\n",
    "                        \n",
    "                        **Retrieved Chunks:**\n",
    "                        - Lower (1-2): Faster, focused answers\n",
    "                        - Higher (5-10): More context, comprehensive answers\n",
    "                        \n",
    "                        **Temperature:**\n",
    "                        - 0.0: Deterministic, same answer every time\n",
    "                        - 0.7: Balanced creativity\n",
    "                        - 1.0: Maximum creativity/randomness\n",
    "                        \"\"\")\n",
    "                \n",
    "                # Wire up events\n",
    "                save_settings_btn.click(\n",
    "                    fn=update_settings,\n",
    "                    inputs=[model_select, chunks_slider, temp_slider],\n",
    "                    outputs=[settings_status]\n",
    "                )\n",
    "        \n",
    "        # Footer\n",
    "        gr.Markdown(\"\"\"\n",
    "        ---\n",
    "        *Built with üíô using Gradio, ChromaDB, and Ollama | Module 4.5 Demo*\n",
    "        \"\"\")\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# Create and launch\n",
    "print(\"Creating RAG Demo...\")\n",
    "rag_demo = create_rag_demo()\n",
    "rag_demo.launch(inline=True, share=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "We built a complete, production-ready RAG demo with:\n",
    "\n",
    "1. **Document Management Tab**\n",
    "   - Multi-file upload with progress indicator\n",
    "   - Real-time indexing status\n",
    "   - Document statistics\n",
    "\n",
    "2. **Chat Tab**\n",
    "   - Clean chat interface with avatars\n",
    "   - Source citations in a sidebar\n",
    "   - Example questions for new users\n",
    "\n",
    "3. **Settings Tab**\n",
    "   - Model selection dropdown\n",
    "   - Adjustable retrieval parameters\n",
    "   - Temperature control\n",
    "\n",
    "4. **Professional Polish**\n",
    "   - Custom theme and CSS\n",
    "   - Responsive layout\n",
    "   - Clear instructions and feedback\n",
    "\n",
    "---\n",
    "\n",
    "## Part 5: Adding Advanced Features\n",
    "\n",
    "Let's enhance our demo with some advanced Gradio features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced feature: Streaming responses\n",
    "def create_streaming_demo():\n",
    "    \"\"\"\n",
    "    Demo showing streaming responses - much better UX!\n",
    "    \n",
    "    Instead of waiting for the full response, users see\n",
    "    text appear word by word, like ChatGPT.\n",
    "    \"\"\"\n",
    "    \n",
    "    def stream_response(message, history):\n",
    "        \"\"\"\n",
    "        Stream the response token by token.\n",
    "        \"\"\"\n",
    "        # Build messages\n",
    "        messages = []\n",
    "        for user_msg, assistant_msg in history:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        messages.append({\"role\": \"user\", \"content\": message})\n",
    "        \n",
    "        # Stream response\n",
    "        response_text = \"\"\n",
    "        for chunk in ollama.chat(\n",
    "            model=\"llama3.2:3b\",\n",
    "            messages=messages,\n",
    "            stream=True  # Enable streaming!\n",
    "        ):\n",
    "            token = chunk[\"message\"][\"content\"]\n",
    "            response_text += token\n",
    "            yield response_text\n",
    "    \n",
    "    with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "        gr.Markdown(\"# ‚ö° Streaming Chat Demo\")\n",
    "        gr.Markdown(\"Watch the response appear word by word!\")\n",
    "        \n",
    "        chatbot = gr.Chatbot(height=400)\n",
    "        msg = gr.Textbox(label=\"Message\", placeholder=\"Say something...\")\n",
    "        \n",
    "        def user_message(message, history):\n",
    "            return \"\", history + [[message, None]]\n",
    "        \n",
    "        def bot_response(history):\n",
    "            message = history[-1][0]\n",
    "            history_without_last = history[:-1]\n",
    "            \n",
    "            for response in stream_response(message, history_without_last):\n",
    "                history[-1][1] = response\n",
    "                yield history\n",
    "        \n",
    "        msg.submit(user_message, [msg, chatbot], [msg, chatbot]).then(\n",
    "            bot_response, [chatbot], [chatbot]\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# Uncomment to run:\n",
    "# streaming_demo = create_streaming_demo()\n",
    "# streaming_demo.launch(inline=True)\n",
    "print(\"Streaming demo defined. Uncomment the last lines to run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced feature: Authentication\n",
    "def create_auth_demo():\n",
    "    \"\"\"\n",
    "    Demo showing basic authentication.\n",
    "    \n",
    "    Useful when you want to restrict access to your demo.\n",
    "    \"\"\"\n",
    "    \n",
    "    def greet(name):\n",
    "        return f\"Hello, {name}! You're authenticated.\"\n",
    "    \n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# üîê Authenticated Demo\")\n",
    "        name = gr.Textbox(label=\"Your Name\")\n",
    "        output = gr.Textbox(label=\"Greeting\")\n",
    "        btn = gr.Button(\"Greet\")\n",
    "        btn.click(greet, name, output)\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# To launch with auth:\n",
    "# auth_demo = create_auth_demo()\n",
    "# auth_demo.launch(\n",
    "#     auth=[(\"admin\", \"password123\"), (\"user\", \"demo\")],\n",
    "#     auth_message=\"Please login to access the demo\"\n",
    "# )\n",
    "print(\"Auth demo defined. See comments for how to launch with authentication.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Deploying to Hugging Face Spaces\n",
    "\n",
    "Now let's deploy our demo to the world! Hugging Face Spaces provides free hosting for Gradio apps.\n",
    "\n",
    "### Step 1: Prepare Your Files\n",
    "\n",
    "Create a folder with these files:\n",
    "\n",
    "```\n",
    "my-rag-demo/\n",
    "‚îú‚îÄ‚îÄ app.py           # Main application\n",
    "‚îú‚îÄ‚îÄ requirements.txt  # Dependencies\n",
    "‚îî‚îÄ‚îÄ README.md         # Space configuration\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the deployment files\n",
    "import os\n",
    "\n",
    "# Create deployment directory\n",
    "deploy_dir = \"rag_demo_deploy\"\n",
    "os.makedirs(deploy_dir, exist_ok=True)\n",
    "\n",
    "# 1. app.py - The main application\n",
    "app_py_content = '''\n",
    "\"\"\"RAG Chat Demo - Hugging Face Spaces Version\"\"\"\n",
    "\n",
    "import gradio as gr\n",
    "import chromadb\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "\n",
    "# Note: For HF Spaces, you might use a different LLM API\n",
    "# such as Hugging Face Inference API or OpenAI\n",
    "# This example uses a mock for demonstration\n",
    "\n",
    "class RAGBackend:\n",
    "    \"\"\"Simplified RAG backend for Spaces.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = chromadb.Client()\n",
    "        self.collection = self.client.get_or_create_collection(\"docs\")\n",
    "    \n",
    "    def index_document(self, text: str, filename: str):\n",
    "        chunks = [text[i:i+500] for i in range(0, len(text), 450)]\n",
    "        chunks = [c for c in chunks if len(c) > 50]\n",
    "        \n",
    "        if not chunks:\n",
    "            return 0, \"Document too short\"\n",
    "        \n",
    "        ids = [f\"{filename}_{i}\" for i in range(len(chunks))]\n",
    "        self.collection.add(\n",
    "            ids=ids,\n",
    "            documents=chunks,\n",
    "            metadatas=[{\"source\": filename}] * len(chunks)\n",
    "        )\n",
    "        return len(chunks), f\"Indexed {len(chunks)} chunks\"\n",
    "    \n",
    "    def search(self, query: str, n=3):\n",
    "        results = self.collection.query(query_texts=[query], n_results=n)\n",
    "        return results[\"documents\"][0] if results[\"documents\"] else []\n",
    "    \n",
    "    def chat(self, query: str, history: list):\n",
    "        context = self.search(query)\n",
    "        if not context:\n",
    "            return \"No documents indexed. Please upload some first!\", \"\"\n",
    "        \n",
    "        # Mock response - replace with actual LLM API\n",
    "        context_text = \"\\n\".join(context[:2])\n",
    "        response = f\"Based on the documents: {context_text[:200]}...\"\n",
    "        sources = f\"Found {len(context)} relevant chunks.\"\n",
    "        \n",
    "        return response, sources\n",
    "    \n",
    "    def count(self):\n",
    "        return self.collection.count()\n",
    "    \n",
    "    def clear(self):\n",
    "        self.client.delete_collection(\"docs\")\n",
    "        self.collection = self.client.create_collection(\"docs\")\n",
    "\n",
    "\n",
    "# Initialize backend\n",
    "rag = RAGBackend()\n",
    "\n",
    "# UI Functions\n",
    "def process_files(files):\n",
    "    if not files:\n",
    "        return \"No files\", 0\n",
    "    \n",
    "    results = []\n",
    "    for file in files:\n",
    "        try:\n",
    "            with open(file.name, \"r\") as f:\n",
    "                text = f.read()\n",
    "            chunks, msg = rag.index_document(text, Path(file.name).name)\n",
    "            results.append(f\"‚úÖ {Path(file.name).name}: {msg}\")\n",
    "        except Exception as e:\n",
    "            results.append(f\"‚ùå Error: {e}\")\n",
    "    \n",
    "    return \"\\n\".join(results), rag.count()\n",
    "\n",
    "def chat_respond(message, history):\n",
    "    response, sources = rag.chat(message, history)\n",
    "    return history + [[message, response]], sources, \"\"\n",
    "\n",
    "def clear_all():\n",
    "    rag.clear()\n",
    "    return \"Cleared!\", 0\n",
    "\n",
    "# Build UI\n",
    "with gr.Blocks(theme=gr.themes.Soft(), title=\"RAG Demo\") as demo:\n",
    "    gr.Markdown(\"# ü§ñ RAG Chat Demo\")\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\"üìÅ Documents\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    files = gr.File(file_count=\"multiple\", label=\"Upload\")\n",
    "                    with gr.Row():\n",
    "                        index_btn = gr.Button(\"Index\", variant=\"primary\")\n",
    "                        clear_btn = gr.Button(\"Clear\")\n",
    "                    status = gr.Textbox(label=\"Status\", lines=4)\n",
    "                with gr.Column():\n",
    "                    count = gr.Number(label=\"Chunks\", value=0)\n",
    "            \n",
    "            index_btn.click(process_files, [files], [status, count])\n",
    "            clear_btn.click(clear_all, [], [status, count])\n",
    "        \n",
    "        with gr.TabItem(\"üí¨ Chat\"):\n",
    "            chatbot = gr.Chatbot(height=400)\n",
    "            with gr.Row():\n",
    "                msg = gr.Textbox(label=\"Message\", scale=4)\n",
    "                send = gr.Button(\"Send\", variant=\"primary\")\n",
    "            sources = gr.Markdown()\n",
    "            \n",
    "            send.click(chat_respond, [msg, chatbot], [chatbot, sources, msg])\n",
    "            msg.submit(chat_respond, [msg, chatbot], [chatbot, sources, msg])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n",
    "'''\n",
    "\n",
    "with open(f\"{deploy_dir}/app.py\", \"w\") as f:\n",
    "    f.write(app_py_content)\n",
    "\n",
    "# 2. requirements.txt\n",
    "requirements_content = '''gradio>=4.0.0\n",
    "chromadb>=0.4.0\n",
    "'''\n",
    "\n",
    "with open(f\"{deploy_dir}/requirements.txt\", \"w\") as f:\n",
    "    f.write(requirements_content)\n",
    "\n",
    "# 3. README.md with Spaces configuration\n",
    "readme_content = '''---\n",
    "title: RAG Chat Demo\n",
    "emoji: ü§ñ\n",
    "colorFrom: blue\n",
    "colorTo: purple\n",
    "sdk: gradio\n",
    "sdk_version: \"4.44.0\"\n",
    "app_file: app.py\n",
    "pinned: false\n",
    "license: mit\n",
    "---\n",
    "\n",
    "# RAG Chat Demo\n",
    "\n",
    "Upload documents and chat with them using RAG (Retrieval Augmented Generation).\n",
    "\n",
    "## Features\n",
    "- Multi-file upload\n",
    "- Document indexing\n",
    "- Question answering\n",
    "\n",
    "## Usage\n",
    "1. Go to the Documents tab and upload text files\n",
    "2. Click \"Index\" to process them\n",
    "3. Go to Chat tab and ask questions!\n",
    "'''\n",
    "\n",
    "with open(f\"{deploy_dir}/README.md\", \"w\") as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(f\"‚úÖ Deployment files created in '{deploy_dir}/'\")\n",
    "print(f\"\\nFiles created:\")\n",
    "for f in os.listdir(deploy_dir):\n",
    "    print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Push to Hugging Face Spaces\n",
    "\n",
    "```bash\n",
    "# 1. Create a new Space on huggingface.co/new-space\n",
    "#    Select \"Gradio\" as the SDK\n",
    "\n",
    "# 2. Clone your space\n",
    "git clone https://huggingface.co/spaces/YOUR_USERNAME/YOUR_SPACE_NAME\n",
    "\n",
    "# 3. Copy your files\n",
    "cp rag_demo_deploy/* YOUR_SPACE_NAME/\n",
    "\n",
    "# 4. Push\n",
    "cd YOUR_SPACE_NAME\n",
    "git add .\n",
    "git commit -m \"Initial deploy\"\n",
    "git push\n",
    "```\n",
    "\n",
    "Your demo will be live at: `https://huggingface.co/spaces/YOUR_USERNAME/YOUR_SPACE_NAME`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Using `gr.Blocks()` Context Manager\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - components outside context\n",
    "demo = gr.Blocks()\n",
    "title = gr.Markdown(\"# Hello\")  # This won't be part of demo!\n",
    "\n",
    "# ‚úÖ Right - everything inside 'with'\n",
    "with gr.Blocks() as demo:\n",
    "    title = gr.Markdown(\"# Hello\")  # Part of demo\n",
    "```\n",
    "\n",
    "**Why:** Components must be created inside the `with` block to be added to the interface.\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 2: Mismatched Inputs/Outputs\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - function returns 2 values but only 1 output\n",
    "def process(text):\n",
    "    return text.upper(), len(text)\n",
    "\n",
    "btn.click(process, [input], [output])  # Missing second output!\n",
    "\n",
    "# ‚úÖ Right - match outputs to return values\n",
    "btn.click(process, [input], [output1, output2])\n",
    "```\n",
    "\n",
    "**Why:** The number of outputs must match the number of values your function returns.\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 3: Blocking Operations Freeze the UI\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - long operation blocks everything\n",
    "def process_files(files):\n",
    "    for file in files:\n",
    "        time.sleep(10)  # UI is frozen!\n",
    "    return \"Done\"\n",
    "\n",
    "# ‚úÖ Right - use progress indicator\n",
    "def process_files(files, progress=gr.Progress()):\n",
    "    for file in progress.tqdm(files):\n",
    "        time.sleep(10)  # Shows progress bar\n",
    "    return \"Done\"\n",
    "```\n",
    "\n",
    "**Why:** Without progress indicators, users don't know if the app is working or frozen.\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 4: Not Handling Errors Gracefully\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - raw exception shown to user\n",
    "def query_llm(prompt):\n",
    "    response = ollama.generate(prompt)  # Might fail!\n",
    "    return response\n",
    "\n",
    "# ‚úÖ Right - catch and show friendly message\n",
    "def query_llm(prompt):\n",
    "    try:\n",
    "        response = ollama.generate(prompt)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"üòî Oops! Something went wrong: {str(e)}. Please try again.\"\n",
    "```\n",
    "\n",
    "**Why:** Users shouldn't see raw Python tracebacks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How to use Gradio Blocks API for complex layouts\n",
    "- ‚úÖ How to structure tabs, rows, and columns\n",
    "- ‚úÖ How to build a complete RAG demo interface\n",
    "- ‚úÖ How to add streaming, authentication, and progress indicators\n",
    "- ‚úÖ How to deploy to Hugging Face Spaces\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "Enhance the RAG demo with these features:\n",
    "\n",
    "1. **Multi-language support** - Add a language selector and translate UI elements\n",
    "2. **Theme toggle** - Let users switch between light and dark mode\n",
    "3. **Export chat** - Add a button to download conversation history as JSON/Markdown\n",
    "4. **Voice input** - Use `gr.Audio` to allow voice questions\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hints</summary>\n",
    "\n",
    "- For themes: `gr.themes.Default()` vs `gr.themes.Monochrome()`\n",
    "- For export: Return a `gr.File` component with the downloaded content\n",
    "- For audio: `gr.Audio(source=\"microphone\", type=\"filepath\")`\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [Gradio Blocks Guide](https://gradio.app/guides/blocks-and-event-listeners)\n",
    "- [Gradio Custom Components](https://gradio.app/guides/custom-components-in-five-minutes)\n",
    "- [Hugging Face Spaces Documentation](https://huggingface.co/docs/hub/spaces)\n",
    "- [Gradio Theming](https://gradio.app/guides/theming-guide)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources\n",
    "import gc\n",
    "\n",
    "# Close any running demos\n",
    "try:\n",
    "    simple_demo.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    layout_demo.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    rag_demo.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚û°Ô∏è Next Steps\n",
    "\n",
    "Continue to [Lab 4.5.2: Agent Playground](lab-4.5.2-agent-playground.ipynb) to build a Streamlit app for visualizing agent reasoning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
