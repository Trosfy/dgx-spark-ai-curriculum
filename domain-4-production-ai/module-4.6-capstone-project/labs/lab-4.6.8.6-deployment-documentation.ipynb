{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 4.6.8.6: Deployment & Documentation\n\n**Capstone Option E:** Browser-Deployed Fine-Tuned LLM (Troscha Matcha Guide)  \n**Phase:** 6 of 6 (Final)  \n**Time:** 6-8 hours  \n**Difficulty:** ‚≠ê‚≠ê‚≠ê\n\n---\n\n## Phase Objectives\n\nBy completing this phase, you will:\n- [ ] Upload model to S3 with proper CORS configuration\n- [ ] Deploy static site to Vercel/Netlify\n- [ ] Create a complete model card\n- [ ] Write technical report outline\n- [ ] Prepare presentation slides\n- [ ] Record demo video\n\n---\n\n## Phase Checklist\n\n- [ ] S3 bucket created with CORS\n- [ ] Model files uploaded\n- [ ] Static site deployed\n- [ ] Demo working publicly\n- [ ] Model card complete\n- [ ] Technical report drafted\n- [ ] Presentation ready\n- [ ] Demo video recorded\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Matters\n",
    "\n",
    "**Documentation is what separates a project from a product.**\n",
    "\n",
    "A well-documented project:\n",
    "- Can be understood by others (and your future self)\n",
    "- Demonstrates professionalism\n",
    "- Enables reproducibility\n",
    "- Shows awareness of limitations and ethics\n",
    "- Is portfolio-ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: S3 Deployment\n",
    "\n",
    "Host your model files on AWS S3 (or any CDN with CORS support)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Configuration\n",
    "\n",
    "s3_cors_config = '''\n",
    "[\n",
    "    {\n",
    "        \"AllowedHeaders\": [\"*\"],\n",
    "        \"AllowedMethods\": [\"GET\", \"HEAD\"],\n",
    "        \"AllowedOrigins\": [\n",
    "            \"https://your-domain.vercel.app\",\n",
    "            \"http://localhost:5173\",\n",
    "            \"http://localhost:3000\"\n",
    "        ],\n",
    "        \"ExposeHeaders\": [\n",
    "            \"Content-Length\",\n",
    "            \"Content-Type\",\n",
    "            \"ETag\"\n",
    "        ],\n",
    "        \"MaxAgeSeconds\": 3600\n",
    "    }\n",
    "]\n",
    "'''\n",
    "\n",
    "print(\"üìÑ S3 CORS Configuration (cors.json)\")\n",
    "print(\"=\"*70)\n",
    "print(s3_cors_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# S3 Upload Commands\n\ns3_commands = '''\n# Create S3 bucket\naws s3 mb s3://troscha-matcha-model --region us-east-1\n\n# Apply CORS configuration\naws s3api put-bucket-cors --bucket troscha-matcha-model --cors-configuration file://cors.json\n\n# Upload model files (from troscha-browser directory)\naws s3 sync ./troscha-browser s3://troscha-matcha-model/ --acl public-read\n\n# Verify upload\naws s3 ls s3://troscha-matcha-model/\n\n# Get public URL\n# Your model will be at: https://troscha-matcha-model.s3.amazonaws.com/\n'''\n\nprint(\"üîß S3 UPLOAD COMMANDS\")\nprint(\"=\"*70)\nprint(s3_commands)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Python upload script\n\nupload_script = '''\n#!/usr/bin/env python3\n\"\"\"\nUpload Troscha model files to S3 with proper configuration.\n\"\"\"\nimport boto3\nfrom pathlib import Path\nimport json\n\ndef upload_model_to_s3(\n    model_dir: str,\n    bucket_name: str,\n    region: str = \"us-east-1\",\n):\n    \"\"\"Upload model files to S3 with CORS configuration.\"\"\"\n    \n    s3 = boto3.client('s3', region_name=region)\n    \n    # Create bucket if doesn't exist\n    try:\n        s3.head_bucket(Bucket=bucket_name)\n        print(f\"Bucket {bucket_name} exists\")\n    except:\n        s3.create_bucket(Bucket=bucket_name)\n        print(f\"Created bucket {bucket_name}\")\n    \n    # Configure CORS\n    cors_config = {\n        'CORSRules': [{\n            'AllowedHeaders': ['*'],\n            'AllowedMethods': ['GET', 'HEAD'],\n            'AllowedOrigins': ['*'],  # Restrict in production!\n            'ExposeHeaders': ['Content-Length', 'Content-Type', 'ETag'],\n            'MaxAgeSeconds': 3600,\n        }]\n    }\n    s3.put_bucket_cors(Bucket=bucket_name, CORSConfiguration=cors_config)\n    print(\"CORS configured\")\n    \n    # Upload files\n    model_path = Path(model_dir)\n    for file_path in model_path.iterdir():\n        if file_path.is_file():\n            key = file_path.name\n            s3.upload_file(\n                str(file_path),\n                bucket_name,\n                key,\n                ExtraArgs={'ACL': 'public-read'}\n            )\n            print(f\"Uploaded {key}\")\n    \n    url = f\"https://{bucket_name}.s3.{region}.amazonaws.com/\"\n    print(f\"\\nModel available at: {url}\")\n    return url\n\nif __name__ == \"__main__\":\n    upload_model_to_s3(\n        model_dir=\"./troscha-matcha/models/troscha-browser\",\n        bucket_name=\"troscha-matcha-model\",\n    )\n'''\n\nprint(\"üìÑ scripts/upload_to_s3.py\")\nprint(\"=\"*70)\nprint(upload_script)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Static Site Deployment\n",
    "\n",
    "Deploy your React app to Vercel or Netlify (both have free tiers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Vercel Deployment\n\nvercel_deploy = '''\n# Install Vercel CLI\nnpm install -g vercel\n\n# Deploy (from project directory)\ncd troscha-chatbot\nvercel\n\n# Follow prompts:\n# - Link to existing project or create new\n# - Accept defaults for build settings\n# - Deploy!\n\n# For production deployment:\nvercel --prod\n\n# Your app will be at: https://troscha-chatbot-xxxxx.vercel.app\n'''\n\nprint(\"üöÄ VERCEL DEPLOYMENT\")\nprint(\"=\"*70)\nprint(vercel_deploy)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Netlify Deployment\n",
    "\n",
    "netlify_deploy = '''\n",
    "# Install Netlify CLI\n",
    "npm install -g netlify-cli\n",
    "\n",
    "# Build the project first\n",
    "npm run build\n",
    "\n",
    "# Deploy\n",
    "netlify deploy --dir=dist\n",
    "\n",
    "# For production:\n",
    "netlify deploy --dir=dist --prod\n",
    "\n",
    "# Your app will be at: https://your-site.netlify.app\n",
    "'''\n",
    "\n",
    "# netlify.toml configuration\n",
    "netlify_toml = '''\n",
    "[[headers]]\n",
    "  for = \"/*\"\n",
    "  [headers.values]\n",
    "    Cross-Origin-Opener-Policy = \"same-origin\"\n",
    "    Cross-Origin-Embedder-Policy = \"require-corp\"\n",
    "\n",
    "[build]\n",
    "  command = \"npm run build\"\n",
    "  publish = \"dist\"\n",
    "'''\n",
    "\n",
    "print(\"üöÄ NETLIFY DEPLOYMENT\")\n",
    "print(\"=\"*70)\n",
    "print(netlify_deploy)\n",
    "print(\"\\nüìÑ netlify.toml\")\n",
    "print(\"-\"*70)\n",
    "print(netlify_toml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Model Card\n",
    "\n",
    "A model card documents your model's capabilities, limitations, and ethical considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model Card Template\n\nmodel_card = '''\n# Model Card: Troscha Matcha Guide\n\n## Model Details\n\n- **Model Name**: Troscha Matcha Guide\n- **Model Type**: Causal Language Model (Chat)\n- **Base Model**: Gemma 3 270M Instruct\n- **Fine-tuning Method**: QLoRA (r=16, alpha=16)\n- **Training Framework**: Unsloth + HuggingFace Transformers\n- **Quantization**: INT4 (ONNX Runtime)\n- **Model Size**: ~150-200MB (browser-ready)\n- **Version**: 1.0.0\n- **Date**: [DATE]\n- **Author**: [YOUR NAME]\n\n## Intended Use\n\n### Primary Use Cases\n- Product recommendations for Troscha matcha menu\n- Explaining differences between matcha grades (Yura to Kiyo)\n- Helping customers find matcha based on preferences (taste, budget)\n- Preparation style guidance (Straight, Latte, Miruku, Strawberry)\n- Addon suggestions (e.g., Oat Milk)\n\n### Out-of-Scope Uses\n- Medical advice (not a substitute for healthcare professionals)\n- General conversation beyond Troscha products\n- Recommendations for competing brands\n- Pricing negotiations or discounts\n\n## Product Catalog\n\nThe model is trained on Troscha's matcha menu:\n\n| Product | Styles Available | Price Range |\n|---------|------------------|-------------|\n| Yura | Latte | Rp 27k |\n| Taku | Straight, Latte, Strawberry | Rp 25-40k |\n| Firu | Straight, Latte, Miruku, Strawberry | Rp 34-52k |\n| Giru | Straight, Latte, Miruku, Strawberry | Rp 39-57k |\n| Zeno | Straight, Latte, Miruku, Strawberry | Rp 44-62k |\n| Moku | Hojicha Latte | Rp 35k |\n| Hiku | Straight, Latte | Rp 79-89k |\n| Kiyo | Straight, Latte | Rp 94-104k |\n\n**Addon**: Oat Milk +Rp 5k\n\n## Training Data\n\n- **Dataset Size**: 300 examples across 12 categories\n- **Data Sources**: Curated product knowledge\n- **Categories**:\n  - Product comparisons\n  - Taste discovery\n  - Budget matching\n  - Preparation style\n  - Seasonal recommendations\n  - And more...\n\n### Data Format\nEach training example includes:\n- System prompt with full menu\n- User question\n- Response with `<preferences>` JSON for structured output\n\n### Data Processing\n- All examples reviewed for accuracy\n- Balanced across product categories\n- No PII or sensitive data\n\n## Training Procedure\n\n- **Hardware**: NVIDIA DGX Spark (128GB unified memory)\n- **Training Time**: ~[X] minutes\n- **Epochs**: 3\n- **Batch Size**: 2 (effective 8 with gradient accumulation)\n- **Learning Rate**: 2e-4 with cosine schedule\n- **Final Training Loss**: [X]\n- **Validation Loss**: [X]\n\n## Structured Output\n\nResponses include `<preferences>` JSON for integration:\n\n```json\n{\n  \"product\": \"Firu\",\n  \"style\": \"Latte\",\n  \"confidence\": 0.9,\n  \"reason\": \"mild sweetness with creamy texture\"\n}\n```\n\n## Evaluation\n\n### Quantitative Metrics\n| Metric | Value |\n|--------|-------|\n| Training Loss | [X] |\n| Validation Loss | [X] |\n| Perplexity (Base) | [X] |\n| Perplexity (Fine-tuned) | [X] |\n\n### Qualitative Assessment\n- Accuracy on product questions: [X/10]\n- Response quality: [X/10]\n- Factual correctness: [X/10]\n- Preferences JSON validity: [X/10]\n\n### Browser Performance\n| Device | Backend | Tokens/sec |\n|--------|---------|------------|\n| [YOUR GPU] | WebGPU | [X] |\n| [LAPTOP] | WASM | [X] |\n\n## Limitations\n\n- **Knowledge Scope**: Limited to Troscha product catalog\n- **Price Updates**: Requires retraining if prices change\n- **Language**: Trained on English/Indonesian mixed content\n- **Performance**: Slower on devices without WebGPU support\n\n## Ethical Considerations\n\n### Potential Benefits\n- Privacy-preserving (runs locally)\n- No ongoing costs for deployment\n- Consistent customer experience\n- Accessible without internet (after initial load)\n\n### Potential Risks\n- May recommend products not in stock\n- Prices may be outdated if not maintained\n- Could provide incorrect product info if asked edge cases\n\n### Mitigations\n- Clear disclaimers about checking current availability\n- Regular evaluation and model updates\n- Fallback to human support for complex queries\n\n## How to Use\n\n### Browser (Transformers.js)\n```javascript\nimport { pipeline } from '@huggingface/transformers';\n\nconst generator = await pipeline(\n  'text-generation',\n  'https://your-s3-url/troscha-matcha',\n  { device: 'webgpu', dtype: 'q4' }\n);\n\nconst response = await generator([\n  { role: 'system', content: TROSCHA_SYSTEM_PROMPT },\n  { role: 'user', content: \"What's good for a first-timer?\" }\n]);\n```\n\n## Citation\n\n```bibtex\n@misc{troscha-matcha-guide-2024,\n  author = {[YOUR NAME]},\n  title = {Troscha Matcha Guide: A Browser-Deployed Fine-Tuned LLM},\n  year = {2024},\n  publisher = {GitHub},\n  url = {https://github.com/yourusername/troscha-matcha}\n}\n```\n\n## License\n\n[Specify license - e.g., MIT, Apache 2.0, or match base model license]\n\n## Contact\n\n- **Author**: [YOUR NAME]\n- **Email**: [YOUR EMAIL]\n- **GitHub**: [YOUR GITHUB]\n'''\n\nprint(\"üìÑ MODEL CARD\")\nprint(\"=\"*70)\nprint(model_card[:3000] + \"\\n...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Technical Report Outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technical Report Outline\n",
    "\n",
    "report_outline = '''\n",
    "# Technical Report: Browser-Deployed Fine-Tuned LLM\n",
    "\n",
    "## Abstract (1 paragraph)\n",
    "- Brief description of the project\n",
    "- Key results and contributions\n",
    "\n",
    "## 1. Introduction (1-2 pages)\n",
    "- Problem statement: Why browser LLMs?\n",
    "- Motivation: Zero cost, privacy, edge deployment\n",
    "- Project goals and scope\n",
    "- Document structure overview\n",
    "\n",
    "## 2. Background (1-2 pages)\n",
    "- QLoRA fine-tuning\n",
    "- ONNX and quantization\n",
    "- Browser ML (WebGPU, WASM)\n",
    "- Related work\n",
    "\n",
    "## 3. System Design (2-3 pages)\n",
    "- Overall architecture diagram\n",
    "- Training pipeline (DGX Spark)\n",
    "- Optimization pipeline\n",
    "- Deployment architecture\n",
    "- Technology choices and rationale\n",
    "\n",
    "## 4. Implementation (3-4 pages)\n",
    "### 4.1 Dataset Preparation\n",
    "- Data collection and curation\n",
    "- Format and structure\n",
    "- Quality assurance\n",
    "\n",
    "### 4.2 Fine-Tuning\n",
    "- Model selection\n",
    "- QLoRA configuration\n",
    "- Training procedure\n",
    "- MLflow tracking\n",
    "\n",
    "### 4.3 Model Optimization\n",
    "- LoRA merging\n",
    "- ONNX export\n",
    "- INT4 quantization\n",
    "\n",
    "### 4.4 Browser Integration\n",
    "- React application\n",
    "- Transformers.js integration\n",
    "- WebGPU optimization\n",
    "\n",
    "## 5. Evaluation (2-3 pages)\n",
    "### 5.1 Training Metrics\n",
    "- Loss curves\n",
    "- Perplexity comparison\n",
    "\n",
    "### 5.2 Quality Assessment\n",
    "- Domain-specific evaluation\n",
    "- Comparison: Base vs Fine-tuned\n",
    "\n",
    "### 5.3 Performance Benchmarks\n",
    "- Inference speed by device\n",
    "- Memory usage\n",
    "- Loading time\n",
    "\n",
    "### 5.4 User Experience\n",
    "- Browser compatibility\n",
    "- Loading experience\n",
    "- Response quality feedback\n",
    "\n",
    "## 6. Discussion (1-2 pages)\n",
    "- What worked well\n",
    "- Challenges and solutions\n",
    "- Lessons learned\n",
    "- Limitations\n",
    "\n",
    "## 7. Conclusion (1 page)\n",
    "- Summary of achievements\n",
    "- Future work\n",
    "- Final thoughts\n",
    "\n",
    "## References\n",
    "\n",
    "## Appendices\n",
    "- A: Complete model card\n",
    "- B: Sample conversations\n",
    "- C: Full code listings\n",
    "- D: Deployment checklist\n",
    "'''\n",
    "\n",
    "print(\"üìÑ TECHNICAL REPORT OUTLINE\")\n",
    "print(\"=\"*70)\n",
    "print(report_outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Presentation Outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Presentation Outline\n\npresentation_outline = '''\n# Troscha Matcha Guide: Browser-Deployed Fine-Tuned LLM\n## Presentation Outline (15-20 slides, 15-20 minutes)\n\n### Slide 1: Title\n- Project name and tagline\n- Your name\n- Date\n\n### Slide 2: The Problem\n- LLMs are expensive to host\n- Privacy concerns with cloud APIs\n- Custom product knowledge needed\n\n### Slide 3: The Solution\n- Train once (DGX Spark)\n- Deploy everywhere (browser)\n- Run locally (zero cost)\n\n### Slide 4: Architecture Overview\n- Visual diagram of the pipeline\n- Train ‚Üí Optimize ‚Üí Deploy\n\n### Slide 5: Why Troscha?\n- Defined product catalog\n- Rich product variations (8 grades, 4 styles)\n- Practical business value\n- Structured output for integration\n\n### Slide 6: Dataset Creation\n- 300 examples across 12 categories\n- Structured `<preferences>` JSON output\n- Product comparison focus\n\n### Slide 7: QLoRA Fine-Tuning\n- Why QLoRA?\n- Configuration\n- Training on DGX Spark\n\n### Slide 8: Model Optimization\n- Merge in BF16 (critical!)\n- ONNX export\n- INT4 quantization\n\n### Slide 9: Size Comparison\n- Visual chart: 2GB ‚Üí 500MB\n- 75% compression\n\n### Slide 10: Browser Integration\n- Transformers.js\n- WebGPU acceleration\n- Preferences JSON parsing\n\n### Slide 11: Live Demo\n- Show the Troscha chatbot\n- Ask: \"What's the difference between Firu and Giru?\"\n- Show preferences badge in UI\n\n### Slide 12: Evaluation Results\n- Training metrics\n- Quality comparison\n- Performance benchmarks\n\n### Slide 13: Deployment\n- S3 for model hosting\n- Vercel for app\n- Cost: ~$0/month\n\n### Slide 14: Challenges & Solutions\n- Technical hurdles faced\n- How you solved them\n\n### Slide 15: Lessons Learned\n- Key takeaways\n- What you'd do differently\n\n### Slide 16: Future Work\n- Larger models\n- Multi-language support\n- Integration with ordering system\n\n### Slide 17: Conclusion\n- Summary of achievements\n- Impact and value\n\n### Slide 18: Questions?\n- Contact info\n- Demo URL\n- GitHub link\n'''\n\nprint(\"üìÑ PRESENTATION OUTLINE\")\nprint(\"=\"*70)\nprint(presentation_outline)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Demo Video Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demo Video Script\n\nvideo_script = '''\n# Troscha Matcha Guide Demo Video Script\n## Duration: 5-10 minutes\n\n### Introduction (30 seconds)\n\"Hi, I'm [NAME] and this is Troscha Matcha Guide - an AI-powered \nproduct recommendation chatbot that runs entirely in your browser, \nwith zero server costs and complete privacy.\n\nLet me show you how it works.\"\n\n### The Problem (45 seconds)\n\"Traditional LLM deployment requires expensive GPU servers. \nUsers' data goes to the cloud. And there's a continuous hosting cost.\n\nFor a product recommendation chatbot, we need consistent, accurate \nresponses about our menu - without ongoing API costs.\"\n\n### Live Demo (2-3 minutes)\n1. Open the website\n2. Show loading process (\"First time downloads ~500MB, then it's cached\")\n3. Ask: \"What's the difference between Firu and Giru?\"\n4. Show response with preferences badge\n5. Ask: \"I want something not too bitter but affordable\"\n6. Show Chrome DevTools - Network tab (\"See? No API calls!\")\n\n### Technical Deep Dive (2 minutes)\n1. Show training notebook\n2. Highlight the system prompt with full menu\n3. Show `<preferences>` JSON format\n4. Explain INT4 quantization\n5. Show Transformers.js parsing code\n\n### Key Metrics (30 seconds)\n- Training: X minutes on DGX Spark\n- Model size: 500MB (was 2GB)\n- Inference: X tokens/second\n- Hosting cost: $0/month\n\n### Conclusion (30 seconds)\n\"Troscha Matcha Guide demonstrates that browser LLMs are practical \nfor product recommendation today.\n\nThe same pipeline works for any product catalog - restaurant menus, \ne-commerce, specialized assistants.\n\nTry it yourself at [URL]. Thanks for watching!\"\n\n---\n\n## Recording Tips\n\n1. Use screen recording software (OBS, Loom, QuickTime)\n2. Clean browser with minimal tabs\n3. Pre-load the model to avoid waiting during demo\n4. Prepare Troscha-specific questions in advance\n5. Keep it concise and engaging\n6. Add captions for accessibility\n'''\n\nprint(\"üìÑ DEMO VIDEO SCRIPT\")\nprint(\"=\"*70)\nprint(video_script)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Final Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Capstone Checklist\n",
    "\n",
    "final_checklist = '''\n",
    "# Option E Capstone: Final Checklist\n",
    "\n",
    "## Artifacts\n",
    "- [ ] Training dataset (150+ examples)\n",
    "- [ ] LoRA adapters (safetensors)\n",
    "- [ ] Merged model (BF16)\n",
    "- [ ] GGUF model (for Ollama)\n",
    "- [ ] ONNX INT4 model (for browser)\n",
    "\n",
    "## Code\n",
    "- [ ] Dataset preparation notebook\n",
    "- [ ] Training notebook with MLflow\n",
    "- [ ] Merge and export script\n",
    "- [ ] ONNX quantization script\n",
    "- [ ] React web application\n",
    "- [ ] S3 upload script\n",
    "\n",
    "## Deployment\n",
    "- [ ] S3 bucket with CORS\n",
    "- [ ] Model files uploaded\n",
    "- [ ] Static site deployed\n",
    "- [ ] Working demo URL\n",
    "\n",
    "## Documentation\n",
    "- [ ] Model card (complete)\n",
    "- [ ] Technical report (15-20 pages)\n",
    "- [ ] README with setup instructions\n",
    "- [ ] Presentation slides (15-20)\n",
    "- [ ] Demo video (5-10 min)\n",
    "\n",
    "## Quality Checks\n",
    "- [ ] Model generates accurate responses\n",
    "- [ ] Browser demo works in Chrome/Edge\n",
    "- [ ] WASM fallback works in Firefox\n",
    "- [ ] Loading experience is smooth\n",
    "- [ ] Error handling is user-friendly\n",
    "\n",
    "## Grading Criteria (Self-Assessment)\n",
    "\n",
    "| Criteria | Points | Self-Score | Notes |\n",
    "|----------|--------|------------|-------|\n",
    "| Dataset Quality | 15 | | |\n",
    "| Training Pipeline | 20 | | |\n",
    "| Optimization Pipeline | 15 | | |\n",
    "| Browser Integration | 20 | | |\n",
    "| Deployment | 10 | | |\n",
    "| Documentation | 10 | | |\n",
    "| Evaluation | 5 | | |\n",
    "| Innovation | 5 | | |\n",
    "| **TOTAL** | **100** | | |\n",
    "'''\n",
    "\n",
    "print(\"üìã FINAL CHECKLIST\")\n",
    "print(\"=\"*70)\n",
    "print(final_checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Capstone Complete!\n",
    "\n",
    "Congratulations! You've built a complete browser-deployed LLM:\n",
    "\n",
    "- ‚úÖ Created a domain-specific training dataset\n",
    "- ‚úÖ Fine-tuned with QLoRA on DGX Spark\n",
    "- ‚úÖ Merged and optimized for browser deployment\n",
    "- ‚úÖ Built a React application with Transformers.js\n",
    "- ‚úÖ Deployed with zero ongoing costs\n",
    "- ‚úÖ Documented your work professionally\n",
    "\n",
    "**You are now AI-ready!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save documentation templates\nfrom pathlib import Path\n\ndocs_dir = Path(\"./troscha-matcha/docs\")\ndocs_dir.mkdir(parents=True, exist_ok=True)\n\n# Save model card\nwith open(docs_dir / \"MODEL_CARD.md\", 'w') as f:\n    f.write(model_card)\n\n# Save report outline\nwith open(docs_dir / \"REPORT_OUTLINE.md\", 'w') as f:\n    f.write(report_outline)\n\n# Save presentation outline  \nwith open(docs_dir / \"PRESENTATION_OUTLINE.md\", 'w') as f:\n    f.write(presentation_outline)\n\n# Save video script\nwith open(docs_dir / \"VIDEO_SCRIPT.md\", 'w') as f:\n    f.write(video_script)\n\n# Save checklist\nwith open(docs_dir / \"FINAL_CHECKLIST.md\", 'w') as f:\n    f.write(final_checklist)\n\nprint(f\"‚úÖ Documentation templates saved to {docs_dir}\")\nprint(\"\\nüìÅ Files created:\")\nfor f in sorted(docs_dir.iterdir()):\n    print(f\"   {f.name}\")\n\nprint(\"\\nüéâ CAPSTONE COMPLETE!\")\nprint(\"\\nüçµ You've successfully built a browser-deployed Troscha Matcha Guide!\")\nprint(\"   Share your demo and inspire others!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}