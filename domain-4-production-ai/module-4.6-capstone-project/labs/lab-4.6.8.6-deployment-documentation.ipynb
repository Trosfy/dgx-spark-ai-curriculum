{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.6.8.6: Deployment & Documentation\n",
    "\n",
    "**Capstone Option E:** Browser-Deployed Fine-Tuned LLM (Matcha Expert)  \n",
    "**Phase:** 6 of 6 (Final)  \n",
    "**Time:** 6-8 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## Phase Objectives\n",
    "\n",
    "By completing this phase, you will:\n",
    "- [ ] Upload model to S3 with proper CORS configuration\n",
    "- [ ] Deploy static site to Vercel/Netlify\n",
    "- [ ] Create a complete model card\n",
    "- [ ] Write technical report outline\n",
    "- [ ] Prepare presentation slides\n",
    "- [ ] Record demo video\n",
    "\n",
    "---\n",
    "\n",
    "## Phase Checklist\n",
    "\n",
    "- [ ] S3 bucket created with CORS\n",
    "- [ ] Model files uploaded\n",
    "- [ ] Static site deployed\n",
    "- [ ] Demo working publicly\n",
    "- [ ] Model card complete\n",
    "- [ ] Technical report drafted\n",
    "- [ ] Presentation ready\n",
    "- [ ] Demo video recorded\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Matters\n",
    "\n",
    "**Documentation is what separates a project from a product.**\n",
    "\n",
    "A well-documented project:\n",
    "- Can be understood by others (and your future self)\n",
    "- Demonstrates professionalism\n",
    "- Enables reproducibility\n",
    "- Shows awareness of limitations and ethics\n",
    "- Is portfolio-ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: S3 Deployment\n",
    "\n",
    "Host your model files on AWS S3 (or any CDN with CORS support)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Configuration\n",
    "\n",
    "s3_cors_config = '''\n",
    "[\n",
    "    {\n",
    "        \"AllowedHeaders\": [\"*\"],\n",
    "        \"AllowedMethods\": [\"GET\", \"HEAD\"],\n",
    "        \"AllowedOrigins\": [\n",
    "            \"https://your-domain.vercel.app\",\n",
    "            \"http://localhost:5173\",\n",
    "            \"http://localhost:3000\"\n",
    "        ],\n",
    "        \"ExposeHeaders\": [\n",
    "            \"Content-Length\",\n",
    "            \"Content-Type\",\n",
    "            \"ETag\"\n",
    "        ],\n",
    "        \"MaxAgeSeconds\": 3600\n",
    "    }\n",
    "]\n",
    "'''\n",
    "\n",
    "print(\"üìÑ S3 CORS Configuration (cors.json)\")\n",
    "print(\"=\"*70)\n",
    "print(s3_cors_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Upload Commands\n",
    "\n",
    "s3_commands = '''\n",
    "# Create S3 bucket\n",
    "aws s3 mb s3://matcha-expert-model --region us-east-1\n",
    "\n",
    "# Apply CORS configuration\n",
    "aws s3api put-bucket-cors --bucket matcha-expert-model --cors-configuration file://cors.json\n",
    "\n",
    "# Upload model files (from matcha-browser directory)\n",
    "aws s3 sync ./matcha-browser s3://matcha-expert-model/ --acl public-read\n",
    "\n",
    "# Verify upload\n",
    "aws s3 ls s3://matcha-expert-model/\n",
    "\n",
    "# Get public URL\n",
    "# Your model will be at: https://matcha-expert-model.s3.amazonaws.com/\n",
    "'''\n",
    "\n",
    "print(\"üîß S3 UPLOAD COMMANDS\")\n",
    "print(\"=\"*70)\n",
    "print(s3_commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python upload script\n",
    "\n",
    "upload_script = '''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Upload model files to S3 with proper configuration.\n",
    "\"\"\"\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def upload_model_to_s3(\n",
    "    model_dir: str,\n",
    "    bucket_name: str,\n",
    "    region: str = \"us-east-1\",\n",
    "):\n",
    "    \"\"\"Upload model files to S3 with CORS configuration.\"\"\"\n",
    "    \n",
    "    s3 = boto3.client('s3', region_name=region)\n",
    "    \n",
    "    # Create bucket if doesn't exist\n",
    "    try:\n",
    "        s3.head_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket {bucket_name} exists\")\n",
    "    except:\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "        print(f\"Created bucket {bucket_name}\")\n",
    "    \n",
    "    # Configure CORS\n",
    "    cors_config = {\n",
    "        'CORSRules': [{\n",
    "            'AllowedHeaders': ['*'],\n",
    "            'AllowedMethods': ['GET', 'HEAD'],\n",
    "            'AllowedOrigins': ['*'],  # Restrict in production!\n",
    "            'ExposeHeaders': ['Content-Length', 'Content-Type', 'ETag'],\n",
    "            'MaxAgeSeconds': 3600,\n",
    "        }]\n",
    "    }\n",
    "    s3.put_bucket_cors(Bucket=bucket_name, CORSConfiguration=cors_config)\n",
    "    print(\"CORS configured\")\n",
    "    \n",
    "    # Upload files\n",
    "    model_path = Path(model_dir)\n",
    "    for file_path in model_path.iterdir():\n",
    "        if file_path.is_file():\n",
    "            key = file_path.name\n",
    "            s3.upload_file(\n",
    "                str(file_path),\n",
    "                bucket_name,\n",
    "                key,\n",
    "                ExtraArgs={'ACL': 'public-read'}\n",
    "            )\n",
    "            print(f\"Uploaded {key}\")\n",
    "    \n",
    "    url = f\"https://{bucket_name}.s3.{region}.amazonaws.com/\"\n",
    "    print(f\"\\nModel available at: {url}\")\n",
    "    return url\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    upload_model_to_s3(\n",
    "        model_dir=\"./matcha-expert/models/matcha-browser\",\n",
    "        bucket_name=\"matcha-expert-model\",\n",
    "    )\n",
    "'''\n",
    "\n",
    "print(\"üìÑ scripts/upload_to_s3.py\")\n",
    "print(\"=\"*70)\n",
    "print(upload_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Static Site Deployment\n",
    "\n",
    "Deploy your React app to Vercel or Netlify (both have free tiers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vercel Deployment\n",
    "\n",
    "vercel_deploy = '''\n",
    "# Install Vercel CLI\n",
    "npm install -g vercel\n",
    "\n",
    "# Deploy (from project directory)\n",
    "cd matcha-chatbot\n",
    "vercel\n",
    "\n",
    "# Follow prompts:\n",
    "# - Link to existing project or create new\n",
    "# - Accept defaults for build settings\n",
    "# - Deploy!\n",
    "\n",
    "# For production deployment:\n",
    "vercel --prod\n",
    "\n",
    "# Your app will be at: https://matcha-chatbot-xxxxx.vercel.app\n",
    "'''\n",
    "\n",
    "print(\"üöÄ VERCEL DEPLOYMENT\")\n",
    "print(\"=\"*70)\n",
    "print(vercel_deploy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Netlify Deployment\n",
    "\n",
    "netlify_deploy = '''\n",
    "# Install Netlify CLI\n",
    "npm install -g netlify-cli\n",
    "\n",
    "# Build the project first\n",
    "npm run build\n",
    "\n",
    "# Deploy\n",
    "netlify deploy --dir=dist\n",
    "\n",
    "# For production:\n",
    "netlify deploy --dir=dist --prod\n",
    "\n",
    "# Your app will be at: https://your-site.netlify.app\n",
    "'''\n",
    "\n",
    "# netlify.toml configuration\n",
    "netlify_toml = '''\n",
    "[[headers]]\n",
    "  for = \"/*\"\n",
    "  [headers.values]\n",
    "    Cross-Origin-Opener-Policy = \"same-origin\"\n",
    "    Cross-Origin-Embedder-Policy = \"require-corp\"\n",
    "\n",
    "[build]\n",
    "  command = \"npm run build\"\n",
    "  publish = \"dist\"\n",
    "'''\n",
    "\n",
    "print(\"üöÄ NETLIFY DEPLOYMENT\")\n",
    "print(\"=\"*70)\n",
    "print(netlify_deploy)\n",
    "print(\"\\nüìÑ netlify.toml\")\n",
    "print(\"-\"*70)\n",
    "print(netlify_toml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Model Card\n",
    "\n",
    "A model card documents your model's capabilities, limitations, and ethical considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model Card Template\n\nmodel_card = '''\n# Model Card: Matcha Expert\n\n## Model Details\n\n- **Model Name**: Matcha Expert\n- **Model Type**: Causal Language Model (Chat)\n- **Base Model**: Gemma 3 270M Instruct\n- **Fine-tuning Method**: QLoRA (r=16, alpha=16)\n- **Training Framework**: Unsloth + HuggingFace Transformers\n- **Quantization**: INT4 (ONNX Runtime)\n- **Model Size**: ~150-200MB (browser-ready)\n- **Version**: 1.0.0\n- **Date**: [DATE]\n- **Author**: [YOUR NAME]\n\n## Intended Use\n\n### Primary Use Cases\n- Educational information about matcha tea\n- Preparation guidance and techniques\n- Quality assessment help\n- Recipe suggestions\n- Cultural context about Japanese tea traditions\n\n### Out-of-Scope Uses\n- Medical advice (not a substitute for healthcare professionals)\n- General conversation beyond matcha topics\n- Commercial recommendations or endorsements\n- Other types of tea beyond matcha\n\n## Training Data\n\n- **Dataset Size**: [X] examples\n- **Data Sources**: Curated domain knowledge\n- **Categories**:\n  - Matcha grades and quality: [X]%\n  - Preparation methods: [X]%\n  - Health benefits: [X]%\n  - Cultural context: [X]%\n  - Recipes: [X]%\n  - Storage and buying guide: [X]%\n\n### Data Processing\n- All examples reviewed for accuracy\n- Balanced across categories\n- No PII or sensitive data\n\n## Training Procedure\n\n- **Hardware**: NVIDIA DGX Spark (128GB unified memory)\n- **Training Time**: ~[X] minutes\n- **Epochs**: 3\n- **Batch Size**: 2 (effective 8 with gradient accumulation)\n- **Learning Rate**: 2e-4 with cosine schedule\n- **Final Training Loss**: [X]\n- **Validation Loss**: [X]\n\n## Evaluation\n\n### Quantitative Metrics\n| Metric | Value |\n|--------|-------|\n| Training Loss | [X] |\n| Validation Loss | [X] |\n| Perplexity (Base) | [X] |\n| Perplexity (Fine-tuned) | [X] |\n\n### Qualitative Assessment\n- Accuracy on domain questions: [X/10]\n- Response quality: [X/10]\n- Factual correctness: [X/10]\n\n### Browser Performance\n| Device | Backend | Tokens/sec |\n|--------|---------|------------|\n| [YOUR GPU] | WebGPU | [X] |\n| [LAPTOP] | WASM | [X] |\n\n## Limitations\n\n- **Knowledge Cutoff**: Training data reflects knowledge as of [DATE]\n- **Domain Scope**: Limited to matcha tea; may not perform well on other topics\n- **Hallucination Risk**: May occasionally generate plausible but incorrect information\n- **Language**: Primarily trained on English content\n- **Performance**: Slower on devices without WebGPU support\n\n## Ethical Considerations\n\n### Potential Benefits\n- Privacy-preserving (runs locally)\n- No ongoing costs for users\n- Educational value\n- Accessible without internet (after initial load)\n\n### Potential Risks\n- Health information should not replace professional advice\n- May perpetuate biases in training data\n- Could provide incorrect information if asked beyond training scope\n\n### Mitigations\n- Clear disclaimers in the UI\n- Focused training on verified information\n- Regular evaluation and updates\n\n## How to Use\n\n### Browser (Transformers.js)\n```javascript\nimport { pipeline } from '@huggingface/transformers';\n\nconst generator = await pipeline(\n  'text-generation',\n  'https://your-s3-url/matcha-expert',\n  { device: 'webgpu', dtype: 'q4' }\n);\n\nconst response = await generator([\n  { role: 'system', content: 'You are a matcha expert.' },\n  { role: 'user', content: 'What is ceremonial grade?' }\n]);\n```\n\n## Citation\n\n```bibtex\n@misc{matcha-expert-2024,\n  author = {[YOUR NAME]},\n  title = {Matcha Expert: A Browser-Deployed Fine-Tuned LLM},\n  year = {2024},\n  publisher = {GitHub},\n  url = {https://github.com/yourusername/matcha-expert}\n}\n```\n\n## License\n\n[Specify license - e.g., MIT, Apache 2.0, or match base model license]\n\n## Contact\n\n- **Author**: [YOUR NAME]\n- **Email**: [YOUR EMAIL]\n- **GitHub**: [YOUR GITHUB]\n'''\n\nprint(\"üìÑ MODEL CARD\")\nprint(\"=\"*70)\nprint(model_card[:3000] + \"\\n...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Technical Report Outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technical Report Outline\n",
    "\n",
    "report_outline = '''\n",
    "# Technical Report: Browser-Deployed Fine-Tuned LLM\n",
    "\n",
    "## Abstract (1 paragraph)\n",
    "- Brief description of the project\n",
    "- Key results and contributions\n",
    "\n",
    "## 1. Introduction (1-2 pages)\n",
    "- Problem statement: Why browser LLMs?\n",
    "- Motivation: Zero cost, privacy, edge deployment\n",
    "- Project goals and scope\n",
    "- Document structure overview\n",
    "\n",
    "## 2. Background (1-2 pages)\n",
    "- QLoRA fine-tuning\n",
    "- ONNX and quantization\n",
    "- Browser ML (WebGPU, WASM)\n",
    "- Related work\n",
    "\n",
    "## 3. System Design (2-3 pages)\n",
    "- Overall architecture diagram\n",
    "- Training pipeline (DGX Spark)\n",
    "- Optimization pipeline\n",
    "- Deployment architecture\n",
    "- Technology choices and rationale\n",
    "\n",
    "## 4. Implementation (3-4 pages)\n",
    "### 4.1 Dataset Preparation\n",
    "- Data collection and curation\n",
    "- Format and structure\n",
    "- Quality assurance\n",
    "\n",
    "### 4.2 Fine-Tuning\n",
    "- Model selection\n",
    "- QLoRA configuration\n",
    "- Training procedure\n",
    "- MLflow tracking\n",
    "\n",
    "### 4.3 Model Optimization\n",
    "- LoRA merging\n",
    "- ONNX export\n",
    "- INT4 quantization\n",
    "\n",
    "### 4.4 Browser Integration\n",
    "- React application\n",
    "- Transformers.js integration\n",
    "- WebGPU optimization\n",
    "\n",
    "## 5. Evaluation (2-3 pages)\n",
    "### 5.1 Training Metrics\n",
    "- Loss curves\n",
    "- Perplexity comparison\n",
    "\n",
    "### 5.2 Quality Assessment\n",
    "- Domain-specific evaluation\n",
    "- Comparison: Base vs Fine-tuned\n",
    "\n",
    "### 5.3 Performance Benchmarks\n",
    "- Inference speed by device\n",
    "- Memory usage\n",
    "- Loading time\n",
    "\n",
    "### 5.4 User Experience\n",
    "- Browser compatibility\n",
    "- Loading experience\n",
    "- Response quality feedback\n",
    "\n",
    "## 6. Discussion (1-2 pages)\n",
    "- What worked well\n",
    "- Challenges and solutions\n",
    "- Lessons learned\n",
    "- Limitations\n",
    "\n",
    "## 7. Conclusion (1 page)\n",
    "- Summary of achievements\n",
    "- Future work\n",
    "- Final thoughts\n",
    "\n",
    "## References\n",
    "\n",
    "## Appendices\n",
    "- A: Complete model card\n",
    "- B: Sample conversations\n",
    "- C: Full code listings\n",
    "- D: Deployment checklist\n",
    "'''\n",
    "\n",
    "print(\"üìÑ TECHNICAL REPORT OUTLINE\")\n",
    "print(\"=\"*70)\n",
    "print(report_outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Presentation Outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Presentation Outline\n",
    "\n",
    "presentation_outline = '''\n",
    "# Matcha Expert: Browser-Deployed Fine-Tuned LLM\n",
    "## Presentation Outline (15-20 slides, 15-20 minutes)\n",
    "\n",
    "### Slide 1: Title\n",
    "- Project name and tagline\n",
    "- Your name\n",
    "- Date\n",
    "\n",
    "### Slide 2: The Problem\n",
    "- LLMs are expensive to host\n",
    "- Privacy concerns with cloud APIs\n",
    "- Not everyone has GPU access\n",
    "\n",
    "### Slide 3: The Solution\n",
    "- Train once (DGX Spark)\n",
    "- Deploy everywhere (browser)\n",
    "- Run locally (zero cost)\n",
    "\n",
    "### Slide 4: Architecture Overview\n",
    "- Visual diagram of the pipeline\n",
    "- Train ‚Üí Optimize ‚Üí Deploy\n",
    "\n",
    "### Slide 5: Why Matcha?\n",
    "- Defined domain\n",
    "- Rich vocabulary\n",
    "- Safe topic\n",
    "- Practical value\n",
    "\n",
    "### Slide 6: Dataset Creation\n",
    "- 150+ examples\n",
    "- 8 categories\n",
    "- Quality over quantity\n",
    "\n",
    "### Slide 7: QLoRA Fine-Tuning\n",
    "- Why QLoRA?\n",
    "- Configuration\n",
    "- Training on DGX Spark\n",
    "\n",
    "### Slide 8: Model Optimization\n",
    "- Merge in BF16 (critical!)\n",
    "- ONNX export\n",
    "- INT4 quantization\n",
    "\n",
    "### Slide 9: Size Comparison\n",
    "- Visual chart: 2GB ‚Üí 500MB\n",
    "- 75% compression\n",
    "\n",
    "### Slide 10: Browser Integration\n",
    "- Transformers.js\n",
    "- WebGPU acceleration\n",
    "- WASM fallback\n",
    "\n",
    "### Slide 11: Live Demo\n",
    "- Show the chatbot\n",
    "- Ask sample questions\n",
    "- Highlight local execution\n",
    "\n",
    "### Slide 12: Evaluation Results\n",
    "- Training metrics\n",
    "- Quality comparison\n",
    "- Performance benchmarks\n",
    "\n",
    "### Slide 13: Deployment\n",
    "- S3 for model hosting\n",
    "- Vercel for app\n",
    "- Cost: ~$0/month\n",
    "\n",
    "### Slide 14: Challenges & Solutions\n",
    "- Technical hurdles faced\n",
    "- How you solved them\n",
    "\n",
    "### Slide 15: Lessons Learned\n",
    "- Key takeaways\n",
    "- What you'd do differently\n",
    "\n",
    "### Slide 16: Future Work\n",
    "- Larger models\n",
    "- More domains\n",
    "- Mobile optimization\n",
    "\n",
    "### Slide 17: Conclusion\n",
    "- Summary of achievements\n",
    "- Impact and value\n",
    "\n",
    "### Slide 18: Questions?\n",
    "- Contact info\n",
    "- Demo URL\n",
    "- GitHub link\n",
    "'''\n",
    "\n",
    "print(\"üìÑ PRESENTATION OUTLINE\")\n",
    "print(\"=\"*70)\n",
    "print(presentation_outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Demo Video Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo Video Script\n",
    "\n",
    "video_script = '''\n",
    "# Matcha Expert Demo Video Script\n",
    "## Duration: 5-10 minutes\n",
    "\n",
    "### Introduction (30 seconds)\n",
    "\"Hi, I'm [NAME] and this is Matcha Expert - an AI chatbot that runs \n",
    "entirely in your browser, with zero server costs and complete privacy.\n",
    "\n",
    "Let me show you how it works.\"\n",
    "\n",
    "### The Problem (45 seconds)\n",
    "\"Traditional LLM deployment requires expensive GPU servers. \n",
    "Users' data goes to the cloud. And there's a continuous hosting cost.\n",
    "\n",
    "What if we could train once and let users run the model themselves?\"\n",
    "\n",
    "### Live Demo (2-3 minutes)\n",
    "1. Open the website\n",
    "2. Show loading process (\"First time downloads ~500MB, then it's cached\")\n",
    "3. Ask: \"What's the difference between ceremonial and culinary grade matcha?\"\n",
    "4. Show response quality\n",
    "5. Ask: \"How should I store matcha?\"\n",
    "6. Show Chrome DevTools - Network tab (\"See? No API calls!\")\n",
    "\n",
    "### Technical Deep Dive (2 minutes)\n",
    "1. Show training notebook\n",
    "2. Highlight QLoRA configuration\n",
    "3. Show size comparison chart\n",
    "4. Explain INT4 quantization\n",
    "5. Show Transformers.js code\n",
    "\n",
    "### Key Metrics (30 seconds)\n",
    "- Training: X minutes on DGX Spark\n",
    "- Model size: 500MB (was 2GB)\n",
    "- Inference: X tokens/second\n",
    "- Hosting cost: $0/month\n",
    "\n",
    "### Conclusion (30 seconds)\n",
    "\"Matcha Expert demonstrates that browser LLMs are practical today.\n",
    "\n",
    "The same pipeline works for any domain - customer support, education, \n",
    "specialized assistants.\n",
    "\n",
    "Try it yourself at [URL]. Thanks for watching!\"\n",
    "\n",
    "---\n",
    "\n",
    "## Recording Tips\n",
    "\n",
    "1. Use screen recording software (OBS, Loom, QuickTime)\n",
    "2. Clean browser with minimal tabs\n",
    "3. Pre-load the model to avoid waiting during demo\n",
    "4. Prepare questions in advance\n",
    "5. Keep it concise and engaging\n",
    "6. Add captions for accessibility\n",
    "'''\n",
    "\n",
    "print(\"üìÑ DEMO VIDEO SCRIPT\")\n",
    "print(\"=\"*70)\n",
    "print(video_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Final Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Capstone Checklist\n",
    "\n",
    "final_checklist = '''\n",
    "# Option E Capstone: Final Checklist\n",
    "\n",
    "## Artifacts\n",
    "- [ ] Training dataset (150+ examples)\n",
    "- [ ] LoRA adapters (safetensors)\n",
    "- [ ] Merged model (BF16)\n",
    "- [ ] GGUF model (for Ollama)\n",
    "- [ ] ONNX INT4 model (for browser)\n",
    "\n",
    "## Code\n",
    "- [ ] Dataset preparation notebook\n",
    "- [ ] Training notebook with MLflow\n",
    "- [ ] Merge and export script\n",
    "- [ ] ONNX quantization script\n",
    "- [ ] React web application\n",
    "- [ ] S3 upload script\n",
    "\n",
    "## Deployment\n",
    "- [ ] S3 bucket with CORS\n",
    "- [ ] Model files uploaded\n",
    "- [ ] Static site deployed\n",
    "- [ ] Working demo URL\n",
    "\n",
    "## Documentation\n",
    "- [ ] Model card (complete)\n",
    "- [ ] Technical report (15-20 pages)\n",
    "- [ ] README with setup instructions\n",
    "- [ ] Presentation slides (15-20)\n",
    "- [ ] Demo video (5-10 min)\n",
    "\n",
    "## Quality Checks\n",
    "- [ ] Model generates accurate responses\n",
    "- [ ] Browser demo works in Chrome/Edge\n",
    "- [ ] WASM fallback works in Firefox\n",
    "- [ ] Loading experience is smooth\n",
    "- [ ] Error handling is user-friendly\n",
    "\n",
    "## Grading Criteria (Self-Assessment)\n",
    "\n",
    "| Criteria | Points | Self-Score | Notes |\n",
    "|----------|--------|------------|-------|\n",
    "| Dataset Quality | 15 | | |\n",
    "| Training Pipeline | 20 | | |\n",
    "| Optimization Pipeline | 15 | | |\n",
    "| Browser Integration | 20 | | |\n",
    "| Deployment | 10 | | |\n",
    "| Documentation | 10 | | |\n",
    "| Evaluation | 5 | | |\n",
    "| Innovation | 5 | | |\n",
    "| **TOTAL** | **100** | | |\n",
    "'''\n",
    "\n",
    "print(\"üìã FINAL CHECKLIST\")\n",
    "print(\"=\"*70)\n",
    "print(final_checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Capstone Complete!\n",
    "\n",
    "Congratulations! You've built a complete browser-deployed LLM:\n",
    "\n",
    "- ‚úÖ Created a domain-specific training dataset\n",
    "- ‚úÖ Fine-tuned with QLoRA on DGX Spark\n",
    "- ‚úÖ Merged and optimized for browser deployment\n",
    "- ‚úÖ Built a React application with Transformers.js\n",
    "- ‚úÖ Deployed with zero ongoing costs\n",
    "- ‚úÖ Documented your work professionally\n",
    "\n",
    "**You are now AI-ready!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save documentation templates\n",
    "from pathlib import Path\n",
    "\n",
    "docs_dir = Path(\"./matcha-expert/docs\")\n",
    "docs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model card\n",
    "with open(docs_dir / \"MODEL_CARD.md\", 'w') as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "# Save report outline\n",
    "with open(docs_dir / \"REPORT_OUTLINE.md\", 'w') as f:\n",
    "    f.write(report_outline)\n",
    "\n",
    "# Save presentation outline  \n",
    "with open(docs_dir / \"PRESENTATION_OUTLINE.md\", 'w') as f:\n",
    "    f.write(presentation_outline)\n",
    "\n",
    "# Save video script\n",
    "with open(docs_dir / \"VIDEO_SCRIPT.md\", 'w') as f:\n",
    "    f.write(video_script)\n",
    "\n",
    "# Save checklist\n",
    "with open(docs_dir / \"FINAL_CHECKLIST.md\", 'w') as f:\n",
    "    f.write(final_checklist)\n",
    "\n",
    "print(f\"‚úÖ Documentation templates saved to {docs_dir}\")\n",
    "print(\"\\nüìÅ Files created:\")\n",
    "for f in sorted(docs_dir.iterdir()):\n",
    "    print(f\"   {f.name}\")\n",
    "\n",
    "print(\"\\nüéâ CAPSTONE COMPLETE!\")\n",
    "print(\"\\nüçµ You've successfully built a browser-deployed fine-tuned LLM!\")\n",
    "print(\"   Share your demo and inspire others!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}