{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.6.8: Option E - Browser-Deployed Fine-Tuned LLM\n",
    "\n",
    "**Module:** 4.6 - Capstone Project (Domain 4: Production AI)  \n",
    "**Time:** 40-50 hours total (6 phases)  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "Build a **complete ML pipeline** that trains a domain-specific chatbot on DGX Spark and deploys it as a **zero-cost static web application** running entirely in the browser.\n",
    "\n",
    "| Phase | Lab | Description | Time |\n",
    "|-------|-----|-------------|------|\n",
    "| 1 | 4.6.8.1 | Dataset Preparation | 4-6 hours |\n",
    "| 2 | 4.6.8.2 | QLoRA Fine-Tuning | 6-8 hours |\n",
    "| 3 | 4.6.8.3 | Merge & Export | 3-4 hours |\n",
    "| 4 | 4.6.8.4 | ONNX Quantization | 4-6 hours |\n",
    "| 5 | 4.6.8.5 | Browser Integration | 8-10 hours |\n",
    "| 6 | 4.6.8.6 | Deployment & Documentation | 6-8 hours |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Capstone Is Special\n",
    "\n",
    "This project demonstrates the **complete ML lifecycle**:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    THE BROWSER LLM JOURNEY                               ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                          ‚îÇ\n",
    "‚îÇ   üìä DATA          üèãÔ∏è TRAIN           üîß OPTIMIZE        üåê DEPLOY       ‚îÇ\n",
    "‚îÇ                                                                          ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ\n",
    "‚îÇ   ‚îÇ Dataset ‚îÇ ‚îÄ‚îÄ‚ñ∂ ‚îÇ QLoRA on    ‚îÇ ‚îÄ‚îÄ‚ñ∂ ‚îÇ Merge +   ‚îÇ ‚îÄ‚îÄ‚ñ∂ ‚îÇ Browser   ‚îÇ   ‚îÇ\n",
    "‚îÇ   ‚îÇ (150+)  ‚îÇ     ‚îÇ DGX Spark   ‚îÇ     ‚îÇ ONNX INT4 ‚îÇ     ‚îÇ + WebGPU  ‚îÇ   ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ\n",
    "‚îÇ                                                                          ‚îÇ\n",
    "‚îÇ   Quality          128GB Memory       ~500MB Final      $0/month!       ‚îÇ\n",
    "‚îÇ   over             = No Limits        Browser-Ready     Privacy-First   ‚îÇ\n",
    "‚îÇ   Quantity                                                               ‚îÇ\n",
    "‚îÇ                                                                          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Unique Benefits:**\n",
    "- **Zero Cost Deployment** - Static hosting = no GPU servers, no API fees\n",
    "- **Privacy First** - All inference happens on user's device\n",
    "- **Instant Loading** - Models are cached after first download\n",
    "- **Full Stack Skills** - ML + Web development combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What Are We Building?\n",
    "\n",
    "> **Imagine you're creating a pocket-sized expert.**\n",
    ">\n",
    "> 1. **First, you teach it** - We train a small AI on everything about matcha tea\n",
    "> 2. **Then, you shrink it** - We compress it to fit in a web browser (~500MB)\n",
    "> 3. **Finally, you share it** - Anyone can visit your website and chat with your expert\n",
    ">\n",
    "> The magic? **It runs on THEIR computer**, not yours. So even if a million people use it, you pay nothing!\n",
    ">\n",
    "> It's like writing a book that reads itself out loud to each reader, using their own voice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Domain: Matcha Tea Expertise\n",
    "\n",
    "We're building a **Matcha Expert Chatbot** because:\n",
    "\n",
    "| Reason | Why It's Perfect |\n",
    "|--------|------------------|\n",
    "| **Defined scope** | Clear expertise boundaries (not open-ended) |\n",
    "| **Rich vocabulary** | Ceremonial, culinary, koicha, usucha, tencha... |\n",
    "| **Cultural context** | Japanese tea ceremony, history, traditions |\n",
    "| **Practical value** | Preparation tips, health info, recipes |\n",
    "| **Safe topic** | No harmful content risks |\n",
    "\n",
    "### Topics Our Chatbot Will Cover\n",
    "\n",
    "1. **Matcha Grades** - Ceremonial, premium, culinary, cooking\n",
    "2. **Preparation Methods** - Whisking, temperature, ratios\n",
    "3. **Health Benefits** - Antioxidants, L-theanine, caffeine\n",
    "4. **Cultural Context** - Tea ceremony, history, traditions\n",
    "5. **Recipes** - Lattes, desserts, smoothies\n",
    "6. **Quality Indicators** - Color, aroma, texture, origin\n",
    "7. **Storage & Freshness** - Oxidation, refrigeration, shelf life\n",
    "8. **Buying Guide** - Brands, authenticity, price points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Technical Architecture\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                         TRAINING PHASE (DGX Spark)                       ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                          ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n",
    "‚îÇ   ‚îÇ Training     ‚îÇ    ‚îÇ Gemma 3 1B        ‚îÇ    ‚îÇ LoRA Adapters    ‚îÇ    ‚îÇ\n",
    "‚îÇ   ‚îÇ Dataset      ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ + QLoRA           ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ (~30MB)          ‚îÇ    ‚îÇ\n",
    "‚îÇ   ‚îÇ (150-200)    ‚îÇ    ‚îÇ + Unsloth 2x      ‚îÇ    ‚îÇ MLflow Tracked   ‚îÇ    ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n",
    "‚îÇ                                                                          ‚îÇ\n",
    "‚îÇ   Memory: ~8GB         Time: ~30 min           Format: safetensors     ‚îÇ\n",
    "‚îÇ                                                                          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                    ‚îÇ\n",
    "                                    ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                        OPTIMIZATION PHASE (DGX Spark)                    ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                          ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n",
    "‚îÇ   ‚îÇ Merge LoRA   ‚îÇ    ‚îÇ Export to ONNX    ‚îÇ    ‚îÇ INT4 Quantize    ‚îÇ    ‚îÇ\n",
    "‚îÇ   ‚îÇ (BF16)       ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ (FP32)            ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ (Browser-Ready)  ‚îÇ    ‚îÇ\n",
    "‚îÇ   ‚îÇ ~2GB         ‚îÇ    ‚îÇ ~4GB              ‚îÇ    ‚îÇ ~500MB           ‚îÇ    ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n",
    "‚îÇ                                                                          ‚îÇ\n",
    "‚îÇ   CRITICAL: BF16!      Task: text-gen       ‚ö†Ô∏è INT4 only, NOT NF4!     ‚îÇ\n",
    "‚îÇ                                                                          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                    ‚îÇ\n",
    "                                    ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                        DEPLOYMENT PHASE (Static)                         ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                          ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n",
    "‚îÇ   ‚îÇ S3 Bucket    ‚îÇ    ‚îÇ React App         ‚îÇ    ‚îÇ User's Browser   ‚îÇ    ‚îÇ\n",
    "‚îÇ   ‚îÇ (Model Host) ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ (Vercel/Netlify)  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ (WebGPU/WASM)    ‚îÇ    ‚îÇ\n",
    "‚îÇ   ‚îÇ + CORS       ‚îÇ    ‚îÇ + COOP/COEP       ‚îÇ    ‚îÇ Cached Locally   ‚îÇ    ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n",
    "‚îÇ                                                                          ‚îÇ\n",
    "‚îÇ   Cost: ~$0.02/GB      Cost: Free tier       Cost: $0 (user's HW)      ‚îÇ\n",
    "‚îÇ                                                                          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Technology Stack\n",
    "\n",
    "### Training (DGX Spark)\n",
    "\n",
    "| Technology | Purpose | Why This Choice |\n",
    "|------------|---------|----------------|\n",
    "| **Gemma 3 1B** | Base model | Small enough for browser, smart enough for domain |\n",
    "| **Unsloth** | Training acceleration | 2x faster, memory efficient |\n",
    "| **QLoRA** | Fine-tuning method | Train in 4-bit, adapters in FP16 |\n",
    "| **MLflow** | Experiment tracking | Log metrics, compare runs |\n",
    "| **Datasets** | Data management | Standard HF format |\n",
    "\n",
    "### Optimization\n",
    "\n",
    "| Technology | Purpose | Why This Choice |\n",
    "|------------|---------|----------------|\n",
    "| **PEFT** | LoRA merging | Clean adapter integration |\n",
    "| **Optimum** | ONNX export | HF ecosystem compatibility |\n",
    "| **ONNX Runtime** | INT4 quantization | Browser-compatible format |\n",
    "\n",
    "### Deployment\n",
    "\n",
    "| Technology | Purpose | Why This Choice |\n",
    "|------------|---------|----------------|\n",
    "| **Transformers.js** | Browser inference | WebGPU + WASM support |\n",
    "| **React + Vite** | Web framework | Fast development, easy deployment |\n",
    "| **AWS S3** | Model hosting | Cheap, reliable, CORS support |\n",
    "| **Vercel/Netlify** | App hosting | Free tier, automatic HTTPS |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Check\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üçµ OPTION E: BROWSER-DEPLOYED FINE-TUNED LLM\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"\\nGPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'Not available'}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\" if torch.cuda.is_available() else \"\")\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "# Verify we have enough memory for this project\n",
    "if torch.cuda.is_available():\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    if total_memory >= 20:\n",
    "        print(f\"\\n‚úÖ Sufficient GPU memory for Option E!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Limited GPU memory - consider smaller model variants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check required packages\n",
    "import importlib\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    (\"transformers\", \">=4.50.0\"),\n",
    "    (\"datasets\", \"\"),\n",
    "    (\"peft\", \"\"),\n",
    "    (\"trl\", \"\"),\n",
    "    (\"accelerate\", \"\"),\n",
    "    (\"bitsandbytes\", \"\"),\n",
    "    (\"mlflow\", \"\"),\n",
    "    (\"optimum\", \"\"),\n",
    "    (\"onnx\", \"\"),\n",
    "    (\"onnxruntime\", \"\"),\n",
    "]\n",
    "\n",
    "print(\"üì¶ DEPENDENCY CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "missing = []\n",
    "for package, version in REQUIRED_PACKAGES:\n",
    "    try:\n",
    "        mod = importlib.import_module(package.replace(\"-\", \"_\"))\n",
    "        ver = getattr(mod, \"__version__\", \"unknown\")\n",
    "        print(f\"   ‚úÖ {package}: {ver}\")\n",
    "    except ImportError:\n",
    "        print(f\"   ‚ùå {package}: NOT INSTALLED\")\n",
    "        missing.append(package)\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\n‚ö†Ô∏è Install missing packages:\")\n",
    "    print(f\"   pip install {' '.join(missing)}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All required packages installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Memory Planning for DGX Spark\n",
    "\n",
    "With 128GB unified memory, this project is well within capacity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory planning for Option E\n",
    "\n",
    "def plan_option_e_memory():\n",
    "    \"\"\"\n",
    "    Plan memory usage for browser LLM pipeline.\n",
    "    \n",
    "    This function calculates expected memory usage for each phase\n",
    "    of the Option E capstone project on DGX Spark.\n",
    "    \"\"\"\n",
    "    DGX_SPARK_MEMORY = 128.0  # GB\n",
    "    \n",
    "    phases = {\n",
    "        \"Phase 2: QLoRA Training\": [\n",
    "            {\"name\": \"Gemma 3 1B (4-bit)\", \"memory_gb\": 2.0},\n",
    "            {\"name\": \"LoRA Adapters (FP16)\", \"memory_gb\": 0.5},\n",
    "            {\"name\": \"Optimizer States\", \"memory_gb\": 2.0},\n",
    "            {\"name\": \"Gradient Buffers\", \"memory_gb\": 1.5},\n",
    "            {\"name\": \"Training Data\", \"memory_gb\": 0.5},\n",
    "            {\"name\": \"Framework Overhead\", \"memory_gb\": 2.0},\n",
    "        ],\n",
    "        \"Phase 3: Merge (BF16)\": [\n",
    "            {\"name\": \"Base Model (BF16)\", \"memory_gb\": 2.5},\n",
    "            {\"name\": \"LoRA Adapters\", \"memory_gb\": 0.5},\n",
    "            {\"name\": \"Merged Model\", \"memory_gb\": 2.5},\n",
    "            {\"name\": \"Framework Overhead\", \"memory_gb\": 1.5},\n",
    "        ],\n",
    "        \"Phase 4: ONNX Export\": [\n",
    "            {\"name\": \"Merged Model (BF16)\", \"memory_gb\": 2.5},\n",
    "            {\"name\": \"ONNX Graph\", \"memory_gb\": 4.0},\n",
    "            {\"name\": \"Export Buffers\", \"memory_gb\": 2.0},\n",
    "            {\"name\": \"Framework Overhead\", \"memory_gb\": 1.5},\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    print(\"üíæ DGX SPARK MEMORY PLAN - OPTION E\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Available: {DGX_SPARK_MEMORY} GB unified memory\")\n",
    "    \n",
    "    for phase_name, components in phases.items():\n",
    "        print(f\"\\nüìä {phase_name}:\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        total = 0\n",
    "        for comp in components:\n",
    "            print(f\"   {comp['name']:<30} {comp['memory_gb']:>6.1f} GB\")\n",
    "            total += comp['memory_gb']\n",
    "        \n",
    "        print(\"-\"*70)\n",
    "        print(f\"   {'TOTAL':<30} {total:>6.1f} GB\")\n",
    "        print(f\"   {'REMAINING':<30} {DGX_SPARK_MEMORY - total:>6.1f} GB\")\n",
    "        \n",
    "        # Visual bar\n",
    "        pct = (total / DGX_SPARK_MEMORY) * 100\n",
    "        bar = \"‚ñà\" * int(pct / 2) + \"‚ñë\" * (50 - int(pct / 2))\n",
    "        print(f\"\\n   [{bar}] {pct:.0f}%\")\n",
    "        print(f\"   ‚úÖ Well within DGX Spark capacity!\")\n",
    "\n",
    "plan_option_e_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Critical Constraints\n",
    "\n",
    "### Browser Quantization Requirement\n",
    "\n",
    "**Browsers ONLY support INT4 quantization - NOT FP4, NOT NF4!**\n",
    "\n",
    "```\n",
    "Training (NF4)  ‚Üí  Merge (BF16)  ‚Üí  Export (FP32)  ‚Üí  Quantize (INT4)\n",
    "     ‚úÖ              CRITICAL!        Intermediate        Browser-Ready!\n",
    "                     Full precision    format              ‚úÖ\n",
    "                     for quality\n",
    "```\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "| Format | Training | Inference | Browser |\n",
    "|--------|----------|-----------|--------|\n",
    "| NF4 | ‚úÖ Great for QLoRA | ‚úÖ Works | ‚ùå Not supported |\n",
    "| FP4 | ‚ö†Ô∏è Less common | ‚úÖ Works | ‚ùå Not supported |\n",
    "| INT4 | ‚ùå Not for training | ‚úÖ Works | ‚úÖ **Required** |\n",
    "| INT8 | ‚ùå Not for training | ‚úÖ Works | ‚úÖ Supported |\n",
    "\n",
    "### The Pipeline\n",
    "\n",
    "1. **Train** in NF4 (memory efficient)\n",
    "2. **Merge** in BF16 (preserve quality)\n",
    "3. **Export** to ONNX FP32 (standard format)\n",
    "4. **Quantize** to INT4 (browser compatible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Deliverables Checklist\n",
    "\n",
    "By the end of this capstone, you will have:\n",
    "\n",
    "### 1. Model Artifacts\n",
    "- [ ] Training dataset (150-200 examples)\n",
    "- [ ] LoRA adapters (safetensors)\n",
    "- [ ] Merged model (BF16)\n",
    "- [ ] GGUF model (for Ollama testing)\n",
    "- [ ] ONNX INT4 model (for browser)\n",
    "\n",
    "### 2. Code Repository\n",
    "- [ ] Dataset preparation scripts\n",
    "- [ ] Training notebooks with MLflow\n",
    "- [ ] Merge and export scripts\n",
    "- [ ] Quantization pipeline\n",
    "- [ ] React web application\n",
    "\n",
    "### 3. Deployment\n",
    "- [ ] S3 bucket with CORS configured\n",
    "- [ ] Static site deployed (Vercel/Netlify)\n",
    "- [ ] Working chatbot demo\n",
    "\n",
    "### 4. Documentation\n",
    "- [ ] Technical report (15-20 pages)\n",
    "- [ ] Model card with training details\n",
    "- [ ] Presentation slides (15-20)\n",
    "- [ ] Demo video (5-10 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Grading Rubric (Self-Assessment)\n",
    "\n",
    "| Criteria | Points | Description |\n",
    "|----------|--------|-------------|\n",
    "| **Dataset Quality** | 15 | Diverse, well-formatted, domain-focused |\n",
    "| **Training Pipeline** | 20 | QLoRA config, MLflow tracking, reproducible |\n",
    "| **Optimization Pipeline** | 15 | Merge, ONNX, INT4 all working |\n",
    "| **Browser Integration** | 20 | WebGPU, streaming, error handling |\n",
    "| **Deployment** | 10 | S3 + static site + proper headers |\n",
    "| **Documentation** | 10 | Report, model card, presentation |\n",
    "| **Evaluation** | 5 | Benchmarks, quality comparison |\n",
    "| **Innovation** | 5 | Extra features, polish, creativity |\n",
    "| **Total** | **100** | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Pitfalls to Avoid\n",
    "\n",
    "### Pitfall 1: Merging in 4-bit\n",
    "```python\n",
    "# ‚ùå WRONG: Merging while model is still quantized\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"base-model\",\n",
    "    load_in_4bit=True  # DON'T DO THIS FOR MERGING!\n",
    ")\n",
    "\n",
    "# ‚úÖ RIGHT: Load in full precision for merging\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"base-model\",\n",
    "    torch_dtype=torch.bfloat16,  # Full precision\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "```\n",
    "\n",
    "### Pitfall 2: Wrong Quantization Format\n",
    "```python\n",
    "# ‚ùå WRONG: NF4 doesn't work in browsers\n",
    "BitsAndBytesConfig(bnb_4bit_quant_type=\"nf4\")  # Training only!\n",
    "\n",
    "# ‚úÖ RIGHT: INT4 for browser deployment\n",
    "quantize_dynamic(weight_type=QuantType.QInt4)  # Browser compatible!\n",
    "```\n",
    "\n",
    "### Pitfall 3: Missing CORS Headers\n",
    "```javascript\n",
    "// ‚ùå Browser will block model download without CORS\n",
    "// Error: CORS policy blocked...\n",
    "\n",
    "// ‚úÖ Configure S3 bucket CORS properly (see Phase 6)\n",
    "```\n",
    "\n",
    "### Pitfall 4: Missing Security Headers\n",
    "```javascript\n",
    "// ‚ùå SharedArrayBuffer not available without headers\n",
    "// Error: SharedArrayBuffer is not defined\n",
    "\n",
    "// ‚úÖ Add COOP/COEP headers in vercel.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Timeline Overview\n",
    "\n",
    "| Week | Phase | Activities |\n",
    "|------|-------|------------|\n",
    "| **35** | Planning | Review this overview, set up environment |\n",
    "| **36** | Phase 1-2 | Dataset preparation, QLoRA training |\n",
    "| **37** | Phase 3-4 | Merge adapters, ONNX quantization |\n",
    "| **38** | Phase 5 | Browser integration, React app |\n",
    "| **39** | Phase 6 | Deployment, testing, optimization |\n",
    "| **40** | Documentation | Report, model card, presentation, video |\n",
    "\n",
    "---\n",
    "\n",
    "## Ready to Start?\n",
    "\n",
    "Proceed to **Lab 4.6.8.1: Dataset Preparation** to begin building your training dataset!\n",
    "\n",
    "```\n",
    "labs/lab-4.6.8.1-dataset-preparation.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Ready to begin Option E!\")\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"   1. Review the architecture and technology stack\")\n",
    "print(\"   2. Verify all dependencies are installed\")\n",
    "print(\"   3. Proceed to Lab 4.6.8.1: Dataset Preparation\")\n",
    "print(\"\\n   Good luck with your capstone! üçµ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
