{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.6.8.2: QLoRA Fine-Tuning\n",
    "\n",
    "**Capstone Option E:** Browser-Deployed Fine-Tuned LLM (Matcha Expert)  \n",
    "**Phase:** 2 of 6  \n",
    "**Time:** 6-8 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## Phase Objectives\n",
    "\n",
    "By completing this phase, you will:\n",
    "- [ ] Configure QLoRA for efficient fine-tuning\n",
    "- [ ] Use Unsloth for 2x training speed\n",
    "- [ ] Track experiments with MLflow\n",
    "- [ ] Train LoRA adapters on matcha dataset\n",
    "- [ ] Evaluate training quality\n",
    "- [ ] Save adapters for merging\n",
    "\n",
    "---\n",
    "\n",
    "## Phase Checklist\n",
    "\n",
    "- [ ] Environment configured\n",
    "- [ ] Dataset loaded from Phase 1\n",
    "- [ ] Base model loaded in 4-bit\n",
    "- [ ] LoRA adapters configured\n",
    "- [ ] MLflow experiment created\n",
    "- [ ] Training completed\n",
    "- [ ] Adapters saved\n",
    "- [ ] Quality verified\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Matters\n",
    "\n",
    "**Fine-tuning transforms a general model into a domain expert.**\n",
    "\n",
    "| Before Fine-Tuning | After Fine-Tuning |\n",
    "|-------------------|-------------------|\n",
    "| Generic responses about tea | Specific matcha expertise |\n",
    "| May hallucinate details | Accurate domain knowledge |\n",
    "| Inconsistent style | Consistent expert persona |\n",
    "| \"I don't know\" on specifics | Detailed, authoritative answers |\n",
    "\n",
    "**DGX Spark Advantage:** With 128GB unified memory, we can fine-tune efficiently while keeping the full model accessible for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: What is QLoRA?\n",
    "\n",
    "> **Imagine teaching a master chef to specialize in Japanese cuisine.**\n",
    ">\n",
    "> **Regular training** would be like sending them back to culinary school for 4 years - relearning everything from scratch. Expensive and time-consuming.\n",
    ">\n",
    "> **LoRA (Low-Rank Adaptation)** is like giving them a specialized notebook where they write down just the Japanese-specific techniques. They keep all their existing skills, and just add the new knowledge.\n",
    ">\n",
    "> **QLoRA (Quantized LoRA)** is even smarter - it compresses the chef's existing knowledge (4-bit quantization) so it takes up less space, while the new notebook stays full quality. This means we can work with a much bigger chef (larger model) in the same kitchen (GPU memory).\n",
    ">\n",
    "> **Result:** We train only the small notebook (LoRA adapters, ~30MB) instead of the entire chef (base model, ~2GB), saving 90%+ of memory and time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import torch\n",
    "\n",
    "print(\"üçµ PHASE 2: QLORA FINE-TUNING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"\\nGPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'Not available'}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Project Configuration\nPROJECT_DIR = Path(\"./matcha-expert\")\nDATA_DIR = PROJECT_DIR / \"data\"\nMODEL_DIR = PROJECT_DIR / \"models\"\nMODEL_DIR.mkdir(parents=True, exist_ok=True)\n\n# Training Configuration\nCONFIG = {\n    # Model\n    \"base_model\": \"unsloth/gemma-3-270m-it\",  # 270M instruction-tuned Gemma 3\n    \"max_seq_length\": 2048,\n    \n    # LoRA\n    \"lora_r\": 16,             # Rank - higher = more capacity\n    \"lora_alpha\": 16,         # Scaling factor\n    \"lora_dropout\": 0,        # Dropout (0 for small datasets)\n    \"target_modules\": [       # Which layers to adapt\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\",\n    ],\n    \n    # Training\n    \"num_epochs\": 3,\n    \"batch_size\": 2,\n    \"gradient_accumulation_steps\": 4,  # Effective batch = 8\n    \"learning_rate\": 2e-4,\n    \"warmup_ratio\": 0.03,\n    \"weight_decay\": 0.01,\n    \n    # Paths\n    \"output_dir\": str(MODEL_DIR / \"matcha-lora\"),\n    \"dataset_path\": str(DATA_DIR / \"matcha-dataset\"),\n}\n\nprint(\"üìã TRAINING CONFIGURATION\")\nprint(\"=\"*70)\nfor key, value in CONFIG.items():\n    if isinstance(value, list):\n        print(f\"   {key}: [{len(value)} modules]\")\n    else:\n        print(f\"   {key}: {value}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory usage helper\n",
    "def log_memory(stage: str = \"\"):\n",
    "    \"\"\"\n",
    "    Log current GPU memory usage.\n",
    "    \n",
    "    Useful for tracking memory consumption at different stages\n",
    "    of the training pipeline on DGX Spark.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        print(f\"üíæ Memory [{stage}]: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "    else:\n",
    "        print(\"üíæ No GPU available\")\n",
    "\n",
    "log_memory(\"Initial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, Dataset\n",
    "\n",
    "# Load dataset from Phase 1\n",
    "dataset_path = Path(CONFIG[\"dataset_path\"])\n",
    "\n",
    "if dataset_path.exists():\n",
    "    dataset = load_from_disk(str(dataset_path))\n",
    "    print(f\"‚úÖ Loaded dataset from {dataset_path}\")\n",
    "    print(f\"   Train: {len(dataset['train'])} examples\")\n",
    "    print(f\"   Validation: {len(dataset['validation'])} examples\")\n",
    "    print(f\"   Test: {len(dataset['test'])} examples\")\n",
    "else:\n",
    "    print(f\"‚ùå Dataset not found at {dataset_path}\")\n",
    "    print(\"   Please complete Phase 1 first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview a training example\n",
    "if 'dataset' in dir():\n",
    "    sample = dataset['train'][0]\n",
    "    print(\"üìù SAMPLE TRAINING EXAMPLE\")\n",
    "    print(\"=\"*70)\n",
    "    for msg in sample['messages']:\n",
    "        role = msg['role'].upper()\n",
    "        content = msg['content'][:200] + \"...\" if len(msg['content']) > 200 else msg['content']\n",
    "        print(f\"\\n[{role}]\")\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Load Model with Unsloth\n",
    "\n",
    "Unsloth provides 2x faster training with 60% less memory through kernel optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with Unsloth for 2x speedup\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "print(\"üöÄ Loading model with Unsloth...\")\n",
    "print(f\"   Model: {CONFIG['base_model']}\")\n",
    "print(f\"   Max Sequence Length: {CONFIG['max_seq_length']}\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=CONFIG[\"base_model\"],\n",
    "    max_seq_length=CONFIG[\"max_seq_length\"],\n",
    "    load_in_4bit=True,      # Load in 4-bit for memory efficiency\n",
    "    dtype=torch.bfloat16,   # Use bfloat16 for DGX Spark\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Model loaded successfully!\")\n",
    "log_memory(\"After model load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapters\n",
    "\n",
    "print(\"üîß Adding LoRA adapters...\")\n",
    "print(f\"   Rank (r): {CONFIG['lora_r']}\")\n",
    "print(f\"   Alpha: {CONFIG['lora_alpha']}\")\n",
    "print(f\"   Target modules: {len(CONFIG['target_modules'])}\")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    target_modules=CONFIG[\"target_modules\"],\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Optimized checkpointing\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Count trainable parameters\n",
    "def count_parameters(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return trainable, total\n",
    "\n",
    "trainable, total = count_parameters(model)\n",
    "print(f\"\\n‚úÖ LoRA adapters added!\")\n",
    "print(f\"   Trainable parameters: {trainable:,} ({trainable/total*100:.2f}%)\")\n",
    "print(f\"   Total parameters: {total:,}\")\n",
    "log_memory(\"After LoRA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format dataset for training\n",
    "\n",
    "def format_chat_template(example):\n",
    "    \"\"\"\n",
    "    Format messages into the chat template expected by the model.\n",
    "    \n",
    "    This function applies the model's chat template to convert\n",
    "    the messages format into a training-ready text format.\n",
    "    \n",
    "    Args:\n",
    "        example: Dataset example with 'messages' field\n",
    "        \n",
    "    Returns:\n",
    "        Dict with 'text' field containing formatted conversation\n",
    "    \"\"\"\n",
    "    messages = example['messages']\n",
    "    \n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "    \n",
    "    return {\"text\": text}\n",
    "\n",
    "# Apply formatting\n",
    "print(\"üìù Formatting dataset for training...\")\n",
    "\n",
    "train_dataset = dataset['train'].map(\n",
    "    format_chat_template,\n",
    "    remove_columns=dataset['train'].column_names,\n",
    ")\n",
    "\n",
    "val_dataset = dataset['validation'].map(\n",
    "    format_chat_template,\n",
    "    remove_columns=dataset['validation'].column_names,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Formatting complete!\")\n",
    "print(f\"   Train examples: {len(train_dataset)}\")\n",
    "print(f\"   Validation examples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview formatted example\n",
    "print(\"üìã FORMATTED EXAMPLE\")\n",
    "print(\"=\"*70)\n",
    "sample_text = train_dataset[0]['text']\n",
    "print(sample_text[:1000] + \"...\" if len(sample_text) > 1000 else sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Configure MLflow Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow_dir = PROJECT_DIR / \"mlruns\"\n",
    "mlflow_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "mlflow.set_tracking_uri(f\"file://{mlflow_dir.absolute()}\")\n",
    "mlflow.set_experiment(\"matcha-expert-training\")\n",
    "\n",
    "print(f\"üìä MLflow configured\")\n",
    "print(f\"   Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"   Experiment: matcha-expert-training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MLflow run\n",
    "run_name = f\"train-{datetime.now().strftime('%Y%m%d-%H%M')}\"\n",
    "\n",
    "mlflow.start_run(run_name=run_name)\n",
    "\n",
    "# Log configuration\n",
    "mlflow.log_params({\n",
    "    \"base_model\": CONFIG[\"base_model\"],\n",
    "    \"lora_r\": CONFIG[\"lora_r\"],\n",
    "    \"lora_alpha\": CONFIG[\"lora_alpha\"],\n",
    "    \"num_epochs\": CONFIG[\"num_epochs\"],\n",
    "    \"batch_size\": CONFIG[\"batch_size\"],\n",
    "    \"learning_rate\": CONFIG[\"learning_rate\"],\n",
    "    \"train_examples\": len(train_dataset),\n",
    "    \"val_examples\": len(val_dataset),\n",
    "})\n",
    "\n",
    "print(f\"‚úÖ MLflow run started: {run_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Configure Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Training arguments\n",
    "training_args = SFTConfig(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    \n",
    "    # Batch size\n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    \n",
    "    # Training duration\n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    max_seq_length=CONFIG[\"max_seq_length\"],\n",
    "    \n",
    "    # Optimizer\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    optim=\"adamw_8bit\",  # Memory-efficient optimizer\n",
    "    \n",
    "    # Precision\n",
    "    bf16=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    \n",
    "    # Saving\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Other\n",
    "    seed=42,\n",
    "    report_to=[],  # We'll use MLflow manually\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=False,  # Don't pack sequences\n",
    ")\n",
    "\n",
    "print(\"üìã TRAINING ARGUMENTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   Precision: {'BF16' if training_args.bf16 else 'FP32'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer configured!\")\n",
    "log_memory(\"After trainer setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "print(\"üèãÔ∏è STARTING TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   Start time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"   Expected duration: ~15-30 minutes for {len(train_dataset)} examples\")\n",
    "print(\"\\n   Progress:\")\n",
    "\n",
    "# Train\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete!\")\n",
    "print(f\"   End time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"   Total steps: {train_result.global_step}\")\n",
    "print(f\"   Final loss: {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log training metrics to MLflow\n",
    "\n",
    "mlflow.log_metrics({\n",
    "    \"train_loss\": train_result.training_loss,\n",
    "    \"train_steps\": train_result.global_step,\n",
    "    \"train_runtime_seconds\": train_result.metrics.get(\"train_runtime\", 0),\n",
    "    \"train_samples_per_second\": train_result.metrics.get(\"train_samples_per_second\", 0),\n",
    "})\n",
    "\n",
    "print(\"üìä Metrics logged to MLflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"üìä Running evaluation...\")\n",
    "\n",
    "eval_result = trainer.evaluate()\n",
    "\n",
    "print(f\"\\nüìä EVALUATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "for key, value in eval_result.items():\n",
    "    print(f\"   {key}: {value:.4f}\" if isinstance(value, float) else f\"   {key}: {value}\")\n",
    "\n",
    "# Log to MLflow\n",
    "mlflow.log_metrics({f\"eval_{k}\": v for k, v in eval_result.items() if isinstance(v, (int, float))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Test the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def generate_response(question: str, max_tokens: int = 256) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response from the fine-tuned model.\n",
    "    \n",
    "    Args:\n",
    "        question: User question about matcha\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        \n",
    "    Returns:\n",
    "        Model's response\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a matcha tea expert with deep knowledge of Japanese tea culture, preparation methods, health benefits, and culinary applications.\"},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "print(\"‚úÖ Inference mode enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample questions\n",
    "\n",
    "TEST_QUESTIONS = [\n",
    "    \"What's the difference between ceremonial and culinary grade matcha?\",\n",
    "    \"How should I store my matcha to keep it fresh?\",\n",
    "    \"What's the correct water temperature for making matcha?\",\n",
    "]\n",
    "\n",
    "print(\"üß™ TESTING FINE-TUNED MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, question in enumerate(TEST_QUESTIONS, 1):\n",
    "    print(f\"\\n‚ùì Question {i}: {question}\")\n",
    "    print(f\"\\nüí¨ Response:\")\n",
    "    response = generate_response(question)\n",
    "    print(response)\n",
    "    print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Save LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters\n",
    "\n",
    "adapter_path = Path(CONFIG[\"output_dir\"]) / \"final\"\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(str(adapter_path))\n",
    "tokenizer.save_pretrained(str(adapter_path))\n",
    "\n",
    "# Calculate adapter size\n",
    "adapter_size = sum(f.stat().st_size for f in adapter_path.glob(\"*.safetensors\")) / 1e6\n",
    "\n",
    "print(f\"‚úÖ LoRA adapters saved!\")\n",
    "print(f\"   Path: {adapter_path}\")\n",
    "print(f\"   Size: {adapter_size:.1f} MB\")\n",
    "\n",
    "# List saved files\n",
    "print(f\"\\nüìÅ Saved files:\")\n",
    "for f in sorted(adapter_path.iterdir()):\n",
    "    size = f.stat().st_size / 1e6\n",
    "    print(f\"   {f.name}: {size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log artifacts to MLflow\n",
    "\n",
    "# Save config\n",
    "config_path = adapter_path / \"training_config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "# Log to MLflow\n",
    "mlflow.log_artifact(str(config_path))\n",
    "mlflow.log_metric(\"adapter_size_mb\", adapter_size)\n",
    "\n",
    "# End run\n",
    "mlflow.end_run()\n",
    "\n",
    "print(\"‚úÖ MLflow run completed and artifacts logged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Issues\n",
    "\n",
    "### Issue 1: CUDA Out of Memory\n",
    "**Symptom:** `RuntimeError: CUDA out of memory`  \n",
    "**Fix:** Reduce batch_size or max_seq_length\n",
    "\n",
    "### Issue 2: Loss Not Decreasing\n",
    "**Symptom:** Loss stays flat or increases  \n",
    "**Fix:** Check learning rate (try lower), check data format\n",
    "\n",
    "### Issue 3: Model Outputs Garbage\n",
    "**Symptom:** Random tokens, incomplete sentences  \n",
    "**Fix:** Check chat template formatting, ensure tokenizer matches model\n",
    "\n",
    "### Issue 4: Training Too Slow\n",
    "**Symptom:** Hours per epoch  \n",
    "**Fix:** Ensure Unsloth is being used, check GPU utilization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Metrics & Outputs\n",
    "\n",
    "| Metric | Expected | Actual |\n",
    "|--------|----------|--------|\n",
    "| Training Loss | < 1.0 | [Fill in] |\n",
    "| Eval Loss | < 1.5 | [Fill in] |\n",
    "| Training Time | 15-30 min | [Fill in] |\n",
    "| Adapter Size | ~20-50 MB | [Fill in] |\n",
    "| Peak Memory | ~8-12 GB | [Fill in] |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase Complete!\n",
    "\n",
    "You've achieved:\n",
    "- ‚úÖ Loaded model with Unsloth for 2x speedup\n",
    "- ‚úÖ Configured QLoRA adapters\n",
    "- ‚úÖ Trained on matcha dataset\n",
    "- ‚úÖ Tracked experiments with MLflow\n",
    "- ‚úÖ Saved LoRA adapters\n",
    "\n",
    "**Next:** [Lab 4.6.8.3: Merge and Export](./lab-4.6.8.3-merge-and-export.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "\n",
    "# Free GPU memory\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Phase 2 Complete!\")\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"   1. Review MLflow logs for training metrics\")\n",
    "print(\"   2. Test model responses for quality\")\n",
    "print(\"   3. Proceed to Lab 4.6.8.3 for LoRA merging\")\n",
    "print(f\"\\n   Adapters saved at: {adapter_path}\")\n",
    "\n",
    "log_memory(\"After cleanup\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}