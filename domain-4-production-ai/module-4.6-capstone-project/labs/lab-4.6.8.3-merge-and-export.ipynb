{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.6.8.3: Merge and Export\n",
    "\n",
    "**Capstone Option E:** Browser-Deployed Fine-Tuned LLM (Matcha Expert)  \n",
    "**Phase:** 3 of 6  \n",
    "**Time:** 3-4 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## Phase Objectives\n",
    "\n",
    "By completing this phase, you will:\n",
    "- [ ] Understand why merging requires full precision\n",
    "- [ ] Load base model in BF16 (not 4-bit!)\n",
    "- [ ] Merge LoRA adapters into base model\n",
    "- [ ] Verify merged model quality\n",
    "- [ ] Export to GGUF for Ollama testing\n",
    "- [ ] Save merged model for ONNX conversion\n",
    "\n",
    "---\n",
    "\n",
    "## Phase Checklist\n",
    "\n",
    "- [ ] Base model loaded in BF16\n",
    "- [ ] LoRA adapters loaded\n",
    "- [ ] Merge completed successfully\n",
    "- [ ] Quality verified (same output as LoRA model)\n",
    "- [ ] Merged model saved\n",
    "- [ ] GGUF exported for Ollama (optional)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Matters\n",
    "\n",
    "**This is the most critical step for browser deployment quality!**\n",
    "\n",
    "| Merge Method | Quality | Browser Works? |\n",
    "|--------------|---------|---------------|\n",
    "| Merge in 4-bit | ‚ùå Degraded | Maybe |\n",
    "| Merge in BF16 | ‚úÖ Full quality | ‚úÖ Yes |\n",
    "\n",
    "**The Rule:** Always merge LoRA adapters into a full-precision model, then quantize afterward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: Why Merge in Full Precision?\n",
    "\n",
    "> **Imagine you're mixing paint colors.**\n",
    ">\n",
    "> - **Your base color** (the model) is stored in a can\n",
    "> - **Your new tint** (LoRA adapters) is a small bottle of concentrated color\n",
    ">\n",
    "> **Merging in 4-bit** is like mixing with the can mostly dried up - the colors don't blend properly, and you get streaks and inconsistencies.\n",
    ">\n",
    "> **Merging in BF16 (full precision)** is like having a full, fresh can - the new tint mixes smoothly and evenly throughout.\n",
    ">\n",
    "> After mixing properly, you CAN compress the result (quantize to INT4) and it will still look great. But you can't compress first and then mix - the damage is already done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import torch\n",
    "\n",
    "print(\"üçµ PHASE 3: MERGE AND EXPORT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"\\nGPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'Not available'}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Project Configuration\nPROJECT_DIR = Path(\"./matcha-expert\")\nMODEL_DIR = PROJECT_DIR / \"models\"\n\n# Paths\nADAPTER_PATH = MODEL_DIR / \"matcha-lora\" / \"final\"\nMERGED_PATH = MODEL_DIR / \"matcha-merged\"\nGGUF_PATH = MODEL_DIR / \"matcha-gguf\"\n\n# Create directories\nMERGED_PATH.mkdir(parents=True, exist_ok=True)\nGGUF_PATH.mkdir(parents=True, exist_ok=True)\n\n# Base model (must match training)\nBASE_MODEL = \"google/gemma-3-270m-it\"\n\nprint(f\"üìÅ Paths:\")\nprint(f\"   Adapters: {ADAPTER_PATH}\")\nprint(f\"   Merged output: {MERGED_PATH}\")\nprint(f\"   GGUF output: {GGUF_PATH}\")\nprint(f\"   Base model: {BASE_MODEL}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory helper\n",
    "def log_memory(stage: str = \"\"):\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        print(f\"üíæ Memory [{stage}]: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "\n",
    "log_memory(\"Initial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Load Base Model in Full Precision\n",
    "\n",
    "**CRITICAL: Load in BF16, NOT 4-bit!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"üîÑ Loading base model in BF16 (full precision)...\")\n",
    "print(\"   This is CRITICAL for quality - do NOT load in 4-bit!\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# Load model in BF16 - NOT quantized!\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,  # Full precision!\n",
    "    device_map=\"auto\",\n",
    "    # NO quantization_config here!\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Base model loaded in BF16\")\n",
    "print(f\"   Model dtype: {base_model.dtype}\")\n",
    "log_memory(\"After base model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Load and Merge LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "print(f\"üîß Loading LoRA adapters from {ADAPTER_PATH}...\")\n",
    "\n",
    "# Check adapters exist\n",
    "if not ADAPTER_PATH.exists():\n",
    "    print(f\"‚ùå Adapters not found at {ADAPTER_PATH}\")\n",
    "    print(\"   Please complete Phase 2 first!\")\n",
    "else:\n",
    "    # Load LoRA adapters onto base model\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        str(ADAPTER_PATH),\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ LoRA adapters loaded\")\n",
    "    log_memory(\"After LoRA load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge adapters into base model\n",
    "\n",
    "print(\"üîÄ Merging LoRA adapters into base model...\")\n",
    "print(\"   This creates a single merged model with the fine-tuned weights\")\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "print(f\"\\n‚úÖ Merge complete!\")\n",
    "print(f\"   Model type: {type(merged_model).__name__}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in merged_model.parameters()):,}\")\n",
    "log_memory(\"After merge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Verify Merged Model Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function for merged model\n",
    "\n",
    "def generate_response(model, tokenizer, question: str, max_tokens: int = 256) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response from the model.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to use for generation\n",
    "        tokenizer: The tokenizer\n",
    "        question: User question\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        \n",
    "    Returns:\n",
    "        Generated response text\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a matcha tea expert with deep knowledge of Japanese tea culture, preparation methods, health benefits, and culinary applications.\"},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "print(\"‚úÖ Generation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test merged model\n",
    "\n",
    "TEST_QUESTIONS = [\n",
    "    \"What's the difference between ceremonial and culinary grade matcha?\",\n",
    "    \"What's the best water temperature for making matcha?\",\n",
    "    \"How should I store matcha?\",\n",
    "]\n",
    "\n",
    "print(\"üß™ TESTING MERGED MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, question in enumerate(TEST_QUESTIONS, 1):\n",
    "    print(f\"\\n‚ùì Question {i}: {question}\")\n",
    "    print(f\"\\nüí¨ Response:\")\n",
    "    response = generate_response(merged_model, tokenizer, question)\n",
    "    print(response[:500] + \"...\" if len(response) > 500 else response)\n",
    "    print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Save Merged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged model\n",
    "\n",
    "print(f\"üíæ Saving merged model to {MERGED_PATH}...\")\n",
    "\n",
    "merged_model.save_pretrained(\n",
    "    str(MERGED_PATH),\n",
    "    safe_serialization=True,  # Use safetensors format\n",
    ")\n",
    "tokenizer.save_pretrained(str(MERGED_PATH))\n",
    "\n",
    "# Calculate size\n",
    "model_size = sum(f.stat().st_size for f in MERGED_PATH.glob(\"*.safetensors\")) / 1e9\n",
    "\n",
    "print(f\"\\n‚úÖ Merged model saved!\")\n",
    "print(f\"   Path: {MERGED_PATH}\")\n",
    "print(f\"   Size: {model_size:.2f} GB\")\n",
    "\n",
    "# List files\n",
    "print(f\"\\nüìÅ Saved files:\")\n",
    "for f in sorted(MERGED_PATH.iterdir()):\n",
    "    size = f.stat().st_size / 1e6\n",
    "    print(f\"   {f.name}: {size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Export to GGUF for Ollama (Optional)\n",
    "\n",
    "GGUF format allows you to test the model locally with Ollama before browser deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to GGUF using llama.cpp\n",
    "# This requires llama.cpp to be installed\n",
    "\n",
    "print(\"üì¶ GGUF EXPORT (Optional)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"To export to GGUF format for Ollama testing:\n",
    "\n",
    "1. Install llama.cpp:\n",
    "   git clone https://github.com/ggerganov/llama.cpp\n",
    "   cd llama.cpp\n",
    "   make\n",
    "\n",
    "2. Convert to GGUF:\n",
    "   python convert-hf-to-gguf.py /path/to/matcha-merged --outfile matcha-expert.gguf\n",
    "\n",
    "3. Quantize (optional, for smaller size):\n",
    "   ./quantize matcha-expert.gguf matcha-expert-q4.gguf Q4_K_M\n",
    "\n",
    "4. Create Ollama Modelfile:\n",
    "   FROM ./matcha-expert-q4.gguf\n",
    "   SYSTEM \"You are a matcha tea expert...\"\n",
    "\n",
    "5. Register with Ollama:\n",
    "   ollama create matcha-expert -f Modelfile\n",
    "\n",
    "6. Test:\n",
    "   ollama run matcha-expert \"What is ceremonial grade matcha?\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Ollama Modelfile template\n",
    "\n",
    "modelfile_content = '''FROM ./matcha-expert-q4.gguf\n",
    "\n",
    "TEMPLATE \"\"\"<start_of_turn>system\n",
    "{{ .System }}<end_of_turn>\n",
    "<start_of_turn>user\n",
    "{{ .Prompt }}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM \"\"\"You are a matcha tea expert with deep knowledge of Japanese tea culture, preparation methods, health benefits, and culinary applications. You provide accurate, helpful information about matcha grades, brewing techniques, traditional ceremonies, and modern recipes.\"\"\"\n",
    "\n",
    "PARAMETER temperature 0.7\n",
    "PARAMETER top_p 0.9\n",
    "PARAMETER num_predict 256\n",
    "'''\n",
    "\n",
    "modelfile_path = GGUF_PATH / \"Modelfile\"\n",
    "with open(modelfile_path, 'w') as f:\n",
    "    f.write(modelfile_content)\n",
    "\n",
    "print(f\"‚úÖ Ollama Modelfile saved to {modelfile_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Issues\n",
    "\n",
    "### Issue 1: Merged Model Quality Degraded\n",
    "**Symptom:** Output is worse than LoRA model  \n",
    "**Cause:** Model was loaded in 4-bit for merging  \n",
    "**Fix:** Reload base model in BF16/FP16 without quantization\n",
    "\n",
    "### Issue 2: CUDA Out of Memory During Merge\n",
    "**Symptom:** OOM when loading full model  \n",
    "**Fix:** Use `device_map=\"auto\"` to spread across GPU/CPU\n",
    "\n",
    "### Issue 3: Tokenizer Mismatch\n",
    "**Symptom:** Output is garbled or wrong  \n",
    "**Fix:** Ensure tokenizer comes from base model, not adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Metrics & Outputs\n",
    "\n",
    "| Metric | Expected | Actual |\n",
    "|--------|----------|--------|\n",
    "| Merged Model Size | ~2 GB | [Fill in] |\n",
    "| Quality Match | Same as LoRA | [Fill in] |\n",
    "| Merge Time | ~1-2 min | [Fill in] |\n",
    "| Peak Memory | ~6-8 GB | [Fill in] |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase Complete!\n",
    "\n",
    "You've achieved:\n",
    "- ‚úÖ Loaded base model in full precision (BF16)\n",
    "- ‚úÖ Merged LoRA adapters into base model\n",
    "- ‚úÖ Verified merged model quality\n",
    "- ‚úÖ Saved merged model for ONNX conversion\n",
    "- ‚úÖ Created Ollama Modelfile template\n",
    "\n",
    "**Next:** [Lab 4.6.8.4: ONNX Quantization](./lab-4.6.8.4-onnx-quantization.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "\n",
    "del merged_model\n",
    "del base_model\n",
    "if 'model' in dir():\n",
    "    del model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Phase 3 Complete!\")\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"   1. Verify merged model produces correct outputs\")\n",
    "print(\"   2. (Optional) Test with Ollama using GGUF export\")\n",
    "print(\"   3. Proceed to Lab 4.6.8.4 for ONNX conversion\")\n",
    "print(f\"\\n   Merged model at: {MERGED_PATH}\")\n",
    "\n",
    "log_memory(\"After cleanup\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}