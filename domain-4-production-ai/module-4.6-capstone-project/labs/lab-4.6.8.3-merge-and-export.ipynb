{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 4.6.8.3: Merge and Export\n\n**Capstone Option E:** Browser-Deployed Fine-Tuned LLM (Troscha Matcha Guide)  \n**Phase:** 3 of 6  \n**Time:** 3-4 hours  \n**Difficulty:** ‚≠ê‚≠ê‚≠ê\n\n---\n\n## Phase Objectives\n\nBy completing this phase, you will:\n- [ ] Understand why merging requires full precision\n- [ ] Load base model in BF16 (not 4-bit!)\n- [ ] Merge LoRA adapters into base model\n- [ ] Verify merged model quality (including `<preferences>` JSON output)\n- [ ] Export to GGUF for Ollama testing\n- [ ] Save merged model for ONNX conversion\n\n---\n\n## Phase Checklist\n\n- [ ] Base model loaded in BF16\n- [ ] LoRA adapters loaded\n- [ ] Merge completed successfully\n- [ ] Quality verified (output matches LoRA model with `<preferences>` JSON)\n- [ ] Merged model saved\n- [ ] GGUF exported for Ollama (optional)\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Matters\n",
    "\n",
    "**This is the most critical step for browser deployment quality!**\n",
    "\n",
    "| Merge Method | Quality | Browser Works? |\n",
    "|--------------|---------|---------------|\n",
    "| Merge in 4-bit | ‚ùå Degraded | Maybe |\n",
    "| Merge in BF16 | ‚úÖ Full quality | ‚úÖ Yes |\n",
    "\n",
    "**The Rule:** Always merge LoRA adapters into a full-precision model, then quantize afterward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: Why Merge in Full Precision?\n",
    "\n",
    "> **Imagine you're mixing paint colors.**\n",
    ">\n",
    "> - **Your base color** (the model) is stored in a can\n",
    "> - **Your new tint** (LoRA adapters) is a small bottle of concentrated color\n",
    ">\n",
    "> **Merging in 4-bit** is like mixing with the can mostly dried up - the colors don't blend properly, and you get streaks and inconsistencies.\n",
    ">\n",
    "> **Merging in BF16 (full precision)** is like having a full, fresh can - the new tint mixes smoothly and evenly throughout.\n",
    ">\n",
    "> After mixing properly, you CAN compress the result (quantize to INT4) and it will still look great. But you can't compress first and then mix - the damage is already done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import torch\n",
    "\n",
    "print(\"üçµ PHASE 3: MERGE AND EXPORT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"\\nGPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'Not available'}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Project Configuration\nPROJECT_DIR = Path(\"./troscha-matcha\")\nMODEL_DIR = PROJECT_DIR / \"models\"\n\n# Paths\nADAPTER_PATH = MODEL_DIR / \"troscha-lora\" / \"final\"\nMERGED_PATH = MODEL_DIR / \"troscha-merged\"\nGGUF_PATH = MODEL_DIR / \"troscha-gguf\"\n\n# Create directories\nMERGED_PATH.mkdir(parents=True, exist_ok=True)\nGGUF_PATH.mkdir(parents=True, exist_ok=True)\n\n# Base model for merging\n# NOTE: We use the underlying google/gemma model, NOT unsloth/gemma\n# Unsloth wraps the base model for fast training, but the actual model\n# weights come from google/gemma-3-270m-it. For merging, we need the\n# original HuggingFace model format.\nBASE_MODEL = \"unsloth/gemma-3-270m-it\"\n\nprint(f\"üìÅ Paths:\")\nprint(f\"   Adapters: {ADAPTER_PATH}\")\nprint(f\"   Merged output: {MERGED_PATH}\")\nprint(f\"   GGUF output: {GGUF_PATH}\")\nprint(f\"   Base model: {BASE_MODEL}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory helper\n",
    "def log_memory(stage: str = \"\"):\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        print(f\"üíæ Memory [{stage}]: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "\n",
    "log_memory(\"Initial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Load Base Model in Full Precision\n",
    "\n",
    "**CRITICAL: Load in BF16, NOT 4-bit!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"üîÑ Loading base model in BF16 (full precision)...\")\n",
    "print(\"   This is CRITICAL for quality - do NOT load in 4-bit!\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# Load model in BF16 - NOT quantized!\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,  # Full precision!\n",
    "    device_map=\"auto\",\n",
    "    # NO quantization_config here!\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Base model loaded in BF16\")\n",
    "print(f\"   Model dtype: {base_model.dtype}\")\n",
    "log_memory(\"After base model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Load and Merge LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "print(f\"üîß Loading LoRA adapters from {ADAPTER_PATH}...\")\n",
    "\n",
    "# Check adapters exist\n",
    "if not ADAPTER_PATH.exists():\n",
    "    print(f\"‚ùå Adapters not found at {ADAPTER_PATH}\")\n",
    "    print(\"   Please complete Phase 2 first!\")\n",
    "else:\n",
    "    # Load LoRA adapters onto base model\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        str(ADAPTER_PATH),\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ LoRA adapters loaded\")\n",
    "    log_memory(\"After LoRA load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge adapters into base model\n",
    "\n",
    "print(\"üîÄ Merging LoRA adapters into base model...\")\n",
    "print(\"   This creates a single merged model with the fine-tuned weights\")\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "print(f\"\\n‚úÖ Merge complete!\")\n",
    "print(f\"   Model type: {type(merged_model).__name__}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in merged_model.parameters()):,}\")\n",
    "log_memory(\"After merge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Verify Merged Model Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Troscha system prompt (must match training data!)\nSYSTEM_PROMPT = \"\"\"You are Troscha's matcha guide.\n\nMENU:\n- Yura: Latte Rp 27k\n- Taku: Straight Rp 25k | Latte Rp 32k | Strawberry Rp 40k\n- Firu: Straight Rp 34k | Latte Rp 44k | Miruku Rp 49k | Strawberry Rp 52k\n- Giru: Straight Rp 39k | Latte Rp 49k | Miruku Rp 54k | Strawberry Rp 57k\n- Zeno: Straight Rp 44k | Latte Rp 54k | Miruku Rp 59k | Strawberry Rp 62k\n- Moku: Hojicha Latte Rp 35k\n- Hiku: Straight Rp 79k | Latte Rp 89k\n- Kiyo: Straight Rp 94k | Latte Rp 104k\n\nADDON: Oat Milk +Rp 5k\n\nEnd responses with <preferences> JSON.\"\"\"\n\ndef generate_response(model, tokenizer, question: str, max_tokens: int = 256) -> str:\n    \"\"\"\n    Generate a response from the Troscha model.\n    \n    Args:\n        model: The model to use for generation\n        tokenizer: The tokenizer\n        question: User question about Troscha products\n        max_tokens: Maximum tokens to generate\n        \n    Returns:\n        Generated response text with <preferences> JSON\n    \"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": question},\n    ]\n    \n    inputs = tokenizer.apply_chat_template(\n        messages,\n        tokenize=True,\n        add_generation_prompt=True,\n        return_tensors=\"pt\",\n    ).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            inputs,\n            max_new_tokens=max_tokens,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True,\n            pad_token_id=tokenizer.pad_token_id,\n        )\n    \n    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n    return response.strip()\n\nprint(\"‚úÖ Generation function defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test merged model with Troscha-specific questions\n\nTEST_QUESTIONS = [\n    \"What's the difference between Firu and Giru?\",\n    \"I want something not too bitter but affordable. What do you recommend?\",\n    \"What's your most premium matcha option?\",\n]\n\nprint(\"üß™ TESTING MERGED TROSCHA MODEL\")\nprint(\"=\"*70)\nprint(\"Expected: Each response should end with <preferences> JSON\")\n\nfor i, question in enumerate(TEST_QUESTIONS, 1):\n    print(f\"\\n‚ùì Question {i}: {question}\")\n    print(f\"\\nüí¨ Response:\")\n    response = generate_response(merged_model, tokenizer, question)\n    print(response[:500] + \"...\" if len(response) > 500 else response)\n    \n    # Check for preferences JSON\n    if \"<preferences>\" in response:\n        print(\"\\n‚úÖ Contains <preferences> JSON\")\n    else:\n        print(\"\\n‚ö†Ô∏è Missing <preferences> JSON\")\n    print(\"-\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Save Merged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged model\n",
    "\n",
    "print(f\"üíæ Saving merged model to {MERGED_PATH}...\")\n",
    "\n",
    "merged_model.save_pretrained(\n",
    "    str(MERGED_PATH),\n",
    "    safe_serialization=True,  # Use safetensors format\n",
    ")\n",
    "tokenizer.save_pretrained(str(MERGED_PATH))\n",
    "\n",
    "# Calculate size\n",
    "model_size = sum(f.stat().st_size for f in MERGED_PATH.glob(\"*.safetensors\")) / 1e9\n",
    "\n",
    "print(f\"\\n‚úÖ Merged model saved!\")\n",
    "print(f\"   Path: {MERGED_PATH}\")\n",
    "print(f\"   Size: {model_size:.2f} GB\")\n",
    "\n",
    "# List files\n",
    "print(f\"\\nüìÅ Saved files:\")\n",
    "for f in sorted(MERGED_PATH.iterdir()):\n",
    "    size = f.stat().st_size / 1e6\n",
    "    print(f\"   {f.name}: {size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Export to GGUF for Ollama (Optional)\n",
    "\n",
    "GGUF format allows you to test the model locally with Ollama before browser deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export to GGUF using llama.cpp\n# This requires llama.cpp to be installed\n\nprint(\"üì¶ GGUF EXPORT (Optional)\")\nprint(\"=\"*70)\nprint(\"\"\"To export to GGUF format for Ollama testing:\n\n1. Install llama.cpp:\n   git clone https://github.com/ggerganov/llama.cpp\n   cd llama.cpp\n   make\n\n2. Convert to GGUF:\n   python convert-hf-to-gguf.py /path/to/troscha-merged --outfile troscha-matcha.gguf\n\n3. Quantize (optional, for smaller size):\n   ./quantize troscha-matcha.gguf troscha-matcha-q4.gguf Q4_K_M\n\n4. Create Ollama Modelfile:\n   FROM ./troscha-matcha-q4.gguf\n   SYSTEM \"You are Troscha's matcha guide...\"\n\n5. Register with Ollama:\n   ollama create troscha-matcha -f Modelfile\n\n6. Test:\n   ollama run troscha-matcha \"What's the difference between Firu and Giru?\"\n\"\"\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save Ollama Modelfile template\n\nmodelfile_content = '''FROM ./troscha-matcha-q4.gguf\n\nTEMPLATE \"\"\"<start_of_turn>system\n{{ .System }}<end_of_turn>\n<start_of_turn>user\n{{ .Prompt }}<end_of_turn>\n<start_of_turn>model\n\"\"\"\n\nSYSTEM \"\"\"You are Troscha's matcha guide.\n\nMENU:\n- Yura: Latte Rp 27k\n- Taku: Straight Rp 25k | Latte Rp 32k | Strawberry Rp 40k\n- Firu: Straight Rp 34k | Latte Rp 44k | Miruku Rp 49k | Strawberry Rp 52k\n- Giru: Straight Rp 39k | Latte Rp 49k | Miruku Rp 54k | Strawberry Rp 57k\n- Zeno: Straight Rp 44k | Latte Rp 54k | Miruku Rp 59k | Strawberry Rp 62k\n- Moku: Hojicha Latte Rp 35k\n- Hiku: Straight Rp 79k | Latte Rp 89k\n- Kiyo: Straight Rp 94k | Latte Rp 104k\n\nADDON: Oat Milk +Rp 5k\n\nEnd responses with <preferences> JSON.\"\"\"\n\nPARAMETER temperature 0.7\nPARAMETER top_p 0.9\nPARAMETER num_predict 256\n'''\n\nmodelfile_path = GGUF_PATH / \"Modelfile\"\nwith open(modelfile_path, 'w') as f:\n    f.write(modelfile_content)\n\nprint(f\"‚úÖ Ollama Modelfile saved to {modelfile_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Issues\n",
    "\n",
    "### Issue 1: Merged Model Quality Degraded\n",
    "**Symptom:** Output is worse than LoRA model  \n",
    "**Cause:** Model was loaded in 4-bit for merging  \n",
    "**Fix:** Reload base model in BF16/FP16 without quantization\n",
    "\n",
    "### Issue 2: CUDA Out of Memory During Merge\n",
    "**Symptom:** OOM when loading full model  \n",
    "**Fix:** Use `device_map=\"auto\"` to spread across GPU/CPU\n",
    "\n",
    "### Issue 3: Tokenizer Mismatch\n",
    "**Symptom:** Output is garbled or wrong  \n",
    "**Fix:** Ensure tokenizer comes from base model, not adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Metrics & Outputs\n\n| Metric | Expected | Actual |\n|--------|----------|--------|\n| Merged Model Size | ~0.5-1 GB | [Your value] |\n| Quality Match | Same as LoRA | [Your value] |\n| Merge Time | ~1-2 min | [Your value] |\n| Peak Memory | ~4-6 GB | [Your value] |\n\n**Note:** Gemma 3 270M is a smaller model, so expect ~0.5-1GB merged size."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase Complete!\n",
    "\n",
    "You've achieved:\n",
    "- ‚úÖ Loaded base model in full precision (BF16)\n",
    "- ‚úÖ Merged LoRA adapters into base model\n",
    "- ‚úÖ Verified merged model quality\n",
    "- ‚úÖ Saved merged model for ONNX conversion\n",
    "- ‚úÖ Created Ollama Modelfile template\n",
    "\n",
    "**Next:** [Lab 4.6.8.4: ONNX Quantization](./lab-4.6.8.4-onnx-quantization.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "\n",
    "del merged_model\n",
    "del base_model\n",
    "if 'model' in dir():\n",
    "    del model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Phase 3 Complete!\")\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"   1. Verify merged model produces correct outputs\")\n",
    "print(\"   2. (Optional) Test with Ollama using GGUF export\")\n",
    "print(\"   3. Proceed to Lab 4.6.8.4 for ONNX conversion\")\n",
    "print(f\"\\n   Merged model at: {MERGED_PATH}\")\n",
    "\n",
    "log_memory(\"After cleanup\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}