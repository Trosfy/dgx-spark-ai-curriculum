{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 4.6.8.4: ONNX Conversion & INT4 Quantization\n\n**Capstone Option E:** Browser-Deployed Fine-Tuned LLM (Troscha Matcha Guide)  \n**Phase:** 4 of 6  \n**Time:** 4-6 hours  \n**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê\n\n---\n\n## Phase Objectives\n\nBy completing this phase, you will:\n- [ ] Export merged model to ONNX format\n- [ ] Apply INT4 quantization (browser-compatible!)\n- [ ] Verify quantized model quality\n- [ ] Compare file sizes at each stage\n- [ ] Prepare model files for browser deployment\n\n---\n\n## Phase Checklist\n\n- [ ] Merged model loaded\n- [ ] ONNX export completed\n- [ ] INT4 quantization applied\n- [ ] Quality verified\n- [ ] Tokenizer files prepared\n- [ ] Model ready for browser\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Matters\n",
    "\n",
    "**ONNX + INT4 is the key to browser deployment!**\n",
    "\n",
    "| Format | File Size | Browser Support |\n",
    "|--------|-----------|----------------|\n",
    "| PyTorch BF16 | ~2 GB | ‚ùå No |\n",
    "| ONNX FP32 | ~4 GB | ‚ö†Ô∏è Too big |\n",
    "| ONNX INT4 | ~500 MB | ‚úÖ Yes! |\n",
    "\n",
    "**Critical:** Browsers ONLY support INT4 quantization, not NF4 or FP4!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: What is INT4 Quantization?\n",
    "\n",
    "> **Imagine you're packing a suitcase for vacation.**\n",
    ">\n",
    "> - **Original (FP32):** You bring your entire wardrobe in full-size - takes 4 suitcases\n",
    "> - **BF16:** You fold things better - 2 suitcases\n",
    "> - **INT4:** You roll everything tight and use vacuum bags - 0.5 suitcases!\n",
    ">\n",
    "> The clothes still work perfectly, they're just compressed for travel.\n",
    ">\n",
    "> **How INT4 works:**\n",
    "> - Original weights: 16 or 32 bits per number (very precise)\n",
    "> - INT4: Only 4 bits per number (16 possible values)\n",
    "> - The model learns to work with this reduced precision\n",
    "> - Result: 75% smaller file, ~5% quality loss (acceptable!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import torch\n",
    "import shutil\n",
    "\n",
    "print(\"üçµ PHASE 4: ONNX CONVERSION & INT4 QUANTIZATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"\\nGPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'Not available'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Project Configuration\nPROJECT_DIR = Path(\"./troscha-matcha\")\nMODEL_DIR = PROJECT_DIR / \"models\"\n\n# Paths\nMERGED_PATH = MODEL_DIR / \"troscha-merged\"\nONNX_PATH = MODEL_DIR / \"troscha-onnx\"\nONNX_INT4_PATH = MODEL_DIR / \"troscha-onnx-int4\"\nBROWSER_PATH = MODEL_DIR / \"troscha-browser\"\n\n# Create directories\nfor path in [ONNX_PATH, ONNX_INT4_PATH, BROWSER_PATH]:\n    path.mkdir(parents=True, exist_ok=True)\n\nprint(f\"üìÅ Paths:\")\nprint(f\"   Merged model: {MERGED_PATH}\")\nprint(f\"   ONNX output: {ONNX_PATH}\")\nprint(f\"   ONNX INT4: {ONNX_INT4_PATH}\")\nprint(f\"   Browser files: {BROWSER_PATH}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check merged model exists\n",
    "if not MERGED_PATH.exists():\n",
    "    print(f\"‚ùå Merged model not found at {MERGED_PATH}\")\n",
    "    print(\"   Please complete Phase 3 first!\")\n",
    "else:\n",
    "    # Show merged model size\n",
    "    merged_size = sum(f.stat().st_size for f in MERGED_PATH.glob(\"*.safetensors\")) / 1e9\n",
    "    print(f\"‚úÖ Merged model found: {merged_size:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from optimum.exporters.onnx import main_export\nfrom optimum.onnxruntime import ORTModelForCausalLM\n\nprint(\"üì¶ Exporting to ONNX format...\")\nprint(\"   This may take 5-10 minutes...\")\n\ntry:\n    # Export to ONNX\n    # Using optimum library for HuggingFace model export\n    main_export(\n        model_name_or_path=str(MERGED_PATH),\n        output=str(ONNX_PATH),\n        task=\"text-generation-with-past\",  # Use KV cache for faster inference\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n        fp16=False,  # Export as FP32 first, then quantize\n    )\n\n    # Calculate size\n    onnx_size = sum(f.stat().st_size for f in ONNX_PATH.glob(\"*.onnx\")) / 1e9\n\n    print(f\"\\n‚úÖ ONNX export complete!\")\n    print(f\"   Path: {ONNX_PATH}\")\n    print(f\"   Size: {onnx_size:.2f} GB\")\n\n    # List files\n    print(f\"\\nüìÅ ONNX files:\")\n    for f in sorted(ONNX_PATH.iterdir()):\n        if f.is_file():\n            size = f.stat().st_size / 1e6\n            print(f\"   {f.name}: {size:.1f} MB\")\n\nexcept Exception as e:\n    print(f\"\\n‚ùå ONNX export failed: {e}\")\n    print(\"\\nüîß Troubleshooting:\")\n    print(\"   1. Ensure merged model exists and is valid\")\n    print(\"   2. Check optimum version: pip install -U optimum[exporters]\")\n    print(\"   3. Verify sufficient GPU memory (clear with torch.cuda.empty_cache())\")\n    print(\"   4. For unsupported ops, try: pip install onnx onnxruntime-gpu\")\n    print(f\"\\n   Full error: {type(e).__name__}: {e}\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Apply INT4 Quantization\n",
    "\n",
    "**This is the critical step for browser compatibility!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import onnx\n",
    "\n",
    "print(\"üîß Applying INT4 quantization...\")\n",
    "print(\"   CRITICAL: Browser ONLY supports INT4, not NF4 or FP4!\")\n",
    "\n",
    "# Find the main model file\n",
    "onnx_files = list(ONNX_PATH.glob(\"*.onnx\"))\n",
    "model_file = None\n",
    "for f in onnx_files:\n",
    "    if \"model\" in f.name.lower():\n",
    "        model_file = f\n",
    "        break\n",
    "\n",
    "if model_file is None and onnx_files:\n",
    "    model_file = onnx_files[0]\n",
    "\n",
    "if model_file:\n",
    "    print(f\"   Source: {model_file.name}\")\n",
    "    \n",
    "    output_file = ONNX_INT4_PATH / \"model_quantized.onnx\"\n",
    "    \n",
    "    # Apply INT4 quantization\n",
    "    quantize_dynamic(\n",
    "        model_input=str(model_file),\n",
    "        model_output=str(output_file),\n",
    "        weight_type=QuantType.QInt4,  # INT4 for browser!\n",
    "        per_channel=True,  # Better quality\n",
    "        reduce_range=False,\n",
    "    )\n",
    "    \n",
    "    # Calculate sizes\n",
    "    int4_size = output_file.stat().st_size / 1e6\n",
    "    original_size = model_file.stat().st_size / 1e6\n",
    "    compression = (1 - int4_size / original_size) * 100\n",
    "    \n",
    "    print(f\"\\n‚úÖ INT4 quantization complete!\")\n",
    "    print(f\"   Output: {output_file}\")\n",
    "    print(f\"   Size: {int4_size:.1f} MB\")\n",
    "    print(f\"   Compression: {compression:.1f}% reduction\")\n",
    "else:\n",
    "    print(\"‚ùå No ONNX model file found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy tokenizer files for browser\n",
    "print(\"üìã Preparing tokenizer files for browser...\")\n",
    "\n",
    "tokenizer_files = [\n",
    "    \"tokenizer.json\",\n",
    "    \"tokenizer_config.json\", \n",
    "    \"special_tokens_map.json\",\n",
    "    \"config.json\",\n",
    "]\n",
    "\n",
    "for fname in tokenizer_files:\n",
    "    src = MERGED_PATH / fname\n",
    "    if src.exists():\n",
    "        dst = ONNX_INT4_PATH / fname\n",
    "        shutil.copy(src, dst)\n",
    "        print(f\"   ‚úÖ Copied {fname}\")\n",
    "    else:\n",
    "        # Try ONNX path\n",
    "        src = ONNX_PATH / fname\n",
    "        if src.exists():\n",
    "            dst = ONNX_INT4_PATH / fname\n",
    "            shutil.copy(src, dst)\n",
    "            print(f\"   ‚úÖ Copied {fname}\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è Not found: {fname}\")\n",
    "\n",
    "print(\"\\n‚úÖ Tokenizer files prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Verify Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "print(\"üß™ Testing quantized ONNX model...\")\n",
    "\n",
    "# Load the quantized model\n",
    "quantized_model_path = ONNX_INT4_PATH / \"model_quantized.onnx\"\n",
    "\n",
    "if quantized_model_path.exists():\n",
    "    # Create ONNX Runtime session\n",
    "    session_options = ort.SessionOptions()\n",
    "    session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    \n",
    "    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "    session = ort.InferenceSession(\n",
    "        str(quantized_model_path),\n",
    "        sess_options=session_options,\n",
    "        providers=providers,\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded successfully\")\n",
    "    print(f\"   Provider: {session.get_providers()[0]}\")\n",
    "    \n",
    "    # Show input/output info\n",
    "    print(f\"\\nüìä Model Info:\")\n",
    "    print(f\"   Inputs:\")\n",
    "    for inp in session.get_inputs():\n",
    "        print(f\"      - {inp.name}: {inp.shape}\")\n",
    "    print(f\"   Outputs:\")\n",
    "    for out in session.get_outputs():\n",
    "        print(f\"      - {out.name}: {out.shape}\")\n",
    "else:\n",
    "    print(f\"‚ùå Quantized model not found at {quantized_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Size Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare sizes at each stage\n",
    "\n",
    "def get_dir_size(path: Path) -> float:\n",
    "    \"\"\"Get total size of directory in MB.\"\"\"\n",
    "    if not path.exists():\n",
    "        return 0\n",
    "    return sum(f.stat().st_size for f in path.rglob(\"*\") if f.is_file()) / 1e6\n",
    "\n",
    "print(\"üìä SIZE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "stages = [\n",
    "    (\"Merged Model (BF16)\", MERGED_PATH),\n",
    "    (\"ONNX (FP32)\", ONNX_PATH),\n",
    "    (\"ONNX INT4 (Browser)\", ONNX_INT4_PATH),\n",
    "]\n",
    "\n",
    "sizes = []\n",
    "for name, path in stages:\n",
    "    size = get_dir_size(path)\n",
    "    sizes.append(size)\n",
    "    print(f\"   {name:<25} {size:>8.1f} MB\")\n",
    "\n",
    "if sizes[0] > 0:\n",
    "    print(\"-\"*70)\n",
    "    final_compression = (1 - sizes[-1] / sizes[0]) * 100\n",
    "    print(f\"   {'Total Compression':<25} {final_compression:>8.1f}%\")\n",
    "    print(f\"\\n‚úÖ Model is now browser-ready at ~{sizes[-1]:.0f} MB!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Prepare for Browser Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create browser deployment package\n",
    "\n",
    "print(\"üì¶ Creating browser deployment package...\")\n",
    "\n",
    "# Copy all needed files to browser directory\n",
    "for f in ONNX_INT4_PATH.iterdir():\n",
    "    if f.is_file():\n",
    "        shutil.copy(f, BROWSER_PATH / f.name)\n",
    "\n",
    "# Create model config for Transformers.js\n",
    "browser_config = {\n",
    "    \"model_type\": \"gemma\",\n",
    "    \"quantization\": \"int4\",\n",
    "    \"framework\": \"onnx\",\n",
    "    \"runtime\": \"transformers.js\",\n",
    "    \"files\": [f.name for f in BROWSER_PATH.iterdir() if f.is_file()],\n",
    "    \"usage\": {\n",
    "        \"device\": \"webgpu\",\n",
    "        \"dtype\": \"q4\",\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(BROWSER_PATH / \"browser_config.json\", 'w') as f:\n",
    "    json.dump(browser_config, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Browser package ready at {BROWSER_PATH}\")\n",
    "print(f\"\\nüìÅ Files for deployment:\")\n",
    "total_size = 0\n",
    "for f in sorted(BROWSER_PATH.iterdir()):\n",
    "    if f.is_file():\n",
    "        size = f.stat().st_size / 1e6\n",
    "        total_size += size\n",
    "        print(f\"   {f.name}: {size:.1f} MB\")\n",
    "print(f\"\\n   Total: {total_size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Issues\n",
    "\n",
    "### Issue 1: ONNX Export Fails\n",
    "**Symptom:** Error during export  \n",
    "**Fix:** Check model architecture is supported, update optimum\n",
    "\n",
    "### Issue 2: Quantization Quality Loss\n",
    "**Symptom:** Output quality significantly degraded  \n",
    "**Fix:** Use per-channel quantization, check input model quality\n",
    "\n",
    "### Issue 3: Large File Size\n",
    "**Symptom:** INT4 model still too big  \n",
    "**Fix:** Verify quantization applied, consider smaller base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Metrics & Outputs\n\n| Metric | Expected | Actual |\n|--------|----------|--------|\n| ONNX Size (FP32) | ~4 GB | [Your value] |\n| ONNX INT4 Size | ~500 MB | [Your value] |\n| Compression Ratio | ~75% | [Your value] |\n| Export Time | ~10 min | [Your value] |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase Complete!\n",
    "\n",
    "You've achieved:\n",
    "- ‚úÖ Exported model to ONNX format\n",
    "- ‚úÖ Applied INT4 quantization (browser-compatible)\n",
    "- ‚úÖ Prepared tokenizer files\n",
    "- ‚úÖ Created browser deployment package\n",
    "\n",
    "**Next:** [Lab 4.6.8.5: Browser Integration](./lab-4.6.8.5-browser-integration.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cleanup\nimport gc\n\n# Clean up ONNX session\nif 'session' in dir():\n    try:\n        del session\n    except:\n        pass\n\n# Clean up intermediate ONNX files to save disk space (optional)\n# Uncomment if you want to remove the FP32 ONNX to save ~4GB\n# import shutil\n# if ONNX_PATH.exists():\n#     shutil.rmtree(ONNX_PATH)\n#     print(f\"üßπ Removed intermediate ONNX files at {ONNX_PATH}\")\n\n# Clear GPU memory\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n\n# Force garbage collection\ngc.collect()\n\nprint(\"‚úÖ Phase 4 Complete!\")\nprint(\"\\nüìä Final Summary:\")\nprint(f\"   Browser-ready model: {BROWSER_PATH}\")\nprint(f\"   Approximate size: ~500MB\")\nprint(\"\\nüéØ Next Steps:\")\nprint(\"   1. Verify INT4 model size is ~500MB\")\nprint(\"   2. Check all tokenizer files are present\")\nprint(\"   3. Proceed to Lab 4.6.8.5 for browser integration\")\nprint(\"\\nüí° Tip: You can delete the intermediate ONNX (FP32) files to save ~4GB\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}