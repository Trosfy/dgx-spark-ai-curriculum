{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.6.3: Option B - Multimodal Document Intelligence\n",
    "\n",
    "**Module:** 4.6 - Capstone Project (Domain 4: Production AI)\n",
    "**Time:** 35-45 hours total\n",
    "**Difficulty:** â­â­â­â­â­\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Project Overview\n",
    "\n",
    "Build a **document intelligence system** that processes and understands complex documents:\n",
    "\n",
    "| Component | Description | DGX Spark Advantage |\n",
    "|-----------|-------------|---------------------|\n",
    "| **Document Ingestion** | PDF, images, diagrams | High-memory batch processing |\n",
    "| **Vision-Language Model** | LLaVA/Qwen2-VL 34B | Full-resolution VLM fits in memory |\n",
    "| **OCR & Layout** | Text extraction with structure | GPU-accelerated OCR |\n",
    "| **Information Extraction** | Structured data from unstructured | Multi-model pipeline |\n",
    "| **Multimodal RAG** | Q&A over documents | VLM + embeddings together |\n",
    "| **Export Pipeline** | JSON, CSV, structured output | Fast batch processing |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By completing this project, you will:\n",
    "- [ ] Build a document ingestion pipeline (PDF, images)\n",
    "- [ ] Integrate vision-language models for understanding\n",
    "- [ ] Extract structured information from documents\n",
    "- [ ] Create a multimodal RAG system\n",
    "- [ ] Build an interactive document Q&A demo\n",
    "- [ ] Implement content safety filtering ğŸ›¡ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸŒ Real-World Context\n",
    "\n",
    "Document intelligence is transforming how organizations handle paperwork:\n",
    "\n",
    "| Industry | Use Case | Value |\n",
    "|----------|----------|-------|\n",
    "| **Finance** | Invoice processing, contract analysis | 90% faster processing |\n",
    "| **Healthcare** | Medical records, lab reports | Reduced transcription errors |\n",
    "| **Legal** | Contract review, discovery | Hours â†’ minutes for review |\n",
    "| **Insurance** | Claims processing | Automated damage assessment |\n",
    "| **Research** | Paper summarization | Rapid literature review |\n",
    "\n",
    "Companies like Google (Document AI), AWS (Textract), and startups like Reducto are building billion-dollar businesses around document understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§’ ELI5: What is Document Intelligence?\n",
    "\n",
    "> **Imagine you have a huge stack of papers** - invoices, contracts, reports, handwritten notes.\n",
    ">\n",
    "> Reading them all would take forever. But what if you had a super-smart assistant who could:\n",
    ">\n",
    "> 1. **Read everything** - even messy handwriting and blurry photos\n",
    "> 2. **Understand what they mean** - not just the words, but the context\n",
    "> 3. **Find specific information** - \"What's the total on invoice #1234?\"\n",
    "> 4. **Organize it all** - put data into neat spreadsheets\n",
    "> 5. **Answer questions** - \"Which contracts expire next month?\"\n",
    ">\n",
    "> **That's document intelligence!** It combines:\n",
    "> - **OCR** (reading text from images)\n",
    "> - **Vision models** (understanding layouts and diagrams)\n",
    "> - **Language models** (understanding meaning)\n",
    "> - **RAG** (finding and retrieving information)\n",
    ">\n",
    "> The result? Turn hours of manual work into seconds of AI processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ—ï¸ System Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Multimodal Document Intelligence                      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                          â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
    "â”‚  â”‚   Gradio     â”‚â”€â”€â”€â–¶â”‚    Query     â”‚â”€â”€â”€â–¶â”‚   Response   â”‚              â”‚\n",
    "â”‚  â”‚  Interface   â”‚    â”‚   Router     â”‚    â”‚   Builder    â”‚              â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚\n",
    "â”‚                              â”‚                                          â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚\n",
    "â”‚         â–¼                    â–¼                    â–¼                     â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
    "â”‚  â”‚  Multimodal  â”‚    â”‚   Vision-    â”‚    â”‚  Structured  â”‚              â”‚\n",
    "â”‚  â”‚     RAG      â”‚    â”‚   Language   â”‚    â”‚  Extraction  â”‚              â”‚\n",
    "â”‚  â”‚              â”‚    â”‚    Model     â”‚    â”‚              â”‚              â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â”‚\n",
    "â”‚          â”‚                                       â”‚                      â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”              â”‚\n",
    "â”‚  â”‚ Vector Store â”‚                        â”‚   Schema     â”‚              â”‚\n",
    "â”‚  â”‚ (text+image) â”‚                        â”‚   Library    â”‚              â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚\n",
    "â”‚          â”‚                                                              â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
    "â”‚  â”‚              Document Processing Pipeline             â”‚              â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚              â”‚\n",
    "â”‚  â”‚  â”‚  PDF   â”‚â”€â–¶â”‚  OCR   â”‚â”€â–¶â”‚ Layout â”‚â”€â–¶â”‚ Chunk  â”‚     â”‚              â”‚\n",
    "â”‚  â”‚  â”‚ Parser â”‚  â”‚ Engine â”‚  â”‚ Detect â”‚  â”‚  +Embedâ”‚     â”‚              â”‚\n",
    "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚              â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚\n",
    "â”‚                                                                          â”‚\n",
    "â”‚  Memory Budget: ~55-65GB of 128GB                                       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"ğŸ” OPTION B: MULTIMODAL DOCUMENT INTELLIGENCE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"\\nGPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'Not available'}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "# Check for vision libraries\n",
    "print(\"\\nğŸ“¦ Vision Libraries:\")\n",
    "libs = [\n",
    "    (\"PIL\", \"pillow\"),\n",
    "    (\"pdf2image\", \"pdf2image\"),\n",
    "    (\"pytesseract\", \"pytesseract\"),\n",
    "    (\"transformers\", \"transformers\"),\n",
    "]\n",
    "for import_name, package_name in libs:\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "        print(f\"   âœ… {package_name}\")\n",
    "    except ImportError:\n",
    "        print(f\"   âŒ {package_name} - pip install {package_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Planning for Document Intelligence\n",
    "\n",
    "def plan_option_b_memory():\n",
    "    \"\"\"Plan memory usage for document intelligence system.\"\"\"\n",
    "    \n",
    "    components = [\n",
    "        {\"name\": \"Qwen2-VL 7B (BF16)\", \"memory_gb\": 14.0, \"alt\": \"Or LLaVA-1.6 34B: 18GB INT4\"},\n",
    "        {\"name\": \"CLIP/SigLIP Embeddings\", \"memory_gb\": 2.0, \"alt\": \"\"},\n",
    "        {\"name\": \"Text Embeddings (BGE-M3)\", \"memory_gb\": 1.2, \"alt\": \"\"},\n",
    "        {\"name\": \"OCR Model (optional)\", \"memory_gb\": 1.0, \"alt\": \"TrOCR or PaddleOCR\"},\n",
    "        {\"name\": \"Layout Detection\", \"memory_gb\": 0.5, \"alt\": \"DiT or LayoutLM\"},\n",
    "        {\"name\": \"Vector Index (FAISS)\", \"memory_gb\": 2.0, \"alt\": \"\"},\n",
    "        {\"name\": \"Image Processing Buffer\", \"memory_gb\": 8.0, \"alt\": \"For batch processing\"},\n",
    "        {\"name\": \"KV Cache\", \"memory_gb\": 6.0, \"alt\": \"\"},\n",
    "        {\"name\": \"System Overhead\", \"memory_gb\": 6.0, \"alt\": \"\"},\n",
    "    ]\n",
    "    \n",
    "    DGX_SPARK_MEMORY = 128.0\n",
    "    \n",
    "    print(\"\\nğŸ’¾ DGX SPARK MEMORY PLAN - OPTION B\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    total = 0\n",
    "    for comp in components:\n",
    "        alt_str = f\" ({comp['alt']})\" if comp['alt'] else \"\"\n",
    "        print(f\"   {comp['name']:<30} {comp['memory_gb']:>6.1f} GB{alt_str}\")\n",
    "        total += comp['memory_gb']\n",
    "    \n",
    "    print(\"-\"*70)\n",
    "    print(f\"   {'TOTAL':<30} {total:>6.1f} GB\")\n",
    "    print(f\"   {'REMAINING':<30} {DGX_SPARK_MEMORY - total:>6.1f} GB\")\n",
    "    \n",
    "    # For larger VLM option\n",
    "    print(\"\\nğŸ“Š Alternative: LLaVA-1.6 34B Configuration\")\n",
    "    print(f\"   VLM (34B INT4): ~18 GB\")\n",
    "    print(f\"   Total with 34B: ~{total + 4:.1f} GB\")\n",
    "    print(f\"   âœ… Still fits comfortably!\")\n",
    "    \n",
    "    return total\n",
    "\n",
    "plan_option_b_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Document Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Processing Classes\n",
    "\n",
    "@dataclass\n",
    "class DocumentPage:\n",
    "    \"\"\"A single page from a document.\"\"\"\n",
    "    page_number: int\n",
    "    image: Any  # PIL Image\n",
    "    text: str = \"\"\n",
    "    layout: Dict[str, Any] = field(default_factory=dict)\n",
    "    tables: List[Dict] = field(default_factory=list)\n",
    "    figures: List[Dict] = field(default_factory=list)\n",
    "\n",
    "@dataclass\n",
    "class ProcessedDocument:\n",
    "    \"\"\"A fully processed document.\"\"\"\n",
    "    id: str\n",
    "    filename: str\n",
    "    pages: List[DocumentPage]\n",
    "    metadata: Dict[str, Any]\n",
    "    full_text: str = \"\"\n",
    "    extracted_data: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"\n",
    "    Process documents (PDF, images) for the intelligence system.\n",
    "    \n",
    "    Features:\n",
    "    - PDF to image conversion\n",
    "    - OCR with layout preservation\n",
    "    - Table and figure detection\n",
    "    - Text chunking for RAG\n",
    "    \n",
    "    Example:\n",
    "        processor = DocumentProcessor()\n",
    "        doc = processor.process(\"invoice.pdf\")\n",
    "        print(doc.full_text)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        ocr_engine: str = \"pytesseract\",\n",
    "        dpi: int = 200,\n",
    "        use_gpu: bool = True,\n",
    "    ):\n",
    "        self.ocr_engine = ocr_engine\n",
    "        self.dpi = dpi\n",
    "        self.use_gpu = use_gpu\n",
    "        self._doc_id_counter = 0\n",
    "    \n",
    "    def process(self, file_path: str) -> ProcessedDocument:\n",
    "        \"\"\"\n",
    "        Process a document file.\n",
    "        \n",
    "        Supports: PDF, PNG, JPG, TIFF\n",
    "        \"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        \n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"Document not found: {file_path}\")\n",
    "        \n",
    "        self._doc_id_counter += 1\n",
    "        doc_id = f\"doc_{self._doc_id_counter:05d}\"\n",
    "        \n",
    "        # Convert to images\n",
    "        if file_path.suffix.lower() == \".pdf\":\n",
    "            images = self._pdf_to_images(file_path)\n",
    "        else:\n",
    "            from PIL import Image\n",
    "            images = [Image.open(file_path)]\n",
    "        \n",
    "        # Process each page\n",
    "        pages = []\n",
    "        all_text = []\n",
    "        \n",
    "        for i, img in enumerate(images):\n",
    "            page = self._process_page(img, i + 1)\n",
    "            pages.append(page)\n",
    "            all_text.append(page.text)\n",
    "        \n",
    "        return ProcessedDocument(\n",
    "            id=doc_id,\n",
    "            filename=file_path.name,\n",
    "            pages=pages,\n",
    "            metadata={\n",
    "                \"source\": str(file_path),\n",
    "                \"num_pages\": len(pages),\n",
    "                \"processed_at\": datetime.now().isoformat(),\n",
    "            },\n",
    "            full_text=\"\\n\\n\".join(all_text)\n",
    "        )\n",
    "    \n",
    "    def _pdf_to_images(self, pdf_path: Path) -> List:\n",
    "        \"\"\"Convert PDF to list of images.\"\"\"\n",
    "        try:\n",
    "            from pdf2image import convert_from_path\n",
    "            return convert_from_path(pdf_path, dpi=self.dpi)\n",
    "        except ImportError:\n",
    "            print(\"âš ï¸ pdf2image not installed. Install with: pip install pdf2image\")\n",
    "            print(\"   Also requires poppler: apt-get install poppler-utils\")\n",
    "            return []\n",
    "    \n",
    "    def _process_page(self, image, page_num: int) -> DocumentPage:\n",
    "        \"\"\"Process a single page image.\"\"\"\n",
    "        # OCR\n",
    "        text = self._ocr(image)\n",
    "        \n",
    "        # Layout detection (simplified)\n",
    "        layout = self._detect_layout(image)\n",
    "        \n",
    "        return DocumentPage(\n",
    "            page_number=page_num,\n",
    "            image=image,\n",
    "            text=text,\n",
    "            layout=layout,\n",
    "        )\n",
    "    \n",
    "    def _ocr(self, image) -> str:\n",
    "        \"\"\"Extract text from image using OCR.\"\"\"\n",
    "        try:\n",
    "            import pytesseract\n",
    "            return pytesseract.image_to_string(image)\n",
    "        except ImportError:\n",
    "            # Fallback: use VLM for OCR (in production)\n",
    "            return \"[OCR not available - install pytesseract]\"\n",
    "    \n",
    "    def _detect_layout(self, image) -> Dict:\n",
    "        \"\"\"Detect document layout (simplified).\"\"\"\n",
    "        # In production, use LayoutLM or DiT\n",
    "        return {\n",
    "            \"width\": image.width if hasattr(image, 'width') else 0,\n",
    "            \"height\": image.height if hasattr(image, 'height') else 0,\n",
    "            \"detected_regions\": [],\n",
    "        }\n",
    "\n",
    "print(\"âœ… Document processing classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Vision-Language Model Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision-Language Model Wrapper\n",
    "\n",
    "class VisionLanguageModel:\n",
    "    \"\"\"\n",
    "    Vision-Language Model for document understanding.\n",
    "    \n",
    "    Supports:\n",
    "    - Qwen2-VL (7B, 72B)\n",
    "    - LLaVA-1.6 (7B, 13B, 34B)\n",
    "    - InternVL\n",
    "    \n",
    "    Example:\n",
    "        vlm = VisionLanguageModel(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "        result = vlm.analyze(image, \"What information is on this invoice?\")\n",
    "    \"\"\"\n",
    "    \n",
    "    SUPPORTED_MODELS = {\n",
    "        \"qwen2-vl-7b\": \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "        \"qwen2-vl-72b\": \"Qwen/Qwen2-VL-72B-Instruct\",\n",
    "        \"llava-1.6-7b\": \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "        \"llava-1.6-34b\": \"llava-hf/llava-v1.6-34b-hf\",\n",
    "    }\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"qwen2-vl-7b\",\n",
    "        load_in_4bit: bool = False,\n",
    "        max_image_size: int = 1024,\n",
    "    ):\n",
    "        self.model_id = self.SUPPORTED_MODELS.get(model_name, model_name)\n",
    "        self.load_in_4bit = load_in_4bit\n",
    "        self.max_image_size = max_image_size\n",
    "        self._model = None\n",
    "        self._processor = None\n",
    "    \n",
    "    def load(self):\n",
    "        \"\"\"Load the VLM into memory.\"\"\"\n",
    "        if self._model is not None:\n",
    "            return\n",
    "        \n",
    "        from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        \n",
    "        print(f\"ğŸ“¥ Loading VLM: {self.model_id}\")\n",
    "        \n",
    "        if self.load_in_4bit:\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            )\n",
    "            self._model = AutoModelForVision2Seq.from_pretrained(\n",
    "                self.model_id,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "        else:\n",
    "            self._model = AutoModelForVision2Seq.from_pretrained(\n",
    "                self.model_id,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "        \n",
    "        self._processor = AutoProcessor.from_pretrained(\n",
    "            self.model_id,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        \n",
    "        mem_gb = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"âœ… VLM loaded. GPU memory: {mem_gb:.2f} GB\")\n",
    "    \n",
    "    def analyze(\n",
    "        self,\n",
    "        image,\n",
    "        prompt: str,\n",
    "        max_tokens: int = 1024,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Analyze an image with a text prompt.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image or path to image\n",
    "            prompt: Question or instruction\n",
    "            max_tokens: Maximum response length\n",
    "            \n",
    "        Returns:\n",
    "            Model's response\n",
    "        \"\"\"\n",
    "        self.load()\n",
    "        \n",
    "        from PIL import Image\n",
    "        \n",
    "        # Load image if path\n",
    "        if isinstance(image, (str, Path)):\n",
    "            image = Image.open(image)\n",
    "        \n",
    "        # Resize if needed\n",
    "        if max(image.size) > self.max_image_size:\n",
    "            ratio = self.max_image_size / max(image.size)\n",
    "            new_size = (int(image.width * ratio), int(image.height * ratio))\n",
    "            image = image.resize(new_size)\n",
    "        \n",
    "        # Process\n",
    "        inputs = self._processor(\n",
    "            text=prompt,\n",
    "            images=image,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self._model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "            )\n",
    "        \n",
    "        response = self._processor.decode(\n",
    "            outputs[0],\n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def extract_structured(\n",
    "        self,\n",
    "        image,\n",
    "        schema: Dict[str, str],\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract structured data from document image.\n",
    "        \n",
    "        Args:\n",
    "            image: Document image\n",
    "            schema: Dict of {field_name: description}\n",
    "            \n",
    "        Returns:\n",
    "            Dict with extracted values\n",
    "        \"\"\"\n",
    "        schema_text = \"\\n\".join([\n",
    "            f\"- {name}: {desc}\" for name, desc in schema.items()\n",
    "        ])\n",
    "        \n",
    "        prompt = f\"\"\"Extract the following information from this document image.\n",
    "Return the result as a JSON object.\n",
    "\n",
    "Fields to extract:\n",
    "{schema_text}\n",
    "\n",
    "Return ONLY the JSON object, no other text.\"\"\"\n",
    "        \n",
    "        response = self.analyze(image, prompt)\n",
    "        \n",
    "        # Parse JSON from response\n",
    "        try:\n",
    "            # Find JSON in response\n",
    "            import re\n",
    "            json_match = re.search(r'\\{[^{}]*\\}', response, re.DOTALL)\n",
    "            if json_match:\n",
    "                return json.loads(json_match.group())\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "        \n",
    "        return {\"raw_response\": response}\n",
    "    \n",
    "    def unload(self):\n",
    "        \"\"\"Unload model to free memory.\"\"\"\n",
    "        if self._model is not None:\n",
    "            del self._model\n",
    "            del self._processor\n",
    "            self._model = None\n",
    "            self._processor = None\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"âœ… VLM unloaded\")\n",
    "\n",
    "print(\"âœ… VisionLanguageModel class defined\")\n",
    "print(f\"\\nSupported models: {list(VisionLanguageModel.SUPPORTED_MODELS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Information Extraction Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Extraction Schemas\n",
    "\n",
    "EXTRACTION_SCHEMAS = {\n",
    "    \"invoice\": {\n",
    "        \"vendor_name\": \"Name of the company issuing the invoice\",\n",
    "        \"invoice_number\": \"Unique invoice identifier\",\n",
    "        \"invoice_date\": \"Date the invoice was issued (YYYY-MM-DD)\",\n",
    "        \"due_date\": \"Payment due date (YYYY-MM-DD)\",\n",
    "        \"total_amount\": \"Total amount due (number only)\",\n",
    "        \"currency\": \"Currency code (USD, EUR, etc.)\",\n",
    "        \"line_items\": \"List of items with description, quantity, and price\",\n",
    "        \"payment_terms\": \"Payment terms or conditions\",\n",
    "    },\n",
    "    \"receipt\": {\n",
    "        \"merchant_name\": \"Store or business name\",\n",
    "        \"date\": \"Transaction date\",\n",
    "        \"total\": \"Total amount paid\",\n",
    "        \"payment_method\": \"How payment was made (cash, card, etc.)\",\n",
    "        \"items\": \"List of purchased items\",\n",
    "    },\n",
    "    \"contract\": {\n",
    "        \"parties\": \"Names of parties involved\",\n",
    "        \"effective_date\": \"When the contract takes effect\",\n",
    "        \"expiration_date\": \"When the contract expires\",\n",
    "        \"contract_type\": \"Type of agreement\",\n",
    "        \"key_terms\": \"Important terms and conditions\",\n",
    "        \"signatures\": \"Names of signatories\",\n",
    "    },\n",
    "    \"research_paper\": {\n",
    "        \"title\": \"Paper title\",\n",
    "        \"authors\": \"List of authors\",\n",
    "        \"abstract\": \"Paper abstract or summary\",\n",
    "        \"keywords\": \"Key topics or keywords\",\n",
    "        \"publication\": \"Journal or conference name\",\n",
    "        \"year\": \"Publication year\",\n",
    "    },\n",
    "    \"form\": {\n",
    "        \"form_type\": \"Type of form\",\n",
    "        \"filled_fields\": \"All filled-in fields with their values\",\n",
    "        \"checkboxes\": \"Status of any checkboxes\",\n",
    "        \"signatures\": \"Any signatures present\",\n",
    "    },\n",
    "}\n",
    "\n",
    "class StructuredExtractor:\n",
    "    \"\"\"\n",
    "    Extract structured data from documents using schemas.\n",
    "    \n",
    "    Example:\n",
    "        extractor = StructuredExtractor(vlm)\n",
    "        data = extractor.extract(image, \"invoice\")\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vlm: VisionLanguageModel):\n",
    "        self.vlm = vlm\n",
    "        self.schemas = EXTRACTION_SCHEMAS\n",
    "    \n",
    "    def extract(\n",
    "        self,\n",
    "        image,\n",
    "        document_type: str,\n",
    "        custom_schema: Dict[str, str] = None,\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract data using predefined or custom schema.\n",
    "        \n",
    "        Args:\n",
    "            image: Document image\n",
    "            document_type: Key in EXTRACTION_SCHEMAS or \"custom\"\n",
    "            custom_schema: Custom extraction schema\n",
    "            \n",
    "        Returns:\n",
    "            Extracted data matching schema\n",
    "        \"\"\"\n",
    "        if custom_schema:\n",
    "            schema = custom_schema\n",
    "        elif document_type in self.schemas:\n",
    "            schema = self.schemas[document_type]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown document type: {document_type}. \"\n",
    "                           f\"Available: {list(self.schemas.keys())}\")\n",
    "        \n",
    "        result = self.vlm.extract_structured(image, schema)\n",
    "        result[\"_document_type\"] = document_type\n",
    "        result[\"_extracted_at\"] = datetime.now().isoformat()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def batch_extract(\n",
    "        self,\n",
    "        documents: List[ProcessedDocument],\n",
    "        document_type: str,\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Extract from multiple documents.\"\"\"\n",
    "        results = []\n",
    "        for doc in documents:\n",
    "            for page in doc.pages:\n",
    "                result = self.extract(page.image, document_type)\n",
    "                result[\"_document_id\"] = doc.id\n",
    "                result[\"_page\"] = page.page_number\n",
    "                results.append(result)\n",
    "        return results\n",
    "\n",
    "print(\"âœ… Extraction schemas defined\")\n",
    "print(f\"\\nAvailable document types: {list(EXTRACTION_SCHEMAS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Multimodal RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multimodal RAG for Document Q&A\n",
    "\n",
    "@dataclass\n",
    "class MultimodalChunk:\n",
    "    \"\"\"A chunk that can contain text and/or image.\"\"\"\n",
    "    id: str\n",
    "    text: str\n",
    "    image: Optional[Any] = None  # PIL Image\n",
    "    embedding: Optional[List[float]] = None\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "class MultimodalRAG:\n",
    "    \"\"\"\n",
    "    RAG system for multimodal document Q&A.\n",
    "    \n",
    "    Combines:\n",
    "    - Text embeddings (for text content)\n",
    "    - Image embeddings (for visual content)\n",
    "    - VLM for answer generation\n",
    "    \n",
    "    Example:\n",
    "        rag = MultimodalRAG(vlm)\n",
    "        rag.add_document(processed_doc)\n",
    "        answer = rag.query(\"What is the total on invoice #1234?\")\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vlm: VisionLanguageModel,\n",
    "        text_embedding_model: str = \"BAAI/bge-m3\",\n",
    "        chunk_size: int = 500,\n",
    "    ):\n",
    "        self.vlm = vlm\n",
    "        self.text_embedding_model = text_embedding_model\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunks: List[MultimodalChunk] = []\n",
    "        self.text_index = None\n",
    "        self._text_embedder = None\n",
    "        self._chunk_id_counter = 0\n",
    "    \n",
    "    def _load_text_embedder(self):\n",
    "        \"\"\"Load text embedding model.\"\"\"\n",
    "        if self._text_embedder is None:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            self._text_embedder = SentenceTransformer(\n",
    "                self.text_embedding_model,\n",
    "                device=\"cuda\"\n",
    "            )\n",
    "        return self._text_embedder\n",
    "    \n",
    "    def add_document(self, doc: ProcessedDocument):\n",
    "        \"\"\"\n",
    "        Add a processed document to the RAG system.\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        \n",
    "        embedder = self._load_text_embedder()\n",
    "        new_chunks = []\n",
    "        \n",
    "        for page in doc.pages:\n",
    "            # Create chunks from page text\n",
    "            text_chunks = self._chunk_text(page.text)\n",
    "            \n",
    "            for i, text in enumerate(text_chunks):\n",
    "                self._chunk_id_counter += 1\n",
    "                chunk = MultimodalChunk(\n",
    "                    id=f\"chunk_{self._chunk_id_counter}\",\n",
    "                    text=text,\n",
    "                    image=page.image if i == 0 else None,  # First chunk gets image\n",
    "                    metadata={\n",
    "                        \"document_id\": doc.id,\n",
    "                        \"filename\": doc.filename,\n",
    "                        \"page\": page.page_number,\n",
    "                        \"chunk_index\": i,\n",
    "                    }\n",
    "                )\n",
    "                new_chunks.append(chunk)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        texts = [c.text for c in new_chunks]\n",
    "        embeddings = embedder.encode(\n",
    "            texts,\n",
    "            normalize_embeddings=True,\n",
    "            show_progress_bar=True,\n",
    "        )\n",
    "        \n",
    "        for chunk, emb in zip(new_chunks, embeddings):\n",
    "            chunk.embedding = emb.tolist()\n",
    "        \n",
    "        self.chunks.extend(new_chunks)\n",
    "        \n",
    "        # Rebuild index\n",
    "        self._build_index()\n",
    "        \n",
    "        print(f\"âœ… Added {len(new_chunks)} chunks from {doc.filename}\")\n",
    "    \n",
    "    def _chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into chunks.\"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(words), self.chunk_size):\n",
    "            chunk = \" \".join(words[i:i + self.chunk_size])\n",
    "            if len(chunk) > 50:\n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        return chunks if chunks else [text]  # At least one chunk\n",
    "    \n",
    "    def _build_index(self):\n",
    "        \"\"\"Build FAISS index.\"\"\"\n",
    "        import faiss\n",
    "        import numpy as np\n",
    "        \n",
    "        embeddings = np.array([\n",
    "            c.embedding for c in self.chunks if c.embedding\n",
    "        ]).astype('float32')\n",
    "        \n",
    "        if len(embeddings) == 0:\n",
    "            return\n",
    "        \n",
    "        dim = embeddings.shape[1]\n",
    "        self.text_index = faiss.IndexFlatIP(dim)\n",
    "        self.text_index.add(embeddings)\n",
    "    \n",
    "    def query(\n",
    "        self,\n",
    "        question: str,\n",
    "        top_k: int = 3,\n",
    "        include_images: bool = True,\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Query the document collection.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            top_k: Number of chunks to retrieve\n",
    "            include_images: Whether to use VLM on retrieved images\n",
    "            \n",
    "        Returns:\n",
    "            Dict with answer, sources, and confidence\n",
    "        \"\"\"\n",
    "        import time\n",
    "        import numpy as np\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        # Retrieve relevant chunks\n",
    "        embedder = self._load_text_embedder()\n",
    "        query_emb = embedder.encode([question], normalize_embeddings=True)\n",
    "        \n",
    "        scores, indices = self.text_index.search(\n",
    "            query_emb.astype('float32'), top_k\n",
    "        )\n",
    "        \n",
    "        retrieved = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx >= 0:\n",
    "                retrieved.append({\n",
    "                    \"chunk\": self.chunks[idx],\n",
    "                    \"score\": float(score),\n",
    "                })\n",
    "        \n",
    "        # Build context\n",
    "        context_parts = []\n",
    "        images = []\n",
    "        \n",
    "        for r in retrieved:\n",
    "            chunk = r[\"chunk\"]\n",
    "            context_parts.append(f\"[Source: {chunk.metadata.get('filename', 'unknown')} \"\n",
    "                                f\"Page {chunk.metadata.get('page', '?')}]\\n{chunk.text}\")\n",
    "            if chunk.image and include_images:\n",
    "                images.append(chunk.image)\n",
    "        \n",
    "        context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Generate answer\n",
    "        if images and include_images:\n",
    "            # Use VLM with first relevant image\n",
    "            prompt = f\"\"\"Based on this document image and the following context, answer the question.\n",
    "\n",
    "Context from documents:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a clear, concise answer based on the document content.\"\"\"\n",
    "            \n",
    "            answer = self.vlm.analyze(images[0], prompt)\n",
    "        else:\n",
    "            # Text-only answer\n",
    "            prompt = f\"\"\"Based on the following context, answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "            # Use text model or VLM without image\n",
    "            answer = f\"Based on the documents: {context[:500]}...\"  # Placeholder\n",
    "        \n",
    "        latency_ms = (time.time() - start) * 1000\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": [\n",
    "                {\n",
    "                    \"filename\": r[\"chunk\"].metadata.get(\"filename\"),\n",
    "                    \"page\": r[\"chunk\"].metadata.get(\"page\"),\n",
    "                    \"score\": r[\"score\"],\n",
    "                    \"text_preview\": r[\"chunk\"].text[:200],\n",
    "                }\n",
    "                for r in retrieved\n",
    "            ],\n",
    "            \"latency_ms\": latency_ms,\n",
    "            \"images_used\": len(images) if include_images else 0,\n",
    "        }\n",
    "\n",
    "print(\"âœ… MultimodalRAG class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Safety & Content Filtering ğŸ›¡ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Content Safety\n",
    "\n",
    "class DocumentSafetyFilter:\n",
    "    \"\"\"\n",
    "    Safety filter for document processing.\n",
    "    \n",
    "    Checks for:\n",
    "    - PII in extracted text (SSN, credit cards, etc.)\n",
    "    - Sensitive document types\n",
    "    - Malicious file content\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pii_patterns = {\n",
    "            \"ssn\": r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\",\n",
    "            \"credit_card\": r\"\\b\\d{4}[- ]?\\d{4}[- ]?\\d{4}[- ]?\\d{4}\\b\",\n",
    "            \"email\": r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\",\n",
    "            \"phone\": r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\",\n",
    "            \"bank_account\": r\"\\b\\d{8,17}\\b\",  # Common account number length\n",
    "        }\n",
    "        \n",
    "        self.sensitive_keywords = [\n",
    "            \"confidential\", \"classified\", \"top secret\",\n",
    "            \"internal only\", \"do not distribute\",\n",
    "            \"attorney-client privilege\", \"hipaa\",\n",
    "        ]\n",
    "    \n",
    "    def check_text(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Check text for PII and sensitive content.\n",
    "        \n",
    "        Returns:\n",
    "            Dict with findings and recommendations\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        findings = {\n",
    "            \"pii_detected\": [],\n",
    "            \"sensitive_markers\": [],\n",
    "            \"is_safe\": True,\n",
    "            \"recommendations\": [],\n",
    "        }\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Check PII patterns\n",
    "        for pii_type, pattern in self.pii_patterns.items():\n",
    "            matches = re.findall(pattern, text)\n",
    "            if matches:\n",
    "                findings[\"pii_detected\"].append({\n",
    "                    \"type\": pii_type,\n",
    "                    \"count\": len(matches),\n",
    "                })\n",
    "                findings[\"is_safe\"] = False\n",
    "        \n",
    "        # Check sensitive keywords\n",
    "        for keyword in self.sensitive_keywords:\n",
    "            if keyword in text_lower:\n",
    "                findings[\"sensitive_markers\"].append(keyword)\n",
    "        \n",
    "        # Generate recommendations\n",
    "        if findings[\"pii_detected\"]:\n",
    "            findings[\"recommendations\"].append(\n",
    "                \"Redact or mask PII before sharing extracted data\"\n",
    "            )\n",
    "        \n",
    "        if findings[\"sensitive_markers\"]:\n",
    "            findings[\"recommendations\"].append(\n",
    "                \"Document may contain confidential information - verify authorization\"\n",
    "            )\n",
    "        \n",
    "        return findings\n",
    "    \n",
    "    def redact_pii(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Redact PII from text.\n",
    "        \n",
    "        Returns:\n",
    "            Text with PII replaced by [REDACTED]\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        redacted = text\n",
    "        \n",
    "        for pii_type, pattern in self.pii_patterns.items():\n",
    "            redacted = re.sub(pattern, f\"[{pii_type.upper()}_REDACTED]\", redacted)\n",
    "        \n",
    "        return redacted\n",
    "\n",
    "# Test safety filter\n",
    "filter = DocumentSafetyFilter()\n",
    "\n",
    "test_text = \"\"\"\n",
    "Invoice for John Smith\n",
    "SSN: 123-45-6789\n",
    "Credit Card: 4111 1111 1111 1111\n",
    "Email: john.smith@example.com\n",
    "CONFIDENTIAL - Internal Use Only\n",
    "\"\"\"\n",
    "\n",
    "print(\"ğŸ›¡ï¸ DOCUMENT SAFETY FILTER TEST\")\n",
    "print(\"=\"*70)\n",
    "result = filter.check_text(test_text)\n",
    "print(f\"\\nSafe: {result['is_safe']}\")\n",
    "print(f\"PII Found: {result['pii_detected']}\")\n",
    "print(f\"Sensitive Markers: {result['sensitive_markers']}\")\n",
    "print(f\"\\nRecommendations:\")\n",
    "for rec in result['recommendations']:\n",
    "    print(f\"  â€¢ {rec}\")\n",
    "\n",
    "print(f\"\\nğŸ“ Redacted Version:\")\n",
    "print(filter.redact_pii(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ Checkpoint\n",
    "\n",
    "You now have the components for a document intelligence system:\n",
    "\n",
    "- âœ… Document processing pipeline (PDF, images)\n",
    "- âœ… Vision-Language Model integration\n",
    "- âœ… Structured extraction with schemas\n",
    "- âœ… Multimodal RAG for Q&A\n",
    "- âœ… Safety filtering for PII ğŸ›¡ï¸\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Implementation Roadmap\n",
    "\n",
    "### Week 1: Document Pipeline\n",
    "- [ ] Set up PDF processing (poppler, pdf2image)\n",
    "- [ ] Configure OCR (Tesseract or TrOCR)\n",
    "- [ ] Test with sample documents\n",
    "\n",
    "### Week 2: VLM & Extraction\n",
    "- [ ] Load and test VLM (Qwen2-VL or LLaVA)\n",
    "- [ ] Create extraction schemas for your document types\n",
    "- [ ] Build structured extraction pipeline\n",
    "\n",
    "### Week 3: RAG & Q&A\n",
    "- [ ] Build multimodal RAG system\n",
    "- [ ] Test retrieval quality\n",
    "- [ ] Implement answer generation\n",
    "\n",
    "### Week 4: Integration\n",
    "- [ ] Build end-to-end pipeline\n",
    "- [ ] Add safety filters\n",
    "- [ ] Create export functionality\n",
    "\n",
    "### Week 5: Evaluation\n",
    "- [ ] Benchmark extraction accuracy\n",
    "- [ ] Test Q&A quality\n",
    "- [ ] Measure latency and throughput\n",
    "\n",
    "### Week 6: Demo & Docs\n",
    "- [ ] Build Gradio demo\n",
    "- [ ] Complete technical report\n",
    "- [ ] Record demo video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§¹ Cleanup\n",
    "import gc\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… Cleanup complete!\")\n",
    "print(\"\\nğŸ¯ Next: Review examples/option-b-document-intelligence/ for complete implementation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
