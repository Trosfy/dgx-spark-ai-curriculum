{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 4.6.8.1: Dataset Preparation\n\n**Capstone Option E:** Browser-Deployed Fine-Tuned LLM (Troscha Matcha Guide)  \n**Phase:** 1 of 6  \n**Time:** 4-6 hours  \n**Difficulty:** ‚≠ê‚≠ê‚≠ê\n\n---\n\n## Phase Objectives\n\nBy completing this phase, you will:\n- [ ] Understand the messages format with `<preferences>` JSON output\n- [ ] Use the pre-built 300 training examples from `data/option-e-browser-llm/`\n- [ ] Implement data validation and quality checks\n- [ ] Split data into train/validation/test sets\n- [ ] Save dataset in Hugging Face format\n\n---\n\n## Phase Checklist\n\n- [ ] Environment setup complete\n- [ ] Dataset format understood (including `<preferences>` JSON)\n- [ ] Training examples loaded from data files\n- [ ] Validation examples created (20+)\n- [ ] Test examples created (20+)\n- [ ] Quality validation passed\n- [ ] Dataset saved locally\n- [ ] (Optional) Dataset pushed to Hub\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Matters\n",
    "\n",
    "**Quality over Quantity** - For domain-specific fine-tuning, 150 excellent examples beat 10,000 mediocre ones.\n",
    "\n",
    "| Dataset Size | Quality Needed | Use Case |\n",
    "|--------------|----------------|----------|\n",
    "| 50-100 | Very High | Narrow domain adaptation |\n",
    "| 100-500 | High | **Our target: Domain expertise** |\n",
    "| 500-5000 | Medium | Broader capabilities |\n",
    "| 5000+ | Mixed OK | General instruction tuning |\n",
    "\n",
    "Think of it like training a specialist vs. a generalist:\n",
    "- **Specialist** (our goal): Deep knowledge in matcha ‚Üí fewer, high-quality examples\n",
    "- **Generalist**: Broad knowledge ‚Üí many diverse examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ELI5: What Makes Good Training Data?\n\n> **Imagine you're writing a study guide for a Troscha barista exam.**\n>\n> Each training example is like a flashcard:\n> - **Front** (user question): \"What's the difference between Firu and Giru?\"\n> - **Back** (expert answer): A detailed, accurate response PLUS structured preferences\n>\n> **Good flashcards:**\n> - Cover all products and common questions\n> - Have clear, specific questions\n> - Have detailed, accurate answers with personality\n> - Include `<preferences>` JSON for product recommendations\n>\n> **Bad flashcards:**\n> - All ask the same thing differently\n> - Have one-word answers\n> - Missing the `<preferences>` JSON structure\n> - Don't match Troscha's product catalog\n\n**Unique to this project:** Every response ends with a `<preferences>` JSON block that enables structured product recommendations in the UI!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass, field, asdict\n",
    "import random\n",
    "\n",
    "# Dataset library\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "print(\"üçµ PHASE 1: DATASET PREPARATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"Working Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Project Configuration\nPROJECT_DIR = Path(\"./troscha-matcha\")\nDATA_DIR = PROJECT_DIR / \"data\"\nDATA_DIR.mkdir(parents=True, exist_ok=True)\n\n# Path to pre-built training data\n# This path is relative to the notebook location (labs/ directory)\n# The data lives in domain-4.../module-4.6.../data/option-e-browser-llm/\nSOURCE_DATA_DIR = Path(\"../data/option-e-browser-llm\")\n\n# Verify source data exists, try alternative paths if needed\nif not SOURCE_DATA_DIR.exists():\n    # Try from project root if running from different directory\n    alt_paths = [\n        Path(\"data/option-e-browser-llm\"),\n        Path(\"./data/option-e-browser-llm\"),\n        Path(__file__).parent.parent / \"data\" / \"option-e-browser-llm\" if \"__file__\" in dir() else None,\n    ]\n    for alt in alt_paths:\n        if alt and alt.exists():\n            SOURCE_DATA_DIR = alt\n            print(f\"   Using alternative path: {SOURCE_DATA_DIR}\")\n            break\n\n# Dataset configuration\nDATASET_CONFIG = {\n    \"name\": \"troscha-matcha-dataset\",\n    \"version\": \"1.0.0\",\n    \"min_train_examples\": 240,  # 80% of 300\n    \"min_val_examples\": 30,     # 10% of 300\n    \"min_test_examples\": 30,    # 10% of 300\n    \"train_split\": 0.8,\n    \"val_split\": 0.1,\n    \"test_split\": 0.1,\n}\n\n# System prompt for Troscha matcha guide\nSYSTEM_PROMPT = \"\"\"You are Troscha's matcha guide.\n\nMENU:\n- Yura: Latte Rp 27k\n- Taku: Straight Rp 25k | Latte Rp 32k | Strawberry Rp 40k\n- Firu: Straight Rp 34k | Latte Rp 44k | Miruku Rp 49k | Strawberry Rp 52k\n- Giru: Straight Rp 39k | Latte Rp 49k | Miruku Rp 54k | Strawberry Rp 57k\n- Zeno: Straight Rp 44k | Latte Rp 54k | Miruku Rp 59k | Strawberry Rp 62k\n- Moku: Hojicha Latte Rp 35k\n- Hiku: Straight Rp 79k | Latte Rp 89k\n- Kiyo: Straight Rp 94k | Latte Rp 104k\n\nADDON: Oat Milk +Rp 5k\n\nEnd responses with <preferences> JSON.\"\"\"\n\nprint(f\"üìÅ Project Directory: {PROJECT_DIR}\")\nprint(f\"üìä Dataset Name: {DATASET_CONFIG['name']}\")\nprint(f\"üìù Source Data: {SOURCE_DATA_DIR}\")\nif SOURCE_DATA_DIR.exists():\n    print(f\"   ‚úÖ Source data directory found\")\nelse:\n    print(f\"   ‚ö†Ô∏è Source data directory not found - check path\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding the Messages Format\n",
    "\n",
    "Modern chat models expect data in a specific **messages format**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# The Messages Format with <preferences> JSON\n\n# Each training example is a conversation with three parts:\nexample_format = {\n    \"messages\": [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are Troscha's matcha guide.\\n\\nMENU:\\n- Yura: Latte Rp 27k\\n...\"\n        },\n        {\n            \"role\": \"user\", \n            \"content\": \"What's the difference between Firu and Giru?\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"\"\"Great question! Firu and Giru are both premium blends but come from different legendary regions.\n\nFiru is from Uji (Kyoto) - Japan's original matcha heartland. It blends Yabukita with Okumidori cultivars for harmonious umami sweetness.\n\nGiru is from Nishio (Aichi) - Japan's largest matcha producer. It blends Saemidori with Okumidori for mellow, well-rounded character.\n\nBoth are excellent for lattes!\n\n<preferences>{\"intent\": \"compare\", \"sweetness\": \"medium\", \"bitterness\": \"low\", \"umami\": \"high\", \"body\": \"medium\", \"serving\": null, \"experience\": null, \"recommended_matcha\": [\"m-003\", \"m-004\"], \"origin_preference\": [\"uji\", \"nishio\"], \"notes\": \"comparing Uji vs Nishio terroir\"}</preferences>\"\"\"\n        }\n    ]\n}\n\nprint(\"üìã MESSAGES FORMAT WITH <preferences>\")\nprint(\"=\"*70)\nprint(json.dumps(example_format, indent=2)[:800] + \"...\")\nprint(\"\\nüí° The <preferences> JSON at the end enables structured recommendations!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Data structures (for reference - our pre-built data already uses this format)\n#\n# The following dataclasses show the expected structure of training examples.\n# Since we're loading pre-built data from JSON files, these are provided\n# for documentation purposes only.\n\n@dataclass\nclass Message:\n    \"\"\"A single message in a conversation.\"\"\"\n    role: str  # \"system\", \"user\", or \"assistant\"\n    content: str\n    \n    def to_dict(self) -> Dict[str, str]:\n        return {\"role\": self.role, \"content\": self.content}\n\n@dataclass\nclass TrainingExample:\n    \"\"\"A complete training example.\"\"\"\n    messages: List[Message]\n    category: str  # For tracking topic coverage\n    \n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"messages\": [m.to_dict() for m in self.messages],\n            \"category\": self.category,\n        }\n\nprint(\"‚úÖ Data structures defined (for reference)\")\nprint(\"   Note: Pre-built training data is loaded from ../data/option-e-browser-llm/\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 3: Understanding the <preferences> JSON Schema\n\nEvery assistant response in our training data ends with a structured `<preferences>` JSON block:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Topic Categories based on actual training data files\n\nCATEGORIES = {\n    \"product_comparisons\": {\n        \"description\": \"Pairwise and group product comparisons\",\n        \"file\": \"01_product_comparisons.json\",\n        \"target_count\": 30,\n    },\n    \"product_specific\": {\n        \"description\": \"Questions about specific products\",\n        \"file\": \"02_product_specific.json\",\n        \"target_count\": 24,\n    },\n    \"taste_discovery\": {\n        \"description\": \"Taste preference discovery\",\n        \"file\": \"03_taste_discovery.json\",\n        \"target_count\": 40,\n    },\n    \"experience_onboarding\": {\n        \"description\": \"Experience-based recommendations\",\n        \"file\": \"04_experience_onboarding.json\",\n        \"target_count\": 35,\n    },\n    \"serving_context\": {\n        \"description\": \"Serving style guidance\",\n        \"file\": \"05_serving_context.json\",\n        \"target_count\": 30,\n    },\n    \"matcha_education\": {\n        \"description\": \"General matcha education\",\n        \"file\": \"06_matcha_education.json\",\n        \"target_count\": 25,\n    },\n    \"price_questions\": {\n        \"description\": \"Price and value questions\",\n        \"file\": \"07_price_questions.json\",\n        \"target_count\": 15,\n    },\n    \"common_questions\": {\n        \"description\": \"Storage, prep, caffeine, etc.\",\n        \"file\": \"08_common_questions.json\",\n        \"target_count\": 20,\n    },\n    \"troubleshooting\": {\n        \"description\": \"Problem-solving conversations\",\n        \"file\": \"09_troubleshooting.json\",\n        \"target_count\": 18,\n    },\n    \"multi_turn\": {\n        \"description\": \"Multi-turn refinement\",\n        \"file\": \"10_multi_turn.json\",\n        \"target_count\": 20,\n    },\n    \"out_of_scope\": {\n        \"description\": \"Out-of-scope handling\",\n        \"file\": \"11_out_of_scope.json\",\n        \"target_count\": 31,\n    },\n    \"brand_identity\": {\n        \"description\": \"Brand and chatbot identity\",\n        \"file\": \"12_brand_identity.json\",\n        \"target_count\": 12,\n    },\n}\n\nprint(\"üìä TOPIC DISTRIBUTION (from data/option-e-browser-llm/)\")\nprint(\"=\"*70)\ntotal = 0\nfor cat, info in CATEGORIES.items():\n    print(f\"   {cat:<25} {info['target_count']:>3} examples  - {info['description']}\")\n    total += info['target_count']\nprint(\"-\"*70)\nprint(f\"   {'TOTAL':<25} {total:>3} examples\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 4: The Preferences JSON Schema\n\nThe `<preferences>` JSON enables structured product recommendations:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Preferences JSON Schema\n\nPREFERENCES_SCHEMA = {\n    \"intent\": \"recommend|educate|compare|troubleshoot|out_of_scope\",\n    \"sweetness\": \"low|medium|high|null\",\n    \"bitterness\": \"low|medium|high|null\",\n    \"umami\": \"low|medium|high|null\",\n    \"body\": \"light|medium|full|null\",\n    \"serving\": \"straight|latte|miruku|null\",\n    \"experience\": \"beginner|intermediate|enthusiast|null\",\n    \"recommended_matcha\": [\"m-001\", \"m-003\", \"...\"],  # Product IDs\n    \"origin_preference\": [\"shiga\", \"uji\", \"nishio\"],  # or null\n    \"notes\": \"string or null\"\n}\n\nprint(\"üìã PREFERENCES JSON SCHEMA\")\nprint(\"=\"*70)\nprint(json.dumps(PREFERENCES_SCHEMA, indent=2))\nprint(\"\\nüí° This structured output enables:\")\nprint(\"   - Product card rendering in the UI\")\nprint(\"   - Filter/search based on preferences\")\nprint(\"   - Analytics on customer preferences\")\nprint(\"   - Personalized recommendation flows\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load all training data from the data/option-e-browser-llm/ directory\n\ndef load_training_data(source_dir: Path) -> List[Dict[str, Any]]:\n    \"\"\"\n    Load all training examples from JSON files in the source directory.\n    \n    Args:\n        source_dir: Path to the option-e-browser-llm data directory\n        \n    Returns:\n        List of training examples with messages format\n    \"\"\"\n    all_examples = []\n    \n    for category, info in CATEGORIES.items():\n        file_path = source_dir / info[\"file\"]\n        if file_path.exists():\n            with open(file_path) as f:\n                examples = json.load(f)\n                # Add category metadata to each example\n                for ex in examples:\n                    ex[\"category\"] = category\n                all_examples.extend(examples)\n                print(f\"   ‚úÖ Loaded {len(examples)} from {info['file']}\")\n        else:\n            print(f\"   ‚ö†Ô∏è File not found: {info['file']}\")\n    \n    return all_examples\n\nprint(\"üìÇ LOADING TRAINING DATA\")\nprint(\"=\"*70)\nALL_EXAMPLES = load_training_data(SOURCE_DATA_DIR)\nprint(f\"\\n   Total: {len(ALL_EXAMPLES)} examples loaded\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Using Pre-Generated Training Data\n#\n# The training dataset has been pre-generated externally and contains 300 examples.\n# This data is ready for use - no generation needed!\n#\n# The data was created using the methodology described in:\n#   ../scripts/option_e_dataset_generation_prompt.md\n#\n# If you ever need to regenerate or expand the dataset for your own domain,\n# you can reference that prompt template with Claude or GPT-4.\n\nprint(\"üìù PRE-GENERATED TRAINING DATA\")\nprint(\"=\"*70)\nprint(f\"   Current dataset: {len(ALL_EXAMPLES)} examples\")\nprint(f\"   Target minimum: {DATASET_CONFIG['min_train_examples'] + DATASET_CONFIG['min_val_examples'] + DATASET_CONFIG['min_test_examples']} examples\")\nprint()\nif len(ALL_EXAMPLES) >= 300:\n    print(\"   ‚úÖ Dataset meets the target of 300 examples!\")\n    print(\"   ‚úÖ No additional data generation required.\")\nelse:\n    print(f\"   ‚ö†Ô∏è Dataset has {len(ALL_EXAMPLES)} examples, target is 300\")\n    print(\"   To generate more examples:\")\n    print(\"   1. Open ../scripts/option_e_dataset_generation_prompt.md\")\n    print(\"   2. Use the prompt with Claude or GPT-4\")\n    print(\"   3. Add to the appropriate JSON file in data/option-e-browser-llm/\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 5: Data Validation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Data Validation Functions for Troscha format\n\ndef validate_example(example: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    Validate a single training example.\n    \n    Checks:\n    - Correct message format (3 messages)\n    - Required roles present (system, user, assistant)\n    - Content length requirements\n    - <preferences> JSON present in assistant response\n    - Valid category\n    \n    Args:\n        example: A training example dict with 'messages' key\n        \n    Returns:\n        Dict with is_valid (bool), errors (list), and warnings (list)\n    \"\"\"\n    errors: List[str] = []\n    warnings: List[str] = []\n    \n    messages = example.get(\"messages\", [])\n    \n    # Check message structure\n    if len(messages) != 3:\n        errors.append(f\"Expected 3 messages, got {len(messages)}\")\n        return {\"is_valid\": False, \"errors\": errors, \"warnings\": warnings}\n    \n    # Check roles\n    roles = [m.get(\"role\") for m in messages]\n    if roles != [\"system\", \"user\", \"assistant\"]:\n        errors.append(f\"Expected roles [system, user, assistant], got {roles}\")\n    \n    # Check content lengths\n    for msg in messages:\n        if len(msg.get(\"content\", \"\").strip()) < 10:\n            errors.append(f\"Message too short: {msg.get('role')}\")\n    \n    # Check for <preferences> JSON in assistant response\n    assistant_content = messages[2].get(\"content\", \"\")\n    if \"<preferences>\" not in assistant_content:\n        errors.append(\"Missing <preferences> JSON in assistant response\")\n    elif \"</preferences>\" not in assistant_content:\n        errors.append(\"Missing closing </preferences> tag\")\n    else:\n        # Try to parse the preferences JSON\n        try:\n            start = assistant_content.index(\"<preferences>\") + len(\"<preferences>\")\n            end = assistant_content.index(\"</preferences>\")\n            prefs_json = assistant_content[start:end]\n            json.loads(prefs_json)\n        except (ValueError, json.JSONDecodeError) as e:\n            errors.append(f\"Invalid <preferences> JSON: {e}\")\n    \n    # Check assistant response quality\n    if len(assistant_content) < 100:\n        warnings.append(\"Assistant response seems short (<100 chars)\")\n    if len(assistant_content) > 3000:\n        warnings.append(\"Assistant response very long (>3000 chars)\")\n    \n    return {\n        \"is_valid\": len(errors) == 0,\n        \"errors\": errors,\n        \"warnings\": warnings,\n    }\n\ndef validate_dataset(examples: List[Dict[str, Any]]) -> Dict[str, Any]:\n    \"\"\"\n    Validate entire dataset.\n    \n    Checks:\n    - Minimum example count\n    - Category distribution\n    - Individual example validation\n    - <preferences> JSON format\n    \n    Args:\n        examples: List of training example dicts\n        \n    Returns:\n        Dict with validation results including total, valid, invalid counts,\n        errors list, warnings list, and category_distribution dict\n    \"\"\"\n    results: Dict[str, Any] = {\n        \"total\": len(examples),\n        \"valid\": 0,\n        \"invalid\": 0,\n        \"errors\": [],\n        \"warnings\": [],\n        \"category_distribution\": {},\n    }\n    \n    # Validate each example\n    for i, ex in enumerate(examples):\n        validation = validate_example(ex)\n        if validation[\"is_valid\"]:\n            results[\"valid\"] += 1\n        else:\n            results[\"invalid\"] += 1\n            for error in validation[\"errors\"]:\n                results[\"errors\"].append(f\"Example {i}: {error}\")\n        \n        for warning in validation[\"warnings\"]:\n            results[\"warnings\"].append(f\"Example {i}: {warning}\")\n        \n        # Track category distribution\n        cat = ex.get(\"category\", \"unknown\")\n        results[\"category_distribution\"][cat] = results[\"category_distribution\"].get(cat, 0) + 1\n    \n    return results\n\nprint(\"‚úÖ Validation functions defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Validate loaded examples\n\nvalidation_results = validate_dataset(ALL_EXAMPLES)\n\nprint(\"üìã DATASET VALIDATION RESULTS\")\nprint(\"=\"*70)\nprint(f\"   Total Examples: {validation_results['total']}\")\nprint(f\"   Valid: {validation_results['valid']}\")\nprint(f\"   Invalid: {validation_results['invalid']}\")\n\nprint(f\"\\nüìä Category Distribution:\")\nfor cat, count in sorted(validation_results['category_distribution'].items()):\n    target = CATEGORIES.get(cat, {}).get('target_count', '?')\n    pct = count / validation_results['total'] * 100 if validation_results['total'] > 0 else 0\n    print(f\"   {cat:<25} {count:>3} ({pct:.0f}%)  target: {target}\")\n\nif validation_results['errors']:\n    print(f\"\\n‚ùå Errors (showing first 5):\")\n    for error in validation_results['errors'][:5]:\n        print(f\"   {error}\")\n\nif validation_results['warnings']:\n    print(f\"\\n‚ö†Ô∏è Warnings (showing first 5):\")\n    for warning in validation_results['warnings'][:5]:\n        print(f\"   {warning}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 6: Split and Save Dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def prepare_final_dataset(examples: List[Dict[str, Any]]) -> DatasetDict:\n    \"\"\"\n    Prepare the final dataset with train/val/test splits.\n    \n    This function:\n    1. Shuffles the examples\n    2. Splits into train (80%), validation (10%), test (10%)\n    3. Converts to Hugging Face Dataset format\n    \n    Args:\n        examples: List of example dicts with 'messages' and 'category' keys\n        \n    Returns:\n        DatasetDict with train, validation, and test splits\n    \"\"\"\n    # Shuffle\n    shuffled = examples.copy()\n    random.seed(42)  # Reproducibility\n    random.shuffle(shuffled)\n    \n    # Calculate split indices\n    n = len(shuffled)\n    train_end = int(n * DATASET_CONFIG[\"train_split\"])\n    val_end = train_end + int(n * DATASET_CONFIG[\"val_split\"])\n    \n    train_examples = shuffled[:train_end]\n    val_examples = shuffled[train_end:val_end]\n    test_examples = shuffled[val_end:]\n    \n    # Convert to dict format for Dataset\n    def examples_to_dict(exs: List[Dict[str, Any]]) -> Dict[str, List]:\n        return {\n            \"messages\": [ex[\"messages\"] for ex in exs],\n            \"category\": [ex.get(\"category\", \"unknown\") for ex in exs],\n        }\n    \n    # Create DatasetDict\n    dataset_dict = DatasetDict({\n        \"train\": Dataset.from_dict(examples_to_dict(train_examples)),\n        \"validation\": Dataset.from_dict(examples_to_dict(val_examples)),\n        \"test\": Dataset.from_dict(examples_to_dict(test_examples)),\n    })\n    \n    print(f\"üìä Dataset Splits:\")\n    print(f\"   Train: {len(train_examples)} examples\")\n    print(f\"   Validation: {len(val_examples)} examples\")\n    print(f\"   Test: {len(test_examples)} examples\")\n    \n    return dataset_dict\n\nprint(\"‚úÖ Dataset preparation function defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare and save the dataset\n\n# Use the loaded examples from external data files\nall_examples = ALL_EXAMPLES\n\n# Prepare dataset with train/val/test splits\ndataset = prepare_final_dataset(all_examples)\n\n# Save locally\ndataset_path = DATA_DIR / \"troscha-dataset\"\ndataset.save_to_disk(str(dataset_path))\n\nprint(f\"\\n‚úÖ Dataset saved to: {dataset_path}\")\n\n# Also save as JSON for inspection\njson_path = DATA_DIR / \"training_data.json\"\nwith open(json_path, 'w') as f:\n    json.dump(all_examples, f, indent=2)\n\nprint(f\"‚úÖ JSON backup saved to: {json_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify saved dataset\n\nfrom datasets import load_from_disk\n\nloaded_dataset = load_from_disk(str(dataset_path))\n\nprint(\"üìä LOADED DATASET\")\nprint(\"=\"*70)\nprint(loaded_dataset)\n\nprint(\"\\nüìù Sample Training Example:\")\nsample = loaded_dataset[\"train\"][0]\nprint(f\"   Category: {sample['category']}\")\nprint(f\"   User: {sample['messages'][1]['content'][:80]}...\")\nprint(f\"   Assistant: {sample['messages'][2]['content'][:100]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Issues\n",
    "\n",
    "### Issue 1: Duplicate Questions\n",
    "**Symptom:** Similar questions phrased slightly differently  \n",
    "**Fix:** Review for semantic duplicates before finalizing\n",
    "\n",
    "### Issue 2: Inconsistent Formatting\n",
    "**Symptom:** Some responses use lists, others don't  \n",
    "**Fix:** Establish a consistent style guide for responses\n",
    "\n",
    "### Issue 3: Factual Errors\n",
    "**Symptom:** Incorrect information in responses  \n",
    "**Fix:** Verify facts against authoritative sources\n",
    "\n",
    "### Issue 4: Responses Too Short\n",
    "**Symptom:** One-sentence answers  \n",
    "**Fix:** Expand with details, examples, and practical tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Metrics & Outputs\n\n| Metric | Target | Actual |\n|--------|--------|--------|\n| Total Examples | 300 | [Your value] |\n| Training Split | ~240 | [Your value] |\n| Validation Split | ~30 | [Your value] |\n| Test Split | ~30 | [Your value] |\n| Categories Covered | 12 | [Your value] |\n| Validation Pass Rate | 100% | [Your value] |\n\n**Expected ranges:**\n- Training examples: 230-250\n- Validation examples: 25-35\n- Test examples: 25-35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Phase Complete!\n\nYou've achieved:\n- ‚úÖ Understood the messages format for chat training\n- ‚úÖ Loaded pre-built training examples from external data files\n- ‚úÖ Implemented data validation with `<preferences>` JSON checking\n- ‚úÖ Saved dataset in Hugging Face format with train/val/test splits\n\n**Next:** [Lab 4.6.8.2: QLoRA Fine-Tuning](./lab-4.6.8.2-qlora-finetuning.ipynb)\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Phase 1 Complete!\")\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"   1. Review your dataset and add more examples if needed\")\n",
    "print(\"   2. Ensure balanced category distribution\")\n",
    "print(\"   3. Proceed to Lab 4.6.8.2 for QLoRA fine-tuning\")\n",
    "print(f\"\\n   Dataset location: {dataset_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}