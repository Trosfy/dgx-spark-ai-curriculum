{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.6.1: Project Planning & Architecture Design\n",
    "\n",
    "**Module:** 4.6 - Capstone Project (Domain 4: Production AI)\n",
    "**Time:** 4-6 hours\n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Design a complete system architecture for your project\n",
    "- [ ] Create detailed component specifications\n",
    "- [ ] Plan your DGX Spark memory budget\n",
    "- [ ] Define API contracts and interfaces\n",
    "- [ ] Plan your safety considerations üõ°Ô∏è\n",
    "- [ ] Set up evaluation infrastructure\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: `lab-4.6.0-project-kickoff.ipynb`\n",
    "- Selected: Your project option (A, B, C, or D)\n",
    "- Started: Initial project proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "At companies like Google, Meta, and OpenAI, engineers spend **20-30% of project time** on planning and design. This isn't wasted time - it's the foundation for everything that follows.\n",
    "\n",
    "### Why Architecture Matters\n",
    "\n",
    "| Without Architecture | With Architecture |\n",
    "|---------------------|-------------------|\n",
    "| \"Let me just start coding...\" | \"Here's what we're building...\" |\n",
    "| Constant rewrites | Incremental progress |\n",
    "| Integration nightmares | Clean interfaces |\n",
    "| \"It works on my machine\" | Reproducible everywhere |\n",
    "| Safety as afterthought | Safety built-in üõ°Ô∏è |\n",
    "\n",
    "This notebook guides you through the same planning process used in production AI systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: System Architecture\n",
    "\n",
    "> **Imagine you're building a treehouse.** Before picking up a hammer, you'd want to:\n",
    ">\n",
    "> 1. **Sketch a plan** - Where does the door go? How big is the window?\n",
    "> 2. **List materials** - How much wood? What kind of nails?\n",
    "> 3. **Plan the order** - Build the floor before the walls!\n",
    "> 4. **Think about safety** - Add railings so nobody falls! üõ°Ô∏è\n",
    "> 5. **Think about problems** - What if it rains during construction?\n",
    ">\n",
    "> **System architecture is your blueprint.** It shows:\n",
    "> - What pieces you're building (components)\n",
    "> - How they connect (interfaces)\n",
    "> - What they're made of (technologies)\n",
    "> - What order to build them (dependencies)\n",
    "> - How to keep it safe (guardrails)\n",
    ">\n",
    "> **Without a blueprint**, you might build the roof first and realize you can't attach it. With one, you build systematically and everything fits together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Architecture Patterns for AI Systems\n",
    "\n",
    "Let's explore common patterns used in production AI systems. Your project will combine elements from these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture Pattern Reference\n",
    "# These are the building blocks for your system design\n",
    "\n",
    "architecture_patterns = {\n",
    "    \"rag_pipeline\": {\n",
    "        \"name\": \"RAG Pipeline\",\n",
    "        \"description\": \"Retrieval-Augmented Generation for knowledge-grounded responses\",\n",
    "        \"components\": [\n",
    "            \"Document Loader\",\n",
    "            \"Chunker/Splitter\",\n",
    "            \"Embedding Model\",\n",
    "            \"Vector Store\",\n",
    "            \"Retriever (with reranking)\",\n",
    "            \"LLM Generator\",\n",
    "            \"Response Formatter\",\n",
    "        ],\n",
    "        \"data_flow\": \"Query ‚Üí Embed ‚Üí Search ‚Üí Retrieve ‚Üí Rerank ‚Üí Augment ‚Üí Generate ‚Üí Response\",\n",
    "        \"best_for\": [\"Option A\", \"Option B\"],\n",
    "        \"dgx_advantage\": \"Keep embedding model + LLM in memory simultaneously\",\n",
    "        \"diagram\": r\"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Query     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Embedder   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Vector DB   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                               ‚îÇ\n",
    "                                               ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Response   ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ     LLM     ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Documents  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        \"\"\"\n",
    "    },\n",
    "    \n",
    "    \"agent_orchestrator\": {\n",
    "        \"name\": \"Agent Orchestrator\",\n",
    "        \"description\": \"Central coordinator managing specialized agents with safety\",\n",
    "        \"components\": [\n",
    "            \"Orchestrator/Router\",\n",
    "            \"Task Planner\",\n",
    "            \"Agent Pool\",\n",
    "            \"Tool Registry\",\n",
    "            \"Memory Manager\",\n",
    "            \"Safety Layer üõ°Ô∏è\",\n",
    "            \"Human Approval Gate\",\n",
    "        ],\n",
    "        \"data_flow\": \"Task ‚Üí Plan ‚Üí Route ‚Üí Execute ‚Üí Verify Safety ‚Üí Aggregate ‚Üí Output\",\n",
    "        \"best_for\": [\"Option C\"],\n",
    "        \"dgx_advantage\": \"Run multiple smaller models concurrently\",\n",
    "        \"diagram\": r\"\"\"\n",
    "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                    ‚îÇ  Orchestrator   ‚îÇ\n",
    "                    ‚îÇ  + Safety üõ°Ô∏è    ‚îÇ\n",
    "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "              ‚ñº              ‚ñº              ‚ñº\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ Agent A  ‚îÇ  ‚îÇ Agent B  ‚îÇ  ‚îÇ Agent C  ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "             ‚îÇ             ‚îÇ             ‚îÇ\n",
    "             ‚ñº             ‚ñº             ‚ñº\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ Tools A  ‚îÇ  ‚îÇ Tools B  ‚îÇ  ‚îÇ Tools C  ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        \"\"\"\n",
    "    },\n",
    "    \n",
    "    \"training_pipeline\": {\n",
    "        \"name\": \"Training Pipeline\",\n",
    "        \"description\": \"End-to-end model training and deployment workflow\",\n",
    "        \"components\": [\n",
    "            \"Data Collector\",\n",
    "            \"Quality Filter\",\n",
    "            \"Preprocessor\",\n",
    "            \"Trainer (SFT/DPO)\",\n",
    "            \"Evaluator\",\n",
    "            \"Model Registry\",\n",
    "            \"Deployment Manager\",\n",
    "            \"Red Team Evaluator üõ°Ô∏è\",\n",
    "        ],\n",
    "        \"data_flow\": \"Collect ‚Üí Filter ‚Üí Clean ‚Üí Train ‚Üí Evaluate ‚Üí Safety Check ‚Üí Register ‚Üí Deploy\",\n",
    "        \"best_for\": [\"Option D\"],\n",
    "        \"dgx_advantage\": \"QLoRA for 100B+ models, full fine-tune for 16B\",\n",
    "        \"diagram\": r\"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Data   ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ  Clean  ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ  Train  ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ  Eval   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                               ‚îÇ\n",
    "                                     Pass?  ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ Fail?\n",
    "                                        ‚îÇ      ‚îÇ      ‚îÇ\n",
    "                                        ‚ñº      ‚îÇ      ‚ñº\n",
    "                                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                                   ‚îÇ Deploy  ‚îÇ ‚îÇ ‚îÇ Iterate ‚îÇ\n",
    "                                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                               ‚îÇ      ‚îÇ\n",
    "                                               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        \"\"\"\n",
    "    },\n",
    "    \n",
    "    \"multimodal_processor\": {\n",
    "        \"name\": \"Multimodal Processor\",\n",
    "        \"description\": \"Process and understand multiple data modalities\",\n",
    "        \"components\": [\n",
    "            \"Input Router\",\n",
    "            \"Vision Encoder (OCR/Layout)\",\n",
    "            \"Text Encoder\",\n",
    "            \"Fusion Layer\",\n",
    "            \"Vision-Language Model\",\n",
    "            \"Task Head\",\n",
    "            \"Output Formatter\",\n",
    "            \"Content Filter üõ°Ô∏è\",\n",
    "        ],\n",
    "        \"data_flow\": \"Input ‚Üí Route ‚Üí Encode ‚Üí Fuse ‚Üí Process ‚Üí Filter ‚Üí Format ‚Üí Output\",\n",
    "        \"best_for\": [\"Option B\"],\n",
    "        \"dgx_advantage\": \"34B VLMs with full resolution images\",\n",
    "        \"diagram\": r\"\"\"\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ  Image  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Vision  ‚îÇ‚îÄ‚îÄ‚îê\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ Encoder ‚îÇ  ‚îÇ\n",
    "                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                                         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Fusion  ‚îÇ\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ     ‚îÇ   VLM   ‚îÇ\n",
    "        ‚îÇ  Text   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Text   ‚îÇ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ Encoder ‚îÇ             ‚îÇ\n",
    "                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚ñº\n",
    "                                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                                              ‚îÇ  Output ‚îÇ\n",
    "                                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        \"\"\"\n",
    "    },\n",
    "}\n",
    "\n",
    "def show_pattern(pattern_name: str):\n",
    "    \"\"\"Display details of an architecture pattern.\"\"\"\n",
    "    pattern = architecture_patterns.get(pattern_name)\n",
    "    if not pattern:\n",
    "        print(f\"‚ùå Unknown pattern: {pattern_name}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüèóÔ∏è {pattern['name']}\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nüìù {pattern['description']}\")\n",
    "    print(f\"\\nüéØ Best for: {', '.join(pattern['best_for'])}\")\n",
    "    print(f\"üöÄ DGX Advantage: {pattern['dgx_advantage']}\")\n",
    "    print(f\"\\nüì¶ Components:\")\n",
    "    for comp in pattern['components']:\n",
    "        print(f\"   ‚Ä¢ {comp}\")\n",
    "    print(f\"\\nüîÑ Data Flow:\")\n",
    "    print(f\"   {pattern['data_flow']}\")\n",
    "    print(f\"\\nüìä Architecture:{pattern['diagram']}\")\n",
    "\n",
    "# Show all patterns\n",
    "for pattern in architecture_patterns:\n",
    "    show_pattern(pattern)\n",
    "    print(\"\\n\" + \"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "We explored four common architecture patterns:\n",
    "\n",
    "1. **RAG Pipeline** - For knowledge-grounded generation (Options A, B)\n",
    "2. **Agent Orchestrator** - For multi-agent coordination (Option C)\n",
    "3. **Training Pipeline** - For model development (Option D)\n",
    "4. **Multimodal Processor** - For mixed media processing (Option B)\n",
    "\n",
    "Your project will likely **combine elements** from multiple patterns. For example, an AI Assistant (Option A) might use:\n",
    "- RAG Pipeline for knowledge retrieval\n",
    "- Agent-like tool calling for actions\n",
    "- Safety layer from orchestrator pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Component Specification Template\n",
    "\n",
    "Every component in your system should be well-defined. Here's a template for component specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Optional\n",
    "from enum import Enum\n",
    "\n",
    "class ComponentStatus(Enum):\n",
    "    \"\"\"Status of a component in development.\"\"\"\n",
    "    PLANNED = \"üìã Planned\"\n",
    "    IN_PROGRESS = \"üîÑ In Progress\"\n",
    "    COMPLETE = \"‚úÖ Complete\"\n",
    "    BLOCKED = \"üö´ Blocked\"\n",
    "\n",
    "@dataclass\n",
    "class ComponentSpec:\n",
    "    \"\"\"\n",
    "    Specification for a system component.\n",
    "    \n",
    "    Use this to document each piece of your system.\n",
    "    \"\"\"\n",
    "    \n",
    "    name: str\n",
    "    purpose: str\n",
    "    inputs: List[Dict[str, str]]  # [{\"name\": ..., \"type\": ..., \"description\": ...}]\n",
    "    outputs: List[Dict[str, str]]\n",
    "    dependencies: List[str]  # Names of components this depends on\n",
    "    technologies: List[str]\n",
    "    estimated_hours: float\n",
    "    memory_gb: float = 0.0  # GPU memory requirement\n",
    "    status: ComponentStatus = ComponentStatus.PLANNED\n",
    "    safety_considerations: str = \"\"  # üõ°Ô∏è\n",
    "    notes: str = \"\"\n",
    "    \n",
    "    def to_markdown(self) -> str:\n",
    "        \"\"\"Generate markdown documentation for this component.\"\"\"\n",
    "        md = f\"## {self.name}\\n\\n\"\n",
    "        md += f\"**Purpose:** {self.purpose}\\n\\n\"\n",
    "        md += f\"**Status:** {self.status.value}\\n\\n\"\n",
    "        md += f\"**Estimated Hours:** {self.estimated_hours}\\n\\n\"\n",
    "        md += f\"**Memory Requirement:** {self.memory_gb} GB\\n\\n\"\n",
    "        \n",
    "        md += \"### Inputs\\n\\n\"\n",
    "        md += \"| Name | Type | Description |\\n\"\n",
    "        md += \"|------|------|-------------|\\n\"\n",
    "        for inp in self.inputs:\n",
    "            md += f\"| {inp['name']} | `{inp['type']}` | {inp['description']} |\\n\"\n",
    "        \n",
    "        md += \"\\n### Outputs\\n\\n\"\n",
    "        md += \"| Name | Type | Description |\\n\"\n",
    "        md += \"|------|------|-------------|\\n\"\n",
    "        for out in self.outputs:\n",
    "            md += f\"| {out['name']} | `{out['type']}` | {out['description']} |\\n\"\n",
    "        \n",
    "        md += f\"\\n### Dependencies\\n\\n\"\n",
    "        for dep in self.dependencies:\n",
    "            md += f\"- {dep}\\n\"\n",
    "        \n",
    "        md += f\"\\n### Technologies\\n\\n\"\n",
    "        for tech in self.technologies:\n",
    "            md += f\"- {tech}\\n\"\n",
    "        \n",
    "        if self.safety_considerations:\n",
    "            md += f\"\\n### Safety Considerations üõ°Ô∏è\\n\\n{self.safety_considerations}\\n\"\n",
    "        \n",
    "        if self.notes:\n",
    "            md += f\"\\n### Notes\\n\\n{self.notes}\\n\"\n",
    "        \n",
    "        return md\n",
    "\n",
    "# Example: RAG Retriever Component\n",
    "example_component = ComponentSpec(\n",
    "    name=\"RAG Retriever\",\n",
    "    purpose=\"Retrieve relevant documents from the knowledge base for a given query\",\n",
    "    inputs=[\n",
    "        {\"name\": \"query\", \"type\": \"str\", \"description\": \"User's natural language question\"},\n",
    "        {\"name\": \"top_k\", \"type\": \"int\", \"description\": \"Number of documents to retrieve\"},\n",
    "        {\"name\": \"filters\", \"type\": \"dict\", \"description\": \"Optional metadata filters\"},\n",
    "    ],\n",
    "    outputs=[\n",
    "        {\"name\": \"documents\", \"type\": \"List[Document]\", \"description\": \"Retrieved documents with scores\"},\n",
    "        {\"name\": \"metadata\", \"type\": \"dict\", \"description\": \"Retrieval metadata (time, scores, etc.)\"},\n",
    "    ],\n",
    "    dependencies=[\"Embedding Model\", \"Vector Store\"],\n",
    "    technologies=[\"sentence-transformers\", \"FAISS\", \"LangChain\"],\n",
    "    estimated_hours=4.0,\n",
    "    memory_gb=1.5,\n",
    "    safety_considerations=\"Filter retrieved content for sensitive information before passing to LLM.\",\n",
    "    notes=\"Consider hybrid search (dense + sparse) for better recall.\"\n",
    ")\n",
    "\n",
    "print(example_component.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Architecture Builder\n",
    "\n",
    "@dataclass\n",
    "class SystemArchitecture:\n",
    "    \"\"\"\n",
    "    Complete system architecture specification.\n",
    "    \n",
    "    Use this to plan your entire capstone project.\n",
    "    \"\"\"\n",
    "    \n",
    "    name: str\n",
    "    description: str\n",
    "    option: str  # A, B, C, or D\n",
    "    components: List[ComponentSpec] = field(default_factory=list)\n",
    "    \n",
    "    def add_component(self, component: ComponentSpec):\n",
    "        \"\"\"Add a component to the architecture.\"\"\"\n",
    "        self.components.append(component)\n",
    "    \n",
    "    def get_build_order(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get components in dependency order (topological sort).\n",
    "        Build these in order!\n",
    "        \"\"\"\n",
    "        # Build dependency graph\n",
    "        all_names = {c.name for c in self.components}\n",
    "        graph = {c.name: set(c.dependencies).intersection(all_names) for c in self.components}\n",
    "        \n",
    "        # Topological sort\n",
    "        order = []\n",
    "        remaining = set(graph.keys())\n",
    "        \n",
    "        while remaining:\n",
    "            # Find nodes with no unprocessed dependencies\n",
    "            ready = [\n",
    "                name for name in remaining \n",
    "                if not graph[name].intersection(remaining - {name})\n",
    "            ]\n",
    "            if not ready:\n",
    "                raise ValueError(\"Circular dependency detected!\")\n",
    "            \n",
    "            order.extend(sorted(ready))\n",
    "            for name in ready:\n",
    "                remaining.remove(name)\n",
    "        \n",
    "        return order\n",
    "    \n",
    "    def total_hours(self) -> float:\n",
    "        \"\"\"Calculate total estimated hours.\"\"\"\n",
    "        return sum(c.estimated_hours for c in self.components)\n",
    "    \n",
    "    def total_memory(self) -> float:\n",
    "        \"\"\"Calculate total memory requirement (may overlap).\"\"\"\n",
    "        return sum(c.memory_gb for c in self.components)\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"Print architecture summary.\"\"\"\n",
    "        status_counts = {}\n",
    "        for c in self.components:\n",
    "            status_counts[c.status.value] = status_counts.get(c.status.value, 0) + 1\n",
    "        \n",
    "        print(f\"\\nüèõÔ∏è ARCHITECTURE: {self.name}\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Option: {self.option}\")\n",
    "        print(f\"\\n{self.description}\\n\")\n",
    "        \n",
    "        print(f\"üì¶ Components ({len(self.components)}):\")\n",
    "        for c in self.components:\n",
    "            mem_str = f\" [{c.memory_gb}GB]\" if c.memory_gb > 0 else \"\"\n",
    "            print(f\"  {c.status.value} {c.name} ({c.estimated_hours}h){mem_str}\")\n",
    "        \n",
    "        print(f\"\\nüìä Status Summary:\")\n",
    "        for status, count in status_counts.items():\n",
    "            print(f\"  {status}: {count}\")\n",
    "        \n",
    "        print(f\"\\n‚è±Ô∏è Total Estimated Hours: {self.total_hours():.0f}\")\n",
    "        print(f\"üíæ Total Memory Estimate: {self.total_memory():.1f} GB\")\n",
    "        print(f\"   DGX Spark Available: 128 GB ‚Üí {'‚úÖ Fits!' if self.total_memory() < 110 else '‚ö†Ô∏è Review memory plan'}\")\n",
    "        \n",
    "        print(f\"\\nüî® Recommended Build Order:\")\n",
    "        for i, name in enumerate(self.get_build_order(), 1):\n",
    "            print(f\"  {i}. {name}\")\n",
    "\n",
    "# Example: Architecture for Option A (AI Assistant)\n",
    "print(\"\\nüìù EXAMPLE: Building an architecture for Option A\\n\")\n",
    "\n",
    "assistant_arch = SystemArchitecture(\n",
    "    name=\"AWS Infrastructure AI Assistant\",\n",
    "    description=\"Fine-tuned LLM with RAG, custom tools, safety guardrails, and streaming API\",\n",
    "    option=\"A\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add components to the example architecture\n",
    "\n",
    "assistant_arch.add_component(ComponentSpec(\n",
    "    name=\"Embedding Model\",\n",
    "    purpose=\"Convert text to vector embeddings for similarity search\",\n",
    "    inputs=[{\"name\": \"text\", \"type\": \"str\", \"description\": \"Text to embed\"}],\n",
    "    outputs=[{\"name\": \"embedding\", \"type\": \"np.ndarray\", \"description\": \"1024-dim vector\"}],\n",
    "    dependencies=[],\n",
    "    technologies=[\"sentence-transformers\", \"BGE-M3\"],\n",
    "    estimated_hours=2.0,\n",
    "    memory_gb=1.5,\n",
    "))\n",
    "\n",
    "assistant_arch.add_component(ComponentSpec(\n",
    "    name=\"Vector Store\",\n",
    "    purpose=\"Store and search document embeddings efficiently\",\n",
    "    inputs=[\n",
    "        {\"name\": \"embeddings\", \"type\": \"np.ndarray\", \"description\": \"Vectors to store\"},\n",
    "        {\"name\": \"query\", \"type\": \"np.ndarray\", \"description\": \"Query vector\"},\n",
    "    ],\n",
    "    outputs=[{\"name\": \"results\", \"type\": \"List[tuple]\", \"description\": \"(id, score) pairs\"}],\n",
    "    dependencies=[\"Embedding Model\"],\n",
    "    technologies=[\"FAISS-GPU\", \"ChromaDB\"],\n",
    "    estimated_hours=3.0,\n",
    "    memory_gb=2.0,\n",
    "))\n",
    "\n",
    "assistant_arch.add_component(ComponentSpec(\n",
    "    name=\"Document Processor\",\n",
    "    purpose=\"Parse, chunk, and prepare documents for indexing\",\n",
    "    inputs=[{\"name\": \"documents\", \"type\": \"List[Path]\", \"description\": \"Files to process\"}],\n",
    "    outputs=[{\"name\": \"chunks\", \"type\": \"List[Chunk]\", \"description\": \"Processed chunks\"}],\n",
    "    dependencies=[],\n",
    "    technologies=[\"LangChain\", \"unstructured\", \"PyPDF\"],\n",
    "    estimated_hours=4.0,\n",
    "    memory_gb=0.5,\n",
    "))\n",
    "\n",
    "assistant_arch.add_component(ComponentSpec(\n",
    "    name=\"RAG Retriever\",\n",
    "    purpose=\"Retrieve relevant context for user queries\",\n",
    "    inputs=[{\"name\": \"query\", \"type\": \"str\", \"description\": \"User question\"}],\n",
    "    outputs=[{\"name\": \"context\", \"type\": \"str\", \"description\": \"Retrieved context\"}],\n",
    "    dependencies=[\"Embedding Model\", \"Vector Store\", \"Document Processor\"],\n",
    "    technologies=[\"LangChain\", \"Hybrid Search\"],\n",
    "    estimated_hours=4.0,\n",
    "    memory_gb=0.5,\n",
    "    safety_considerations=\"Filter retrieved content for sensitive PII before use.\",\n",
    "))\n",
    "\n",
    "assistant_arch.add_component(ComponentSpec(\n",
    "    name=\"Fine-tuned LLM\",\n",
    "    purpose=\"Generate domain-specific responses using QLoRA-trained 70B model\",\n",
    "    inputs=[\n",
    "        {\"name\": \"prompt\", \"type\": \"str\", \"description\": \"System + user prompt\"},\n",
    "        {\"name\": \"context\", \"type\": \"str\", \"description\": \"RAG context\"},\n",
    "    ],\n",
    "    outputs=[{\"name\": \"response\", \"type\": \"str\", \"description\": \"Model response\"}],\n",
    "    dependencies=[],\n",
    "    technologies=[\"transformers\", \"PEFT\", \"bitsandbytes\"],\n",
    "    estimated_hours=12.0,\n",
    "    memory_gb=38.0,  # 70B in INT4\n",
    "    safety_considerations=\"Apply guardrails BEFORE returning response.\",\n",
    "))\n",
    "\n",
    "assistant_arch.add_component(ComponentSpec(\n",
    "    name=\"Tool Registry\",\n",
    "    purpose=\"Manage available tools and execute them safely\",\n",
    "    inputs=[{\"name\": \"tool_call\", \"type\": \"ToolCall\", \"description\": \"Tool request\"}],\n",
    "    outputs=[{\"name\": \"result\", \"type\": \"str\", \"description\": \"Tool output\"}],\n",
    "    dependencies=[],\n",
    "    technologies=[\"LangChain Tools\", \"custom\"],\n",
    "    estimated_hours=6.0,\n",
    "    memory_gb=0.1,\n",
    "    safety_considerations=\"Validate tool inputs, limit destructive operations.\",\n",
    "))\n",
    "\n",
    "assistant_arch.add_component(ComponentSpec(\n",
    "    name=\"Safety Guardrails üõ°Ô∏è\",\n",
    "    purpose=\"Filter inputs/outputs for safety violations\",\n",
    "    inputs=[{\"name\": \"text\", \"type\": \"str\", \"description\": \"Text to check\"}],\n",
    "    outputs=[{\"name\": \"safe\", \"type\": \"bool\", \"description\": \"Whether text is safe\"}],\n",
    "    dependencies=[],\n",
    "    technologies=[\"NeMo Guardrails\", \"Llama Guard\"],\n",
    "    estimated_hours=5.0,\n",
    "    memory_gb=4.0,  # Llama Guard 8B\n",
    "    safety_considerations=\"This IS the safety component!\",\n",
    "))\n",
    "\n",
    "assistant_arch.add_component(ComponentSpec(\n",
    "    name=\"Orchestrator\",\n",
    "    purpose=\"Coordinate RAG, LLM, tools, and safety for query handling\",\n",
    "    inputs=[{\"name\": \"user_message\", \"type\": \"str\", \"description\": \"User input\"}],\n",
    "    outputs=[{\"name\": \"response\", \"type\": \"AssistantResponse\", \"description\": \"Full response\"}],\n",
    "    dependencies=[\"RAG Retriever\", \"Fine-tuned LLM\", \"Tool Registry\", \"Safety Guardrails üõ°Ô∏è\"],\n",
    "    technologies=[\"custom\"],\n",
    "    estimated_hours=6.0,\n",
    "    memory_gb=0.1,\n",
    "))\n",
    "\n",
    "assistant_arch.add_component(ComponentSpec(\n",
    "    name=\"Streaming API\",\n",
    "    purpose=\"FastAPI endpoint with SSE streaming support\",\n",
    "    inputs=[{\"name\": \"request\", \"type\": \"ChatRequest\", \"description\": \"API request\"}],\n",
    "    outputs=[{\"name\": \"stream\", \"type\": \"AsyncGenerator\", \"description\": \"Token stream\"}],\n",
    "    dependencies=[\"Orchestrator\"],\n",
    "    technologies=[\"FastAPI\", \"SSE\", \"uvicorn\"],\n",
    "    estimated_hours=4.0,\n",
    "    memory_gb=0.1,\n",
    "))\n",
    "\n",
    "assistant_arch.add_component(ComponentSpec(\n",
    "    name=\"Gradio Demo\",\n",
    "    purpose=\"Interactive chat interface for demonstrations\",\n",
    "    inputs=[{\"name\": \"message\", \"type\": \"str\", \"description\": \"User message\"}],\n",
    "    outputs=[{\"name\": \"response\", \"type\": \"str\", \"description\": \"Assistant response\"}],\n",
    "    dependencies=[\"Streaming API\"],\n",
    "    technologies=[\"Gradio\"],\n",
    "    estimated_hours=3.0,\n",
    "    memory_gb=0.1,\n",
    "))\n",
    "\n",
    "# Display the architecture summary\n",
    "assistant_arch.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself\n",
    "\n",
    "Create a `SystemArchitecture` for YOUR chosen project. Use the template above as a starting point.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Component Ideas for Each Option</summary>\n",
    "\n",
    "**Option A (AI Assistant):** Use the example above!\n",
    "\n",
    "**Option B (Document Intelligence):**\n",
    "- PDF Parser, Image Processor, OCR Engine\n",
    "- Vision-Language Model (LLaVA/Qwen-VL)\n",
    "- Schema Extractor, Entity Recognizer\n",
    "- Multimodal RAG, QA System\n",
    "- Export Formatter (JSON, CSV)\n",
    "- Content Filter (PII detection)\n",
    "\n",
    "**Option C (Agent Swarm):**\n",
    "- Coordinator Agent\n",
    "- Specialized Agents (Research, Code, Data, etc.)\n",
    "- Tool Registry\n",
    "- Shared Memory Store\n",
    "- Human Approval Gate üõ°Ô∏è\n",
    "- Action Validator üõ°Ô∏è\n",
    "\n",
    "**Option D (Training Pipeline):**\n",
    "- Data Collector, Quality Filter\n",
    "- Preprocessor, Format Converter\n",
    "- SFT Trainer, DPO Trainer\n",
    "- Evaluation Suite\n",
    "- Model Registry (MLflow)\n",
    "- Red Team Evaluator üõ°Ô∏è\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: DGX Spark Memory Planning\n",
    "\n",
    "Your DGX Spark has 128GB unified memory. Let's plan how to use it effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DGX Spark Memory Planner\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "@dataclass\n",
    "class ModelFootprint:\n",
    "    \"\"\"Memory footprint of a model at different precisions.\"\"\"\n",
    "    name: str\n",
    "    params: str  # e.g., \"70B\"\n",
    "    fp32_gb: float\n",
    "    bf16_gb: float\n",
    "    int8_gb: float\n",
    "    int4_gb: float\n",
    "    nvfp4_gb: float  # Blackwell native\n",
    "\n",
    "# Common models and their footprints (approximate)\n",
    "MODEL_FOOTPRINTS = [\n",
    "    ModelFootprint(\"Llama 3.3 8B\", \"8B\", 32, 16, 8, 4.5, 4.5),\n",
    "    ModelFootprint(\"Llama 3.3 70B\", \"70B\", 280, 140, 70, 38, 38),\n",
    "    ModelFootprint(\"Llama 3.1 405B\", \"405B\", 1620, 810, 405, 210, 210),\n",
    "    ModelFootprint(\"Qwen2.5 7B\", \"7B\", 28, 14, 7, 4, 4),\n",
    "    ModelFootprint(\"Qwen2.5 72B\", \"72B\", 288, 144, 72, 40, 40),\n",
    "    ModelFootprint(\"LLaVA 1.6 34B\", \"34B\", 136, 68, 34, 18, 18),\n",
    "    ModelFootprint(\"Qwen2-VL 7B\", \"7B\", 28, 14, 7, 4, 4),\n",
    "    ModelFootprint(\"BGE-M3 (embedding)\", \"568M\", 2.3, 1.2, 0.6, 0.4, 0.4),\n",
    "    ModelFootprint(\"Llama Guard 3 8B\", \"8B\", 32, 16, 8, 4.5, 4.5),\n",
    "    ModelFootprint(\"Whisper Large v3\", \"1.5B\", 6, 3, 1.5, 0.8, 0.8),\n",
    "]\n",
    "\n",
    "DGX_SPARK_MEMORY_GB = 128.0\n",
    "SYSTEM_RESERVE_GB = 8.0  # Leave some headroom\n",
    "\n",
    "def plan_memory(\n",
    "    models: List[Tuple[str, str]],  # [(model_name, precision), ...]\n",
    "    additional_gb: float = 10.0,    # KV cache, activations, etc.\n",
    "    training: bool = False          # Training needs more memory\n",
    "):\n",
    "    \"\"\"\n",
    "    Plan memory usage for a set of models.\n",
    "    \n",
    "    Args:\n",
    "        models: List of (model_name, precision) tuples\n",
    "                precision: \"fp32\", \"bf16\", \"int8\", \"int4\", \"nvfp4\"\n",
    "        additional_gb: Extra memory for KV cache, activations\n",
    "        training: If True, add training overhead (gradients, optimizer)\n",
    "    \"\"\"\n",
    "    print(\"\\nüíæ DGX SPARK MEMORY PLAN\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Available: {DGX_SPARK_MEMORY_GB} GB (unified CPU+GPU)\")\n",
    "    print(f\"System Reserve: {SYSTEM_RESERVE_GB} GB\")\n",
    "    print(f\"Usable: {DGX_SPARK_MEMORY_GB - SYSTEM_RESERVE_GB} GB\\n\")\n",
    "    \n",
    "    total_used = 0\n",
    "    \n",
    "    print(\"üì¶ Model Allocations:\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for model_name, precision in models:\n",
    "        # Find model footprint\n",
    "        footprint = None\n",
    "        for m in MODEL_FOOTPRINTS:\n",
    "            if m.name.lower() == model_name.lower():\n",
    "                footprint = m\n",
    "                break\n",
    "        \n",
    "        if not footprint:\n",
    "            print(f\"  ‚ö†Ô∏è Unknown model: {model_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Get memory for precision\n",
    "        precision_map = {\n",
    "            \"fp32\": footprint.fp32_gb,\n",
    "            \"bf16\": footprint.bf16_gb,\n",
    "            \"fp16\": footprint.bf16_gb,\n",
    "            \"int8\": footprint.int8_gb,\n",
    "            \"int4\": footprint.int4_gb,\n",
    "            \"nvfp4\": footprint.nvfp4_gb,\n",
    "        }\n",
    "        memory = precision_map.get(precision.lower(), footprint.bf16_gb)\n",
    "        total_used += memory\n",
    "        \n",
    "        print(f\"  ‚Ä¢ {model_name:<25} ({precision:>6}): {memory:>6.1f} GB\")\n",
    "    \n",
    "    # Training overhead\n",
    "    training_overhead = 0\n",
    "    if training:\n",
    "        # Gradients + optimizer states for LoRA\n",
    "        training_overhead = 8.0\n",
    "        print(f\"  ‚Ä¢ Training overhead (LoRA):          {training_overhead:>6.1f} GB\")\n",
    "        total_used += training_overhead\n",
    "    \n",
    "    # Additional memory\n",
    "    print(f\"  ‚Ä¢ KV cache, activations, buffer:     {additional_gb:>6.1f} GB\")\n",
    "    total_used += additional_gb\n",
    "    \n",
    "    print(\"-\"*70)\n",
    "    remaining = DGX_SPARK_MEMORY_GB - SYSTEM_RESERVE_GB - total_used\n",
    "    \n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"  Total Model Memory: {total_used - additional_gb - training_overhead:.1f} GB\")\n",
    "    print(f\"  Total Used: {total_used:.1f} GB\")\n",
    "    print(f\"  Remaining: {remaining:.1f} GB\")\n",
    "    \n",
    "    # Status bar\n",
    "    used_pct = min(100, (total_used / (DGX_SPARK_MEMORY_GB - SYSTEM_RESERVE_GB)) * 100)\n",
    "    bar = \"‚ñà\" * int(used_pct / 2) + \"‚ñë\" * (50 - int(used_pct / 2))\n",
    "    print(f\"\\n  [{bar}] {used_pct:.0f}%\")\n",
    "    \n",
    "    if remaining < 0:\n",
    "        print(f\"\\n  ‚ùå OVER BUDGET by {-remaining:.1f} GB!\")\n",
    "        print(\"  Consider: Use INT4/NVFP4 or smaller models\")\n",
    "    elif remaining < 15:\n",
    "        print(f\"\\n  ‚ö†Ô∏è Tight on memory - reduce batch size if needed\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚úÖ Good memory headroom!\")\n",
    "    \n",
    "    return total_used, remaining\n",
    "\n",
    "# Example: Option A - AI Assistant\n",
    "print(\"\\nüéØ EXAMPLE: Option A - AI Assistant Memory Plan\")\n",
    "plan_memory([\n",
    "    (\"Llama 3.3 70B\", \"int4\"),        # Main LLM\n",
    "    (\"BGE-M3 (embedding)\", \"bf16\"),    # Embedding model\n",
    "    (\"Llama Guard 3 8B\", \"int4\"),      # Safety model\n",
    "], additional_gb=15.0, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More examples for other options\n",
    "\n",
    "print(\"\\nüéØ EXAMPLE: Option B - Document Intelligence Memory Plan\")\n",
    "plan_memory([\n",
    "    (\"LLaVA 1.6 34B\", \"int4\"),         # Vision-Language model\n",
    "    (\"BGE-M3 (embedding)\", \"bf16\"),    # Multimodal embedding\n",
    "], additional_gb=20.0)  # Higher for image processing\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "print(\"\\nüéØ EXAMPLE: Option C - Agent Swarm Memory Plan\")\n",
    "plan_memory([\n",
    "    (\"Llama 3.3 8B\", \"bf16\"),          # Coordinator\n",
    "    (\"Qwen2.5 7B\", \"bf16\"),            # Code agent\n",
    "    (\"Qwen2.5 7B\", \"bf16\"),            # Research agent\n",
    "    (\"BGE-M3 (embedding)\", \"bf16\"),    # Memory embedding\n",
    "    (\"Llama Guard 3 8B\", \"int4\"),      # Safety\n",
    "], additional_gb=15.0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "print(\"\\nüéØ EXAMPLE: Option D - Training Pipeline Memory Plan\")\n",
    "plan_memory([\n",
    "    (\"Llama 3.3 70B\", \"int4\"),         # Base model for QLoRA\n",
    "], additional_gb=20.0, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model footprint reference table\n",
    "\n",
    "print(\"\\nüìã MODEL FOOTPRINT REFERENCE\")\n",
    "print(\"=\"*90)\n",
    "print(f\"{'Model':<28} {'Params':<8} {'FP32':>8} {'BF16':>8} {'INT8':>8} {'INT4':>8} {'NVFP4':>8}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "for model in MODEL_FOOTPRINTS:\n",
    "    print(f\"{model.name:<28} {model.params:<8} {model.fp32_gb:>7.1f}G {model.bf16_gb:>7.1f}G \"\n",
    "          f\"{model.int8_gb:>7.1f}G {model.int4_gb:>7.1f}G {model.nvfp4_gb:>7.1f}G\")\n",
    "\n",
    "print(\"-\"*90)\n",
    "print(f\"\\nüí° DGX Spark Capacity: {DGX_SPARK_MEMORY_GB}GB unified memory\")\n",
    "print(\"   NVFP4 is exclusive to Blackwell architecture!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: API Contract Design\n",
    "\n",
    "If your project includes an API, defining contracts early prevents integration headaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Contract Templates using Pydantic\n",
    "\n",
    "from typing import List, Optional, Dict, Any\n",
    "from enum import Enum\n",
    "import json\n",
    "\n",
    "# Try Pydantic v2, fall back to v1\n",
    "try:\n",
    "    from pydantic import BaseModel, Field\n",
    "    PYDANTIC_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PYDANTIC_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Pydantic not installed. Run: pip install pydantic\")\n",
    "\n",
    "if PYDANTIC_AVAILABLE:\n",
    "    # Common schemas for AI projects\n",
    "    \n",
    "    class MessageRole(str, Enum):\n",
    "        SYSTEM = \"system\"\n",
    "        USER = \"user\"\n",
    "        ASSISTANT = \"assistant\"\n",
    "        TOOL = \"tool\"\n",
    "\n",
    "    class Message(BaseModel):\n",
    "        \"\"\"A chat message.\"\"\"\n",
    "        role: MessageRole = Field(description=\"Role of the message sender\")\n",
    "        content: str = Field(description=\"Message content\")\n",
    "        name: Optional[str] = Field(default=None, description=\"Name for tool messages\")\n",
    "\n",
    "    class ToolCall(BaseModel):\n",
    "        \"\"\"A tool call request.\"\"\"\n",
    "        id: str = Field(description=\"Unique tool call ID\")\n",
    "        name: str = Field(description=\"Tool name\")\n",
    "        arguments: Dict[str, Any] = Field(description=\"Tool arguments\")\n",
    "\n",
    "    class ChatRequest(BaseModel):\n",
    "        \"\"\"Request to the chat API.\"\"\"\n",
    "        messages: List[Message] = Field(description=\"Conversation history\")\n",
    "        stream: bool = Field(default=True, description=\"Enable streaming\")\n",
    "        temperature: float = Field(default=0.7, ge=0, le=2, description=\"Sampling temperature\")\n",
    "        max_tokens: int = Field(default=2048, ge=1, le=8192, description=\"Max tokens to generate\")\n",
    "        tools: Optional[List[Dict]] = Field(default=None, description=\"Available tools\")\n",
    "\n",
    "    class UsageStats(BaseModel):\n",
    "        \"\"\"Token usage statistics.\"\"\"\n",
    "        prompt_tokens: int\n",
    "        completion_tokens: int\n",
    "        total_tokens: int\n",
    "\n",
    "    class ChatResponse(BaseModel):\n",
    "        \"\"\"Response from the chat API.\"\"\"\n",
    "        id: str = Field(description=\"Response ID\")\n",
    "        message: Message = Field(description=\"Assistant's response\")\n",
    "        tool_calls: Optional[List[ToolCall]] = Field(default=None, description=\"Tool calls if any\")\n",
    "        sources: List[Dict[str, Any]] = Field(default=[], description=\"Retrieved sources\")\n",
    "        usage: UsageStats = Field(description=\"Token usage\")\n",
    "        latency_ms: float = Field(description=\"Response latency\")\n",
    "        safety_filtered: bool = Field(default=False, description=\"Whether safety filter was triggered\")\n",
    "\n",
    "    class HealthResponse(BaseModel):\n",
    "        \"\"\"Health check response.\"\"\"\n",
    "        status: str = Field(description=\"Service status\")\n",
    "        model_loaded: bool = Field(description=\"Whether model is loaded\")\n",
    "        gpu_memory_used_gb: float = Field(description=\"GPU memory in use\")\n",
    "        guardrails_active: bool = Field(default=True, description=\"Safety guardrails status\")\n",
    "\n",
    "    print(\"‚úÖ API schemas defined with Pydantic\")\n",
    "    \n",
    "    # Show example request\n",
    "    example_request = ChatRequest(\n",
    "        messages=[\n",
    "            Message(role=MessageRole.SYSTEM, content=\"You are a helpful assistant.\"),\n",
    "            Message(role=MessageRole.USER, content=\"How do I create an S3 bucket?\"),\n",
    "        ],\n",
    "        stream=True,\n",
    "        temperature=0.7,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüìã Example ChatRequest:\")\n",
    "    print(example_request.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Endpoint Documentation\n",
    "\n",
    "api_endpoints = [\n",
    "    {\n",
    "        \"method\": \"POST\",\n",
    "        \"path\": \"/v1/chat/completions\",\n",
    "        \"description\": \"Send messages and get AI response (OpenAI compatible)\",\n",
    "        \"request\": \"ChatRequest\",\n",
    "        \"response\": \"ChatResponse or SSE stream\",\n",
    "        \"safety\": \"Input validated by guardrails before processing\",\n",
    "    },\n",
    "    {\n",
    "        \"method\": \"POST\",\n",
    "        \"path\": \"/v1/embeddings\",\n",
    "        \"description\": \"Generate embeddings for text\",\n",
    "        \"request\": \"{input: str | List[str]}\",\n",
    "        \"response\": \"{embeddings: List[List[float]]}\",\n",
    "        \"safety\": \"N/A\",\n",
    "    },\n",
    "    {\n",
    "        \"method\": \"POST\",\n",
    "        \"path\": \"/v1/documents\",\n",
    "        \"description\": \"Upload documents to knowledge base\",\n",
    "        \"request\": \"{files: List[File], collection: str}\",\n",
    "        \"response\": \"{document_ids: List[str], chunks_created: int}\",\n",
    "        \"safety\": \"Files scanned for malicious content\",\n",
    "    },\n",
    "    {\n",
    "        \"method\": \"POST\",\n",
    "        \"path\": \"/v1/search\",\n",
    "        \"description\": \"Search the knowledge base\",\n",
    "        \"request\": \"{query: str, top_k: int, filters: dict}\",\n",
    "        \"response\": \"{results: List[SearchResult]}\",\n",
    "        \"safety\": \"N/A\",\n",
    "    },\n",
    "    {\n",
    "        \"method\": \"GET\",\n",
    "        \"path\": \"/health\",\n",
    "        \"description\": \"Health check endpoint\",\n",
    "        \"request\": \"None\",\n",
    "        \"response\": \"HealthResponse\",\n",
    "        \"safety\": \"Reports guardrails status\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"\\nüîå API ENDPOINTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for endpoint in api_endpoints:\n",
    "    print(f\"\\n{endpoint['method']} {endpoint['path']}\")\n",
    "    print(f\"  üìù {endpoint['description']}\")\n",
    "    print(f\"  ‚û°Ô∏è Request: {endpoint['request']}\")\n",
    "    print(f\"  ‚¨ÖÔ∏è Response: {endpoint['response']}\")\n",
    "    print(f\"  üõ°Ô∏è Safety: {endpoint['safety']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Safety Planning üõ°Ô∏è\n",
    "\n",
    "Every capstone project must include safety considerations. Let's plan yours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safety Planning Template\n",
    "\n",
    "@dataclass\n",
    "class SafetyPlan:\n",
    "    \"\"\"Safety plan for your capstone project.\"\"\"\n",
    "    \n",
    "    project_option: str\n",
    "    input_validation: List[str]      # How you validate inputs\n",
    "    output_filtering: List[str]      # How you filter outputs\n",
    "    guardrails_used: List[str]       # What guardrails you're using\n",
    "    human_oversight: List[str]       # Where humans are in the loop\n",
    "    evaluation_plan: List[str]       # How you'll evaluate safety\n",
    "    risk_mitigations: Dict[str, str] # Risk -> Mitigation\n",
    "    \n",
    "    def display(self):\n",
    "        print(f\"\\nüõ°Ô∏è SAFETY PLAN - Option {self.project_option}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\nüì• Input Validation:\")\n",
    "        for item in self.input_validation:\n",
    "            print(f\"   ‚Ä¢ {item}\")\n",
    "        \n",
    "        print(\"\\nüì§ Output Filtering:\")\n",
    "        for item in self.output_filtering:\n",
    "            print(f\"   ‚Ä¢ {item}\")\n",
    "        \n",
    "        print(\"\\nüöß Guardrails:\")\n",
    "        for item in self.guardrails_used:\n",
    "            print(f\"   ‚Ä¢ {item}\")\n",
    "        \n",
    "        print(\"\\nüë§ Human Oversight:\")\n",
    "        for item in self.human_oversight:\n",
    "            print(f\"   ‚Ä¢ {item}\")\n",
    "        \n",
    "        print(\"\\nüìä Safety Evaluation:\")\n",
    "        for item in self.evaluation_plan:\n",
    "            print(f\"   ‚Ä¢ {item}\")\n",
    "        \n",
    "        print(\"\\n‚ö†Ô∏è Risk Mitigations:\")\n",
    "        for risk, mitigation in self.risk_mitigations.items():\n",
    "            print(f\"   ‚Ä¢ {risk}\")\n",
    "            print(f\"     ‚Üí {mitigation}\")\n",
    "\n",
    "# Example safety plan for Option A\n",
    "option_a_safety = SafetyPlan(\n",
    "    project_option=\"A\",\n",
    "    input_validation=[\n",
    "        \"Check input length (max 4096 tokens)\",\n",
    "        \"Run through Llama Guard for harmful content\",\n",
    "        \"Detect and reject prompt injection attempts\",\n",
    "        \"Rate limiting per user\",\n",
    "    ],\n",
    "    output_filtering=[\n",
    "        \"Run all outputs through NeMo Guardrails\",\n",
    "        \"Filter PII from responses\",\n",
    "        \"Check for hallucinated commands (AWS)\",\n",
    "        \"Validate code snippets before returning\",\n",
    "    ],\n",
    "    guardrails_used=[\n",
    "        \"NeMo Guardrails with custom rails\",\n",
    "        \"Llama Guard 3 for content classification\",\n",
    "        \"Custom AWS command validator\",\n",
    "    ],\n",
    "    human_oversight=[\n",
    "        \"Destructive AWS commands require confirmation\",\n",
    "        \"Production deployment changes need approval\",\n",
    "        \"Logging all interactions for review\",\n",
    "    ],\n",
    "    evaluation_plan=[\n",
    "        \"Test with red team prompts (PromptFoo)\",\n",
    "        \"Run harmful content benchmark\",\n",
    "        \"Measure guardrail false positive rate\",\n",
    "        \"User study for edge cases\",\n",
    "    ],\n",
    "    risk_mitigations={\n",
    "        \"Harmful output\": \"Multi-layer filtering: Llama Guard + NeMo + custom rules\",\n",
    "        \"Prompt injection\": \"Input sanitization + jailbreak detection\",\n",
    "        \"Dangerous commands\": \"Whitelist safe commands, require confirmation for others\",\n",
    "        \"Data leakage\": \"PII detection and masking in all outputs\",\n",
    "    }\n",
    ")\n",
    "\n",
    "option_a_safety.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Evaluation Planning\n",
    "\n",
    "How will you know if your project is successful? Define metrics now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Plan Template\n",
    "\n",
    "@dataclass\n",
    "class EvaluationMetric:\n",
    "    \"\"\"An evaluation metric definition.\"\"\"\n",
    "    name: str\n",
    "    description: str\n",
    "    target: str\n",
    "    measurement: str\n",
    "    frequency: str\n",
    "\n",
    "@dataclass\n",
    "class EvaluationPlan:\n",
    "    \"\"\"Complete evaluation plan.\"\"\"\n",
    "    project_name: str\n",
    "    metrics: List[EvaluationMetric]\n",
    "    datasets: List[Dict[str, str]]\n",
    "    baselines: List[str]\n",
    "    safety_metrics: List[EvaluationMetric]  # üõ°Ô∏è\n",
    "    \n",
    "    def display(self):\n",
    "        print(f\"\\nüìä EVALUATION PLAN: {self.project_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\nüìè Performance Metrics:\")\n",
    "        for m in self.metrics:\n",
    "            print(f\"\\n  üìå {m.name}\")\n",
    "            print(f\"     {m.description}\")\n",
    "            print(f\"     Target: {m.target}\")\n",
    "            print(f\"     Measured by: {m.measurement}\")\n",
    "            print(f\"     Frequency: {m.frequency}\")\n",
    "        \n",
    "        print(\"\\nüõ°Ô∏è Safety Metrics:\")\n",
    "        for m in self.safety_metrics:\n",
    "            print(f\"\\n  üìå {m.name}\")\n",
    "            print(f\"     {m.description}\")\n",
    "            print(f\"     Target: {m.target}\")\n",
    "        \n",
    "        print(\"\\nüìö Evaluation Datasets:\")\n",
    "        for ds in self.datasets:\n",
    "            print(f\"  ‚Ä¢ {ds['name']}: {ds['size']} samples from {ds['source']}\")\n",
    "        \n",
    "        print(\"\\nüéØ Baselines for Comparison:\")\n",
    "        for b in self.baselines:\n",
    "            print(f\"  ‚Ä¢ {b}\")\n",
    "\n",
    "# Example evaluation plan for Option A\n",
    "option_a_eval = EvaluationPlan(\n",
    "    project_name=\"AWS AI Assistant\",\n",
    "    metrics=[\n",
    "        EvaluationMetric(\n",
    "            name=\"Answer Accuracy\",\n",
    "            description=\"Correct answers on AWS-specific test set\",\n",
    "            target=\"‚â• 80%\",\n",
    "            measurement=\"Human eval + automated checks\",\n",
    "            frequency=\"Weekly + final\"\n",
    "        ),\n",
    "        EvaluationMetric(\n",
    "            name=\"Retrieval Recall@5\",\n",
    "            description=\"Relevant docs in top 5 retrieved\",\n",
    "            target=\"‚â• 90%\",\n",
    "            measurement=\"Test with known-answer queries\",\n",
    "            frequency=\"After RAG changes\"\n",
    "        ),\n",
    "        EvaluationMetric(\n",
    "            name=\"Response Latency (P95)\",\n",
    "            description=\"95th percentile response time\",\n",
    "            target=\"< 3 seconds\",\n",
    "            measurement=\"API load testing\",\n",
    "            frequency=\"After optimization\"\n",
    "        ),\n",
    "        EvaluationMetric(\n",
    "            name=\"User Satisfaction\",\n",
    "            description=\"Helpfulness rating\",\n",
    "            target=\"‚â• 4.0/5.0\",\n",
    "            measurement=\"Demo session feedback\",\n",
    "            frequency=\"Demo sessions\"\n",
    "        ),\n",
    "    ],\n",
    "    safety_metrics=[\n",
    "        EvaluationMetric(\n",
    "            name=\"Harmful Output Rate\",\n",
    "            description=\"% of outputs flagged as harmful\",\n",
    "            target=\"< 0.1%\",\n",
    "            measurement=\"Red team prompts + Llama Guard\",\n",
    "            frequency=\"Final evaluation\"\n",
    "        ),\n",
    "        EvaluationMetric(\n",
    "            name=\"Guardrail False Positive Rate\",\n",
    "            description=\"% of safe outputs incorrectly blocked\",\n",
    "            target=\"< 2%\",\n",
    "            measurement=\"Benign test set\",\n",
    "            frequency=\"After guardrail changes\"\n",
    "        ),\n",
    "        EvaluationMetric(\n",
    "            name=\"Jailbreak Success Rate\",\n",
    "            description=\"% of jailbreak attempts that succeed\",\n",
    "            target=\"< 1%\",\n",
    "            measurement=\"Jailbreak prompt suite\",\n",
    "            frequency=\"Final evaluation\"\n",
    "        ),\n",
    "    ],\n",
    "    datasets=[\n",
    "        {\"name\": \"AWS FAQ Test Set\", \"source\": \"Curated from AWS forums\", \"size\": \"100\"},\n",
    "        {\"name\": \"CLI Command Dataset\", \"source\": \"AWS CLI docs\", \"size\": \"500\"},\n",
    "        {\"name\": \"Safety Red Team Set\", \"source\": \"PromptFoo + custom\", \"size\": \"200\"},\n",
    "    ],\n",
    "    baselines=[\n",
    "        \"Raw Llama 3.3 70B (no fine-tuning, no RAG)\",\n",
    "        \"GPT-4 with AWS docs in context\",\n",
    "        \"AWS official documentation search\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "option_a_eval.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Planning Mistakes\n",
    "\n",
    "### Mistake 1: Vague Architecture\n",
    "```python\n",
    "# ‚ùå Too vague\n",
    "components = [\"data stuff\", \"model\", \"api\"]\n",
    "\n",
    "# ‚úÖ Specific and actionable\n",
    "components = [\n",
    "    \"DocumentParser: Extract text from PDFs using pypdf2\",\n",
    "    \"ChunkingService: 512-token chunks with 50 overlap\",\n",
    "    \"EmbeddingService: BGE-M3 for 1024-dim vectors\",\n",
    "    \"VectorStore: FAISS-GPU IVF index\",\n",
    "]\n",
    "```\n",
    "\n",
    "### Mistake 2: No Memory Planning\n",
    "```python\n",
    "# ‚ùå \"I'll just load everything\"\n",
    "models = [\"llama-70b-fp16\", \"llava-34b-fp16\"]  # 140+68 = 208 GB! üí•\n",
    "\n",
    "# ‚úÖ Plan memory carefully\n",
    "models = [\n",
    "    (\"llama-70b\", \"int4\", 38),      # 38 GB\n",
    "    (\"bge-m3\", \"bf16\", 1.2),        # 1.2 GB  \n",
    "    (\"overhead\", \"-\", 15),          # 15 GB for KV cache\n",
    "]  # Total: 54.2 GB - plenty of headroom!\n",
    "```\n",
    "\n",
    "### Mistake 3: Safety as Afterthought\n",
    "```python\n",
    "# ‚ùå \"I'll add guardrails at the end\"\n",
    "week_6 = [\"Add safety stuff\", \"Fix issues\", \"Demo\"]\n",
    "\n",
    "# ‚úÖ Safety built in from the start\n",
    "week_1 = [\"Plan safety architecture\"]\n",
    "week_4 = [\"Implement guardrails\", \"Safety testing\"]\n",
    "week_5 = [\"Red team evaluation\", \"Fix safety gaps\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've completed project planning! You should now have:\n",
    "\n",
    "- ‚úÖ System architecture with all components defined\n",
    "- ‚úÖ Build order based on dependencies\n",
    "- ‚úÖ Memory plan for DGX Spark\n",
    "- ‚úÖ API contracts (if applicable)\n",
    "- ‚úÖ Safety plan üõ°Ô∏è\n",
    "- ‚úÖ Evaluation plan with metrics and datasets\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "1. **Document your architecture** in `docs/architecture.md`\n",
    "\n",
    "2. **Complete your project proposal** using `templates/project-proposal.md`\n",
    "\n",
    "3. **Open your project-specific guide:**\n",
    "   - Option A: `lab-4.6.2-option-a-ai-assistant.ipynb`\n",
    "   - Option B: `lab-4.6.3-option-b-document-intelligence.ipynb`\n",
    "   - Option C: `lab-4.6.4-option-c-agent-swarm.ipynb`\n",
    "   - Option D: `lab-4.6.5-option-d-training-pipeline.ipynb`\n",
    "\n",
    "4. **Start building!**\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [Designing Machine Learning Systems](https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/) by Chip Huyen\n",
    "- [Building LLM Applications](https://huyenchip.com/2023/04/11/llm-engineering.html)\n",
    "- [NeMo Guardrails Documentation](https://github.com/NVIDIA/NeMo-Guardrails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßπ Cleanup\n",
    "print(\"‚úÖ No cleanup needed - your architecture plans are saved!\")\n",
    "print(\"\\nüìù Next: Open your project-specific guide and start implementing.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
