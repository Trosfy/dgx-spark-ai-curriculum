{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.6.5: Option D - Custom Training Pipeline\n",
    "\n",
    "**Module:** 4.6 - Capstone Project (Domain 4: Production AI)\n",
    "**Time:** 35-45 hours total\n",
    "**Difficulty:** â­â­â­â­â­\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Project Overview\n",
    "\n",
    "Build **infrastructure for continuous model improvement**:\n",
    "\n",
    "| Component | Description | DGX Spark Advantage |\n",
    "|-----------|-------------|---------------------|\n",
    "| **Data Pipeline** | Collection & curation | High-speed preprocessing |\n",
    "| **Multi-stage Training** | SFT â†’ DPO â†’ RLHF | Full fine-tuning of 16B models |\n",
    "| **Automated Eval** | Benchmark suite | Run evals without moving models |\n",
    "| **Model Registry** | Versioning & comparison | Store multiple model versions |\n",
    "| **A/B Testing** | Compare model variants | Side-by-side inference |\n",
    "| **Deployment** | Automated model updates | Fast model swapping |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By completing this project, you will:\n",
    "- [ ] Build a data collection and curation pipeline\n",
    "- [ ] Implement multiple fine-tuning approaches (SFT, DPO)\n",
    "- [ ] Create automated evaluation benchmarks\n",
    "- [ ] Build model versioning and comparison tools\n",
    "- [ ] Implement A/B testing framework\n",
    "- [ ] Create automated deployment pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸŒ Real-World Context\n",
    "\n",
    "ML training pipelines are the backbone of AI companies:\n",
    "\n",
    "| Company | Pipeline Scale | Key Features |\n",
    "|---------|---------------|---------------|\n",
    "| **OpenAI** | Thousands of experiments | RLHF, Constitutional AI |\n",
    "| **Anthropic** | Continuous training | CAI, safety training |\n",
    "| **Google** | Distributed training | Gemini, multimodal |\n",
    "| **Meta** | Open models | Llama training infrastructure |\n",
    "| **Hugging Face** | Community | AutoTrain, evaluation hub |\n",
    "\n",
    "A good training pipeline can reduce iteration time from weeks to hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§’ ELI5: What is a Training Pipeline?\n",
    "\n",
    "> **Imagine you're running a cooking school** that needs to get better over time:\n",
    ">\n",
    "> 1. **Collect recipes** (data collection) - Gather the best recipes from everywhere\n",
    "> 2. **Test recipes** (data curation) - Remove bad ones, keep good ones\n",
    "> 3. **Train students** (fine-tuning) - Teach them with the good recipes\n",
    "> 4. **Grade students** (evaluation) - See who learned best\n",
    "> 5. **Pick the best** (model selection) - Graduate the top performers\n",
    "> 6. **Open restaurants** (deployment) - Put them to work!\n",
    "> 7. **Get feedback** (monitoring) - Learn what to improve\n",
    "> 8. **Repeat!** - Keep getting better\n",
    ">\n",
    "> A **training pipeline** automates this whole process for AI models:\n",
    "> - New data comes in automatically\n",
    "> - Models are trained and tested automatically\n",
    "> - The best models are deployed automatically\n",
    "> - Everything is tracked and versioned\n",
    ">\n",
    "> The result? Your AI gets better continuously without manual intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ—ï¸ System Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      Custom Training Pipeline                           â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                          â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚                     DATA PIPELINE                                 â”‚   â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚\n",
    "â”‚  â”‚  â”‚Collect â”‚â”€â–¶â”‚ Clean  â”‚â”€â–¶â”‚ Format â”‚â”€â–¶â”‚ Split  â”‚â”€â–¶â”‚ Store  â”‚    â”‚   â”‚\n",
    "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                               â”‚                                          â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚                    TRAINING STAGES                                â”‚   â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚   â”‚\n",
    "â”‚  â”‚  â”‚  SFT   â”‚â”€â–¶â”‚  DPO   â”‚â”€â–¶â”‚ Safety â”‚â”€â–¶â”‚ Merge  â”‚                 â”‚   â”‚\n",
    "â”‚  â”‚  â”‚Trainingâ”‚  â”‚Trainingâ”‚  â”‚Trainingâ”‚  â”‚Adaptersâ”‚                 â”‚   â”‚\n",
    "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                               â”‚                                          â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚                    EVALUATION & REGISTRY                          â”‚   â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚   â”‚\n",
    "â”‚  â”‚  â”‚Bench-  â”‚â”€â–¶â”‚Compare â”‚â”€â–¶â”‚Registerâ”‚â”€â–¶â”‚Version â”‚                 â”‚   â”‚\n",
    "â”‚  â”‚  â”‚marks   â”‚  â”‚Baselineâ”‚  â”‚ Model  â”‚  â”‚Control â”‚                 â”‚   â”‚\n",
    "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                               â”‚                                          â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚                    DEPLOYMENT & TESTING                           â”‚   â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚   â”‚\n",
    "â”‚  â”‚  â”‚A/B Testâ”‚â”€â–¶â”‚ Deploy â”‚â”€â–¶â”‚Monitor â”‚â”€â–¶â”‚Feedbackâ”‚                 â”‚   â”‚\n",
    "â”‚  â”‚  â”‚ Setup  â”‚  â”‚ Winner â”‚  â”‚ Metricsâ”‚  â”‚  Loop  â”‚                 â”‚   â”‚\n",
    "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                                                                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional, Callable\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "import json\n",
    "import hashlib\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"ğŸ”§ OPTION D: CUSTOM TRAINING PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"\\nGPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'Not available'}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Planning for Training Pipeline\n",
    "\n",
    "def plan_option_d_memory():\n",
    "    \"\"\"\n",
    "    Plan memory for training pipeline.\n",
    "    \n",
    "    Key insight: We don't need all components loaded simultaneously.\n",
    "    Pipeline stages run sequentially.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nğŸ’¾ DGX SPARK MEMORY PLAN - OPTION D\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Strategy: Sequential stages, not concurrent\")\n",
    "    \n",
    "    stages = [\n",
    "        {\n",
    "            \"name\": \"Stage 1: Data Processing\",\n",
    "            \"components\": [\n",
    "                (\"Data loading buffers\", 10.0),\n",
    "                (\"Text processing\", 4.0),\n",
    "                (\"Tokenization cache\", 6.0),\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Stage 2: SFT Training (16B Model)\",\n",
    "            \"components\": [\n",
    "                (\"Base model (16B INT4)\", 10.0),\n",
    "                (\"LoRA adapters\", 2.0),\n",
    "                (\"Optimizer states\", 8.0),\n",
    "                (\"Activations (grad ckpt)\", 12.0),\n",
    "                (\"Data loader\", 4.0),\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Stage 3: DPO Training\",\n",
    "            \"components\": [\n",
    "                (\"Policy model (16B INT4)\", 10.0),\n",
    "                (\"Reference model (16B INT4)\", 10.0),\n",
    "                (\"LoRA adapters\", 2.0),\n",
    "                (\"Optimizer states\", 8.0),\n",
    "                (\"Preference pairs\", 4.0),\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Stage 4: Evaluation\",\n",
    "            \"components\": [\n",
    "                (\"Model under test (16B)\", 10.0),\n",
    "                (\"Eval datasets\", 2.0),\n",
    "                (\"Metrics computation\", 2.0),\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Stage 5: A/B Testing (Inference)\",\n",
    "            \"components\": [\n",
    "                (\"Model A (16B INT4)\", 10.0),\n",
    "                (\"Model B (16B INT4)\", 10.0),\n",
    "                (\"Routing logic\", 1.0),\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    max_stage_memory = 0\n",
    "    \n",
    "    for stage in stages:\n",
    "        stage_total = sum(c[1] for c in stage[\"components\"])\n",
    "        max_stage_memory = max(max_stage_memory, stage_total)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š {stage['name']}\")\n",
    "        print(\"-\"*50)\n",
    "        for name, mem in stage[\"components\"]:\n",
    "            print(f\"   {name:<30} {mem:>6.1f} GB\")\n",
    "        print(f\"   {'STAGE TOTAL':<30} {stage_total:>6.1f} GB\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Peak memory (Stage 3 - DPO): {max_stage_memory:.1f} GB\")\n",
    "    print(f\"DGX Spark available: 128 GB\")\n",
    "    print(f\"Headroom: {128 - max_stage_memory:.1f} GB\")\n",
    "    print(\"\\nâœ… All stages fit! Can even use larger models (up to ~40B for SFT)\")\n",
    "    \n",
    "    return max_stage_memory\n",
    "\n",
    "plan_option_d_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pipeline Components\n",
    "\n",
    "@dataclass\n",
    "class DataSample:\n",
    "    \"\"\"A single data sample.\"\"\"\n",
    "    id: str\n",
    "    content: Dict[str, Any]  # instruction, input, output, etc.\n",
    "    source: str\n",
    "    quality_score: float = 0.0\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "@dataclass\n",
    "class Dataset:\n",
    "    \"\"\"A dataset with samples.\"\"\"\n",
    "    name: str\n",
    "    samples: List[DataSample]\n",
    "    created_at: str\n",
    "    version: str\n",
    "    split: str = \"train\"  # train, val, test\n",
    "\n",
    "class DataPipeline:\n",
    "    \"\"\"\n",
    "    Pipeline for collecting, cleaning, and preparing training data.\n",
    "    \n",
    "    Stages:\n",
    "    1. Collection - Gather raw data from sources\n",
    "    2. Cleaning - Remove duplicates, filter quality\n",
    "    3. Formatting - Convert to training format\n",
    "    4. Splitting - Create train/val/test splits\n",
    "    5. Versioning - Track dataset versions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: str = \"./data\"):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.data_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.sample_id_counter = 0\n",
    "    \n",
    "    def collect_from_jsonl(self, file_path: str, source: str = \"jsonl\") -> List[DataSample]:\n",
    "        \"\"\"\n",
    "        Collect samples from a JSONL file.\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    self.sample_id_counter += 1\n",
    "                    data = json.loads(line)\n",
    "                    samples.append(DataSample(\n",
    "                        id=f\"sample_{self.sample_id_counter}\",\n",
    "                        content=data,\n",
    "                        source=source,\n",
    "                    ))\n",
    "        print(f\"âœ… Collected {len(samples)} samples from {file_path}\")\n",
    "        return samples\n",
    "    \n",
    "    def clean_samples(\n",
    "        self,\n",
    "        samples: List[DataSample],\n",
    "        min_length: int = 10,\n",
    "        max_length: int = 10000,\n",
    "        remove_duplicates: bool = True,\n",
    "    ) -> List[DataSample]:\n",
    "        \"\"\"\n",
    "        Clean and filter samples.\n",
    "        \"\"\"\n",
    "        cleaned = []\n",
    "        seen_hashes = set()\n",
    "        \n",
    "        for sample in samples:\n",
    "            # Get text content\n",
    "            text = json.dumps(sample.content)\n",
    "            \n",
    "            # Length filter\n",
    "            if len(text) < min_length or len(text) > max_length:\n",
    "                continue\n",
    "            \n",
    "            # Deduplication\n",
    "            if remove_duplicates:\n",
    "                text_hash = hashlib.md5(text.encode()).hexdigest()\n",
    "                if text_hash in seen_hashes:\n",
    "                    continue\n",
    "                seen_hashes.add(text_hash)\n",
    "            \n",
    "            cleaned.append(sample)\n",
    "        \n",
    "        print(f\"âœ… Cleaned: {len(samples)} â†’ {len(cleaned)} samples\")\n",
    "        return cleaned\n",
    "    \n",
    "    def format_for_sft(\n",
    "        self,\n",
    "        samples: List[DataSample],\n",
    "        template: str = \"alpaca\",\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Format samples for SFT training.\n",
    "        \n",
    "        Templates:\n",
    "        - alpaca: instruction, input, output\n",
    "        - chatml: messages list\n",
    "        - raw: plain text\n",
    "        \"\"\"\n",
    "        formatted = []\n",
    "        \n",
    "        for sample in samples:\n",
    "            content = sample.content\n",
    "            \n",
    "            if template == \"alpaca\":\n",
    "                text = f\"\"\"### Instruction:\n",
    "{content.get('instruction', '')}\n",
    "\n",
    "### Input:\n",
    "{content.get('input', '')}\n",
    "\n",
    "### Response:\n",
    "{content.get('output', '')}\"\"\"\n",
    "            elif template == \"chatml\":\n",
    "                messages = content.get('messages', [])\n",
    "                text = \"\\n\".join([\n",
    "                    f\"<|im_start|>{m['role']}\\n{m['content']}<|im_end|>\"\n",
    "                    for m in messages\n",
    "                ])\n",
    "            else:\n",
    "                text = content.get('text', str(content))\n",
    "            \n",
    "            formatted.append({\"text\": text, \"id\": sample.id})\n",
    "        \n",
    "        print(f\"âœ… Formatted {len(formatted)} samples for SFT ({template} template)\")\n",
    "        return formatted\n",
    "    \n",
    "    def format_for_dpo(\n",
    "        self,\n",
    "        samples: List[DataSample],\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Format samples for DPO training.\n",
    "        \n",
    "        Expects samples with 'chosen' and 'rejected' responses.\n",
    "        \"\"\"\n",
    "        formatted = []\n",
    "        \n",
    "        for sample in samples:\n",
    "            content = sample.content\n",
    "            if 'chosen' in content and 'rejected' in content:\n",
    "                formatted.append({\n",
    "                    \"prompt\": content.get('prompt', content.get('instruction', '')),\n",
    "                    \"chosen\": content['chosen'],\n",
    "                    \"rejected\": content['rejected'],\n",
    "                    \"id\": sample.id,\n",
    "                })\n",
    "        \n",
    "        print(f\"âœ… Formatted {len(formatted)} samples for DPO\")\n",
    "        return formatted\n",
    "    \n",
    "    def create_splits(\n",
    "        self,\n",
    "        samples: List[DataSample],\n",
    "        train_ratio: float = 0.9,\n",
    "        val_ratio: float = 0.05,\n",
    "        seed: int = 42,\n",
    "    ) -> Dict[str, List[DataSample]]:\n",
    "        \"\"\"\n",
    "        Create train/val/test splits.\n",
    "        \"\"\"\n",
    "        import random\n",
    "        random.seed(seed)\n",
    "        \n",
    "        shuffled = samples.copy()\n",
    "        random.shuffle(shuffled)\n",
    "        \n",
    "        n = len(shuffled)\n",
    "        train_end = int(n * train_ratio)\n",
    "        val_end = int(n * (train_ratio + val_ratio))\n",
    "        \n",
    "        splits = {\n",
    "            \"train\": shuffled[:train_end],\n",
    "            \"val\": shuffled[train_end:val_end],\n",
    "            \"test\": shuffled[val_end:],\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… Created splits: train={len(splits['train'])}, val={len(splits['val'])}, test={len(splits['test'])}\")\n",
    "        return splits\n",
    "    \n",
    "    def save_dataset(\n",
    "        self,\n",
    "        samples: List[Dict],\n",
    "        name: str,\n",
    "        version: str = \"v1.0\",\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Save dataset to disk.\n",
    "        \"\"\"\n",
    "        output_path = self.data_dir / f\"{name}_{version}.jsonl\"\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            for sample in samples:\n",
    "                f.write(json.dumps(sample) + '\\n')\n",
    "        \n",
    "        print(f\"âœ… Saved dataset to {output_path}\")\n",
    "        return str(output_path)\n",
    "\n",
    "print(\"âœ… DataPipeline class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Training Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Stage Configuration\n",
    "\n",
    "class TrainingStage(Enum):\n",
    "    SFT = \"sft\"          # Supervised Fine-Tuning\n",
    "    DPO = \"dpo\"          # Direct Preference Optimization\n",
    "    ORPO = \"orpo\"        # Odds Ratio Preference Optimization\n",
    "    KTO = \"kto\"          # Kahneman-Tversky Optimization\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration for a training run.\"\"\"\n",
    "    stage: TrainingStage\n",
    "    base_model: str\n",
    "    dataset_path: str\n",
    "    output_dir: str\n",
    "    \n",
    "    # LoRA config\n",
    "    lora_r: int = 64\n",
    "    lora_alpha: int = 128\n",
    "    lora_dropout: float = 0.05\n",
    "    \n",
    "    # Training config\n",
    "    num_epochs: int = 3\n",
    "    batch_size: int = 2\n",
    "    gradient_accumulation: int = 8\n",
    "    learning_rate: float = 2e-4\n",
    "    max_seq_length: int = 2048\n",
    "    \n",
    "    # DPO specific\n",
    "    dpo_beta: float = 0.1\n",
    "\n",
    "@dataclass\n",
    "class TrainingResult:\n",
    "    \"\"\"Result of a training run.\"\"\"\n",
    "    run_id: str\n",
    "    stage: TrainingStage\n",
    "    model_path: str\n",
    "    metrics: Dict[str, float]\n",
    "    duration_seconds: float\n",
    "    config: TrainingConfig\n",
    "    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())\n",
    "\n",
    "class TrainingOrchestrator:\n",
    "    \"\"\"\n",
    "    Orchestrates multi-stage training runs.\n",
    "    \n",
    "    Supports:\n",
    "    - SFT (Supervised Fine-Tuning)\n",
    "    - DPO (Direct Preference Optimization)\n",
    "    - Sequential stage execution\n",
    "    - Checkpoint management\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir: str = \"./training_runs\"):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.base_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.runs: List[TrainingResult] = []\n",
    "        self._run_id_counter = 0\n",
    "    \n",
    "    def run_sft(\n",
    "        self,\n",
    "        config: TrainingConfig,\n",
    "        resume_from: str = None,\n",
    "    ) -> TrainingResult:\n",
    "        \"\"\"\n",
    "        Run SFT training.\n",
    "        \n",
    "        In production, this would use TRL's SFTTrainer.\n",
    "        \"\"\"\n",
    "        import time\n",
    "        \n",
    "        self._run_id_counter += 1\n",
    "        run_id = f\"sft_run_{self._run_id_counter:04d}\"\n",
    "        \n",
    "        print(f\"\\nğŸš€ Starting SFT Training: {run_id}\")\n",
    "        print(f\"   Model: {config.base_model}\")\n",
    "        print(f\"   Dataset: {config.dataset_path}\")\n",
    "        print(f\"   Epochs: {config.num_epochs}\")\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        # Simulated training (in production, use TRL)\n",
    "        training_code = f'''\n",
    "from trl import SFTTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Load model\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"{config.base_model}\", quantization_config=bnb_config)\n",
    "\n",
    "# LoRA\n",
    "lora_config = LoraConfig(r={config.lora_r}, lora_alpha={config.lora_alpha})\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Train\n",
    "trainer = SFTTrainer(model=model, train_dataset=dataset, ...)\n",
    "trainer.train()\n",
    "trainer.save_model(\"{config.output_dir}\")\n",
    "'''\n",
    "        \n",
    "        # Placeholder metrics\n",
    "        metrics = {\n",
    "            \"train_loss\": 0.45,\n",
    "            \"eval_loss\": 0.52,\n",
    "            \"perplexity\": 3.2,\n",
    "        }\n",
    "        \n",
    "        duration = time.time() - start\n",
    "        \n",
    "        result = TrainingResult(\n",
    "            run_id=run_id,\n",
    "            stage=TrainingStage.SFT,\n",
    "            model_path=config.output_dir,\n",
    "            metrics=metrics,\n",
    "            duration_seconds=duration,\n",
    "            config=config,\n",
    "        )\n",
    "        \n",
    "        self.runs.append(result)\n",
    "        print(f\"âœ… SFT Complete. Loss: {metrics['train_loss']:.4f}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def run_dpo(\n",
    "        self,\n",
    "        config: TrainingConfig,\n",
    "        sft_model_path: str = None,\n",
    "    ) -> TrainingResult:\n",
    "        \"\"\"\n",
    "        Run DPO training.\n",
    "        \n",
    "        Requires preference data with (prompt, chosen, rejected).\n",
    "        \"\"\"\n",
    "        import time\n",
    "        \n",
    "        self._run_id_counter += 1\n",
    "        run_id = f\"dpo_run_{self._run_id_counter:04d}\"\n",
    "        \n",
    "        base = sft_model_path or config.base_model\n",
    "        \n",
    "        print(f\"\\nğŸš€ Starting DPO Training: {run_id}\")\n",
    "        print(f\"   Policy Model: {base}\")\n",
    "        print(f\"   Beta: {config.dpo_beta}\")\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        # Placeholder metrics\n",
    "        metrics = {\n",
    "            \"train_loss\": 0.35,\n",
    "            \"chosen_reward\": 0.8,\n",
    "            \"rejected_reward\": -0.5,\n",
    "            \"reward_margin\": 1.3,\n",
    "        }\n",
    "        \n",
    "        duration = time.time() - start\n",
    "        \n",
    "        result = TrainingResult(\n",
    "            run_id=run_id,\n",
    "            stage=TrainingStage.DPO,\n",
    "            model_path=config.output_dir,\n",
    "            metrics=metrics,\n",
    "            duration_seconds=duration,\n",
    "            config=config,\n",
    "        )\n",
    "        \n",
    "        self.runs.append(result)\n",
    "        print(f\"âœ… DPO Complete. Reward margin: {metrics['reward_margin']:.4f}\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"âœ… TrainingOrchestrator class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Registry\n",
    "\n",
    "@dataclass\n",
    "class ModelVersion:\n",
    "    \"\"\"A versioned model in the registry.\"\"\"\n",
    "    model_id: str\n",
    "    version: str\n",
    "    base_model: str\n",
    "    path: str\n",
    "    training_stages: List[str]\n",
    "    metrics: Dict[str, float]\n",
    "    created_at: str\n",
    "    status: str = \"registered\"  # registered, validated, deployed, archived\n",
    "    tags: List[str] = field(default_factory=list)\n",
    "\n",
    "class ModelRegistry:\n",
    "    \"\"\"\n",
    "    Registry for model versioning and tracking.\n",
    "    \n",
    "    Features:\n",
    "    - Version control for models\n",
    "    - Metrics comparison\n",
    "    - Model lineage tracking\n",
    "    - Deployment status management\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, registry_path: str = \"./model_registry\"):\n",
    "        self.registry_path = Path(registry_path)\n",
    "        self.registry_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.models: Dict[str, List[ModelVersion]] = {}\n",
    "        self._load_registry()\n",
    "    \n",
    "    def _load_registry(self):\n",
    "        \"\"\"Load existing registry from disk.\"\"\"\n",
    "        registry_file = self.registry_path / \"registry.json\"\n",
    "        if registry_file.exists():\n",
    "            with open(registry_file) as f:\n",
    "                data = json.load(f)\n",
    "                for model_id, versions in data.items():\n",
    "                    self.models[model_id] = [\n",
    "                        ModelVersion(**v) for v in versions\n",
    "                    ]\n",
    "    \n",
    "    def _save_registry(self):\n",
    "        \"\"\"Save registry to disk.\"\"\"\n",
    "        registry_file = self.registry_path / \"registry.json\"\n",
    "        data = {\n",
    "            model_id: [vars(v) for v in versions]\n",
    "            for model_id, versions in self.models.items()\n",
    "        }\n",
    "        with open(registry_file, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    \n",
    "    def register(\n",
    "        self,\n",
    "        model_id: str,\n",
    "        path: str,\n",
    "        base_model: str,\n",
    "        training_stages: List[str],\n",
    "        metrics: Dict[str, float],\n",
    "        tags: List[str] = None,\n",
    "    ) -> ModelVersion:\n",
    "        \"\"\"\n",
    "        Register a new model version.\n",
    "        \"\"\"\n",
    "        # Determine version\n",
    "        existing = self.models.get(model_id, [])\n",
    "        version = f\"v{len(existing) + 1}.0\"\n",
    "        \n",
    "        model_version = ModelVersion(\n",
    "            model_id=model_id,\n",
    "            version=version,\n",
    "            base_model=base_model,\n",
    "            path=path,\n",
    "            training_stages=training_stages,\n",
    "            metrics=metrics,\n",
    "            created_at=datetime.now().isoformat(),\n",
    "            tags=tags or [],\n",
    "        )\n",
    "        \n",
    "        if model_id not in self.models:\n",
    "            self.models[model_id] = []\n",
    "        self.models[model_id].append(model_version)\n",
    "        \n",
    "        self._save_registry()\n",
    "        \n",
    "        print(f\"âœ… Registered: {model_id} {version}\")\n",
    "        return model_version\n",
    "    \n",
    "    def get_latest(self, model_id: str) -> Optional[ModelVersion]:\n",
    "        \"\"\"Get latest version of a model.\"\"\"\n",
    "        versions = self.models.get(model_id, [])\n",
    "        return versions[-1] if versions else None\n",
    "    \n",
    "    def compare(\n",
    "        self,\n",
    "        model_id: str,\n",
    "        version_a: str,\n",
    "        version_b: str,\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Compare two model versions.\n",
    "        \"\"\"\n",
    "        versions = {v.version: v for v in self.models.get(model_id, [])}\n",
    "        \n",
    "        a = versions.get(version_a)\n",
    "        b = versions.get(version_b)\n",
    "        \n",
    "        if not a or not b:\n",
    "            return {\"error\": \"Version not found\"}\n",
    "        \n",
    "        comparison = {\n",
    "            \"model_id\": model_id,\n",
    "            \"version_a\": version_a,\n",
    "            \"version_b\": version_b,\n",
    "            \"metrics_diff\": {},\n",
    "        }\n",
    "        \n",
    "        all_metrics = set(a.metrics.keys()) | set(b.metrics.keys())\n",
    "        for metric in all_metrics:\n",
    "            val_a = a.metrics.get(metric, 0)\n",
    "            val_b = b.metrics.get(metric, 0)\n",
    "            comparison[\"metrics_diff\"][metric] = {\n",
    "                \"a\": val_a,\n",
    "                \"b\": val_b,\n",
    "                \"diff\": val_b - val_a,\n",
    "                \"pct_change\": ((val_b - val_a) / val_a * 100) if val_a != 0 else 0,\n",
    "            }\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def list_models(self) -> List[Dict]:\n",
    "        \"\"\"List all registered models.\"\"\"\n",
    "        result = []\n",
    "        for model_id, versions in self.models.items():\n",
    "            latest = versions[-1] if versions else None\n",
    "            result.append({\n",
    "                \"model_id\": model_id,\n",
    "                \"versions\": len(versions),\n",
    "                \"latest\": latest.version if latest else None,\n",
    "                \"status\": latest.status if latest else None,\n",
    "            })\n",
    "        return result\n",
    "\n",
    "print(\"âœ… ModelRegistry class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: A/B Testing Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A/B Testing Framework\n",
    "\n",
    "@dataclass\n",
    "class ABTestResult:\n",
    "    \"\"\"Result of an A/B test.\"\"\"\n",
    "    test_id: str\n",
    "    model_a: str\n",
    "    model_b: str\n",
    "    num_samples: int\n",
    "    metrics: Dict[str, Dict[str, float]]  # {metric: {a: val, b: val, winner: str}}\n",
    "    winner: str\n",
    "    confidence: float\n",
    "    created_at: str\n",
    "\n",
    "class ABTestingFramework:\n",
    "    \"\"\"\n",
    "    Framework for A/B testing model variants.\n",
    "    \n",
    "    Features:\n",
    "    - Side-by-side model comparison\n",
    "    - Statistical significance testing\n",
    "    - Automatic winner selection\n",
    "    - Traffic splitting\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.active_tests: Dict[str, Dict] = {}\n",
    "        self.completed_tests: List[ABTestResult] = []\n",
    "        self._test_id_counter = 0\n",
    "    \n",
    "    def create_test(\n",
    "        self,\n",
    "        model_a_path: str,\n",
    "        model_b_path: str,\n",
    "        traffic_split: float = 0.5,  # Fraction to model A\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Create a new A/B test.\n",
    "        \n",
    "        Returns:\n",
    "            Test ID\n",
    "        \"\"\"\n",
    "        self._test_id_counter += 1\n",
    "        test_id = f\"ab_test_{self._test_id_counter:04d}\"\n",
    "        \n",
    "        self.active_tests[test_id] = {\n",
    "            \"model_a\": model_a_path,\n",
    "            \"model_b\": model_b_path,\n",
    "            \"traffic_split\": traffic_split,\n",
    "            \"samples_a\": [],\n",
    "            \"samples_b\": [],\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… Created A/B test: {test_id}\")\n",
    "        print(f\"   Model A: {model_a_path}\")\n",
    "        print(f\"   Model B: {model_b_path}\")\n",
    "        print(f\"   Traffic split: {traffic_split*100:.0f}% / {(1-traffic_split)*100:.0f}%\")\n",
    "        \n",
    "        return test_id\n",
    "    \n",
    "    def route_request(self, test_id: str) -> str:\n",
    "        \"\"\"\n",
    "        Route a request to model A or B.\n",
    "        \n",
    "        Returns:\n",
    "            'a' or 'b'\n",
    "        \"\"\"\n",
    "        import random\n",
    "        \n",
    "        test = self.active_tests.get(test_id)\n",
    "        if not test:\n",
    "            raise ValueError(f\"Test not found: {test_id}\")\n",
    "        \n",
    "        if random.random() < test[\"traffic_split\"]:\n",
    "            return \"a\"\n",
    "        return \"b\"\n",
    "    \n",
    "    def record_result(\n",
    "        self,\n",
    "        test_id: str,\n",
    "        variant: str,  # 'a' or 'b'\n",
    "        metrics: Dict[str, float],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Record result for a request.\n",
    "        \"\"\"\n",
    "        test = self.active_tests.get(test_id)\n",
    "        if test:\n",
    "            key = f\"samples_{variant}\"\n",
    "            test[key].append(metrics)\n",
    "    \n",
    "    def analyze_test(self, test_id: str) -> ABTestResult:\n",
    "        \"\"\"\n",
    "        Analyze test results and determine winner.\n",
    "        \"\"\"\n",
    "        test = self.active_tests.get(test_id)\n",
    "        if not test:\n",
    "            raise ValueError(f\"Test not found: {test_id}\")\n",
    "        \n",
    "        samples_a = test[\"samples_a\"]\n",
    "        samples_b = test[\"samples_b\"]\n",
    "        \n",
    "        if not samples_a or not samples_b:\n",
    "            raise ValueError(\"Not enough samples for analysis\")\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        metrics_comparison = {}\n",
    "        all_metrics = set()\n",
    "        for s in samples_a + samples_b:\n",
    "            all_metrics.update(s.keys())\n",
    "        \n",
    "        a_wins = 0\n",
    "        b_wins = 0\n",
    "        \n",
    "        for metric in all_metrics:\n",
    "            a_values = [s.get(metric, 0) for s in samples_a]\n",
    "            b_values = [s.get(metric, 0) for s in samples_b]\n",
    "            \n",
    "            mean_a = sum(a_values) / len(a_values) if a_values else 0\n",
    "            mean_b = sum(b_values) / len(b_values) if b_values else 0\n",
    "            \n",
    "            # Higher is better assumption (customize per metric)\n",
    "            winner = \"a\" if mean_a > mean_b else \"b\"\n",
    "            if winner == \"a\":\n",
    "                a_wins += 1\n",
    "            else:\n",
    "                b_wins += 1\n",
    "            \n",
    "            metrics_comparison[metric] = {\n",
    "                \"a\": mean_a,\n",
    "                \"b\": mean_b,\n",
    "                \"winner\": winner,\n",
    "            }\n",
    "        \n",
    "        overall_winner = \"a\" if a_wins >= b_wins else \"b\"\n",
    "        confidence = max(a_wins, b_wins) / (a_wins + b_wins) if (a_wins + b_wins) > 0 else 0.5\n",
    "        \n",
    "        result = ABTestResult(\n",
    "            test_id=test_id,\n",
    "            model_a=test[\"model_a\"],\n",
    "            model_b=test[\"model_b\"],\n",
    "            num_samples=len(samples_a) + len(samples_b),\n",
    "            metrics=metrics_comparison,\n",
    "            winner=overall_winner,\n",
    "            confidence=confidence,\n",
    "            created_at=test[\"created_at\"],\n",
    "        )\n",
    "        \n",
    "        self.completed_tests.append(result)\n",
    "        del self.active_tests[test_id]\n",
    "        \n",
    "        print(f\"\\nğŸ“Š A/B Test Results: {test_id}\")\n",
    "        print(f\"   Samples: A={len(samples_a)}, B={len(samples_b)}\")\n",
    "        print(f\"   Winner: Model {result.winner.upper()} (confidence: {result.confidence:.1%})\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"âœ… ABTestingFramework class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ Checkpoint\n",
    "\n",
    "You now have the components for a training pipeline:\n",
    "\n",
    "- âœ… Data pipeline (collection, cleaning, formatting)\n",
    "- âœ… Training orchestrator (SFT, DPO)\n",
    "- âœ… Model registry with versioning\n",
    "- âœ… A/B testing framework\n",
    "- âœ… Automated evaluation integration\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Implementation Roadmap\n",
    "\n",
    "### Week 1: Data Pipeline\n",
    "- [ ] Implement data collection\n",
    "- [ ] Build cleaning and filtering\n",
    "- [ ] Create dataset versioning\n",
    "\n",
    "### Week 2: Training\n",
    "- [ ] Set up SFT training\n",
    "- [ ] Implement DPO training\n",
    "- [ ] Add checkpoint management\n",
    "\n",
    "### Week 3: Registry & Eval\n",
    "- [ ] Build model registry\n",
    "- [ ] Create evaluation benchmarks\n",
    "- [ ] Automate evaluation runs\n",
    "\n",
    "### Week 4: A/B Testing\n",
    "- [ ] Implement A/B framework\n",
    "- [ ] Create deployment automation\n",
    "- [ ] Build monitoring dashboards\n",
    "\n",
    "### Week 5-6: Demo & Docs\n",
    "- [ ] End-to-end pipeline demo\n",
    "- [ ] Complete technical report\n",
    "- [ ] Record demo video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§¹ Cleanup\n",
    "import gc\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… Cleanup complete!\")\n",
    "print(\"\\nğŸ¯ Next: Check examples/option-d-training-pipeline/ for complete implementation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
