{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option E: Model Quality Comparison\n",
    "\n",
    "This notebook provides tools to compare response quality across different model versions:\n",
    "1. **Base Model** - Original pretrained model\n",
    "2. **Fine-Tuned (BF16)** - After QLoRA training and merge\n",
    "3. **INT4 ONNX** - Final browser-deployable version\n",
    "\n",
    "## Objectives\n",
    "- Generate responses from all model versions\n",
    "- Compare quality systematically\n",
    "- Verify INT4 quantization doesn't significantly degrade quality\n",
    "- Document results for your technical report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "\n",
    "# For PyTorch models\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# For ONNX models\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvalConfig:\n",
    "    \"\"\"Configuration for model evaluation.\"\"\"\n",
    "    # Model paths - UPDATE THESE\n",
    "    base_model_id: str = \"google/gemma-3-1b-it\"  # Original model\n",
    "    finetuned_model_path: str = \"./models/matcha-expert-merged\"  # Merged BF16\n",
    "    onnx_model_path: str = \"./models/matcha-expert-onnx-int4\"  # INT4 ONNX\n",
    "    \n",
    "    # System prompt\n",
    "    system_prompt: str = \"\"\"You are a matcha tea expert with deep knowledge of Japanese tea culture, \n",
    "preparation methods, health benefits, and culinary applications.\"\"\"\n",
    "    \n",
    "    # Generation settings\n",
    "    max_new_tokens: int = 256\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.9\n",
    "\n",
    "config = EvalConfig()\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Questions\n",
    "\n",
    "Define a set of test questions that cover different aspects of your domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_QUESTIONS = [\n",
    "    # Basic questions\n",
    "    \"What is the difference between ceremonial and culinary grade matcha?\",\n",
    "    \"How do I make traditional matcha with a bamboo whisk?\",\n",
    "    \n",
    "    # Intermediate questions\n",
    "    \"What are the health benefits of matcha compared to regular green tea?\",\n",
    "    \"How should I store matcha to keep it fresh?\",\n",
    "    \n",
    "    # Advanced questions\n",
    "    \"Can you explain the different tea cultivars used for matcha and their characteristics?\",\n",
    "    \"What is the significance of the tea ceremony in Japanese culture?\",\n",
    "    \n",
    "    # Edge cases\n",
    "    \"Is it safe to drink matcha every day? Are there any side effects?\",\n",
    "    \"How can I tell if my matcha has gone bad?\",\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(TEST_QUESTIONS)} test questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pytorch_model(model_path: str, device: str = \"cuda\"):\n",
    "    \"\"\"Load a PyTorch model for evaluation.\"\"\"\n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=device,\n",
    "    )\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Model loaded on {device}\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def generate_pytorch_response(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    question: str, \n",
    "    system_prompt: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 0.7,\n",
    ") -> str:\n",
    "    \"\"\"Generate a response using a PyTorch model.\"\"\"\n",
    "    # Format as chat\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs.input_ids.shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvalResult:\n",
    "    \"\"\"Store evaluation results for a single question.\"\"\"\n",
    "    question: str\n",
    "    base_response: str = \"\"\n",
    "    finetuned_response: str = \"\"\n",
    "    onnx_response: str = \"\"\n",
    "    base_score: Optional[int] = None  # 1-5 manual score\n",
    "    finetuned_score: Optional[int] = None\n",
    "    onnx_score: Optional[int] = None\n",
    "    notes: str = \"\"\n",
    "\n",
    "\n",
    "# Initialize results\n",
    "eval_results = [EvalResult(question=q) for q in TEST_QUESTIONS]\n",
    "print(f\"Initialized {len(eval_results)} evaluation entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Evaluate Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "base_model, base_tokenizer = load_pytorch_model(config.base_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate responses from base model\n",
    "print(\"Generating base model responses...\\n\")\n",
    "\n",
    "for i, result in enumerate(eval_results):\n",
    "    print(f\"Question {i+1}/{len(eval_results)}: {result.question[:50]}...\")\n",
    "    \n",
    "    response = generate_pytorch_response(\n",
    "        base_model,\n",
    "        base_tokenizer,\n",
    "        result.question,\n",
    "        config.system_prompt,\n",
    "        config.max_new_tokens,\n",
    "        config.temperature,\n",
    "    )\n",
    "    result.base_response = response\n",
    "    print(f\"  Response length: {len(response)} chars\\n\")\n",
    "\n",
    "print(\"Base model evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "del base_model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Base model unloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Evaluate Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned model\n",
    "ft_model, ft_tokenizer = load_pytorch_model(config.finetuned_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate responses from fine-tuned model\n",
    "print(\"Generating fine-tuned model responses...\\n\")\n",
    "\n",
    "for i, result in enumerate(eval_results):\n",
    "    print(f\"Question {i+1}/{len(eval_results)}: {result.question[:50]}...\")\n",
    "    \n",
    "    response = generate_pytorch_response(\n",
    "        ft_model,\n",
    "        ft_tokenizer,\n",
    "        result.question,\n",
    "        config.system_prompt,\n",
    "        config.max_new_tokens,\n",
    "        config.temperature,\n",
    "    )\n",
    "    result.finetuned_response = response\n",
    "    print(f\"  Response length: {len(response)} chars\\n\")\n",
    "\n",
    "print(\"Fine-tuned model evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "del ft_model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Fine-tuned model unloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate ONNX INT4 Model\n",
    "\n",
    "Note: For accurate browser comparison, you should also test in an actual browser.\n",
    "This section tests the ONNX model using onnxruntime in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Full ONNX text generation requires more setup.\n",
    "# For a quick comparison, you can use the transformers optimum library.\n",
    "\n",
    "try:\n",
    "    from optimum.onnxruntime import ORTModelForCausalLM\n",
    "    \n",
    "    print(f\"Loading ONNX model from {config.onnx_model_path}...\")\n",
    "    onnx_model = ORTModelForCausalLM.from_pretrained(config.onnx_model_path)\n",
    "    onnx_tokenizer = AutoTokenizer.from_pretrained(config.onnx_model_path)\n",
    "    print(\"ONNX model loaded!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"optimum not installed. Install with: pip install optimum[onnxruntime]\")\n",
    "    print(\"Skipping ONNX evaluation in Python - test in browser instead.\")\n",
    "    onnx_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if onnx_model is not None:\n",
    "    print(\"Generating ONNX model responses...\\n\")\n",
    "    \n",
    "    for i, result in enumerate(eval_results):\n",
    "        print(f\"Question {i+1}/{len(eval_results)}: {result.question[:50]}...\")\n",
    "        \n",
    "        # Format as chat\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": config.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": result.question},\n",
    "        ]\n",
    "        prompt = onnx_tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        inputs = onnx_tokenizer(prompt, return_tensors=\"pt\")\n",
    "        outputs = onnx_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=config.max_new_tokens,\n",
    "            temperature=config.temperature,\n",
    "            do_sample=True,\n",
    "        )\n",
    "        \n",
    "        response = onnx_tokenizer.decode(\n",
    "            outputs[0][inputs.input_ids.shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        result.onnx_response = response.strip()\n",
    "        print(f\"  Response length: {len(result.onnx_response)} chars\\n\")\n",
    "    \n",
    "    print(\"ONNX model evaluation complete!\")\n",
    "else:\n",
    "    print(\"ONNX model not loaded. Fill in ONNX responses manually from browser testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results Side-by-Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_comparison(result: EvalResult):\n",
    "    \"\"\"Display responses side-by-side for comparison.\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"QUESTION: {result.question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\n--- BASE MODEL ---\")\n",
    "    print(result.base_response[:500] + \"...\" if len(result.base_response) > 500 else result.base_response)\n",
    "    \n",
    "    print(\"\\n--- FINE-TUNED MODEL ---\")\n",
    "    print(result.finetuned_response[:500] + \"...\" if len(result.finetuned_response) > 500 else result.finetuned_response)\n",
    "    \n",
    "    print(\"\\n--- ONNX INT4 MODEL ---\")\n",
    "    if result.onnx_response:\n",
    "        print(result.onnx_response[:500] + \"...\" if len(result.onnx_response) > 500 else result.onnx_response)\n",
    "    else:\n",
    "        print(\"[Not evaluated - test in browser]\")\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all comparisons\n",
    "for result in eval_results:\n",
    "    display_comparison(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Scoring\n",
    "\n",
    "Score each response from 1-5:\n",
    "- **5**: Excellent - accurate, detailed, well-structured\n",
    "- **4**: Good - correct with minor omissions\n",
    "- **3**: Acceptable - mostly correct, some issues\n",
    "- **2**: Poor - significant errors\n",
    "- **1**: Unacceptable - incorrect or nonsensical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Score the first question\n",
    "# Update these scores based on your evaluation\n",
    "\n",
    "eval_results[0].base_score = 3  # Example score\n",
    "eval_results[0].finetuned_score = 5  # Example score\n",
    "eval_results[0].onnx_score = 5  # Example score\n",
    "eval_results[0].notes = \"Fine-tuned model provides more detailed matcha-specific information.\"\n",
    "\n",
    "# Continue for all questions...\n",
    "# eval_results[1].base_score = ...\n",
    "# eval_results[1].finetuned_score = ...\n",
    "# etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(results: list[EvalResult]) -> pd.DataFrame:\n",
    "    \"\"\"Generate a summary DataFrame of all results.\"\"\"\n",
    "    data = []\n",
    "    for r in results:\n",
    "        data.append({\n",
    "            \"Question\": r.question[:50] + \"...\",\n",
    "            \"Base Score\": r.base_score,\n",
    "            \"Fine-Tuned Score\": r.finetuned_score,\n",
    "            \"ONNX Score\": r.onnx_score,\n",
    "            \"Notes\": r.notes[:30] + \"...\" if len(r.notes) > 30 else r.notes,\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "summary_df = generate_summary(eval_results)\n",
    "print(summary_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate averages\n",
    "scored_results = [r for r in eval_results if r.base_score is not None]\n",
    "\n",
    "if scored_results:\n",
    "    avg_base = sum(r.base_score for r in scored_results) / len(scored_results)\n",
    "    avg_ft = sum(r.finetuned_score for r in scored_results if r.finetuned_score) / len([r for r in scored_results if r.finetuned_score])\n",
    "    avg_onnx = sum(r.onnx_score for r in scored_results if r.onnx_score) / len([r for r in scored_results if r.onnx_score]) if any(r.onnx_score for r in scored_results) else None\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"AVERAGE SCORES\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Base Model:      {avg_base:.2f}/5\")\n",
    "    print(f\"Fine-Tuned:      {avg_ft:.2f}/5\")\n",
    "    if avg_onnx:\n",
    "        print(f\"ONNX INT4:       {avg_onnx:.2f}/5\")\n",
    "    print(f\"\\nImprovement:     +{((avg_ft - avg_base) / avg_base * 100):.1f}%\")\n",
    "else:\n",
    "    print(\"No scores recorded yet. Complete the manual scoring section above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "results_data = [\n",
    "    {\n",
    "        \"question\": r.question,\n",
    "        \"base_response\": r.base_response,\n",
    "        \"finetuned_response\": r.finetuned_response,\n",
    "        \"onnx_response\": r.onnx_response,\n",
    "        \"base_score\": r.base_score,\n",
    "        \"finetuned_score\": r.finetuned_score,\n",
    "        \"onnx_score\": r.onnx_score,\n",
    "        \"notes\": r.notes,\n",
    "    }\n",
    "    for r in eval_results\n",
    "]\n",
    "\n",
    "output_path = Path(\"./evaluation_results.json\")\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(results_data, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Use this section to document your findings:\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Fine-tuning effectiveness**: [Did fine-tuning improve domain responses?]\n",
    "\n",
    "2. **Quantization impact**: [How much quality was lost with INT4?]\n",
    "\n",
    "3. **Specific improvements**: [Which question types improved most?]\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "- [Recommendation 1]\n",
    "- [Recommendation 2]\n",
    "- [Recommendation 3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
