# ============================================
# Production ML Inference Stack
# ============================================
# Components:
#   - LLM Inference Server (GPU-enabled)
#   - ChromaDB (Vector Database)
#   - Redis (Response Cache)
#   - Prometheus (Metrics)
#   - Grafana (Dashboards)
# ============================================

version: '3.8'

services:
  # ============================================
  # LLM Inference Server
  # ============================================
  inference:
    build:
      context: ../inference-server
      dockerfile: Dockerfile
    image: llm-inference:latest
    container_name: llm-inference
    ports:
      - "8000:8000"
    environment:
      - MODEL_PATH=/models
      - CUDA_VISIBLE_DEVICES=0
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - MAX_NEW_TOKENS=512
      - TEMPERATURE=0.7
    volumes:
      - ${MODEL_PATH:-./models}:/models:ro
      - inference_cache:/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      redis:
        condition: service_healthy
      vectordb:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    restart: unless-stopped
    networks:
      - ml-network

  # ============================================
  # ChromaDB - Vector Database
  # ============================================
  vectordb:
    image: chromadb/chroma:latest
    container_name: chromadb
    ports:
      - "8001:8000"
    environment:
      - IS_PERSISTENT=TRUE
      - ANONYMIZED_TELEMETRY=FALSE
    volumes:
      - chroma_data:/chroma/chroma
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - ml-network

  # ============================================
  # Redis - Response Cache
  # ============================================
  redis:
    image: redis:7-alpine
    container_name: redis-cache
    ports:
      - "6379:6379"
    command: >
      redis-server
      --appendonly yes
      --maxmemory 2gb
      --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    networks:
      - ml-network

  # ============================================
  # Prometheus - Metrics Collection
  # ============================================
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - ml-network

  # ============================================
  # Grafana - Dashboards
  # ============================================
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - ml-network

# ============================================
# Volumes
# ============================================
volumes:
  inference_cache:
    driver: local
  chroma_data:
    driver: local
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

# ============================================
# Networks
# ============================================
networks:
  ml-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16
