{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Lab 4.4.2: Docker Compose Stack - SOLUTION\n",
    "\n",
    "**Module:** 4.4 - Containerization & Cloud Deployment  \n",
    "**This is the complete solution notebook with all exercises solved.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise1-header",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Add Redis Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise1-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Redis service configuration\n",
    "\n",
    "redis_config = '''\n",
    "  # ============================================\n",
    "  # Redis - Response Cache\n",
    "  # ============================================\n",
    "  redis:\n",
    "    image: redis:7-alpine\n",
    "    container_name: redis-cache\n",
    "    ports:\n",
    "      - \"6379:6379\"\n",
    "    command: redis-server --appendonly yes --maxmemory 2gb --maxmemory-policy allkeys-lru\n",
    "    volumes:\n",
    "      - redis_data:/data\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"redis-cli\", \"ping\"]\n",
    "      interval: 10s\n",
    "      timeout: 5s\n",
    "      retries: 3\n",
    "    restart: unless-stopped\n",
    "    networks:\n",
    "      - ml-network\n",
    "'''\n",
    "\n",
    "print(\"REDIS SERVICE CONFIGURATION:\")\n",
    "print(\"=\" * 60)\n",
    "print(redis_config)\n",
    "\n",
    "print(\"\\nKEY FEATURES:\")\n",
    "print(\"  - redis:7-alpine: Lightweight Alpine-based image\")\n",
    "print(\"  - appendonly yes: Persistent storage\")\n",
    "print(\"  - maxmemory 2gb: Memory limit for cache\")\n",
    "print(\"  - allkeys-lru: Eviction policy for cache\")\n",
    "print(\"  - Health check with redis-cli ping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference-with-redis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated inference server to use Redis cache\n",
    "\n",
    "inference_with_cache = '''\n",
    "  inference:\n",
    "    image: llm-inference:latest\n",
    "    container_name: llm-inference\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - MODEL_PATH=/models\n",
    "      - CUDA_VISIBLE_DEVICES=0\n",
    "      - REDIS_HOST=redis\n",
    "      - REDIS_PORT=6379\n",
    "      - CACHE_TTL=3600\n",
    "    volumes:\n",
    "      - ./models:/models\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              count: 1\n",
    "              capabilities: [gpu]\n",
    "    depends_on:\n",
    "      redis:\n",
    "        condition: service_healthy\n",
    "      vectordb:\n",
    "        condition: service_healthy\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "      start_period: 60s\n",
    "    restart: unless-stopped\n",
    "    networks:\n",
    "      - ml-network\n",
    "'''\n",
    "\n",
    "print(\"INFERENCE SERVER WITH REDIS CACHE:\")\n",
    "print(\"=\" * 60)\n",
    "print(inference_with_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cache-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python code for using Redis cache in inference server\n",
    "\n",
    "cache_code = '''\n",
    "\"\"\"Redis caching for LLM inference.\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from typing import Optional\n",
    "import redis\n",
    "\n",
    "\n",
    "class ResponseCache:\n",
    "    \"\"\"Cache LLM responses to reduce latency and costs.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        host: str = None,\n",
    "        port: int = None,\n",
    "        ttl: int = None,\n",
    "    ):\n",
    "        self.host = host or os.environ.get(\"REDIS_HOST\", \"localhost\")\n",
    "        self.port = port or int(os.environ.get(\"REDIS_PORT\", 6379))\n",
    "        self.ttl = ttl or int(os.environ.get(\"CACHE_TTL\", 3600))\n",
    "        \n",
    "        self.client = redis.Redis(\n",
    "            host=self.host,\n",
    "            port=self.port,\n",
    "            decode_responses=True,\n",
    "        )\n",
    "    \n",
    "    def _make_key(self, prompt: str, params: dict) -> str:\n",
    "        \"\"\"Create cache key from prompt and parameters.\"\"\"\n",
    "        key_data = json.dumps({\"prompt\": prompt, \"params\": params}, sort_keys=True)\n",
    "        return f\"llm:response:{hashlib.sha256(key_data.encode()).hexdigest()[:16]}\"\n",
    "    \n",
    "    def get(self, prompt: str, params: dict) -> Optional[str]:\n",
    "        \"\"\"Get cached response.\"\"\"\n",
    "        key = self._make_key(prompt, params)\n",
    "        return self.client.get(key)\n",
    "    \n",
    "    def set(self, prompt: str, params: dict, response: str) -> None:\n",
    "        \"\"\"Cache a response.\"\"\"\n",
    "        key = self._make_key(prompt, params)\n",
    "        self.client.setex(key, self.ttl, response)\n",
    "    \n",
    "    def stats(self) -> dict:\n",
    "        \"\"\"Get cache statistics.\"\"\"\n",
    "        info = self.client.info(\"stats\")\n",
    "        return {\n",
    "            \"hits\": info.get(\"keyspace_hits\", 0),\n",
    "            \"misses\": info.get(\"keyspace_misses\", 0),\n",
    "            \"keys\": self.client.dbsize(),\n",
    "        }\n",
    "\n",
    "\n",
    "# Usage in FastAPI endpoint\n",
    "cache = ResponseCache()\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(request: GenerateRequest):\n",
    "    params = {\"max_tokens\": request.max_tokens, \"temperature\": request.temperature}\n",
    "    \n",
    "    # Check cache first\n",
    "    cached = cache.get(request.prompt, params)\n",
    "    if cached:\n",
    "        return {\"generated_text\": cached, \"cached\": True}\n",
    "    \n",
    "    # Generate response\n",
    "    response = model.generate(request.prompt, **params)\n",
    "    \n",
    "    # Cache the response\n",
    "    cache.set(request.prompt, params, response)\n",
    "    \n",
    "    return {\"generated_text\": response, \"cached\": False}\n",
    "'''\n",
    "\n",
    "print(\"REDIS CACHE IMPLEMENTATION:\")\n",
    "print(\"=\" * 60)\n",
    "print(cache_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise2-header",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Add Traefik API Gateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise2-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Traefik configuration with automatic service discovery\n",
    "\n",
    "traefik_config = '''\n",
    "  # ============================================\n",
    "  # Traefik - API Gateway / Load Balancer\n",
    "  # ============================================\n",
    "  traefik:\n",
    "    image: traefik:v3.0\n",
    "    container_name: traefik\n",
    "    command:\n",
    "      # API and Dashboard\n",
    "      - \"--api.dashboard=true\"\n",
    "      - \"--api.insecure=true\"\n",
    "      # Docker provider\n",
    "      - \"--providers.docker=true\"\n",
    "      - \"--providers.docker.exposedbydefault=false\"\n",
    "      - \"--providers.docker.network=ml-network\"\n",
    "      # Entrypoints\n",
    "      - \"--entrypoints.web.address=:80\"\n",
    "      - \"--entrypoints.websecure.address=:443\"\n",
    "      # Access logs\n",
    "      - \"--accesslog=true\"\n",
    "      - \"--accesslog.format=json\"\n",
    "      # Metrics\n",
    "      - \"--metrics.prometheus=true\"\n",
    "      - \"--metrics.prometheus.entrypoint=metrics\"\n",
    "      - \"--entrypoints.metrics.address=:8082\"\n",
    "    ports:\n",
    "      - \"80:80\"       # HTTP\n",
    "      - \"443:443\"     # HTTPS\n",
    "      - \"8080:8080\"   # Dashboard\n",
    "      - \"8082:8082\"   # Metrics\n",
    "    volumes:\n",
    "      - /var/run/docker.sock:/var/run/docker.sock:ro\n",
    "      - ./traefik/acme.json:/acme.json\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"traefik\", \"healthcheck\"]\n",
    "      interval: 10s\n",
    "      timeout: 5s\n",
    "      retries: 3\n",
    "    restart: unless-stopped\n",
    "    networks:\n",
    "      - ml-network\n",
    "    labels:\n",
    "      # Dashboard route\n",
    "      - \"traefik.enable=true\"\n",
    "      - \"traefik.http.routers.dashboard.rule=Host(`traefik.localhost`)\"\n",
    "      - \"traefik.http.routers.dashboard.service=api@internal\"\n",
    "'''\n",
    "\n",
    "print(\"TRAEFIK API GATEWAY:\")\n",
    "print(\"=\" * 60)\n",
    "print(traefik_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference-with-traefik",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated inference service with Traefik labels for automatic discovery\n",
    "\n",
    "inference_with_traefik = '''\n",
    "  inference:\n",
    "    image: llm-inference:latest\n",
    "    container_name: llm-inference\n",
    "    # No need to expose ports - Traefik handles routing\n",
    "    environment:\n",
    "      - MODEL_PATH=/models\n",
    "      - CUDA_VISIBLE_DEVICES=0\n",
    "    volumes:\n",
    "      - ./models:/models\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              count: 1\n",
    "              capabilities: [gpu]\n",
    "    labels:\n",
    "      # Enable Traefik routing\n",
    "      - \"traefik.enable=true\"\n",
    "      # HTTP router\n",
    "      - \"traefik.http.routers.inference.rule=Host(`api.localhost`) && PathPrefix(`/v1`)\"\n",
    "      - \"traefik.http.routers.inference.entrypoints=web\"\n",
    "      # Service configuration\n",
    "      - \"traefik.http.services.inference.loadbalancer.server.port=8000\"\n",
    "      # Rate limiting middleware\n",
    "      - \"traefik.http.middlewares.inference-ratelimit.ratelimit.average=100\"\n",
    "      - \"traefik.http.middlewares.inference-ratelimit.ratelimit.burst=50\"\n",
    "      - \"traefik.http.routers.inference.middlewares=inference-ratelimit\"\n",
    "      # Retry middleware\n",
    "      - \"traefik.http.middlewares.inference-retry.retry.attempts=3\"\n",
    "      - \"traefik.http.middlewares.inference-retry.retry.initialinterval=100ms\"\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "    restart: unless-stopped\n",
    "    networks:\n",
    "      - ml-network\n",
    "'''\n",
    "\n",
    "print(\"INFERENCE WITH TRAEFIK LABELS:\")\n",
    "print(\"=\" * 60)\n",
    "print(inference_with_traefik)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-stack-header",
   "metadata": {},
   "source": [
    "## Complete Production Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete production docker-compose.yml\n",
    "\n",
    "complete_stack = '''\n",
    "version: '3.8'\n",
    "\n",
    "# ==============================================\n",
    "# Production ML Inference Stack\n",
    "# ==============================================\n",
    "# Components:\n",
    "#   - Traefik (API Gateway)\n",
    "#   - Inference Server (GPU-enabled)\n",
    "#   - Redis (Response Cache)\n",
    "#   - ChromaDB (Vector Database)\n",
    "#   - Prometheus (Metrics)\n",
    "#   - Grafana (Dashboards)\n",
    "# ==============================================\n",
    "\n",
    "services:\n",
    "  # API Gateway\n",
    "  traefik:\n",
    "    image: traefik:v3.0\n",
    "    container_name: traefik\n",
    "    command:\n",
    "      - \"--api.dashboard=true\"\n",
    "      - \"--api.insecure=true\"\n",
    "      - \"--providers.docker=true\"\n",
    "      - \"--providers.docker.exposedbydefault=false\"\n",
    "      - \"--entrypoints.web.address=:80\"\n",
    "      - \"--metrics.prometheus=true\"\n",
    "    ports:\n",
    "      - \"80:80\"\n",
    "      - \"8080:8080\"\n",
    "    volumes:\n",
    "      - /var/run/docker.sock:/var/run/docker.sock:ro\n",
    "    networks:\n",
    "      - ml-network\n",
    "\n",
    "  # LLM Inference\n",
    "  inference:\n",
    "    image: llm-inference:latest\n",
    "    container_name: llm-inference\n",
    "    environment:\n",
    "      - MODEL_PATH=/models\n",
    "      - REDIS_HOST=redis\n",
    "      - CHROMADB_HOST=vectordb\n",
    "    volumes:\n",
    "      - ./models:/models\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              count: 1\n",
    "              capabilities: [gpu]\n",
    "    labels:\n",
    "      - \"traefik.enable=true\"\n",
    "      - \"traefik.http.routers.inference.rule=PathPrefix(`/v1`)\"\n",
    "      - \"traefik.http.services.inference.loadbalancer.server.port=8000\"\n",
    "    depends_on:\n",
    "      - redis\n",
    "      - vectordb\n",
    "    networks:\n",
    "      - ml-network\n",
    "\n",
    "  # Response Cache\n",
    "  redis:\n",
    "    image: redis:7-alpine\n",
    "    container_name: redis\n",
    "    command: redis-server --appendonly yes --maxmemory 2gb --maxmemory-policy allkeys-lru\n",
    "    volumes:\n",
    "      - redis_data:/data\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"redis-cli\", \"ping\"]\n",
    "      interval: 10s\n",
    "      timeout: 5s\n",
    "      retries: 3\n",
    "    networks:\n",
    "      - ml-network\n",
    "\n",
    "  # Vector Database\n",
    "  vectordb:\n",
    "    image: chromadb/chroma:latest\n",
    "    container_name: chromadb\n",
    "    ports:\n",
    "      - \"8001:8000\"\n",
    "    volumes:\n",
    "      - chroma_data:/chroma/chroma\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/api/v1/heartbeat\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "    networks:\n",
    "      - ml-network\n",
    "\n",
    "  # Metrics\n",
    "  prometheus:\n",
    "    image: prom/prometheus:latest\n",
    "    container_name: prometheus\n",
    "    ports:\n",
    "      - \"9090:9090\"\n",
    "    volumes:\n",
    "      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml\n",
    "      - prometheus_data:/prometheus\n",
    "    networks:\n",
    "      - ml-network\n",
    "\n",
    "  # Dashboards\n",
    "  grafana:\n",
    "    image: grafana/grafana:latest\n",
    "    container_name: grafana\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "    environment:\n",
    "      - GF_SECURITY_ADMIN_PASSWORD=admin\n",
    "    volumes:\n",
    "      - grafana_data:/var/lib/grafana\n",
    "    depends_on:\n",
    "      - prometheus\n",
    "    networks:\n",
    "      - ml-network\n",
    "\n",
    "volumes:\n",
    "  redis_data:\n",
    "  chroma_data:\n",
    "  prometheus_data:\n",
    "  grafana_data:\n",
    "\n",
    "networks:\n",
    "  ml-network:\n",
    "    driver: bridge\n",
    "'''\n",
    "\n",
    "print(\"COMPLETE PRODUCTION STACK:\")\n",
    "print(\"=\" * 60)\n",
    "print(complete_stack)\n",
    "\n",
    "# Save the file\n",
    "import os\n",
    "os.makedirs(\"../docker-examples/production-stack\", exist_ok=True)\n",
    "with open(\"../docker-examples/production-stack/docker-compose.yml\", \"w\") as f:\n",
    "    f.write(complete_stack)\n",
    "\n",
    "print(\"\\nSaved to: ../docker-examples/production-stack/docker-compose.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This solution demonstrated:\n",
    "\n",
    "1. **Redis Cache Service**\n",
    "   - Alpine-based lightweight image\n",
    "   - Persistence with appendonly\n",
    "   - Memory limits and LRU eviction\n",
    "   - Integration with inference server\n",
    "\n",
    "2. **Traefik API Gateway**\n",
    "   - Automatic service discovery via Docker labels\n",
    "   - Rate limiting middleware\n",
    "   - Retry middleware\n",
    "   - Prometheus metrics integration\n",
    "\n",
    "3. **Complete Production Stack**\n",
    "   - All services connected\n",
    "   - Health checks throughout\n",
    "   - Proper dependency ordering\n",
    "   - Persistent volumes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
