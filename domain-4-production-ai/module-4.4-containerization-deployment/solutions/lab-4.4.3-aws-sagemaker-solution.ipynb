{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Lab 4.4.3: AWS SageMaker Deployment - SOLUTION\n",
    "\n",
    "**Module:** 4.4 - Containerization & Cloud Deployment  \n",
    "**This is the complete solution notebook with all exercises solved.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise1-header",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Multi-Model Endpoint Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise1-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-model endpoint with A/B testing\n",
    "\n",
    "multimodel_config = '''\n",
    "\"\"\"Multi-Model SageMaker Endpoint with A/B Testing.\"\"\"\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.model import Model\n",
    "\n",
    "\n",
    "def create_ab_test_endpoint(\n",
    "    model_a_uri: str,\n",
    "    model_b_uri: str,\n",
    "    traffic_split: float = 0.5,  # 50% to model A\n",
    "    endpoint_name: str = \"ab-test-endpoint\",\n",
    "    role: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create A/B test endpoint with two model variants.\n",
    "    \n",
    "    Args:\n",
    "        model_a_uri: S3 path to model A (control)\n",
    "        model_b_uri: S3 path to model B (treatment)\n",
    "        traffic_split: Traffic % to model A (0.0-1.0)\n",
    "        endpoint_name: Name for the endpoint\n",
    "        role: SageMaker execution role\n",
    "    \"\"\"\n",
    "    role = role or sagemaker.get_execution_role()\n",
    "    session = sagemaker.Session()\n",
    "    \n",
    "    # Create Model A (Control)\n",
    "    model_a = HuggingFaceModel(\n",
    "        model_data=model_a_uri,\n",
    "        role=role,\n",
    "        transformers_version=\"4.37\",\n",
    "        pytorch_version=\"2.1\",\n",
    "        py_version=\"py310\",\n",
    "        env={\n",
    "            \"HF_MODEL_ID\": model_a_uri,\n",
    "            \"VARIANT\": \"control\",\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # Create Model B (Treatment)\n",
    "    model_b = HuggingFaceModel(\n",
    "        model_data=model_b_uri,\n",
    "        role=role,\n",
    "        transformers_version=\"4.37\",\n",
    "        pytorch_version=\"2.1\",\n",
    "        py_version=\"py310\",\n",
    "        env={\n",
    "            \"HF_MODEL_ID\": model_b_uri,\n",
    "            \"VARIANT\": \"treatment\",\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # Create endpoint configuration with production variants\n",
    "    from sagemaker.production_variant import ProductionVariant\n",
    "    \n",
    "    # Calculate weights (must be integers that sum to match instance counts)\n",
    "    weight_a = int(traffic_split * 100)\n",
    "    weight_b = 100 - weight_a\n",
    "    \n",
    "    # Deploy with A/B split\n",
    "    predictor = model_a.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=\"ml.g5.xlarge\",\n",
    "        endpoint_name=endpoint_name,\n",
    "        data_capture_config=sagemaker.model_monitor.DataCaptureConfig(\n",
    "            enable_capture=True,\n",
    "            sampling_percentage=100,\n",
    "            destination_s3_uri=f\"s3://bucket/data-capture/{endpoint_name}\",\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Add variant B\n",
    "    session.sagemaker_client.update_endpoint_weights_and_capacities(\n",
    "        EndpointName=endpoint_name,\n",
    "        DesiredWeightsAndCapacities=[\n",
    "            {\"VariantName\": \"AllTraffic\", \"DesiredWeight\": weight_a},\n",
    "            {\"VariantName\": \"VariantB\", \"DesiredWeight\": weight_b},\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "        \"variants\": [\n",
    "            {\"name\": \"AllTraffic\", \"weight\": weight_a, \"model\": \"model_a\"},\n",
    "            {\"name\": \"VariantB\", \"weight\": weight_b, \"model\": \"model_b\"},\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_ab_results(endpoint_name: str, metric: str = \"latency\"):\n",
    "    \"\"\"\n",
    "    Analyze A/B test results from CloudWatch metrics.\n",
    "    \"\"\"\n",
    "    import boto3\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    cloudwatch = boto3.client(\"cloudwatch\")\n",
    "    \n",
    "    end_time = datetime.utcnow()\n",
    "    start_time = end_time - timedelta(hours=24)\n",
    "    \n",
    "    results = {}\n",
    "    for variant in [\"AllTraffic\", \"VariantB\"]:\n",
    "        response = cloudwatch.get_metric_statistics(\n",
    "            Namespace=\"AWS/SageMaker\",\n",
    "            MetricName=\"ModelLatency\",\n",
    "            Dimensions=[\n",
    "                {\"Name\": \"EndpointName\", \"Value\": endpoint_name},\n",
    "                {\"Name\": \"VariantName\", \"Value\": variant},\n",
    "            ],\n",
    "            StartTime=start_time,\n",
    "            EndTime=end_time,\n",
    "            Period=3600,\n",
    "            Statistics=[\"Average\", \"p50\", \"p99\"],\n",
    "        )\n",
    "        \n",
    "        results[variant] = response[\"Datapoints\"]\n",
    "    \n",
    "    return results\n",
    "'''\n",
    "\n",
    "print(\"MULTI-MODEL ENDPOINT WITH A/B TESTING:\")\n",
    "print(\"=\" * 60)\n",
    "print(multimodel_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise2-header",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Auto-Scaling Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise2-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete auto-scaling configuration\n",
    "\n",
    "autoscaling_config = '''\n",
    "\"\"\"SageMaker Auto-Scaling Configuration.\"\"\"\n",
    "\n",
    "import boto3\n",
    "\n",
    "\n",
    "def configure_autoscaling(\n",
    "    endpoint_name: str,\n",
    "    variant_name: str = \"AllTraffic\",\n",
    "    min_capacity: int = 1,\n",
    "    max_capacity: int = 10,\n",
    "    target_value: float = 70.0,  # Invocations per instance per minute\n",
    "    scale_in_cooldown: int = 600,  # 10 minutes\n",
    "    scale_out_cooldown: int = 300,  # 5 minutes\n",
    "):\n",
    "    \"\"\"\n",
    "    Configure auto-scaling for a SageMaker endpoint.\n",
    "    \n",
    "    Args:\n",
    "        endpoint_name: SageMaker endpoint name\n",
    "        variant_name: Production variant name\n",
    "        min_capacity: Minimum instances\n",
    "        max_capacity: Maximum instances\n",
    "        target_value: Target invocations per instance per minute\n",
    "        scale_in_cooldown: Seconds before scaling in again\n",
    "        scale_out_cooldown: Seconds before scaling out again\n",
    "    \"\"\"\n",
    "    autoscaling = boto3.client(\"application-autoscaling\")\n",
    "    \n",
    "    resource_id = f\"endpoint/{endpoint_name}/variant/{variant_name}\"\n",
    "    \n",
    "    # Step 1: Register scalable target\n",
    "    autoscaling.register_scalable_target(\n",
    "        ServiceNamespace=\"sagemaker\",\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "        MinCapacity=min_capacity,\n",
    "        MaxCapacity=max_capacity,\n",
    "    )\n",
    "    print(f\"Registered scalable target: {min_capacity}-{max_capacity} instances\")\n",
    "    \n",
    "    # Step 2: Create target tracking scaling policy\n",
    "    autoscaling.put_scaling_policy(\n",
    "        PolicyName=f\"{endpoint_name}-scaling-policy\",\n",
    "        ServiceNamespace=\"sagemaker\",\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "        PolicyType=\"TargetTrackingScaling\",\n",
    "        TargetTrackingScalingPolicyConfiguration={\n",
    "            \"TargetValue\": target_value,\n",
    "            \"PredefinedMetricSpecification\": {\n",
    "                \"PredefinedMetricType\": \"SageMakerVariantInvocationsPerInstance\",\n",
    "            },\n",
    "            \"ScaleInCooldown\": scale_in_cooldown,\n",
    "            \"ScaleOutCooldown\": scale_out_cooldown,\n",
    "        },\n",
    "    )\n",
    "    print(f\"Created scaling policy: target {target_value} invocations/instance/minute\")\n",
    "    \n",
    "    # Step 3: Add scheduled scaling for known traffic patterns (optional)\n",
    "    # Scale up during business hours\n",
    "    autoscaling.put_scheduled_action(\n",
    "        ServiceNamespace=\"sagemaker\",\n",
    "        ScheduledActionName=f\"{endpoint_name}-scale-up-business-hours\",\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "        Schedule=\"cron(0 8 ? * MON-FRI *)\",  # 8 AM UTC weekdays\n",
    "        ScalableTargetAction={\n",
    "            \"MinCapacity\": max(2, min_capacity),\n",
    "            \"MaxCapacity\": max_capacity,\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # Scale down after hours\n",
    "    autoscaling.put_scheduled_action(\n",
    "        ServiceNamespace=\"sagemaker\",\n",
    "        ScheduledActionName=f\"{endpoint_name}-scale-down-after-hours\",\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "        Schedule=\"cron(0 20 ? * MON-FRI *)\",  # 8 PM UTC weekdays\n",
    "        ScalableTargetAction={\n",
    "            \"MinCapacity\": min_capacity,\n",
    "            \"MaxCapacity\": max_capacity,\n",
    "        },\n",
    "    )\n",
    "    print(\"Created scheduled scaling actions for business hours\")\n",
    "    \n",
    "    return {\n",
    "        \"resource_id\": resource_id,\n",
    "        \"min_capacity\": min_capacity,\n",
    "        \"max_capacity\": max_capacity,\n",
    "        \"target_value\": target_value,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_scaling_status(endpoint_name: str, variant_name: str = \"AllTraffic\"):\n",
    "    \"\"\"Get current scaling status.\"\"\"\n",
    "    autoscaling = boto3.client(\"application-autoscaling\")\n",
    "    sagemaker = boto3.client(\"sagemaker\")\n",
    "    \n",
    "    # Get current capacity\n",
    "    resource_id = f\"endpoint/{endpoint_name}/variant/{variant_name}\"\n",
    "    \n",
    "    targets = autoscaling.describe_scalable_targets(\n",
    "        ServiceNamespace=\"sagemaker\",\n",
    "        ResourceIds=[resource_id],\n",
    "    )\n",
    "    \n",
    "    # Get endpoint status\n",
    "    endpoint = sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "    \n",
    "    return {\n",
    "        \"endpoint_status\": endpoint[\"EndpointStatus\"],\n",
    "        \"scaling_config\": targets[\"ScalableTargets\"][0] if targets[\"ScalableTargets\"] else None,\n",
    "        \"current_instances\": endpoint.get(\"ProductionVariants\", [{}])[0].get(\"CurrentInstanceCount\", 0),\n",
    "    }\n",
    "'''\n",
    "\n",
    "print(\"AUTO-SCALING CONFIGURATION:\")\n",
    "print(\"=\" * 60)\n",
    "print(autoscaling_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise3-header",
   "metadata": {},
   "source": [
    "## Exercise 3 Solution: Cost Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise3-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker cost calculator\n",
    "\n",
    "cost_calculator = '''\n",
    "\"\"\"SageMaker Cost Calculator.\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SageMakerCostEstimate:\n",
    "    \"\"\"Cost estimate for SageMaker endpoint.\"\"\"\n",
    "    instance_type: str\n",
    "    instance_count: int\n",
    "    hourly_cost: float\n",
    "    daily_cost: float\n",
    "    monthly_cost: float\n",
    "    cost_per_1k_requests: float\n",
    "    spot_savings: float = 0.0\n",
    "\n",
    "\n",
    "# SageMaker instance pricing (us-west-2, on-demand)\n",
    "SAGEMAKER_PRICING = {\n",
    "    # GPU instances\n",
    "    \"ml.g5.xlarge\": {\"price\": 1.006, \"gpu\": \"A10G\", \"vram\": 24, \"vcpu\": 4, \"memory\": 16},\n",
    "    \"ml.g5.2xlarge\": {\"price\": 1.515, \"gpu\": \"A10G\", \"vram\": 24, \"vcpu\": 8, \"memory\": 32},\n",
    "    \"ml.g5.4xlarge\": {\"price\": 2.533, \"gpu\": \"A10G\", \"vram\": 24, \"vcpu\": 16, \"memory\": 64},\n",
    "    \"ml.g5.8xlarge\": {\"price\": 4.051, \"gpu\": \"A10G\", \"vram\": 24, \"vcpu\": 32, \"memory\": 128},\n",
    "    \"ml.g5.12xlarge\": {\"price\": 7.598, \"gpu\": \"4xA10G\", \"vram\": 96, \"vcpu\": 48, \"memory\": 192},\n",
    "    \"ml.g5.24xlarge\": {\"price\": 10.131, \"gpu\": \"4xA10G\", \"vram\": 96, \"vcpu\": 96, \"memory\": 384},\n",
    "    \"ml.g5.48xlarge\": {\"price\": 20.262, \"gpu\": \"8xA10G\", \"vram\": 192, \"vcpu\": 192, \"memory\": 768},\n",
    "    \"ml.p4d.24xlarge\": {\"price\": 32.773, \"gpu\": \"8xA100\", \"vram\": 320, \"vcpu\": 96, \"memory\": 1152},\n",
    "    # Inf2 (Inferentia)\n",
    "    \"ml.inf2.xlarge\": {\"price\": 0.758, \"accelerator\": \"Inferentia2\", \"vcpu\": 4, \"memory\": 16},\n",
    "    \"ml.inf2.8xlarge\": {\"price\": 3.032, \"accelerator\": \"Inferentia2\", \"vcpu\": 32, \"memory\": 128},\n",
    "}\n",
    "\n",
    "# Spot discount (approximate)\n",
    "SPOT_DISCOUNT = 0.7  # 70% discount\n",
    "\n",
    "\n",
    "def calculate_endpoint_cost(\n",
    "    instance_type: str,\n",
    "    instance_count: int = 1,\n",
    "    requests_per_day: int = 10000,\n",
    "    avg_latency_ms: float = 100,\n",
    "    use_spot: bool = False,\n",
    ") -> SageMakerCostEstimate:\n",
    "    \"\"\"\n",
    "    Calculate estimated cost for a SageMaker endpoint.\n",
    "    \n",
    "    Args:\n",
    "        instance_type: SageMaker instance type\n",
    "        instance_count: Number of instances\n",
    "        requests_per_day: Expected daily requests\n",
    "        avg_latency_ms: Average inference latency\n",
    "        use_spot: Use spot instances\n",
    "    \"\"\"\n",
    "    if instance_type not in SAGEMAKER_PRICING:\n",
    "        raise ValueError(f\"Unknown instance type: {instance_type}\")\n",
    "    \n",
    "    base_price = SAGEMAKER_PRICING[instance_type][\"price\"]\n",
    "    \n",
    "    # Apply spot discount if applicable\n",
    "    spot_savings = 0.0\n",
    "    if use_spot:\n",
    "        spot_savings = base_price * SPOT_DISCOUNT\n",
    "        base_price = base_price * (1 - SPOT_DISCOUNT)\n",
    "    \n",
    "    hourly_cost = base_price * instance_count\n",
    "    daily_cost = hourly_cost * 24\n",
    "    monthly_cost = daily_cost * 30\n",
    "    \n",
    "    # Calculate cost per 1k requests\n",
    "    # Assuming each instance can handle requests_per_second = 1000 / avg_latency_ms\n",
    "    requests_per_second = 1000 / avg_latency_ms\n",
    "    requests_per_hour = requests_per_second * 3600 * instance_count\n",
    "    cost_per_1k = (hourly_cost / requests_per_hour) * 1000\n",
    "    \n",
    "    return SageMakerCostEstimate(\n",
    "        instance_type=instance_type,\n",
    "        instance_count=instance_count,\n",
    "        hourly_cost=hourly_cost,\n",
    "        daily_cost=daily_cost,\n",
    "        monthly_cost=monthly_cost,\n",
    "        cost_per_1k_requests=cost_per_1k,\n",
    "        spot_savings=spot_savings * instance_count * 24 * 30,\n",
    "    )\n",
    "\n",
    "\n",
    "def recommend_instance(\n",
    "    model_size_gb: float,\n",
    "    target_latency_ms: float = 100,\n",
    "    budget_monthly: float = None,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Recommend instance types for a model.\n",
    "    \n",
    "    Args:\n",
    "        model_size_gb: Model size in GB (fp16)\n",
    "        target_latency_ms: Target latency in ms\n",
    "        budget_monthly: Monthly budget cap\n",
    "    \"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    # Rule: need ~2x model size for inference memory\n",
    "    required_vram = model_size_gb * 2\n",
    "    \n",
    "    for instance_type, specs in SAGEMAKER_PRICING.items():\n",
    "        vram = specs.get(\"vram\", 0)\n",
    "        if vram >= required_vram:\n",
    "            monthly_cost = specs[\"price\"] * 24 * 30\n",
    "            \n",
    "            if budget_monthly and monthly_cost > budget_monthly:\n",
    "                continue\n",
    "            \n",
    "            recommendations.append({\n",
    "                \"instance_type\": instance_type,\n",
    "                \"gpu\": specs.get(\"gpu\", \"N/A\"),\n",
    "                \"vram_gb\": vram,\n",
    "                \"monthly_cost\": monthly_cost,\n",
    "                \"headroom_gb\": vram - required_vram,\n",
    "            })\n",
    "    \n",
    "    # Sort by cost\n",
    "    return sorted(recommendations, key=lambda x: x[\"monthly_cost\"])\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Calculate cost for 7B model deployment\n",
    "    estimate = calculate_endpoint_cost(\n",
    "        instance_type=\"ml.g5.xlarge\",\n",
    "        instance_count=2,\n",
    "        requests_per_day=50000,\n",
    "        avg_latency_ms=150,\n",
    "    )\n",
    "    \n",
    "    print(f\"Instance: {estimate.instance_type} x {estimate.instance_count}\")\n",
    "    print(f\"Hourly: ${estimate.hourly_cost:.2f}\")\n",
    "    print(f\"Monthly: ${estimate.monthly_cost:,.0f}\")\n",
    "    print(f\"Per 1K requests: ${estimate.cost_per_1k_requests:.4f}\")\n",
    "'''\n",
    "\n",
    "print(\"SAGEMAKER COST CALCULATOR:\")\n",
    "print(\"=\" * 60)\n",
    "print(cost_calculator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This solution demonstrated:\n",
    "\n",
    "1. **Multi-Model Endpoints with A/B Testing**\n",
    "   - Production variants for traffic splitting\n",
    "   - Data capture for analysis\n",
    "   - CloudWatch metrics analysis\n",
    "\n",
    "2. **Auto-Scaling Configuration**\n",
    "   - Target tracking scaling policy\n",
    "   - Scheduled scaling for known patterns\n",
    "   - Proper cooldown configuration\n",
    "\n",
    "3. **Cost Calculator**\n",
    "   - Instance pricing reference\n",
    "   - Cost estimation per request\n",
    "   - Instance recommendation engine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
