{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Lab 4.4.1: Docker ML Image - SOLUTION\n",
    "\n",
    "**Module:** 4.4 - Containerization & Cloud Deployment  \n",
    "**This is the complete solution notebook with all exercises solved.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise1-header",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Optimize the Dockerfile\n",
    "\n",
    "**Original (with issues):**\n",
    "```dockerfile\n",
    "FROM python:3.10\n",
    "\n",
    "RUN apt-get update\n",
    "RUN apt-get install -y curl\n",
    "RUN apt-get install -y git\n",
    "\n",
    "COPY . /app\n",
    "WORKDIR /app\n",
    "\n",
    "RUN pip install torch\n",
    "RUN pip install transformers\n",
    "RUN pip install fastapi\n",
    "\n",
    "CMD python main.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise1-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Optimized Dockerfile\n",
    "\n",
    "optimized_dockerfile = '''\n",
    "# ============================================\n",
    "# OPTIMIZED DOCKERFILE FOR LLM INFERENCE\n",
    "# ============================================\n",
    "\n",
    "# Improvement 1: Use NGC base image for ARM64 + CUDA compatibility\n",
    "FROM nvcr.io/nvidia/pytorch:24.12-py3 AS builder\n",
    "\n",
    "# Improvement 2: Combine apt commands, clean up cache\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    curl \\\n",
    "    git \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Improvement 3: Copy requirements first for layer caching\n",
    "COPY requirements.txt /tmp/requirements.txt\n",
    "\n",
    "# Improvement 4: Combine pip installs, use --no-cache-dir, install to --user\n",
    "RUN pip install --user --no-cache-dir \\\n",
    "    torch \\\n",
    "    transformers \\\n",
    "    fastapi \\\n",
    "    uvicorn\n",
    "\n",
    "# ============================================\n",
    "# Production stage - minimal image\n",
    "# ============================================\n",
    "FROM nvcr.io/nvidia/pytorch:24.12-py3\n",
    "\n",
    "# Copy only installed packages from builder\n",
    "COPY --from=builder /root/.local /root/.local\n",
    "ENV PATH=/root/.local/bin:$PATH\n",
    "\n",
    "# Improvement 5: Set workdir before copying code\n",
    "WORKDIR /app\n",
    "\n",
    "# Improvement 6: Copy app code LAST (changes most frequently)\n",
    "COPY . /app\n",
    "\n",
    "# Improvement 7: Add health check\n",
    "HEALTHCHECK --interval=30s --timeout=10s --retries=3 \\\n",
    "    CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# Improvement 8: Run as non-root user (security)\n",
    "RUN useradd -m appuser && chown -R appuser:appuser /app\n",
    "USER appuser\n",
    "\n",
    "# Improvement 9: Use explicit command format\n",
    "EXPOSE 8000\n",
    "CMD [\"python\", \"-m\", \"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "'''\n",
    "\n",
    "print(\"OPTIMIZED DOCKERFILE:\")\n",
    "print(\"=\" * 60)\n",
    "print(optimized_dockerfile)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"IMPROVEMENTS MADE:\")\n",
    "print(\"=\" * 60)\n",
    "improvements = [\n",
    "    \"1. NGC base image: Ensures ARM64 + CUDA compatibility for DGX Spark\",\n",
    "    \"2. Combined apt commands: Reduces layers, cleans cache\",\n",
    "    \"3. Requirements first: Better layer caching (deps only reinstall if requirements.txt changes)\",\n",
    "    \"4. Combined pip install: Fewer layers, --no-cache-dir saves space\",\n",
    "    \"5. Multi-stage build: Final image is smaller (no build tools)\",\n",
    "    \"6. Copy code last: Changes to code don't invalidate dependency cache\",\n",
    "    \"7. Health check: Kubernetes/Docker can monitor container health\",\n",
    "    \"8. Non-root user: Security best practice\",\n",
    "    \"9. Explicit CMD format: More robust, easier to override\",\n",
    "]\n",
    "for imp in improvements:\n",
    "    print(f\"  {imp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise2-header",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: RAG Application Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise2-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from scripts.docker_utils import DockerImageBuilder\n",
    "\n",
    "# Create RAG application Dockerfile\n",
    "rag_builder = DockerImageBuilder(\"rag-server\", use_multistage=True)\n",
    "\n",
    "# Use NGC PyTorch base for GPU support\n",
    "rag_builder.add_base(\"nvcr.io/nvidia/pytorch:24.12-py3\")\n",
    "\n",
    "# Add dependencies for RAG pipeline\n",
    "rag_builder.add_python_deps([\n",
    "    # Vector database\n",
    "    \"chromadb>=0.4.0\",\n",
    "    \n",
    "    # Embeddings\n",
    "    \"sentence-transformers>=2.2.0\",\n",
    "    \n",
    "    # LLM\n",
    "    \"transformers>=4.37.0\",\n",
    "    \"accelerate>=0.25.0\",\n",
    "    \"bitsandbytes>=0.41.0\",\n",
    "    \n",
    "    # API\n",
    "    \"fastapi>=0.109.0\",\n",
    "    \"uvicorn>=0.27.0\",\n",
    "    \"pydantic>=2.0.0\",\n",
    "    \n",
    "    # Document processing\n",
    "    \"pypdf>=3.0.0\",\n",
    "    \"python-multipart>=0.0.6\",\n",
    "    \n",
    "    # Utilities\n",
    "    \"langchain>=0.1.0\",\n",
    "    \"tiktoken>=0.5.0\",\n",
    "])\n",
    "\n",
    "# Environment variables\n",
    "rag_builder.add_env(\"MODEL_PATH\", \"/models\")\n",
    "rag_builder.add_env(\"CHROMA_PATH\", \"/data/chroma\")\n",
    "rag_builder.add_env(\"EMBEDDING_MODEL\", \"BAAI/bge-small-en-v1.5\")\n",
    "rag_builder.add_env(\"LLM_MODEL\", \"llama-8b\")\n",
    "rag_builder.add_env(\"CUDA_VISIBLE_DEVICES\", \"0\")\n",
    "\n",
    "# Copy application\n",
    "rag_builder.add_copy(\"app/\", \"/app/\")\n",
    "\n",
    "# Set working directory\n",
    "rag_builder.set_workdir(\"/app\")\n",
    "\n",
    "# Expose port\n",
    "rag_builder.expose(8000)\n",
    "\n",
    "# Add health check\n",
    "rag_builder.add_healthcheck(\"/health\", port=8000, interval=30, timeout=10)\n",
    "\n",
    "# Set entrypoint\n",
    "rag_builder.add_entrypoint(\"python -m uvicorn main:app --host 0.0.0.0 --port 8000\")\n",
    "\n",
    "# Generate and display\n",
    "print(\"RAG APPLICATION DOCKERFILE:\")\n",
    "print(\"=\" * 60)\n",
    "print(rag_builder.generate())\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag-server-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete RAG server implementation\n",
    "\n",
    "rag_server_code = '''\n",
    "\"\"\"RAG Server with ChromaDB and HuggingFace models.\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import List, Optional\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "from fastapi import FastAPI, UploadFile, File, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Configuration\n",
    "CHROMA_PATH = os.environ.get(\"CHROMA_PATH\", \"/data/chroma\")\n",
    "EMBEDDING_MODEL = os.environ.get(\"EMBEDDING_MODEL\", \"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Global clients\n",
    "embedding_model = None\n",
    "chroma_client = None\n",
    "collection = None\n",
    "\n",
    "\n",
    "class QueryRequest(BaseModel):\n",
    "    query: str\n",
    "    top_k: int = 5\n",
    "\n",
    "\n",
    "class QueryResponse(BaseModel):\n",
    "    query: str\n",
    "    results: List[dict]\n",
    "    answer: Optional[str] = None\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    \"\"\"Initialize models on startup.\"\"\"\n",
    "    global embedding_model, chroma_client, collection\n",
    "    \n",
    "    # Load embedding model\n",
    "    embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "    \n",
    "    # Initialize ChromaDB\n",
    "    chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "    collection = chroma_client.get_or_create_collection(\"documents\")\n",
    "    \n",
    "    yield\n",
    "    \n",
    "    # Cleanup\n",
    "    del embedding_model, chroma_client, collection\n",
    "\n",
    "\n",
    "app = FastAPI(title=\"RAG Server\", lifespan=lifespan)\n",
    "\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"healthy\", \"collection_count\": collection.count()}\n",
    "\n",
    "\n",
    "@app.post(\"/ingest\")\n",
    "async def ingest_document(file: UploadFile = File(...)):\n",
    "    \"\"\"Ingest a document into the vector store.\"\"\"\n",
    "    content = await file.read()\n",
    "    text = content.decode(\"utf-8\")\n",
    "    \n",
    "    # Chunk the document\n",
    "    chunks = [text[i:i+512] for i in range(0, len(text), 400)]  # Overlapping chunks\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = embedding_model.encode(chunks).tolist()\n",
    "    \n",
    "    # Add to collection\n",
    "    ids = [f\"{file.filename}_{i}\" for i in range(len(chunks))]\n",
    "    collection.add(\n",
    "        documents=chunks,\n",
    "        embeddings=embeddings,\n",
    "        ids=ids,\n",
    "        metadatas=[{\"source\": file.filename}] * len(chunks),\n",
    "    )\n",
    "    \n",
    "    return {\"status\": \"success\", \"chunks_added\": len(chunks)}\n",
    "\n",
    "\n",
    "@app.post(\"/query\", response_model=QueryResponse)\n",
    "async def query(request: QueryRequest):\n",
    "    \"\"\"Query the vector store.\"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = embedding_model.encode([request.query]).tolist()\n",
    "    \n",
    "    # Search\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,\n",
    "        n_results=request.top_k,\n",
    "    )\n",
    "    \n",
    "    # Format results\n",
    "    formatted_results = []\n",
    "    for i, doc in enumerate(results[\"documents\"][0]):\n",
    "        formatted_results.append({\n",
    "            \"content\": doc,\n",
    "            \"source\": results[\"metadatas\"][0][i].get(\"source\", \"unknown\"),\n",
    "            \"distance\": results[\"distances\"][0][i] if \"distances\" in results else None,\n",
    "        })\n",
    "    \n",
    "    return QueryResponse(\n",
    "        query=request.query,\n",
    "        results=formatted_results,\n",
    "    )\n",
    "'''\n",
    "\n",
    "print(\"RAG Server Implementation:\")\n",
    "print(\"=\" * 60)\n",
    "print(rag_server_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenge-header",
   "metadata": {},
   "source": [
    "## Challenge Solution: Complete Production-Ready Docker Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenge-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete production-ready Dockerfile with vLLM and OpenTelemetry\n",
    "\n",
    "production_dockerfile = '''\n",
    "# ============================================\n",
    "# PRODUCTION-READY LLM INFERENCE SERVER\n",
    "# Features:\n",
    "#   - vLLM for high-performance inference\n",
    "#   - OpenTelemetry for observability\n",
    "#   - Multi-model support via env vars\n",
    "#   - GPU memory monitoring\n",
    "#   - Health and metrics endpoints\n",
    "# ============================================\n",
    "\n",
    "# Build stage\n",
    "FROM nvcr.io/nvidia/pytorch:24.12-py3 AS builder\n",
    "\n",
    "# Install build dependencies\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    build-essential \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Install Python dependencies\n",
    "RUN pip install --user --no-cache-dir \\\n",
    "    vllm>=0.3.0 \\\n",
    "    opentelemetry-api>=1.20.0 \\\n",
    "    opentelemetry-sdk>=1.20.0 \\\n",
    "    opentelemetry-exporter-otlp>=1.20.0 \\\n",
    "    opentelemetry-instrumentation-fastapi>=0.41b0 \\\n",
    "    prometheus-client>=0.19.0 \\\n",
    "    py-spy>=0.3.14 \\\n",
    "    nvidia-ml-py>=12.535.0\n",
    "\n",
    "# ============================================\n",
    "# Production stage\n",
    "# ============================================\n",
    "FROM nvcr.io/nvidia/pytorch:24.12-py3\n",
    "\n",
    "# Copy Python packages\n",
    "COPY --from=builder /root/.local /root/.local\n",
    "ENV PATH=/root/.local/bin:$PATH\n",
    "\n",
    "# Create non-root user\n",
    "RUN useradd -m -u 1000 inference && \\\n",
    "    mkdir -p /app /models /data && \\\n",
    "    chown -R inference:inference /app /models /data\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy application code\n",
    "COPY --chown=inference:inference app/ /app/\n",
    "\n",
    "# Environment variables\n",
    "ENV MODEL_PATH=/models \\\n",
    "    MODEL_NAME=meta-llama/Llama-2-7b-chat-hf \\\n",
    "    CUDA_VISIBLE_DEVICES=0 \\\n",
    "    MAX_MODEL_LEN=4096 \\\n",
    "    GPU_MEMORY_UTILIZATION=0.9 \\\n",
    "    TENSOR_PARALLEL_SIZE=1 \\\n",
    "    OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317 \\\n",
    "    OTEL_SERVICE_NAME=llm-inference\n",
    "\n",
    "# Expose ports\n",
    "EXPOSE 8000\n",
    "EXPOSE 9090\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=15s --start-period=120s --retries=3 \\\n",
    "    CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# Switch to non-root user\n",
    "USER inference\n",
    "\n",
    "# Start server\n",
    "CMD [\"python\", \"-m\", \"vllm.entrypoints.openai.api_server\", \\\n",
    "     \"--model\", \"$MODEL_NAME\", \\\n",
    "     \"--host\", \"0.0.0.0\", \\\n",
    "     \"--port\", \"8000\"]\n",
    "'''\n",
    "\n",
    "print(\"PRODUCTION-READY DOCKERFILE:\")\n",
    "print(\"=\" * 60)\n",
    "print(production_dockerfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpu-metrics-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU monitoring endpoint implementation\n",
    "\n",
    "gpu_monitoring_code = '''\n",
    "\"\"\"GPU Metrics Endpoint for inference server.\"\"\"\n",
    "\n",
    "from fastapi import APIRouter\n",
    "from prometheus_client import Gauge, generate_latest, CONTENT_TYPE_LATEST\n",
    "from starlette.responses import Response\n",
    "\n",
    "try:\n",
    "    import pynvml\n",
    "    NVML_AVAILABLE = True\n",
    "except ImportError:\n",
    "    NVML_AVAILABLE = False\n",
    "\n",
    "router = APIRouter()\n",
    "\n",
    "# Prometheus metrics\n",
    "gpu_memory_used = Gauge(\"gpu_memory_used_bytes\", \"GPU memory used\", [\"gpu_id\"])\n",
    "gpu_memory_total = Gauge(\"gpu_memory_total_bytes\", \"GPU memory total\", [\"gpu_id\"])\n",
    "gpu_utilization = Gauge(\"gpu_utilization_percent\", \"GPU utilization\", [\"gpu_id\"])\n",
    "gpu_temperature = Gauge(\"gpu_temperature_celsius\", \"GPU temperature\", [\"gpu_id\"])\n",
    "\n",
    "\n",
    "def get_gpu_metrics():\n",
    "    \"\"\"Get GPU metrics using NVML.\"\"\"\n",
    "    if not NVML_AVAILABLE:\n",
    "        return [{\"error\": \"NVML not available\"}]\n",
    "    \n",
    "    pynvml.nvmlInit()\n",
    "    device_count = pynvml.nvmlDeviceGetCount()\n",
    "    \n",
    "    metrics = []\n",
    "    for i in range(device_count):\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "        \n",
    "        # Memory info\n",
    "        mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        \n",
    "        # Utilization\n",
    "        util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "        \n",
    "        # Temperature\n",
    "        temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\n",
    "        \n",
    "        # Update Prometheus metrics\n",
    "        gpu_memory_used.labels(gpu_id=str(i)).set(mem_info.used)\n",
    "        gpu_memory_total.labels(gpu_id=str(i)).set(mem_info.total)\n",
    "        gpu_utilization.labels(gpu_id=str(i)).set(util.gpu)\n",
    "        gpu_temperature.labels(gpu_id=str(i)).set(temp)\n",
    "        \n",
    "        metrics.append({\n",
    "            \"gpu_id\": i,\n",
    "            \"memory_used_gb\": mem_info.used / (1024**3),\n",
    "            \"memory_total_gb\": mem_info.total / (1024**3),\n",
    "            \"memory_percent\": (mem_info.used / mem_info.total) * 100,\n",
    "            \"gpu_utilization\": util.gpu,\n",
    "            \"temperature_c\": temp,\n",
    "        })\n",
    "    \n",
    "    pynvml.nvmlShutdown()\n",
    "    return metrics\n",
    "\n",
    "\n",
    "@router.get(\"/gpu\")\n",
    "async def gpu_status():\n",
    "    \"\"\"Get GPU status.\"\"\"\n",
    "    return {\"gpus\": get_gpu_metrics()}\n",
    "\n",
    "\n",
    "@router.get(\"/metrics\")\n",
    "async def prometheus_metrics():\n",
    "    \"\"\"Prometheus metrics endpoint.\"\"\"\n",
    "    # Update GPU metrics before returning\n",
    "    get_gpu_metrics()\n",
    "    \n",
    "    return Response(\n",
    "        content=generate_latest(),\n",
    "        media_type=CONTENT_TYPE_LATEST,\n",
    "    )\n",
    "'''\n",
    "\n",
    "print(\"GPU MONITORING ENDPOINT:\")\n",
    "print(\"=\" * 60)\n",
    "print(gpu_monitoring_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This solution notebook demonstrated:\n",
    "\n",
    "1. **Dockerfile Optimization**\n",
    "   - NGC base images for DGX Spark compatibility\n",
    "   - Multi-stage builds for smaller images\n",
    "   - Layer caching optimization\n",
    "   - Security best practices (non-root user)\n",
    "\n",
    "2. **RAG Application Dockerfile**\n",
    "   - Complete dependency list for RAG pipeline\n",
    "   - ChromaDB + sentence-transformers integration\n",
    "   - Production-ready FastAPI server\n",
    "\n",
    "3. **Production Features**\n",
    "   - vLLM for high-performance inference\n",
    "   - OpenTelemetry for observability\n",
    "   - GPU memory monitoring with NVML\n",
    "   - Prometheus metrics endpoint"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
