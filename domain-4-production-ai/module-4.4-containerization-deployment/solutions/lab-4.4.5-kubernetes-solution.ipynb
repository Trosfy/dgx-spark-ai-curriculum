{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Lab 4.4.5: Kubernetes Deployment - SOLUTION\n",
    "\n",
    "**Module:** 4.4 - Containerization & Cloud Deployment  \n",
    "**This is the complete solution notebook with all exercises solved.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise1-header",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Complete ML Inference Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise1-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from scripts.k8s_utils import (\n",
    "    generate_deployment_manifest,\n",
    "    generate_service_manifest,\n",
    "    generate_hpa_manifest,\n",
    "    generate_configmap_manifest,\n",
    "    generate_pvc_manifest,\n",
    "    save_manifests,\n",
    ")\n",
    "\n",
    "# Generate all components for ML inference stack\n",
    "resources = []\n",
    "\n",
    "# 1. ConfigMap for model configuration\n",
    "configmap = generate_configmap_manifest(\n",
    "    name=\"llm-config\",\n",
    "    data={\n",
    "        \"MODEL_PATH\": \"/models/llama-8b\",\n",
    "        \"MAX_BATCH_SIZE\": \"32\",\n",
    "        \"MAX_INPUT_LENGTH\": \"2048\",\n",
    "        \"MAX_OUTPUT_LENGTH\": \"512\",\n",
    "        \"TEMPERATURE\": \"0.7\",\n",
    "        \"TOP_P\": \"0.9\",\n",
    "    },\n",
    ")\n",
    "resources.append(configmap)\n",
    "\n",
    "# 2. PVC for model storage\n",
    "pvc = generate_pvc_manifest(\n",
    "    name=\"model-storage\",\n",
    "    storage_size=\"100Gi\",\n",
    "    storage_class=\"fast-ssd\",\n",
    ")\n",
    "resources.append(pvc)\n",
    "\n",
    "# 3. Deployment with GPU\n",
    "deployment = generate_deployment_manifest(\n",
    "    name=\"llm-inference\",\n",
    "    image=\"my-registry/llm-inference:v1.0\",\n",
    "    replicas=2,\n",
    "    port=8000,\n",
    "    gpu_count=1,\n",
    "    memory_request=\"32Gi\",\n",
    "    memory_limit=\"64Gi\",\n",
    "    cpu_request=\"8\",\n",
    "    cpu_limit=\"16\",\n",
    "    env_vars={\n",
    "        \"CUDA_VISIBLE_DEVICES\": \"0\",\n",
    "    },\n",
    "    health_path=\"/health\",\n",
    "    volumes=[{\n",
    "        \"name\": \"model-storage\",\n",
    "        \"persistentVolumeClaim\": {\"claimName\": \"model-storage\"},\n",
    "    }, {\n",
    "        \"name\": \"config\",\n",
    "        \"configMap\": {\"name\": \"llm-config\"},\n",
    "    }],\n",
    "    volume_mounts=[{\n",
    "        \"name\": \"model-storage\",\n",
    "        \"mountPath\": \"/models\",\n",
    "    }, {\n",
    "        \"name\": \"config\",\n",
    "        \"mountPath\": \"/etc/config\",\n",
    "    }],\n",
    ")\n",
    "resources.append(deployment)\n",
    "\n",
    "# 4. Service (LoadBalancer for external access)\n",
    "service = generate_service_manifest(\n",
    "    name=\"llm-inference\",\n",
    "    port=80,\n",
    "    target_port=8000,\n",
    "    service_type=\"LoadBalancer\",\n",
    ")\n",
    "resources.append(service)\n",
    "\n",
    "# 5. HPA for auto-scaling\n",
    "hpa = generate_hpa_manifest(\n",
    "    deployment_name=\"llm-inference\",\n",
    "    min_replicas=1,\n",
    "    max_replicas=5,\n",
    "    cpu_target=70,\n",
    ")\n",
    "resources.append(hpa)\n",
    "\n",
    "print(f\"Generated {len(resources)} resources:\")\n",
    "for r in resources:\n",
    "    print(f\"  - {r.kind}: {r.name}\")\n",
    "\n",
    "# Print all manifests\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPLETE K8s MANIFESTS:\")\n",
    "print(\"=\" * 60)\n",
    "for resource in resources:\n",
    "    print(f\"\\n# {resource.kind}: {resource.name}\")\n",
    "    print(resource.to_yaml())\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise2-header",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Pod Disruption Budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise2-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pod Disruption Budget for high availability\n",
    "\n",
    "pdb_manifest = '''\n",
    "apiVersion: policy/v1\n",
    "kind: PodDisruptionBudget\n",
    "metadata:\n",
    "  name: llm-inference-pdb\n",
    "spec:\n",
    "  # Ensure at least 1 pod is always available during disruptions\n",
    "  # (node maintenance, upgrades, etc.)\n",
    "  minAvailable: 1\n",
    "  \n",
    "  # Alternative: maxUnavailable: 1\n",
    "  # (allow only 1 pod to be down at a time)\n",
    "  \n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: llm-inference\n",
    "'''\n",
    "\n",
    "print(\"POD DISRUPTION BUDGET:\")\n",
    "print(\"=\" * 60)\n",
    "print(pdb_manifest)\n",
    "\n",
    "print(\"\\nWHY PDB IS IMPORTANT:\")\n",
    "print(\"  - Prevents all pods being terminated during node drain\")\n",
    "print(\"  - Ensures zero-downtime during cluster upgrades\")\n",
    "print(\"  - Protects against accidental mass deletion\")\n",
    "print(\"  - Required for production workloads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise3-header",
   "metadata": {},
   "source": [
    "## Exercise 3 Solution: Network Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise3-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Policy for security\n",
    "\n",
    "network_policy = '''\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: NetworkPolicy\n",
    "metadata:\n",
    "  name: llm-inference-network-policy\n",
    "spec:\n",
    "  podSelector:\n",
    "    matchLabels:\n",
    "      app: llm-inference\n",
    "  policyTypes:\n",
    "    - Ingress\n",
    "    - Egress\n",
    "  \n",
    "  ingress:\n",
    "    # Allow traffic from API gateway\n",
    "    - from:\n",
    "        - podSelector:\n",
    "            matchLabels:\n",
    "              app: api-gateway\n",
    "      ports:\n",
    "        - protocol: TCP\n",
    "          port: 8000\n",
    "    \n",
    "    # Allow traffic from monitoring\n",
    "    - from:\n",
    "        - namespaceSelector:\n",
    "            matchLabels:\n",
    "              name: monitoring\n",
    "      ports:\n",
    "        - protocol: TCP\n",
    "          port: 9090  # Prometheus metrics\n",
    "  \n",
    "  egress:\n",
    "    # Allow DNS resolution\n",
    "    - to:\n",
    "        - namespaceSelector: {}\n",
    "          podSelector:\n",
    "            matchLabels:\n",
    "              k8s-app: kube-dns\n",
    "      ports:\n",
    "        - protocol: UDP\n",
    "          port: 53\n",
    "    \n",
    "    # Allow access to model storage (NFS/S3)\n",
    "    - to:\n",
    "        - ipBlock:\n",
    "            cidr: 10.0.0.0/8  # Internal network\n",
    "      ports:\n",
    "        - protocol: TCP\n",
    "          port: 2049  # NFS\n",
    "        - protocol: TCP\n",
    "          port: 443   # S3/HTTPS\n",
    "'''\n",
    "\n",
    "print(\"NETWORK POLICY:\")\n",
    "print(\"=\" * 60)\n",
    "print(network_policy)\n",
    "\n",
    "print(\"\\nSECURITY BENEFITS:\")\n",
    "print(\"  - Limits attack surface (only allowed traffic)\")\n",
    "print(\"  - Prevents lateral movement in cluster\")\n",
    "print(\"  - Enforces zero-trust networking\")\n",
    "print(\"  - Required for compliance (SOC2, HIPAA)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise4-header",
   "metadata": {},
   "source": [
    "## Exercise 4 Solution: Custom HPA with GPU Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise4-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPA with custom GPU metrics\n",
    "\n",
    "gpu_hpa_manifest = '''\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: llm-inference-gpu-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: llm-inference\n",
    "  minReplicas: 1\n",
    "  maxReplicas: 5\n",
    "  \n",
    "  metrics:\n",
    "    # Standard CPU metric\n",
    "    - type: Resource\n",
    "      resource:\n",
    "        name: cpu\n",
    "        target:\n",
    "          type: Utilization\n",
    "          averageUtilization: 70\n",
    "    \n",
    "    # Custom GPU utilization metric\n",
    "    # Requires DCGM exporter + Prometheus adapter\n",
    "    - type: Pods\n",
    "      pods:\n",
    "        metric:\n",
    "          name: DCGM_FI_DEV_GPU_UTIL\n",
    "        target:\n",
    "          type: AverageValue\n",
    "          averageValue: \"80\"  # 80% GPU utilization\n",
    "    \n",
    "    # Custom GPU memory metric\n",
    "    - type: Pods\n",
    "      pods:\n",
    "        metric:\n",
    "          name: DCGM_FI_DEV_FB_USED_PERCENT\n",
    "        target:\n",
    "          type: AverageValue\n",
    "          averageValue: \"85\"  # 85% GPU memory usage\n",
    "    \n",
    "    # Requests per second (from Prometheus)\n",
    "    - type: Object\n",
    "      object:\n",
    "        metric:\n",
    "          name: http_requests_per_second\n",
    "        describedObject:\n",
    "          apiVersion: v1\n",
    "          kind: Service\n",
    "          name: llm-inference-service\n",
    "        target:\n",
    "          type: Value\n",
    "          value: \"100\"  # Scale at 100 RPS\n",
    "  \n",
    "  behavior:\n",
    "    scaleDown:\n",
    "      # Slow scale down (models take time to load)\n",
    "      stabilizationWindowSeconds: 300\n",
    "      policies:\n",
    "        - type: Pods\n",
    "          value: 1\n",
    "          periodSeconds: 60\n",
    "    scaleUp:\n",
    "      # Fast scale up for traffic spikes\n",
    "      stabilizationWindowSeconds: 60\n",
    "      policies:\n",
    "        - type: Pods\n",
    "          value: 2\n",
    "          periodSeconds: 60\n",
    "'''\n",
    "\n",
    "print(\"GPU-AWARE HPA:\")\n",
    "print(\"=\" * 60)\n",
    "print(gpu_hpa_manifest)\n",
    "\n",
    "print(\"\\nREQUIREMENTS FOR GPU METRICS:\")\n",
    "print(\"  1. Install DCGM exporter:\")\n",
    "print(\"     kubectl apply -f https://github.com/NVIDIA/dcgm-exporter/...\")\n",
    "print(\"  2. Install Prometheus adapter:\")\n",
    "print(\"     helm install prometheus-adapter prometheus-community/prometheus-adapter\")\n",
    "print(\"  3. Configure custom metrics API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenge-header",
   "metadata": {},
   "source": [
    "## Challenge Solution: Blue-Green Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenge-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blue-Green deployment strategy\n",
    "\n",
    "blue_green_manifests = '''\n",
    "# ==============================================\n",
    "# Blue-Green Deployment for LLM Inference\n",
    "# ==============================================\n",
    "\n",
    "# Blue Deployment (current production)\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: llm-inference-blue\n",
    "  labels:\n",
    "    app: llm-inference\n",
    "    version: blue\n",
    "spec:\n",
    "  replicas: 2\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: llm-inference\n",
    "      version: blue\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: llm-inference\n",
    "        version: blue\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: llm-inference\n",
    "        image: my-registry/llm-inference:v1.0\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        resources:\n",
    "          limits:\n",
    "            nvidia.com/gpu: 1\n",
    "---\n",
    "# Green Deployment (new version)\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: llm-inference-green\n",
    "  labels:\n",
    "    app: llm-inference\n",
    "    version: green\n",
    "spec:\n",
    "  replicas: 2\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: llm-inference\n",
    "      version: green\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: llm-inference\n",
    "        version: green\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: llm-inference\n",
    "        image: my-registry/llm-inference:v2.0  # New version\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        resources:\n",
    "          limits:\n",
    "            nvidia.com/gpu: 1\n",
    "---\n",
    "# Service (points to blue by default)\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: llm-inference-service\n",
    "spec:\n",
    "  type: LoadBalancer\n",
    "  ports:\n",
    "  - port: 80\n",
    "    targetPort: 8000\n",
    "  selector:\n",
    "    app: llm-inference\n",
    "    version: blue  # Change to \"green\" to switch\n",
    "'''\n",
    "\n",
    "print(\"BLUE-GREEN DEPLOYMENT:\")\n",
    "print(\"=\" * 60)\n",
    "print(blue_green_manifests)\n",
    "\n",
    "# Switching script\n",
    "switch_script = '''\n",
    "#!/bin/bash\n",
    "# switch-traffic.sh - Switch between blue and green\n",
    "\n",
    "TARGET=${1:-green}  # Default to green\n",
    "\n",
    "echo \"Switching traffic to: $TARGET\"\n",
    "\n",
    "# Update service selector\n",
    "kubectl patch service llm-inference-service -p '{\"spec\":{\"selector\":{\"version\":\"'$TARGET'\"}}}'\n",
    "\n",
    "# Verify\n",
    "echo \"Current service target:\"\n",
    "kubectl get service llm-inference-service -o jsonpath='{.spec.selector.version}'\n",
    "echo \"\"\n",
    "'''\n",
    "\n",
    "print(\"\\nTRAFFIC SWITCHING SCRIPT:\")\n",
    "print(\"=\" * 60)\n",
    "print(switch_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This solution demonstrated:\n",
    "\n",
    "1. **Complete ML Stack**\n",
    "   - ConfigMap for configuration\n",
    "   - PVC for model storage\n",
    "   - GPU-enabled Deployment\n",
    "   - LoadBalancer Service\n",
    "   - HPA for auto-scaling\n",
    "\n",
    "2. **Production Features**\n",
    "   - Pod Disruption Budget for HA\n",
    "   - Network Policy for security\n",
    "   - GPU-aware HPA with custom metrics\n",
    "   - Blue-Green deployment strategy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
