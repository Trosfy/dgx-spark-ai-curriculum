{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Lab 4.4.6: Gradio Demo - SOLUTION\n",
    "\n",
    "**Module:** 4.4 - Containerization & Cloud Deployment  \n",
    "**This is the complete solution notebook with all exercises solved.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise1-header",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Multi-Model Comparison Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise1-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Multi-model comparison interface\n",
    "\n",
    "model_comparison_code = '''\n",
    "\"\"\"Multi-model comparison interface with Gradio.\"\"\"\n",
    "\n",
    "import gradio as gr\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "# Mock inference functions (replace with real models)\n",
    "def model_a_inference(prompt: str) -> Tuple[str, float]:\n",
    "    \"\"\"Model A (e.g., Llama-7B).\"\"\"\n",
    "    start = time.time()\n",
    "    # Simulate inference\n",
    "    time.sleep(0.5)\n",
    "    response = f\"[Model A] Response to: {prompt}\\n\\nThis is a simulated response from Llama-7B.\"\n",
    "    latency = (time.time() - start) * 1000\n",
    "    return response, latency\n",
    "\n",
    "\n",
    "def model_b_inference(prompt: str) -> Tuple[str, float]:\n",
    "    \"\"\"Model B (e.g., Mistral-7B).\"\"\"\n",
    "    start = time.time()\n",
    "    # Simulate inference\n",
    "    time.sleep(0.3)\n",
    "    response = f\"[Model B] Response to: {prompt}\\n\\nThis is a simulated response from Mistral-7B.\"\n",
    "    latency = (time.time() - start) * 1000\n",
    "    return response, latency\n",
    "\n",
    "\n",
    "def compare_models(prompt: str, model_a_name: str, model_b_name: str):\n",
    "    \"\"\"Compare two models side by side.\"\"\"\n",
    "    # Get responses from both models\n",
    "    response_a, latency_a = model_a_inference(prompt)\n",
    "    response_b, latency_b = model_b_inference(prompt)\n",
    "    \n",
    "    return (\n",
    "        response_a,\n",
    "        response_b,\n",
    "        f\"{latency_a:.0f}ms\",\n",
    "        f\"{latency_b:.0f}ms\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Build the interface\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# Model Comparison Tool\")\n",
    "    gr.Markdown(\"Compare responses from two LLMs side by side\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            model_a_select = gr.Dropdown(\n",
    "                choices=[\"llama-7b\", \"llama-13b\", \"llama-70b\"],\n",
    "                value=\"llama-7b\",\n",
    "                label=\"Model A\",\n",
    "            )\n",
    "        with gr.Column():\n",
    "            model_b_select = gr.Dropdown(\n",
    "                choices=[\"mistral-7b\", \"mixtral-8x7b\", \"codellama-7b\"],\n",
    "                value=\"mistral-7b\",\n",
    "                label=\"Model B\",\n",
    "            )\n",
    "    \n",
    "    prompt_input = gr.Textbox(\n",
    "        label=\"Prompt\",\n",
    "        placeholder=\"Enter your prompt here...\",\n",
    "        lines=3,\n",
    "    )\n",
    "    \n",
    "    compare_btn = gr.Button(\"Compare\", variant=\"primary\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### Model A Response\")\n",
    "            output_a = gr.Textbox(label=\"Response\", lines=10)\n",
    "            latency_a = gr.Textbox(label=\"Latency\")\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### Model B Response\")\n",
    "            output_b = gr.Textbox(label=\"Response\", lines=10)\n",
    "            latency_b = gr.Textbox(label=\"Latency\")\n",
    "    \n",
    "    # Connect the button\n",
    "    compare_btn.click(\n",
    "        fn=compare_models,\n",
    "        inputs=[prompt_input, model_a_select, model_b_select],\n",
    "        outputs=[output_a, output_b, latency_a, latency_b],\n",
    "    )\n",
    "    \n",
    "    # Examples\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            [\"Explain quantum computing in simple terms\"],\n",
    "            [\"Write a Python function to reverse a string\"],\n",
    "            [\"What are the benefits of using Docker?\"],\n",
    "        ],\n",
    "        inputs=[prompt_input],\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n",
    "'''\n",
    "\n",
    "print(\"MODEL COMPARISON INTERFACE:\")\n",
    "print(\"=\" * 60)\n",
    "print(model_comparison_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise2-header",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: RAG Chat Interface with Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise2-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG interface with source display\n",
    "\n",
    "rag_interface_code = '''\n",
    "\"\"\"RAG Chat Interface with Source Display.\"\"\"\n",
    "\n",
    "import gradio as gr\n",
    "from typing import List, Tuple, Dict\n",
    "import json\n",
    "\n",
    "\n",
    "# Simulated RAG functions\n",
    "def search_documents(query: str) -> List[Dict]:\n",
    "    \"\"\"Search vector database for relevant documents.\"\"\"\n",
    "    # Mock search results\n",
    "    return [\n",
    "        {\n",
    "            \"content\": f\"Document 1 relevant to: {query}\",\n",
    "            \"source\": \"docs/guide.md\",\n",
    "            \"score\": 0.92,\n",
    "        },\n",
    "        {\n",
    "            \"content\": f\"Document 2 with more context about: {query}\",\n",
    "            \"source\": \"docs/tutorial.md\", \n",
    "            \"score\": 0.87,\n",
    "        },\n",
    "        {\n",
    "            \"content\": f\"Document 3 providing details on: {query}\",\n",
    "            \"source\": \"docs/reference.md\",\n",
    "            \"score\": 0.81,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "\n",
    "def generate_with_context(query: str, context: List[Dict]) -> str:\n",
    "    \"\"\"Generate response using retrieved context.\"\"\"\n",
    "    # Mock LLM generation\n",
    "    context_text = \"\\n\".join([d[\"content\"] for d in context])\n",
    "    return f\"Based on the provided documents:\\n\\n{context_text}\\n\\nHere is my answer to: {query}\"\n",
    "\n",
    "\n",
    "def rag_chat(message: str, history: List, show_sources: bool = True):\n",
    "    \"\"\"RAG chat function with optional source display.\"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    sources = search_documents(message)\n",
    "    \n",
    "    # Step 2: Generate response with context\n",
    "    response = generate_with_context(message, sources)\n",
    "    \n",
    "    # Step 3: Format sources if requested\n",
    "    if show_sources:\n",
    "        source_text = \"\\n\\n---\\n**Sources:**\\n\"\n",
    "        for s in sources:\n",
    "            source_text += f\"- [{s['source']}] (score: {s['score']:.2f})\\n\"\n",
    "        response += source_text\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "# Document store\n",
    "uploaded_documents = []\n",
    "\n",
    "\n",
    "def upload_document(file):\n",
    "    \"\"\"Process uploaded document.\"\"\"\n",
    "    if file is None:\n",
    "        return \"No file uploaded\"\n",
    "    \n",
    "    # Read and store document\n",
    "    try:\n",
    "        with open(file.name, 'r') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        uploaded_documents.append({\n",
    "            \"name\": file.name.split(\"/\")[-1],\n",
    "            \"content\": content,\n",
    "        })\n",
    "        \n",
    "        return f\"Uploaded: {file.name.split('/')[-1]} ({len(content)} chars)\\nTotal documents: {len(uploaded_documents)}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "# Build interface\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# RAG Chat Demo\")\n",
    "    gr.Markdown(\"Chat with your documents using Retrieval-Augmented Generation\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        # Left panel: Document upload\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"### Documents\")\n",
    "            file_upload = gr.File(\n",
    "                label=\"Upload Document\",\n",
    "                file_types=[\".txt\", \".md\", \".pdf\"],\n",
    "            )\n",
    "            upload_status = gr.Textbox(\n",
    "                label=\"Status\",\n",
    "                interactive=False,\n",
    "            )\n",
    "            file_upload.change(upload_document, file_upload, upload_status)\n",
    "            \n",
    "            show_sources = gr.Checkbox(\n",
    "                label=\"Show Sources\",\n",
    "                value=True,\n",
    "            )\n",
    "        \n",
    "        # Right panel: Chat\n",
    "        with gr.Column(scale=3):\n",
    "            chatbot = gr.Chatbot(\n",
    "                height=500,\n",
    "                show_label=False,\n",
    "            )\n",
    "            msg = gr.Textbox(\n",
    "                label=\"Message\",\n",
    "                placeholder=\"Ask a question about your documents...\",\n",
    "            )\n",
    "            \n",
    "            def respond(message, history, show_src):\n",
    "                response = rag_chat(message, history, show_src)\n",
    "                history.append((message, response))\n",
    "                return \"\", history\n",
    "            \n",
    "            msg.submit(respond, [msg, chatbot, show_sources], [msg, chatbot])\n",
    "            \n",
    "            clear_btn = gr.Button(\"Clear Chat\")\n",
    "            clear_btn.click(lambda: [], outputs=[chatbot])\n",
    "    \n",
    "    # Examples\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            [\"What are the main topics covered?\"],\n",
    "            [\"Summarize the key points\"],\n",
    "            [\"How do I get started?\"],\n",
    "        ],\n",
    "        inputs=[msg],\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n",
    "'''\n",
    "\n",
    "print(\"RAG CHAT INTERFACE:\")\n",
    "print(\"=\" * 60)\n",
    "print(rag_interface_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise3-header",
   "metadata": {},
   "source": [
    "## Exercise 3 Solution: Production-Ready Hugging Face Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise3-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete HuggingFace Space app.py\n",
    "\n",
    "hf_space_app = '''\n",
    "\"\"\"Production-ready Gradio App for Hugging Face Spaces.\n",
    "\n",
    "Features:\n",
    "- Streaming responses\n",
    "- Token counting\n",
    "- Error handling\n",
    "- Performance metrics\n",
    "- Rate limiting\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from typing import List, Iterator, Tuple\n",
    "\n",
    "import gradio as gr\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# ============================================\n",
    "# Configuration\n",
    "# ============================================\n",
    "\n",
    "MODEL_ID = os.environ.get(\"MODEL_ID\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "MAX_TOKENS = int(os.environ.get(\"MAX_TOKENS\", 512))\n",
    "RATE_LIMIT_PER_MINUTE = int(os.environ.get(\"RATE_LIMIT\", 10))\n",
    "\n",
    "# Rate limiting\n",
    "request_times = deque(maxlen=100)\n",
    "request_lock = threading.Lock()\n",
    "\n",
    "# ============================================\n",
    "# Model Loading\n",
    "# ============================================\n",
    "\n",
    "print(f\"Loading model: {MODEL_ID}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "# ============================================\n",
    "# Helper Functions\n",
    "# ============================================\n",
    "\n",
    "def check_rate_limit() -> bool:\n",
    "    \"\"\"Check if request is within rate limit.\"\"\"\n",
    "    with request_lock:\n",
    "        now = time.time()\n",
    "        # Remove old requests\n",
    "        while request_times and now - request_times[0] > 60:\n",
    "            request_times.popleft()\n",
    "        \n",
    "        if len(request_times) >= RATE_LIMIT_PER_MINUTE:\n",
    "            return False\n",
    "        \n",
    "        request_times.append(now)\n",
    "        return True\n",
    "\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Count tokens in text.\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "\n",
    "def format_chat_prompt(message: str, history: List[Tuple[str, str]]) -> str:\n",
    "    \"\"\"Format chat history into a prompt.\"\"\"\n",
    "    prompt = \"<|system|>You are a helpful AI assistant.</s>\"\n",
    "    \n",
    "    for user_msg, assistant_msg in history:\n",
    "        prompt += f\"<|user|>{user_msg}</s>\"\n",
    "        prompt += f\"<|assistant|>{assistant_msg}</s>\"\n",
    "    \n",
    "    prompt += f\"<|user|>{message}</s><|assistant|>\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Inference Function\n",
    "# ============================================\n",
    "\n",
    "def chat(message: str, history: List[Tuple[str, str]]) -> Iterator[str]:\n",
    "    \"\"\"Chat function with streaming.\"\"\"\n",
    "    \n",
    "    # Rate limiting\n",
    "    if not check_rate_limit():\n",
    "        yield \"Rate limit exceeded. Please wait a moment and try again.\"\n",
    "        return\n",
    "    \n",
    "    # Input validation\n",
    "    if not message.strip():\n",
    "        yield \"Please enter a message.\"\n",
    "        return\n",
    "    \n",
    "    if count_tokens(message) > 1000:\n",
    "        yield \"Message too long. Please keep it under 1000 tokens.\"\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Format prompt\n",
    "        prompt = format_chat_prompt(message, history)\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # Generate with streaming\n",
    "        streamer = transformers.TextIteratorStreamer(\n",
    "            tokenizer,\n",
    "            skip_prompt=True,\n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "        \n",
    "        generation_kwargs = {\n",
    "            **inputs,\n",
    "            \"streamer\": streamer,\n",
    "            \"max_new_tokens\": MAX_TOKENS,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"do_sample\": True,\n",
    "        }\n",
    "        \n",
    "        # Start generation in background\n",
    "        thread = threading.Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "        thread.start()\n",
    "        \n",
    "        # Stream output\n",
    "        generated = \"\"\n",
    "        for token in streamer:\n",
    "            generated += token\n",
    "            yield generated\n",
    "        \n",
    "        thread.join()\n",
    "        \n",
    "    except Exception as e:\n",
    "        yield f\"Error: {str(e)}. Please try again.\"\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Gradio Interface\n",
    "# ============================================\n",
    "\n",
    "# Custom CSS for NVIDIA theme\n",
    "custom_css = \"\"\"\n",
    ".gradio-container {\n",
    "    max-width: 900px !important;\n",
    "}\n",
    "#component-0 {\n",
    "    background: linear-gradient(135deg, #76b900 0%, #1a1a2e 100%);\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Build interface\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    title=\"DGX Spark AI Assistant\",\n",
    "    description=f\"\"\"\n",
    "    Powered by {MODEL_ID} running on DGX Spark.\n",
    "    \n",
    "    - Max tokens: {MAX_TOKENS}\n",
    "    - Rate limit: {RATE_LIMIT_PER_MINUTE} requests/minute\n",
    "    \"\"\",\n",
    "    examples=[\n",
    "        \"What is the DGX Spark?\",\n",
    "        \"Explain GPU memory management in simple terms\",\n",
    "        \"Write a Python function to calculate factorial\",\n",
    "        \"What are the benefits of containerization?\",\n",
    "    ],\n",
    "    theme=gr.themes.Soft(\n",
    "        primary_hue=\"green\",\n",
    "        neutral_hue=\"slate\",\n",
    "    ),\n",
    "    css=custom_css,\n",
    "    retry_btn=\"Retry\",\n",
    "    undo_btn=\"Undo\",\n",
    "    clear_btn=\"Clear\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(\n",
    "        server_name=\"0.0.0.0\",\n",
    "        server_port=7860,\n",
    "    )\n",
    "'''\n",
    "\n",
    "print(\"PRODUCTION HUGGING FACE SPACE APP:\")\n",
    "print(\"=\" * 60)\n",
    "print(hf_space_app)\n",
    "\n",
    "# Save to file\n",
    "import os\n",
    "os.makedirs(\"../docker-examples/gradio-space\", exist_ok=True)\n",
    "with open(\"../docker-examples/gradio-space/app.py\", \"w\") as f:\n",
    "    f.write(hf_space_app)\n",
    "print(\"\\nSaved to: ../docker-examples/gradio-space/app.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This solution demonstrated:\n",
    "\n",
    "1. **Model Comparison Interface**\n",
    "   - Side-by-side model outputs\n",
    "   - Latency measurement\n",
    "   - Dropdown model selection\n",
    "\n",
    "2. **RAG Chat Interface**\n",
    "   - Document upload and processing\n",
    "   - Source attribution display\n",
    "   - Toggle for showing sources\n",
    "\n",
    "3. **Production HuggingFace Space**\n",
    "   - Streaming responses\n",
    "   - Rate limiting\n",
    "   - Error handling\n",
    "   - Custom theming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
