{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Lab 4.4.4: GCP Vertex AI Deployment - SOLUTION\n",
    "\n",
    "**Module:** 4.4 - Containerization & Cloud Deployment  \n",
    "**This is the complete solution notebook with all exercises solved.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise1-header",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Custom Container for Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise1-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertex AI compliant container implementation\n",
    "\n",
    "vertex_container_code = '''\n",
    "\"\"\"FastAPI Application for Vertex AI Custom Container.\n",
    "\n",
    "This server is designed to meet Vertex AI's requirements:\n",
    "- Health endpoint at /health (or AIP_HEALTH_ROUTE)\n",
    "- Prediction endpoint at /predict (or AIP_PREDICT_ROUTE)\n",
    "- Listens on port 8080 (or AIP_HTTP_PORT)\n",
    "- Model loaded from AIP_STORAGE_URI\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Configuration from Vertex AI environment\n",
    "# ============================================\n",
    "\n",
    "MODEL_PATH = os.environ.get(\"AIP_STORAGE_URI\", \"/models\")\n",
    "HTTP_PORT = int(os.environ.get(\"AIP_HTTP_PORT\", 8080))\n",
    "HEALTH_ROUTE = os.environ.get(\"AIP_HEALTH_ROUTE\", \"/health\")\n",
    "PREDICT_ROUTE = os.environ.get(\"AIP_PREDICT_ROUTE\", \"/predict\")\n",
    "\n",
    "# Model configuration\n",
    "MAX_NEW_TOKENS = int(os.environ.get(\"MAX_NEW_TOKENS\", 512))\n",
    "TEMPERATURE = float(os.environ.get(\"TEMPERATURE\", 0.7))\n",
    "TOP_P = float(os.environ.get(\"TOP_P\", 0.9))\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Global model storage\n",
    "# ============================================\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "model_loaded = False\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Request/Response Models (Vertex AI format)\n",
    "# ============================================\n",
    "\n",
    "class PredictionInstance(BaseModel):\n",
    "    \"\"\"Single prediction instance.\"\"\"\n",
    "    prompt: str\n",
    "    max_tokens: Optional[int] = None\n",
    "    temperature: Optional[float] = None\n",
    "    top_p: Optional[float] = None\n",
    "\n",
    "\n",
    "class PredictionRequest(BaseModel):\n",
    "    \"\"\"Vertex AI prediction request format.\"\"\"\n",
    "    instances: List[PredictionInstance]\n",
    "\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    \"\"\"Vertex AI prediction response format.\"\"\"\n",
    "    predictions: List[Dict[str, Any]]\n",
    "    metadata: Optional[Dict[str, Any]] = None\n",
    "\n",
    "\n",
    "class HealthResponse(BaseModel):\n",
    "    \"\"\"Health check response.\"\"\"\n",
    "    status: str\n",
    "    model_loaded: bool\n",
    "    gpu_available: bool\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Model loading\n",
    "# ============================================\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    \"\"\"Load model on startup.\"\"\"\n",
    "    global model, tokenizer, model_loaded\n",
    "    \n",
    "    print(f\"Loading model from: {MODEL_PATH}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        model_loaded = True\n",
    "        \n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"Model loaded in {load_time:.2f}s\")\n",
    "        print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load model: {e}\")\n",
    "        model_loaded = False\n",
    "    \n",
    "    yield\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# FastAPI App\n",
    "# ============================================\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Vertex AI LLM Server\",\n",
    "    description=\"Custom container for Vertex AI deployment\",\n",
    "    lifespan=lifespan,\n",
    ")\n",
    "\n",
    "\n",
    "@app.get(HEALTH_ROUTE, response_model=HealthResponse)\n",
    "@app.get(\"/ping\")  # Alternative health route\n",
    "async def health():\n",
    "    \"\"\"Health check endpoint (required by Vertex AI).\"\"\"\n",
    "    return HealthResponse(\n",
    "        status=\"healthy\" if model_loaded else \"unhealthy\",\n",
    "        model_loaded=model_loaded,\n",
    "        gpu_available=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "\n",
    "@app.post(PREDICT_ROUTE, response_model=PredictionResponse)\n",
    "async def predict(request: PredictionRequest):\n",
    "    \"\"\"Prediction endpoint (required by Vertex AI).\"\"\"\n",
    "    if not model_loaded:\n",
    "        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n",
    "    \n",
    "    predictions = []\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for instance in request.instances:\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            instance.prompt,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(model.device)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=instance.max_tokens or MAX_NEW_TOKENS,\n",
    "                temperature=instance.temperature or TEMPERATURE,\n",
    "                top_p=instance.top_p or TOP_P,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        generated = tokenizer.decode(\n",
    "            outputs[0][inputs[\"input_ids\"].shape[1]:],\n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "        \n",
    "        tokens_generated = outputs.shape[1] - inputs[\"input_ids\"].shape[1]\n",
    "        total_tokens += tokens_generated\n",
    "        \n",
    "        predictions.append({\n",
    "            \"generated_text\": generated,\n",
    "            \"tokens_generated\": tokens_generated,\n",
    "        })\n",
    "    \n",
    "    return PredictionResponse(\n",
    "        predictions=predictions,\n",
    "        metadata={\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"model_path\": MODEL_PATH,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=HTTP_PORT)\n",
    "'''\n",
    "\n",
    "print(\"VERTEX AI CUSTOM CONTAINER CODE:\")\n",
    "print(\"=\" * 60)\n",
    "print(vertex_container_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise2-header",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Platform Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise2-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive platform comparison\n",
    "\n",
    "platform_comparison = '''\n",
    "\"\"\"Cloud Platform Comparison for ML Deployment.\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PlatformComparison:\n",
    "    \"\"\"Side-by-side platform comparison.\"\"\"\n",
    "    \n",
    "    def compare_all(self, model_size_gb: float, requests_per_day: int) -> Dict[str, Any]:\n",
    "        \"\"\"Generate comprehensive comparison.\"\"\"\n",
    "        \n",
    "        comparison = {\n",
    "            \"model_size_gb\": model_size_gb,\n",
    "            \"requests_per_day\": requests_per_day,\n",
    "            \"platforms\": {},\n",
    "        }\n",
    "        \n",
    "        # AWS SageMaker\n",
    "        comparison[\"platforms\"][\"sagemaker\"] = {\n",
    "            \"name\": \"AWS SageMaker\",\n",
    "            \"recommended_instance\": self._recommend_sagemaker_instance(model_size_gb),\n",
    "            \"pros\": [\n",
    "                \"Best GPU instance selection (A10G, A100, H100)\",\n",
    "                \"Mature MLOps tooling (Pipelines, Model Registry)\",\n",
    "                \"HuggingFace integration built-in\",\n",
    "                \"Inference recommender for instance selection\",\n",
    "                \"Shadow testing for safe deployments\",\n",
    "            ],\n",
    "            \"cons\": [\n",
    "                \"Complex IAM configuration\",\n",
    "                \"Steeper learning curve\",\n",
    "                \"Pricing can be confusing\",\n",
    "                \"Cold start times can be high\",\n",
    "            ],\n",
    "            \"best_for\": [\n",
    "                \"AWS-centric organizations\",\n",
    "                \"Complex ML pipelines\",\n",
    "                \"Large-scale production workloads\",\n",
    "            ],\n",
    "        }\n",
    "        \n",
    "        # GCP Vertex AI\n",
    "        comparison[\"platforms\"][\"vertex\"] = {\n",
    "            \"name\": \"GCP Vertex AI\",\n",
    "            \"recommended_instance\": self._recommend_vertex_instance(model_size_gb),\n",
    "            \"pros\": [\n",
    "                \"Native BigQuery integration\",\n",
    "                \"Simpler pricing model\",\n",
    "                \"Strong AutoML capabilities\",\n",
    "                \"Easy custom container deployment\",\n",
    "                \"Good Kubernetes integration (GKE)\",\n",
    "            ],\n",
    "            \"cons\": [\n",
    "                \"Fewer GPU options than AWS\",\n",
    "                \"Less mature than SageMaker\",\n",
    "                \"Smaller community/ecosystem\",\n",
    "                \"Limited spot instance support\",\n",
    "            ],\n",
    "            \"best_for\": [\n",
    "                \"GCP-centric organizations\",\n",
    "                \"Data in BigQuery\",\n",
    "                \"Simpler deployment needs\",\n",
    "            ],\n",
    "        }\n",
    "        \n",
    "        # Self-hosted (EKS/GKE + vLLM)\n",
    "        comparison[\"platforms\"][\"self_hosted\"] = {\n",
    "            \"name\": \"Self-hosted (K8s + vLLM)\",\n",
    "            \"recommended_instance\": self._recommend_k8s_instance(model_size_gb),\n",
    "            \"pros\": [\n",
    "                \"Maximum flexibility\",\n",
    "                \"30-50% cost savings vs managed\",\n",
    "                \"No vendor lock-in\",\n",
    "                \"Latest optimizations (vLLM, SGLang)\",\n",
    "                \"Full control over infrastructure\",\n",
    "            ],\n",
    "            \"cons\": [\n",
    "                \"Operational overhead\",\n",
    "                \"Need K8s expertise\",\n",
    "                \"Build own monitoring/scaling\",\n",
    "                \"Security responsibility\",\n",
    "            ],\n",
    "            \"best_for\": [\n",
    "                \"High-volume workloads\",\n",
    "                \"Cost-sensitive projects\",\n",
    "                \"Teams with K8s experience\",\n",
    "            ],\n",
    "        }\n",
    "        \n",
    "        # Calculate costs\n",
    "        for platform in comparison[\"platforms\"].values():\n",
    "            instance = platform[\"recommended_instance\"]\n",
    "            platform[\"estimated_monthly_cost\"] = instance[\"price\"] * 24 * 30\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def _recommend_sagemaker_instance(self, model_size_gb: float) -> Dict:\n",
    "        if model_size_gb <= 14:\n",
    "            return {\"type\": \"ml.g5.xlarge\", \"gpu\": \"A10G 24GB\", \"price\": 1.006}\n",
    "        elif model_size_gb <= 28:\n",
    "            return {\"type\": \"ml.g5.2xlarge\", \"gpu\": \"A10G 24GB\", \"price\": 1.515}\n",
    "        elif model_size_gb <= 48:\n",
    "            return {\"type\": \"ml.g5.4xlarge\", \"gpu\": \"A10G 24GB\", \"price\": 2.533}\n",
    "        else:\n",
    "            return {\"type\": \"ml.g5.12xlarge\", \"gpu\": \"4xA10G 96GB\", \"price\": 7.598}\n",
    "    \n",
    "    def _recommend_vertex_instance(self, model_size_gb: float) -> Dict:\n",
    "        if model_size_gb <= 14:\n",
    "            return {\"type\": \"n1-standard-4 + T4\", \"gpu\": \"T4 16GB\", \"price\": 0.54}\n",
    "        elif model_size_gb <= 24:\n",
    "            return {\"type\": \"n1-standard-8 + L4\", \"gpu\": \"L4 24GB\", \"price\": 1.08}\n",
    "        elif model_size_gb <= 40:\n",
    "            return {\"type\": \"n1-standard-8 + A100\", \"gpu\": \"A100 40GB\", \"price\": 3.31}\n",
    "        else:\n",
    "            return {\"type\": \"a2-highgpu-2g\", \"gpu\": \"2xA100 80GB\", \"price\": 7.34}\n",
    "    \n",
    "    def _recommend_k8s_instance(self, model_size_gb: float) -> Dict:\n",
    "        if model_size_gb <= 14:\n",
    "            return {\"type\": \"g5.xlarge (EKS)\", \"gpu\": \"A10G 24GB\", \"price\": 0.75}  # ~25% cheaper\n",
    "        elif model_size_gb <= 28:\n",
    "            return {\"type\": \"g5.2xlarge (EKS)\", \"gpu\": \"A10G 24GB\", \"price\": 1.13}\n",
    "        elif model_size_gb <= 48:\n",
    "            return {\"type\": \"g5.4xlarge (EKS)\", \"gpu\": \"A10G 24GB\", \"price\": 1.90}\n",
    "        else:\n",
    "            return {\"type\": \"g5.12xlarge (EKS)\", \"gpu\": \"4xA10G 96GB\", \"price\": 5.70}\n",
    "\n",
    "\n",
    "# Generate comparison\n",
    "if __name__ == \"__main__\":\n",
    "    import json\n",
    "    \n",
    "    comparator = PlatformComparison()\n",
    "    result = comparator.compare_all(model_size_gb=14.0, requests_per_day=50000)\n",
    "    \n",
    "    print(json.dumps(result, indent=2))\n",
    "'''\n",
    "\n",
    "print(\"PLATFORM COMPARISON CODE:\")\n",
    "print(\"=\" * 60)\n",
    "print(platform_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This solution demonstrated:\n",
    "\n",
    "1. **Custom Container for Vertex AI**\n",
    "   - Proper environment variable handling\n",
    "   - Required endpoint structure\n",
    "   - GPU memory management\n",
    "   - Health check implementation\n",
    "\n",
    "2. **Platform Comparison**\n",
    "   - Side-by-side feature comparison\n",
    "   - Instance recommendations by model size\n",
    "   - Cost estimates across platforms\n",
    "   - Best-fit scenarios for each platform"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
