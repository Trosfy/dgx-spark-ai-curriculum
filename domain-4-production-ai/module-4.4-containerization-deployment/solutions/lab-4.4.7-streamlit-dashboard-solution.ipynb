{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Lab 4.4.7: Streamlit Dashboard - SOLUTION\n",
    "\n",
    "**Module:** 4.4 - Containerization & Cloud Deployment  \n",
    "**This is the complete solution notebook with all exercises solved.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise1-header",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Complete Multi-Page Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise1-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete production Streamlit dashboard\n",
    "\n",
    "streamlit_dashboard = '''\n",
    "\"\"\"Production Streamlit Dashboard for ML Models.\n",
    "\n",
    "Features:\n",
    "- Multi-page layout\n",
    "- Chat playground\n",
    "- Performance metrics\n",
    "- Model comparison\n",
    "- Real-time GPU monitoring\n",
    "\n",
    "Run with: streamlit run dashboard.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import requests\n",
    "\n",
    "# ============================================\n",
    "# Page Configuration\n",
    "# ============================================\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"ML Model Dashboard\",\n",
    "    page_icon=\"\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\",\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# Session State Initialization\n",
    "# ============================================\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "if \"total_requests\" not in st.session_state:\n",
    "    st.session_state.total_requests = 0\n",
    "\n",
    "if \"request_history\" not in st.session_state:\n",
    "    st.session_state.request_history = []\n",
    "\n",
    "if \"model_metrics\" not in st.session_state:\n",
    "    st.session_state.model_metrics = {\n",
    "        \"latencies\": [],\n",
    "        \"tokens\": [],\n",
    "        \"timestamps\": [],\n",
    "    }\n",
    "\n",
    "# ============================================\n",
    "# Helper Functions\n",
    "# ============================================\n",
    "\n",
    "@st.cache_data(ttl=60)\n",
    "def get_gpu_metrics() -> Dict[str, Any]:\n",
    "    \"\"\"Get GPU metrics (cached for 60 seconds).\"\"\"\n",
    "    try:\n",
    "        import pynvml\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "        \n",
    "        mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "        temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\n",
    "        \n",
    "        pynvml.nvmlShutdown()\n",
    "        \n",
    "        return {\n",
    "            \"memory_used_gb\": mem_info.used / (1024**3),\n",
    "            \"memory_total_gb\": mem_info.total / (1024**3),\n",
    "            \"memory_percent\": (mem_info.used / mem_info.total) * 100,\n",
    "            \"gpu_utilization\": util.gpu,\n",
    "            \"temperature\": temp,\n",
    "        }\n",
    "    except:\n",
    "        # Mock data for demo\n",
    "        return {\n",
    "            \"memory_used_gb\": 45.2,\n",
    "            \"memory_total_gb\": 128.0,\n",
    "            \"memory_percent\": 35.3,\n",
    "            \"gpu_utilization\": 42,\n",
    "            \"temperature\": 52,\n",
    "        }\n",
    "\n",
    "\n",
    "def get_llm_response(prompt: str, model: str = \"qwen3:8b\") -> Dict[str, Any]:\n",
    "    \"\"\"Get response from LLM.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/chat\",\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"stream\": False,\n",
    "            },\n",
    "            timeout=60,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        return {\n",
    "            \"content\": data[\"message\"][\"content\"],\n",
    "            \"latency_ms\": (time.time() - start_time) * 1000,\n",
    "            \"tokens\": data.get(\"eval_count\", len(data[\"message\"][\"content\"].split())),\n",
    "        }\n",
    "    except:\n",
    "        # Mock response\n",
    "        time.sleep(0.5)\n",
    "        return {\n",
    "            \"content\": f\"[Mock Response] You asked: {prompt}\",\n",
    "            \"latency_ms\": (time.time() - start_time) * 1000,\n",
    "            \"tokens\": 20,\n",
    "        }\n",
    "\n",
    "\n",
    "def update_metrics(latency_ms: float, tokens: int):\n",
    "    \"\"\"Update session metrics.\"\"\"\n",
    "    st.session_state.total_requests += 1\n",
    "    st.session_state.model_metrics[\"latencies\"].append(latency_ms)\n",
    "    st.session_state.model_metrics[\"tokens\"].append(tokens)\n",
    "    st.session_state.model_metrics[\"timestamps\"].append(datetime.now())\n",
    "    \n",
    "    # Keep only last 100 entries\n",
    "    for key in st.session_state.model_metrics:\n",
    "        st.session_state.model_metrics[key] = st.session_state.model_metrics[key][-100:]\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Sidebar Navigation\n",
    "# ============================================\n",
    "\n",
    "st.sidebar.title(\"Navigation\")\n",
    "page = st.sidebar.selectbox(\n",
    "    \"Select Page\",\n",
    "    [\"Chat Playground\", \"Metrics Dashboard\", \"Model Comparison\", \"Settings\"]\n",
    ")\n",
    "\n",
    "st.sidebar.markdown(\"---\")\n",
    "\n",
    "# GPU status in sidebar\n",
    "st.sidebar.subheader(\"GPU Status\")\n",
    "gpu = get_gpu_metrics()\n",
    "st.sidebar.metric(\"Memory\", f\"{gpu['memory_used_gb']:.1f} / {gpu['memory_total_gb']:.0f} GB\")\n",
    "st.sidebar.progress(gpu[\"memory_percent\"] / 100)\n",
    "st.sidebar.metric(\"GPU Util\", f\"{gpu['gpu_utilization']}%\")\n",
    "st.sidebar.metric(\"Temperature\", f\"{gpu['temperature']}C\")\n",
    "\n",
    "# ============================================\n",
    "# Page: Chat Playground\n",
    "# ============================================\n",
    "\n",
    "if page == \"Chat Playground\":\n",
    "    st.title(\"Chat Playground\")\n",
    "    \n",
    "    # Model selection\n",
    "    col1, col2 = st.columns([3, 1])\n",
    "    with col1:\n",
    "        model = st.selectbox(\n",
    "            \"Model\",\n",
    "            [\"qwen3:8b\", \"qwen3:32b\", \"codellama:7b\", \"mistral:7b\"],\n",
    "        )\n",
    "    with col2:\n",
    "        if st.button(\"Clear Chat\"):\n",
    "            st.session_state.messages = []\n",
    "            st.rerun()\n",
    "    \n",
    "    # Chat history\n",
    "    for msg in st.session_state.messages:\n",
    "        with st.chat_message(msg[\"role\"]):\n",
    "            st.write(msg[\"content\"])\n",
    "            if msg[\"role\"] == \"assistant\" and \"latency\" in msg:\n",
    "                st.caption(f\"Latency: {msg['latency']:.0f}ms | Tokens: {msg['tokens']}\")\n",
    "    \n",
    "    # Chat input\n",
    "    if prompt := st.chat_input(\"Ask something...\"):\n",
    "        # Add user message\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.write(prompt)\n",
    "        \n",
    "        # Get response\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            with st.spinner(\"Thinking...\"):\n",
    "                result = get_llm_response(prompt, model)\n",
    "            \n",
    "            st.write(result[\"content\"])\n",
    "            st.caption(f\"Latency: {result['latency_ms']:.0f}ms | Tokens: {result['tokens']}\")\n",
    "        \n",
    "        # Store response and update metrics\n",
    "        st.session_state.messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": result[\"content\"],\n",
    "            \"latency\": result[\"latency_ms\"],\n",
    "            \"tokens\": result[\"tokens\"],\n",
    "        })\n",
    "        update_metrics(result[\"latency_ms\"], result[\"tokens\"])\n",
    "\n",
    "# ============================================\n",
    "# Page: Metrics Dashboard\n",
    "# ============================================\n",
    "\n",
    "elif page == \"Metrics Dashboard\":\n",
    "    st.title(\"Performance Metrics\")\n",
    "    \n",
    "    # Top row: Key metrics\n",
    "    col1, col2, col3, col4 = st.columns(4)\n",
    "    \n",
    "    latencies = st.session_state.model_metrics[\"latencies\"]\n",
    "    tokens = st.session_state.model_metrics[\"tokens\"]\n",
    "    \n",
    "    with col1:\n",
    "        st.metric(\"Total Requests\", st.session_state.total_requests)\n",
    "    with col2:\n",
    "        avg_latency = sum(latencies) / len(latencies) if latencies else 0\n",
    "        st.metric(\"Avg Latency\", f\"{avg_latency:.0f}ms\")\n",
    "    with col3:\n",
    "        total_tokens = sum(tokens)\n",
    "        st.metric(\"Total Tokens\", f\"{total_tokens:,}\")\n",
    "    with col4:\n",
    "        tokens_per_sec = total_tokens / (sum(latencies) / 1000) if latencies else 0\n",
    "        st.metric(\"Tokens/sec\", f\"{tokens_per_sec:.1f}\")\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    # Charts\n",
    "    if latencies:\n",
    "        col1, col2 = st.columns(2)\n",
    "        \n",
    "        with col1:\n",
    "            st.subheader(\"Latency Over Time\")\n",
    "            df = pd.DataFrame({\n",
    "                \"Request\": range(1, len(latencies) + 1),\n",
    "                \"Latency (ms)\": latencies,\n",
    "            })\n",
    "            fig = px.line(df, x=\"Request\", y=\"Latency (ms)\")\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "        \n",
    "        with col2:\n",
    "            st.subheader(\"Tokens per Request\")\n",
    "            df = pd.DataFrame({\n",
    "                \"Request\": range(1, len(tokens) + 1),\n",
    "                \"Tokens\": tokens,\n",
    "            })\n",
    "            fig = px.bar(df, x=\"Request\", y=\"Tokens\")\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "        \n",
    "        # Latency distribution\n",
    "        st.subheader(\"Latency Distribution\")\n",
    "        fig = px.histogram(latencies, nbins=20, labels={\"value\": \"Latency (ms)\"})\n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "    else:\n",
    "        st.info(\"No metrics yet. Start chatting to generate data!\")\n",
    "\n",
    "# ============================================\n",
    "# Page: Model Comparison\n",
    "# ============================================\n",
    "\n",
    "elif page == \"Model Comparison\":\n",
    "    st.title(\"Model Comparison\")\n",
    "    \n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        model_a = st.selectbox(\"Model A\", [\"qwen3:8b\", \"mistral:7b\"], key=\"model_a\")\n",
    "    with col2:\n",
    "        model_b = st.selectbox(\"Model B\", [\"mistral:7b\", \"qwen3:8b\"], key=\"model_b\")\n",
    "    \n",
    "    prompt = st.text_area(\"Prompt\", \"Explain quantum computing in simple terms.\")\n",
    "    \n",
    "    if st.button(\"Compare\", type=\"primary\"):\n",
    "        col1, col2 = st.columns(2)\n",
    "        \n",
    "        with col1:\n",
    "            st.subheader(f\"{model_a}\")\n",
    "            with st.spinner(\"Generating...\"):\n",
    "                result_a = get_llm_response(prompt, model_a)\n",
    "            st.write(result_a[\"content\"])\n",
    "            st.metric(\"Latency\", f\"{result_a['latency_ms']:.0f}ms\")\n",
    "            st.metric(\"Tokens\", result_a[\"tokens\"])\n",
    "        \n",
    "        with col2:\n",
    "            st.subheader(f\"{model_b}\")\n",
    "            with st.spinner(\"Generating...\"):\n",
    "                result_b = get_llm_response(prompt, model_b)\n",
    "            st.write(result_b[\"content\"])\n",
    "            st.metric(\"Latency\", f\"{result_b['latency_ms']:.0f}ms\")\n",
    "            st.metric(\"Tokens\", result_b[\"tokens\"])\n",
    "\n",
    "# ============================================\n",
    "# Page: Settings\n",
    "# ============================================\n",
    "\n",
    "elif page == \"Settings\":\n",
    "    st.title(\"Settings\")\n",
    "    \n",
    "    st.subheader(\"Model Configuration\")\n",
    "    \n",
    "    temperature = st.slider(\"Temperature\", 0.0, 2.0, 0.7, 0.1)\n",
    "    max_tokens = st.number_input(\"Max Tokens\", 100, 4096, 512)\n",
    "    top_p = st.slider(\"Top P\", 0.0, 1.0, 0.9, 0.05)\n",
    "    \n",
    "    st.subheader(\"System Prompt\")\n",
    "    system_prompt = st.text_area(\n",
    "        \"System Prompt\",\n",
    "        \"You are a helpful AI assistant.\",\n",
    "        height=100,\n",
    "    )\n",
    "    \n",
    "    if st.button(\"Save Settings\"):\n",
    "        st.success(\"Settings saved!\")\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"About\")\n",
    "    st.markdown(\"\"\"\n",
    "    **ML Model Dashboard**\n",
    "    \n",
    "    Built with Streamlit for the DGX Spark AI Curriculum.\n",
    "    \n",
    "    Features:\n",
    "    - Real-time chat interface\n",
    "    - Performance metrics\n",
    "    - Model comparison\n",
    "    - GPU monitoring\n",
    "    \"\"\")\n",
    "\n",
    "# ============================================\n",
    "# Footer\n",
    "# ============================================\n",
    "\n",
    "st.sidebar.markdown(\"---\")\n",
    "st.sidebar.caption(f\"Last updated: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "'''\n",
    "\n",
    "print(\"COMPLETE STREAMLIT DASHBOARD:\")\n",
    "print(\"=\" * 60)\n",
    "print(streamlit_dashboard)\n",
    "\n",
    "# Save to file\n",
    "import os\n",
    "os.makedirs(\"../app-examples\", exist_ok=True)\n",
    "with open(\"../app-examples/streamlit_dashboard.py\", \"w\") as f:\n",
    "    f.write(streamlit_dashboard)\n",
    "print(\"\\nSaved to: ../app-examples/streamlit_dashboard.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise2-header",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Deployment to Streamlit Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise2-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlit Cloud deployment files\n",
    "\n",
    "# requirements.txt\n",
    "requirements = '''\n",
    "streamlit>=1.30.0\n",
    "pandas>=2.0.0\n",
    "plotly>=5.18.0\n",
    "requests>=2.31.0\n",
    "nvidia-ml-py>=12.535.0; platform_system==\"Linux\"\n",
    "'''\n",
    "\n",
    "print(\"REQUIREMENTS.TXT:\")\n",
    "print(requirements)\n",
    "\n",
    "# .streamlit/config.toml\n",
    "config_toml = '''\n",
    "[theme]\n",
    "primaryColor = \"#76b900\"  # NVIDIA green\n",
    "backgroundColor = \"#0e1117\"\n",
    "secondaryBackgroundColor = \"#262730\"\n",
    "textColor = \"#fafafa\"\n",
    "\n",
    "[server]\n",
    "maxUploadSize = 50\n",
    "enableXsrfProtection = true\n",
    "enableCORS = false\n",
    "\n",
    "[browser]\n",
    "gatherUsageStats = false\n",
    "'''\n",
    "\n",
    "print(\"\\n.STREAMLIT/CONFIG.TOML:\")\n",
    "print(config_toml)\n",
    "\n",
    "# Save files\n",
    "import os\n",
    "os.makedirs(\"../app-examples/.streamlit\", exist_ok=True)\n",
    "\n",
    "with open(\"../app-examples/requirements.txt\", \"w\") as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "with open(\"../app-examples/.streamlit/config.toml\", \"w\") as f:\n",
    "    f.write(config_toml)\n",
    "\n",
    "print(\"\\nSaved configuration files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This solution demonstrated:\n",
    "\n",
    "1. **Multi-Page Streamlit Dashboard**\n",
    "   - Chat Playground with streaming\n",
    "   - Metrics visualization with Plotly\n",
    "   - Model comparison tool\n",
    "   - Settings management\n",
    "\n",
    "2. **Production Features**\n",
    "   - GPU monitoring integration\n",
    "   - Session state for persistence\n",
    "   - Caching for performance\n",
    "   - Real-time metrics updates\n",
    "\n",
    "3. **Deployment Configuration**\n",
    "   - Streamlit Cloud setup\n",
    "   - Custom theming\n",
    "   - Requirements management"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
