{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.4.6: Building Interactive Demos with Gradio\n",
    "\n",
    "**Module:** 4.4 - Containerization & Cloud Deployment  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐ (Beginner-Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- [ ] Create chat interfaces with Gradio\n",
    "- [ ] Implement streaming responses for LLMs\n",
    "- [ ] Add file upload capabilities for RAG\n",
    "- [ ] Customize themes and styling\n",
    "- [ ] Deploy to Hugging Face Spaces\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic Python knowledge\n",
    "- Ollama running locally (optional, mock mode available)\n",
    "- Hugging Face account (for deployment)\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "**Why Gradio?**\n",
    "\n",
    "- Used by HuggingFace for all demo Spaces\n",
    "- Create a shareable demo in minutes\n",
    "- Perfect for showing ML models to stakeholders\n",
    "- Built-in support for chat, image, audio, and more\n",
    "\n",
    "**Use cases:**\n",
    "- LLM chat interfaces\n",
    "- Model comparison tools\n",
    "- Prototype testing\n",
    "- Stakeholder demos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: What is Gradio?\n",
    "\n",
    "> **Imagine you made an amazing cake (ML model)...**\n",
    ">\n",
    "> But to share it, people need to come to your kitchen and know how to operate your oven.\n",
    ">\n",
    "> **Gradio is like a bakery display case.** People can see your cake, try samples, and order more - all without touching your kitchen equipment!\n",
    ">\n",
    "> **In ML terms:**\n",
    "> - Your model = The cake\n",
    "> - Gradio interface = The display case\n",
    "> - Users = Customers who can taste without knowing the recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Gradio if needed\n",
    "# !pip install gradio>=4.0.0\n",
    "\n",
    "import gradio as gr\n",
    "print(f\"Gradio version: {gr.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Understanding Our Demo Utilities Module\n\nThis curriculum provides a `demo_utils` module with classes for building LLM interfaces. Let's understand what it offers before using it.\n\n### LLM Client Classes\n\n| Class | Description | Use Case |\n|-------|-------------|----------|\n| `StreamingLLMClient` | Client for real LLM backends | Production with Ollama/OpenAI |\n| `MockLLMClient` | Simulated LLM for testing | Development/demos without GPU |\n\n### MockLLMClient\n\nA simulated LLM client for testing without a real model:\n\n```python\nmock_client = MockLLMClient(\n    responses=[\"Response 1\", \"Response 2\"],  # Cycle through these\n    delay_per_token=0.02,  # Simulate streaming delay\n)\n```\n\n**Methods:**\n- `stream_chat(messages)`: Generator yielding response tokens\n- `chat(messages)`: Return complete response\n\n### StreamingLLMClient\n\nA client for real LLM backends:\n\n```python\nclient = StreamingLLMClient(\n    model=\"qwen3:8b\",\n    backend=\"ollama\",  # or \"openai\"\n    base_url=\"http://localhost:11434\",\n)\n```\n\n### Message Class\n\nRepresents a chat message:\n\n```python\nfrom scripts.demo_utils import Message\nmsg = Message(role=\"user\", content=\"Hello!\")\n# role can be: \"user\", \"assistant\", or \"system\"\n```\n\n### Helper Functions\n\n| Function | Description |\n|----------|-------------|\n| `create_gradio_chat_interface(client, title, ...)` | Create a Gradio ChatInterface |\n| `generate_gradio_space_config(title, sdk, ...)` | Generate HuggingFace Spaces README |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our demo utilities\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from scripts.demo_utils import (\n",
    "    StreamingLLMClient,\n",
    "    MockLLMClient,\n",
    "    Message,\n",
    "    create_gradio_chat_interface,\n",
    "    generate_gradio_space_config,\n",
    ")\n",
    "\n",
    "print(\"Demo utilities loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Your First Gradio Interface\n",
    "\n",
    "Let's start with the simplest possible Gradio app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to greet\n",
    "def greet(name):\n",
    "    return f\"Hello, {name}! Welcome to Gradio!\"\n",
    "\n",
    "# Create interface\n",
    "demo = gr.Interface(\n",
    "    fn=greet,\n",
    "    inputs=gr.Textbox(label=\"Your Name\"),\n",
    "    outputs=gr.Textbox(label=\"Greeting\"),\n",
    "    title=\"Hello World\",\n",
    "    description=\"Enter your name to get a greeting!\",\n",
    ")\n",
    "\n",
    "# Launch (in notebook, this creates an iframe)\n",
    "demo.launch(share=False, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "1. `gr.Interface` wraps your function with a UI\n",
    "2. `inputs` defines what the user provides\n",
    "3. `outputs` defines what gets displayed\n",
    "4. `launch()` starts a local web server\n",
    "\n",
    "**Pro tip:** Use `share=True` to get a public URL (temporary, 72 hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Building a Chat Interface\n",
    "\n",
    "Gradio has a built-in `ChatInterface` for conversational AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mock LLM client for testing\n",
    "mock_client = MockLLMClient(\n",
    "    responses=[\n",
    "        \"Hello! I'm a simulated AI assistant running on DGX Spark. How can I help you today?\",\n",
    "        \"That's a great question! Let me think about it...\\n\\nBased on my training, I would say that machine learning is the process of teaching computers to learn from data rather than being explicitly programmed.\",\n",
    "        \"Here's a simple Python example:\\n\\n```python\\ndef fibonacci(n):\\n    if n <= 1:\\n        return n\\n    return fibonacci(n-1) + fibonacci(n-2)\\n```\\n\\nThis recursive function calculates Fibonacci numbers!\",\n",
    "    ],\n",
    "    delay_per_token=0.02,  # Simulate streaming\n",
    ")\n",
    "\n",
    "print(\"Mock client created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a chat interface with streaming\n",
    "def chat_with_llm(message, history):\n",
    "    \"\"\"\n",
    "    Chat function for Gradio.\n",
    "    \n",
    "    Args:\n",
    "        message: User's current message\n",
    "        history: List of [user_msg, assistant_msg] pairs\n",
    "    \n",
    "    Yields:\n",
    "        Partial response for streaming effect\n",
    "    \"\"\"\n",
    "    # Convert history to Message objects\n",
    "    messages = []\n",
    "    for user_msg, assistant_msg in history:\n",
    "        messages.append(Message(\"user\", user_msg))\n",
    "        messages.append(Message(\"assistant\", assistant_msg))\n",
    "    messages.append(Message(\"user\", message))\n",
    "    \n",
    "    # Stream response\n",
    "    full_response = \"\"\n",
    "    for chunk in mock_client.stream_chat(messages):\n",
    "        full_response += chunk\n",
    "        yield full_response\n",
    "\n",
    "# Create chat interface\n",
    "chat_demo = gr.ChatInterface(\n",
    "    fn=chat_with_llm,\n",
    "    title=\"AI Chat Demo\",\n",
    "    description=\"Chat with a simulated LLM (replace with real model for production)\",\n",
    "    examples=[\n",
    "        \"Hello! What can you do?\",\n",
    "        \"Explain machine learning in simple terms\",\n",
    "        \"Write a Python function for Fibonacci numbers\",\n",
    "    ],\n",
    "    theme=gr.themes.Soft(),\n",
    ")\n",
    "\n",
    "chat_demo.launch(share=False, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Connecting to a Real LLM\n",
    "\n",
    "Let's connect to Ollama (if running) or use the mock client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to connect to Ollama\n",
    "import requests\n",
    "\n",
    "def check_ollama():\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get(\"models\", [])\n",
    "            return [m[\"name\"] for m in models]\n",
    "    except:\n",
    "        pass\n",
    "    return []\n",
    "\n",
    "available_models = check_ollama()\n",
    "\n",
    "if available_models:\n",
    "    print(f\"Ollama is running with models: {available_models}\")\n",
    "    # Use real client\n",
    "    llm_client = StreamingLLMClient(\n",
    "        model=available_models[0],\n",
    "        backend=\"ollama\",\n",
    "    )\n",
    "else:\n",
    "    print(\"Ollama not available, using mock client\")\n",
    "    llm_client = mock_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chat interface with real/mock client\n",
    "def chat_function(message, history):\n",
    "    messages = []\n",
    "    for user_msg, assistant_msg in history:\n",
    "        messages.append(Message(\"user\", user_msg))\n",
    "        messages.append(Message(\"assistant\", assistant_msg))\n",
    "    messages.append(Message(\"user\", message))\n",
    "    \n",
    "    full_response = \"\"\n",
    "    for chunk in llm_client.stream_chat(messages):\n",
    "        full_response += chunk\n",
    "        yield full_response\n",
    "\n",
    "# Use the helper function\n",
    "demo = create_gradio_chat_interface(\n",
    "    client=llm_client,\n",
    "    title=\"DGX Spark AI Assistant\",\n",
    "    description=f\"Powered by {'Ollama: ' + available_models[0] if available_models else 'Mock LLM'}\",\n",
    "    examples=[\n",
    "        \"What is the DGX Spark?\",\n",
    "        \"Explain GPU memory management\",\n",
    "        \"Write a CUDA kernel example\",\n",
    "    ],\n",
    "    theme=\"soft\",\n",
    ")\n",
    "\n",
    "demo.launch(share=False, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Adding File Upload for RAG\n",
    "\n",
    "Let's add document upload capability for RAG-style applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG-enabled chat interface\n",
    "\n",
    "# Store uploaded documents (in memory for demo)\n",
    "uploaded_docs = []\n",
    "\n",
    "def process_document(file):\n",
    "    \"\"\"Process uploaded document.\"\"\"\n",
    "    global uploaded_docs\n",
    "    \n",
    "    if file is None:\n",
    "        return \"No file uploaded\"\n",
    "    \n",
    "    # Read file content\n",
    "    try:\n",
    "        with open(file.name, 'r') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        uploaded_docs.append({\n",
    "            \"name\": file.name.split(\"/\")[-1],\n",
    "            \"content\": content[:5000],  # Limit for demo\n",
    "        })\n",
    "        \n",
    "        return f\"Uploaded: {file.name.split('/')[-1]} ({len(content)} characters)\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def chat_with_context(message, history):\n",
    "    \"\"\"Chat with document context.\"\"\"\n",
    "    # Build context from uploaded docs\n",
    "    context = \"\"\n",
    "    if uploaded_docs:\n",
    "        context = \"\\n\\nUploaded documents:\\n\"\n",
    "        for doc in uploaded_docs[-3:]:  # Last 3 docs\n",
    "            context += f\"\\n--- {doc['name']} ---\\n{doc['content'][:1000]}...\\n\"\n",
    "    \n",
    "    # Build messages with context\n",
    "    messages = []\n",
    "    if context:\n",
    "        messages.append(Message(\"system\", f\"Use this context to answer questions:{context}\"))\n",
    "    \n",
    "    for user_msg, assistant_msg in history:\n",
    "        messages.append(Message(\"user\", user_msg))\n",
    "        messages.append(Message(\"assistant\", assistant_msg))\n",
    "    messages.append(Message(\"user\", message))\n",
    "    \n",
    "    # Stream response\n",
    "    full_response = \"\"\n",
    "    for chunk in llm_client.stream_chat(messages):\n",
    "        full_response += chunk\n",
    "        yield full_response\n",
    "\n",
    "# Create interface with file upload\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as rag_demo:\n",
    "    gr.Markdown(\"# RAG Chat Demo\")\n",
    "    gr.Markdown(\"Upload documents and ask questions about them.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            file_upload = gr.File(\n",
    "                label=\"Upload Document\",\n",
    "                file_types=[\".txt\", \".md\", \".py\"],\n",
    "            )\n",
    "            upload_status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "            file_upload.change(process_document, file_upload, upload_status)\n",
    "            \n",
    "        with gr.Column(scale=3):\n",
    "            chatbot = gr.ChatInterface(\n",
    "                fn=chat_with_context,\n",
    "                examples=[\n",
    "                    \"What's in the uploaded document?\",\n",
    "                    \"Summarize the main points\",\n",
    "                    \"What are the key concepts?\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "rag_demo.launch(share=False, height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Custom Themes and Styling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available built-in themes\n",
    "themes = [\n",
    "    \"default\",\n",
    "    \"soft\",\n",
    "    \"glass\",\n",
    "    \"monochrome\",\n",
    "]\n",
    "\n",
    "print(\"Available Gradio themes:\")\n",
    "for theme in themes:\n",
    "    print(f\"  - gr.themes.{theme.capitalize()}()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom theme example\n",
    "custom_theme = gr.themes.Soft(\n",
    "    primary_hue=\"green\",\n",
    "    secondary_hue=\"blue\",\n",
    "    neutral_hue=\"slate\",\n",
    ").set(\n",
    "    button_primary_background_fill=\"#76b900\",  # NVIDIA green\n",
    "    button_primary_background_fill_hover=\"#5a8f00\",\n",
    ")\n",
    "\n",
    "# Custom CSS\n",
    "custom_css = \"\"\"\n",
    ".gradio-container {\n",
    "    max-width: 900px !important;\n",
    "}\n",
    ".message {\n",
    "    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Create themed interface\n",
    "themed_demo = gr.ChatInterface(\n",
    "    fn=chat_function,\n",
    "    title=\"NVIDIA DGX Spark Assistant\",\n",
    "    description=\"Powered by DGX Spark with 128GB unified memory\",\n",
    "    theme=custom_theme,\n",
    "    css=custom_css,\n",
    "    examples=[\n",
    "        \"What makes DGX Spark special?\",\n",
    "        \"How much GPU memory is available?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "themed_demo.launch(share=False, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Deploying to Hugging Face Spaces\n",
    "\n",
    "Hugging Face Spaces provides free hosting for Gradio apps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Spaces configuration\n",
    "config = generate_gradio_space_config(\n",
    "    title=\"DGX Spark AI Demo\",\n",
    "    sdk=\"gradio\",\n",
    "    sdk_version=\"4.0\",\n",
    "    requirements=[\"transformers\", \"torch\", \"accelerate\"],\n",
    ")\n",
    "\n",
    "print(\"README.md for Hugging Face Spaces:\")\n",
    "print(\"=\" * 60)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complete Space-ready app\n",
    "import os\n",
    "\n",
    "os.makedirs(\"../docker-examples/gradio-space\", exist_ok=True)\n",
    "\n",
    "# Save README.md\n",
    "with open(\"../docker-examples/gradio-space/README.md\", \"w\") as f:\n",
    "    f.write(config)\n",
    "\n",
    "# Save requirements.txt\n",
    "requirements = \"\"\"gradio>=4.0.0\n",
    "transformers>=4.37.0\n",
    "torch>=2.0.0\n",
    "accelerate>=0.25.0\n",
    "requests>=2.28.0\n",
    "\"\"\"\n",
    "\n",
    "with open(\"../docker-examples/gradio-space/requirements.txt\", \"w\") as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "# Save app.py\n",
    "app_code = '''\"\"\"Gradio App for Hugging Face Spaces.\"\"\"\n",
    "\n",
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "# Load model (small model for demo)\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "def chat(message, history):\n",
    "    \"\"\"Chat function.\"\"\"\n",
    "    # Build prompt from history\n",
    "    prompt = \"\"\n",
    "    for user_msg, assistant_msg in history:\n",
    "        prompt += f\"User: {user_msg}\\\\nAssistant: {assistant_msg}\\\\n\"\n",
    "    prompt += f\"User: {message}\\\\nAssistant:\"\n",
    "    \n",
    "    # Generate response\n",
    "    response = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "    )[0][\"generated_text\"]\n",
    "    \n",
    "    # Extract just the assistant's response\n",
    "    assistant_response = response.split(\"Assistant:\")[-1].strip()\n",
    "    \n",
    "    return assistant_response\n",
    "\n",
    "# Create interface\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    title=\"DGX Spark AI Demo\",\n",
    "    description=\"A demo chatbot deployed from DGX Spark\",\n",
    "    examples=[\n",
    "        \"Hello! How are you?\",\n",
    "        \"Tell me a joke\",\n",
    "        \"What is machine learning?\",\n",
    "    ],\n",
    "    theme=gr.themes.Soft(),\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n",
    "'''\n",
    "\n",
    "with open(\"../docker-examples/gradio-space/app.py\", \"w\") as f:\n",
    "    f.write(app_code)\n",
    "\n",
    "print(\"Created Hugging Face Space files:\")\n",
    "print(\"  - docker-examples/gradio-space/README.md\")\n",
    "print(\"  - docker-examples/gradio-space/requirements.txt\")\n",
    "print(\"  - docker-examples/gradio-space/app.py\")\n",
    "print(\"\\nTo deploy:\")\n",
    "print(\"  1. Create a new Space on huggingface.co/spaces\")\n",
    "print(\"  2. Upload these files\")\n",
    "print(\"  3. Your app will be live in minutes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Gradio app with multiple tabs\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as advanced_demo:\n",
    "    gr.Markdown(\"# AI Playground\")\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        # Tab 1: Chat\n",
    "        with gr.Tab(\"Chat\"):\n",
    "            chatbot = gr.Chatbot(height=400)\n",
    "            msg = gr.Textbox(label=\"Message\", placeholder=\"Type a message...\")\n",
    "            clear = gr.Button(\"Clear\")\n",
    "            \n",
    "            def respond(message, history):\n",
    "                # Simple echo for demo\n",
    "                response = f\"You said: {message}\"\n",
    "                history.append((message, response))\n",
    "                return \"\", history\n",
    "            \n",
    "            msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "            clear.click(lambda: None, None, chatbot, queue=False)\n",
    "        \n",
    "        # Tab 2: Settings\n",
    "        with gr.Tab(\"Settings\"):\n",
    "            temperature = gr.Slider(\n",
    "                minimum=0, maximum=2, value=0.7, step=0.1,\n",
    "                label=\"Temperature\"\n",
    "            )\n",
    "            max_tokens = gr.Slider(\n",
    "                minimum=50, maximum=2048, value=256, step=50,\n",
    "                label=\"Max Tokens\"\n",
    "            )\n",
    "            system_prompt = gr.Textbox(\n",
    "                label=\"System Prompt\",\n",
    "                value=\"You are a helpful AI assistant.\",\n",
    "                lines=3,\n",
    "            )\n",
    "        \n",
    "        # Tab 3: Info\n",
    "        with gr.Tab(\"About\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            ## About This Demo\n",
    "            \n",
    "            This is a Gradio demo showcasing:\n",
    "            - Tabbed interfaces\n",
    "            - Chat functionality\n",
    "            - Settings management\n",
    "            \n",
    "            Built with Gradio and deployed from DGX Spark.\n",
    "            \"\"\")\n",
    "\n",
    "advanced_demo.launch(share=False, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Blocking in Streaming Functions\n",
    "\n",
    "```python\n",
    "# BAD - No streaming, user waits for full response\n",
    "def chat(message, history):\n",
    "    response = model.generate(message)  # Long wait\n",
    "    return response\n",
    "\n",
    "# GOOD - Yield partial responses\n",
    "def chat(message, history):\n",
    "    partial = \"\"\n",
    "    for token in model.stream_generate(message):\n",
    "        partial += token\n",
    "        yield partial  # User sees tokens appear\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 2: Not Handling Errors\n",
    "\n",
    "```python\n",
    "# BAD - Crashes on error\n",
    "def chat(message, history):\n",
    "    return model.generate(message)\n",
    "\n",
    "# GOOD - Graceful error handling\n",
    "def chat(message, history):\n",
    "    try:\n",
    "        return model.generate(message)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}. Please try again.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- Creating simple and chat interfaces with Gradio\n",
    "- Implementing streaming responses\n",
    "- Adding file upload for RAG applications\n",
    "- Customizing themes\n",
    "- Deploying to Hugging Face Spaces\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "Build a complete demo that:\n",
    "1. Allows model selection from a dropdown\n",
    "2. Shows token count and generation speed\n",
    "3. Supports image input for multimodal models\n",
    "4. Has a \"compare models\" mode\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Gradio Documentation](https://gradio.app/docs/)\n",
    "- [Hugging Face Spaces](https://huggingface.co/docs/hub/spaces)\n",
    "- [Gradio Components](https://gradio.app/docs/components)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close any running Gradio servers\n",
    "gr.close_all()\n",
    "print(\"All Gradio servers closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}