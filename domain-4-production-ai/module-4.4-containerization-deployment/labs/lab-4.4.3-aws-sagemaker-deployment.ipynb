{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.4.3: Deploying Models to AWS SageMaker\n",
    "\n",
    "**Module:** 4.4 - Containerization & Cloud Deployment  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐⭐⭐ (Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- [ ] Understand AWS SageMaker deployment concepts\n",
    "- [ ] Package models for SageMaker deployment\n",
    "- [ ] Deploy HuggingFace models to real-time endpoints\n",
    "- [ ] Configure auto-scaling for production\n",
    "- [ ] Monitor endpoint performance\n",
    "- [ ] Calculate and optimize costs\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS Account with SageMaker permissions\n",
    "- AWS CLI configured (`aws configure`)\n",
    "- Completed: Labs 4.4.1-4.4.2\n",
    "\n",
    "**Note:** This lab can be completed in simulation mode without an AWS account.\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "**When to use SageMaker vs. self-hosted?**\n",
    "\n",
    "| Use Case | SageMaker | Self-Hosted (DGX Spark) |\n",
    "|----------|-----------|------------------------|\n",
    "| Variable traffic | Best (auto-scaling) | Harder |\n",
    "| Cost at scale | $$$ | $ (if GPU already owned) |\n",
    "| Latency sensitive | Good (multi-region) | Best (no network hop) |\n",
    "| Compliance | AWS certifications | Full control |\n",
    "| Experimentation | More setup | Faster iteration |\n",
    "\n",
    "**SageMaker shines when:**\n",
    "- You need global availability\n",
    "- Traffic is unpredictable\n",
    "- You want managed infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: What is SageMaker?\n",
    "\n",
    "> **Imagine running a lemonade stand...**\n",
    ">\n",
    "> You could build your own stand, buy ingredients, make lemonade, and sell it yourself.\n",
    ">\n",
    "> **OR** you could use a food truck service that:\n",
    "> - Provides the truck (infrastructure)\n",
    "> - Handles permits (security)\n",
    "> - Sends more trucks when busy (auto-scaling)\n",
    "> - Tracks your sales (monitoring)\n",
    ">\n",
    "> **SageMaker is that food truck service for ML models.** You bring the recipe (your model), they handle everything else.\n",
    ">\n",
    "> **The tradeoff?** You pay rent for the trucks instead of owning them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: SageMaker Architecture\n",
    "\n",
    "### Key Components\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    AWS SageMaker                             │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                              │\n",
    "│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐  │\n",
    "│  │    Model     │    │   Endpoint   │    │   Endpoint   │  │\n",
    "│  │   Artifact   │───>│    Config    │───>│  (Runtime)   │  │\n",
    "│  │   (S3)       │    │              │    │              │  │\n",
    "│  └──────────────┘    └──────────────┘    └──────────────┘  │\n",
    "│                                                 │           │\n",
    "│                                                 ▼           │\n",
    "│                                          ┌──────────────┐  │\n",
    "│                                          │  Auto-Scale  │  │\n",
    "│                                          │   Variants   │  │\n",
    "│                                          └──────────────┘  │\n",
    "│                                                              │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **Model Artifact** | Your model files (model.tar.gz in S3) |\n",
    "| **Endpoint Config** | Instance type, count, model settings |\n",
    "| **Endpoint** | The actual running inference service |\n",
    "| **Variant** | Traffic-split for A/B testing |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for AWS credentials and dependencies\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "print(\"AWS Environment Check\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check AWS CLI\n",
    "result = subprocess.run([\"aws\", \"--version\"], capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(f\"AWS CLI: {result.stdout.strip().split()[0]}\")\n",
    "else:\n",
    "    print(\" AWS CLI not installed\")\n",
    "\n",
    "# Check credentials\n",
    "creds_configured = False\n",
    "if os.path.exists(os.path.expanduser(\"~/.aws/credentials\")):\n",
    "    print(\"AWS Credentials: Configured\")\n",
    "    creds_configured = True\n",
    "elif os.environ.get(\"AWS_ACCESS_KEY_ID\"):\n",
    "    print(\"AWS Credentials: From environment\")\n",
    "    creds_configured = True\n",
    "else:\n",
    "    print(\" AWS Credentials: Not configured\")\n",
    "    print(\"   Run: aws configure\")\n",
    "\n",
    "# Check boto3\n",
    "try:\n",
    "    import boto3\n",
    "    print(f\"boto3: {boto3.__version__}\")\n",
    "except ImportError:\n",
    "    print(\" boto3 not installed. Run: pip install boto3\")\n",
    "\n",
    "# Check sagemaker SDK\n",
    "try:\n",
    "    import sagemaker\n",
    "    print(f\"sagemaker SDK: {sagemaker.__version__}\")\n",
    "except ImportError:\n",
    "    print(\" sagemaker not installed. Run: pip install sagemaker\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if not creds_configured:\n",
    "    print(\"\\n This lab will run in SIMULATION mode.\")\n",
    "    print(\"All examples will work without real AWS access.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our cloud utilities\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from scripts.cloud_utils import (\n",
    "    SageMakerDeployer,\n",
    "    estimate_cloud_costs,\n",
    "    compare_platforms,\n",
    "    create_deployment_checklist,\n",
    ")\n",
    "\n",
    "print(\"Cloud utilities loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding SageMaker Instance Types\n",
    "\n",
    "### GPU Instances for ML Inference\n",
    "\n",
    "| Instance | GPU | VRAM | Cost/hr | Best For |\n",
    "|----------|-----|------|---------|----------|\n",
    "| ml.g5.xlarge | 1x A10G | 24GB | ~$1.00 | 7B models |\n",
    "| ml.g5.2xlarge | 1x A10G | 24GB | ~$1.50 | 7B + more CPU |\n",
    "| ml.g5.4xlarge | 1x A10G | 24GB | ~$2.50 | 7B + large batch |\n",
    "| ml.g5.12xlarge | 4x A10G | 96GB | ~$7.60 | 30B models |\n",
    "| ml.p4d.24xlarge | 8x A100 | 320GB | ~$32.77 | 70B+ models |\n",
    "\n",
    "### Choosing the Right Instance\n",
    "\n",
    "**Rule of thumb:** GPU memory needed ≈ 2x model size (for FP16)\n",
    "\n",
    "| Model Size | FP16 Memory | Recommended Instance |\n",
    "|------------|-------------|----------------------|\n",
    "| 7B params | ~14GB | ml.g5.xlarge |\n",
    "| 13B params | ~26GB | ml.g5.12xlarge |\n",
    "| 70B params | ~140GB | ml.p4d.24xlarge |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at SageMaker instance pricing\n",
    "print(\"SageMaker GPU Instance Pricing (us-west-2)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for instance, price in SageMakerDeployer.INSTANCE_PRICING.items():\n",
    "    if 'g5' in instance or 'p4' in instance or 'p5' in instance:\n",
    "        monthly = price * 24 * 30\n",
    "        print(f\"{instance:20} ${price:>8.3f}/hr  (${monthly:>8,.0f}/month)\")\n",
    "\n",
    "print(\"\\n Tip: Use spot instances for 60-80% savings on non-critical workloads!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Deploying a HuggingFace Model\n",
    "\n",
    "SageMaker has built-in support for HuggingFace models through their Deep Learning Containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SageMaker deployer\n",
    "deployer = SageMakerDeployer(region=\"us-west-2\")\n",
    "\n",
    "print(\"SageMaker Deployer initialized\")\n",
    "print(f\"Region: {deployer.region}\")\n",
    "print(f\"AWS SDK available: {deployer._sagemaker_available}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy a HuggingFace model (simulated if no AWS access)\n",
    "print(\"Deploying model to SageMaker...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configuration\n",
    "model_config = {\n",
    "    \"model_id\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"instance_type\": \"ml.g5.xlarge\",\n",
    "    \"instance_count\": 1,\n",
    "    \"quantization\": \"bitsandbytes4\",  # 4-bit quantization for memory efficiency\n",
    "    \"environment\": {\n",
    "        \"MAX_INPUT_LENGTH\": \"2048\",\n",
    "        \"MAX_TOTAL_TOKENS\": \"4096\",\n",
    "        \"MAX_BATCH_SIZE\": \"8\",\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Model: {model_config['model_id']}\")\n",
    "print(f\"Instance: {model_config['instance_type']}\")\n",
    "print(f\"Quantization: {model_config['quantization']}\")\n",
    "print()\n",
    "\n",
    "# Deploy (will simulate if no AWS access)\n",
    "endpoint = deployer.deploy_huggingface_model(**model_config)\n",
    "\n",
    "print(\"\\nDeployment Result:\")\n",
    "print(f\"  Endpoint Name: {endpoint.name}\")\n",
    "print(f\"  Status: {endpoint.status}\")\n",
    "print(f\"  Instance Type: {endpoint.instance_type}\")\n",
    "print(f\"  Cost: ${endpoint.cost_per_hour:.2f}/hour\")\n",
    "print(f\"  Monthly Cost: ${endpoint.cost_per_hour * 24 * 30:.2f}/month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the deployment details\n",
    "import json\n",
    "\n",
    "print(\"Endpoint Details:\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(endpoint.to_dict(), indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Invoking the Endpoint\n",
    "\n",
    "Once deployed, you can invoke the endpoint via the SageMaker Runtime API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Invoke the endpoint\n",
    "print(\"Invoking SageMaker Endpoint\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Request payload\n",
    "payload = {\n",
    "    \"inputs\": \"What is the capital of France?\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"do_sample\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Request: {payload['inputs']}\")\n",
    "print()\n",
    "\n",
    "# Invoke (simulated response if no AWS access)\n",
    "response = deployer.invoke_endpoint(endpoint.name, payload)\n",
    "\n",
    "print(\"Response:\")\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Python code for real invocation (copy this for your applications)\n",
    "example_code = '''\n",
    "# Real SageMaker Invocation Code\n",
    "# ================================\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "# Create runtime client\n",
    "runtime = boto3.client(\"sagemaker-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "# Prepare request\n",
    "payload = {\n",
    "    \"inputs\": \"What is the capital of France?\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"temperature\": 0.7,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Invoke endpoint\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=\"my-llm-endpoint\",\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(payload),\n",
    ")\n",
    "\n",
    "# Parse response\n",
    "result = json.loads(response[\"Body\"].read().decode())\n",
    "print(result[0][\"generated_text\"])\n",
    "'''\n",
    "\n",
    "print(example_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Auto-Scaling Configuration\n",
    "\n",
    "### ELI5: Auto-Scaling\n",
    "\n",
    "> **Imagine a restaurant during lunch rush...**\n",
    ">\n",
    "> When it's busy, you need more servers (waiters, not computers!).\n",
    "> When it's quiet, you send some home to save money.\n",
    ">\n",
    "> **Auto-scaling does this automatically:**\n",
    "> - Watches metrics (requests/second, latency)\n",
    "> - Adds instances when busy\n",
    "> - Removes instances when quiet\n",
    "> - Keeps costs proportional to usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure auto-scaling\n",
    "print(\"Configuring Auto-Scaling\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "autoscaling_config = deployer.configure_autoscaling(\n",
    "    endpoint_name=endpoint.name,\n",
    "    min_capacity=1,          # Never go below 1 instance\n",
    "    max_capacity=5,          # Scale up to 5 instances\n",
    "    target_invocations=70,   # Target 70 invocations per instance per minute\n",
    ")\n",
    "\n",
    "print(f\"Auto-scaling configured:\")\n",
    "print(f\"  Min instances: {autoscaling_config['min_capacity']}\")\n",
    "print(f\"  Max instances: {autoscaling_config['max_capacity']}\")\n",
    "print(f\"  Target: {autoscaling_config.get('target_invocations', 70)} invocations/instance/min\")\n",
    "print()\n",
    "print(\"Scaling behavior:\")\n",
    "print(\"  - Scale OUT when avg > 70 invocations/min (5 min cooldown)\")\n",
    "print(\"  - Scale IN when avg < 70 invocations/min (10 min cooldown)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example auto-scaling policy (for reference)\n",
    "autoscaling_policy = '''\n",
    "# AWS Auto-Scaling Policy Configuration\n",
    "# ======================================\n",
    "\n",
    "# Using boto3 directly:\n",
    "\n",
    "import boto3\n",
    "\n",
    "autoscaling = boto3.client(\"application-autoscaling\")\n",
    "\n",
    "# Register scalable target\n",
    "autoscaling.register_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=f\"endpoint/{endpoint_name}/variant/AllTraffic\",\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=5,\n",
    ")\n",
    "\n",
    "# Create scaling policy\n",
    "autoscaling.put_scaling_policy(\n",
    "    PolicyName=f\"{endpoint_name}-scaling-policy\",\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=f\"endpoint/{endpoint_name}/variant/AllTraffic\",\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    PolicyType=\"TargetTrackingScaling\",\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        \"TargetValue\": 70,  # Target invocations per instance per minute\n",
    "        \"PredefinedMetricSpecification\": {\n",
    "            \"PredefinedMetricType\": \"SageMakerVariantInvocationsPerInstance\",\n",
    "        },\n",
    "        \"ScaleInCooldown\": 600,   # 10 minutes\n",
    "        \"ScaleOutCooldown\": 300,  # 5 minutes\n",
    "    },\n",
    ")\n",
    "'''\n",
    "\n",
    "print(autoscaling_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Cost Analysis\n",
    "\n",
    "Let's analyze the costs for different deployment scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost estimation for different scenarios\n",
    "print(\"Cost Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Estimate costs for a 7B model\n",
    "estimates = estimate_cloud_costs(\n",
    "    model_size_gb=14.0,          # 7B model at FP16\n",
    "    expected_requests_per_day=10000,\n",
    "    avg_latency_ms=150,          # 150ms per request\n",
    ")\n",
    "\n",
    "print(\"\\nCost Estimates for 7B Model (10K requests/day):\")\n",
    "print(\"-\" * 60)\n",
    "for est in estimates:\n",
    "    print(f\"\\n{est.platform}:\")\n",
    "    print(f\"  Instance: {est.instance_type}\")\n",
    "    print(f\"  Hourly: ${est.hourly_cost:.2f}\")\n",
    "    print(f\"  Monthly: ${est.monthly_cost:.0f}\")\n",
    "    print(f\"  Per 1K requests: ${est.cost_per_1k_requests:.4f}\")\n",
    "    if est.notes:\n",
    "        print(f\"  Note: {est.notes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with DGX Spark\n",
    "print(\"\\nDGX Spark vs. Cloud Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dgx_spark_cost = 4999  # Approximate purchase price\n",
    "power_cost_monthly = 50  # ~500W * 24h * 30d * $0.15/kWh\n",
    "\n",
    "# Break-even analysis\n",
    "cloud_monthly = estimates[0].monthly_cost  # SageMaker\n",
    "months_to_breakeven = dgx_spark_cost / (cloud_monthly - power_cost_monthly)\n",
    "\n",
    "print(f\"\\nDGX Spark:\")\n",
    "print(f\"  Purchase: ${dgx_spark_cost:,}\")\n",
    "print(f\"  Monthly power: ~${power_cost_monthly}\")\n",
    "print(f\"  Capability: Can run up to 50B models locally!\")\n",
    "\n",
    "print(f\"\\nAWS SageMaker (ml.g5.xlarge):\")\n",
    "print(f\"  Monthly: ${cloud_monthly:.0f}\")\n",
    "print(f\"  Capability: 7B models with 4-bit quantization\")\n",
    "\n",
    "print(f\"\\n Break-even: {months_to_breakeven:.1f} months\")\n",
    "print(f\"   If you run 24/7 for more than {months_to_breakeven:.1f} months,\")\n",
    "print(f\"   DGX Spark is more cost-effective!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Deployment Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get deployment checklist\n",
    "checklist = create_deployment_checklist()\n",
    "\n",
    "print(\"Production Deployment Checklist\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for category in checklist:\n",
    "    print(f\"\\n{category['category']}\")\n",
    "    print(\"-\" * 40)\n",
    "    for item in category['items']:\n",
    "        print(f\"  [ ] {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Custom Container Deployment\n",
    "\n",
    "For more control, you can deploy your own container to SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Deploy custom container\n",
    "custom_deployment_code = '''\n",
    "# Custom Container Deployment to SageMaker\n",
    "# =========================================\n",
    "\n",
    "# 1. Build and push container to ECR\n",
    "# -----------------------------------\n",
    "\n",
    "# Build image\n",
    "docker build -t my-inference:latest .\n",
    "\n",
    "# Tag for ECR\n",
    "aws ecr get-login-password --region us-west-2 | \\\n",
    "    docker login --username AWS --password-stdin 123456789.dkr.ecr.us-west-2.amazonaws.com\n",
    "\n",
    "docker tag my-inference:latest 123456789.dkr.ecr.us-west-2.amazonaws.com/my-inference:latest\n",
    "docker push 123456789.dkr.ecr.us-west-2.amazonaws.com/my-inference:latest\n",
    "\n",
    "# 2. Package model artifacts\n",
    "# --------------------------\n",
    "\n",
    "# Create model.tar.gz\n",
    "tar -czvf model.tar.gz model/\n",
    "\n",
    "# Upload to S3\n",
    "aws s3 cp model.tar.gz s3://my-bucket/models/\n",
    "\n",
    "# 3. Deploy using Python SDK\n",
    "# --------------------------\n",
    "\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "model = Model(\n",
    "    image_uri=\"123456789.dkr.ecr.us-west-2.amazonaws.com/my-inference:latest\",\n",
    "    model_data=\"s3://my-bucket/models/model.tar.gz\",\n",
    "    role=get_execution_role(),\n",
    "    env={\n",
    "        \"MODEL_PATH\": \"/opt/ml/model\",\n",
    "        \"CUDA_VISIBLE_DEVICES\": \"0\",\n",
    "    },\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.xlarge\",\n",
    "    endpoint_name=\"my-custom-endpoint\",\n",
    ")\n",
    "'''\n",
    "\n",
    "print(custom_deployment_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Wrong Instance Size\n",
    "\n",
    "```python\n",
    "# BAD - Model too large for instance\n",
    "model.deploy(instance_type=\"ml.g5.xlarge\")  # 24GB VRAM for 70B model\n",
    "\n",
    "# GOOD - Match model size to instance\n",
    "model.deploy(instance_type=\"ml.p4d.24xlarge\")  # 320GB VRAM for 70B model\n",
    "\n",
    "# BETTER - Use quantization\n",
    "model.deploy(\n",
    "    instance_type=\"ml.g5.xlarge\",\n",
    "    env={\"HF_MODEL_QUANTIZE\": \"bitsandbytes-nf4\"},  # 4-bit quantization\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 2: No Health Check Timeout\n",
    "\n",
    "```python\n",
    "# BAD - Default timeout too short for LLM loading\n",
    "model.deploy(instance_type=\"ml.g5.xlarge\")\n",
    "\n",
    "# GOOD - Increase container startup timeout\n",
    "model.deploy(\n",
    "    instance_type=\"ml.g5.xlarge\",\n",
    "    container_startup_health_check_timeout=600,  # 10 minutes\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 3: Forgetting to Delete Endpoints\n",
    "\n",
    "```python\n",
    "# SageMaker charges by the hour even if not in use!\n",
    "# Always clean up when done testing\n",
    "\n",
    "predictor.delete_endpoint()  # Stop billing\n",
    "predictor.delete_model()     # Clean up model artifact\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup - Delete the endpoint\n",
    "print(\"Cleanup\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if endpoint.status == \"Simulated\":\n",
    "    print(\"Simulated endpoint - no cleanup needed.\")\n",
    "else:\n",
    "    print(f\"To delete the real endpoint, run:\")\n",
    "    print(f\"  deployer.delete_endpoint('{endpoint.name}')\")\n",
    "\n",
    "print(\"\\n Remember: Endpoints charge by the hour!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself\n",
    "\n",
    "### Exercise 1: Cost Calculator\n",
    "\n",
    "Create a function that calculates the monthly cost for a given:\n",
    "- Number of requests per day\n",
    "- Average latency\n",
    "- Model size\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Use the `estimate_cloud_costs` function and consider auto-scaling.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement cost calculator\n",
    "def calculate_monthly_cost(\n",
    "    requests_per_day: int,\n",
    "    avg_latency_ms: float,\n",
    "    model_size_gb: float,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate estimated monthly cost on SageMaker.\n",
    "    \n",
    "    Args:\n",
    "        requests_per_day: Expected daily request volume\n",
    "        avg_latency_ms: Average request latency\n",
    "        model_size_gb: Model size in GB\n",
    "    \n",
    "    Returns:\n",
    "        Estimated monthly cost in USD\n",
    "    \"\"\"\n",
    "    # TODO: Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test your function:\n",
    "# cost = calculate_monthly_cost(10000, 150, 14.0)\n",
    "# print(f\"Estimated monthly cost: ${cost:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- SageMaker architecture and components\n",
    "- How to deploy HuggingFace models\n",
    "- Instance type selection for different model sizes\n",
    "- Auto-scaling configuration\n",
    "- Cost analysis and optimization\n",
    "- Custom container deployment\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "Create a SageMaker deployment pipeline that:\n",
    "1. Packages a fine-tuned model from your DGX Spark\n",
    "2. Uploads to S3 with versioning\n",
    "3. Deploys to SageMaker with A/B testing (80/20 traffic split)\n",
    "4. Sets up CloudWatch alarms for latency and errors\n",
    "5. Implements automatic rollback on high error rate\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [SageMaker Documentation](https://docs.aws.amazon.com/sagemaker/)\n",
    "- [HuggingFace SageMaker Integration](https://huggingface.co/docs/sagemaker/)\n",
    "- [SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/)\n",
    "- [SageMaker Best Practices](https://docs.aws.amazon.com/sagemaker/latest/dg/best-practices.html)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all endpoints (if using real AWS)\n",
    "print(\"To list all your SageMaker endpoints:\")\n",
    "print(\"  aws sagemaker list-endpoints\")\n",
    "print(\"\\nTo delete an endpoint:\")\n",
    "print(\"  aws sagemaker delete-endpoint --endpoint-name <name>\")\n",
    "print(\"  aws sagemaker delete-endpoint-config --endpoint-config-name <name>\")\n",
    "print(\"  aws sagemaker delete-model --model-name <name>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
