{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.4.2: Building ML Stacks with Docker Compose\n",
    "\n",
    "**Module:** 4.4 - Containerization & Cloud Deployment  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐⭐ (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- [ ] Understand Docker Compose for multi-container applications\n",
    "- [ ] Create a complete ML stack with inference server, vector DB, and monitoring\n",
    "- [ ] Configure networking between services\n",
    "- [ ] Set up GPU allocation for ML services\n",
    "- [ ] Implement health checks and dependencies\n",
    "- [ ] Use volumes for persistent storage\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Lab 4.4.1 (Docker ML Image)\n",
    "- Docker Compose installed\n",
    "- Basic understanding of YAML\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "**Production ML systems are never just one container.**\n",
    "\n",
    "A typical production ML system includes:\n",
    "- **Inference Server**: Runs your model (GPU)\n",
    "- **Vector Database**: Stores embeddings for RAG (ChromaDB, Qdrant, Milvus)\n",
    "- **Monitoring**: Tracks performance (Prometheus, Grafana)\n",
    "- **API Gateway**: Handles load balancing, auth (Traefik, Nginx)\n",
    "- **Cache**: Reduces latency (Redis)\n",
    "\n",
    "Docker Compose lets you define and run all these services together with a single command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: Docker Compose\n",
    "\n",
    "> **Imagine you're running a restaurant...**\n",
    ">\n",
    "> You don't just have a chef. You have:\n",
    "> - A chef (makes the food) - *Inference Server*\n",
    "> - A pantry (stores ingredients) - *Vector Database*\n",
    "> - A manager (tracks orders, timing) - *Monitoring*\n",
    "> - A host (greets customers, assigns tables) - *API Gateway*\n",
    ">\n",
    "> **Docker Compose is like the restaurant blueprints** - it shows where each station goes, how they connect, and what each one needs to operate.\n",
    ">\n",
    "> With one command (`docker compose up`), the entire restaurant opens for business!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Docker Compose Fundamentals\n",
    "\n",
    "### Compose File Structure\n",
    "\n",
    "```yaml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  service-name:\n",
    "    image: image-name:tag\n",
    "    ports:\n",
    "      - \"host:container\"\n",
    "    environment:\n",
    "      - KEY=value\n",
    "    volumes:\n",
    "      - host-path:container-path\n",
    "    depends_on:\n",
    "      - other-service\n",
    "\n",
    "volumes:\n",
    "  named-volume:\n",
    "\n",
    "networks:\n",
    "  custom-network:\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description | Example |\n",
    "|---------|-------------|---------|\n",
    "| **Services** | Individual containers | inference, vectordb, prometheus |\n",
    "| **Ports** | Expose container ports to host | \"8000:8000\" |\n",
    "| **Volumes** | Persist data across restarts | models:/models |\n",
    "| **Networks** | Communication between services | Internal DNS |\n",
    "| **depends_on** | Startup order | vectordb before inference |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by checking Docker Compose is available\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "def run_command(cmd):\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    return result.stdout.strip(), result.returncode\n",
    "\n",
    "print(\"Docker Compose Check\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check Docker Compose (v2 style)\n",
    "output, code = run_command(\"docker compose version\")\n",
    "if code == 0:\n",
    "    print(f\"Docker Compose: {output}\")\n",
    "else:\n",
    "    # Try v1 style\n",
    "    output, code = run_command(\"docker-compose --version\")\n",
    "    if code == 0:\n",
    "        print(f\"Docker Compose (legacy): {output}\")\n",
    "    else:\n",
    "        print(\" Docker Compose not installed!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Understanding Our Docker Compose Utilities\n\nThis curriculum provides a `DockerComposeManager` class to simplify multi-container stack creation. Let's understand its API before using it.\n\n### DockerComposeManager Class\n\nThe `DockerComposeManager` class helps generate docker-compose.yml files programmatically:\n\n| Method | Description | Parameters |\n|--------|-------------|------------|\n| `add_inference_service()` | Add an ML inference container | `name`, `image`, `port`, `model_path`, `gpu=True` |\n| `add_vector_db(type)` | Add a vector database | `\"chromadb\"`, `\"qdrant\"`, or `\"milvus\"`, `port` |\n| `add_monitoring(level)` | Add monitoring stack | `\"minimal\"` (Prometheus) or `\"full\"` (+ Grafana) |\n| `generate()` | Generate YAML content | Returns string |\n| `save(path)` | Save to file | Path to docker-compose.yml |\n\n### Example Usage\n\n```python\nfrom scripts.docker_utils import DockerComposeManager\n\ncompose = DockerComposeManager()\ncompose.add_inference_service(name=\"llm\", image=\"my-llm:v1\", port=8000, gpu=True)\ncompose.add_vector_db(\"chromadb\", port=8001)\ncompose.add_monitoring(\"full\", prometheus_port=9090, grafana_port=3000)\nprint(compose.generate())\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Creating an ML Stack\n",
    "\n",
    "Let's build a complete ML stack with:\n",
    "\n",
    "1. **Inference Server** - Our LLM from Lab 4.4.1\n",
    "2. **ChromaDB** - Vector database for RAG\n",
    "3. **Prometheus** - Metrics collection\n",
    "4. **Grafana** - Metrics visualization\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "                    ┌─────────────┐\n",
    "                    │   Client    │\n",
    "                    └──────┬──────┘\n",
    "                           │\n",
    "                    ┌──────▼──────┐\n",
    "                    │  Inference  │◄────── GPU\n",
    "                    │   :8000     │\n",
    "                    └──────┬──────┘\n",
    "                           │\n",
    "               ┌───────────┴───────────┐\n",
    "               │                       │\n",
    "        ┌──────▼──────┐         ┌──────▼──────┐\n",
    "        │  ChromaDB   │         │ Prometheus  │\n",
    "        │    :8001    │         │   :9090     │\n",
    "        └─────────────┘         └──────┬──────┘\n",
    "                                       │\n",
    "                                ┌──────▼──────┐\n",
    "                                │   Grafana   │\n",
    "                                │    :3000    │\n",
    "                                └─────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our DockerComposeManager to create the stack\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from scripts.docker_utils import DockerComposeManager\n",
    "\n",
    "# Create compose manager\n",
    "compose = DockerComposeManager()\n",
    "\n",
    "# Add inference server (GPU-enabled)\n",
    "compose.add_inference_service(\n",
    "    name=\"inference\",\n",
    "    image=\"llm-inference:latest\",\n",
    "    port=8000,\n",
    "    model_path=\"/models\",\n",
    "    gpu=True,\n",
    ")\n",
    "\n",
    "# Add vector database\n",
    "compose.add_vector_db(\"chromadb\", port=8001)\n",
    "\n",
    "# Add monitoring (Prometheus + Grafana)\n",
    "compose.add_monitoring(\"full\", prometheus_port=9090, grafana_port=3000)\n",
    "\n",
    "print(\"Generated docker-compose.yml:\")\n",
    "print(\"=\" * 60)\n",
    "print(compose.generate())\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a more complete, production-ready compose file manually\n",
    "# This gives us more control over the configuration\n",
    "\n",
    "docker_compose_yaml = '''version: '3.8'\n",
    "\n",
    "# ==============================================\n",
    "# ML Inference Stack for DGX Spark\n",
    "# ==============================================\n",
    "# Components:\n",
    "#   - Inference Server (GPU-enabled)\n",
    "#   - ChromaDB (Vector Database)\n",
    "#   - Prometheus (Metrics)\n",
    "#   - Grafana (Dashboards)\n",
    "# ==============================================\n",
    "\n",
    "services:\n",
    "  # ============================================\n",
    "  # LLM Inference Server\n",
    "  # ============================================\n",
    "  inference:\n",
    "    image: llm-inference:latest\n",
    "    build:\n",
    "      context: ./inference-server\n",
    "      dockerfile: Dockerfile\n",
    "    container_name: llm-inference\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - MODEL_PATH=/models\n",
    "      - MODEL_NAME=gpt2\n",
    "      - CUDA_VISIBLE_DEVICES=0\n",
    "      - TRANSFORMERS_CACHE=/models/cache\n",
    "      - CHROMADB_HOST=vectordb\n",
    "      - CHROMADB_PORT=8000\n",
    "    volumes:\n",
    "      - ./models:/models\n",
    "      - model_cache:/models/cache\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              count: 1\n",
    "              capabilities: [gpu]\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "      start_period: 60s\n",
    "    depends_on:\n",
    "      vectordb:\n",
    "        condition: service_healthy\n",
    "    restart: unless-stopped\n",
    "    networks:\n",
    "      - ml-network\n",
    "\n",
    "  # ============================================\n",
    "  # ChromaDB - Vector Database\n",
    "  # ============================================\n",
    "  vectordb:\n",
    "    image: chromadb/chroma:latest\n",
    "    container_name: chromadb\n",
    "    ports:\n",
    "      - \"8001:8000\"\n",
    "    environment:\n",
    "      - ANONYMIZED_TELEMETRY=false\n",
    "      - CHROMA_SERVER_AUTH_PROVIDER=\n",
    "    volumes:\n",
    "      - chroma_data:/chroma/chroma\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/api/v1/heartbeat\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "    restart: unless-stopped\n",
    "    networks:\n",
    "      - ml-network\n",
    "\n",
    "  # ============================================\n",
    "  # Prometheus - Metrics Collection\n",
    "  # ============================================\n",
    "  prometheus:\n",
    "    image: prom/prometheus:latest\n",
    "    container_name: prometheus\n",
    "    ports:\n",
    "      - \"9090:9090\"\n",
    "    volumes:\n",
    "      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro\n",
    "      - prometheus_data:/prometheus\n",
    "    command:\n",
    "      - \"--config.file=/etc/prometheus/prometheus.yml\"\n",
    "      - \"--storage.tsdb.path=/prometheus\"\n",
    "      - \"--storage.tsdb.retention.time=15d\"\n",
    "      - \"--web.enable-lifecycle\"\n",
    "    restart: unless-stopped\n",
    "    networks:\n",
    "      - ml-network\n",
    "\n",
    "  # ============================================\n",
    "  # Grafana - Visualization\n",
    "  # ============================================\n",
    "  grafana:\n",
    "    image: grafana/grafana:latest\n",
    "    container_name: grafana\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "    environment:\n",
    "      - GF_SECURITY_ADMIN_USER=admin\n",
    "      - GF_SECURITY_ADMIN_PASSWORD=admin\n",
    "      - GF_USERS_ALLOW_SIGN_UP=false\n",
    "      - GF_DASHBOARDS_DEFAULT_HOME_DASHBOARD_PATH=/var/lib/grafana/dashboards/ml-dashboard.json\n",
    "    volumes:\n",
    "      - grafana_data:/var/lib/grafana\n",
    "      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro\n",
    "      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro\n",
    "    depends_on:\n",
    "      - prometheus\n",
    "    restart: unless-stopped\n",
    "    networks:\n",
    "      - ml-network\n",
    "\n",
    "# ============================================\n",
    "# Volumes - Persistent Storage\n",
    "# ============================================\n",
    "volumes:\n",
    "  model_cache:\n",
    "    name: ml-model-cache\n",
    "  chroma_data:\n",
    "    name: ml-chroma-data\n",
    "  prometheus_data:\n",
    "    name: ml-prometheus-data\n",
    "  grafana_data:\n",
    "    name: ml-grafana-data\n",
    "\n",
    "# ============================================\n",
    "# Networks\n",
    "# ============================================\n",
    "networks:\n",
    "  ml-network:\n",
    "    name: ml-inference-network\n",
    "    driver: bridge\n",
    "'''\n",
    "\n",
    "# Save the compose file\n",
    "os.makedirs(\"../docker-examples/ml-stack\", exist_ok=True)\n",
    "\n",
    "with open(\"../docker-examples/ml-stack/docker-compose.yml\", \"w\") as f:\n",
    "    f.write(docker_compose_yaml)\n",
    "\n",
    "print(\"Created: docker-compose.yml\")\n",
    "print(\"\\nThis configuration includes:\")\n",
    "print(\"  - Inference server with GPU support\")\n",
    "print(\"  - ChromaDB for vector storage\")\n",
    "print(\"  - Prometheus for metrics\")\n",
    "print(\"  - Grafana for dashboards\")\n",
    "print(\"  - Health checks for all services\")\n",
    "print(\"  - Persistent volumes for data\")\n",
    "print(\"  - Custom network for inter-service communication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Prometheus Configuration\n",
    "\n",
    "Prometheus needs a configuration file to know what to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Prometheus configuration\n",
    "prometheus_config = '''# Prometheus Configuration for ML Stack\n",
    "global:\n",
    "  scrape_interval: 15s\n",
    "  evaluation_interval: 15s\n",
    "  external_labels:\n",
    "    environment: development\n",
    "    stack: ml-inference\n",
    "\n",
    "# Alerting configuration (optional)\n",
    "alerting:\n",
    "  alertmanagers:\n",
    "    - static_configs:\n",
    "        - targets: []\n",
    "\n",
    "# Scrape configurations\n",
    "scrape_configs:\n",
    "  # Prometheus self-monitoring\n",
    "  - job_name: 'prometheus'\n",
    "    static_configs:\n",
    "      - targets: ['localhost:9090']\n",
    "    metrics_path: /metrics\n",
    "\n",
    "  # LLM Inference Server\n",
    "  - job_name: 'inference-server'\n",
    "    static_configs:\n",
    "      - targets: ['inference:8000']\n",
    "    metrics_path: /metrics\n",
    "    scrape_interval: 10s\n",
    "    relabel_configs:\n",
    "      - source_labels: [__address__]\n",
    "        target_label: instance\n",
    "        replacement: 'llm-inference'\n",
    "\n",
    "  # ChromaDB metrics (if exposed)\n",
    "  - job_name: 'chromadb'\n",
    "    static_configs:\n",
    "      - targets: ['vectordb:8000']\n",
    "    metrics_path: /api/v1/metrics\n",
    "    scrape_interval: 30s\n",
    "\n",
    "  # GPU metrics (NVIDIA DCGM exporter if running)\n",
    "  - job_name: 'gpu-metrics'\n",
    "    static_configs:\n",
    "      - targets: ['host.docker.internal:9400']\n",
    "    scrape_interval: 15s\n",
    "'''\n",
    "\n",
    "# Create monitoring directory structure\n",
    "os.makedirs(\"../docker-examples/ml-stack/monitoring\", exist_ok=True)\n",
    "\n",
    "with open(\"../docker-examples/ml-stack/monitoring/prometheus.yml\", \"w\") as f:\n",
    "    f.write(prometheus_config)\n",
    "\n",
    "print(\"Created: monitoring/prometheus.yml\")\n",
    "print(\"\\nPrometheus will scrape:\")\n",
    "print(\"  - Itself (self-monitoring)\")\n",
    "print(\"  - Inference server metrics\")\n",
    "print(\"  - ChromaDB metrics\")\n",
    "print(\"  - GPU metrics (if DCGM exporter is running)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Grafana provisioning configuration\n",
    "os.makedirs(\"../docker-examples/ml-stack/monitoring/grafana/provisioning/datasources\", exist_ok=True)\n",
    "os.makedirs(\"../docker-examples/ml-stack/monitoring/grafana/provisioning/dashboards\", exist_ok=True)\n",
    "os.makedirs(\"../docker-examples/ml-stack/monitoring/grafana/dashboards\", exist_ok=True)\n",
    "\n",
    "# Datasource configuration\n",
    "datasource_config = '''apiVersion: 1\n",
    "\n",
    "datasources:\n",
    "  - name: Prometheus\n",
    "    type: prometheus\n",
    "    access: proxy\n",
    "    url: http://prometheus:9090\n",
    "    isDefault: true\n",
    "    editable: false\n",
    "'''\n",
    "\n",
    "with open(\"../docker-examples/ml-stack/monitoring/grafana/provisioning/datasources/datasources.yml\", \"w\") as f:\n",
    "    f.write(datasource_config)\n",
    "\n",
    "# Dashboard provisioning\n",
    "dashboard_provisioning = '''apiVersion: 1\n",
    "\n",
    "providers:\n",
    "  - name: 'ML Dashboards'\n",
    "    orgId: 1\n",
    "    folder: 'ML Monitoring'\n",
    "    type: file\n",
    "    disableDeletion: false\n",
    "    updateIntervalSeconds: 10\n",
    "    options:\n",
    "      path: /var/lib/grafana/dashboards\n",
    "'''\n",
    "\n",
    "with open(\"../docker-examples/ml-stack/monitoring/grafana/provisioning/dashboards/dashboards.yml\", \"w\") as f:\n",
    "    f.write(dashboard_provisioning)\n",
    "\n",
    "print(\"Created Grafana provisioning configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple ML monitoring dashboard for Grafana\n",
    "import json\n",
    "\n",
    "ml_dashboard = {\n",
    "    \"annotations\": {\"list\": []},\n",
    "    \"editable\": True,\n",
    "    \"fiscalYearStartMonth\": 0,\n",
    "    \"graphTooltip\": 0,\n",
    "    \"id\": None,\n",
    "    \"links\": [],\n",
    "    \"liveNow\": False,\n",
    "    \"panels\": [\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"title\": \"Inference Requests/sec\",\n",
    "            \"type\": \"timeseries\",\n",
    "            \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 0, \"y\": 0},\n",
    "            \"targets\": [\n",
    "                {\n",
    "                    \"expr\": 'rate(http_requests_total{job=\"inference-server\"}[5m])',\n",
    "                    \"legendFormat\": \"{{method}} {{path}}\",\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"id\": 2,\n",
    "            \"title\": \"Response Latency (p99)\",\n",
    "            \"type\": \"timeseries\",\n",
    "            \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 12, \"y\": 0},\n",
    "            \"targets\": [\n",
    "                {\n",
    "                    \"expr\": 'histogram_quantile(0.99, rate(http_request_duration_seconds_bucket{job=\"inference-server\"}[5m]))',\n",
    "                    \"legendFormat\": \"p99 latency\",\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"id\": 3,\n",
    "            \"title\": \"GPU Memory Usage\",\n",
    "            \"type\": \"gauge\",\n",
    "            \"gridPos\": {\"h\": 8, \"w\": 8, \"x\": 0, \"y\": 8},\n",
    "            \"targets\": [\n",
    "                {\n",
    "                    \"expr\": 'DCGM_FI_DEV_FB_USED / DCGM_FI_DEV_FB_TOTAL * 100',\n",
    "                    \"legendFormat\": \"GPU Memory %\",\n",
    "                }\n",
    "            ],\n",
    "            \"fieldConfig\": {\n",
    "                \"defaults\": {\n",
    "                    \"max\": 100,\n",
    "                    \"min\": 0,\n",
    "                    \"unit\": \"percent\",\n",
    "                    \"thresholds\": {\n",
    "                        \"mode\": \"absolute\",\n",
    "                        \"steps\": [\n",
    "                            {\"color\": \"green\", \"value\": None},\n",
    "                            {\"color\": \"yellow\", \"value\": 70},\n",
    "                            {\"color\": \"red\", \"value\": 90},\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"id\": 4,\n",
    "            \"title\": \"GPU Utilization\",\n",
    "            \"type\": \"gauge\",\n",
    "            \"gridPos\": {\"h\": 8, \"w\": 8, \"x\": 8, \"y\": 8},\n",
    "            \"targets\": [\n",
    "                {\n",
    "                    \"expr\": 'DCGM_FI_DEV_GPU_UTIL',\n",
    "                    \"legendFormat\": \"GPU Util %\",\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"id\": 5,\n",
    "            \"title\": \"Tokens Generated/sec\",\n",
    "            \"type\": \"timeseries\",\n",
    "            \"gridPos\": {\"h\": 8, \"w\": 8, \"x\": 16, \"y\": 8},\n",
    "            \"targets\": [\n",
    "                {\n",
    "                    \"expr\": 'rate(tokens_generated_total{job=\"inference-server\"}[5m])',\n",
    "                    \"legendFormat\": \"tokens/sec\",\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    \"refresh\": \"5s\",\n",
    "    \"schemaVersion\": 38,\n",
    "    \"style\": \"dark\",\n",
    "    \"tags\": [\"ml\", \"inference\"],\n",
    "    \"templating\": {\"list\": []},\n",
    "    \"time\": {\"from\": \"now-1h\", \"to\": \"now\"},\n",
    "    \"timepicker\": {},\n",
    "    \"timezone\": \"\",\n",
    "    \"title\": \"ML Inference Dashboard\",\n",
    "    \"uid\": \"ml-inference-dashboard\",\n",
    "    \"version\": 1,\n",
    "    \"weekStart\": \"\"\n",
    "}\n",
    "\n",
    "with open(\"../docker-examples/ml-stack/monitoring/grafana/dashboards/ml-dashboard.json\", \"w\") as f:\n",
    "    json.dump(ml_dashboard, f, indent=2)\n",
    "\n",
    "print(\"Created: monitoring/grafana/dashboards/ml-dashboard.json\")\n",
    "print(\"\\nDashboard includes:\")\n",
    "print(\"  - Request rate graph\")\n",
    "print(\"  - Latency percentiles\")\n",
    "print(\"  - GPU memory gauge\")\n",
    "print(\"  - GPU utilization gauge\")\n",
    "print(\"  - Token generation rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Service Dependencies and Health Checks\n",
    "\n",
    "### ELI5: Why Dependencies Matter\n",
    "\n",
    "> **Imagine making a sandwich...**\n",
    ">\n",
    "> You can't put on the top bread before the fillings. Order matters!\n",
    ">\n",
    "> In Docker Compose:\n",
    "> - **depends_on** = \"Don't start me until these services are running\"\n",
    "> - **condition: service_healthy** = \"Wait until they pass health checks\"\n",
    ">\n",
    "> Without proper dependencies, your inference server might crash trying to connect to a database that isn't ready yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at dependency patterns in detail\n",
    "\n",
    "dependency_examples = '''\n",
    "# ============================================\n",
    "# Dependency Patterns in Docker Compose\n",
    "# ============================================\n",
    "\n",
    "# Pattern 1: Simple dependency (just wait for container to start)\n",
    "inference:\n",
    "  depends_on:\n",
    "    - vectordb\n",
    "\n",
    "# Pattern 2: Wait for service to be healthy (RECOMMENDED)\n",
    "inference:\n",
    "  depends_on:\n",
    "    vectordb:\n",
    "      condition: service_healthy\n",
    "    prometheus:\n",
    "      condition: service_started\n",
    "\n",
    "# Pattern 3: Multiple dependencies with mixed conditions\n",
    "api-gateway:\n",
    "  depends_on:\n",
    "    inference:\n",
    "      condition: service_healthy\n",
    "    auth-service:\n",
    "      condition: service_healthy\n",
    "    cache:\n",
    "      condition: service_started\n",
    "\n",
    "# Health check best practices:\n",
    "healthcheck:\n",
    "  test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n",
    "  interval: 30s      # How often to check\n",
    "  timeout: 10s       # How long to wait for response\n",
    "  retries: 3         # How many failures before unhealthy\n",
    "  start_period: 60s  # Grace period for slow-starting services (like ML models)\n",
    "'''\n",
    "\n",
    "print(dependency_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: GPU Allocation in Docker Compose\n",
    "\n",
    "### GPU Configuration Options\n",
    "\n",
    "```yaml\n",
    "# Option 1: Use all GPUs\n",
    "deploy:\n",
    "  resources:\n",
    "    reservations:\n",
    "      devices:\n",
    "        - driver: nvidia\n",
    "          count: all\n",
    "          capabilities: [gpu]\n",
    "\n",
    "# Option 2: Specific number of GPUs\n",
    "deploy:\n",
    "  resources:\n",
    "    reservations:\n",
    "      devices:\n",
    "        - driver: nvidia\n",
    "          count: 1\n",
    "          capabilities: [gpu]\n",
    "\n",
    "# Option 3: Specific GPU by ID\n",
    "deploy:\n",
    "  resources:\n",
    "    reservations:\n",
    "      devices:\n",
    "        - driver: nvidia\n",
    "          device_ids: ['0', '1']\n",
    "          capabilities: [gpu]\n",
    "```\n",
    "\n",
    "### DGX Spark Consideration\n",
    "\n",
    "DGX Spark has **one** powerful Blackwell GPU with 128GB unified memory. You'll typically use:\n",
    "```yaml\n",
    "count: 1  # or just 'all' since there's only one\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Networking Between Services\n",
    "\n",
    "### How Services Talk to Each Other\n",
    "\n",
    "Docker Compose creates a private network where:\n",
    "- Services can reach each other by **name** (DNS)\n",
    "- No need for IP addresses\n",
    "- Ports are only exposed to host if specified\n",
    "\n",
    "```\n",
    "inference container:  http://vectordb:8000  (internal)\n",
    "host machine:         http://localhost:8001 (exposed port)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a helper script to test the stack\n",
    "test_script = '''#!/bin/bash\n",
    "# Test script for ML Stack\n",
    "\n",
    "echo \"==================================================\"\n",
    "echo \"Testing ML Inference Stack\"\n",
    "echo \"==================================================\"\n",
    "\n",
    "# Wait for services to be ready\n",
    "echo \"\\nWaiting for services to start...\"\n",
    "sleep 5\n",
    "\n",
    "# Test inference server health\n",
    "echo \"\\n[1/4] Testing Inference Server...\"\n",
    "curl -s http://localhost:8000/health | python3 -m json.tool\n",
    "\n",
    "# Test ChromaDB\n",
    "echo \"\\n[2/4] Testing ChromaDB...\"\n",
    "curl -s http://localhost:8001/api/v1/heartbeat | python3 -m json.tool\n",
    "\n",
    "# Test Prometheus\n",
    "echo \"\\n[3/4] Testing Prometheus...\"\n",
    "curl -s http://localhost:9090/-/healthy\n",
    "echo \"\"\n",
    "\n",
    "# Test Grafana\n",
    "echo \"\\n[4/4] Testing Grafana...\"\n",
    "curl -s http://localhost:3000/api/health | python3 -m json.tool\n",
    "\n",
    "echo \"\\n==================================================\"\n",
    "echo \"Stack Test Complete!\"\n",
    "echo \"==================================================\"\n",
    "echo \"\\nAccess URLs:\"\n",
    "echo \"  - Inference API:  http://localhost:8000/docs\"\n",
    "echo \"  - ChromaDB:       http://localhost:8001\"\n",
    "echo \"  - Prometheus:     http://localhost:9090\"\n",
    "echo \"  - Grafana:        http://localhost:3000 (admin/admin)\"\n",
    "'''\n",
    "\n",
    "with open(\"../docker-examples/ml-stack/test-stack.sh\", \"w\") as f:\n",
    "    f.write(test_script)\n",
    "\n",
    "os.chmod(\"../docker-examples/ml-stack/test-stack.sh\", 0o755)\n",
    "\n",
    "print(\"Created: test-stack.sh\")\n",
    "print(\"\\nTo run the full stack:\")\n",
    "print(\"  cd ../docker-examples/ml-stack\")\n",
    "print(\"  docker compose up -d\")\n",
    "print(\"  ./test-stack.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a README for the stack\n",
    "readme = '''# ML Inference Stack\n",
    "\n",
    "A complete ML inference stack for DGX Spark with:\n",
    "- LLM Inference Server (GPU-enabled)\n",
    "- ChromaDB Vector Database\n",
    "- Prometheus Monitoring\n",
    "- Grafana Dashboards\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "```bash\n",
    "# Start the stack\n",
    "docker compose up -d\n",
    "\n",
    "# View logs\n",
    "docker compose logs -f\n",
    "\n",
    "# Check status\n",
    "docker compose ps\n",
    "\n",
    "# Stop the stack\n",
    "docker compose down\n",
    "```\n",
    "\n",
    "## Services\n",
    "\n",
    "| Service | Port | Description |\n",
    "|---------|------|-------------|\n",
    "| Inference | 8000 | LLM API server |\n",
    "| ChromaDB | 8001 | Vector database |\n",
    "| Prometheus | 9090 | Metrics |\n",
    "| Grafana | 3000 | Dashboards |\n",
    "\n",
    "## API Endpoints\n",
    "\n",
    "### Inference Server\n",
    "\n",
    "- `GET /health` - Health check\n",
    "- `POST /predict` - Text generation\n",
    "- `POST /chat` - Chat completion\n",
    "- `GET /docs` - API documentation\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "```bash\n",
    "# Generate text\n",
    "curl -X POST http://localhost:8000/predict \\\\\n",
    "  -H \"Content-Type: application/json\" \\\\\n",
    "  -d '{\"prompt\": \"Hello, how are you?\", \"max_tokens\": 100}'\n",
    "\n",
    "# Chat\n",
    "curl -X POST http://localhost:8000/chat \\\\\n",
    "  -H \"Content-Type: application/json\" \\\\\n",
    "  -d '{\"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n",
    "```\n",
    "\n",
    "## Configuration\n",
    "\n",
    "Environment variables for the inference server:\n",
    "\n",
    "- `MODEL_PATH` - Path to model weights\n",
    "- `MODEL_NAME` - HuggingFace model ID\n",
    "- `CUDA_VISIBLE_DEVICES` - GPU selection\n",
    "\n",
    "## Monitoring\n",
    "\n",
    "Access Grafana at http://localhost:3000 (admin/admin)\n",
    "\n",
    "Pre-configured dashboards:\n",
    "- ML Inference Dashboard\n",
    "\n",
    "## Volumes\n",
    "\n",
    "Persistent data is stored in Docker volumes:\n",
    "- `ml-model-cache` - Model weights cache\n",
    "- `ml-chroma-data` - Vector database\n",
    "- `ml-prometheus-data` - Metrics history\n",
    "- `ml-grafana-data` - Dashboard configs\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### GPU not detected\n",
    "```bash\n",
    "# Verify NVIDIA Container Toolkit\n",
    "docker run --rm --gpus all nvidia/cuda:12.0-base nvidia-smi\n",
    "```\n",
    "\n",
    "### Service not healthy\n",
    "```bash\n",
    "# Check logs\n",
    "docker compose logs inference\n",
    "\n",
    "# Restart specific service\n",
    "docker compose restart inference\n",
    "```\n",
    "'''\n",
    "\n",
    "with open(\"../docker-examples/ml-stack/README.md\", \"w\") as f:\n",
    "    f.write(readme)\n",
    "\n",
    "print(\"Created: README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "Let's see the complete structure we've created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the final project structure\n",
    "print(\"ML Stack Project Structure:\")\n",
    "print(\"=\" * 60)\n",
    "!find ../docker-examples/ml-stack -type f | sort | head -20\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Using Health Checks with depends_on\n",
    "\n",
    "```yaml\n",
    "# BAD - Service might start before database is ready\n",
    "inference:\n",
    "  depends_on:\n",
    "    - vectordb\n",
    "\n",
    "# GOOD - Wait for database to be healthy\n",
    "inference:\n",
    "  depends_on:\n",
    "    vectordb:\n",
    "      condition: service_healthy\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 2: Exposing All Ports\n",
    "\n",
    "```yaml\n",
    "# BAD - Exposes internal service to host\n",
    "internal-service:\n",
    "  ports:\n",
    "    - \"5432:5432\"  # Now accessible from outside!\n",
    "\n",
    "# GOOD - Only expose what's needed\n",
    "internal-service:\n",
    "  # No ports exposed - only accessible within network\n",
    "  networks:\n",
    "    - internal\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 3: Not Using Named Volumes\n",
    "\n",
    "```yaml\n",
    "# BAD - Data lost on container removal\n",
    "volumes:\n",
    "  - ./data:/data\n",
    "\n",
    "# GOOD - Data persists in named volume\n",
    "volumes:\n",
    "  - app_data:/data\n",
    "\n",
    "volumes:\n",
    "  app_data:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself\n",
    "\n",
    "### Exercise 1: Add Redis Cache\n",
    "\n",
    "Add a Redis service to the stack for caching inference results.\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Use the official redis:alpine image and expose port 6379.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add Redis service configuration\n",
    "redis_config = '''\n",
    "# Add Redis to docker-compose.yml\n",
    "redis:\n",
    "  # TODO: Complete this configuration\n",
    "'''\n",
    "\n",
    "print(redis_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Add Traefik as API Gateway\n",
    "\n",
    "Add Traefik to handle load balancing and SSL termination.\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Traefik can automatically discover services using Docker labels.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- How to create multi-container ML stacks with Docker Compose\n",
    "- Service dependencies and health checks\n",
    "- GPU allocation for ML workloads\n",
    "- Networking between containers\n",
    "- Prometheus and Grafana for monitoring\n",
    "- Best practices for production deployments\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "Create a complete RAG (Retrieval-Augmented Generation) stack with:\n",
    "1. Document ingestion service\n",
    "2. Embedding service (GPU)\n",
    "3. ChromaDB for vector storage\n",
    "4. LLM inference service (GPU)\n",
    "5. API gateway with rate limiting\n",
    "6. Full monitoring\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Docker Compose Documentation](https://docs.docker.com/compose/)\n",
    "- [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/)\n",
    "- [Prometheus Monitoring](https://prometheus.io/docs/)\n",
    "- [Grafana Dashboards](https://grafana.com/docs/grafana/latest/dashboards/)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To clean up the stack:\n",
    "print(\"To stop and clean up the stack:\")\n",
    "print(\"  cd ../docker-examples/ml-stack\")\n",
    "print(\"  docker compose down -v  # -v removes volumes too\")\n",
    "print(\"\\nTo remove all related images:\")\n",
    "print(\"  docker compose down --rmi all\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}