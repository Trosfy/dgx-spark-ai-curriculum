{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.4.5: Kubernetes for ML Deployments\n",
    "\n",
    "**Module:** 4.4 - Containerization & Cloud Deployment  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐⭐⭐ (Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- [ ] Understand core Kubernetes concepts (Pods, Deployments, Services)\n",
    "- [ ] Create deployment manifests for ML inference servers\n",
    "- [ ] Configure GPU scheduling in Kubernetes\n",
    "- [ ] Implement Horizontal Pod Autoscaler (HPA)\n",
    "- [ ] Monitor and troubleshoot K8s deployments\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Docker image ready (from Lab 4.4.1)\n",
    "- minikube or kind installed (or access to a K8s cluster)\n",
    "- kubectl CLI installed\n",
    "\n",
    "**Note:** This lab can be completed without a running cluster using manifest generation.\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "**58% of organizations use Kubernetes for AI workloads.**\n",
    "\n",
    "Why Kubernetes over Docker Compose?\n",
    "\n",
    "| Feature | Docker Compose | Kubernetes |\n",
    "|---------|---------------|------------|\n",
    "| Scale | Single host | Multi-node cluster |\n",
    "| Failover | Manual | Automatic |\n",
    "| Updates | Downtime | Rolling updates |\n",
    "| Scheduling | Simple | GPU-aware |\n",
    "| Production | Dev/test | Enterprise-ready |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: What is Kubernetes?\n",
    "\n",
    "> **Imagine you're running a fleet of food trucks...**\n",
    ">\n",
    "> Docker Compose is like having one food truck you manually park and manage.\n",
    ">\n",
    "> **Kubernetes is like having a dispatcher** who:\n",
    "> - Sends trucks where they're needed (scheduling)\n",
    "> - Calls in backup trucks when one breaks (failover)\n",
    "> - Adds more trucks during lunch rush (auto-scaling)\n",
    "> - Upgrades trucks one at a time without closing (rolling updates)\n",
    ">\n",
    "> **K8s concepts:**\n",
    "> - **Pod** = Food truck\n",
    "> - **Deployment** = Fleet management rules\n",
    "> - **Service** = How customers find your trucks\n",
    "> - **Node** = Parking lot for trucks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Kubernetes Architecture\n",
    "\n",
    "### Core Components\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    Kubernetes Cluster                        │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│  Control Plane                                               │\n",
    "│  ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐       │\n",
    "│  │   API    │ │  etcd    │ │Scheduler │ │Controller│       │\n",
    "│  │  Server  │ │          │ │          │ │ Manager  │       │\n",
    "│  └──────────┘ └──────────┘ └──────────┘ └──────────┘       │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│  Worker Nodes                                                │\n",
    "│  ┌─────────────────────┐  ┌─────────────────────┐          │\n",
    "│  │      Node 1         │  │      Node 2         │          │\n",
    "│  │  ┌─────┐ ┌─────┐   │  │  ┌─────┐ ┌─────┐   │          │\n",
    "│  │  │Pod 1│ │Pod 2│   │  │  │Pod 3│ │Pod 4│   │          │\n",
    "│  │  └─────┘ └─────┘   │  │  └─────┘ └─────┘   │          │\n",
    "│  │  [GPU]             │  │  [GPU]             │          │\n",
    "│  └─────────────────────┘  └─────────────────────┘          │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **Pod** | Smallest deployable unit (1+ containers) |\n",
    "| **Deployment** | Manages Pod replicas and updates |\n",
    "| **Service** | Exposes Pods to network traffic |\n",
    "| **Node** | Physical/virtual machine running Pods |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Kubernetes environment\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"Kubernetes Environment Check\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check kubectl\n",
    "result = subprocess.run([\"kubectl\", \"version\", \"--client\", \"--short\"], \n",
    "                       capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(f\"kubectl: {result.stdout.strip()}\")\n",
    "else:\n",
    "    print(\" kubectl not installed\")\n",
    "    print(\"   Install: https://kubernetes.io/docs/tasks/tools/\")\n",
    "\n",
    "# Check cluster connection\n",
    "result = subprocess.run([\"kubectl\", \"cluster-info\"], capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(f\"Cluster: Connected\")\n",
    "    # Check for GPU nodes\n",
    "    result = subprocess.run(\n",
    "        [\"kubectl\", \"get\", \"nodes\", \"-o\", \"jsonpath={.items[*].status.capacity.nvidia\\.com/gpu}\"],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    if result.stdout.strip():\n",
    "        print(f\"GPUs detected: {result.stdout.strip()}\")\n",
    "else:\n",
    "    print(\" No cluster connected\")\n",
    "    print(\"   For local testing, install minikube or kind\")\n",
    "\n",
    "# Check minikube\n",
    "result = subprocess.run([\"minikube\", \"version\"], capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(f\"minikube: {result.stdout.strip().split()[2]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our K8s utilities\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from scripts.k8s_utils import (\n",
    "    K8sDeploymentManager,\n",
    "    generate_deployment_manifest,\n",
    "    generate_service_manifest,\n",
    "    generate_hpa_manifest,\n",
    "    generate_complete_ml_stack,\n",
    "    save_manifests,\n",
    ")\n",
    "\n",
    "print(\"K8s utilities loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Creating a Deployment Manifest\n",
    "\n",
    "### Deployment YAML Structure\n",
    "\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: my-app\n",
    "spec:\n",
    "  replicas: 2              # Number of Pods\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: my-app\n",
    "  template:                # Pod template\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: my-app\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: my-app\n",
    "        image: my-image:latest\n",
    "        resources:\n",
    "          limits:\n",
    "            nvidia.com/gpu: 1  # GPU request\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a deployment manifest for LLM inference\n",
    "deployment = generate_deployment_manifest(\n",
    "    name=\"llm-inference\",\n",
    "    image=\"llm-inference:latest\",\n",
    "    replicas=2,\n",
    "    port=8000,\n",
    "    gpu_count=1,\n",
    "    memory_request=\"16Gi\",\n",
    "    memory_limit=\"32Gi\",\n",
    "    cpu_request=\"4\",\n",
    "    cpu_limit=\"8\",\n",
    "    env_vars={\n",
    "        \"MODEL_PATH\": \"/models/llama-8b\",\n",
    "        \"MAX_BATCH_SIZE\": \"8\",\n",
    "        \"CUDA_VISIBLE_DEVICES\": \"0\",\n",
    "    },\n",
    "    health_path=\"/health\",\n",
    ")\n",
    "\n",
    "print(\"Generated Deployment Manifest:\")\n",
    "print(\"=\" * 60)\n",
    "print(deployment.to_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Manifest\n",
    "\n",
    "| Field | Purpose |\n",
    "|-------|--------|\n",
    "| `replicas` | How many Pod copies to run |\n",
    "| `selector.matchLabels` | How Deployment finds its Pods |\n",
    "| `resources.limits.nvidia.com/gpu` | Request GPU access |\n",
    "| `livenessProbe` | Restart Pod if it fails |\n",
    "| `readinessProbe` | Don't send traffic until ready |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Creating a Service\n",
    "\n",
    "### Service Types\n",
    "\n",
    "| Type | Use Case | Access |\n",
    "|------|----------|--------|\n",
    "| **ClusterIP** | Internal only | Within cluster |\n",
    "| **NodePort** | Development | Node IP + port |\n",
    "| **LoadBalancer** | Production | External IP |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Service manifest\n",
    "service = generate_service_manifest(\n",
    "    name=\"llm-inference\",\n",
    "    port=80,\n",
    "    target_port=8000,\n",
    "    service_type=\"LoadBalancer\",\n",
    ")\n",
    "\n",
    "print(\"Generated Service Manifest:\")\n",
    "print(\"=\" * 60)\n",
    "print(service.to_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Horizontal Pod Autoscaler (HPA)\n",
    "\n",
    "### ELI5: Auto-Scaling\n",
    "\n",
    "> **Like a restaurant adding tables...**\n",
    ">\n",
    "> When customers are waiting (high CPU), the manager opens more tables (adds Pods).\n",
    "> When the rush ends (low CPU), tables are closed (Pods removed).\n",
    ">\n",
    "> **HPA watches:**\n",
    "> - CPU utilization\n",
    "> - Memory usage\n",
    "> - Custom metrics (requests/second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate HPA manifest\n",
    "hpa = generate_hpa_manifest(\n",
    "    deployment_name=\"llm-inference\",\n",
    "    min_replicas=1,\n",
    "    max_replicas=5,\n",
    "    cpu_target=70,  # Scale when CPU > 70%\n",
    ")\n",
    "\n",
    "print(\"Generated HPA Manifest:\")\n",
    "print(\"=\" * 60)\n",
    "print(hpa.to_yaml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain HPA behavior\n",
    "hpa_explanation = '''\n",
    "# HPA Scaling Behavior\n",
    "# ====================\n",
    "\n",
    "Given:\n",
    "  - Target CPU: 70%\n",
    "  - Min replicas: 1\n",
    "  - Max replicas: 5\n",
    "\n",
    "Scaling Logic:\n",
    "  \n",
    "  desiredReplicas = ceil[currentReplicas × (currentMetric / targetMetric)]\n",
    "\n",
    "Example:\n",
    "  - Current: 2 replicas at 85% CPU each\n",
    "  - Desired = ceil[2 × (85/70)] = ceil[2.43] = 3 replicas\n",
    "  \n",
    "Cooldown Periods:\n",
    "  - Scale UP: 3 minutes (quick response to load)\n",
    "  - Scale DOWN: 5 minutes (avoid flapping)\n",
    "\n",
    "Best Practices for ML:\n",
    "  1. Set higher min replicas (model loading is slow)\n",
    "  2. Use custom metrics (requests/sec better than CPU for inference)\n",
    "  3. Increase stabilization window (models take time to warm up)\n",
    "'''\n",
    "\n",
    "print(hpa_explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: GPU Scheduling\n",
    "\n",
    "### NVIDIA Device Plugin\n",
    "\n",
    "Kubernetes uses the NVIDIA device plugin to:\n",
    "1. Discover GPUs on nodes\n",
    "2. Advertise GPU resources\n",
    "3. Allocate GPUs to Pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU scheduling configuration examples\n",
    "gpu_scheduling = '''\n",
    "# GPU Scheduling in Kubernetes\n",
    "# ============================\n",
    "\n",
    "# 1. Request specific number of GPUs\n",
    "resources:\n",
    "  limits:\n",
    "    nvidia.com/gpu: 1  # Request 1 GPU\n",
    "\n",
    "# 2. Request specific GPU type (with node labels)\n",
    "nodeSelector:\n",
    "  gpu-type: a100\n",
    "\n",
    "# 3. Tolerate GPU node taints\n",
    "tolerations:\n",
    "- key: nvidia.com/gpu\n",
    "  operator: Exists\n",
    "  effect: NoSchedule\n",
    "\n",
    "# 4. GPU affinity (prefer GPU nodes)\n",
    "affinity:\n",
    "  nodeAffinity:\n",
    "    preferredDuringSchedulingIgnoredDuringExecution:\n",
    "    - weight: 100\n",
    "      preference:\n",
    "        matchExpressions:\n",
    "        - key: nvidia.com/gpu\n",
    "          operator: Exists\n",
    "\n",
    "# 5. Check GPU availability\n",
    "kubectl describe nodes | grep -A5 \"Allocatable:\"\n",
    "# Look for: nvidia.com/gpu: 1\n",
    "\n",
    "# 6. Install NVIDIA device plugin (if not installed)\n",
    "kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.14.0/nvidia-device-plugin.yml\n",
    "'''\n",
    "\n",
    "print(gpu_scheduling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Complete ML Stack\n",
    "\n",
    "Let's generate a complete ML inference stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate complete ML stack\n",
    "resources = generate_complete_ml_stack(\n",
    "    name=\"llm-inference\",\n",
    "    image=\"my-registry/llm-inference:v1.0\",\n",
    "    port=8000,\n",
    "    replicas=2,\n",
    "    gpu_count=1,\n",
    "    enable_hpa=True,\n",
    "    enable_pvc=True,\n",
    "    storage_size=\"50Gi\",\n",
    "    env_vars={\n",
    "        \"MODEL_PATH\": \"/models/llama-8b\",\n",
    "        \"MAX_BATCH_SIZE\": \"32\",\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(resources)} Kubernetes resources:\")\n",
    "for r in resources:\n",
    "    print(f\"  - {r.kind}: {r.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save manifests to files\n",
    "import os\n",
    "\n",
    "output_dir = \"../configs/k8s\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "saved_files = save_manifests(resources, output_dir, single_file=False)\n",
    "\n",
    "print(\"Saved manifest files:\")\n",
    "for f in saved_files:\n",
    "    print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show combined manifest\n",
    "print(\"Combined Manifests (all-in-one):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for resource in resources:\n",
    "    print(resource.to_yaml())\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Deploying and Managing\n",
    "\n",
    "### Common kubectl Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common kubectl commands for ML deployments\n",
    "kubectl_commands = '''\n",
    "# Kubernetes Commands Cheatsheet for ML\n",
    "# =====================================\n",
    "\n",
    "# Deploy\n",
    "kubectl apply -f deployment.yaml\n",
    "kubectl apply -f ./configs/k8s/  # Apply all manifests in directory\n",
    "\n",
    "# Check status\n",
    "kubectl get pods -w                     # Watch pod status\n",
    "kubectl get deployment llm-inference    # Check deployment\n",
    "kubectl describe pod <pod-name>         # Detailed pod info\n",
    "\n",
    "# View logs\n",
    "kubectl logs <pod-name>                 # Current logs\n",
    "kubectl logs -f <pod-name>              # Follow logs\n",
    "kubectl logs <pod-name> --previous      # Previous container logs\n",
    "\n",
    "# Debug\n",
    "kubectl exec -it <pod-name> -- bash     # Shell into pod\n",
    "kubectl port-forward svc/llm-inference 8000:80  # Local access\n",
    "\n",
    "# Scale\n",
    "kubectl scale deployment llm-inference --replicas=3\n",
    "kubectl autoscale deployment llm-inference --min=1 --max=5 --cpu-percent=70\n",
    "\n",
    "# Update\n",
    "kubectl set image deployment/llm-inference llm-inference=my-image:v2\n",
    "kubectl rollout status deployment/llm-inference  # Watch rollout\n",
    "kubectl rollout undo deployment/llm-inference    # Rollback\n",
    "\n",
    "# Delete\n",
    "kubectl delete -f deployment.yaml\n",
    "kubectl delete deployment llm-inference\n",
    "\n",
    "# GPU-specific\n",
    "kubectl describe nodes | grep -A5 nvidia.com/gpu  # Check GPU availability\n",
    "kubectl top nodes                                 # Resource usage\n",
    "'''\n",
    "\n",
    "print(kubectl_commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deployment helper script\n",
    "deploy_script = '''#!/bin/bash\n",
    "# ML Deployment Script\n",
    "# Usage: ./deploy.sh [apply|delete|status|logs]\n",
    "\n",
    "NAMESPACE=${NAMESPACE:-default}\n",
    "DEPLOYMENT=\"llm-inference\"\n",
    "\n",
    "case \"$1\" in\n",
    "    apply)\n",
    "        echo \"Deploying ML stack...\"\n",
    "        kubectl apply -f ./k8s/ -n $NAMESPACE\n",
    "        echo \"\\nWaiting for rollout...\"\n",
    "        kubectl rollout status deployment/$DEPLOYMENT -n $NAMESPACE\n",
    "        ;;\n",
    "    delete)\n",
    "        echo \"Deleting ML stack...\"\n",
    "        kubectl delete -f ./k8s/ -n $NAMESPACE\n",
    "        ;;\n",
    "    status)\n",
    "        echo \"Deployment Status:\"\n",
    "        kubectl get deployment,svc,hpa,pvc -n $NAMESPACE\n",
    "        echo \"\\nPod Status:\"\n",
    "        kubectl get pods -n $NAMESPACE -l app=$DEPLOYMENT\n",
    "        ;;\n",
    "    logs)\n",
    "        POD=$(kubectl get pods -n $NAMESPACE -l app=$DEPLOYMENT -o jsonpath=\"{.items[0].metadata.name}\")\n",
    "        kubectl logs -f $POD -n $NAMESPACE\n",
    "        ;;\n",
    "    port-forward)\n",
    "        echo \"Port forwarding to localhost:8000...\"\n",
    "        kubectl port-forward svc/$DEPLOYMENT-service 8000:80 -n $NAMESPACE\n",
    "        ;;\n",
    "    *)\n",
    "        echo \"Usage: $0 {apply|delete|status|logs|port-forward}\"\n",
    "        exit 1\n",
    "        ;;\n",
    "esac\n",
    "'''\n",
    "\n",
    "with open(\"../configs/k8s/deploy.sh\", \"w\") as f:\n",
    "    f.write(deploy_script)\n",
    "\n",
    "os.chmod(\"../configs/k8s/deploy.sh\", 0o755)\n",
    "print(\"Created: configs/k8s/deploy.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Monitoring and Troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troubleshooting guide\n",
    "troubleshooting = '''\n",
    "# Kubernetes Troubleshooting for ML\n",
    "# ==================================\n",
    "\n",
    "## Pod stuck in Pending\n",
    "\n",
    "# Check events\n",
    "kubectl describe pod <pod-name>\n",
    "\n",
    "# Common causes:\n",
    "# - No GPU nodes available\n",
    "# - Insufficient memory\n",
    "# - PVC not bound\n",
    "\n",
    "## Pod in CrashLoopBackOff\n",
    "\n",
    "# Check logs from crashed container\n",
    "kubectl logs <pod-name> --previous\n",
    "\n",
    "# Common causes:\n",
    "# - Model loading failure (OOM)\n",
    "# - Missing dependencies\n",
    "# - Health check failing\n",
    "\n",
    "## GPU not detected\n",
    "\n",
    "# Verify NVIDIA device plugin\n",
    "kubectl get pods -n kube-system | grep nvidia\n",
    "\n",
    "# Check node GPU resources\n",
    "kubectl describe node <node-name> | grep nvidia\n",
    "\n",
    "# Fix: Install device plugin\n",
    "kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.14.0/nvidia-device-plugin.yml\n",
    "\n",
    "## High Latency\n",
    "\n",
    "# Check if HPA is scaling\n",
    "kubectl get hpa\n",
    "\n",
    "# Check pod resource usage\n",
    "kubectl top pods\n",
    "\n",
    "# Consider:\n",
    "# - Increase replicas\n",
    "# - Use larger instance type\n",
    "# - Enable request batching\n",
    "\n",
    "## Memory OOM\n",
    "\n",
    "# Check pod events\n",
    "kubectl describe pod <pod-name> | grep -A5 \"Events:\"\n",
    "\n",
    "# Solutions:\n",
    "# - Increase memory limits\n",
    "# - Use model quantization\n",
    "# - Enable gradient checkpointing\n",
    "'''\n",
    "\n",
    "print(troubleshooting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Setting Resource Limits\n",
    "\n",
    "```yaml\n",
    "# BAD - Pod can consume all node resources\n",
    "containers:\n",
    "- name: inference\n",
    "  image: my-image\n",
    "\n",
    "# GOOD - Resource limits prevent noisy neighbor\n",
    "containers:\n",
    "- name: inference\n",
    "  image: my-image\n",
    "  resources:\n",
    "    requests:\n",
    "      memory: \"16Gi\"\n",
    "      cpu: \"4\"\n",
    "    limits:\n",
    "      memory: \"32Gi\"\n",
    "      nvidia.com/gpu: 1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 2: Short Liveness Probe Timeout\n",
    "\n",
    "```yaml\n",
    "# BAD - LLM models take minutes to load\n",
    "livenessProbe:\n",
    "  initialDelaySeconds: 10  # Too short!\n",
    "\n",
    "# GOOD - Allow time for model loading\n",
    "livenessProbe:\n",
    "  initialDelaySeconds: 120  # 2 minutes\n",
    "  periodSeconds: 10\n",
    "  timeoutSeconds: 5\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 3: Ignoring Pod Disruption Budgets\n",
    "\n",
    "```yaml\n",
    "# Add PDB for production deployments\n",
    "apiVersion: policy/v1\n",
    "kind: PodDisruptionBudget\n",
    "metadata:\n",
    "  name: llm-inference-pdb\n",
    "spec:\n",
    "  minAvailable: 1  # At least 1 pod always running\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: llm-inference\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- Core Kubernetes concepts for ML\n",
    "- Creating Deployment, Service, and HPA manifests\n",
    "- GPU scheduling and configuration\n",
    "- Common kubectl commands\n",
    "- Troubleshooting techniques\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "Create a complete production-ready K8s setup with:\n",
    "1. Blue-green deployment strategy\n",
    "2. Ingress with SSL termination\n",
    "3. Prometheus ServiceMonitor\n",
    "4. Pod Disruption Budget\n",
    "5. Network Policy for security\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Kubernetes Documentation](https://kubernetes.io/docs/)\n",
    "- [NVIDIA GPU Operator](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/)\n",
    "- [Kube-scheduler GPU Support](https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/)\n",
    "- [HPA Custom Metrics](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup commands\n",
    "print(\"Kubernetes Cleanup\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n# Delete all resources\")\n",
    "print(\"kubectl delete -f ./configs/k8s/\")\n",
    "print(\"\\n# Or use the deploy script\")\n",
    "print(\"./configs/k8s/deploy.sh delete\")\n",
    "print(\"\\n# Delete persistent volumes (if needed)\")\n",
    "print(\"kubectl delete pvc llm-inference-storage\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
