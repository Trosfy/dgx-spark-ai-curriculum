{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.4.4: Deploying Models to GCP Vertex AI\n",
    "\n",
    "**Module:** 4.4 - Containerization & Cloud Deployment  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐⭐⭐ (Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- [ ] Understand GCP Vertex AI architecture\n",
    "- [ ] Deploy custom containers to Vertex AI\n",
    "- [ ] Configure GPU-accelerated endpoints\n",
    "- [ ] Compare Vertex AI with AWS SageMaker\n",
    "- [ ] Optimize for cost and performance\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- GCP Account with Vertex AI enabled\n",
    "- gcloud CLI configured (`gcloud auth login`)\n",
    "- Completed: Lab 4.4.3 (SageMaker for comparison)\n",
    "\n",
    "**Note:** This lab can be completed in simulation mode without GCP access.\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "**Why consider Vertex AI?**\n",
    "\n",
    "| Feature | Vertex AI | SageMaker |\n",
    "|---------|-----------|----------|\n",
    "| BigQuery integration | Native | Via S3/Athena |\n",
    "| Pricing simplicity | Simpler | Complex |\n",
    "| GPU options | Good | Excellent |\n",
    "| AutoML | Strong | Good |\n",
    "| Ecosystem | GCP-centric | AWS-centric |\n",
    "\n",
    "**Choose Vertex AI when:**\n",
    "- Your data is in BigQuery\n",
    "- You're already on GCP\n",
    "- You need simpler pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: Vertex AI vs SageMaker\n",
    "\n",
    "> **It's like choosing between pizza delivery services...**\n",
    ">\n",
    "> **SageMaker** is like a big chain (Domino's). Huge menu, lots of options, sometimes confusing pricing, works everywhere.\n",
    ">\n",
    "> **Vertex AI** is like a local pizzeria that knows your neighborhood. Fewer options, but simpler menu, and if you already shop at nearby stores (BigQuery, GCS), they know your preferences.\n",
    ">\n",
    "> **Both deliver great pizza (ML models)** - the choice depends on where you live (your cloud ecosystem)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GCP environment\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"GCP Environment Check\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check gcloud CLI\n",
    "result = subprocess.run([\"gcloud\", \"--version\"], capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    version = result.stdout.split('\\n')[0]\n",
    "    print(f\"gcloud CLI: {version}\")\n",
    "else:\n",
    "    print(\" gcloud CLI not installed\")\n",
    "\n",
    "# Check authentication\n",
    "result = subprocess.run(\n",
    "    [\"gcloud\", \"auth\", \"list\", \"--format=value(account)\"],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "if result.stdout.strip():\n",
    "    print(f\"Authenticated as: {result.stdout.strip().split()[0]}\")\n",
    "else:\n",
    "    print(\" Not authenticated. Run: gcloud auth login\")\n",
    "\n",
    "# Check google-cloud-aiplatform SDK\n",
    "try:\n",
    "    from google.cloud import aiplatform\n",
    "    print(f\"google-cloud-aiplatform: Installed\")\n",
    "except ImportError:\n",
    "    print(\" google-cloud-aiplatform not installed\")\n",
    "    print(\"   Run: pip install google-cloud-aiplatform\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our cloud utilities\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from scripts.cloud_utils import (\n",
    "    VertexAIDeployer,\n",
    "    compare_platforms,\n",
    "    estimate_cloud_costs,\n",
    ")\n",
    "\n",
    "print(\"Cloud utilities loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Vertex AI Architecture\n",
    "\n",
    "### Key Components\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    GCP Vertex AI                             │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                              │\n",
    "│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐  │\n",
    "│  │   Model      │    │   Model      │    │   Endpoint   │  │\n",
    "│  │   Artifact   │───>│   Registry   │───>│              │  │\n",
    "│  │   (GCS)      │    │              │    │              │  │\n",
    "│  └──────────────┘    └──────────────┘    └──────────────┘  │\n",
    "│                                                 │           │\n",
    "│                            ┌────────────────────┴─────┐    │\n",
    "│                            │      Deployed Model       │    │\n",
    "│                            │  ┌─────┐ ┌─────┐ ┌─────┐ │    │\n",
    "│                            │  │ v1  │ │ v2  │ │ v3  │ │    │\n",
    "│                            │  │ 50% │ │ 30% │ │ 20% │ │    │\n",
    "│                            │  └─────┘ └─────┘ └─────┘ │    │\n",
    "│                            └──────────────────────────┘    │\n",
    "│                                                              │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **Model Artifact** | Model files in Google Cloud Storage |\n",
    "| **Model Registry** | Versioned model management |\n",
    "| **Endpoint** | The deployed prediction service |\n",
    "| **Deployed Model** | A model version with traffic allocation |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Vertex AI Machine Types\n",
    "\n",
    "### GPU Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertex AI pricing information\n",
    "print(\"Vertex AI GPU Pricing (us-central1)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "vertex_pricing = {\n",
    "    \"NVIDIA_TESLA_T4\": {\"vram\": \"16GB\", \"cost\": 0.35, \"best_for\": \"7B models\"},\n",
    "    \"NVIDIA_L4\": {\"vram\": \"24GB\", \"cost\": 0.70, \"best_for\": \"7-13B models\"},\n",
    "    \"NVIDIA_TESLA_A100\": {\"vram\": \"40GB\", \"cost\": 2.93, \"best_for\": \"30B models\"},\n",
    "    \"NVIDIA_A100_80GB\": {\"vram\": \"80GB\", \"cost\": 3.67, \"best_for\": \"70B models\"},\n",
    "    \"NVIDIA_H100_80GB\": {\"vram\": \"80GB\", \"cost\": 10.00, \"best_for\": \"High performance\"},\n",
    "}\n",
    "\n",
    "for gpu, info in vertex_pricing.items():\n",
    "    monthly = info['cost'] * 24 * 30\n",
    "    print(f\"{gpu:25} {info['vram']:>6} ${info['cost']:>6.2f}/hr (${monthly:>7,.0f}/mo) - {info['best_for']}\")\n",
    "\n",
    "print(\"\\n Note: Add machine type cost (~$0.19-0.76/hr for n1-standard-4 to n1-standard-16)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Deploying to Vertex AI\n",
    "\n",
    "### Custom Container Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vertex AI deployer\n",
    "deployer = VertexAIDeployer(\n",
    "    project=\"my-gcp-project\",  # Replace with your project\n",
    "    region=\"us-central1\",\n",
    ")\n",
    "\n",
    "print(\"Vertex AI Deployer initialized\")\n",
    "print(f\"Project: {deployer.project}\")\n",
    "print(f\"Region: {deployer.region}\")\n",
    "print(f\"SDK available: {deployer._aiplatform_available}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model to Vertex AI\n",
    "print(\"Deploying model to Vertex AI...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configuration\n",
    "deploy_config = {\n",
    "    \"model_path\": \"gs://my-bucket/models/llama-7b\",\n",
    "    \"serving_container_image_uri\": \"us-docker.pkg.dev/my-project/inference/llm-server:latest\",\n",
    "    \"machine_type\": \"n1-standard-8\",\n",
    "    \"accelerator_type\": \"NVIDIA_L4\",\n",
    "    \"accelerator_count\": 1,\n",
    "    \"min_replica_count\": 1,\n",
    "    \"max_replica_count\": 3,\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Model: {deploy_config['model_path']}\")\n",
    "print(f\"  Machine: {deploy_config['machine_type']}\")\n",
    "print(f\"  GPU: {deploy_config['accelerator_type']}\")\n",
    "print(f\"  Replicas: {deploy_config['min_replica_count']}-{deploy_config['max_replica_count']}\")\n",
    "print()\n",
    "\n",
    "# Deploy (simulated if no GCP access)\n",
    "endpoint = deployer.deploy_model(**deploy_config)\n",
    "\n",
    "print(\"\\nDeployment Result:\")\n",
    "print(f\"  Endpoint Name: {endpoint.name}\")\n",
    "print(f\"  Status: {endpoint.status}\")\n",
    "print(f\"  Instance Type: {endpoint.instance_type}\")\n",
    "print(f\"  Cost: ${endpoint.cost_per_hour:.2f}/hour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Platform Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare platforms\n",
    "import json\n",
    "\n",
    "comparison = compare_platforms(\"Qwen/Qwen3-8B-Instruct\")\n",
    "\n",
    "print(\"Platform Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for platform, info in comparison[\"platforms\"].items():\n",
    "    print(f\"\\n{platform.upper()}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Instance: {info['instance']}\")\n",
    "    print(f\"  Hourly cost: ${info['hourly_cost']:.2f}\")\n",
    "    print(f\"  Setup: {info['setup_complexity']}\")\n",
    "    print(f\"  Auto-scaling: {info['auto_scaling']}\")\n",
    "    print(f\"  Cold start: {info['cold_start']}\")\n",
    "    print(f\"  \\n  Pros:\")\n",
    "    for pro in info['pros'][:2]:\n",
    "        print(f\"    + {pro}\")\n",
    "    print(f\"  Cons:\")\n",
    "    for con in info['cons'][:2]:\n",
    "        print(f\"    - {con}\")\n",
    "\n",
    "print(f\"\\n Recommendation:\")\n",
    "print(comparison['recommendation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side cost comparison\n",
    "print(\"\\nCost Comparison for 7B Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "scenarios = [\n",
    "    {\"name\": \"Light (1K req/day)\", \"requests\": 1000},\n",
    "    {\"name\": \"Medium (10K req/day)\", \"requests\": 10000},\n",
    "    {\"name\": \"Heavy (100K req/day)\", \"requests\": 100000},\n",
    "]\n",
    "\n",
    "print(f\"{'Scenario':<25} {'SageMaker':>12} {'Vertex AI':>12} {'Winner':>10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for scenario in scenarios:\n",
    "    estimates = estimate_cloud_costs(\n",
    "        model_size_gb=14.0,\n",
    "        expected_requests_per_day=scenario['requests'],\n",
    "        avg_latency_ms=150,\n",
    "    )\n",
    "    \n",
    "    sm_cost = estimates[0].monthly_cost\n",
    "    vertex_cost = estimates[1].monthly_cost\n",
    "    winner = \"SageMaker\" if sm_cost < vertex_cost else \"Vertex AI\"\n",
    "    \n",
    "    print(f\"{scenario['name']:<25} ${sm_cost:>10,.0f} ${vertex_cost:>10,.0f} {winner:>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Custom Container Requirements\n",
    "\n",
    "Vertex AI has specific requirements for custom containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertex AI container requirements\n",
    "vertex_requirements = '''\n",
    "# Vertex AI Custom Container Requirements\n",
    "# ========================================\n",
    "\n",
    "## Required Endpoints\n",
    "\n",
    "1. Health Check: GET /health or /ping\n",
    "   - Must return 200 when ready\n",
    "   \n",
    "2. Prediction: POST /predict\n",
    "   - Input: {\"instances\": [{...}, {...}]}\n",
    "   - Output: {\"predictions\": [{...}, {...}]}\n",
    "\n",
    "## Environment Variables (Auto-set)\n",
    "\n",
    "- AIP_STORAGE_URI: GCS path to model artifacts\n",
    "- AIP_HTTP_PORT: Port to listen on (default: 8080)\n",
    "- AIP_HEALTH_ROUTE: Health check route\n",
    "- AIP_PREDICT_ROUTE: Prediction route\n",
    "\n",
    "## Dockerfile Template\n",
    "\n",
    "FROM nvcr.io/nvidia/pytorch:24.12-py3\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY app/ /app/\n",
    "\n",
    "# Vertex AI expects port 8080 by default\n",
    "ENV AIP_HTTP_PORT=8080\n",
    "ENV AIP_HEALTH_ROUTE=/health\n",
    "ENV AIP_PREDICT_ROUTE=/predict\n",
    "\n",
    "EXPOSE 8080\n",
    "\n",
    "CMD [\"python\", \"-m\", \"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n",
    "'''\n",
    "\n",
    "print(vertex_requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example FastAPI app for Vertex AI\n",
    "vertex_app_code = '''\n",
    "# FastAPI Application for Vertex AI\n",
    "# ==================================\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Model loading (happens once on startup)\n",
    "@app.on_event(\"startup\")\n",
    "async def load_model():\n",
    "    global model, tokenizer\n",
    "    \n",
    "    # Vertex AI provides model path via environment\n",
    "    model_path = os.environ.get(\"AIP_STORAGE_URI\", \"/models\")\n",
    "    \n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "# Health check (required)\n",
    "@app.get(\"/health\")\n",
    "@app.get(\"/ping\")  # Alternative route\n",
    "async def health():\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "# Prediction request format\n",
    "class PredictionRequest(BaseModel):\n",
    "    instances: List[Dict[str, Any]]\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    predictions: List[Dict[str, Any]]\n",
    "\n",
    "# Prediction endpoint (required)\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "async def predict(request: PredictionRequest):\n",
    "    predictions = []\n",
    "    \n",
    "    for instance in request.instances:\n",
    "        prompt = instance.get(\"prompt\", \"\")\n",
    "        max_tokens = instance.get(\"max_tokens\", 100)\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_tokens)\n",
    "        text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        predictions.append({\"generated_text\": text})\n",
    "    \n",
    "    return PredictionResponse(predictions=predictions)\n",
    "'''\n",
    "\n",
    "print(vertex_app_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Deployment Workflow\n",
    "\n",
    "Complete workflow for deploying to Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Vertex AI deployment workflow\n",
    "deployment_workflow = '''\n",
    "# Vertex AI Deployment Workflow\n",
    "# ==============================\n",
    "\n",
    "# 1. Build and push container to Artifact Registry\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Enable APIs\n",
    "gcloud services enable artifactregistry.googleapis.com\n",
    "gcloud services enable aiplatform.googleapis.com\n",
    "\n",
    "# Create repository\n",
    "gcloud artifacts repositories create ml-models \\\\\n",
    "    --repository-format=docker \\\\\n",
    "    --location=us-central1\n",
    "\n",
    "# Configure Docker\n",
    "gcloud auth configure-docker us-central1-docker.pkg.dev\n",
    "\n",
    "# Build and push\n",
    "docker build -t us-central1-docker.pkg.dev/PROJECT/ml-models/inference:v1 .\n",
    "docker push us-central1-docker.pkg.dev/PROJECT/ml-models/inference:v1\n",
    "\n",
    "# 2. Upload model to GCS\n",
    "# ----------------------\n",
    "\n",
    "# Create bucket\n",
    "gsutil mb -l us-central1 gs://my-models-bucket\n",
    "\n",
    "# Upload model files\n",
    "gsutil -m cp -r ./model/* gs://my-models-bucket/llama-7b/\n",
    "\n",
    "# 3. Deploy using Python SDK\n",
    "# --------------------------\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=\"my-project\", location=\"us-central1\")\n",
    "\n",
    "# Upload model to registry\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=\"llama-7b\",\n",
    "    artifact_uri=\"gs://my-models-bucket/llama-7b\",\n",
    "    serving_container_image_uri=\"us-central1-docker.pkg.dev/PROJECT/ml-models/inference:v1\",\n",
    "    serving_container_predict_route=\"/predict\",\n",
    "    serving_container_health_route=\"/health\",\n",
    ")\n",
    "\n",
    "# Deploy to endpoint\n",
    "endpoint = model.deploy(\n",
    "    machine_type=\"n1-standard-8\",\n",
    "    accelerator_type=\"NVIDIA_L4\",\n",
    "    accelerator_count=1,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=3,\n",
    ")\n",
    "\n",
    "# 4. Test the endpoint\n",
    "# --------------------\n",
    "\n",
    "response = endpoint.predict(\n",
    "    instances=[{\"prompt\": \"Hello, how are you?\", \"max_tokens\": 50}]\n",
    ")\n",
    "print(response.predictions)\n",
    "'''\n",
    "\n",
    "print(deployment_workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Wrong Port Configuration\n",
    "\n",
    "```dockerfile\n",
    "# BAD - Vertex AI expects 8080 by default\n",
    "EXPOSE 8000\n",
    "CMD [\"uvicorn\", \"main:app\", \"--port\", \"8000\"]\n",
    "\n",
    "# GOOD - Use AIP_HTTP_PORT\n",
    "ENV AIP_HTTP_PORT=8080\n",
    "EXPOSE 8080\n",
    "CMD [\"uvicorn\", \"main:app\", \"--port\", \"8080\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 2: Wrong Request/Response Format\n",
    "\n",
    "```python\n",
    "# BAD - Vertex AI expects specific format\n",
    "@app.post(\"/predict\")\n",
    "def predict(prompt: str):  # Wrong input format\n",
    "    return {\"text\": \"...\"}  # Wrong output format\n",
    "\n",
    "# GOOD - Use instances/predictions format\n",
    "@app.post(\"/predict\")\n",
    "def predict(request: dict):  # {\"instances\": [...]}\n",
    "    return {\"predictions\": [...]}  # Required format\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 3: Forgetting Model Artifacts Path\n",
    "\n",
    "```python\n",
    "# BAD - Hardcoded path\n",
    "model = load_model(\"/models/llama-7b\")\n",
    "\n",
    "# GOOD - Use environment variable\n",
    "model_path = os.environ.get(\"AIP_STORAGE_URI\", \"/models\")\n",
    "model = load_model(model_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- Vertex AI architecture and components\n",
    "- Custom container requirements\n",
    "- GPU machine types and pricing\n",
    "- Platform comparison with SageMaker\n",
    "- Complete deployment workflow\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "Create a multi-region deployment:\n",
    "1. Deploy to us-central1 (primary)\n",
    "2. Deploy to europe-west1 (DR)\n",
    "3. Implement traffic splitting\n",
    "4. Create monitoring dashboard\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Vertex AI Documentation](https://cloud.google.com/vertex-ai/docs)\n",
    "- [Custom Containers Guide](https://cloud.google.com/vertex-ai/docs/predictions/use-custom-container)\n",
    "- [Vertex AI Pricing](https://cloud.google.com/vertex-ai/pricing)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup commands\n",
    "print(\"Vertex AI Cleanup\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n# Delete endpoint (undeploy model first)\")\n",
    "print(\"endpoint.undeploy_all()\")\n",
    "print(\"endpoint.delete()\")\n",
    "print(\"\\n# Delete model from registry\")\n",
    "print(\"model.delete()\")\n",
    "print(\"\\n# Via gcloud:\")\n",
    "print(\"gcloud ai endpoints delete ENDPOINT_ID --region=us-central1\")\n",
    "print(\"gcloud ai models delete MODEL_ID --region=us-central1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
