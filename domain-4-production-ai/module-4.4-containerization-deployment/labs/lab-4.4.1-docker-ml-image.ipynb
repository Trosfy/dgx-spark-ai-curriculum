{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.4.1: Building Optimized Docker Images for ML\n",
    "\n",
    "**Module:** 4.4 - Containerization & Cloud Deployment  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐⭐ (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- [ ] Understand Docker fundamentals and why containerization matters for ML\n",
    "- [ ] Create Dockerfiles extending NGC base images\n",
    "- [ ] Implement multi-stage builds for smaller image sizes\n",
    "- [ ] Configure GPU support in containers\n",
    "- [ ] Add health check endpoints for production readiness\n",
    "- [ ] Optimize Dockerfile layers for faster builds\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Docker installed with NVIDIA Container Toolkit\n",
    "- Basic understanding of Linux commands\n",
    "- Completed: Module 3.3 (Deployment basics)\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "**Why Docker for ML?**\n",
    "\n",
    "Imagine you've trained an amazing model on your DGX Spark. It works perfectly! But then:\n",
    "- Your colleague tries to run it and gets `ModuleNotFoundError`\n",
    "- You deploy to production and PyTorch version mismatch breaks everything\n",
    "- Six months later, your own code doesn't work because dependencies changed\n",
    "\n",
    "**58% of organizations now use Kubernetes for AI workloads**, and containers are the foundation.\n",
    "\n",
    "Docker solves this by packaging your model, code, and EXACT environment into a portable container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: What is Docker?\n",
    "\n",
    "> **Imagine you're shipping a fish tank...**\n",
    ">\n",
    "> You could ship just the fish, but then the recipient needs to have the right tank, the right water temperature, the right filter, the right food...\n",
    ">\n",
    "> **Docker is like shipping the entire aquarium** - fish, water, tank, filter, heater - everything needed for the fish to be happy. It works exactly the same whether it's in your house or your friend's house.\n",
    ">\n",
    "> **In AI terms:**\n",
    "> - **Fish** = Your model\n",
    "> - **Water/Tank/Filter** = Python, PyTorch, CUDA, libraries\n",
    "> - **Aquarium** = Docker container\n",
    ">\n",
    "> The container includes everything your model needs to run, so it works the same everywhere!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Docker Fundamentals\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Term | Description | Analogy |\n",
    "|------|-------------|---------|\n",
    "| **Image** | A template with all files and config | Recipe |\n",
    "| **Container** | A running instance of an image | Dish made from recipe |\n",
    "| **Dockerfile** | Instructions to build an image | Recipe card |\n",
    "| **Layer** | Each instruction creates a cacheable layer | Recipe step |\n",
    "| **Registry** | Where images are stored (Docker Hub, NGC) | Cookbook library |\n",
    "\n",
    "### Why NVIDIA NGC Containers?\n",
    "\n",
    "NVIDIA's NGC (GPU Cloud) provides pre-optimized containers that:\n",
    "- Are **tested on NVIDIA hardware** (including DGX Spark's Blackwell GPU)\n",
    "- Have **correct CUDA/cuDNN versions** pre-installed\n",
    "- Support **ARM64 architecture** (DGX Spark uses ARM v9.2 CPUs)\n",
    "- Include **TensorRT, NCCL, and other optimizations**\n",
    "\n",
    "Using NGC containers saves hours of debugging CUDA compatibility issues!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's verify Docker is properly installed with GPU support\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def run_command(cmd, capture=True):\n",
    "    \"\"\"Run a shell command and return output.\"\"\"\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=capture, text=True)\n",
    "    if capture:\n",
    "        return result.stdout.strip(), result.returncode\n",
    "    return None, result.returncode\n",
    "\n",
    "# Check Docker installation\n",
    "print(\"=\" * 60)\n",
    "print(\"Docker Environment Check\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Docker version\n",
    "output, code = run_command(\"docker --version\")\n",
    "if code == 0:\n",
    "    print(f\"Docker: {output}\")\n",
    "else:\n",
    "    print(\"Docker NOT installed! Please install Docker first.\")\n",
    "\n",
    "# Docker Compose version\n",
    "output, code = run_command(\"docker compose version 2>/dev/null || docker-compose --version\")\n",
    "if code == 0:\n",
    "    print(f\"Docker Compose: {output}\")\n",
    "else:\n",
    "    print(\"Docker Compose: Not available\")\n",
    "\n",
    "# NVIDIA Container Toolkit\n",
    "output, code = run_command(\"nvidia-container-cli --version 2>/dev/null\")\n",
    "if code == 0:\n",
    "    print(f\"NVIDIA Container Toolkit: {output.split()[0] if output else 'Installed'}\")\n",
    "else:\n",
    "    print(\"NVIDIA Container Toolkit: Not installed (GPU support unavailable)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GPU access in Docker (this may take a moment to pull the image)\n",
    "print(\"Testing GPU access in Docker...\")\n",
    "print(\"(This may take a minute if the image isn't cached)\\n\")\n",
    "\n",
    "output, code = run_command(\n",
    "    \"docker run --rm --gpus all nvidia/cuda:12.6.0-base-ubuntu22.04 nvidia-smi --query-gpu=name,memory.total --format=csv\",\n",
    "    capture=True\n",
    ")\n",
    "\n",
    "if code == 0:\n",
    "    print(\"GPU accessible in Docker container!\")\n",
    "    print(\"\\nGPU Information:\")\n",
    "    print(output)\n",
    "else:\n",
    "    print(\"GPU access failed. Possible issues:\")\n",
    "    print(\"1. NVIDIA Container Toolkit not installed\")\n",
    "    print(\"2. Docker not configured for GPU\")\n",
    "    print(\"3. GPU drivers not properly installed\")\n",
    "    print(\"\\nTo install NVIDIA Container Toolkit:\")\n",
    "    print(\"  curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\")\n",
    "    print(\"  sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit\")\n",
    "    print(\"  sudo systemctl restart docker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding NGC Base Images\n",
    "\n",
    "### Available NGC Containers\n",
    "\n",
    "NVIDIA provides optimized containers for different ML frameworks:\n",
    "\n",
    "| Container | Use Case | ARM64 Support |\n",
    "|-----------|----------|---------------|\n",
    "| `nvcr.io/nvidia/pytorch:25.11-py3` | PyTorch training/inference | Full |\n",
    "| `nvcr.io/nvidia/tensorrt:25.01-py3` | Optimized inference | Full |\n",
    "| `nvcr.io/nvidia/tritonserver:25.01-py3` | Multi-model serving | Full |\n",
    "| `nvcr.io/nvidia/nemo:25.01` | NLP with NeMo | Full |\n",
    "\n",
    "### DGX Spark Considerations\n",
    "\n",
    "Your DGX Spark has:\n",
    "- **ARM64 architecture** (not x86!) - must use ARM-compatible containers\n",
    "- **128GB unified memory** - can run large models\n",
    "- **Blackwell GPU with FP4/FP8** - use latest containers for hardware support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine what's inside an NGC container\n",
    "# This shows you what you get \"for free\" with NGC base images\n",
    "\n",
    "print(\"Examining NGC PyTorch container contents...\")\n",
    "print(\"(This shows what's pre-installed in the base image)\\n\")\n",
    "\n",
    "# Check Python version\n",
    "output, _ = run_command(\n",
    "    \"docker run --rm nvcr.io/nvidia/pytorch:24.12-py3 python --version 2>&1 || echo 'Image not pulled'\"\n",
    ")\n",
    "print(f\"Python: {output}\")\n",
    "\n",
    "# Check PyTorch version\n",
    "output, _ = run_command(\n",
    "    \"docker run --rm nvcr.io/nvidia/pytorch:24.12-py3 python -c 'import torch; print(torch.__version__)' 2>&1 || echo 'Image not pulled'\"\n",
    ")\n",
    "print(f\"PyTorch: {output}\")\n",
    "\n",
    "# Check CUDA version\n",
    "output, _ = run_command(\n",
    "    \"docker run --rm nvcr.io/nvidia/pytorch:24.12-py3 nvcc --version 2>&1 | grep release || echo 'Image not pulled'\"\n",
    ")\n",
    "print(f\"CUDA: {output}\")\n",
    "\n",
    "print(\"\\n These are all pre-configured and tested by NVIDIA!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Your First ML Dockerfile\n",
    "\n",
    "Let's create a Dockerfile for an LLM inference server step by step.\n",
    "\n",
    "### Dockerfile Anatomy\n",
    "\n",
    "```dockerfile\n",
    "# 1. Start from a base image\n",
    "FROM nvcr.io/nvidia/pytorch:25.11-py3\n",
    "\n",
    "# 2. Install dependencies\n",
    "RUN pip install transformers accelerate\n",
    "\n",
    "# 3. Copy your code\n",
    "COPY app/ /app/\n",
    "\n",
    "# 4. Set the working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# 5. Define how to run\n",
    "CMD [\"python\", \"serve.py\"]\n",
    "```\n",
    "\n",
    "Each instruction creates a **layer** that gets cached. Order matters for build speed!"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Understanding Our Docker Utilities Module\n\nThis curriculum provides a `docker_utils` module to simplify Dockerfile creation. Let's understand what it offers before using it.\n\n### DockerImageBuilder Class\n\nThe `DockerImageBuilder` class provides a fluent API for generating Dockerfiles:\n\n| Method | Description | Example |\n|--------|-------------|---------|\n| `add_base(image)` | Set the base image | `add_base(\"nvcr.io/nvidia/pytorch:24.12-py3\")` |\n| `add_python_deps(list)` | Add Python packages | `add_python_deps([\"transformers\", \"fastapi\"])` |\n| `add_env(key, value)` | Set environment variable | `add_env(\"MODEL_PATH\", \"/models\")` |\n| `add_copy(src, dst)` | Copy files into image | `add_copy(\"app/\", \"/app/\")` |\n| `set_workdir(path)` | Set working directory | `set_workdir(\"/app\")` |\n| `expose(port)` | Expose a port | `expose(8000)` |\n| `add_healthcheck(path, port)` | Add health check | `add_healthcheck(\"/health\", port=8000)` |\n| `add_entrypoint(cmd)` | Set entrypoint command | `add_entrypoint(\"python main.py\")` |\n| `generate()` | Generate Dockerfile content | Returns string |\n| `save(path)` | Save to file | `save(\"./Dockerfile\")` |\n\n### Other Utility Functions\n\n| Function | Description |\n|----------|-------------|\n| `create_dockerignore(path)` | Create a .dockerignore file with ML-friendly defaults |\n| `optimize_dockerfile(path)` | Analyze a Dockerfile and suggest optimizations |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use our docker_utils to build a proper Dockerfile\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from scripts.docker_utils import DockerImageBuilder\n",
    "\n",
    "# Create a builder for an LLM inference server\n",
    "builder = DockerImageBuilder(\"llm-inference-server\", use_multistage=True)\n",
    "\n",
    "# Configure the image\n",
    "builder.add_base(\"nvcr.io/nvidia/pytorch:24.12-py3\")  # Using 24.12 for stability\n",
    "\n",
    "# Add Python dependencies (commonly used for LLM serving)\n",
    "builder.add_python_deps([\n",
    "    \"transformers>=4.37.0\",\n",
    "    \"accelerate>=0.25.0\",\n",
    "    \"bitsandbytes>=0.41.0\",\n",
    "    \"fastapi>=0.109.0\",\n",
    "    \"uvicorn>=0.27.0\",\n",
    "    \"pydantic>=2.0.0\",\n",
    "])\n",
    "\n",
    "# Add environment variables\n",
    "builder.add_env(\"MODEL_PATH\", \"/models\")\n",
    "builder.add_env(\"CUDA_VISIBLE_DEVICES\", \"0\")\n",
    "builder.add_env(\"TRANSFORMERS_CACHE\", \"/models/cache\")\n",
    "\n",
    "# Copy application code\n",
    "builder.add_copy(\"app/\", \"/app/\")\n",
    "\n",
    "# Set working directory\n",
    "builder.set_workdir(\"/app\")\n",
    "\n",
    "# Expose port for API\n",
    "builder.expose(8000)\n",
    "\n",
    "# Add health check (critical for Kubernetes!)\n",
    "builder.add_healthcheck(\"/health\", port=8000, interval=30, timeout=10)\n",
    "\n",
    "# Set the entrypoint\n",
    "builder.add_entrypoint(\"python -m uvicorn main:app --host 0.0.0.0 --port 8000\")\n",
    "\n",
    "# Generate the Dockerfile\n",
    "dockerfile_content = builder.generate()\n",
    "\n",
    "print(\"Generated Dockerfile:\")\n",
    "print(\"=\" * 60)\n",
    "print(dockerfile_content)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Happening Here?\n",
    "\n",
    "Let's break down the generated Dockerfile:\n",
    "\n",
    "1. **Multi-stage build**: We have two stages:\n",
    "   - `builder`: Installs dependencies (can be larger)\n",
    "   - `production`: Only copies what's needed (smaller final image)\n",
    "\n",
    "2. **`--user` flag**: Installs packages to `/root/.local` for easy copying\n",
    "\n",
    "3. **Health check**: Kubernetes/Docker will ping `/health` to verify the container is running\n",
    "\n",
    "4. **Environment variables**: Configure model path and GPU settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also create the .dockerignore file\n",
    "# This prevents unnecessary files from being copied into the container\n",
    "\n",
    "from scripts.docker_utils import create_dockerignore\n",
    "import os\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"../docker-examples/inference-server\", exist_ok=True)\n",
    "\n",
    "# Create .dockerignore\n",
    "dockerignore_path = create_dockerignore(\"../docker-examples/inference-server/.dockerignore\")\n",
    "\n",
    "# Show content\n",
    "with open(dockerignore_path) as f:\n",
    "    content = f.read()\n",
    "\n",
    "print(\".dockerignore content:\")\n",
    "print(\"=\" * 60)\n",
    "print(content)\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nSaved to: {dockerignore_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Creating the Inference Server Code\n",
    "\n",
    "Now let's create the actual Python code that will run inside our container.\n",
    "\n",
    "We'll build a simple FastAPI server that:\n",
    "1. Loads a language model\n",
    "2. Provides a `/predict` endpoint\n",
    "3. Has a `/health` endpoint for Kubernetes probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the application directory structure\n",
    "import os\n",
    "\n",
    "app_dir = \"../docker-examples/inference-server/app\"\n",
    "os.makedirs(app_dir, exist_ok=True)\n",
    "\n",
    "# Create main.py - the FastAPI application\n",
    "main_py = '''\"\"\"FastAPI LLM Inference Server.\n",
    "\n",
    "This server provides endpoints for:\n",
    "- Health checks (/health)\n",
    "- Text generation (/predict)\n",
    "- Chat completion (/chat)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from typing import Optional, List\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Global model reference\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "\n",
    "class GenerateRequest(BaseModel):\n",
    "    \"\"\"Request body for text generation.\"\"\"\n",
    "    prompt: str\n",
    "    max_tokens: int = 256\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.9\n",
    "\n",
    "\n",
    "class ChatMessage(BaseModel):\n",
    "    \"\"\"A single chat message.\"\"\"\n",
    "    role: str  # \"user\", \"assistant\", or \"system\"\n",
    "    content: str\n",
    "\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    \"\"\"Request body for chat completion.\"\"\"\n",
    "    messages: List[ChatMessage]\n",
    "    max_tokens: int = 256\n",
    "    temperature: float = 0.7\n",
    "\n",
    "\n",
    "class GenerateResponse(BaseModel):\n",
    "    \"\"\"Response from generation endpoints.\"\"\"\n",
    "    generated_text: str\n",
    "    tokens_generated: int\n",
    "    latency_ms: float\n",
    "\n",
    "\n",
    "class HealthResponse(BaseModel):\n",
    "    \"\"\"Health check response.\"\"\"\n",
    "    status: str\n",
    "    model_loaded: bool\n",
    "    gpu_available: bool\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Load the language model.\"\"\"\n",
    "    global model, tokenizer\n",
    "    \n",
    "    model_path = os.environ.get(\"MODEL_PATH\", \"/models\")\n",
    "    model_name = os.environ.get(\"MODEL_NAME\", \"gpt2\")  # Default to small model\n",
    "    \n",
    "    logger.info(f\"Loading model: {model_name}\")\n",
    "    \n",
    "    try:\n",
    "        import torch\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "        \n",
    "        # Check if local model exists\n",
    "        local_path = os.path.join(model_path, model_name)\n",
    "        if os.path.exists(local_path):\n",
    "            load_path = local_path\n",
    "        else:\n",
    "            load_path = model_name\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            load_path,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        \n",
    "        # Add pad token if missing\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Load model with GPU if available\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            load_path,\n",
    "            torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\" if device == \"cuda\" else None,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Model loaded successfully on {device}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load model: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    \"\"\"Lifespan handler for startup/shutdown.\"\"\"\n",
    "    # Startup\n",
    "    logger.info(\"Starting inference server...\")\n",
    "    load_model()\n",
    "    yield\n",
    "    # Shutdown\n",
    "    logger.info(\"Shutting down...\")\n",
    "\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"LLM Inference Server\",\n",
    "    description=\"A simple LLM inference server for DGX Spark\",\n",
    "    version=\"1.0.0\",\n",
    "    lifespan=lifespan,\n",
    ")\n",
    "\n",
    "\n",
    "@app.get(\"/health\", response_model=HealthResponse)\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint for Kubernetes probes.\"\"\"\n",
    "    import torch\n",
    "    \n",
    "    return HealthResponse(\n",
    "        status=\"healthy\" if model is not None else \"loading\",\n",
    "        model_loaded=model is not None,\n",
    "        gpu_available=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "\n",
    "@app.post(\"/predict\", response_model=GenerateResponse)\n",
    "async def generate(request: GenerateRequest):\n",
    "    \"\"\"Generate text from a prompt.\"\"\"\n",
    "    if model is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n",
    "    \n",
    "    import torch\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(\n",
    "            request.prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=2048,\n",
    "        )\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=request.max_tokens,\n",
    "                temperature=request.temperature,\n",
    "                top_p=request.top_p,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "        \n",
    "        # Decode output\n",
    "        generated_text = tokenizer.decode(\n",
    "            outputs[0][inputs[\"input_ids\"].shape[1]:],\n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "        \n",
    "        tokens_generated = outputs.shape[1] - inputs[\"input_ids\"].shape[1]\n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        return GenerateResponse(\n",
    "            generated_text=generated_text,\n",
    "            tokens_generated=tokens_generated,\n",
    "            latency_ms=latency_ms,\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Generation failed: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "\n",
    "@app.post(\"/chat\", response_model=GenerateResponse)\n",
    "async def chat(request: ChatRequest):\n",
    "    \"\"\"Chat completion endpoint.\"\"\"\n",
    "    if model is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n",
    "    \n",
    "    # Format messages as prompt\n",
    "    prompt_parts = []\n",
    "    for msg in request.messages:\n",
    "        if msg.role == \"system\":\n",
    "            prompt_parts.append(f\"System: {msg.content}\")\n",
    "        elif msg.role == \"user\":\n",
    "            prompt_parts.append(f\"User: {msg.content}\")\n",
    "        elif msg.role == \"assistant\":\n",
    "            prompt_parts.append(f\"Assistant: {msg.content}\")\n",
    "    \n",
    "    prompt_parts.append(\"Assistant:\")\n",
    "    prompt = \"\\\\n\".join(prompt_parts)\n",
    "    \n",
    "    # Use the generate endpoint\n",
    "    return await generate(GenerateRequest(\n",
    "        prompt=prompt,\n",
    "        max_tokens=request.max_tokens,\n",
    "        temperature=request.temperature,\n",
    "    ))\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    \"\"\"Root endpoint with API info.\"\"\"\n",
    "    return {\n",
    "        \"name\": \"LLM Inference Server\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"endpoints\": [\n",
    "            \"/health - Health check\",\n",
    "            \"/predict - Text generation\",\n",
    "            \"/chat - Chat completion\",\n",
    "            \"/docs - API documentation\",\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "'''\n",
    "\n",
    "# Save main.py\n",
    "with open(f\"{app_dir}/main.py\", \"w\") as f:\n",
    "    f.write(main_py)\n",
    "\n",
    "print(f\"Created: {app_dir}/main.py\")\n",
    "print(f\"Lines of code: {len(main_py.splitlines())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create requirements.txt\n",
    "requirements = \"\"\"# LLM Inference Server Dependencies\n",
    "# Optimized for DGX Spark (ARM64 + CUDA)\n",
    "\n",
    "# API Framework\n",
    "fastapi>=0.109.0\n",
    "uvicorn[standard]>=0.27.0\n",
    "pydantic>=2.0.0\n",
    "\n",
    "# ML Libraries (PyTorch is pre-installed in NGC container)\n",
    "transformers>=4.37.0\n",
    "accelerate>=0.25.0\n",
    "bitsandbytes>=0.41.0\n",
    "\n",
    "# Optional: For better performance\n",
    "# vllm  # Use for production - much faster than HF generate\n",
    "\"\"\"\n",
    "\n",
    "with open(\"../docker-examples/inference-server/requirements.txt\", \"w\") as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "print(\"Created: requirements.txt\")\n",
    "print(requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now save the Dockerfile\n",
    "builder.save(\"../docker-examples/inference-server/Dockerfile\")\n",
    "\n",
    "print(\"Complete project structure:\")\n",
    "print(\"=\" * 60)\n",
    "!ls -la ../docker-examples/inference-server/\n",
    "print(\"\\napp/ contents:\")\n",
    "!ls -la ../docker-examples/inference-server/app/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Understanding Layer Optimization\n",
    "\n",
    "### ELI5: Docker Layers\n",
    "\n",
    "> **Imagine building with LEGO...**\n",
    ">\n",
    "> Each LEGO layer you add stays in place. If you want to change a layer at the bottom, you have to remove all the layers above it.\n",
    ">\n",
    "> Docker works the same way. Each instruction creates a layer. When you rebuild:\n",
    "> - Unchanged layers are **reused from cache** (fast!)\n",
    "> - Changed layers and everything after must be **rebuilt** (slow!)\n",
    ">\n",
    "> **Best Practice:** Put things that change rarely (dependencies) at the TOP, and things that change often (your code) at the BOTTOM.\n",
    "\n",
    "### Layer Order Matters!\n",
    "\n",
    "```dockerfile\n",
    "# BAD - Code changes invalidate dependency cache\n",
    "COPY . /app\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "# GOOD - Dependencies only reinstall when requirements.txt changes\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt\n",
    "COPY . /app\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's analyze a Dockerfile for optimization opportunities\n",
    "from scripts.docker_utils import optimize_dockerfile\n",
    "\n",
    "# First, let's create a \"bad\" Dockerfile to analyze\n",
    "bad_dockerfile = \"\"\"FROM python:3.10\n",
    "\n",
    "COPY . /app\n",
    "WORKDIR /app\n",
    "\n",
    "RUN apt-get update\n",
    "RUN apt-get install -y gcc\n",
    "RUN pip install torch transformers\n",
    "RUN pip install fastapi uvicorn\n",
    "\n",
    "CMD [\"python\", \"main.py\"]\n",
    "\"\"\"\n",
    "\n",
    "# Save it temporarily\n",
    "with open(\"/tmp/bad_dockerfile\", \"w\") as f:\n",
    "    f.write(bad_dockerfile)\n",
    "\n",
    "# Analyze it\n",
    "suggestions = optimize_dockerfile(\"/tmp/bad_dockerfile\")\n",
    "\n",
    "print(\"Dockerfile Analysis\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nOriginal Dockerfile:\")\n",
    "print(bad_dockerfile)\n",
    "print(\"\\n Optimization Suggestions:\")\n",
    "print(\"-\" * 60)\n",
    "for i, suggestion in enumerate(suggestions, 1):\n",
    "    print(f\"\\n{i}. {suggestion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Building and Testing the Image\n",
    "\n",
    "Now let's build our Docker image and test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Docker image\n",
    "# Note: This may take several minutes the first time (downloading base image)\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "project_dir = \"../docker-examples/inference-server\"\n",
    "\n",
    "print(\"Building Docker image...\")\n",
    "print(\"This may take 5-10 minutes on first build.\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# For learning purposes, let's create a smaller test image first\n",
    "# using a simpler Dockerfile\n",
    "\n",
    "test_dockerfile = \"\"\"# Simple test Dockerfile\n",
    "FROM python:3.10-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install --no-cache-dir fastapi uvicorn\n",
    "\n",
    "# Copy app\n",
    "COPY app/main.py /app/\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=10s CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "EXPOSE 8000\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "\"\"\"\n",
    "\n",
    "# Save test Dockerfile\n",
    "with open(f\"{project_dir}/Dockerfile.test\", \"w\") as f:\n",
    "    f.write(test_dockerfile)\n",
    "\n",
    "print(\"Created simple test Dockerfile (without ML dependencies for quick testing)\")\n",
    "print(\"\\nTo build the full ML image, run:\")\n",
    "print(f\"  cd {os.path.abspath(project_dir)}\")\n",
    "print(\"  docker build -t llm-inference:latest .\")\n",
    "print(\"\\nTo build the test image:\")\n",
    "print(f\"  docker build -f Dockerfile.test -t llm-inference:test .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to actually build (uncomment to run):\n",
    "# This builds the lightweight test version\n",
    "\n",
    "# Build command\n",
    "build_cmd = f\"cd {os.path.abspath(project_dir)} && docker build -f Dockerfile.test -t llm-inference:test . 2>&1\"\n",
    "\n",
    "print(\"Building test image...\")\n",
    "print(\"Command:\", build_cmd.split(\" && \")[1])\n",
    "print()\n",
    "\n",
    "# Uncomment to actually build:\n",
    "# result = subprocess.run(build_cmd, shell=True, capture_output=True, text=True)\n",
    "# print(result.stdout)\n",
    "# if result.returncode != 0:\n",
    "#     print(\"Build errors:\", result.stderr)\n",
    "\n",
    "print(\"\\n Uncomment the lines above to actually build the image.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Using NGC Containers\n",
    "\n",
    "```dockerfile\n",
    "# BAD - Will have CUDA compatibility issues on ARM64\n",
    "FROM python:3.10\n",
    "RUN pip install torch\n",
    "\n",
    "# GOOD - Pre-configured for NVIDIA hardware\n",
    "FROM nvcr.io/nvidia/pytorch:24.12-py3\n",
    "```\n",
    "**Why:** pip-installed PyTorch doesn't work on DGX Spark's ARM64 + CUDA architecture.\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 2: Copying Everything Before Installing Dependencies\n",
    "\n",
    "```dockerfile\n",
    "# BAD - Any code change reinstalls all dependencies\n",
    "COPY . /app\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "# GOOD - Dependencies cached unless requirements.txt changes\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt\n",
    "COPY . /app\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 3: Running as Root\n",
    "\n",
    "```dockerfile\n",
    "# BAD - Security risk\n",
    "CMD [\"python\", \"app.py\"]\n",
    "\n",
    "# GOOD - Non-root user\n",
    "RUN useradd -m appuser\n",
    "USER appuser\n",
    "CMD [\"python\", \"app.py\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 4: Not Including Health Checks\n",
    "\n",
    "```dockerfile\n",
    "# BAD - Kubernetes doesn't know if container is healthy\n",
    "CMD [\"python\", \"app.py\"]\n",
    "\n",
    "# GOOD - Kubernetes can monitor health\n",
    "HEALTHCHECK --interval=30s CMD curl -f http://localhost:8000/health || exit 1\n",
    "CMD [\"python\", \"app.py\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself\n",
    "\n",
    "### Exercise 1: Optimize This Dockerfile\n",
    "\n",
    "The following Dockerfile has several issues. Identify and fix them:\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.10\n",
    "\n",
    "RUN apt-get update\n",
    "RUN apt-get install -y curl\n",
    "RUN apt-get install -y git\n",
    "\n",
    "COPY . /app\n",
    "WORKDIR /app\n",
    "\n",
    "RUN pip install torch\n",
    "RUN pip install transformers\n",
    "RUN pip install fastapi\n",
    "\n",
    "CMD python main.py\n",
    "```\n",
    "\n",
    "<details>\n",
    "<summary>Hint 1</summary>\n",
    "Combine RUN commands with && to reduce layers.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 2</summary>\n",
    "Add --no-cache-dir to pip install.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 3</summary>\n",
    "Use NGC base image instead of python:3.10.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your optimized Dockerfile here:\n",
    "optimized_dockerfile = \"\"\"\n",
    "# TODO: Write your optimized Dockerfile\n",
    "# Consider:\n",
    "# 1. Base image choice\n",
    "# 2. Layer combining\n",
    "# 3. Dependency ordering\n",
    "# 4. Cache optimization\n",
    "# 5. Health checks\n",
    "\"\"\"\n",
    "\n",
    "print(optimized_dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Create a Dockerfile for a RAG Application\n",
    "\n",
    "Create a Dockerfile for a RAG (Retrieval-Augmented Generation) application that needs:\n",
    "- ChromaDB for vector storage\n",
    "- Sentence Transformers for embeddings\n",
    "- FastAPI for the API\n",
    "- An LLM for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the DockerImageBuilder to create a RAG application Dockerfile\n",
    "from scripts.docker_utils import DockerImageBuilder\n",
    "\n",
    "# TODO: Complete this\n",
    "rag_builder = DockerImageBuilder(\"rag-server\")\n",
    "\n",
    "# Add your configuration here:\n",
    "# rag_builder.add_base(...)\n",
    "# rag_builder.add_python_deps([...])\n",
    "# etc.\n",
    "\n",
    "# Uncomment when ready:\n",
    "# print(rag_builder.generate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- Why Docker is essential for ML deployment (reproducibility, portability)\n",
    "- How to use NGC containers optimized for DGX Spark\n",
    "- Multi-stage builds for smaller images\n",
    "- Layer optimization for faster builds\n",
    "- Health checks for production readiness\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "Create a complete Docker image that:\n",
    "1. Uses NGC PyTorch base\n",
    "2. Includes vLLM for faster inference\n",
    "3. Has OpenTelemetry for observability\n",
    "4. Supports multiple models via environment variables\n",
    "5. Includes GPU memory monitoring endpoint\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Docker Best Practices for Python](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/)\n",
    "- [NVIDIA NGC Catalog](https://catalog.ngc.nvidia.com/containers)\n",
    "- [Multi-stage Builds](https://docs.docker.com/build/building/multi-stage/)\n",
    "- [Docker BuildKit](https://docs.docker.com/build/buildkit/)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up Docker resources (optional)\n",
    "print(\"To clean up Docker resources, run:\")\n",
    "print(\"  docker system prune -f          # Remove unused data\")\n",
    "print(\"  docker image prune -a -f        # Remove unused images\")\n",
    "print(\"  docker volume prune -f          # Remove unused volumes\")\n",
    "print(\"\\nTo remove the test image:\")\n",
    "print(\"  docker rmi llm-inference:test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}