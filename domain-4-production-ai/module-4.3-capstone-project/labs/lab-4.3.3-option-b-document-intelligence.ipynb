{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Option B: Multimodal Document Intelligence\n\n**Module:** 4.3 - Capstone Project (Domain 4: Production AI)\n**Time:** 35-45 hours total\n**Difficulty:** â­â­â­â­â­\n\n---\n\n## ðŸŽ¯ Project Overview\n\nBuild a system for understanding complex documents with:\n- **Document Ingestion** - PDFs, images, scanned documents\n- **Vision-Language Model** - Understand diagrams, charts, layouts\n- **Structured Extraction** - Pull data into structured formats\n- **Question Answering** - Answer questions about documents\n- **Export Pipeline** - Output to JSON, CSV, or databases\n\n---\n\n## ðŸŽ¯ Learning Objectives\n\nBy completing this project, you will:\n- [ ] Process PDFs and images for AI analysis\n- [ ] Use Vision-Language Models for document understanding\n- [ ] Extract structured information from unstructured documents\n- [ ] Build a multimodal RAG system\n- [ ] Create evaluation benchmarks for document AI\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒ Real-World Context\n",
    "\n",
    "Document intelligence is transforming industries:\n",
    "\n",
    "| Industry | Use Case | Impact |\n",
    "|----------|----------|--------|\n",
    "| **Finance** | Invoice processing, KYC | 80% faster processing |\n",
    "| **Healthcare** | Medical record extraction | Better patient care |\n",
    "| **Legal** | Contract analysis | Hoursâ†’Minutes for review |\n",
    "| **Research** | Paper analysis, data extraction | Accelerated discovery |\n",
    "\n",
    "Companies like AWS (Textract), Google (Document AI), and Microsoft (Form Recognizer) have built billion-dollar products around document intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ§’ ELI5: What is Document Intelligence?\n",
    "\n",
    "> **Imagine you're a librarian** who needs to organize a million books.\n",
    ">\n",
    "> A regular computer can only read the typed words. But what about:\n",
    "> - **Handwritten notes** in the margins?\n",
    "> - **Diagrams and charts** explaining concepts?\n",
    "> - **Tables** with important data?\n",
    "> - **Photos** included in the text?\n",
    ">\n",
    "> **Document Intelligence** is like giving a computer eyes AND understanding. It can:\n",
    "> 1. **See** the document like a photo\n",
    "> 2. **Read** all the text (printed or handwritten)\n",
    "> 3. **Understand** charts, diagrams, and layouts\n",
    "> 4. **Extract** the important information\n",
    "> 5. **Answer questions** about what it found\n",
    ">\n",
    "> It's the difference between a scanner (just copies) and a smart assistant (understands and helps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ—ï¸ System Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  Multimodal Document Intelligence                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚  â”‚   Document   â”‚â”€â”€â”€â–¶â”‚   Ingestion  â”‚â”€â”€â”€â–¶â”‚   Analysis   â”‚          â”‚\n",
    "â”‚  â”‚   Upload     â”‚    â”‚   Pipeline   â”‚    â”‚   Engine     â”‚          â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â”‚                              â”‚                   â”‚                  â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\n",
    "â”‚         â”‚                    â”‚                                      â”‚\n",
    "â”‚         â–¼                    â–¼                                      â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚  â”‚    OCR /     â”‚    â”‚    Vision    â”‚    â”‚  Structured  â”‚          â”‚\n",
    "â”‚  â”‚  Text Ext.   â”‚    â”‚   Language   â”‚    â”‚  Extraction  â”‚          â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    Model     â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚                  â”‚\n",
    "â”‚                                                  â–¼                  â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚  â”‚  Multimodal  â”‚â—€â”€â”€â”€â”‚     QA       â”‚â—€â”€â”€â”€â”‚   Export     â”‚          â”‚\n",
    "â”‚  â”‚     RAG      â”‚    â”‚   Engine     â”‚    â”‚   (JSON/CSV) â”‚          â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment & Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"ðŸš€ OPTION B: MULTIMODAL DOCUMENT INTELLIGENCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"\\nGPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'Not available'}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Vision-Language Model Options\n",
    "VLM_OPTIONS = {\n",
    "    \"llava-1.6-34b\": {\n",
    "        \"name\": \"LLaVA 1.6 34B\",\n",
    "        \"memory_bf16\": 68,\n",
    "        \"memory_int4\": 17,\n",
    "        \"strengths\": [\"General document understanding\", \"OCR capability\", \"Chart analysis\"],\n",
    "    },\n",
    "    \"qwen2-vl-72b\": {\n",
    "        \"name\": \"Qwen2-VL 72B\",\n",
    "        \"memory_bf16\": 144,\n",
    "        \"memory_int4\": 36,\n",
    "        \"strengths\": [\"Multilingual\", \"High resolution\", \"Document layout\"],\n",
    "    },\n",
    "    \"internvl2-26b\": {\n",
    "        \"name\": \"InternVL2 26B\",\n",
    "        \"memory_bf16\": 52,\n",
    "        \"memory_int4\": 13,\n",
    "        \"strengths\": [\"Fast inference\", \"Good for tables\", \"Multi-image\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ“¦ Vision-Language Model Options:\")\n",
    "for key, model in VLM_OPTIONS.items():\n",
    "    print(f\"\\n  {model['name']}\")\n",
    "    print(f\"    Memory (INT4): {model['memory_int4']} GB\")\n",
    "    print(f\"    Strengths: {', '.join(model['strengths'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Configuration\n",
    "\n",
    "PROJECT_NAME = \"doc-intelligence\"\n",
    "VLM_MODEL = \"llava-hf/llava-v1.6-34b-hf\"  # Good balance of quality and speed\n",
    "EMBEDDING_MODEL = \"BAAI/bge-m3\"\n",
    "\n",
    "# Document types to support\n",
    "SUPPORTED_FORMATS = [\".pdf\", \".png\", \".jpg\", \".jpeg\", \".tiff\", \".bmp\"]\n",
    "\n",
    "# Extraction schemas (define what to extract)\n",
    "EXTRACTION_SCHEMAS = {\n",
    "    \"invoice\": {\n",
    "        \"vendor_name\": \"string\",\n",
    "        \"invoice_number\": \"string\",\n",
    "        \"date\": \"date\",\n",
    "        \"total_amount\": \"number\",\n",
    "        \"line_items\": [{\"description\": \"string\", \"quantity\": \"number\", \"price\": \"number\"}],\n",
    "    },\n",
    "    \"research_paper\": {\n",
    "        \"title\": \"string\",\n",
    "        \"authors\": [\"string\"],\n",
    "        \"abstract\": \"string\",\n",
    "        \"key_findings\": [\"string\"],\n",
    "        \"methodology\": \"string\",\n",
    "    },\n",
    "    \"generic\": {\n",
    "        \"title\": \"string\",\n",
    "        \"content_summary\": \"string\",\n",
    "        \"key_entities\": [\"string\"],\n",
    "        \"tables\": [{\"description\": \"string\", \"data\": \"object\"}],\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"âœ… Configuration set\")\n",
    "print(f\"   VLM: {VLM_MODEL}\")\n",
    "print(f\"   Formats: {', '.join(SUPPORTED_FORMATS)}\")\n",
    "print(f\"   Schemas: {', '.join(EXTRACTION_SCHEMAS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Document Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Ingestion System\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "@dataclass\n",
    "class DocumentPage:\n",
    "    \"\"\"A single page from a document.\"\"\"\n",
    "    page_number: int\n",
    "    image: Image.Image\n",
    "    text: Optional[str] = None\n",
    "    width: int = 0\n",
    "    height: int = 0\n",
    "    \n",
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"A complete document with all pages.\"\"\"\n",
    "    path: str\n",
    "    pages: List[DocumentPage]\n",
    "    metadata: Dict[str, Any]\n",
    "    doc_type: str = \"generic\"\n",
    "\n",
    "class DocumentIngester:\n",
    "    \"\"\"\n",
    "    Ingests documents from various formats into a unified structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dpi: int = 150):\n",
    "        self.dpi = dpi\n",
    "        self._pdf_lib = None\n",
    "    \n",
    "    def ingest(self, path: Union[str, Path]) -> Document:\n",
    "        \"\"\"\n",
    "        Ingest a document from file path.\n",
    "        \n",
    "        Args:\n",
    "            path: Path to document file\n",
    "            \n",
    "        Returns:\n",
    "            Document object with all pages\n",
    "        \"\"\"\n",
    "        path = Path(path)\n",
    "        \n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"Document not found: {path}\")\n",
    "        \n",
    "        suffix = path.suffix.lower()\n",
    "        \n",
    "        if suffix == \".pdf\":\n",
    "            return self._ingest_pdf(path)\n",
    "        elif suffix in [\".png\", \".jpg\", \".jpeg\", \".tiff\", \".bmp\"]:\n",
    "            return self._ingest_image(path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported format: {suffix}\")\n",
    "    \n",
    "    def _ingest_pdf(self, path: Path) -> Document:\n",
    "        \"\"\"Ingest a PDF document.\"\"\"\n",
    "        try:\n",
    "            import fitz  # PyMuPDF\n",
    "        except ImportError:\n",
    "            print(\"âš ï¸ PyMuPDF not installed. Install with: pip install pymupdf\")\n",
    "            raise\n",
    "        \n",
    "        doc = fitz.open(str(path))\n",
    "        pages = []\n",
    "        \n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            \n",
    "            # Render to image\n",
    "            mat = fitz.Matrix(self.dpi / 72, self.dpi / 72)\n",
    "            pix = page.get_pixmap(matrix=mat)\n",
    "            img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "            \n",
    "            # Extract text\n",
    "            text = page.get_text()\n",
    "            \n",
    "            pages.append(DocumentPage(\n",
    "                page_number=page_num + 1,\n",
    "                image=img,\n",
    "                text=text,\n",
    "                width=pix.width,\n",
    "                height=pix.height,\n",
    "            ))\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "        return Document(\n",
    "            path=str(path),\n",
    "            pages=pages,\n",
    "            metadata={\n",
    "                \"filename\": path.name,\n",
    "                \"num_pages\": len(pages),\n",
    "                \"format\": \"pdf\",\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def _ingest_image(self, path: Path) -> Document:\n",
    "        \"\"\"Ingest an image document.\"\"\"\n",
    "        img = Image.open(path)\n",
    "        \n",
    "        # Convert to RGB if necessary\n",
    "        if img.mode != \"RGB\":\n",
    "            img = img.convert(\"RGB\")\n",
    "        \n",
    "        page = DocumentPage(\n",
    "            page_number=1,\n",
    "            image=img,\n",
    "            width=img.width,\n",
    "            height=img.height,\n",
    "        )\n",
    "        \n",
    "        return Document(\n",
    "            path=str(path),\n",
    "            pages=[page],\n",
    "            metadata={\n",
    "                \"filename\": path.name,\n",
    "                \"num_pages\": 1,\n",
    "                \"format\": path.suffix[1:],\n",
    "                \"size\": (img.width, img.height),\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(\"âœ… Document ingestion system ready\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  ingester = DocumentIngester()\")\n",
    "print(\"  doc = ingester.ingest('path/to/document.pdf')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Vision-Language Model Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision-Language Model Wrapper\n",
    "\n",
    "class VisionLanguageModel:\n",
    "    \"\"\"\n",
    "    Wrapper for Vision-Language Models.\n",
    "    \n",
    "    Supports multiple VLM backends with a unified interface.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"llava-hf/llava-v1.6-34b-hf\",\n",
    "        load_in_4bit: bool = True,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.load_in_4bit = load_in_4bit\n",
    "        self._model = None\n",
    "        self._processor = None\n",
    "    \n",
    "    def _load(self):\n",
    "        \"\"\"Lazy load the model.\"\"\"\n",
    "        if self._model is not None:\n",
    "            return\n",
    "        \n",
    "        from transformers import (\n",
    "            AutoModelForVision2Seq,\n",
    "            AutoProcessor,\n",
    "            BitsAndBytesConfig,\n",
    "        )\n",
    "        \n",
    "        print(f\"Loading VLM: {self.model_name}\")\n",
    "        \n",
    "        if self.load_in_4bit:\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "            )\n",
    "            self._model = AutoModelForVision2Seq.from_pretrained(\n",
    "                self.model_name,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "        else:\n",
    "            self._model = AutoModelForVision2Seq.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "        \n",
    "        self._processor = AutoProcessor.from_pretrained(self.model_name)\n",
    "        print(f\"âœ… VLM loaded\")\n",
    "    \n",
    "    def analyze_image(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        prompt: str,\n",
    "        max_new_tokens: int = 1024,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Analyze an image with a given prompt.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image to analyze\n",
    "            prompt: Question or instruction\n",
    "            max_new_tokens: Maximum response length\n",
    "            \n",
    "        Returns:\n",
    "            Model's response\n",
    "        \"\"\"\n",
    "        self._load()\n",
    "        \n",
    "        # Format prompt with image token\n",
    "        full_prompt = f\"<image>\\nUSER: {prompt}\\nASSISTANT:\"\n",
    "        \n",
    "        inputs = self._processor(\n",
    "            text=full_prompt,\n",
    "            images=image,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self._model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "            )\n",
    "        \n",
    "        response = self._processor.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract assistant response\n",
    "        if \"ASSISTANT:\" in response:\n",
    "            response = response.split(\"ASSISTANT:\")[-1].strip()\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def extract_text(self, image: Image.Image) -> str:\n",
    "        \"\"\"Extract all text from an image using OCR capability.\"\"\"\n",
    "        return self.analyze_image(\n",
    "            image,\n",
    "            \"Extract all the text from this image. Preserve the formatting as much as possible.\"\n",
    "        )\n",
    "    \n",
    "    def describe_image(self, image: Image.Image) -> str:\n",
    "        \"\"\"Get a detailed description of the image.\"\"\"\n",
    "        return self.analyze_image(\n",
    "            image,\n",
    "            \"Describe this image in detail. Include information about text, diagrams, charts, and layout.\"\n",
    "        )\n",
    "    \n",
    "    def extract_table(self, image: Image.Image) -> str:\n",
    "        \"\"\"Extract table data from an image.\"\"\"\n",
    "        return self.analyze_image(\n",
    "            image,\n",
    "            \"Extract the table data from this image. Output in markdown table format.\"\n",
    "        )\n",
    "\n",
    "print(\"âœ… VLM wrapper ready\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  vlm = VisionLanguageModel()\")\n",
    "print(\"  result = vlm.analyze_image(image, 'What is in this document?')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Structured Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured Extraction System\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "class StructuredExtractor:\n",
    "    \"\"\"\n",
    "    Extract structured information from documents using VLM.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vlm: VisionLanguageModel):\n",
    "        self.vlm = vlm\n",
    "    \n",
    "    def extract(\n",
    "        self,\n",
    "        document: Document,\n",
    "        schema: Dict[str, Any],\n",
    "        schema_name: str = \"extraction\",\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract structured data according to a schema.\n",
    "        \n",
    "        Args:\n",
    "            document: Document to extract from\n",
    "            schema: JSON schema defining fields to extract\n",
    "            schema_name: Name of the schema (for prompting)\n",
    "            \n",
    "        Returns:\n",
    "            Extracted data matching the schema\n",
    "        \"\"\"\n",
    "        # Build extraction prompt\n",
    "        schema_str = json.dumps(schema, indent=2)\n",
    "        \n",
    "        prompt = f\"\"\"Extract the following information from this document.\n",
    "\n",
    "Schema ({schema_name}):\n",
    "{schema_str}\n",
    "\n",
    "Return a JSON object matching this schema exactly. Use null for missing values.\n",
    "Only return the JSON, no other text.\"\"\"\n",
    "        \n",
    "        # Process first page (or combine pages for multi-page docs)\n",
    "        if len(document.pages) == 1:\n",
    "            response = self.vlm.analyze_image(document.pages[0].image, prompt)\n",
    "        else:\n",
    "            # For multi-page, extract from each and merge\n",
    "            all_extractions = []\n",
    "            for page in document.pages[:5]:  # Limit to first 5 pages\n",
    "                response = self.vlm.analyze_image(page.image, prompt)\n",
    "                try:\n",
    "                    extracted = self._parse_json(response)\n",
    "                    all_extractions.append(extracted)\n",
    "                except:\n",
    "                    continue\n",
    "            response = json.dumps(self._merge_extractions(all_extractions, schema))\n",
    "        \n",
    "        # Parse and validate response\n",
    "        try:\n",
    "            result = self._parse_json(response)\n",
    "            return self._validate_against_schema(result, schema)\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e), \"raw_response\": response}\n",
    "    \n",
    "    def _parse_json(self, text: str) -> Dict:\n",
    "        \"\"\"Extract JSON from text that might contain other content.\"\"\"\n",
    "        # Try direct parse\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Find JSON in text\n",
    "        json_match = re.search(r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}', text, re.DOTALL)\n",
    "        if json_match:\n",
    "            try:\n",
    "                return json.loads(json_match.group())\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        raise ValueError(f\"Could not parse JSON from response\")\n",
    "    \n",
    "    def _validate_against_schema(\n",
    "        self,\n",
    "        data: Dict,\n",
    "        schema: Dict\n",
    "    ) -> Dict:\n",
    "        \"\"\"Validate and coerce data to match schema.\"\"\"\n",
    "        result = {}\n",
    "        \n",
    "        for key, expected_type in schema.items():\n",
    "            if key in data:\n",
    "                result[key] = data[key]\n",
    "            else:\n",
    "                result[key] = None\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _merge_extractions(\n",
    "        self,\n",
    "        extractions: List[Dict],\n",
    "        schema: Dict\n",
    "    ) -> Dict:\n",
    "        \"\"\"Merge extractions from multiple pages.\"\"\"\n",
    "        result = {}\n",
    "        \n",
    "        for key in schema:\n",
    "            values = [e.get(key) for e in extractions if e.get(key) is not None]\n",
    "            if values:\n",
    "                # Take first non-null value (could be smarter)\n",
    "                result[key] = values[0]\n",
    "            else:\n",
    "                result[key] = None\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"âœ… Structured extraction system ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Multimodal RAG for Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multimodal RAG System\n",
    "\n",
    "class MultimodalRAG:\n",
    "    \"\"\"\n",
    "    RAG system that combines text and visual understanding.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vlm: VisionLanguageModel,\n",
    "        embedding_model: str = \"BAAI/bge-m3\",\n",
    "    ):\n",
    "        self.vlm = vlm\n",
    "        self.embedding_model_name = embedding_model\n",
    "        self._embedding_model = None\n",
    "        self.documents: List[Document] = []\n",
    "        self.page_descriptions: List[Dict] = []  # {doc_idx, page_num, description, embedding}\n",
    "        self.index = None\n",
    "    \n",
    "    def add_document(self, document: Document):\n",
    "        \"\"\"Add a document to the knowledge base.\"\"\"\n",
    "        doc_idx = len(self.documents)\n",
    "        self.documents.append(document)\n",
    "        \n",
    "        # Generate description for each page\n",
    "        print(f\"Processing {document.metadata['filename']}...\")\n",
    "        \n",
    "        for page in document.pages:\n",
    "            # Get visual description\n",
    "            description = self.vlm.describe_image(page.image)\n",
    "            \n",
    "            # Combine with extracted text\n",
    "            combined = f\"{description}\\n\\nExtracted text:\\n{page.text or 'No text extracted'}\"\n",
    "            \n",
    "            self.page_descriptions.append({\n",
    "                \"doc_idx\": doc_idx,\n",
    "                \"page_num\": page.page_number,\n",
    "                \"description\": combined,\n",
    "            })\n",
    "        \n",
    "        # Rebuild index\n",
    "        self._build_index()\n",
    "        print(f\"âœ… Added {len(document.pages)} pages from {document.metadata['filename']}\")\n",
    "    \n",
    "    def _build_index(self):\n",
    "        \"\"\"Build search index over page descriptions.\"\"\"\n",
    "        if not self.page_descriptions:\n",
    "            return\n",
    "        \n",
    "        # Load embedding model\n",
    "        if self._embedding_model is None:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            self._embedding_model = SentenceTransformer(self.embedding_model_name, device=\"cuda\")\n",
    "        \n",
    "        # Generate embeddings\n",
    "        texts = [p[\"description\"] for p in self.page_descriptions]\n",
    "        embeddings = self._embedding_model.encode(texts)\n",
    "        \n",
    "        # Build FAISS index\n",
    "        import faiss\n",
    "        import numpy as np\n",
    "        \n",
    "        embeddings_array = np.array(embeddings).astype('float32')\n",
    "        faiss.normalize_L2(embeddings_array)\n",
    "        \n",
    "        self.index = faiss.IndexFlatIP(embeddings_array.shape[1])\n",
    "        self.index.add(embeddings_array)\n",
    "    \n",
    "    def answer_question(\n",
    "        self,\n",
    "        question: str,\n",
    "        top_k: int = 3,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Answer a question using retrieved document context.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            top_k: Number of pages to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Answer with sources and confidence\n",
    "        \"\"\"\n",
    "        if self.index is None or not self.page_descriptions:\n",
    "            return {\"answer\": \"No documents loaded.\", \"sources\": []}\n",
    "        \n",
    "        # Retrieve relevant pages\n",
    "        query_embedding = self._embedding_model.encode([question])\n",
    "        query_array = np.array(query_embedding).astype('float32')\n",
    "        faiss.normalize_L2(query_array)\n",
    "        \n",
    "        scores, indices = self.index.search(query_array, top_k)\n",
    "        \n",
    "        # Get retrieved pages\n",
    "        retrieved = []\n",
    "        for idx, score in zip(indices[0], scores[0]):\n",
    "            if idx >= 0:\n",
    "                page_info = self.page_descriptions[idx]\n",
    "                doc = self.documents[page_info[\"doc_idx\"]]\n",
    "                page = doc.pages[page_info[\"page_num\"] - 1]\n",
    "                retrieved.append({\n",
    "                    \"page\": page,\n",
    "                    \"doc_name\": doc.metadata[\"filename\"],\n",
    "                    \"page_num\": page_info[\"page_num\"],\n",
    "                    \"score\": float(score),\n",
    "                    \"description\": page_info[\"description\"],\n",
    "                })\n",
    "        \n",
    "        # Use VLM to answer based on best matching page\n",
    "        if retrieved:\n",
    "            best_page = retrieved[0][\"page\"]\n",
    "            \n",
    "            prompt = f\"\"\"Based on this document page, answer the following question.\n",
    "            \n",
    "Question: {question}\n",
    "\n",
    "Provide a detailed answer. If the information is not in the document, say so.\"\"\"\n",
    "            \n",
    "            answer = self.vlm.analyze_image(best_page.image, prompt)\n",
    "        else:\n",
    "            answer = \"No relevant information found in the documents.\"\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": [\n",
    "                {\n",
    "                    \"document\": r[\"doc_name\"],\n",
    "                    \"page\": r[\"page_num\"],\n",
    "                    \"relevance\": r[\"score\"],\n",
    "                }\n",
    "                for r in retrieved\n",
    "            ],\n",
    "            \"question\": question,\n",
    "        }\n",
    "\n",
    "print(\"âœ… Multimodal RAG system ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Complete Pipeline & API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Intelligence Pipeline\n",
    "\n",
    "class DocumentIntelligencePipeline:\n",
    "    \"\"\"\n",
    "    Complete pipeline for document intelligence.\n",
    "    \n",
    "    Combines:\n",
    "    - Document ingestion\n",
    "    - VLM analysis\n",
    "    - Structured extraction\n",
    "    - Q&A with RAG\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vlm_model: str = \"llava-hf/llava-v1.6-34b-hf\"):\n",
    "        self.ingester = DocumentIngester()\n",
    "        self.vlm = VisionLanguageModel(vlm_model)\n",
    "        self.extractor = StructuredExtractor(self.vlm)\n",
    "        self.rag = MultimodalRAG(self.vlm)\n",
    "    \n",
    "    def process_document(\n",
    "        self,\n",
    "        path: str,\n",
    "        extract_schema: str = None,\n",
    "        add_to_rag: bool = True,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process a document through the full pipeline.\n",
    "        \n",
    "        Args:\n",
    "            path: Path to document\n",
    "            extract_schema: Schema name for extraction (or None)\n",
    "            add_to_rag: Whether to add to RAG index\n",
    "            \n",
    "        Returns:\n",
    "            Processing results\n",
    "        \"\"\"\n",
    "        results = {\"path\": path, \"status\": \"processing\"}\n",
    "        \n",
    "        # 1. Ingest\n",
    "        try:\n",
    "            document = self.ingester.ingest(path)\n",
    "            results[\"metadata\"] = document.metadata\n",
    "            results[\"num_pages\"] = len(document.pages)\n",
    "        except Exception as e:\n",
    "            results[\"status\"] = \"error\"\n",
    "            results[\"error\"] = f\"Ingestion failed: {e}\"\n",
    "            return results\n",
    "        \n",
    "        # 2. Extract structured data\n",
    "        if extract_schema and extract_schema in EXTRACTION_SCHEMAS:\n",
    "            schema = EXTRACTION_SCHEMAS[extract_schema]\n",
    "            results[\"extracted_data\"] = self.extractor.extract(\n",
    "                document, schema, extract_schema\n",
    "            )\n",
    "        \n",
    "        # 3. Add to RAG\n",
    "        if add_to_rag:\n",
    "            self.rag.add_document(document)\n",
    "            results[\"indexed\"] = True\n",
    "        \n",
    "        results[\"status\"] = \"complete\"\n",
    "        return results\n",
    "    \n",
    "    def ask(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Ask a question about indexed documents.\"\"\"\n",
    "        return self.rag.answer_question(question)\n",
    "    \n",
    "    def extract(\n",
    "        self,\n",
    "        path: str,\n",
    "        schema: Dict[str, Any]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Extract structured data from a document.\"\"\"\n",
    "        document = self.ingester.ingest(path)\n",
    "        return self.extractor.extract(document, schema)\n",
    "\n",
    "print(\"âœ… Document Intelligence Pipeline ready\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  pipeline = DocumentIntelligencePipeline()\")\n",
    "print(\"  pipeline.process_document('invoice.pdf', extract_schema='invoice')\")\n",
    "print(\"  pipeline.ask('What is the total amount?')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Checkpoint & Next Steps\n",
    "\n",
    "You now have the foundation for a Multimodal Document Intelligence system:\n",
    "\n",
    "- âœ… Document ingestion (PDF, images)\n",
    "- âœ… Vision-Language Model integration\n",
    "- âœ… Structured data extraction\n",
    "- âœ… Multimodal RAG for Q&A\n",
    "- âœ… Complete pipeline\n",
    "\n",
    "### Implementation Roadmap\n",
    "\n",
    "**Week 1-2:** Build document processing pipeline\n",
    "- [ ] Implement ingestion for all formats\n",
    "- [ ] Test VLM on various document types\n",
    "- [ ] Create extraction schemas for your domain\n",
    "\n",
    "**Week 3:** Build RAG and Q&A\n",
    "- [ ] Implement multimodal RAG\n",
    "- [ ] Add table extraction\n",
    "- [ ] Create API endpoints\n",
    "\n",
    "**Week 4:** Optimization\n",
    "- [ ] Optimize for batch processing\n",
    "- [ ] Add caching\n",
    "- [ ] Profile memory usage\n",
    "\n",
    "**Week 5-6:** Evaluation & Documentation\n",
    "- [ ] Create benchmark dataset\n",
    "- [ ] Measure accuracy\n",
    "- [ ] Write documentation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… Ready to build your Document Intelligence system!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}