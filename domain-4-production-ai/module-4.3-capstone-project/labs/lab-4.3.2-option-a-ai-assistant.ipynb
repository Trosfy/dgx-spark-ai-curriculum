{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option A: Domain-Specific AI Assistant\n",
    "\n",
    "**Module:** 16 - Capstone Project (Phase 4)\n",
    "**Time:** 35-45 hours total\n",
    "**Difficulty:** â­â­â­â­â­\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Project Overview\n",
    "\n",
    "Build a complete AI assistant specialized for a specific domain:\n",
    "- **Fine-tuned 70B LLM** using QLoRA\n",
    "- **RAG system** with domain knowledge base\n",
    "- **Custom tools** for domain-specific operations\n",
    "- **Production API** with streaming responses\n",
    "- **Evaluation framework** to measure quality\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By completing this project, you will:\n",
    "- [ ] Fine-tune a 70B parameter model with QLoRA on DGX Spark\n",
    "- [ ] Build a production-quality RAG system\n",
    "- [ ] Implement custom tools with proper error handling\n",
    "- [ ] Create a streaming API with FastAPI\n",
    "- [ ] Evaluate your assistant's performance systematically\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Prerequisites\n",
    "\n",
    "- Module 10: LLM Fine-tuning (QLoRA)\n",
    "- Module 13: AI Agents (RAG, tools)\n",
    "- Module 12: Deployment (FastAPI, streaming)\n",
    "- Module 11: Quantization (inference optimization)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒ Real-World Context\n",
    "\n",
    "Domain-specific AI assistants are transforming industries:\n",
    "\n",
    "| Domain | Example Use Cases | Value Created |\n",
    "|--------|------------------|---------------|\n",
    "| **DevOps/Cloud** | AWS CLI help, infrastructure advice | Faster deployments, fewer errors |\n",
    "| **Finance** | Trading analysis, compliance checking | Better decisions, reduced risk |\n",
    "| **Healthcare** | Medical literature search, drug interactions | Improved patient care |\n",
    "| **Legal** | Contract analysis, case research | Faster research, reduced costs |\n",
    "| **Code Review** | PR analysis, best practices | Higher code quality |\n",
    "\n",
    "Companies like Bloomberg (BloombergGPT), Harvey (legal AI), and Hippocratic AI have built specialized assistants that outperform general models in their domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ§’ ELI5: What is a Domain-Specific AI Assistant?\n",
    "\n",
    "> **Imagine you're in a foreign country and need help.** You could ask:\n",
    ">\n",
    "> 1. **A random tourist** - They might help, but don't know the area\n",
    "> 2. **A local guide** - They know the streets, restaurants, and customs\n",
    ">\n",
    "> **General AI models are like tourists.** They know a lot about everything, but nothing deeply.\n",
    ">\n",
    "> **Domain-specific assistants are like local guides.** They've been:\n",
    "> - **Trained** on domain knowledge (fine-tuning)\n",
    "> - **Given a reference book** (RAG knowledge base)\n",
    "> - **Equipped with tools** (custom functions)\n",
    ">\n",
    "> The result? An assistant that speaks your domain's language, knows its nuances, and can actually DO things in that domain - not just talk about them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ—ï¸ System Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     Domain-Specific AI Assistant                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚  â”‚  FastAPI     â”‚â”€â”€â”€â–¶â”‚  Orchestratorâ”‚â”€â”€â”€â–¶â”‚  Response    â”‚          â”‚\n",
    "â”‚  â”‚  Endpoint    â”‚    â”‚              â”‚    â”‚  Streamer    â”‚          â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â”‚                              â”‚                                      â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚\n",
    "â”‚         â–¼                    â–¼                    â–¼                 â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚  â”‚    RAG       â”‚    â”‚  Fine-tuned  â”‚    â”‚    Tool      â”‚          â”‚\n",
    "â”‚  â”‚  Retriever   â”‚    â”‚     LLM      â”‚    â”‚  Executor    â”‚          â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â”‚          â”‚                                       â”‚                  â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚  â”‚ Vector Store â”‚                        â”‚  Tool        â”‚          â”‚\n",
    "â”‚  â”‚ (FAISS)      â”‚                        â”‚  Registry    â”‚          â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â”‚          â”‚                                                          â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                                                   â”‚\n",
    "â”‚  â”‚  Knowledge   â”‚                                                   â”‚\n",
    "â”‚  â”‚  Base (Docs) â”‚                                                   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment Setup & Domain Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "print(\"ðŸš€ OPTION A: DOMAIN-SPECIFIC AI ASSISTANT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"\\nGPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'Not available'}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain Selection Helper\n",
    "\n",
    "DOMAIN_OPTIONS = {\n",
    "    \"aws\": {\n",
    "        \"name\": \"AWS Infrastructure Assistant\",\n",
    "        \"description\": \"Help with AWS CLI, services, and best practices\",\n",
    "        \"knowledge_sources\": [\n",
    "            \"AWS Documentation\",\n",
    "            \"AWS CLI Reference\",\n",
    "            \"AWS Best Practices Guides\",\n",
    "        ],\n",
    "        \"example_tools\": [\n",
    "            \"validate_cli_command\",\n",
    "            \"estimate_cost\",\n",
    "            \"check_security_group\",\n",
    "        ],\n",
    "        \"training_data_ideas\": [\n",
    "            \"Stack Overflow AWS questions\",\n",
    "            \"AWS re:Post discussions\",\n",
    "            \"Synthetic CLI command Q&A\",\n",
    "        ],\n",
    "    },\n",
    "    \"finance\": {\n",
    "        \"name\": \"Financial Analysis Assistant\",\n",
    "        \"description\": \"Help with market analysis, financial metrics, and reports\",\n",
    "        \"knowledge_sources\": [\n",
    "            \"SEC Filings (10-K, 10-Q)\",\n",
    "            \"Financial News Archives\",\n",
    "            \"Investment Research Reports\",\n",
    "        ],\n",
    "        \"example_tools\": [\n",
    "            \"calculate_ratios\",\n",
    "            \"fetch_stock_data\",\n",
    "            \"compare_companies\",\n",
    "        ],\n",
    "        \"training_data_ideas\": [\n",
    "            \"Financial Q&A datasets\",\n",
    "            \"Analyst report summaries\",\n",
    "            \"Earnings call transcripts\",\n",
    "        ],\n",
    "    },\n",
    "    \"code_review\": {\n",
    "        \"name\": \"Code Review Assistant\",\n",
    "        \"description\": \"Help review PRs, suggest improvements, check for bugs\",\n",
    "        \"knowledge_sources\": [\n",
    "            \"Language-specific style guides\",\n",
    "            \"Security best practices\",\n",
    "            \"Design patterns documentation\",\n",
    "        ],\n",
    "        \"example_tools\": [\n",
    "            \"run_linter\",\n",
    "            \"check_security\",\n",
    "            \"generate_tests\",\n",
    "        ],\n",
    "        \"training_data_ideas\": [\n",
    "            \"GitHub PR comments\",\n",
    "            \"Code review Q&A\",\n",
    "            \"Bug fix examples\",\n",
    "        ],\n",
    "    },\n",
    "    \"medical\": {\n",
    "        \"name\": \"Medical Literature Assistant\",\n",
    "        \"description\": \"Help search and summarize medical research\",\n",
    "        \"knowledge_sources\": [\n",
    "            \"PubMed Abstracts\",\n",
    "            \"Clinical Guidelines\",\n",
    "            \"Drug Databases\",\n",
    "        ],\n",
    "        \"example_tools\": [\n",
    "            \"search_pubmed\",\n",
    "            \"check_drug_interactions\",\n",
    "            \"summarize_study\",\n",
    "        ],\n",
    "        \"training_data_ideas\": [\n",
    "            \"Medical Q&A datasets\",\n",
    "            \"PubMed question-answer pairs\",\n",
    "            \"Clinical scenario responses\",\n",
    "        ],\n",
    "    },\n",
    "    \"custom\": {\n",
    "        \"name\": \"Custom Domain\",\n",
    "        \"description\": \"Define your own domain\",\n",
    "        \"knowledge_sources\": [\"Define your sources\"],\n",
    "        \"example_tools\": [\"Define your tools\"],\n",
    "        \"training_data_ideas\": [\"Define your data strategy\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"\\nðŸŽ¯ AVAILABLE DOMAINS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for key, domain in DOMAIN_OPTIONS.items():\n",
    "    print(f\"\\nðŸ“Œ {key}: {domain['name']}\")\n",
    "    print(f\"   {domain['description']}\")\n",
    "    print(f\"   Knowledge: {', '.join(domain['knowledge_sources'][:2])}...\")\n",
    "    print(f\"   Tools: {', '.join(domain['example_tools'][:2])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select your domain (modify this cell)\n",
    "\n",
    "# ========================================\n",
    "# CONFIGURE YOUR PROJECT HERE\n",
    "# ========================================\n",
    "\n",
    "SELECTED_DOMAIN = \"aws\"  # Options: aws, finance, code_review, medical, custom\n",
    "PROJECT_NAME = \"aws-assistant\"\n",
    "BASE_MODEL = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "\n",
    "# ========================================\n",
    "\n",
    "domain_config = DOMAIN_OPTIONS[SELECTED_DOMAIN]\n",
    "\n",
    "print(f\"\\nâœ… Project Configuration\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Project Name: {PROJECT_NAME}\")\n",
    "print(f\"Domain: {domain_config['name']}\")\n",
    "print(f\"Base Model: {BASE_MODEL}\")\n",
    "print(f\"\\nKnowledge Sources:\")\n",
    "for source in domain_config['knowledge_sources']:\n",
    "    print(f\"  â€¢ {source}\")\n",
    "print(f\"\\nPlanned Tools:\")\n",
    "for tool in domain_config['example_tools']:\n",
    "    print(f\"  â€¢ {tool}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Knowledge Base & RAG Setup\n",
    "\n",
    "The RAG system is the foundation of your assistant's domain expertise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG System Implementation Template\n",
    "\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"A document in the knowledge base.\"\"\"\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "    embedding: Optional[List[float]] = None\n",
    "    \n",
    "@dataclass\n",
    "class RetrievalResult:\n",
    "    \"\"\"Result from RAG retrieval.\"\"\"\n",
    "    documents: List[Document]\n",
    "    scores: List[float]\n",
    "    query: str\n",
    "    latency_ms: float\n",
    "\n",
    "class RAGSystem:\n",
    "    \"\"\"\n",
    "    RAG System for the domain-specific assistant.\n",
    "    \n",
    "    This is a template - implement the methods for your specific use case.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model: str = \"BAAI/bge-m3\",\n",
    "        chunk_size: int = 512,\n",
    "        chunk_overlap: int = 50,\n",
    "    ):\n",
    "        self.embedding_model_name = embedding_model\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.documents: List[Document] = []\n",
    "        self.index = None  # Will be FAISS index\n",
    "        self._embedding_model = None\n",
    "        \n",
    "    def _load_embedding_model(self):\n",
    "        \"\"\"Lazy load the embedding model.\"\"\"\n",
    "        if self._embedding_model is None:\n",
    "            try:\n",
    "                from sentence_transformers import SentenceTransformer\n",
    "                self._embedding_model = SentenceTransformer(\n",
    "                    self.embedding_model_name,\n",
    "                    device=\"cuda\"\n",
    "                )\n",
    "                print(f\"âœ… Loaded embedding model: {self.embedding_model_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Failed to load embedding model: {e}\")\n",
    "                raise\n",
    "        return self._embedding_model\n",
    "    \n",
    "    def add_documents(self, documents: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        Add documents to the knowledge base.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of {\"content\": str, \"metadata\": dict}\n",
    "        \"\"\"\n",
    "        import time\n",
    "        import numpy as np\n",
    "        \n",
    "        start = time.time()\n",
    "        model = self._load_embedding_model()\n",
    "        \n",
    "        # Chunk documents\n",
    "        chunks = []\n",
    "        for doc in documents:\n",
    "            doc_chunks = self._chunk_text(doc[\"content\"])\n",
    "            for i, chunk in enumerate(doc_chunks):\n",
    "                chunks.append({\n",
    "                    \"content\": chunk,\n",
    "                    \"metadata\": {**doc.get(\"metadata\", {}), \"chunk_id\": i}\n",
    "                })\n",
    "        \n",
    "        print(f\"ðŸ“„ Created {len(chunks)} chunks from {len(documents)} documents\")\n",
    "        \n",
    "        # Generate embeddings\n",
    "        contents = [c[\"content\"] for c in chunks]\n",
    "        embeddings = model.encode(contents, show_progress_bar=True)\n",
    "        \n",
    "        # Create Document objects\n",
    "        for chunk, embedding in zip(chunks, embeddings):\n",
    "            self.documents.append(Document(\n",
    "                content=chunk[\"content\"],\n",
    "                metadata=chunk[\"metadata\"],\n",
    "                embedding=embedding.tolist()\n",
    "            ))\n",
    "        \n",
    "        # Build FAISS index\n",
    "        self._build_index(embeddings)\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        print(f\"âœ… Added {len(chunks)} chunks in {elapsed:.1f}s\")\n",
    "    \n",
    "    def _chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(words), self.chunk_size - self.chunk_overlap):\n",
    "            chunk_words = words[i:i + self.chunk_size]\n",
    "            if len(chunk_words) > 50:  # Minimum chunk size\n",
    "                chunks.append(\" \".join(chunk_words))\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _build_index(self, embeddings):\n",
    "        \"\"\"Build FAISS index from embeddings.\"\"\"\n",
    "        try:\n",
    "            import faiss\n",
    "            import numpy as np\n",
    "            \n",
    "            embeddings_array = np.array(embeddings).astype('float32')\n",
    "            dim = embeddings_array.shape[1]\n",
    "            \n",
    "            # Use IVF for larger datasets\n",
    "            if len(embeddings) > 10000:\n",
    "                nlist = min(100, len(embeddings) // 100)\n",
    "                quantizer = faiss.IndexFlatIP(dim)\n",
    "                self.index = faiss.IndexIVFFlat(quantizer, dim, nlist)\n",
    "                self.index.train(embeddings_array)\n",
    "            else:\n",
    "                self.index = faiss.IndexFlatIP(dim)\n",
    "            \n",
    "            # Normalize for cosine similarity\n",
    "            faiss.normalize_L2(embeddings_array)\n",
    "            self.index.add(embeddings_array)\n",
    "            \n",
    "            print(f\"âœ… Built FAISS index with {self.index.ntotal} vectors\")\n",
    "        except ImportError:\n",
    "            print(\"âš ï¸ FAISS not installed. Install with: pip install faiss-gpu\")\n",
    "    \n",
    "    def retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 5,\n",
    "        score_threshold: float = 0.5\n",
    "    ) -> RetrievalResult:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            top_k: Number of documents to retrieve\n",
    "            score_threshold: Minimum relevance score\n",
    "            \n",
    "        Returns:\n",
    "            RetrievalResult with documents and scores\n",
    "        \"\"\"\n",
    "        import time\n",
    "        import numpy as np\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        if self.index is None:\n",
    "            return RetrievalResult([], [], query, 0)\n",
    "        \n",
    "        # Embed query\n",
    "        model = self._load_embedding_model()\n",
    "        query_embedding = model.encode([query])\n",
    "        \n",
    "        # Normalize\n",
    "        import faiss\n",
    "        query_array = np.array(query_embedding).astype('float32')\n",
    "        faiss.normalize_L2(query_array)\n",
    "        \n",
    "        # Search\n",
    "        scores, indices = self.index.search(query_array, top_k)\n",
    "        \n",
    "        # Filter by threshold and collect results\n",
    "        result_docs = []\n",
    "        result_scores = []\n",
    "        \n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx >= 0 and score >= score_threshold:\n",
    "                result_docs.append(self.documents[idx])\n",
    "                result_scores.append(float(score))\n",
    "        \n",
    "        elapsed_ms = (time.time() - start) * 1000\n",
    "        \n",
    "        return RetrievalResult(\n",
    "            documents=result_docs,\n",
    "            scores=result_scores,\n",
    "            query=query,\n",
    "            latency_ms=elapsed_ms\n",
    "        )\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save the RAG system to disk.\"\"\"\n",
    "        import faiss\n",
    "        import pickle\n",
    "        \n",
    "        path = Path(path)\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save FAISS index\n",
    "        if self.index:\n",
    "            faiss.write_index(self.index, str(path / \"index.faiss\"))\n",
    "        \n",
    "        # Save documents (without embeddings to save space)\n",
    "        docs_data = [\n",
    "            {\"content\": d.content, \"metadata\": d.metadata}\n",
    "            for d in self.documents\n",
    "        ]\n",
    "        with open(path / \"documents.json\", \"w\") as f:\n",
    "            json.dump(docs_data, f)\n",
    "        \n",
    "        print(f\"âœ… Saved RAG system to {path}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> \"RAGSystem\":\n",
    "        \"\"\"Load a RAG system from disk.\"\"\"\n",
    "        import faiss\n",
    "        \n",
    "        path = Path(path)\n",
    "        rag = cls()\n",
    "        \n",
    "        # Load index\n",
    "        index_path = path / \"index.faiss\"\n",
    "        if index_path.exists():\n",
    "            rag.index = faiss.read_index(str(index_path))\n",
    "        \n",
    "        # Load documents\n",
    "        docs_path = path / \"documents.json\"\n",
    "        if docs_path.exists():\n",
    "            with open(docs_path) as f:\n",
    "                docs_data = json.load(f)\n",
    "            rag.documents = [\n",
    "                Document(content=d[\"content\"], metadata=d[\"metadata\"])\n",
    "                for d in docs_data\n",
    "            ]\n",
    "        \n",
    "        print(f\"âœ… Loaded RAG system with {len(rag.documents)} documents\")\n",
    "        return rag\n",
    "\n",
    "print(\"âœ… RAG System template defined\")\n",
    "print(\"\\nTo use:\")\n",
    "print(\"  rag = RAGSystem()\")\n",
    "print(\"  rag.add_documents([{'content': '...', 'metadata': {...}}])\")\n",
    "print(\"  results = rag.retrieve('your question')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Fine-Tuning with QLoRA\n",
    "\n",
    "Fine-tune Llama 3.3 70B on your domain-specific data using QLoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA Fine-tuning Configuration Template\n",
    "\n",
    "LORA_CONFIG = {\n",
    "    # LoRA parameters\n",
    "    \"r\": 64,                    # LoRA rank (higher = more capacity, more memory)\n",
    "    \"lora_alpha\": 128,          # Scaling factor (usually 2*r)\n",
    "    \"lora_dropout\": 0.05,       # Dropout for regularization\n",
    "    \"target_modules\": [         # Modules to apply LoRA to\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "}\n",
    "\n",
    "QUANTIZATION_CONFIG = {\n",
    "    \"load_in_4bit\": True,\n",
    "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
    "    \"bnb_4bit_quant_type\": \"nf4\",\n",
    "    \"bnb_4bit_use_double_quant\": True,\n",
    "}\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 8,  # Effective batch size = 16\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"max_seq_length\": 2048,\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_steps\": 100,\n",
    "    \"bf16\": True,              # Use bfloat16 (native Blackwell support)\n",
    "    \"gradient_checkpointing\": True,  # Save memory\n",
    "}\n",
    "\n",
    "# Memory estimation\n",
    "def estimate_memory():\n",
    "    \"\"\"Estimate memory requirements for training.\"\"\"\n",
    "    base_model_4bit = 35  # GB\n",
    "    lora_params = (LORA_CONFIG[\"r\"] * 2 * len(LORA_CONFIG[\"target_modules\"]) * 8 * 8192) / 1e9  # GB\n",
    "    optimizer_states = lora_params * 8  # AdamW states\n",
    "    activations = TRAINING_CONFIG[\"per_device_train_batch_size\"] * TRAINING_CONFIG[\"max_seq_length\"] * 8192 * 4 / 1e9\n",
    "    \n",
    "    total = base_model_4bit + lora_params + optimizer_states + activations + 10  # +10 for overhead\n",
    "    \n",
    "    print(\"\\nðŸ’¾ MEMORY ESTIMATION\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Base Model (4-bit): ~{base_model_4bit:.1f} GB\")\n",
    "    print(f\"LoRA Parameters: ~{lora_params:.1f} GB\")\n",
    "    print(f\"Optimizer States: ~{optimizer_states:.1f} GB\")\n",
    "    print(f\"Activations: ~{activations:.1f} GB\")\n",
    "    print(f\"Overhead: ~10 GB\")\n",
    "    print(f\"-\"*50)\n",
    "    print(f\"Estimated Total: ~{total:.1f} GB\")\n",
    "    print(f\"DGX Spark Available: 128 GB\")\n",
    "    print(f\"Status: {'âœ… Fits!' if total < 120 else 'âš ï¸ Tight on memory'}\")\n",
    "\n",
    "estimate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation for Fine-tuning\n",
    "\n",
    "def prepare_training_data(examples: List[Dict[str, str]], tokenizer) -> Dict:\n",
    "    \"\"\"\n",
    "    Prepare training data in chat format.\n",
    "    \n",
    "    Args:\n",
    "        examples: List of {\"instruction\": ..., \"input\": ..., \"output\": ...}\n",
    "        tokenizer: The tokenizer to use\n",
    "        \n",
    "    Returns:\n",
    "        Tokenized dataset ready for training\n",
    "    \"\"\"\n",
    "    formatted = []\n",
    "    \n",
    "    for ex in examples:\n",
    "        # Format as chat\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": f\"You are a helpful {SELECTED_DOMAIN} assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": ex[\"instruction\"] + (f\"\\n\\nContext: {ex['input']}\" if ex.get(\"input\") else \"\")},\n",
    "            {\"role\": \"assistant\", \"content\": ex[\"output\"]}\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        formatted.append(text)\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "# Example training data format\n",
    "EXAMPLE_TRAINING_DATA = [\n",
    "    {\n",
    "        \"instruction\": \"How do I create an S3 bucket using the AWS CLI?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"\"\"To create an S3 bucket using the AWS CLI, use the following command:\n",
    "\n",
    "```bash\n",
    "aws s3 mb s3://your-bucket-name --region us-east-1\n",
    "```\n",
    "\n",
    "Key points:\n",
    "- `s3 mb` stands for \"S3 make bucket\"\n",
    "- Bucket names must be globally unique\n",
    "- Use lowercase letters, numbers, and hyphens only\n",
    "- Specify the region for data residency requirements\n",
    "\n",
    "For additional options like enabling versioning:\n",
    "```bash\n",
    "aws s3api put-bucket-versioning --bucket your-bucket-name --versioning-configuration Status=Enabled\n",
    "```\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What are the best practices for S3 bucket security?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"\"\"Here are essential S3 security best practices:\n",
    "\n",
    "1. **Block Public Access** (most important):\n",
    "   ```bash\n",
    "   aws s3api put-public-access-block --bucket your-bucket --public-access-block-configuration BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true\n",
    "   ```\n",
    "\n",
    "2. **Enable Default Encryption**:\n",
    "   ```bash\n",
    "   aws s3api put-bucket-encryption --bucket your-bucket --server-side-encryption-configuration '{\"Rules\":[{\"ApplyServerSideEncryptionByDefault\":{\"SSEAlgorithm\":\"AES256\"}}]}'\n",
    "   ```\n",
    "\n",
    "3. **Enable Versioning** for data protection\n",
    "4. **Use IAM Policies** with least privilege\n",
    "5. **Enable Access Logging** for audit trails\n",
    "6. **Consider S3 Object Lock** for compliance\"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"âœ… Training data template ready\")\n",
    "print(f\"\\nExample data format:\")\n",
    "print(json.dumps(EXAMPLE_TRAINING_DATA[0], indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning Script Template\n",
    "# This is a template - you'll run this as a separate script for training\n",
    "\n",
    "FINETUNE_SCRIPT = '''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "QLoRA Fine-tuning Script for Domain-Specific Assistant\n",
    "\n",
    "Usage:\n",
    "    python finetune.py --data_path data/training.jsonl --output_dir models/finetuned\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "import argparse\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_name\", default=\"meta-llama/Llama-3.3-70B-Instruct\")\n",
    "    parser.add_argument(\"--data_path\", required=True)\n",
    "    parser.add_argument(\"--output_dir\", required=True)\n",
    "    parser.add_argument(\"--max_steps\", type=int, default=500)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Quantization config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"Loading model: {args.model_name}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Prepare model for training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    # LoRA config\n",
    "    lora_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                       \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\"json\", data_files=args.data_path)[\"train\"]\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.03,\n",
    "        logging_steps=10,\n",
    "        save_steps=100,\n",
    "        bf16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        max_steps=args.max_steps,\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=2048,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save\n",
    "    trainer.save_model(args.output_dir)\n",
    "    print(f\"Model saved to {args.output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save the script\n",
    "script_path = Path(f\"/workspace/{PROJECT_NAME}/scripts\") if Path(\"/workspace\").exists() else Path(f\"./{PROJECT_NAME}/scripts\")\n",
    "script_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(script_path / \"finetune.py\", \"w\") as f:\n",
    "    f.write(FINETUNE_SCRIPT)\n",
    "\n",
    "print(f\"âœ… Fine-tuning script saved to: {script_path / 'finetune.py'}\")\n",
    "print(\"\\nTo run fine-tuning:\")\n",
    "print(f\"  python {script_path / 'finetune.py'} --data_path data/training.jsonl --output_dir models/finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Custom Tools Implementation\n",
    "\n",
    "Tools allow your assistant to perform actions, not just generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool Framework\n",
    "\n",
    "from typing import Callable, Dict, Any, List\n",
    "from dataclasses import dataclass\n",
    "import inspect\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class ToolDefinition:\n",
    "    \"\"\"Definition of a tool for the assistant.\"\"\"\n",
    "    name: str\n",
    "    description: str\n",
    "    parameters: Dict[str, Any]  # JSON Schema\n",
    "    function: Callable\n",
    "    \n",
    "    def to_openai_format(self) -> Dict:\n",
    "        \"\"\"Convert to OpenAI tool format.\"\"\"\n",
    "        return {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": self.name,\n",
    "                \"description\": self.description,\n",
    "                \"parameters\": self.parameters,\n",
    "            }\n",
    "        }\n",
    "\n",
    "class ToolRegistry:\n",
    "    \"\"\"Registry for managing available tools.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tools: Dict[str, ToolDefinition] = {}\n",
    "    \n",
    "    def register(self, tool: ToolDefinition):\n",
    "        \"\"\"Register a tool.\"\"\"\n",
    "        self.tools[tool.name] = tool\n",
    "        print(f\"âœ… Registered tool: {tool.name}\")\n",
    "    \n",
    "    def get(self, name: str) -> ToolDefinition:\n",
    "        \"\"\"Get a tool by name.\"\"\"\n",
    "        return self.tools.get(name)\n",
    "    \n",
    "    def execute(self, name: str, **kwargs) -> str:\n",
    "        \"\"\"Execute a tool with given arguments.\"\"\"\n",
    "        tool = self.get(name)\n",
    "        if not tool:\n",
    "            return f\"Error: Unknown tool '{name}'\"\n",
    "        \n",
    "        try:\n",
    "            result = tool.function(**kwargs)\n",
    "            return json.dumps(result) if not isinstance(result, str) else result\n",
    "        except Exception as e:\n",
    "            return f\"Error executing {name}: {str(e)}\"\n",
    "    \n",
    "    def get_all_definitions(self) -> List[Dict]:\n",
    "        \"\"\"Get all tool definitions in OpenAI format.\"\"\"\n",
    "        return [tool.to_openai_format() for tool in self.tools.values()]\n",
    "\n",
    "# Create registry\n",
    "registry = ToolRegistry()\n",
    "\n",
    "# Example tools for AWS domain\n",
    "def validate_cli_command(command: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Validate an AWS CLI command syntax.\n",
    "    \n",
    "    Args:\n",
    "        command: The AWS CLI command to validate\n",
    "        \n",
    "    Returns:\n",
    "        Validation result with any errors or warnings\n",
    "    \"\"\"\n",
    "    # Simple validation - in production, this would be more sophisticated\n",
    "    errors = []\n",
    "    warnings = []\n",
    "    \n",
    "    if not command.startswith(\"aws \"):\n",
    "        errors.append(\"Command should start with 'aws'\")\n",
    "    \n",
    "    # Check for common issues\n",
    "    if \"--region\" not in command and \"s3\" in command:\n",
    "        warnings.append(\"Consider specifying --region for S3 operations\")\n",
    "    \n",
    "    if \"rm\" in command and \"-r\" in command and \"--force\" not in command:\n",
    "        warnings.append(\"Recursive delete without --force - will prompt for confirmation\")\n",
    "    \n",
    "    return {\n",
    "        \"valid\": len(errors) == 0,\n",
    "        \"command\": command,\n",
    "        \"errors\": errors,\n",
    "        \"warnings\": warnings,\n",
    "    }\n",
    "\n",
    "registry.register(ToolDefinition(\n",
    "    name=\"validate_cli_command\",\n",
    "    description=\"Validate the syntax of an AWS CLI command and check for common issues\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"command\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The AWS CLI command to validate\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"command\"]\n",
    "    },\n",
    "    function=validate_cli_command\n",
    "))\n",
    "\n",
    "def estimate_cost(service: str, usage: Dict[str, float]) -> Dict:\n",
    "    \"\"\"\n",
    "    Estimate AWS service cost.\n",
    "    \n",
    "    Args:\n",
    "        service: AWS service name (e.g., \"s3\", \"ec2\")\n",
    "        usage: Usage metrics (e.g., {\"storage_gb\": 100, \"requests\": 1000000})\n",
    "    \"\"\"\n",
    "    # Simplified pricing - in production, use AWS Pricing API\n",
    "    pricing = {\n",
    "        \"s3\": {\n",
    "            \"storage_gb\": 0.023,  # $/GB/month\n",
    "            \"requests\": 0.0000004,  # $ per request\n",
    "        },\n",
    "        \"ec2\": {\n",
    "            \"hours\": 0.0416,  # t3.medium\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    service_pricing = pricing.get(service.lower(), {})\n",
    "    total = 0\n",
    "    breakdown = []\n",
    "    \n",
    "    for metric, value in usage.items():\n",
    "        if metric in service_pricing:\n",
    "            cost = value * service_pricing[metric]\n",
    "            total += cost\n",
    "            breakdown.append(f\"{metric}: ${cost:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        \"service\": service,\n",
    "        \"estimated_monthly_cost\": f\"${total:.2f}\",\n",
    "        \"breakdown\": breakdown,\n",
    "        \"note\": \"Estimates based on us-east-1 pricing. Actual costs may vary.\"\n",
    "    }\n",
    "\n",
    "registry.register(ToolDefinition(\n",
    "    name=\"estimate_cost\",\n",
    "    description=\"Estimate monthly AWS service costs based on usage\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"service\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"AWS service name (s3, ec2, etc.)\"\n",
    "            },\n",
    "            \"usage\": {\n",
    "                \"type\": \"object\",\n",
    "                \"description\": \"Usage metrics (e.g., {storage_gb: 100})\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"service\", \"usage\"]\n",
    "    },\n",
    "    function=estimate_cost\n",
    "))\n",
    "\n",
    "# Test tools\n",
    "print(\"\\nðŸ§ª Testing tools:\")\n",
    "print(registry.execute(\"validate_cli_command\", command=\"aws s3 ls s3://my-bucket\"))\n",
    "print(registry.execute(\"estimate_cost\", service=\"s3\", usage={\"storage_gb\": 100, \"requests\": 1000000}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Orchestrator & API\n",
    "\n",
    "The orchestrator ties everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orchestrator Template\n",
    "\n",
    "from typing import AsyncGenerator, Optional\n",
    "import asyncio\n",
    "\n",
    "class AssistantOrchestrator:\n",
    "    \"\"\"\n",
    "    Orchestrates RAG, LLM, and tools for the domain assistant.\n",
    "    \n",
    "    This coordinates:\n",
    "    1. Query understanding\n",
    "    2. RAG retrieval\n",
    "    3. LLM generation\n",
    "    4. Tool execution\n",
    "    5. Response streaming\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        rag_system: RAGSystem,\n",
    "        tool_registry: ToolRegistry,\n",
    "        model_name: str = \"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "        system_prompt: str = None,\n",
    "    ):\n",
    "        self.rag = rag_system\n",
    "        self.tools = tool_registry\n",
    "        self.model_name = model_name\n",
    "        self.system_prompt = system_prompt or self._default_system_prompt()\n",
    "        self._model = None\n",
    "        self._tokenizer = None\n",
    "    \n",
    "    def _default_system_prompt(self) -> str:\n",
    "        return f\"\"\"You are an expert {SELECTED_DOMAIN} assistant. Your role is to:\n",
    "\n",
    "1. Answer questions accurately using your knowledge and the provided context\n",
    "2. Use available tools when they can help provide better answers\n",
    "3. Be concise but thorough\n",
    "4. Cite sources when using retrieved information\n",
    "5. Admit when you're uncertain\n",
    "\n",
    "Available tools: {', '.join(self.tools.tools.keys())}\n",
    "\n",
    "When using tools, explain what you're doing and why.\"\"\"\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the fine-tuned model.\"\"\"\n",
    "        if self._model is None:\n",
    "            from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "            \n",
    "            print(f\"Loading model: {self.model_name}\")\n",
    "            \n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "            )\n",
    "            \n",
    "            self._model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "            self._tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            \n",
    "            print(f\"âœ… Model loaded\")\n",
    "    \n",
    "    def process_query(\n",
    "        self,\n",
    "        query: str,\n",
    "        conversation_history: List[Dict] = None,\n",
    "        use_rag: bool = True,\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Process a user query and generate a response.\n",
    "        \n",
    "        Args:\n",
    "            query: The user's question\n",
    "            conversation_history: Previous messages\n",
    "            use_rag: Whether to retrieve context\n",
    "            \n",
    "        Returns:\n",
    "            Response with answer, sources, and tool calls\n",
    "        \"\"\"\n",
    "        self._load_model()\n",
    "        \n",
    "        # Step 1: Retrieve context\n",
    "        context = \"\"\n",
    "        sources = []\n",
    "        if use_rag and self.rag.index is not None:\n",
    "            retrieval = self.rag.retrieve(query, top_k=3)\n",
    "            if retrieval.documents:\n",
    "                context = \"\\n\\n\".join([\n",
    "                    f\"[Source {i+1}]: {doc.content}\"\n",
    "                    for i, doc in enumerate(retrieval.documents)\n",
    "                ])\n",
    "                sources = [\n",
    "                    {\"content\": doc.content[:200], \"score\": score}\n",
    "                    for doc, score in zip(retrieval.documents, retrieval.scores)\n",
    "                ]\n",
    "        \n",
    "        # Step 2: Build prompt\n",
    "        messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "        \n",
    "        if conversation_history:\n",
    "            messages.extend(conversation_history)\n",
    "        \n",
    "        user_content = query\n",
    "        if context:\n",
    "            user_content = f\"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "        \n",
    "        messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "        \n",
    "        # Step 3: Generate response\n",
    "        prompt = self._tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        inputs = self._tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self._model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1024,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self._tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        response_text = self._tokenizer.decode(\n",
    "            outputs[0][inputs[\"input_ids\"].shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        # Step 4: Check for tool calls (simplified)\n",
    "        tool_results = []\n",
    "        # In production, parse response for tool call requests\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response_text,\n",
    "            \"sources\": sources,\n",
    "            \"tool_results\": tool_results,\n",
    "            \"query\": query,\n",
    "        }\n",
    "\n",
    "print(\"âœ… Orchestrator template defined\")\n",
    "print(\"\\nTo use:\")\n",
    "print(\"  orchestrator = AssistantOrchestrator(rag_system, tool_registry)\")\n",
    "print(\"  response = orchestrator.process_query('How do I create an S3 bucket?')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastAPI Server Template\n",
    "\n",
    "API_SERVER_CODE = '''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "FastAPI Server for Domain-Specific AI Assistant\n",
    "\n",
    "Usage:\n",
    "    uvicorn api_server:app --host 0.0.0.0 --port 8000\n",
    "\"\"\"\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.responses import StreamingResponse\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional, Dict, Any\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Domain AI Assistant API\",\n",
    "    description=\"API for the domain-specific AI assistant\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Pydantic models\n",
    "class Message(BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    messages: List[Message]\n",
    "    stream: bool = True\n",
    "    temperature: float = 0.7\n",
    "    max_tokens: int = 2048\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    id: str\n",
    "    message: Message\n",
    "    sources: List[Dict[str, Any]] = []\n",
    "    usage: Dict[str, int]\n",
    "\n",
    "# Global orchestrator (loaded on startup)\n",
    "orchestrator = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup():\n",
    "    \"\"\"Load models on startup.\"\"\"\n",
    "    global orchestrator\n",
    "    # Load your orchestrator here\n",
    "    # orchestrator = AssistantOrchestrator(...)\n",
    "    print(\"âœ… Assistant loaded\")\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    \"\"\"Health check endpoint.\"\"\"\n",
    "    import torch\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"gpu_available\": torch.cuda.is_available(),\n",
    "        \"gpu_memory_used_gb\": torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0,\n",
    "    }\n",
    "\n",
    "@app.post(\"/v1/chat/completions\")\n",
    "async def chat(request: ChatRequest):\n",
    "    \"\"\"\n",
    "    Chat endpoint compatible with OpenAI format.\n",
    "    \"\"\"\n",
    "    if orchestrator is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Assistant not loaded\")\n",
    "    \n",
    "    # Get the last user message\n",
    "    user_message = request.messages[-1].content\n",
    "    history = [m.dict() for m in request.messages[:-1]]\n",
    "    \n",
    "    if request.stream:\n",
    "        return StreamingResponse(\n",
    "            generate_stream(user_message, history),\n",
    "            media_type=\"text/event-stream\"\n",
    "        )\n",
    "    else:\n",
    "        result = orchestrator.process_query(user_message, history)\n",
    "        return ChatResponse(\n",
    "            id=\"chat-\" + str(hash(user_message))[:8],\n",
    "            message=Message(role=\"assistant\", content=result[\"answer\"]),\n",
    "            sources=result.get(\"sources\", []),\n",
    "            usage={\"prompt_tokens\": 0, \"completion_tokens\": 0}\n",
    "        )\n",
    "\n",
    "async def generate_stream(query: str, history: list):\n",
    "    \"\"\"Stream response tokens.\"\"\"\n",
    "    result = orchestrator.process_query(query, history)\n",
    "    \n",
    "    # Simulate streaming (in production, use model's streaming capability)\n",
    "    words = result[\"answer\"].split()\n",
    "    for i, word in enumerate(words):\n",
    "        chunk = {\n",
    "            \"id\": \"chatcmpl-\" + str(i),\n",
    "            \"object\": \"chat.completion.chunk\",\n",
    "            \"choices\": [{\n",
    "                \"delta\": {\"content\": word + \" \"},\n",
    "                \"index\": 0,\n",
    "                \"finish_reason\": None\n",
    "            }]\n",
    "        }\n",
    "        yield f\"data: {json.dumps(chunk)}\\\\n\\\\n\"\n",
    "        await asyncio.sleep(0.02)  # Simulate generation time\n",
    "    \n",
    "    yield \"data: [DONE]\\\\n\\\\n\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "'''\n",
    "\n",
    "# Save API server\n",
    "with open(script_path / \"api_server.py\", \"w\") as f:\n",
    "    f.write(API_SERVER_CODE)\n",
    "\n",
    "print(f\"âœ… API server saved to: {script_path / 'api_server.py'}\")\n",
    "print(\"\\nTo run:\")\n",
    "print(f\"  uvicorn api_server:app --host 0.0.0.0 --port 8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Evaluation Framework\n",
    "\n",
    "How will you measure your assistant's quality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Framework\n",
    "\n",
    "@dataclass\n",
    "class TestCase:\n",
    "    \"\"\"A test case for evaluation.\"\"\"\n",
    "    query: str\n",
    "    expected_answer: str  # Or key points that should be covered\n",
    "    category: str  # e.g., \"factual\", \"procedural\", \"troubleshooting\"\n",
    "    difficulty: str  # \"easy\", \"medium\", \"hard\"\n",
    "    required_tool: Optional[str] = None\n",
    "\n",
    "class AssistantEvaluator:\n",
    "    \"\"\"Evaluate assistant quality.\"\"\"\n",
    "    \n",
    "    def __init__(self, orchestrator: AssistantOrchestrator):\n",
    "        self.orchestrator = orchestrator\n",
    "        self.results = []\n",
    "    \n",
    "    def run_test_suite(self, test_cases: List[TestCase]) -> Dict:\n",
    "        \"\"\"\n",
    "        Run a test suite and collect metrics.\n",
    "        \"\"\"\n",
    "        import time\n",
    "        \n",
    "        results = {\n",
    "            \"total\": len(test_cases),\n",
    "            \"passed\": 0,\n",
    "            \"failed\": 0,\n",
    "            \"by_category\": {},\n",
    "            \"by_difficulty\": {},\n",
    "            \"latencies\": [],\n",
    "            \"details\": [],\n",
    "        }\n",
    "        \n",
    "        for tc in test_cases:\n",
    "            start = time.time()\n",
    "            \n",
    "            try:\n",
    "                response = self.orchestrator.process_query(tc.query)\n",
    "                latency = (time.time() - start) * 1000\n",
    "                \n",
    "                # Simple evaluation: check if key terms are present\n",
    "                passed = self._evaluate_response(response[\"answer\"], tc.expected_answer)\n",
    "                \n",
    "                results[\"latencies\"].append(latency)\n",
    "                \n",
    "                if passed:\n",
    "                    results[\"passed\"] += 1\n",
    "                else:\n",
    "                    results[\"failed\"] += 1\n",
    "                \n",
    "                # Track by category\n",
    "                if tc.category not in results[\"by_category\"]:\n",
    "                    results[\"by_category\"][tc.category] = {\"passed\": 0, \"total\": 0}\n",
    "                results[\"by_category\"][tc.category][\"total\"] += 1\n",
    "                if passed:\n",
    "                    results[\"by_category\"][tc.category][\"passed\"] += 1\n",
    "                \n",
    "                results[\"details\"].append({\n",
    "                    \"query\": tc.query,\n",
    "                    \"passed\": passed,\n",
    "                    \"latency_ms\": latency,\n",
    "                    \"category\": tc.category,\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                results[\"failed\"] += 1\n",
    "                results[\"details\"].append({\n",
    "                    \"query\": tc.query,\n",
    "                    \"passed\": False,\n",
    "                    \"error\": str(e),\n",
    "                })\n",
    "        \n",
    "        # Calculate summary metrics\n",
    "        results[\"accuracy\"] = results[\"passed\"] / results[\"total\"] if results[\"total\"] > 0 else 0\n",
    "        results[\"avg_latency_ms\"] = sum(results[\"latencies\"]) / len(results[\"latencies\"]) if results[\"latencies\"] else 0\n",
    "        results[\"p95_latency_ms\"] = sorted(results[\"latencies\"])[int(len(results[\"latencies\"]) * 0.95)] if results[\"latencies\"] else 0\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _evaluate_response(self, response: str, expected: str) -> bool:\n",
    "        \"\"\"\n",
    "        Evaluate if response covers expected content.\n",
    "        \n",
    "        This is a simple keyword-based evaluation.\n",
    "        In production, use LLM-as-judge or human evaluation.\n",
    "        \"\"\"\n",
    "        response_lower = response.lower()\n",
    "        \n",
    "        # Extract key terms from expected answer\n",
    "        key_terms = [term.strip().lower() for term in expected.split(\",\")]\n",
    "        \n",
    "        # Check how many key terms are present\n",
    "        matches = sum(1 for term in key_terms if term in response_lower)\n",
    "        \n",
    "        # Pass if >50% of key terms are present\n",
    "        return matches >= len(key_terms) * 0.5\n",
    "\n",
    "# Example test cases\n",
    "EXAMPLE_TEST_CASES = [\n",
    "    TestCase(\n",
    "        query=\"How do I create an S3 bucket?\",\n",
    "        expected_answer=\"aws, s3, mb, bucket, --region\",\n",
    "        category=\"procedural\",\n",
    "        difficulty=\"easy\"\n",
    "    ),\n",
    "    TestCase(\n",
    "        query=\"What's the difference between S3 Standard and Glacier?\",\n",
    "        expected_answer=\"storage class, retrieval, cost, archive, access\",\n",
    "        category=\"factual\",\n",
    "        difficulty=\"medium\"\n",
    "    ),\n",
    "    TestCase(\n",
    "        query=\"My Lambda function is timing out. How do I fix it?\",\n",
    "        expected_answer=\"timeout, memory, cold start, configuration, optimize\",\n",
    "        category=\"troubleshooting\",\n",
    "        difficulty=\"hard\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"âœ… Evaluation framework ready\")\n",
    "print(f\"\\nExample test cases: {len(EXAMPLE_TEST_CASES)}\")\n",
    "for tc in EXAMPLE_TEST_CASES:\n",
    "    print(f\"  â€¢ [{tc.difficulty}] {tc.category}: {tc.query[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš ï¸ Common Mistakes\n",
    "\n",
    "### Mistake 1: RAG Retrieval Too Narrow\n",
    "```python\n",
    "# âŒ Wrong: Only retrieve based on exact keywords\n",
    "results = search(query, method=\"keyword_match\")\n",
    "\n",
    "# âœ… Right: Use semantic search + reranking\n",
    "results = semantic_search(query, top_k=10)\n",
    "reranked = rerank(query, results, top_k=3)\n",
    "```\n",
    "\n",
    "### Mistake 2: No Tool Error Handling\n",
    "```python\n",
    "# âŒ Wrong: Assume tools always work\n",
    "result = tool.execute(args)\n",
    "return f\"Result: {result}\"\n",
    "\n",
    "# âœ… Right: Handle errors gracefully\n",
    "try:\n",
    "    result = tool.execute(args)\n",
    "    return f\"Result: {result}\"\n",
    "except ToolError as e:\n",
    "    return f\"I tried to use {tool.name} but encountered an error: {e}. Let me try another approach...\"\n",
    "```\n",
    "\n",
    "### Mistake 3: No Response Validation\n",
    "```python\n",
    "# âŒ Wrong: Return LLM output directly\n",
    "return model.generate(prompt)\n",
    "\n",
    "# âœ… Right: Validate and filter\n",
    "response = model.generate(prompt)\n",
    "if contains_hallucination(response, context):\n",
    "    response = regenerate_with_warning(prompt)\n",
    "if contains_sensitive_data(response):\n",
    "    response = redact(response)\n",
    "return response\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Checkpoint\n",
    "\n",
    "You now have templates for:\n",
    "- âœ… RAG system with FAISS index\n",
    "- âœ… QLoRA fine-tuning configuration\n",
    "- âœ… Custom tool framework\n",
    "- âœ… Orchestrator for coordination\n",
    "- âœ… FastAPI server with streaming\n",
    "- âœ… Evaluation framework\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Implementation Roadmap\n",
    "\n",
    "### Week 1: Foundation\n",
    "- [ ] Collect and preprocess domain documents\n",
    "- [ ] Build and test RAG system\n",
    "- [ ] Create training dataset (100+ examples)\n",
    "\n",
    "### Week 2: Model Development\n",
    "- [ ] Fine-tune base model with QLoRA\n",
    "- [ ] Implement domain-specific tools\n",
    "- [ ] Basic integration testing\n",
    "\n",
    "### Week 3: Integration\n",
    "- [ ] Build orchestrator\n",
    "- [ ] Create FastAPI server\n",
    "- [ ] End-to-end testing\n",
    "\n",
    "### Week 4: Optimization\n",
    "- [ ] Profile and optimize memory usage\n",
    "- [ ] Improve retrieval quality\n",
    "- [ ] Add caching where appropriate\n",
    "\n",
    "### Week 5: Evaluation\n",
    "- [ ] Run comprehensive test suite\n",
    "- [ ] Compare to baselines\n",
    "- [ ] Iterate based on results\n",
    "\n",
    "### Week 6: Documentation\n",
    "- [ ] Complete technical report\n",
    "- [ ] Record demo video\n",
    "- [ ] Prepare presentation\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“– Further Reading\n",
    "\n",
    "- [RAG Best Practices](https://www.pinecone.io/learn/retrieval-augmented-generation/)\n",
    "- [QLoRA Paper](https://arxiv.org/abs/2305.14314)\n",
    "- [Building Production LLM Applications](https://huyenchip.com/2023/04/11/llm-engineering.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§¹ Cleanup\n",
    "import gc\n",
    "\n",
    "# Clear any loaded models\n",
    "if 'model' in dir():\n",
    "    del model\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… Cleanup complete\")\n",
    "print(f\"\\nGPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB allocated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
