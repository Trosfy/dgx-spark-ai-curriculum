{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Option D: Custom Training Pipeline\n\n**Module:** 4.3 - Capstone Project (Domain 4: Production AI)\n**Time:** 35-45 hours total\n**Difficulty:** â­â­â­â­â­\n\n---\n\n## ðŸŽ¯ Project Overview\n\nBuild infrastructure for continuous model improvement:\n- **Data Pipeline** - Collection, curation, preprocessing\n- **Training System** - SFT, DPO, RLHF support\n- **Evaluation Framework** - Automated benchmarking\n- **Model Registry** - Versioning and comparison\n- **Deployment Automation** - A/B testing ready\n\n---\n\n## ðŸŽ¯ Learning Objectives\n\nBy completing this project, you will:\n- [ ] Build a complete data curation pipeline\n- [ ] Implement multiple training approaches (SFT, DPO)\n- [ ] Create automated evaluation systems\n- [ ] Build model versioning and registry\n- [ ] Implement deployment with A/B testing\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒ Real-World Context\n",
    "\n",
    "Training pipelines are the backbone of AI companies:\n",
    "\n",
    "| Company | Pipeline Focus | Scale |\n",
    "|---------|---------------|-------|\n",
    "| **OpenAI** | RLHF, Constitutional AI | Millions of samples |\n",
    "| **Anthropic** | CAI, Preference Learning | Safety-focused |\n",
    "| **Meta** | SFT + DPO | Open-source models |\n",
    "| **Google** | Multi-task, Instruction tuning | Massive scale |\n",
    "\n",
    "Building your own pipeline gives you control over:\n",
    "- Data quality and bias\n",
    "- Training efficiency\n",
    "- Model behavior\n",
    "- Continuous improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ§’ ELI5: What is a Training Pipeline?\n",
    "\n",
    "> **Imagine you're training a new employee.** You don't just throw them into work:\n",
    ">\n",
    "> 1. **Gather training materials** (data collection)\n",
    "> 2. **Organize the content** (data curation)\n",
    "> 3. **Conduct training sessions** (model training)\n",
    "> 4. **Give quizzes to check progress** (evaluation)\n",
    "> 5. **Promote good performers** (model selection)\n",
    "> 6. **Deploy to their role** (serving)\n",
    "> 7. **Get feedback and retrain** (continuous improvement)\n",
    ">\n",
    "> **A training pipeline automates this entire process** for AI models. Feed in data, get out improved models - automatically, reproducibly, at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ—ï¸ System Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Custom Training Pipeline                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚  â”‚   Data     â”‚â”€â”€â–¶â”‚   Data     â”‚â”€â”€â–¶â”‚  Training  â”‚â”€â”€â–¶â”‚   Model    â”‚ â”‚\n",
    "â”‚  â”‚ Collection â”‚   â”‚  Curation  â”‚   â”‚   Loop     â”‚   â”‚  Registry  â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â”‚                                          â”‚                â”‚        â”‚\n",
    "â”‚                                          â–¼                â–¼        â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚      Evaluation Suite      â”‚â—€â”€â”€â”€â”‚   Model    â”‚â”€â”€â–¶â”‚  Deploy  â”‚  â”‚\n",
    "â”‚  â”‚ â€¢ Benchmarks               â”‚    â”‚ Comparison â”‚   â”‚  A/B     â”‚  â”‚\n",
    "â”‚  â”‚ â€¢ Safety Checks            â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  Testing â”‚  â”‚\n",
    "â”‚  â”‚ â€¢ Quality Metrics          â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚  â”‚                    Experiment Tracking                      â”‚    â”‚\n",
    "â”‚  â”‚              (MLflow / Weights & Biases)                   â”‚    â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pipeline Foundation\n",
    "\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional, Iterator\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "import json\n",
    "import hashlib\n",
    "\n",
    "print(\"ðŸš€ OPTION D: CUSTOM TRAINING PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"\\nGPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'Not available'}\")\n",
    "\n",
    "@dataclass\n",
    "class TrainingSample:\n",
    "    \"\"\"A single training sample.\"\"\"\n",
    "    id: str\n",
    "    instruction: str\n",
    "    input: str = \"\"\n",
    "    output: str = \"\"\n",
    "    system_prompt: str = \"\"\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"id\": self.id,\n",
    "            \"instruction\": self.instruction,\n",
    "            \"input\": self.input,\n",
    "            \"output\": self.output,\n",
    "            \"system_prompt\": self.system_prompt,\n",
    "            \"metadata\": self.metadata,\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data: Dict) -> \"TrainingSample\":\n",
    "        return cls(\n",
    "            id=data.get(\"id\", hashlib.md5(data[\"instruction\"].encode()).hexdigest()[:8]),\n",
    "            instruction=data[\"instruction\"],\n",
    "            input=data.get(\"input\", \"\"),\n",
    "            output=data.get(\"output\", \"\"),\n",
    "            system_prompt=data.get(\"system_prompt\", \"\"),\n",
    "            metadata=data.get(\"metadata\", {}),\n",
    "        )\n",
    "\n",
    "@dataclass\n",
    "class PreferencePair:\n",
    "    \"\"\"A preference pair for DPO training.\"\"\"\n",
    "    id: str\n",
    "    instruction: str\n",
    "    input: str = \"\"\n",
    "    chosen: str = \"\"  # Preferred response\n",
    "    rejected: str = \"\"  # Non-preferred response\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "class DataCollector:\n",
    "    \"\"\"\n",
    "    Collects training data from various sources.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str = \"./data/collected\"):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.samples: List[TrainingSample] = []\n",
    "    \n",
    "    def add_sample(self, sample: TrainingSample):\n",
    "        \"\"\"Add a single sample.\"\"\"\n",
    "        self.samples.append(sample)\n",
    "    \n",
    "    def load_from_jsonl(self, path: str) -> int:\n",
    "        \"\"\"Load samples from JSONL file.\"\"\"\n",
    "        count = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                self.samples.append(TrainingSample.from_dict(data))\n",
    "                count += 1\n",
    "        print(f\"âœ… Loaded {count} samples from {path}\")\n",
    "        return count\n",
    "    \n",
    "    def load_from_hf_dataset(self, dataset_name: str, split: str = \"train\") -> int:\n",
    "        \"\"\"Load from Hugging Face datasets.\"\"\"\n",
    "        from datasets import load_dataset\n",
    "        \n",
    "        dataset = load_dataset(dataset_name, split=split)\n",
    "        count = 0\n",
    "        \n",
    "        for item in dataset:\n",
    "            # Handle various dataset formats\n",
    "            sample = TrainingSample(\n",
    "                id=item.get(\"id\", str(count)),\n",
    "                instruction=item.get(\"instruction\", item.get(\"question\", \"\")),\n",
    "                input=item.get(\"input\", item.get(\"context\", \"\")),\n",
    "                output=item.get(\"output\", item.get(\"answer\", \"\")),\n",
    "                metadata={\"source\": dataset_name}\n",
    "            )\n",
    "            self.samples.append(sample)\n",
    "            count += 1\n",
    "        \n",
    "        print(f\"âœ… Loaded {count} samples from {dataset_name}\")\n",
    "        return count\n",
    "    \n",
    "    def save(self, filename: str = \"collected.jsonl\"):\n",
    "        \"\"\"Save all collected samples.\"\"\"\n",
    "        output_path = self.output_dir / filename\n",
    "        with open(output_path, 'w') as f:\n",
    "            for sample in self.samples:\n",
    "                f.write(json.dumps(sample.to_dict()) + \"\\n\")\n",
    "        print(f\"âœ… Saved {len(self.samples)} samples to {output_path}\")\n",
    "\n",
    "print(\"\\nâœ… Data collection system defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Curation System\n",
    "\n",
    "class DataCurator:\n",
    "    \"\"\"\n",
    "    Curates and filters training data for quality.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.filters = []\n",
    "        self.transformations = []\n",
    "    \n",
    "    def add_filter(self, filter_fn, name: str = \"\"):\n",
    "        \"\"\"Add a filter function. Returns True to keep, False to discard.\"\"\"\n",
    "        self.filters.append((filter_fn, name))\n",
    "    \n",
    "    def add_transformation(self, transform_fn, name: str = \"\"):\n",
    "        \"\"\"Add a transformation function.\"\"\"\n",
    "        self.transformations.append((transform_fn, name))\n",
    "    \n",
    "    def curate(self, samples: List[TrainingSample]) -> List[TrainingSample]:\n",
    "        \"\"\"Apply all filters and transformations.\"\"\"\n",
    "        curated = []\n",
    "        filter_stats = {name: 0 for _, name in self.filters}\n",
    "        \n",
    "        for sample in samples:\n",
    "            # Apply filters\n",
    "            keep = True\n",
    "            for filter_fn, name in self.filters:\n",
    "                if not filter_fn(sample):\n",
    "                    filter_stats[name] += 1\n",
    "                    keep = False\n",
    "                    break\n",
    "            \n",
    "            if not keep:\n",
    "                continue\n",
    "            \n",
    "            # Apply transformations\n",
    "            for transform_fn, _ in self.transformations:\n",
    "                sample = transform_fn(sample)\n",
    "            \n",
    "            curated.append(sample)\n",
    "        \n",
    "        # Print stats\n",
    "        print(f\"\\nðŸ“Š Curation Stats:\")\n",
    "        print(f\"   Input: {len(samples)}\")\n",
    "        print(f\"   Output: {len(curated)}\")\n",
    "        for name, count in filter_stats.items():\n",
    "            if count > 0:\n",
    "                print(f\"   Filtered by {name}: {count}\")\n",
    "        \n",
    "        return curated\n",
    "\n",
    "# Common filters\n",
    "def min_length_filter(min_len: int = 10):\n",
    "    \"\"\"Filter samples with output shorter than min_len.\"\"\"\n",
    "    def filter_fn(sample: TrainingSample) -> bool:\n",
    "        return len(sample.output) >= min_len\n",
    "    return filter_fn\n",
    "\n",
    "def max_length_filter(max_len: int = 4096):\n",
    "    \"\"\"Filter samples with total length exceeding max_len.\"\"\"\n",
    "    def filter_fn(sample: TrainingSample) -> bool:\n",
    "        total = len(sample.instruction) + len(sample.input) + len(sample.output)\n",
    "        return total <= max_len\n",
    "    return filter_fn\n",
    "\n",
    "def no_pii_filter():\n",
    "    \"\"\"Filter samples containing potential PII.\"\"\"\n",
    "    import re\n",
    "    pii_patterns = [\n",
    "        r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',  # Email\n",
    "        r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b',  # Phone\n",
    "        r'\\b\\d{3}-\\d{2}-\\d{4}\\b',  # SSN\n",
    "    ]\n",
    "    \n",
    "    def filter_fn(sample: TrainingSample) -> bool:\n",
    "        text = f\"{sample.instruction} {sample.input} {sample.output}\"\n",
    "        for pattern in pii_patterns:\n",
    "            if re.search(pattern, text):\n",
    "                return False\n",
    "        return True\n",
    "    return filter_fn\n",
    "\n",
    "def deduplication_filter(seen_hashes: set = None):\n",
    "    \"\"\"Filter duplicate samples.\"\"\"\n",
    "    if seen_hashes is None:\n",
    "        seen_hashes = set()\n",
    "    \n",
    "    def filter_fn(sample: TrainingSample) -> bool:\n",
    "        content = f\"{sample.instruction}{sample.output}\"\n",
    "        content_hash = hashlib.md5(content.encode()).hexdigest()\n",
    "        if content_hash in seen_hashes:\n",
    "            return False\n",
    "        seen_hashes.add(content_hash)\n",
    "        return True\n",
    "    return filter_fn\n",
    "\n",
    "print(\"âœ… Data curation system defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Training System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration for training runs.\"\"\"\n",
    "    # Model\n",
    "    base_model: str = \"meta-llama/Llama-3.3-8B-Instruct\"\n",
    "    \n",
    "    # LoRA\n",
    "    lora_r: int = 64\n",
    "    lora_alpha: int = 128\n",
    "    lora_dropout: float = 0.05\n",
    "    target_modules: List[str] = field(default_factory=lambda: [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ])\n",
    "    \n",
    "    # Training\n",
    "    training_method: str = \"sft\"  # sft, dpo\n",
    "    num_epochs: int = 3\n",
    "    batch_size: int = 4\n",
    "    gradient_accumulation: int = 4\n",
    "    learning_rate: float = 2e-4\n",
    "    max_seq_length: int = 2048\n",
    "    warmup_ratio: float = 0.03\n",
    "    \n",
    "    # DPO specific\n",
    "    dpo_beta: float = 0.1\n",
    "    \n",
    "    # Output\n",
    "    output_dir: str = \"./models/trained\"\n",
    "    run_name: str = \"\"\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"base_model\": self.base_model,\n",
    "            \"lora_r\": self.lora_r,\n",
    "            \"lora_alpha\": self.lora_alpha,\n",
    "            \"training_method\": self.training_method,\n",
    "            \"num_epochs\": self.num_epochs,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"max_seq_length\": self.max_seq_length,\n",
    "        }\n",
    "\n",
    "class TrainingPipeline:\n",
    "    \"\"\"\n",
    "    Unified training pipeline supporting SFT and DPO.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.run_id = f\"{config.run_name or config.training_method}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    def prepare_model(self):\n",
    "        \"\"\"Load and prepare model for training.\"\"\"\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "        from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "        \n",
    "        print(f\"\\nðŸ”„ Loading model: {self.config.base_model}\")\n",
    "        \n",
    "        # Quantization\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        \n",
    "        # Load model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.config.base_model,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.base_model)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Prepare for k-bit training\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "        \n",
    "        # Apply LoRA\n",
    "        lora_config = LoraConfig(\n",
    "            r=self.config.lora_r,\n",
    "            lora_alpha=self.config.lora_alpha,\n",
    "            lora_dropout=self.config.lora_dropout,\n",
    "            target_modules=self.config.target_modules,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        \n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        self.model.print_trainable_parameters()\n",
    "        \n",
    "        print(\"âœ… Model ready for training\")\n",
    "    \n",
    "    def train_sft(self, train_data: List[TrainingSample]):\n",
    "        \"\"\"Supervised Fine-Tuning.\"\"\"\n",
    "        from transformers import TrainingArguments\n",
    "        from trl import SFTTrainer\n",
    "        from datasets import Dataset\n",
    "        \n",
    "        print(f\"\\nðŸ‹ï¸ Starting SFT training with {len(train_data)} samples\")\n",
    "        \n",
    "        if self.model is None:\n",
    "            self.prepare_model()\n",
    "        \n",
    "        # Prepare dataset\n",
    "        def format_sample(sample: TrainingSample) -> str:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": sample.system_prompt or \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": sample.instruction + (f\"\\n\\n{sample.input}\" if sample.input else \"\")},\n",
    "                {\"role\": \"assistant\", \"content\": sample.output}\n",
    "            ]\n",
    "            return self.tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        \n",
    "        formatted = [format_sample(s) for s in train_data]\n",
    "        dataset = Dataset.from_dict({\"text\": formatted})\n",
    "        \n",
    "        # Training arguments\n",
    "        output_dir = f\"{self.config.output_dir}/{self.run_id}\"\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=self.config.num_epochs,\n",
    "            per_device_train_batch_size=self.config.batch_size,\n",
    "            gradient_accumulation_steps=self.config.gradient_accumulation,\n",
    "            learning_rate=self.config.learning_rate,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            warmup_ratio=self.config.warmup_ratio,\n",
    "            logging_steps=10,\n",
    "            save_steps=100,\n",
    "            bf16=True,\n",
    "            gradient_checkpointing=True,\n",
    "            report_to=[],  # Disable wandb by default\n",
    "        )\n",
    "        \n",
    "        # Trainer\n",
    "        trainer = SFTTrainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=self.config.max_seq_length,\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        trainer.train()\n",
    "        trainer.save_model(output_dir)\n",
    "        \n",
    "        print(f\"\\nâœ… Training complete. Model saved to {output_dir}\")\n",
    "        return output_dir\n",
    "    \n",
    "    def train_dpo(self, preference_data: List[PreferencePair]):\n",
    "        \"\"\"Direct Preference Optimization training.\"\"\"\n",
    "        from transformers import TrainingArguments\n",
    "        from trl import DPOTrainer\n",
    "        from datasets import Dataset\n",
    "        \n",
    "        print(f\"\\nðŸ‹ï¸ Starting DPO training with {len(preference_data)} pairs\")\n",
    "        \n",
    "        if self.model is None:\n",
    "            self.prepare_model()\n",
    "        \n",
    "        # Prepare dataset\n",
    "        dataset_dict = {\n",
    "            \"prompt\": [p.instruction + (f\"\\n\\n{p.input}\" if p.input else \"\") for p in preference_data],\n",
    "            \"chosen\": [p.chosen for p in preference_data],\n",
    "            \"rejected\": [p.rejected for p in preference_data],\n",
    "        }\n",
    "        dataset = Dataset.from_dict(dataset_dict)\n",
    "        \n",
    "        output_dir = f\"{self.config.output_dir}/{self.run_id}\"\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=self.config.num_epochs,\n",
    "            per_device_train_batch_size=self.config.batch_size // 2,  # DPO needs more memory\n",
    "            gradient_accumulation_steps=self.config.gradient_accumulation * 2,\n",
    "            learning_rate=self.config.learning_rate / 10,  # Lower LR for DPO\n",
    "            bf16=True,\n",
    "            gradient_checkpointing=True,\n",
    "            report_to=[],\n",
    "        )\n",
    "        \n",
    "        # DPO Trainer\n",
    "        trainer = DPOTrainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            beta=self.config.dpo_beta,\n",
    "            train_dataset=dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_seq_length,\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        trainer.save_model(output_dir)\n",
    "        \n",
    "        print(f\"\\nâœ… DPO training complete. Model saved to {output_dir}\")\n",
    "        return output_dir\n",
    "\n",
    "print(\"âœ… Training pipeline defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Evaluation & Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Registry\n",
    "\n",
    "@dataclass\n",
    "class ModelVersion:\n",
    "    \"\"\"A versioned model in the registry.\"\"\"\n",
    "    id: str\n",
    "    name: str\n",
    "    version: str\n",
    "    path: str\n",
    "    base_model: str\n",
    "    training_config: Dict\n",
    "    metrics: Dict[str, float]\n",
    "    created_at: datetime\n",
    "    status: str = \"active\"  # active, deprecated, archived\n",
    "    notes: str = \"\"\n",
    "\n",
    "class ModelRegistry:\n",
    "    \"\"\"\n",
    "    Registry for tracking model versions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, storage_path: str = \"./registry\"):\n",
    "        self.storage_path = Path(storage_path)\n",
    "        self.storage_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.models: Dict[str, List[ModelVersion]] = {}\n",
    "        self._load()\n",
    "    \n",
    "    def _load(self):\n",
    "        \"\"\"Load registry from disk.\"\"\"\n",
    "        registry_file = self.storage_path / \"registry.json\"\n",
    "        if registry_file.exists():\n",
    "            with open(registry_file) as f:\n",
    "                data = json.load(f)\n",
    "            for name, versions in data.items():\n",
    "                self.models[name] = [\n",
    "                    ModelVersion(\n",
    "                        **{**v, \"created_at\": datetime.fromisoformat(v[\"created_at\"])}\n",
    "                    ) for v in versions\n",
    "                ]\n",
    "    \n",
    "    def _save(self):\n",
    "        \"\"\"Save registry to disk.\"\"\"\n",
    "        registry_file = self.storage_path / \"registry.json\"\n",
    "        data = {}\n",
    "        for name, versions in self.models.items():\n",
    "            data[name] = [\n",
    "                {**v.__dict__, \"created_at\": v.created_at.isoformat()}\n",
    "                for v in versions\n",
    "            ]\n",
    "        with open(registry_file, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    \n",
    "    def register(\n",
    "        self,\n",
    "        name: str,\n",
    "        path: str,\n",
    "        base_model: str,\n",
    "        training_config: Dict,\n",
    "        metrics: Dict[str, float],\n",
    "        notes: str = \"\"\n",
    "    ) -> ModelVersion:\n",
    "        \"\"\"Register a new model version.\"\"\"\n",
    "        if name not in self.models:\n",
    "            self.models[name] = []\n",
    "        \n",
    "        version = f\"v{len(self.models[name]) + 1}\"\n",
    "        \n",
    "        model_version = ModelVersion(\n",
    "            id=f\"{name}_{version}\",\n",
    "            name=name,\n",
    "            version=version,\n",
    "            path=path,\n",
    "            base_model=base_model,\n",
    "            training_config=training_config,\n",
    "            metrics=metrics,\n",
    "            created_at=datetime.now(),\n",
    "            notes=notes,\n",
    "        )\n",
    "        \n",
    "        self.models[name].append(model_version)\n",
    "        self._save()\n",
    "        \n",
    "        print(f\"âœ… Registered {name} {version}\")\n",
    "        return model_version\n",
    "    \n",
    "    def get_latest(self, name: str) -> Optional[ModelVersion]:\n",
    "        \"\"\"Get the latest version of a model.\"\"\"\n",
    "        if name not in self.models or not self.models[name]:\n",
    "            return None\n",
    "        active = [m for m in self.models[name] if m.status == \"active\"]\n",
    "        return active[-1] if active else None\n",
    "    \n",
    "    def get_best(self, name: str, metric: str) -> Optional[ModelVersion]:\n",
    "        \"\"\"Get the best version by a specific metric.\"\"\"\n",
    "        if name not in self.models:\n",
    "            return None\n",
    "        \n",
    "        versions = [m for m in self.models[name] if metric in m.metrics and m.status == \"active\"]\n",
    "        if not versions:\n",
    "            return None\n",
    "        \n",
    "        return max(versions, key=lambda m: m.metrics[metric])\n",
    "    \n",
    "    def compare(self, name: str) -> str:\n",
    "        \"\"\"Compare all versions of a model.\"\"\"\n",
    "        if name not in self.models:\n",
    "            return f\"No model named {name}\"\n",
    "        \n",
    "        lines = [f\"\\nðŸ“Š Model Comparison: {name}\", \"=\"*60]\n",
    "        \n",
    "        for v in self.models[name]:\n",
    "            lines.append(f\"\\n{v.version} ({v.status})\")\n",
    "            lines.append(f\"  Created: {v.created_at.strftime('%Y-%m-%d %H:%M')}\")\n",
    "            lines.append(f\"  Metrics:\")\n",
    "            for metric, value in v.metrics.items():\n",
    "                lines.append(f\"    {metric}: {value:.4f}\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "print(\"âœ… Model registry defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Checkpoint & Next Steps\n",
    "\n",
    "You now have the foundation for a Custom Training Pipeline:\n",
    "\n",
    "- âœ… Data collection and curation\n",
    "- âœ… SFT and DPO training support\n",
    "- âœ… Model registry with versioning\n",
    "- âœ… Evaluation framework (basic)\n",
    "\n",
    "### Implementation Roadmap\n",
    "\n",
    "**Week 1-2:** Data pipeline\n",
    "- [ ] Add more data sources\n",
    "- [ ] Build quality scoring\n",
    "- [ ] Implement synthetic data generation\n",
    "\n",
    "**Week 3:** Training system\n",
    "- [ ] Add distributed training support\n",
    "- [ ] Implement curriculum learning\n",
    "- [ ] Add RLHF support\n",
    "\n",
    "**Week 4:** Evaluation\n",
    "- [ ] Create comprehensive benchmarks\n",
    "- [ ] Add safety evaluation\n",
    "- [ ] Implement A/B testing\n",
    "\n",
    "**Week 5-6:** Automation & docs\n",
    "- [ ] Build CI/CD pipeline\n",
    "- [ ] Add monitoring\n",
    "- [ ] Document everything\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… Ready to build your Training Pipeline!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}