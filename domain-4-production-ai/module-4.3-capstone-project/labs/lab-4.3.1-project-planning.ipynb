{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Project Planning & Architecture Design\n\n**Module:** 4.3 - Capstone Project (Domain 4: Production AI)\n**Time:** 4-6 hours\n**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê\n\n---\n\n## üéØ Learning Objectives\n\nBy the end of this notebook, you will:\n- [ ] Design a complete system architecture for your project\n- [ ] Create detailed component specifications\n- [ ] Plan your data pipeline and model strategy\n- [ ] Define your API contracts\n- [ ] Set up monitoring and evaluation infrastructure\n\n---\n\n## üìö Prerequisites\n\n- Completed: `lab-4.3.0-project-kickoff.ipynb`\n- Selected: Your project option (A, B, C, or D)\n- Created: Initial project proposal\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåç Real-World Context\n",
    "\n",
    "At companies like Google, Meta, and OpenAI, engineers spend 20-30% of their project time on planning and design. This isn't wasted time - it's the foundation for everything that follows.\n",
    "\n",
    "**Why Architecture Matters:**\n",
    "\n",
    "| Without Architecture | With Architecture |\n",
    "|---------------------|-------------------|\n",
    "| \"Let me just start coding...\" | \"Here's what we're building...\" |\n",
    "| Constant rewrites | Incremental progress |\n",
    "| Integration nightmares | Clean interfaces |\n",
    "| \"It works on my machine\" | Reproducible everywhere |\n",
    "| Unclear requirements | Testable specifications |\n",
    "\n",
    "This notebook guides you through the same planning process used in production AI systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: System Architecture\n",
    "\n",
    "> **Imagine you're building a treehouse.** Before picking up a hammer, you'd want to:\n",
    ">\n",
    "> 1. **Sketch a plan** - Where does the door go? How big is the window?\n",
    "> 2. **List materials** - How much wood? What kind of nails?\n",
    "> 3. **Plan the order** - Build the floor before the walls!\n",
    "> 4. **Think about problems** - What if it rains during construction?\n",
    ">\n",
    "> **System architecture is your blueprint.** It shows:\n",
    "> - What pieces you're building (components)\n",
    "> - How they connect (interfaces)\n",
    "> - What they're made of (technologies)\n",
    "> - What order to build them (dependencies)\n",
    ">\n",
    "> **Without a blueprint**, you might build the roof first and realize you can't attach it. With one, you build systematically and everything fits together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Architecture Patterns for AI Systems\n",
    "\n",
    "Let's explore common patterns used in production AI systems. Understanding these helps you design your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture Pattern Reference\n",
    "# These are the building blocks for your system design\n",
    "\n",
    "architecture_patterns = {\n",
    "    \"rag_pipeline\": {\n",
    "        \"name\": \"RAG Pipeline\",\n",
    "        \"description\": \"Retrieval-Augmented Generation for knowledge-grounded responses\",\n",
    "        \"components\": [\n",
    "            \"Document Loader\",\n",
    "            \"Embedding Model\",\n",
    "            \"Vector Store\",\n",
    "            \"Retriever\",\n",
    "            \"LLM Generator\",\n",
    "            \"Response Formatter\",\n",
    "        ],\n",
    "        \"data_flow\": \"Query ‚Üí Embed ‚Üí Search ‚Üí Retrieve ‚Üí Augment ‚Üí Generate ‚Üí Response\",\n",
    "        \"best_for\": [\"Option A\", \"Option B\"],\n",
    "        \"diagram\": \"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Query     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Embedder   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Vector DB   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                               ‚îÇ\n",
    "                                               ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Response   ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ     LLM     ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Documents  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        \"\"\"\n",
    "    },\n",
    "    \n",
    "    \"agent_orchestrator\": {\n",
    "        \"name\": \"Agent Orchestrator\",\n",
    "        \"description\": \"Central coordinator managing specialized agents\",\n",
    "        \"components\": [\n",
    "            \"Orchestrator\",\n",
    "            \"Task Planner\",\n",
    "            \"Agent Pool\",\n",
    "            \"Tool Registry\",\n",
    "            \"Memory Manager\",\n",
    "            \"Safety Layer\",\n",
    "        ],\n",
    "        \"data_flow\": \"Task ‚Üí Plan ‚Üí Dispatch ‚Üí Execute ‚Üí Aggregate ‚Üí Verify ‚Üí Output\",\n",
    "        \"best_for\": [\"Option C\"],\n",
    "        \"diagram\": \"\"\"\n",
    "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                    ‚îÇ  Orchestrator   ‚îÇ\n",
    "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "              ‚ñº              ‚ñº              ‚ñº\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ Agent A  ‚îÇ  ‚îÇ Agent B  ‚îÇ  ‚îÇ Agent C  ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "             ‚îÇ             ‚îÇ             ‚îÇ\n",
    "             ‚ñº             ‚ñº             ‚ñº\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ Tools A  ‚îÇ  ‚îÇ Tools B  ‚îÇ  ‚îÇ Tools C  ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        \"\"\"\n",
    "    },\n",
    "    \n",
    "    \"training_pipeline\": {\n",
    "        \"name\": \"Training Pipeline\",\n",
    "        \"description\": \"End-to-end model training and deployment workflow\",\n",
    "        \"components\": [\n",
    "            \"Data Collector\",\n",
    "            \"Preprocessor\",\n",
    "            \"Trainer\",\n",
    "            \"Evaluator\",\n",
    "            \"Model Registry\",\n",
    "            \"Deployment Manager\",\n",
    "        ],\n",
    "        \"data_flow\": \"Collect ‚Üí Clean ‚Üí Train ‚Üí Evaluate ‚Üí Register ‚Üí Deploy\",\n",
    "        \"best_for\": [\"Option D\"],\n",
    "        \"diagram\": \"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Data   ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ  Clean  ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ  Train  ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ  Eval   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                               ‚îÇ\n",
    "                                     Pass?  ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ Fail?\n",
    "                                        ‚îÇ      ‚îÇ      ‚îÇ\n",
    "                                        ‚ñº      ‚îÇ      ‚ñº\n",
    "                                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                                   ‚îÇ Deploy  ‚îÇ ‚îÇ ‚îÇ Iterate ‚îÇ\n",
    "                                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                               ‚îÇ      ‚îÇ\n",
    "                                               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        \"\"\"\n",
    "    },\n",
    "    \n",
    "    \"multimodal_processor\": {\n",
    "        \"name\": \"Multimodal Processor\",\n",
    "        \"description\": \"Process and understand multiple data modalities\",\n",
    "        \"components\": [\n",
    "            \"Input Router\",\n",
    "            \"Vision Encoder\",\n",
    "            \"Text Encoder\",\n",
    "            \"Fusion Layer\",\n",
    "            \"Task Head\",\n",
    "            \"Output Formatter\",\n",
    "        ],\n",
    "        \"data_flow\": \"Input ‚Üí Route ‚Üí Encode ‚Üí Fuse ‚Üí Process ‚Üí Format ‚Üí Output\",\n",
    "        \"best_for\": [\"Option B\"],\n",
    "        \"diagram\": \"\"\"\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ  Image  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Vision  ‚îÇ‚îÄ‚îÄ‚îê\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ Encoder ‚îÇ  ‚îÇ\n",
    "                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                                         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Fusion  ‚îÇ\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        ‚îÇ  Text   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Text   ‚îÇ‚îÄ‚îÄ‚îò          ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ Encoder ‚îÇ             ‚ñº\n",
    "                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                                              ‚îÇ  Output ‚îÇ\n",
    "                                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        \"\"\"\n",
    "    },\n",
    "}\n",
    "\n",
    "def show_pattern(pattern_name: str):\n",
    "    \"\"\"Display details of an architecture pattern.\"\"\"\n",
    "    pattern = architecture_patterns.get(pattern_name)\n",
    "    if not pattern:\n",
    "        print(f\"‚ùå Unknown pattern: {pattern_name}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüèóÔ∏è {pattern['name']}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nüìù {pattern['description']}\")\n",
    "    print(f\"\\nüéØ Best for: {', '.join(pattern['best_for'])}\")\n",
    "    print(f\"\\nüì¶ Components:\")\n",
    "    for comp in pattern['components']:\n",
    "        print(f\"   ‚Ä¢ {comp}\")\n",
    "    print(f\"\\nüîÑ Data Flow:\\n   {pattern['data_flow']}\")\n",
    "    print(f\"\\nüìä Architecture Diagram:\")\n",
    "    print(pattern['diagram'])\n",
    "\n",
    "# Show all patterns\n",
    "for pattern in architecture_patterns:\n",
    "    show_pattern(pattern)\n",
    "    print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "We explored four common architecture patterns:\n",
    "\n",
    "1. **RAG Pipeline** - For knowledge-grounded generation (Options A, B)\n",
    "2. **Agent Orchestrator** - For multi-agent coordination (Option C)\n",
    "3. **Training Pipeline** - For model development (Option D)\n",
    "4. **Multimodal Processor** - For mixed media processing (Option B)\n",
    "\n",
    "Your project will likely combine elements from multiple patterns. For example, an AI Assistant (Option A) might use:\n",
    "- RAG Pipeline for knowledge retrieval\n",
    "- Agent-like tool calling for actions\n",
    "- Training elements for fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Component Design Template\n",
    "\n",
    "Every component in your system should be well-defined. Here's a template for component specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Optional\n",
    "from enum import Enum\n",
    "\n",
    "class ComponentStatus(Enum):\n",
    "    PLANNED = \"planned\"\n",
    "    IN_PROGRESS = \"in_progress\"\n",
    "    COMPLETE = \"complete\"\n",
    "    BLOCKED = \"blocked\"\n",
    "\n",
    "@dataclass\n",
    "class ComponentSpec:\n",
    "    \"\"\"Specification for a system component.\"\"\"\n",
    "    \n",
    "    name: str\n",
    "    purpose: str\n",
    "    inputs: List[Dict[str, str]]  # [{\"name\": ..., \"type\": ..., \"description\": ...}]\n",
    "    outputs: List[Dict[str, str]]\n",
    "    dependencies: List[str]  # Names of components this depends on\n",
    "    technologies: List[str]\n",
    "    estimated_hours: float\n",
    "    status: ComponentStatus = ComponentStatus.PLANNED\n",
    "    notes: str = \"\"\n",
    "    \n",
    "    def to_markdown(self) -> str:\n",
    "        \"\"\"Generate markdown documentation for this component.\"\"\"\n",
    "        md = f\"## {self.name}\\n\\n\"\n",
    "        md += f\"**Purpose:** {self.purpose}\\n\\n\"\n",
    "        md += f\"**Status:** {self.status.value}\\n\\n\"\n",
    "        md += f\"**Estimated Hours:** {self.estimated_hours}\\n\\n\"\n",
    "        \n",
    "        md += \"### Inputs\\n\\n\"\n",
    "        md += \"| Name | Type | Description |\\n\"\n",
    "        md += \"|------|------|-------------|\\n\"\n",
    "        for inp in self.inputs:\n",
    "            md += f\"| {inp['name']} | `{inp['type']}` | {inp['description']} |\\n\"\n",
    "        \n",
    "        md += \"\\n### Outputs\\n\\n\"\n",
    "        md += \"| Name | Type | Description |\\n\"\n",
    "        md += \"|------|------|-------------|\\n\"\n",
    "        for out in self.outputs:\n",
    "            md += f\"| {out['name']} | `{out['type']}` | {out['description']} |\\n\"\n",
    "        \n",
    "        md += f\"\\n### Dependencies\\n\\n\"\n",
    "        for dep in self.dependencies:\n",
    "            md += f\"- {dep}\\n\"\n",
    "        \n",
    "        md += f\"\\n### Technologies\\n\\n\"\n",
    "        for tech in self.technologies:\n",
    "            md += f\"- {tech}\\n\"\n",
    "        \n",
    "        if self.notes:\n",
    "            md += f\"\\n### Notes\\n\\n{self.notes}\\n\"\n",
    "        \n",
    "        return md\n",
    "\n",
    "# Example: RAG Retriever Component\n",
    "example_component = ComponentSpec(\n",
    "    name=\"RAG Retriever\",\n",
    "    purpose=\"Retrieve relevant documents from the knowledge base for a given query\",\n",
    "    inputs=[\n",
    "        {\"name\": \"query\", \"type\": \"str\", \"description\": \"User's natural language question\"},\n",
    "        {\"name\": \"top_k\", \"type\": \"int\", \"description\": \"Number of documents to retrieve\"},\n",
    "        {\"name\": \"filters\", \"type\": \"dict\", \"description\": \"Optional metadata filters\"},\n",
    "    ],\n",
    "    outputs=[\n",
    "        {\"name\": \"documents\", \"type\": \"List[Document]\", \"description\": \"Retrieved documents with scores\"},\n",
    "        {\"name\": \"metadata\", \"type\": \"dict\", \"description\": \"Retrieval metadata (time, scores, etc.)\"},\n",
    "    ],\n",
    "    dependencies=[\"Embedding Model\", \"Vector Store\"],\n",
    "    technologies=[\"sentence-transformers\", \"FAISS\", \"LangChain\"],\n",
    "    estimated_hours=4.0,\n",
    "    notes=\"Consider hybrid search (dense + sparse) for better recall.\"\n",
    ")\n",
    "\n",
    "print(example_component.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Architecture Builder\n",
    "\n",
    "@dataclass\n",
    "class SystemArchitecture:\n",
    "    \"\"\"Complete system architecture specification.\"\"\"\n",
    "    \n",
    "    name: str\n",
    "    description: str\n",
    "    components: List[ComponentSpec] = field(default_factory=list)\n",
    "    \n",
    "    def add_component(self, component: ComponentSpec):\n",
    "        \"\"\"Add a component to the architecture.\"\"\"\n",
    "        self.components.append(component)\n",
    "    \n",
    "    def get_build_order(self) -> List[str]:\n",
    "        \"\"\"Get components in dependency order (topological sort).\"\"\"\n",
    "        # Build dependency graph\n",
    "        graph = {c.name: set(c.dependencies) for c in self.components}\n",
    "        all_names = set(graph.keys())\n",
    "        \n",
    "        # Topological sort\n",
    "        order = []\n",
    "        while graph:\n",
    "            # Find nodes with no dependencies (or deps outside our system)\n",
    "            ready = [name for name, deps in graph.items() \n",
    "                     if not deps.intersection(all_names - set(order))]\n",
    "            if not ready:\n",
    "                raise ValueError(\"Circular dependency detected!\")\n",
    "            order.extend(sorted(ready))  # Alphabetical for consistency\n",
    "            for name in ready:\n",
    "                del graph[name]\n",
    "        \n",
    "        return order\n",
    "    \n",
    "    def total_hours(self) -> float:\n",
    "        \"\"\"Calculate total estimated hours.\"\"\"\n",
    "        return sum(c.estimated_hours for c in self.components)\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"Print architecture summary.\"\"\"\n",
    "        print(f\"\\nüèõÔ∏è ARCHITECTURE: {self.name}\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\n{self.description}\\n\")\n",
    "        \n",
    "        print(f\"üì¶ Components ({len(self.components)}):\")\n",
    "        for c in self.components:\n",
    "            status_emoji = {\n",
    "                ComponentStatus.PLANNED: \"üìã\",\n",
    "                ComponentStatus.IN_PROGRESS: \"üîÑ\",\n",
    "                ComponentStatus.COMPLETE: \"‚úÖ\",\n",
    "                ComponentStatus.BLOCKED: \"üö´\",\n",
    "            }\n",
    "            print(f\"  {status_emoji[c.status]} {c.name} ({c.estimated_hours}h)\")\n",
    "        \n",
    "        print(f\"\\n‚è±Ô∏è Total Estimated Hours: {self.total_hours()}\")\n",
    "        \n",
    "        print(f\"\\nüî® Build Order:\")\n",
    "        for i, name in enumerate(self.get_build_order(), 1):\n",
    "            print(f\"  {i}. {name}\")\n",
    "\n",
    "# Example: Build an architecture for Option A (AI Assistant)\n",
    "assistant_arch = SystemArchitecture(\n",
    "    name=\"Domain-Specific AI Assistant\",\n",
    "    description=\"Fine-tuned LLM with RAG, custom tools, and streaming API\"\n",
    ")\n",
    "\n",
    "# Add components\n",
    "assistant_arch.add_component(ComponentSpec(\n",
    "    name=\"Embedding Model\",\n",
    "    purpose=\"Convert text to vector embeddings\",\n",
    "    inputs=[{\"name\": \"text\", \"type\": \"str\", \"description\": \"Text to embed\"}],\n",
    "    outputs=[{\"name\": \"embedding\", \"type\": \"np.ndarray\", \"description\": \"768-dim vector\"}],\n",
    "    dependencies=[],\n",
    "    technologies=[\"sentence-transformers\", \"BGE-M3\"],\n",
    "    estimated_hours=2.0\n",
    "))\n",
    "\n",
    "assistant_arch.add_component(ComponentSpec(\n",
    "    name=\"Vector Store\",\n",
    "    purpose=\"Store and search document embeddings\",\n",
    "    inputs=[\n",
    "        {\"name\": \"embeddings\", \"type\": \"np.ndarray\", \"description\": \"Vectors to store\"},\n",
    "        {\"name\": \"query\", \"type\": \"np.ndarray\", \"description\": \"Query vector\"},\n",
    "    ],\n",
    "    outputs=[{\"name\": \"results\", \"type\": \"List[tuple]\", \"description\": \"(id, score) pairs\"}],\n",
    "    dependencies=[\"Embedding Model\"],\n",
    "    technologies=[\"FAISS\", \"ChromaDB\"],\n",
    "    estimated_hours=3.0\n",
    "))\n",
    "\n",
    "assistant_arch.add_component(ComponentSpec(\n",
    "    name=\"Document Processor\",\n",
    "    purpose=\"Parse and chunk documents for indexing\",\n",
    "    inputs=[{\"name\": \"documents\", \"type\": \"List[Path]\", \"description\": \"Files to process\"}],\n",
    "    outputs=[{\"name\": \"chunks\", \"type\": \"List[Chunk]\", \"description\": \"Processed chunks\"}],\n",
    "    dependencies=[],\n",
    "    technologies=[\"LangChain\", \"unstructured\"],\n",
    "    estimated_hours=4.0\n",
    "))\n",
    "\n",
    "assistant_arch.add_component(ComponentSpec(\n",
    "    name=\"RAG Retriever\",\n",
    "    purpose=\"Retrieve relevant context for queries\",\n",
    "    inputs=[{\"name\": \"query\", \"type\": \"str\", \"description\": \"User question\"}],\n",
    "    outputs=[{\"name\": \"context\", \"type\": \"str\", \"description\": \"Retrieved context\"}],\n",
    "    dependencies=[\"Embedding Model\", \"Vector Store\", \"Document Processor\"],\n",
    "    technologies=[\"LangChain\"],\n",
    "    estimated_hours=4.0\n",
    "))\n",
    "\n",
    "assistant_arch.add_component(ComponentSpec(\n",
    "    name=\"Fine-tuned LLM\",\n",
    "    purpose=\"Generate domain-specific responses\",\n",
    "    inputs=[\n",
    "        {\"name\": \"prompt\", \"type\": \"str\", \"description\": \"System + user prompt\"},\n",
    "        {\"name\": \"context\", \"type\": \"str\", \"description\": \"RAG context\"},\n",
    "    ],\n",
    "    outputs=[{\"name\": \"response\", \"type\": \"str\", \"description\": \"Model response\"}],\n",
    "    dependencies=[],\n",
    "    technologies=[\"transformers\", \"PEFT\", \"bitsandbytes\"],\n",
    "    estimated_hours=12.0\n",
    "))\n",
    "\n",
    "assistant_arch.add_component(ComponentSpec(\n",
    "    name=\"Tool Registry\",\n",
    "    purpose=\"Manage available tools and their execution\",\n",
    "    inputs=[{\"name\": \"tool_call\", \"type\": \"ToolCall\", \"description\": \"Tool request\"}],\n",
    "    outputs=[{\"name\": \"result\", \"type\": \"str\", \"description\": \"Tool output\"}],\n",
    "    dependencies=[],\n",
    "    technologies=[\"LangChain\", \"custom\"],\n",
    "    estimated_hours=6.0\n",
    "))\n",
    "\n",
    "assistant_arch.add_component(ComponentSpec(\n",
    "    name=\"Orchestrator\",\n",
    "    purpose=\"Coordinate RAG, LLM, and tools for query handling\",\n",
    "    inputs=[{\"name\": \"user_message\", \"type\": \"str\", \"description\": \"User input\"}],\n",
    "    outputs=[{\"name\": \"response\", \"type\": \"AssistantResponse\", \"description\": \"Full response\"}],\n",
    "    dependencies=[\"RAG Retriever\", \"Fine-tuned LLM\", \"Tool Registry\"],\n",
    "    technologies=[\"custom\"],\n",
    "    estimated_hours=6.0\n",
    "))\n",
    "\n",
    "assistant_arch.add_component(ComponentSpec(\n",
    "    name=\"Streaming API\",\n",
    "    purpose=\"FastAPI endpoint with SSE streaming\",\n",
    "    inputs=[{\"name\": \"request\", \"type\": \"ChatRequest\", \"description\": \"API request\"}],\n",
    "    outputs=[{\"name\": \"stream\", \"type\": \"AsyncGenerator\", \"description\": \"Token stream\"}],\n",
    "    dependencies=[\"Orchestrator\"],\n",
    "    technologies=[\"FastAPI\", \"SSE\"],\n",
    "    estimated_hours=4.0\n",
    "))\n",
    "\n",
    "# Display summary\n",
    "assistant_arch.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself\n",
    "\n",
    "Create a `SystemArchitecture` for your chosen project. Use the template above as a starting point.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hints for Each Option</summary>\n",
    "\n",
    "**Option A (AI Assistant):**\n",
    "- Start with the example above\n",
    "- Add domain-specific tools\n",
    "- Consider evaluation components\n",
    "\n",
    "**Option B (Document Intelligence):**\n",
    "- Document Ingestion (PDF, images)\n",
    "- OCR/Vision components\n",
    "- Extraction pipeline\n",
    "- QA system\n",
    "- Export formatters\n",
    "\n",
    "**Option C (Agent Swarm):**\n",
    "- Individual agent definitions\n",
    "- Coordinator/Orchestrator\n",
    "- Shared memory system\n",
    "- Tool registry\n",
    "- Safety/approval layer\n",
    "\n",
    "**Option D (Training Pipeline):**\n",
    "- Data collection\n",
    "- Preprocessing\n",
    "- Training loop\n",
    "- Evaluation framework\n",
    "- Model registry\n",
    "- Deployment automation\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: DGX Spark Resource Planning\n",
    "\n",
    "Your DGX Spark has incredible resources. Let's plan how to use them effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DGX Spark Resource Planner\n",
    "\n",
    "@dataclass\n",
    "class ModelFootprint:\n",
    "    \"\"\"Memory footprint of a model.\"\"\"\n",
    "    name: str\n",
    "    params: str  # e.g., \"70B\"\n",
    "    fp32_gb: float\n",
    "    bf16_gb: float\n",
    "    int8_gb: float\n",
    "    int4_gb: float\n",
    "    fp4_gb: float  # Blackwell native\n",
    "    \n",
    "# Common models and their footprints\n",
    "MODEL_FOOTPRINTS = [\n",
    "    ModelFootprint(\"Llama 3.3 8B\", \"8B\", 32.0, 16.0, 8.0, 4.0, 4.0),\n",
    "    ModelFootprint(\"Llama 3.3 70B\", \"70B\", 280.0, 140.0, 70.0, 35.0, 35.0),\n",
    "    ModelFootprint(\"Llama 3.1 405B\", \"405B\", 1620.0, 810.0, 405.0, 202.0, 202.0),\n",
    "    ModelFootprint(\"Qwen2.5 7B\", \"7B\", 28.0, 14.0, 7.0, 3.5, 3.5),\n",
    "    ModelFootprint(\"Qwen2.5 72B\", \"72B\", 288.0, 144.0, 72.0, 36.0, 36.0),\n",
    "    ModelFootprint(\"BGE-M3 (embedding)\", \"568M\", 2.3, 1.1, 0.6, 0.3, 0.3),\n",
    "    ModelFootprint(\"LLaVA 1.6 34B\", \"34B\", 136.0, 68.0, 34.0, 17.0, 17.0),\n",
    "    ModelFootprint(\"Whisper Large v3\", \"1.5B\", 6.0, 3.0, 1.5, 0.8, 0.8),\n",
    "]\n",
    "\n",
    "DGX_SPARK_MEMORY_GB = 128.0\n",
    "\n",
    "def plan_memory_usage(models: List[tuple], additional_gb: float = 10.0):\n",
    "    \"\"\"\n",
    "    Plan memory usage for a set of models.\n",
    "    \n",
    "    Args:\n",
    "        models: List of (model_name, precision) tuples\n",
    "        additional_gb: Buffer for KV cache, activations, etc.\n",
    "    \"\"\"\n",
    "    print(\"\\nüíæ DGX SPARK MEMORY PLANNING\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Available Memory: {DGX_SPARK_MEMORY_GB} GB\\n\")\n",
    "    \n",
    "    total_used = 0\n",
    "    \n",
    "    print(\"üì¶ Model Allocations:\")\n",
    "    for model_name, precision in models:\n",
    "        # Find model footprint\n",
    "        footprint = next((m for m in MODEL_FOOTPRINTS if m.name == model_name), None)\n",
    "        if not footprint:\n",
    "            print(f\"  ‚ö†Ô∏è Unknown model: {model_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Get memory for precision\n",
    "        precision_map = {\n",
    "            \"fp32\": footprint.fp32_gb,\n",
    "            \"bf16\": footprint.bf16_gb,\n",
    "            \"int8\": footprint.int8_gb,\n",
    "            \"int4\": footprint.int4_gb,\n",
    "            \"fp4\": footprint.fp4_gb,\n",
    "        }\n",
    "        memory = precision_map.get(precision, footprint.bf16_gb)\n",
    "        total_used += memory\n",
    "        \n",
    "        print(f\"  ‚Ä¢ {model_name} ({precision}): {memory:.1f} GB\")\n",
    "    \n",
    "    # Add additional memory\n",
    "    total_used += additional_gb\n",
    "    print(f\"  ‚Ä¢ Additional (cache, activations): {additional_gb:.1f} GB\")\n",
    "    \n",
    "    remaining = DGX_SPARK_MEMORY_GB - total_used\n",
    "    \n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"  Total Used: {total_used:.1f} GB\")\n",
    "    print(f\"  Remaining: {remaining:.1f} GB\")\n",
    "    \n",
    "    if remaining < 0:\n",
    "        print(f\"  ‚ùå OVER BUDGET by {-remaining:.1f} GB!\")\n",
    "        print(\"  Consider: Use lower precision or smaller models\")\n",
    "    elif remaining < 10:\n",
    "        print(f\"  ‚ö†Ô∏è Tight on memory - be careful with batch sizes\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ Good memory headroom!\")\n",
    "    \n",
    "    # Visualize\n",
    "    print(\"\\n  Memory Bar:\")\n",
    "    used_pct = min(100, (total_used / DGX_SPARK_MEMORY_GB) * 100)\n",
    "    bar = \"‚ñà\" * int(used_pct / 2) + \"‚ñë\" * (50 - int(used_pct / 2))\n",
    "    print(f\"  [{bar}] {used_pct:.0f}%\")\n",
    "\n",
    "# Example: Plan for Option A (AI Assistant)\n",
    "print(\"\\nüéØ Example: Option A - AI Assistant\")\n",
    "plan_memory_usage([\n",
    "    (\"Llama 3.3 70B\", \"int4\"),\n",
    "    (\"BGE-M3 (embedding)\", \"bf16\"),\n",
    "], additional_gb=15.0)\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "\n",
    "# Example: Plan for Option C (Agent Swarm with multiple smaller models)\n",
    "print(\"\\nüéØ Example: Option C - Agent Swarm\")\n",
    "plan_memory_usage([\n",
    "    (\"Llama 3.3 8B\", \"bf16\"),  # Main agent\n",
    "    (\"Qwen2.5 7B\", \"bf16\"),     # Code agent\n",
    "    (\"BGE-M3 (embedding)\", \"bf16\"),  # Embedding\n",
    "], additional_gb=20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all available models\n",
    "print(\"\\nüìã AVAILABLE MODEL FOOTPRINTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model':<25} {'Params':<8} {'FP32':<10} {'BF16':<10} {'INT8':<10} {'INT4':<10} {'FP4':<10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for model in MODEL_FOOTPRINTS:\n",
    "    print(f\"{model.name:<25} {model.params:<8} {model.fp32_gb:<10.1f} {model.bf16_gb:<10.1f} \"\n",
    "          f\"{model.int8_gb:<10.1f} {model.int4_gb:<10.1f} {model.fp4_gb:<10.1f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(f\"DGX Spark Memory: {DGX_SPARK_MEMORY_GB} GB (unified CPU+GPU)\")\n",
    "print(f\"Note: FP4 is exclusive to Blackwell architecture!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: API Contract Design\n",
    "\n",
    "If your project includes an API, defining contracts early prevents integration headaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Contract Templates\n",
    "\n",
    "from typing import List, Optional, Dict, Any\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "\n",
    "# Try to import pydantic for schema validation\n",
    "try:\n",
    "    from pydantic import BaseModel, Field\n",
    "    PYDANTIC_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PYDANTIC_AVAILABLE = False\n",
    "    # Fallback to dataclasses\n",
    "    from dataclasses import dataclass as BaseModel\n",
    "    def Field(default=None, description=\"\"):\n",
    "        return default\n",
    "\n",
    "# Common API schemas for AI projects\n",
    "\n",
    "class MessageRole(str, Enum):\n",
    "    SYSTEM = \"system\"\n",
    "    USER = \"user\"\n",
    "    ASSISTANT = \"assistant\"\n",
    "    TOOL = \"tool\"\n",
    "\n",
    "if PYDANTIC_AVAILABLE:\n",
    "    class Message(BaseModel):\n",
    "        \"\"\"A chat message.\"\"\"\n",
    "        role: MessageRole = Field(description=\"Role of the message sender\")\n",
    "        content: str = Field(description=\"Message content\")\n",
    "        name: Optional[str] = Field(default=None, description=\"Name for tool messages\")\n",
    "        tool_calls: Optional[List[Dict]] = Field(default=None, description=\"Tool calls made\")\n",
    "\n",
    "    class ChatRequest(BaseModel):\n",
    "        \"\"\"Request to the chat API.\"\"\"\n",
    "        messages: List[Message] = Field(description=\"Conversation history\")\n",
    "        stream: bool = Field(default=True, description=\"Enable streaming\")\n",
    "        temperature: float = Field(default=0.7, description=\"Sampling temperature\")\n",
    "        max_tokens: int = Field(default=2048, description=\"Max tokens to generate\")\n",
    "        tools: Optional[List[Dict]] = Field(default=None, description=\"Available tools\")\n",
    "\n",
    "    class ChatResponse(BaseModel):\n",
    "        \"\"\"Response from the chat API.\"\"\"\n",
    "        id: str = Field(description=\"Response ID\")\n",
    "        message: Message = Field(description=\"Assistant's response\")\n",
    "        usage: Dict[str, int] = Field(description=\"Token usage stats\")\n",
    "        latency_ms: float = Field(description=\"Response latency in milliseconds\")\n",
    "\n",
    "    class DocumentUploadRequest(BaseModel):\n",
    "        \"\"\"Request to upload documents to knowledge base.\"\"\"\n",
    "        files: List[str] = Field(description=\"File paths to upload\")\n",
    "        collection: str = Field(default=\"default\", description=\"Target collection\")\n",
    "        chunk_size: int = Field(default=512, description=\"Chunk size for splitting\")\n",
    "        chunk_overlap: int = Field(default=50, description=\"Overlap between chunks\")\n",
    "\n",
    "    class SearchRequest(BaseModel):\n",
    "        \"\"\"Request to search the knowledge base.\"\"\"\n",
    "        query: str = Field(description=\"Search query\")\n",
    "        top_k: int = Field(default=5, description=\"Number of results\")\n",
    "        collection: Optional[str] = Field(default=None, description=\"Collection to search\")\n",
    "        filters: Optional[Dict] = Field(default=None, description=\"Metadata filters\")\n",
    "\n",
    "    class SearchResult(BaseModel):\n",
    "        \"\"\"A single search result.\"\"\"\n",
    "        content: str = Field(description=\"Document content\")\n",
    "        score: float = Field(description=\"Relevance score\")\n",
    "        metadata: Dict = Field(description=\"Document metadata\")\n",
    "\n",
    "    print(\"‚úÖ API schemas defined with Pydantic\")\n",
    "    \n",
    "    # Show schema\n",
    "    print(\"\\nüìã ChatRequest Schema:\")\n",
    "    print(ChatRequest.model_json_schema())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Pydantic not available. Install with: pip install pydantic\")\n",
    "    print(\"   Schemas shown as examples only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Endpoint Documentation Template\n",
    "\n",
    "api_endpoints = [\n",
    "    {\n",
    "        \"method\": \"POST\",\n",
    "        \"path\": \"/api/v1/chat\",\n",
    "        \"description\": \"Send a message and get a response\",\n",
    "        \"request\": \"ChatRequest\",\n",
    "        \"response\": \"ChatResponse (or SSE stream)\",\n",
    "        \"example\": {\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"How do I create an S3 bucket?\"}],\n",
    "            \"stream\": True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"method\": \"POST\",\n",
    "        \"path\": \"/api/v1/documents\",\n",
    "        \"description\": \"Upload documents to knowledge base\",\n",
    "        \"request\": \"DocumentUploadRequest\",\n",
    "        \"response\": \"{status, document_ids, chunks_created}\",\n",
    "        \"example\": {\n",
    "            \"files\": [\"/data/aws-docs/s3-guide.pdf\"],\n",
    "            \"collection\": \"aws-docs\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"method\": \"POST\",\n",
    "        \"path\": \"/api/v1/search\",\n",
    "        \"description\": \"Search the knowledge base\",\n",
    "        \"request\": \"SearchRequest\",\n",
    "        \"response\": \"List[SearchResult]\",\n",
    "        \"example\": {\n",
    "            \"query\": \"S3 bucket policy\",\n",
    "            \"top_k\": 5\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"method\": \"GET\",\n",
    "        \"path\": \"/api/v1/health\",\n",
    "        \"description\": \"Health check endpoint\",\n",
    "        \"request\": \"None\",\n",
    "        \"response\": \"{status, model_loaded, memory_usage}\",\n",
    "        \"example\": None\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"\\nüîå API ENDPOINTS\")\n",
    "print(\"=\"*70)\n",
    "for endpoint in api_endpoints:\n",
    "    print(f\"\\n{endpoint['method']} {endpoint['path']}\")\n",
    "    print(f\"  Description: {endpoint['description']}\")\n",
    "    print(f\"  Request: {endpoint['request']}\")\n",
    "    print(f\"  Response: {endpoint['response']}\")\n",
    "    if endpoint['example']:\n",
    "        import json\n",
    "        print(f\"  Example: {json.dumps(endpoint['example'], indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Evaluation Framework Planning\n",
    "\n",
    "How will you know if your project is successful? Define metrics and evaluation strategy now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Framework Template\n",
    "\n",
    "@dataclass\n",
    "class EvaluationMetric:\n",
    "    \"\"\"Definition of an evaluation metric.\"\"\"\n",
    "    name: str\n",
    "    description: str\n",
    "    target: str  # Target value or range\n",
    "    measurement_method: str\n",
    "    frequency: str  # How often to measure\n",
    "\n",
    "@dataclass  \n",
    "class EvaluationPlan:\n",
    "    \"\"\"Complete evaluation plan for the project.\"\"\"\n",
    "    project_name: str\n",
    "    metrics: List[EvaluationMetric]\n",
    "    datasets: List[Dict[str, str]]  # [{\"name\": ..., \"source\": ..., \"size\": ...}]\n",
    "    baselines: List[str]\n",
    "    \n",
    "    def display(self):\n",
    "        print(f\"\\nüìä EVALUATION PLAN: {self.project_name}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(\"\\nüìè Metrics:\")\n",
    "        for metric in self.metrics:\n",
    "            print(f\"\\n  {metric.name}\")\n",
    "            print(f\"    Description: {metric.description}\")\n",
    "            print(f\"    Target: {metric.target}\")\n",
    "            print(f\"    Method: {metric.measurement_method}\")\n",
    "            print(f\"    Frequency: {metric.frequency}\")\n",
    "        \n",
    "        print(\"\\nüìö Datasets:\")\n",
    "        for ds in self.datasets:\n",
    "            print(f\"  ‚Ä¢ {ds['name']}: {ds['size']} samples from {ds['source']}\")\n",
    "        \n",
    "        print(\"\\nüéØ Baselines:\")\n",
    "        for baseline in self.baselines:\n",
    "            print(f\"  ‚Ä¢ {baseline}\")\n",
    "\n",
    "# Example: Evaluation plan for AI Assistant\n",
    "assistant_eval = EvaluationPlan(\n",
    "    project_name=\"AWS AI Assistant\",\n",
    "    metrics=[\n",
    "        EvaluationMetric(\n",
    "            name=\"Answer Accuracy\",\n",
    "            description=\"Percentage of correct answers on test set\",\n",
    "            target=\"‚â• 80%\",\n",
    "            measurement_method=\"Human evaluation + automated checks\",\n",
    "            frequency=\"Weekly + final\"\n",
    "        ),\n",
    "        EvaluationMetric(\n",
    "            name=\"Retrieval Recall@5\",\n",
    "            description=\"Relevant docs in top 5 retrieved\",\n",
    "            target=\"‚â• 90%\",\n",
    "            measurement_method=\"Test with known-answer queries\",\n",
    "            frequency=\"After RAG changes\"\n",
    "        ),\n",
    "        EvaluationMetric(\n",
    "            name=\"Latency P95\",\n",
    "            description=\"95th percentile response time\",\n",
    "            target=\"< 3 seconds\",\n",
    "            measurement_method=\"API benchmarking\",\n",
    "            frequency=\"After optimization changes\"\n",
    "        ),\n",
    "        EvaluationMetric(\n",
    "            name=\"Throughput\",\n",
    "            description=\"Requests per second\",\n",
    "            target=\"‚â• 10 req/s\",\n",
    "            measurement_method=\"Load testing\",\n",
    "            frequency=\"Final evaluation\"\n",
    "        ),\n",
    "        EvaluationMetric(\n",
    "            name=\"User Satisfaction\",\n",
    "            description=\"Self-reported helpfulness score\",\n",
    "            target=\"‚â• 4.0/5.0\",\n",
    "            measurement_method=\"Demo session feedback\",\n",
    "            frequency=\"Demo sessions\"\n",
    "        ),\n",
    "    ],\n",
    "    datasets=[\n",
    "        {\"name\": \"AWS FAQ Test Set\", \"source\": \"Hand-curated from AWS forums\", \"size\": \"100\"},\n",
    "        {\"name\": \"CLI Command Dataset\", \"source\": \"Generated from AWS CLI docs\", \"size\": \"500\"},\n",
    "        {\"name\": \"Edge Cases\", \"source\": \"Identified during development\", \"size\": \"50\"},\n",
    "    ],\n",
    "    baselines=[\n",
    "        \"Raw Llama 3.3 70B (no fine-tuning, no RAG)\",\n",
    "        \"GPT-4 with AWS docs in context (if available)\",\n",
    "        \"AWS official documentation search\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "assistant_eval.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Vague Architecture\n",
    "```python\n",
    "# ‚ùå Wrong: Too vague\n",
    "components = [\"data stuff\", \"model\", \"api\"]\n",
    "\n",
    "# ‚úÖ Right: Specific and actionable\n",
    "components = [\n",
    "    \"DocumentParser: Extract text from PDFs using pypdf2\",\n",
    "    \"ChunkingService: Split into 512-token chunks with 50 overlap\",\n",
    "    \"EmbeddingService: Use BGE-M3 to create 768-dim vectors\",\n",
    "    \"VectorStore: FAISS index with IVF for fast search\",\n",
    "    \"...\"\n",
    "]\n",
    "```\n",
    "\n",
    "### Mistake 2: No Interface Definitions\n",
    "```python\n",
    "# ‚ùå Wrong: \"The retriever will talk to the LLM somehow\"\n",
    "\n",
    "# ‚úÖ Right: Clear interface\n",
    "def retrieve(query: str, top_k: int = 5) -> List[Document]:\n",
    "    \"\"\"Returns documents with content, score, and metadata.\"\"\"\n",
    "    ...\n",
    "\n",
    "def generate(prompt: str, context: List[Document]) -> str:\n",
    "    \"\"\"Generates response using context.\"\"\"\n",
    "    ...\n",
    "```\n",
    "\n",
    "### Mistake 3: Ignoring Memory Constraints\n",
    "```python\n",
    "# ‚ùå Wrong: \"I'll just load everything\"\n",
    "models = [\"llama-70b-fp16\", \"llava-34b-fp16\", \"whisper-large\"]  # 85+68+3 = 156 GB!\n",
    "\n",
    "# ‚úÖ Right: Plan memory carefully\n",
    "models = [\n",
    "    (\"llama-70b\", \"int4\", 35),  # 35 GB\n",
    "    (\"bge-m3\", \"bf16\", 1.1),     # 1.1 GB  \n",
    "    (\"overhead\", \"-\", 20),       # 20 GB for KV cache, etc.\n",
    "]  # Total: 56.1 GB - plenty of headroom!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## üéâ Checkpoint\n\nYou've completed project planning! You should now have:\n\n- ‚úÖ System architecture with all components defined\n- ‚úÖ Build order based on dependencies\n- ‚úÖ Memory plan for DGX Spark\n- ‚úÖ API contracts (if applicable)\n- ‚úÖ Evaluation plan with metrics and datasets\n\n---\n\n## üöÄ Next Steps\n\n1. **Document your architecture** in `docs/architecture.md`\n2. **Review your project proposal** - update based on planning\n3. **Open your project-specific guide:**\n   - Option A: `lab-4.3.2-option-a-ai-assistant.ipynb`\n   - Option B: `lab-4.3.3-option-b-document-intelligence.ipynb`\n   - Option C: `lab-4.3.4-option-c-agent-swarm.ipynb`\n   - Option D: `lab-4.3.5-option-d-training-pipeline.ipynb`\n4. **Start building!**\n\n---\n\n## üìñ Further Reading\n\n- [Designing Machine Learning Systems (Chip Huyen)](https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/)\n- [System Design for LLM Applications](https://huyenchip.com/2023/04/11/llm-engineering.html)\n- [The Architecture of Open Source Applications](https://aosabook.org/en/index.html)\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßπ Cleanup\n",
    "print(\"‚úÖ No cleanup needed - your architecture plans are saved!\")\n",
    "print(\"\\nüìù Next: Document your architecture and start implementation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}