{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Framework for Capstone Projects\n",
    "\n",
    "**Module:** 16 - Capstone Project (Phase 4)\n",
    "**Time:** 4-6 hours\n",
    "**Difficulty:** â­â­â­â­\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand evaluation best practices for AI systems\n",
    "- [ ] Create custom evaluation metrics\n",
    "- [ ] Build automated benchmark suites\n",
    "- [ ] Use LLM-as-judge for quality assessment\n",
    "- [ ] Generate evaluation reports\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§’ ELI5: Why Evaluation Matters\n",
    "\n",
    "> **Imagine you baked a cake** but never tasted it before serving:\n",
    ">\n",
    "> - Is it sweet enough?\n",
    "> - Is it cooked through?\n",
    "> - Would guests like it?\n",
    ">\n",
    "> **Evaluation is tasting your AI system.** Without it, you don't know if:\n",
    "> - The model gives correct answers\n",
    "> - The system is fast enough\n",
    "> - Users will find it helpful\n",
    ">\n",
    "> Good evaluation tells you: \"This works well\" or \"Fix this part.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Evaluation Fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Framework Foundation\n",
    "\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Callable, Optional\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import time\n",
    "import statistics\n",
    "\n",
    "print(\"ðŸŽ¯ CAPSTONE EVALUATION FRAMEWORK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "\n",
    "@dataclass\n",
    "class EvalSample:\n",
    "    \"\"\"A single evaluation sample.\"\"\"\n",
    "    id: str\n",
    "    input: str\n",
    "    expected: str = \"\"\n",
    "    category: str = \"general\"\n",
    "    difficulty: str = \"medium\"\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "@dataclass\n",
    "class EvalResult:\n",
    "    \"\"\"Result of evaluating a sample.\"\"\"\n",
    "    sample_id: str\n",
    "    input: str\n",
    "    expected: str\n",
    "    actual: str\n",
    "    scores: Dict[str, float]\n",
    "    latency_ms: float\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "@dataclass\n",
    "class EvalReport:\n",
    "    \"\"\"Complete evaluation report.\"\"\"\n",
    "    name: str\n",
    "    timestamp: datetime\n",
    "    num_samples: int\n",
    "    aggregate_scores: Dict[str, float]\n",
    "    by_category: Dict[str, Dict[str, float]]\n",
    "    by_difficulty: Dict[str, Dict[str, float]]\n",
    "    latency_stats: Dict[str, float]\n",
    "    results: List[EvalResult]\n",
    "    \n",
    "    def to_markdown(self) -> str:\n",
    "        \"\"\"Generate markdown report.\"\"\"\n",
    "        lines = [\n",
    "            f\"# Evaluation Report: {self.name}\",\n",
    "            f\"Generated: {self.timestamp.strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "            f\"Samples: {self.num_samples}\",\n",
    "            \"\",\n",
    "            \"## Aggregate Scores\",\n",
    "            \"| Metric | Score |\",\n",
    "            \"|--------|-------|\",\n",
    "        ]\n",
    "        \n",
    "        for metric, score in self.aggregate_scores.items():\n",
    "            lines.append(f\"| {metric} | {score:.4f} |\")\n",
    "        \n",
    "        lines.extend([\n",
    "            \"\",\n",
    "            \"## Latency Stats\",\n",
    "            f\"- Mean: {self.latency_stats.get('mean', 0):.1f} ms\",\n",
    "            f\"- P50: {self.latency_stats.get('p50', 0):.1f} ms\",\n",
    "            f\"- P95: {self.latency_stats.get('p95', 0):.1f} ms\",\n",
    "            f\"- Max: {self.latency_stats.get('max', 0):.1f} ms\",\n",
    "        ])\n",
    "        \n",
    "        if self.by_category:\n",
    "            lines.extend([\"\", \"## By Category\"])\n",
    "            for cat, scores in self.by_category.items():\n",
    "                lines.append(f\"\\n### {cat}\")\n",
    "                for metric, score in scores.items():\n",
    "                    lines.append(f\"- {metric}: {score:.4f}\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "print(\"âœ… Evaluation data structures defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Metric Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Evaluation Metrics\n",
    "\n",
    "def exact_match(expected: str, actual: str) -> float:\n",
    "    \"\"\"Exact string match (case-insensitive).\"\"\"\n",
    "    return 1.0 if expected.lower().strip() == actual.lower().strip() else 0.0\n",
    "\n",
    "def contains_answer(expected: str, actual: str) -> float:\n",
    "    \"\"\"Check if expected answer is contained in actual.\"\"\"\n",
    "    return 1.0 if expected.lower() in actual.lower() else 0.0\n",
    "\n",
    "def keyword_coverage(expected: str, actual: str) -> float:\n",
    "    \"\"\"Check coverage of keywords from expected in actual.\"\"\"\n",
    "    # Extract keywords (words longer than 3 chars)\n",
    "    expected_words = set(w.lower() for w in expected.split() if len(w) > 3)\n",
    "    if not expected_words:\n",
    "        return 1.0\n",
    "    \n",
    "    actual_lower = actual.lower()\n",
    "    matches = sum(1 for w in expected_words if w in actual_lower)\n",
    "    return matches / len(expected_words)\n",
    "\n",
    "def response_length_score(expected: str, actual: str, tolerance: float = 0.5) -> float:\n",
    "    \"\"\"Score based on response length similarity.\"\"\"\n",
    "    if not expected:\n",
    "        return 1.0 if len(actual) > 0 else 0.0\n",
    "    \n",
    "    ratio = len(actual) / len(expected)\n",
    "    \n",
    "    # Score drops as ratio deviates from 1.0\n",
    "    if ratio < (1 - tolerance):\n",
    "        return ratio / (1 - tolerance)\n",
    "    elif ratio > (1 + tolerance * 2):\n",
    "        return max(0, 1 - (ratio - 1 - tolerance * 2) / 2)\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "def semantic_similarity(expected: str, actual: str, model=None) -> float:\n",
    "    \"\"\"\n",
    "    Compute semantic similarity using sentence embeddings.\n",
    "    \n",
    "    Requires sentence-transformers.\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
    "        except ImportError:\n",
    "            print(\"âš ï¸ sentence-transformers not installed\")\n",
    "            return keyword_coverage(expected, actual)  # Fallback\n",
    "    \n",
    "    embeddings = model.encode([expected, actual])\n",
    "    similarity = float(embeddings[0] @ embeddings[1] / \n",
    "                      (sum(embeddings[0]**2)**0.5 * sum(embeddings[1]**2)**0.5))\n",
    "    return max(0, similarity)\n",
    "\n",
    "# Metric registry\n",
    "METRICS = {\n",
    "    \"exact_match\": exact_match,\n",
    "    \"contains_answer\": contains_answer,\n",
    "    \"keyword_coverage\": keyword_coverage,\n",
    "    \"length_score\": response_length_score,\n",
    "}\n",
    "\n",
    "print(\"âœ… Metric functions defined\")\n",
    "print(f\"\\nAvailable metrics: {list(METRICS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: LLM-as-Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-as-Judge Evaluation\n",
    "\n",
    "class LLMJudge:\n",
    "    \"\"\"\n",
    "    Use an LLM to judge response quality.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"meta-llama/Llama-3.3-8B-Instruct\"):\n",
    "        self.model_name = model_name\n",
    "        self._model = None\n",
    "        self._tokenizer = None\n",
    "    \n",
    "    def _load(self):\n",
    "        if self._model is not None:\n",
    "            return\n",
    "        \n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "        \n",
    "        print(f\"Loading judge model: {self.model_name}\")\n",
    "        \n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "        \n",
    "        self._model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "    \n",
    "    def judge(\n",
    "        self,\n",
    "        question: str,\n",
    "        expected: str,\n",
    "        actual: str,\n",
    "        criteria: List[str] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Judge a response using an LLM.\n",
    "        \n",
    "        Args:\n",
    "            question: The original question\n",
    "            expected: Expected/reference answer\n",
    "            actual: Model's actual response\n",
    "            criteria: List of evaluation criteria\n",
    "            \n",
    "        Returns:\n",
    "            Dict with scores and reasoning\n",
    "        \"\"\"\n",
    "        self._load()\n",
    "        \n",
    "        criteria = criteria or [\"accuracy\", \"completeness\", \"clarity\"]\n",
    "        criteria_str = \"\\n\".join([f\"- {c}\" for c in criteria])\n",
    "        \n",
    "        prompt = f\"\"\"You are an expert evaluator. Rate the following response.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Reference Answer: {expected}\n",
    "\n",
    "Response to Evaluate: {actual}\n",
    "\n",
    "Evaluate on these criteria:\n",
    "{criteria_str}\n",
    "\n",
    "For each criterion, give a score from 1-5 and brief explanation.\n",
    "Then give an overall score from 1-5.\n",
    "\n",
    "Format your response as:\n",
    "CRITERION: [name]\n",
    "SCORE: [1-5]\n",
    "REASON: [explanation]\n",
    "\n",
    "OVERALL: [1-5]\n",
    "SUMMARY: [brief overall assessment]\"\"\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a fair and thorough evaluator.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        text = self._tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        inputs = self._tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self._model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512,\n",
    "                temperature=0.3,\n",
    "                do_sample=True,\n",
    "            )\n",
    "        \n",
    "        response = self._tokenizer.decode(\n",
    "            outputs[0][inputs[\"input_ids\"].shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        # Parse response\n",
    "        return self._parse_judgement(response, criteria)\n",
    "    \n",
    "    def _parse_judgement(self, response: str, criteria: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Parse the judge's response.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        result = {\n",
    "            \"raw_response\": response,\n",
    "            \"scores\": {},\n",
    "            \"reasons\": {},\n",
    "            \"overall\": 0.0,\n",
    "            \"summary\": \"\",\n",
    "        }\n",
    "        \n",
    "        # Parse criterion scores\n",
    "        for criterion in criteria:\n",
    "            pattern = rf\"{criterion}.*?SCORE:\\s*(\\d)\"  \n",
    "            match = re.search(pattern, response, re.IGNORECASE | re.DOTALL)\n",
    "            if match:\n",
    "                result[\"scores\"][criterion] = int(match.group(1)) / 5.0\n",
    "        \n",
    "        # Parse overall\n",
    "        overall_match = re.search(r\"OVERALL:\\s*(\\d)\", response, re.IGNORECASE)\n",
    "        if overall_match:\n",
    "            result[\"overall\"] = int(overall_match.group(1)) / 5.0\n",
    "        \n",
    "        # Parse summary\n",
    "        summary_match = re.search(r\"SUMMARY:\\s*(.+?)(?:\\n|$)\", response, re.IGNORECASE)\n",
    "        if summary_match:\n",
    "            result[\"summary\"] = summary_match.group(1).strip()\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"âœ… LLM Judge defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Evaluation Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Evaluation Runner\n",
    "\n",
    "class EvaluationRunner:\n",
    "    \"\"\"\n",
    "    Runs evaluations on AI systems.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        metrics: List[str] = None,\n",
    "        use_llm_judge: bool = False,\n",
    "    ):\n",
    "        self.metrics = metrics or [\"keyword_coverage\", \"length_score\"]\n",
    "        self.use_llm_judge = use_llm_judge\n",
    "        self.judge = LLMJudge() if use_llm_judge else None\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        system_fn: Callable[[str], str],\n",
    "        samples: List[EvalSample],\n",
    "        name: str = \"evaluation\"\n",
    "    ) -> EvalReport:\n",
    "        \"\"\"\n",
    "        Run evaluation on a set of samples.\n",
    "        \n",
    "        Args:\n",
    "            system_fn: Function that takes input and returns output\n",
    "            samples: Evaluation samples\n",
    "            name: Name for this evaluation\n",
    "            \n",
    "        Returns:\n",
    "            Complete evaluation report\n",
    "        \"\"\"\n",
    "        print(f\"\\nðŸ”„ Running evaluation: {name}\")\n",
    "        print(f\"   Samples: {len(samples)}\")\n",
    "        print(f\"   Metrics: {self.metrics}\")\n",
    "        \n",
    "        results = []\n",
    "        latencies = []\n",
    "        \n",
    "        for i, sample in enumerate(samples):\n",
    "            # Generate response\n",
    "            start = time.time()\n",
    "            try:\n",
    "                actual = system_fn(sample.input)\n",
    "            except Exception as e:\n",
    "                actual = f\"ERROR: {e}\"\n",
    "            latency_ms = (time.time() - start) * 1000\n",
    "            latencies.append(latency_ms)\n",
    "            \n",
    "            # Compute metrics\n",
    "            scores = {}\n",
    "            for metric_name in self.metrics:\n",
    "                if metric_name in METRICS:\n",
    "                    scores[metric_name] = METRICS[metric_name](sample.expected, actual)\n",
    "            \n",
    "            # LLM judge\n",
    "            if self.use_llm_judge and self.judge:\n",
    "                judgement = self.judge.judge(sample.input, sample.expected, actual)\n",
    "                scores[\"llm_judge\"] = judgement[\"overall\"]\n",
    "            \n",
    "            results.append(EvalResult(\n",
    "                sample_id=sample.id,\n",
    "                input=sample.input,\n",
    "                expected=sample.expected,\n",
    "                actual=actual,\n",
    "                scores=scores,\n",
    "                latency_ms=latency_ms,\n",
    "                metadata={\"category\": sample.category, \"difficulty\": sample.difficulty}\n",
    "            ))\n",
    "            \n",
    "            # Progress\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"   Processed {i+1}/{len(samples)}\")\n",
    "        \n",
    "        # Aggregate results\n",
    "        report = self._create_report(name, results, latencies)\n",
    "        \n",
    "        print(f\"\\nâœ… Evaluation complete\")\n",
    "        print(f\"   Overall scores: {report.aggregate_scores}\")\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def _create_report(\n",
    "        self,\n",
    "        name: str,\n",
    "        results: List[EvalResult],\n",
    "        latencies: List[float]\n",
    "    ) -> EvalReport:\n",
    "        \"\"\"Create report from results.\"\"\"\n",
    "        # Aggregate scores\n",
    "        all_metrics = set()\n",
    "        for r in results:\n",
    "            all_metrics.update(r.scores.keys())\n",
    "        \n",
    "        aggregate = {}\n",
    "        for metric in all_metrics:\n",
    "            scores = [r.scores.get(metric, 0) for r in results]\n",
    "            aggregate[metric] = statistics.mean(scores) if scores else 0\n",
    "        \n",
    "        # By category\n",
    "        by_category = {}\n",
    "        categories = set(r.metadata.get(\"category\", \"general\") for r in results)\n",
    "        for cat in categories:\n",
    "            cat_results = [r for r in results if r.metadata.get(\"category\") == cat]\n",
    "            by_category[cat] = {}\n",
    "            for metric in all_metrics:\n",
    "                scores = [r.scores.get(metric, 0) for r in cat_results]\n",
    "                by_category[cat][metric] = statistics.mean(scores) if scores else 0\n",
    "        \n",
    "        # By difficulty\n",
    "        by_difficulty = {}\n",
    "        difficulties = set(r.metadata.get(\"difficulty\", \"medium\") for r in results)\n",
    "        for diff in difficulties:\n",
    "            diff_results = [r for r in results if r.metadata.get(\"difficulty\") == diff]\n",
    "            by_difficulty[diff] = {}\n",
    "            for metric in all_metrics:\n",
    "                scores = [r.scores.get(metric, 0) for r in diff_results]\n",
    "                by_difficulty[diff][metric] = statistics.mean(scores) if scores else 0\n",
    "        \n",
    "        # Latency stats\n",
    "        sorted_latencies = sorted(latencies)\n",
    "        latency_stats = {\n",
    "            \"mean\": statistics.mean(latencies),\n",
    "            \"p50\": sorted_latencies[len(sorted_latencies) // 2],\n",
    "            \"p95\": sorted_latencies[int(len(sorted_latencies) * 0.95)],\n",
    "            \"max\": max(latencies),\n",
    "            \"min\": min(latencies),\n",
    "        }\n",
    "        \n",
    "        return EvalReport(\n",
    "            name=name,\n",
    "            timestamp=datetime.now(),\n",
    "            num_samples=len(results),\n",
    "            aggregate_scores=aggregate,\n",
    "            by_category=by_category,\n",
    "            by_difficulty=by_difficulty,\n",
    "            latency_stats=latency_stats,\n",
    "            results=results\n",
    "        )\n",
    "\n",
    "print(\"âœ… Evaluation runner ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Running an evaluation\n",
    "\n",
    "# Create sample evaluation dataset\n",
    "example_samples = [\n",
    "    EvalSample(\n",
    "        id=\"1\",\n",
    "        input=\"What is the capital of France?\",\n",
    "        expected=\"Paris is the capital of France.\",\n",
    "        category=\"factual\",\n",
    "        difficulty=\"easy\"\n",
    "    ),\n",
    "    EvalSample(\n",
    "        id=\"2\",\n",
    "        input=\"Explain how photosynthesis works.\",\n",
    "        expected=\"Photosynthesis is the process by which plants convert sunlight, water, and carbon dioxide into glucose and oxygen.\",\n",
    "        category=\"explanation\",\n",
    "        difficulty=\"medium\"\n",
    "    ),\n",
    "    EvalSample(\n",
    "        id=\"3\",\n",
    "        input=\"What are the pros and cons of renewable energy?\",\n",
    "        expected=\"Pros: sustainable, clean, reduces emissions. Cons: intermittent, high initial cost, land use.\",\n",
    "        category=\"analysis\",\n",
    "        difficulty=\"hard\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Mock system function (replace with your actual system)\n",
    "def mock_system(input_text: str) -> str:\n",
    "    \"\"\"Mock system for demonstration.\"\"\"\n",
    "    if \"capital\" in input_text.lower():\n",
    "        return \"Paris is the capital city of France, located in the north of the country.\"\n",
    "    elif \"photosynthesis\" in input_text.lower():\n",
    "        return \"Photosynthesis converts sunlight into energy for plants using water and CO2.\"\n",
    "    elif \"renewable\" in input_text.lower():\n",
    "        return \"Renewable energy has benefits like being sustainable but drawbacks like intermittency.\"\n",
    "    else:\n",
    "        return \"I don't have information about that.\"\n",
    "\n",
    "# Run evaluation\n",
    "runner = EvaluationRunner(\n",
    "    metrics=[\"keyword_coverage\", \"contains_answer\", \"length_score\"],\n",
    "    use_llm_judge=False  # Set True if you want LLM evaluation\n",
    ")\n",
    "\n",
    "report = runner.evaluate(\n",
    "    system_fn=mock_system,\n",
    "    samples=example_samples,\n",
    "    name=\"Demo Evaluation\"\n",
    ")\n",
    "\n",
    "# Print report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(report.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Checkpoint\n",
    "\n",
    "You now have a complete evaluation framework:\n",
    "\n",
    "- âœ… Evaluation data structures\n",
    "- âœ… Multiple metric functions\n",
    "- âœ… LLM-as-judge capability\n",
    "- âœ… Automated evaluation runner\n",
    "- âœ… Report generation\n",
    "\n",
    "### Applying to Your Project\n",
    "\n",
    "1. Create evaluation samples specific to your domain\n",
    "2. Choose or create metrics that matter for your use case\n",
    "3. Run evaluations during development\n",
    "4. Compare against baselines\n",
    "5. Include results in your technical report\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… Ready to evaluate your capstone project!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
