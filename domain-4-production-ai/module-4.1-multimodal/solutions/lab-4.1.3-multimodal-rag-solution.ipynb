{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.1.3 Solutions: Multimodal RAG\n",
    "\n",
    "This notebook contains solutions to the exercises in the Multimodal RAG notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Challenge Solution: Hybrid Search System\n\nThe challenge was to create a system that accepts queries with both text AND an example image.\n\n### Approach\nWe use weighted combination of CLIP embeddings:\n1. **Get both embeddings**: Compute CLIP embeddings for both text query and image query\n2. **Weighted average**: `combined = text_weight * text_emb + (1-text_weight) * image_emb`\n3. **Re-normalize**: Normalize the combined embedding for cosine similarity search\n4. **Search**: Use the combined embedding to query ChromaDB\n\n### Why This Works\nCLIP embeds both images and text into the same vector space where similar concepts are close together. By combining embeddings, we get:\n- `text_weight=1.0`: Pure text search (finds images matching the description)\n- `text_weight=0.0`: Pure image search (finds visually similar images)\n- `text_weight=0.5`: Balanced search (finds images matching both criteria)\n\n### Key Insight\nThe weight parameter lets users control the search behavior dynamically, making the system flexible for different use cases."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gc\n",
    "from PIL import Image\n",
    "from typing import List, Dict, Optional\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load CLIP\nfrom transformers import CLIPProcessor, CLIPModel\n\nprint(\"Loading CLIP...\")\nclip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\nclip_model = CLIPModel.from_pretrained(\n    \"openai/clip-vit-large-patch14\",\n    torch_dtype=torch.bfloat16  # Optimized for Blackwell\n).to(\"cuda\")\nclip_model.eval()\nprint(\"Loaded!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embedding(image: Image.Image) -> np.ndarray:\n",
    "    \"\"\"Get normalized CLIP embedding for an image.\"\"\"\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(clip_model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    return features.cpu().numpy()[0]\n",
    "\n",
    "\n",
    "def get_text_embedding(text: str) -> np.ndarray:\n",
    "    \"\"\"Get normalized CLIP embedding for text.\"\"\"\n",
    "    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(clip_model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    return features.cpu().numpy()[0]\n",
    "\n",
    "\n",
    "def image_to_base64(image: Image.Image) -> str:\n",
    "    \"\"\"Convert PIL Image to base64.\"\"\"\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format=\"PNG\")\n",
    "    return base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "\n",
    "def base64_to_image(b64: str) -> Image.Image:\n",
    "    \"\"\"Convert base64 to PIL Image.\"\"\"\n",
    "    return Image.open(BytesIO(base64.b64decode(b64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(\n",
    "    text_query: str,\n",
    "    image_query: Image.Image,\n",
    "    collection,\n",
    "    text_weight: float = 0.7,\n",
    "    n_results: int = 5\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search using both text and image queries with weighted combination.\n",
    "    \n",
    "    Args:\n",
    "        text_query: Natural language query\n",
    "        image_query: Image to match\n",
    "        collection: ChromaDB collection\n",
    "        text_weight: Weight for text query (image_weight = 1 - text_weight)\n",
    "        n_results: Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of matching images with metadata and scores\n",
    "    \"\"\"\n",
    "    # Get embeddings\n",
    "    text_emb = get_text_embedding(text_query)\n",
    "    image_emb = get_image_embedding(image_query)\n",
    "    \n",
    "    # Weighted combination\n",
    "    image_weight = 1.0 - text_weight\n",
    "    combined_emb = text_weight * text_emb + image_weight * image_emb\n",
    "    \n",
    "    # Normalize the combined embedding\n",
    "    combined_emb = combined_emb / np.linalg.norm(combined_emb)\n",
    "    \n",
    "    # Search\n",
    "    results = collection.query(\n",
    "        query_embeddings=[combined_emb.tolist()],\n",
    "        n_results=n_results,\n",
    "        include=['metadatas', 'distances']\n",
    "    )\n",
    "    \n",
    "    # Format results\n",
    "    formatted = []\n",
    "    for i in range(len(results['ids'][0])):\n",
    "        meta = results['metadatas'][0][i].copy()\n",
    "        img_b64 = meta.pop('image_b64', None)\n",
    "        \n",
    "        result = {\n",
    "            'id': results['ids'][0][i],\n",
    "            'similarity': 1 - results['distances'][0][i],\n",
    "            'metadata': meta\n",
    "        }\n",
    "        \n",
    "        if img_b64:\n",
    "            result['image'] = base64_to_image(img_b64)\n",
    "        \n",
    "        formatted.append(result)\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "print(\"hybrid_search() function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test dataset\n",
    "from PIL import ImageDraw\n",
    "\n",
    "def create_shape(shape: str, color: str) -> Image.Image:\n",
    "    img = Image.new('RGB', (224, 224), 'white')\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    if shape == 'circle':\n",
    "        draw.ellipse([50, 50, 174, 174], fill=color)\n",
    "    elif shape == 'square':\n",
    "        draw.rectangle([50, 50, 174, 174], fill=color)\n",
    "    elif shape == 'triangle':\n",
    "        draw.polygon([(112, 50), (50, 174), (174, 174)], fill=color)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Create dataset\n",
    "shapes = ['circle', 'square', 'triangle']\n",
    "colors = ['red', 'blue', 'green', 'yellow', 'purple', 'orange']\n",
    "\n",
    "dataset = []\n",
    "for shape in shapes:\n",
    "    for color in colors:\n",
    "        img = create_shape(shape, color)\n",
    "        dataset.append({\n",
    "            'image': img,\n",
    "            'shape': shape,\n",
    "            'color': color\n",
    "        })\n",
    "\n",
    "print(f\"Created {len(dataset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index the dataset\n",
    "client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "\n",
    "try:\n",
    "    client.delete_collection(\"hybrid_demo\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "collection = client.create_collection(\n",
    "    name=\"hybrid_demo\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "print(\"Indexing images...\")\n",
    "for i, item in enumerate(dataset):\n",
    "    emb = get_image_embedding(item['image'])\n",
    "    collection.add(\n",
    "        ids=[f\"img_{i}\"],\n",
    "        embeddings=[emb.tolist()],\n",
    "        metadatas=[{\n",
    "            'shape': item['shape'],\n",
    "            'color': item['color'],\n",
    "            'image_b64': image_to_base64(item['image'])\n",
    "        }]\n",
    "    )\n",
    "\n",
    "print(f\"Indexed {collection.count()} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hybrid search!\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a query image (a pink circle - not in our dataset)\n",
    "query_image = create_shape('circle', 'pink')\n",
    "\n",
    "# Text query for something red\n",
    "text_query = \"something red\"\n",
    "\n",
    "print(f\"Query image: pink circle\")\n",
    "print(f\"Text query: '{text_query}'\")\n",
    "print(\"\\nThis should find red circles (matching both text 'red' and image 'circle')\")\n",
    "\n",
    "# Test with different weights\n",
    "weights = [0.0, 0.3, 0.5, 0.7, 1.0]\n",
    "\n",
    "fig, axes = plt.subplots(len(weights), 6, figsize=(12, 10))\n",
    "\n",
    "for row, w in enumerate(weights):\n",
    "    results = hybrid_search(text_query, query_image, collection, text_weight=w, n_results=5)\n",
    "    \n",
    "    axes[row, 0].set_ylabel(f\"text={w:.1f}\\nimg={1-w:.1f}\", fontsize=10)\n",
    "    axes[row, 0].imshow(query_image)\n",
    "    axes[row, 0].set_title(\"Query\", fontsize=8)\n",
    "    axes[row, 0].axis('off')\n",
    "    \n",
    "    for col, r in enumerate(results, start=1):\n",
    "        if col < 6:\n",
    "            axes[row, col].imshow(r['image'])\n",
    "            axes[row, col].set_title(f\"{r['metadata']['color']}\\n{r['metadata']['shape']}\", fontsize=8)\n",
    "            axes[row, col].axis('off')\n",
    "\n",
    "plt.suptitle(f\"Hybrid Search: '{text_query}' + pink circle image\", fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the results\n",
    "print(\"\\nAnalysis of weight effects:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"\\n- text_weight=0.0 (100% image): Finds circles of any color\")\n",
    "print(\"- text_weight=0.5 (balanced): Finds red + circles\")\n",
    "print(\"- text_weight=1.0 (100% text): Finds red shapes of any type\")\n",
    "print(\"\\nThe red circle should rank highest with balanced weights!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del clip_model, clip_processor\n",
    "clear_gpu_memory()\n",
    "print(\"Solutions notebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}