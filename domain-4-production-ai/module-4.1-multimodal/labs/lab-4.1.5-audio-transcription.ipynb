{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.1.5: Audio Transcription with Whisper\n",
    "\n",
    "**Module:** 4.1 - Multimodal AI  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐⭐\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand how Whisper transcription works\n",
    "- [ ] Transcribe audio files with high accuracy\n",
    "- [ ] Handle multiple languages and accents\n",
    "- [ ] Build an Audio Q&A pipeline with LLMs\n",
    "- [ ] Process audio in real-time scenarios\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Tasks 14.1-14.4\n",
    "- Knowledge of: Basic audio concepts, LLMs\n",
    "- Running in: NGC PyTorch container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "Audio transcription powers many applications we use daily:\n",
    "\n",
    "**Industry Applications:**\n",
    "- **Meeting Notes**: Automatic transcription of video calls\n",
    "- **Podcasts**: Generate searchable transcripts and summaries\n",
    "- **Customer Service**: Transcribe and analyze support calls\n",
    "- **Medical**: Dictation to medical records\n",
    "- **Legal**: Court and deposition transcription\n",
    "- **Accessibility**: Real-time captions for hearing impaired\n",
    "\n",
    "**Why DGX Spark?**\n",
    "- Whisper large-v3: ~4GB - runs with room to spare\n",
    "- Process hours of audio without cloud costs\n",
    "- Keep sensitive recordings on-premise\n",
    "- Combine with LLM for audio Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: How Does Whisper Work?\n",
    "\n",
    "> **Imagine you're a super-powered listener at a party:**\n",
    ">\n",
    "> 1. **Hear the Sound** (Audio Input): Someone speaks, and you hear the sound waves\n",
    ">\n",
    "> 2. **Break It Down** (Mel Spectrogram): You mentally \"see\" the sound as a picture - high notes look different from low notes, loud from quiet\n",
    ">\n",
    "> 3. **Recognize Patterns** (Encoder): Your trained brain recognizes these sound patterns - \"That's an 'S' sound, that's an 'AH' sound...\"\n",
    ">\n",
    "> 4. **Form Words** (Decoder): You put the sounds together into words, words into sentences\n",
    ">\n",
    "> 5. **Write It Down** (Text Output): You transcribe what you heard!\n",
    ">\n",
    "> **In AI terms:**\n",
    "> - **Audio → Mel Spectrogram**: Convert waveform to time-frequency representation\n",
    "> - **Encoder**: Transformer that processes audio features\n",
    "> - **Decoder**: Autoregressive transformer that generates text tokens\n",
    "> - **Multilingual**: Trained on 680,000 hours of audio in 98 languages!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages (run once)\n# !pip install openai-whisper soundfile librosa scipy -q"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport gc\nimport numpy as np\nimport librosa\nimport matplotlib.pyplot as plt\nimport time\nfrom typing import Dict, List, Optional, Tuple\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Check GPU\nprint(\"=\" * 50)\nprint(\"GPU Configuration\")\nprint(\"=\" * 50)\n\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"GPU: {gpu_name}\")\n    print(f\"Total Memory: {total_memory:.1f} GB\")\nelse:\n    print(\"No GPU available - Whisper will run on CPU (slower)\")\n\nprint(f\"\\nPyTorch: {torch.__version__}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    print(\"GPU memory cleared!\")\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        return f\"Allocated: {allocated:.2f}GB\"\n",
    "    return \"No GPU\"\n",
    "\n",
    "print(\"Utility functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Creating Sample Audio\n",
    "\n",
    "Let's create some sample audio to work with. We'll use text-to-speech to generate test audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_sine_wave(frequency: float, duration: float, sample_rate: int = 16000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate a sine wave tone.\n",
    "    \n",
    "    Args:\n",
    "        frequency: Frequency in Hz\n",
    "        duration: Duration in seconds\n",
    "        sample_rate: Sample rate in Hz\n",
    "        \n",
    "    Returns:\n",
    "        Audio array\n",
    "    \"\"\"\n",
    "    t = np.linspace(0, duration, int(sample_rate * duration), endpoint=False)\n",
    "    return np.sin(2 * np.pi * frequency * t).astype(np.float32)\n",
    "\n",
    "def generate_dtmf_tone(digit: str, duration: float = 0.5, sample_rate: int = 16000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate a DTMF (phone dial) tone.\n",
    "    \n",
    "    Args:\n",
    "        digit: Phone keypad digit (0-9, *, #)\n",
    "        duration: Duration in seconds\n",
    "        sample_rate: Sample rate\n",
    "        \n",
    "    Returns:\n",
    "        Audio array\n",
    "    \"\"\"\n",
    "    # DTMF frequency pairs\n",
    "    dtmf_freqs = {\n",
    "        '1': (697, 1209), '2': (697, 1336), '3': (697, 1477),\n",
    "        '4': (770, 1209), '5': (770, 1336), '6': (770, 1477),\n",
    "        '7': (852, 1209), '8': (852, 1336), '9': (852, 1477),\n",
    "        '*': (941, 1209), '0': (941, 1336), '#': (941, 1477)\n",
    "    }\n",
    "    \n",
    "    if digit not in dtmf_freqs:\n",
    "        return np.zeros(int(sample_rate * duration), dtype=np.float32)\n",
    "    \n",
    "    low_freq, high_freq = dtmf_freqs[digit]\n",
    "    t = np.linspace(0, duration, int(sample_rate * duration), endpoint=False)\n",
    "    \n",
    "    signal = np.sin(2 * np.pi * low_freq * t) + np.sin(2 * np.pi * high_freq * t)\n",
    "    return (signal * 0.5).astype(np.float32)\n",
    "\n",
    "print(\"Audio generation functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some test audio\n",
    "sample_rate = 16000  # Whisper expects 16kHz\n",
    "\n",
    "# Create a simple tone sequence\n",
    "audio_data = np.concatenate([\n",
    "    generate_sine_wave(440, 0.5, sample_rate),  # A4 note\n",
    "    np.zeros(int(sample_rate * 0.1), dtype=np.float32),  # Silence\n",
    "    generate_sine_wave(523, 0.5, sample_rate),  # C5 note\n",
    "    np.zeros(int(sample_rate * 0.1), dtype=np.float32),\n",
    "    generate_sine_wave(659, 0.5, sample_rate),  # E5 note\n",
    "])\n",
    "\n",
    "print(f\"Generated {len(audio_data) / sample_rate:.2f} seconds of audio\")\n",
    "print(f\"Sample rate: {sample_rate} Hz\")\n",
    "print(f\"Audio shape: {audio_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize the audio\nfig, axes = plt.subplots(2, 1, figsize=(12, 6))\n\n# Waveform\ntime_axis = np.arange(len(audio_data)) / sample_rate\naxes[0].plot(time_axis, audio_data, linewidth=0.5)\naxes[0].set_xlabel('Time (s)')\naxes[0].set_ylabel('Amplitude')\naxes[0].set_title('Audio Waveform')\naxes[0].grid(True, alpha=0.3)\n\n# Spectrogram (what Whisper \"sees\")\n# nperseg=512 gives ~32ms windows at 16kHz, good balance of time/frequency resolution\nfrom scipy import signal\n\nfrequencies, times, Sxx = signal.spectrogram(audio_data, sample_rate, nperseg=512)\naxes[1].pcolormesh(times, frequencies, 10 * np.log10(Sxx + 1e-10), shading='gouraud', cmap='viridis')\naxes[1].set_xlabel('Time (s)')\naxes[1].set_ylabel('Frequency (Hz)')\naxes[1].set_title('Spectrogram (What Whisper Sees)')\naxes[1].set_ylim(0, 2000)  # Human speech is mostly below 2kHz\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Loading Whisper\n",
    "\n",
    "We'll use OpenAI's Whisper model for transcription. Let's load the large-v3 model for best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "print(\"Loading Whisper large-v3...\")\n",
    "print(f\"Memory before: {get_memory_usage()}\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the model (downloads on first run)\n",
    "# Options: tiny, base, small, medium, large, large-v2, large-v3\n",
    "whisper_model = whisper.load_model(\"large-v3\")\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"\\nWhisper loaded in {load_time:.1f} seconds!\")\n",
    "print(f\"Memory after: {get_memory_usage()}\")\n",
    "\n",
    "# Model info\n",
    "print(f\"\\nModel: large-v3\")\n",
    "print(f\"Parameters: ~1.5B\")\n",
    "print(f\"Languages: 98+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also try with the Hugging Face version for comparison\n",
    "# This gives us more flexibility for integration\n",
    "\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "print(\"Loading Whisper from Hugging Face...\")\n",
    "start_time = time.time()\n",
    "\n",
    "hf_whisper_processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v3\")\n",
    "hf_whisper_model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    \"openai/whisper-large-v3\",\n",
    "    torch_dtype=torch.bfloat16  # Optimized for Blackwell\n",
    ").to(\"cuda\")\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"\\nHF Whisper loaded in {load_time:.1f} seconds!\")\n",
    "print(f\"Memory after: {get_memory_usage()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Basic Transcription\n",
    "\n",
    "Let's transcribe some audio! Since we don't have real speech recordings, we'll work with synthetic examples and discuss the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(\n",
    "    audio: np.ndarray,\n",
    "    sample_rate: int = 16000,\n",
    "    language: Optional[str] = None,\n",
    "    task: str = \"transcribe\"\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Transcribe audio using Whisper.\n",
    "    \n",
    "    Args:\n",
    "        audio: Audio data as numpy array\n",
    "        sample_rate: Sample rate of audio\n",
    "        language: Language code (e.g., 'en', 'es', 'zh') or None for auto-detect\n",
    "        task: 'transcribe' or 'translate' (translate to English)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with transcription results\n",
    "    \"\"\"\n",
    "    # Resample if needed\n",
    "    if sample_rate != 16000:\n",
    "        import librosa\n",
    "        audio = librosa.resample(audio, orig_sr=sample_rate, target_sr=16000)\n",
    "    \n",
    "    # Ensure audio is float32 and normalized\n",
    "    audio = audio.astype(np.float32)\n",
    "    if audio.max() > 1.0:\n",
    "        audio = audio / np.abs(audio).max()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Transcribe using OpenAI Whisper\n",
    "    result = whisper_model.transcribe(\n",
    "        audio,\n",
    "        language=language,\n",
    "        task=task,\n",
    "        fp16=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    transcribe_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'text': result['text'],\n",
    "        'language': result['language'],\n",
    "        'segments': result['segments'],\n",
    "        'duration': len(audio) / 16000,\n",
    "        'transcribe_time': transcribe_time,\n",
    "        'rtf': transcribe_time / (len(audio) / 16000)  # Real-time factor\n",
    "    }\n",
    "\n",
    "print(\"transcribe_audio() function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we don't have real speech, let's demonstrate the API\n",
    "# Whisper will detect that our tone sequence isn't speech\n",
    "\n",
    "print(\"Attempting to transcribe tone sequence...\")\n",
    "result = transcribe_audio(audio_data)\n",
    "\n",
    "print(f\"\\nTranscription: '{result['text']}'\")\n",
    "print(f\"Detected language: {result['language']}\")\n",
    "print(f\"Audio duration: {result['duration']:.2f}s\")\n",
    "print(f\"Transcription time: {result['transcribe_time']:.2f}s\")\n",
    "print(f\"Real-time factor: {result['rtf']:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Real Audio Files\n",
    "\n",
    "In practice, you would load audio from files. Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "def load_audio(file_path: str, target_sr: int = 16000) -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"\n",
    "    Load audio from a file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to audio file (wav, mp3, flac, etc.)\n",
    "        target_sr: Target sample rate\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (audio_data, sample_rate)\n",
    "    \"\"\"\n",
    "    # Load with librosa (handles many formats)\n",
    "    audio, sr = librosa.load(file_path, sr=target_sr, mono=True)\n",
    "    return audio, sr\n",
    "\n",
    "def save_audio(file_path: str, audio: np.ndarray, sample_rate: int = 16000):\n",
    "    \"\"\"\n",
    "    Save audio to a file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Output file path\n",
    "        audio: Audio data\n",
    "        sample_rate: Sample rate\n",
    "    \"\"\"\n",
    "    sf.write(file_path, audio, sample_rate)\n",
    "\n",
    "# Example usage (commented since we don't have a file)\n",
    "# audio, sr = load_audio(\"path/to/audio.wav\")\n",
    "# result = transcribe_audio(audio, sr)\n",
    "# print(result['text'])\n",
    "\n",
    "print(\"Audio I/O functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Whisper with Hugging Face Transformers\n",
    "\n",
    "The Hugging Face version provides more flexibility for integration with other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def transcribe_hf(\n    audio: np.ndarray,\n    sample_rate: int = 16000,\n    language: Optional[str] = None,\n    return_timestamps: bool = False\n) -> Dict:\n    \"\"\"\n    Transcribe audio using Hugging Face Whisper.\n    \n    Args:\n        audio: Audio data as numpy array\n        sample_rate: Sample rate of audio\n        language: Language code (e.g., 'en', 'es')\n        return_timestamps: Whether to return word timestamps\n        \n    Returns:\n        Dictionary with transcription results\n    \"\"\"\n    # Resample if needed\n    if sample_rate != 16000:\n        import librosa\n        audio = librosa.resample(audio, orig_sr=sample_rate, target_sr=16000)\n    \n    # Process audio\n    input_features = hf_whisper_processor(\n        audio,\n        sampling_rate=16000,\n        return_tensors=\"pt\"\n    ).input_features.to(hf_whisper_model.device, dtype=torch.bfloat16)\n    \n    # Set language if specified\n    forced_decoder_ids = None\n    if language:\n        forced_decoder_ids = hf_whisper_processor.get_decoder_prompt_ids(\n            language=language,\n            task=\"transcribe\"\n        )\n    \n    start_time = time.time()\n    \n    # Generate transcription\n    with torch.inference_mode():\n        predicted_ids = hf_whisper_model.generate(\n            input_features,\n            forced_decoder_ids=forced_decoder_ids,\n            return_timestamps=return_timestamps,\n            max_new_tokens=448\n        )\n    \n    transcribe_time = time.time() - start_time\n    \n    # Decode\n    transcription = hf_whisper_processor.batch_decode(\n        predicted_ids,\n        skip_special_tokens=True\n    )[0]\n    \n    return {\n        'text': transcription,\n        'duration': len(audio) / 16000,\n        'transcribe_time': transcribe_time,\n        'rtf': transcribe_time / (len(audio) / 16000)\n    }\n\nprint(\"transcribe_hf() function ready!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test HF version with our tone sequence\n",
    "print(\"Testing Hugging Face Whisper...\")\n",
    "result_hf = transcribe_hf(audio_data)\n",
    "\n",
    "print(f\"\\nTranscription: '{result_hf['text']}'\")\n",
    "print(f\"Audio duration: {result_hf['duration']:.2f}s\")\n",
    "print(f\"Transcription time: {result_hf['transcribe_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Audio Q&A Pipeline\n",
    "\n",
    "Now let's build a complete pipeline that:\n",
    "1. Transcribes audio\n",
    "2. Uses an LLM to answer questions about the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class AudioQA:\n    \"\"\"\n    Audio Question-Answering Pipeline.\n    \n    Transcribes audio and answers questions using an LLM.\n    \"\"\"\n    \n    def __init__(self, whisper_model, whisper_processor):\n        \"\"\"\n        Initialize the Audio QA system.\n        \n        Args:\n            whisper_model: Loaded Whisper model\n            whisper_processor: Whisper processor\n        \"\"\"\n        self.whisper_model = whisper_model\n        self.whisper_processor = whisper_processor\n        self.llm = None\n        self.llm_tokenizer = None\n        self.transcripts = {}  # Store transcripts by ID\n        \n    def transcribe(self, audio: np.ndarray, audio_id: str, sample_rate: int = 16000) -> str:\n        \"\"\"\n        Transcribe audio and store the result.\n        \n        Args:\n            audio: Audio data\n            audio_id: Unique identifier for this audio\n            sample_rate: Sample rate\n            \n        Returns:\n            Transcription text\n        \"\"\"\n        # Resample if needed\n        if sample_rate != 16000:\n            import librosa\n            audio = librosa.resample(audio, orig_sr=sample_rate, target_sr=16000)\n        \n        # Process audio\n        input_features = self.whisper_processor(\n            audio,\n            sampling_rate=16000,\n            return_tensors=\"pt\"\n        ).input_features.to(self.whisper_model.device, dtype=torch.bfloat16)\n        \n        # Generate transcription\n        with torch.inference_mode():\n            predicted_ids = self.whisper_model.generate(\n                input_features,\n                max_new_tokens=448\n            )\n        \n        # Decode\n        transcription = self.whisper_processor.batch_decode(\n            predicted_ids,\n            skip_special_tokens=True\n        )[0]\n        \n        # Store\n        self.transcripts[audio_id] = {\n            'text': transcription,\n            'duration': len(audio) / 16000,\n            'timestamp': time.time()\n        }\n        \n        return transcription\n    \n    def load_llm(self):\n        \"\"\"Load the LLM for answering questions.\"\"\"\n        if self.llm is not None:\n            return\n            \n        from transformers import AutoTokenizer, AutoModelForCausalLM\n        \n        print(\"Loading LLM for Q&A...\")\n        model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n        \n        self.llm_tokenizer = AutoTokenizer.from_pretrained(model_id)\n        self.llm = AutoModelForCausalLM.from_pretrained(\n            model_id,\n            torch_dtype=torch.bfloat16,\n            device_map=\"auto\"\n        )\n        print(\"LLM loaded!\")\n    \n    def ask(self, question: str, audio_id: Optional[str] = None) -> Dict:\n        \"\"\"\n        Ask a question about transcribed audio.\n        \n        Args:\n            question: Question to ask\n            audio_id: Specific audio to query (None = all)\n            \n        Returns:\n            Dictionary with answer and sources\n        \"\"\"\n        if not self.transcripts:\n            return {'answer': \"No audio has been transcribed yet.\", 'sources': []}\n        \n        if self.llm is None:\n            self.load_llm()\n        \n        # Get relevant transcripts\n        if audio_id:\n            if audio_id not in self.transcripts:\n                return {'answer': f\"Audio '{audio_id}' not found.\", 'sources': []}\n            context = f\"Transcript from {audio_id}:\\n{self.transcripts[audio_id]['text']}\"\n            sources = [audio_id]\n        else:\n            context_parts = []\n            for aid, data in self.transcripts.items():\n                context_parts.append(f\"Transcript from {aid}:\\n{data['text']}\")\n            context = \"\\n\\n\".join(context_parts)\n            sources = list(self.transcripts.keys())\n        \n        # Create prompt\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on audio transcripts. Be concise and accurate.\"},\n            {\"role\": \"user\", \"content\": f\"Based on the following audio transcript(s), answer the question.\\n\\n{context}\\n\\nQuestion: {question}\"}\n        ]\n        \n        text = self.llm_tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        \n        inputs = self.llm_tokenizer(text, return_tensors=\"pt\").to(self.llm.device)\n        \n        with torch.inference_mode():\n            outputs = self.llm.generate(\n                **inputs,\n                max_new_tokens=300,\n                do_sample=True,\n                temperature=0.7\n            )\n        \n        response = self.llm_tokenizer.decode(\n            outputs[0][inputs.input_ids.shape[1]:],\n            skip_special_tokens=True\n        )\n        \n        return {\n            'question': question,\n            'answer': response,\n            'sources': sources\n        }\n    \n    def summarize(self, audio_id: str) -> str:\n        \"\"\"\n        Summarize a transcript.\n        \n        Args:\n            audio_id: Audio to summarize\n            \n        Returns:\n            Summary text\n        \"\"\"\n        result = self.ask(f\"Summarize the main points from the transcript.\", audio_id)\n        return result['answer']\n    \n    def list_transcripts(self) -> List[Dict]:\n        \"\"\"List all transcribed audio.\"\"\"\n        return [\n            {'id': aid, 'duration': data['duration'], 'preview': data['text'][:100]}\n            for aid, data in self.transcripts.items()\n        ]\n\nprint(\"AudioQA class defined!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample transcript manually (since we don't have speech audio)\n",
    "# In practice, this would come from actual transcription\n",
    "\n",
    "sample_transcript = \"\"\"\n",
    "Welcome to the quarterly earnings call for TechCorp. I'm the CEO, and today I'll discuss our Q4 results.\n",
    "\n",
    "Revenue reached 3.4 million dollars, up 23% from the previous quarter. Our AI services division \n",
    "saw particularly strong growth, contributing 60% of total revenue. We're excited about the \n",
    "upcoming product launch in March.\n",
    "\n",
    "Operating expenses were well controlled at 1.8 million, giving us a healthy profit margin of 47%.\n",
    "We plan to invest heavily in R&D next year, focusing on multimodal AI capabilities.\n",
    "\n",
    "Questions from analysts:\n",
    "- Analyst: What's driving the AI services growth?\n",
    "- CEO: Enterprise adoption of our document processing solution has accelerated.\n",
    "- Analyst: Any concerns about competition?\n",
    "- CEO: We maintain our competitive edge through our unified platform approach.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the Audio QA system\n",
    "audio_qa = AudioQA(hf_whisper_model, hf_whisper_processor)\n",
    "\n",
    "# Manually add the sample transcript (simulating transcription)\n",
    "audio_qa.transcripts[\"earnings_call_q4\"] = {\n",
    "    'text': sample_transcript,\n",
    "    'duration': 120.0,  # Simulated 2 minutes\n",
    "    'timestamp': time.time()\n",
    "}\n",
    "\n",
    "print(\"Sample transcript added!\")\n",
    "print(f\"Transcripts: {audio_qa.list_transcripts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask questions about the transcript\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"AUDIO Q&A DEMO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "questions = [\n",
    "    \"What was the revenue in Q4?\",\n",
    "    \"What percentage of revenue came from AI services?\",\n",
    "    \"What is the company planning for next year?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    result = audio_qa.ask(q, \"earnings_call_q4\")\n",
    "    print(f\"A: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"TRANSCRIPT SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "summary = audio_qa.summarize(\"earnings_call_q4\")\n",
    "print(f\"\\n{summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Whisper Model Comparison\n",
    "\n",
    "Let's understand the different Whisper model sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whisper model comparison\n",
    "model_info = [\n",
    "    (\"tiny\", \"39M\", \"~1GB\", \"Fastest, basic accuracy\"),\n",
    "    (\"base\", \"74M\", \"~1GB\", \"Good for quick transcription\"),\n",
    "    (\"small\", \"244M\", \"~2GB\", \"Balanced speed/accuracy\"),\n",
    "    (\"medium\", \"769M\", \"~5GB\", \"High accuracy\"),\n",
    "    (\"large\", \"1.55B\", \"~10GB\", \"Best English accuracy\"),\n",
    "    (\"large-v2\", \"1.55B\", \"~10GB\", \"Improved multilingual\"),\n",
    "    (\"large-v3\", \"1.55B\", \"~10GB\", \"State-of-the-art\")\n",
    "]\n",
    "\n",
    "print(\"Whisper Model Comparison\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Model':<12} {'Params':<10} {'VRAM':<10} {'Notes'}\")\n",
    "print(\"-\" * 70)\n",
    "for model, params, vram, notes in model_info:\n",
    "    print(f\"{model:<12} {params:<10} {vram:<10} {notes}\")\n",
    "print(\"\\n* DGX Spark can easily run large-v3 with room to spare!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Build a Meeting Transcription System\n",
    "\n",
    "Create a system that:\n",
    "1. Accepts audio files (simulated or real)\n",
    "2. Transcribes them\n",
    "3. Extracts action items and key decisions\n",
    "4. Generates meeting minutes\n",
    "\n",
    "<details>\n",
    "<summary>Hint: Action Item Extraction</summary>\n",
    "\n",
    "```python\n",
    "def extract_action_items(transcript: str) -> List[str]:\n",
    "    prompt = f\"\"\"\n",
    "    Extract all action items from this meeting transcript.\n",
    "    Format as a numbered list.\n",
    "    \n",
    "    Transcript:\n",
    "    {transcript}\n",
    "    \"\"\"\n",
    "    # Use the LLM to extract...\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Build your meeting transcription system!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Wrong Sample Rate\n",
    "\n",
    "```python\n",
    "# Wrong - passing audio at wrong sample rate\n",
    "audio, sr = load_audio(\"file.wav\")  # sr = 44100\n",
    "result = whisper.transcribe(audio)  # Expects 16kHz!\n",
    "\n",
    "# Right - resample to 16kHz\n",
    "audio, sr = load_audio(\"file.wav\")\n",
    "audio_16k = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
    "result = whisper.transcribe(audio_16k)  # Correct!\n",
    "```\n",
    "\n",
    "### Mistake 2: Audio Too Long\n",
    "\n",
    "```python\n",
    "# Wrong - feeding entire 2-hour audio\n",
    "result = whisper.transcribe(long_audio)  # May OOM or lose context\n",
    "\n",
    "# Right - chunk the audio\n",
    "chunk_length = 30 * 16000  # 30 seconds\n",
    "for i in range(0, len(long_audio), chunk_length):\n",
    "    chunk = long_audio[i:i+chunk_length]\n",
    "    result = whisper.transcribe(chunk)\n",
    "    # Combine results...\n",
    "```\n",
    "\n",
    "### Mistake 3: Ignoring Language Setting\n",
    "\n",
    "```python\n",
    "# Wrong - letting Whisper guess language for known content\n",
    "result = whisper.transcribe(spanish_audio)  # Might misdetect\n",
    "\n",
    "# Right - specify language when known\n",
    "result = whisper.transcribe(spanish_audio, language=\"es\")  # Accurate!\n",
    "```\n",
    "\n",
    "### Mistake 4: Not Handling Stereo Audio\n",
    "\n",
    "```python\n",
    "# Wrong - stereo audio (2 channels)\n",
    "audio = load_stereo_audio()  # Shape: (2, samples)\n",
    "result = whisper.transcribe(audio)  # May fail\n",
    "\n",
    "# Right - convert to mono\n",
    "audio = audio.mean(axis=0)  # Average channels\n",
    "result = whisper.transcribe(audio)  # Works!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- How Whisper transcription works\n",
    "- How to transcribe audio with different model sizes\n",
    "- How to use both OpenAI Whisper and Hugging Face versions\n",
    "- How to build an Audio Q&A pipeline\n",
    "- How to combine transcription with LLM analysis\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Whisper is multilingual**: Works on 98+ languages out of the box\n",
    "2. **Sample rate matters**: Always use 16kHz for Whisper\n",
    "3. **Model size trade-offs**: Larger = more accurate but slower\n",
    "4. **Combine with LLMs**: Transcription + LLM = powerful Q&A system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "### Build a Real-Time Transcription System\n",
    "\n",
    "Create a system that:\n",
    "1. Processes audio in chunks (simulating streaming)\n",
    "2. Transcribes each chunk\n",
    "3. Maintains context across chunks\n",
    "4. Provides real-time text output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE\n",
    "\n",
    "class StreamingTranscriber:\n",
    "    \"\"\"\n",
    "    Real-time streaming transcription.\n",
    "    \n",
    "    Processes audio in chunks and provides incremental transcription.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size_seconds: float = 5.0):\n",
    "        \"\"\"\n",
    "        Initialize the streaming transcriber.\n",
    "        \n",
    "        Args:\n",
    "            chunk_size_seconds: Size of each audio chunk\n",
    "        \"\"\"\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "    \n",
    "    def process_chunk(self, audio_chunk: np.ndarray) -> str:\n",
    "        \"\"\"\n",
    "        Process a single audio chunk.\n",
    "        \n",
    "        Args:\n",
    "            audio_chunk: Audio data for this chunk\n",
    "            \n",
    "        Returns:\n",
    "            Transcription for this chunk\n",
    "        \"\"\"\n",
    "        # TODO: Implement\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Whisper Paper](https://arxiv.org/abs/2212.04356)\n",
    "- [OpenAI Whisper GitHub](https://github.com/openai/whisper)\n",
    "- [Hugging Face Whisper](https://huggingface.co/openai/whisper-large-v3)\n",
    "- [Faster Whisper (CTranslate2)](https://github.com/guillaumekln/faster-whisper)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "if 'whisper_model' in dir():\n",
    "    del whisper_model\n",
    "if 'hf_whisper_model' in dir():\n",
    "    del hf_whisper_model\n",
    "    del hf_whisper_processor\n",
    "if 'audio_qa' in dir():\n",
    "    if audio_qa.llm is not None:\n",
    "        del audio_qa.llm\n",
    "        del audio_qa.llm_tokenizer\n",
    "    del audio_qa\n",
    "\n",
    "clear_gpu_memory()\n",
    "print(f\"Final memory state: {get_memory_usage()}\")\n",
    "print(\"\\nModule 14 complete! Congratulations!\")\n",
    "print(\"\\nYou've learned:\")\n",
    "print(\"  - Vision-Language Models (LLaVA, Qwen-VL)\")\n",
    "print(\"  - Image Generation (SDXL, ControlNet, Flux)\")\n",
    "print(\"  - Multimodal RAG (CLIP + ChromaDB)\")\n",
    "print(\"  - Document AI (VLM-based extraction)\")\n",
    "print(\"  - Audio Transcription (Whisper + LLM Q&A)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}