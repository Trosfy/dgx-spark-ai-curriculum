{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.1.2: Image Generation with Diffusion Models\n",
    "\n",
    "**Module:** 4.1 - Multimodal AI  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐⭐\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand how diffusion models generate images\n",
    "- [ ] Use SDXL to generate high-quality images from text\n",
    "- [ ] Apply ControlNet for guided image generation\n",
    "- [ ] Explore Flux for state-of-the-art image quality\n",
    "- [ ] Customize generation with different samplers and parameters\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Lab 4.1.1 (Vision-Language Demo)\n",
    "- Knowledge of: Neural networks, basic image processing\n",
    "- Running in: NGC PyTorch container with diffusers installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "AI image generation is revolutionizing creative industries:\n",
    "\n",
    "**Industry Applications:**\n",
    "- **Marketing**: Generate product mockups and advertisements\n",
    "- **Gaming**: Create concept art and textures\n",
    "- **Architecture**: Visualize building designs\n",
    "- **Fashion**: Design clothing patterns and styles\n",
    "- **Film**: Storyboarding and concept visualization\n",
    "\n",
    "**Why DGX Spark?**\n",
    "- SDXL: ~8GB VRAM - generates in 5-8 seconds\n",
    "- Flux.1-dev: ~24GB - highest quality, ~15-20 seconds\n",
    "- No cloud API costs - generate unlimited images locally!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: How Do Diffusion Models Work?\n",
    "\n",
    "> **Imagine you're playing a game of \"guess the drawing\" but in reverse!**\n",
    ">\n",
    "> Normally, someone draws a cat, and you guess \"cat!\" But diffusion models learned to play it backwards:\n",
    ">\n",
    "> 1. **Training**: You show the AI a picture of a cat, then slowly add TV static (noise) until it's ALL static. The AI learns: \"To get from this static to a cat, I need to remove THIS specific pattern of noise.\"\n",
    ">\n",
    "> 2. **Generation**: When you say \"draw me a cat!\", the AI starts with pure static (random noise), then removes noise step by step, guided by the word \"cat\", until a cat appears!\n",
    ">\n",
    "> Think of it like a sculptor: you start with a block of marble (random noise) and chip away (denoise) until a statue (image) emerges.\n",
    ">\n",
    "> **In AI terms:**\n",
    "> - **Forward process**: Add Gaussian noise to images gradually (training data)\n",
    "> - **Reverse process**: Learn to predict and remove noise step by step\n",
    "> - **Conditioning**: Text embeddings guide which noise patterns to remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages (run once)\n# !pip install diffusers>=0.27.0 transformers accelerate matplotlib -q\n# !pip install controlnet_aux opencv-python -q"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport gc\nfrom PIL import Image\nimport numpy as np\nimport time\nfrom typing import Optional, List\nimport warnings\nfrom IPython.display import display\nimport matplotlib.pyplot as plt  # For displaying image grids\nwarnings.filterwarnings('ignore')\n\n# Check GPU\nprint(\"=\" * 50)\nprint(\"GPU Configuration\")\nprint(\"=\" * 50)\n\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"GPU: {gpu_name}\")\n    print(f\"Total Memory: {total_memory:.1f} GB\")\n    print(f\"\\nDGX Spark has plenty of room for image generation!\")\nelse:\n    print(\"No GPU available - generation will be very slow!\")"
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Key Libraries for This Lab\n\n### Matplotlib for Image Display\n\nWe'll use matplotlib to display multiple images in grids:\n\n```python\nimport matplotlib.pyplot as plt\n\n# Create a grid of subplots (rows x cols)\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))  # 2 rows, 3 columns\n\n# axes is a 2D array - flatten for easy iteration\nfor ax, img in zip(axes.flatten(), images):\n    ax.imshow(img)           # Display image\n    ax.axis('off')           # Hide axis ticks\n    ax.set_title(\"Title\")    # Add title\n\nplt.tight_layout()  # Adjust spacing\nplt.show()\n```\n\n### OpenCV for Image Processing\n\nOpenCV (cv2) provides powerful image processing functions:\n\n```python\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\n# Convert PIL Image to numpy array for OpenCV\nimg_array = np.array(pil_image)\n\n# Convert RGB to grayscale\ngray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n\n# Canny edge detection - finds edges in images\n# low_threshold: edges with gradient below this are discarded\n# high_threshold: edges with gradient above this are kept\nedges = cv2.Canny(gray, low_threshold=100, high_threshold=200)\n\n# Convert back to RGB for display (edges are single channel)\nedges_rgb = np.stack([edges, edges, edges], axis=-1)\nresult = Image.fromarray(edges_rgb)\n```\n\n### PIL ImageDraw for Creating Images\n\nWe'll also use PIL's ImageDraw (covered in Lab 4.1.1) to create control images:\n\n```python\nfrom PIL import Image, ImageDraw\n\nimg = Image.new('RGB', (1024, 1024), color='white')\ndraw = ImageDraw.Draw(img)\ndraw.rectangle([x1, y1, x2, y2], outline='black', width=5)\ndraw.polygon([(x1,y1), (x2,y2), (x3,y3)], outline='black')\ndraw.ellipse([x1, y1, x2, y2], outline='black')\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory cache.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    print(\"GPU memory cleared!\")\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "        return f\"Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\"\n",
    "    return \"No GPU\"\n",
    "\n",
    "def display_images_grid(images: List[Image.Image], titles: Optional[List[str]] = None, cols: int = 2):\n",
    "    \"\"\"Display multiple images in a grid.\"\"\"\n",
    "    from IPython.display import display\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    rows = (len(images) + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 5*rows))\n",
    "    axes = axes.flatten() if hasattr(axes, 'flatten') else [axes]\n",
    "    \n",
    "    for idx, (ax, img) in enumerate(zip(axes, images)):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        if titles and idx < len(titles):\n",
    "            ax.set_title(titles[idx], fontsize=10)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(images), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Utility functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding the Diffusion Pipeline\n",
    "\n",
    "### The Components\n",
    "\n",
    "```\n",
    "┌──────────────┐    ┌───────────────┐    ┌─────────────┐    ┌──────────────┐\n",
    "│ Text Prompt  │───▶│ Text Encoder  │───▶│   U-Net     │───▶│    VAE       │\n",
    "│              │    │  (CLIP/T5)    │    │  Denoiser   │    │   Decoder    │\n",
    "└──────────────┘    └───────────────┘    └─────────────┘    └──────────────┘\n",
    "                                              ▲\n",
    "                                              │\n",
    "                                    ┌─────────────────┐\n",
    "                                    │  Noise/Latents  │\n",
    "                                    │   (Random)      │\n",
    "                                    └─────────────────┘\n",
    "```\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "| Parameter | What It Does | Typical Range |\n",
    "|-----------|--------------|---------------|\n",
    "| `num_inference_steps` | How many denoising steps | 20-50 |\n",
    "| `guidance_scale` | How closely to follow the prompt | 5.0-15.0 |\n",
    "| `negative_prompt` | What to avoid in the image | Text string |\n",
    "| `seed` | Controls randomness for reproducibility | Any integer |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Stable Diffusion XL (SDXL)\n",
    "\n",
    "SDXL is a powerful open-source model that generates high-quality 1024x1024 images.\n",
    "\n",
    "### Why SDXL?\n",
    "- **High resolution**: Native 1024x1024 output\n",
    "- **Two-stage**: Base + Refiner for extra detail\n",
    "- **Fast**: 5-8 seconds on DGX Spark\n",
    "- **Flexible**: Works with many LoRAs and ControlNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline, DPMSolverMultistepScheduler\n",
    "\n",
    "clear_gpu_memory()\n",
    "\n",
    "print(\"Loading SDXL Base model...\")\n",
    "print(f\"Memory before: {get_memory_usage()}\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Load SDXL pipeline\n",
    "sdxl_pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.bfloat16,  # bfloat16 for Blackwell\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\"  # Use fp16 weights (converted to bf16)\n",
    ")\n",
    "\n",
    "# Use a faster scheduler\n",
    "sdxl_pipe.scheduler = DPMSolverMultistepScheduler.from_config(sdxl_pipe.scheduler.config)\n",
    "\n",
    "# Move to GPU\n",
    "sdxl_pipe = sdxl_pipe.to(\"cuda\")\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"\\nSDXL loaded in {load_time:.1f} seconds!\")\n",
    "print(f\"Memory after: {get_memory_usage()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(\n",
    "    prompt: str,\n",
    "    negative_prompt: str = \"ugly, blurry, low quality, distorted\",\n",
    "    num_steps: int = 30,\n",
    "    guidance_scale: float = 7.5,\n",
    "    seed: Optional[int] = None,\n",
    "    width: int = 1024,\n",
    "    height: int = 1024\n",
    ") -> Image.Image:\n",
    "    \"\"\"\n",
    "    Generate an image using SDXL.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Text description of desired image\n",
    "        negative_prompt: What to avoid in the image\n",
    "        num_steps: Number of denoising steps (more = higher quality but slower)\n",
    "        guidance_scale: How closely to follow the prompt (higher = more literal)\n",
    "        seed: Random seed for reproducibility\n",
    "        width: Output width in pixels\n",
    "        height: Output height in pixels\n",
    "        \n",
    "    Returns:\n",
    "        Generated PIL Image\n",
    "    \"\"\"\n",
    "    # Set seed for reproducibility\n",
    "    if seed is not None:\n",
    "        generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "    else:\n",
    "        generator = None\n",
    "        seed = torch.randint(0, 2**32, (1,)).item()\n",
    "    \n",
    "    print(f\"Generating with seed: {seed}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate image\n",
    "    with torch.inference_mode():\n",
    "        result = sdxl_pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_inference_steps=num_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            generator=generator,\n",
    "            width=width,\n",
    "            height=height\n",
    "        )\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"Generated in {generation_time:.1f} seconds\")\n",
    "    \n",
    "    return result.images[0]\n",
    "\n",
    "print(\"generate_image() function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate our first image!\n",
    "prompt = \"A majestic mountain landscape at sunset, golden hour lighting, \\\n",
    "photorealistic, 8k resolution, dramatic clouds, peaceful lake reflection\"\n",
    "\n",
    "image1 = generate_image(prompt, seed=42)\n",
    "display(image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with different styles\n",
    "prompts = [\n",
    "    \"A cyberpunk city at night, neon lights, rain, blade runner style, cinematic\",\n",
    "    \"A serene Japanese garden with cherry blossoms, koi pond, peaceful, watercolor style\",\n",
    "    \"Portrait of a wise wizard, long white beard, magical staff, fantasy art, detailed\",\n",
    "    \"Futuristic spaceship interior, sci-fi, clean design, blue holographic displays\"\n",
    "]\n",
    "\n",
    "print(\"Generating 4 images with different styles...\")\n",
    "print(\"This will take about 30 seconds...\\n\")\n",
    "\n",
    "images = []\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"\\n[{i+1}/4] {prompt[:50]}...\")\n",
    "    img = generate_image(prompt, seed=42+i, num_steps=25)\n",
    "    images.append(img)\n",
    "\n",
    "# Display in grid\n",
    "display_images_grid(images, [p[:30] + \"...\" for p in prompts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "For each image:\n",
    "1. **Text Encoding**: CLIP encoded your prompt into embeddings (what the image should look like)\n",
    "2. **Noise Generation**: Random latent noise was created (starting point)\n",
    "3. **Iterative Denoising**: The U-Net predicted and removed noise 25 times, guided by text embeddings\n",
    "4. **VAE Decoding**: The final latents were decoded to a full-resolution image\n",
    "\n",
    "The `seed` parameter controls the initial noise, so the same seed + prompt = same image!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Prompt Engineering for Better Results\n",
    "\n",
    "### The Art of Prompting\n",
    "\n",
    "Great prompts follow a structure:\n",
    "\n",
    "```\n",
    "[Subject] + [Style] + [Details] + [Quality Boosters]\n",
    "```\n",
    "\n",
    "**Examples:**\n",
    "- Subject: \"A red fox in a snowy forest\"\n",
    "- Style: \"oil painting, impressionist\"\n",
    "- Details: \"morning light, misty atmosphere\"\n",
    "- Quality: \"highly detailed, 4k, artstation\"\n",
    "\n",
    "### Quality Boosters That Work\n",
    "- `highly detailed`, `intricate details`\n",
    "- `8k`, `4k resolution`\n",
    "- `professional photography`, `cinematic`\n",
    "- `trending on artstation`, `award winning`\n",
    "- `golden hour`, `dramatic lighting`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare basic vs detailed prompts\n",
    "\n",
    "basic_prompt = \"A cat sitting on a couch\"\n",
    "\n",
    "detailed_prompt = \"\"\"A fluffy orange tabby cat sitting regally on a vintage velvet couch, \n",
    "soft afternoon sunlight streaming through lace curtains, cozy living room, \n",
    "photorealistic, shallow depth of field, professional pet photography, \n",
    "warm color palette, 8k resolution\"\"\"\n",
    "\n",
    "print(\"Comparing basic vs detailed prompts...\\n\")\n",
    "\n",
    "print(\"Basic prompt:\")\n",
    "img_basic = generate_image(basic_prompt, seed=123, num_steps=25)\n",
    "\n",
    "print(\"\\nDetailed prompt:\")\n",
    "img_detailed = generate_image(detailed_prompt, seed=123, num_steps=25)\n",
    "\n",
    "display_images_grid([img_basic, img_detailed], [\"Basic Prompt\", \"Detailed Prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The power of negative prompts\n",
    "\n",
    "prompt = \"Portrait of a young woman, natural beauty, soft lighting\"\n",
    "\n",
    "# Standard negative prompt\n",
    "negative_basic = \"ugly, blurry, low quality\"\n",
    "\n",
    "# Detailed negative prompt for portraits\n",
    "negative_detailed = \"\"\"ugly, deformed, noisy, blurry, distorted, grainy, \n",
    "bad anatomy, bad proportions, extra limbs, cloned face, disfigured, \n",
    "gross proportions, malformed limbs, missing arms, missing legs, \n",
    "extra arms, extra legs, mutated hands, fused fingers, too many fingers, \n",
    "long neck, mutation, mutilated, morbid, ugly, poorly drawn hands, \n",
    "poorly drawn face, extra fingers, mutated, dehydrated, bad quality, \n",
    "worst quality, watermark, text, signature\"\"\"\n",
    "\n",
    "print(\"Basic negative prompt:\")\n",
    "img_neg_basic = generate_image(prompt, negative_prompt=negative_basic, seed=456, num_steps=25)\n",
    "\n",
    "print(\"\\nDetailed negative prompt:\")\n",
    "img_neg_detailed = generate_image(prompt, negative_prompt=negative_detailed, seed=456, num_steps=25)\n",
    "\n",
    "display_images_grid([img_neg_basic, img_neg_detailed], [\"Basic Negative\", \"Detailed Negative\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Guidance Scale - Finding the Sweet Spot\n",
    "\n",
    "Guidance scale controls how closely the image follows your prompt:\n",
    "- **Low (1-5)**: More creative, may ignore parts of prompt\n",
    "- **Medium (7-10)**: Good balance\n",
    "- **High (12-20)**: Very literal, but may look \"overcooked\"\n",
    "\n",
    "Let's experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different guidance scales\n",
    "prompt = \"A castle floating in the clouds, fantasy art, magical atmosphere\"\n",
    "\n",
    "guidance_values = [3.0, 7.5, 12.0, 20.0]\n",
    "images = []\n",
    "\n",
    "print(\"Generating with different guidance scales...\\n\")\n",
    "\n",
    "for guidance in guidance_values:\n",
    "    print(f\"Guidance scale: {guidance}\")\n",
    "    img = generate_image(prompt, guidance_scale=guidance, seed=789, num_steps=25)\n",
    "    images.append(img)\n",
    "\n",
    "display_images_grid(images, [f\"Guidance: {g}\" for g in guidance_values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: ControlNet - Guided Generation\n",
    "\n",
    "ControlNet allows you to guide image generation using:\n",
    "- **Canny edges**: Outline of shapes\n",
    "- **Depth maps**: 3D structure\n",
    "- **Pose**: Human body positions\n",
    "- **Segmentation**: Region definitions\n",
    "\n",
    "### ELI5: What is ControlNet?\n",
    "\n",
    "> **Imagine you're giving directions to an artist:**\n",
    ">\n",
    "> Without ControlNet: \"Draw me a house\" - the artist uses imagination\n",
    ">\n",
    "> With ControlNet: \"Draw me a house, but follow these lines\" - you give them a rough sketch\n",
    ">\n",
    "> The artist still adds their own style and details, but the basic structure follows your guide!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's clean up SDXL to make room for ControlNet\n",
    "del sdxl_pipe\n",
    "clear_gpu_memory()\n",
    "print(f\"Memory after cleanup: {get_memory_usage()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel\n",
    "from diffusers.utils import load_image\n",
    "import cv2\n",
    "\n",
    "print(\"Loading SDXL with Canny ControlNet...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Load ControlNet\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"diffusers/controlnet-canny-sdxl-1.0\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "# Load pipeline with ControlNet\n",
    "controlnet_pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\"\n",
    ")\n",
    "\n",
    "controlnet_pipe = controlnet_pipe.to(\"cuda\")\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"\\nControlNet pipeline loaded in {load_time:.1f} seconds!\")\n",
    "print(f\"Memory: {get_memory_usage()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_canny_edges(image: Image.Image, low_threshold: int = 100, high_threshold: int = 200) -> Image.Image:\n",
    "    \"\"\"Extract Canny edges from an image.\"\"\"\n",
    "    # Convert to numpy and grayscale\n",
    "    img_array = np.array(image)\n",
    "    gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Apply Canny edge detection\n",
    "    edges = cv2.Canny(gray, low_threshold, high_threshold)\n",
    "    \n",
    "    # Convert back to RGB PIL Image\n",
    "    edges_rgb = np.stack([edges, edges, edges], axis=-1)\n",
    "    return Image.fromarray(edges_rgb)\n",
    "\n",
    "print(\"get_canny_edges() function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple test image with clear edges\n",
    "# We'll draw some geometric shapes\n",
    "\n",
    "from PIL import ImageDraw\n",
    "\n",
    "# Create a simple composition\n",
    "control_source = Image.new('RGB', (1024, 1024), color='white')\n",
    "draw = ImageDraw.Draw(control_source)\n",
    "\n",
    "# Draw a simple house shape\n",
    "draw.rectangle([300, 400, 700, 800], outline='black', width=5)  # House body\n",
    "draw.polygon([(250, 400), (500, 150), (750, 400)], outline='black', width=5)  # Roof\n",
    "draw.rectangle([420, 550, 580, 800], outline='black', width=5)  # Door\n",
    "draw.rectangle([350, 500, 400, 580], outline='black', width=5)  # Window 1\n",
    "draw.rectangle([600, 500, 650, 580], outline='black', width=5)  # Window 2\n",
    "draw.ellipse([800, 100, 950, 250], outline='black', width=5)  # Sun\n",
    "\n",
    "# Get edges\n",
    "canny_image = get_canny_edges(control_source)\n",
    "\n",
    "display_images_grid([control_source, canny_image], [\"Source Drawing\", \"Canny Edges\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images guided by the edges\n",
    "prompts = [\n",
    "    \"A cozy cottage in a meadow, wildflowers, sunny day, photorealistic\",\n",
    "    \"A haunted house at night, spooky atmosphere, full moon, gothic style\",\n",
    "    \"A gingerbread house, candy decorations, whimsical, children's book illustration\"\n",
    "]\n",
    "\n",
    "print(\"Generating images with ControlNet guidance...\\n\")\n",
    "\n",
    "controlled_images = []\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"\\n[{i+1}/3] {prompt[:40]}...\")\n",
    "    \n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(42 + i)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        result = controlnet_pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=\"ugly, blurry, low quality\",\n",
    "            image=canny_image,\n",
    "            num_inference_steps=25,\n",
    "            guidance_scale=7.5,\n",
    "            controlnet_conditioning_scale=0.7,  # How much to follow the control image\n",
    "            generator=generator\n",
    "        )\n",
    "    \n",
    "    controlled_images.append(result.images[0])\n",
    "    print(f\"Generated!\")\n",
    "\n",
    "# Display results\n",
    "all_images = [canny_image] + controlled_images\n",
    "titles = [\"Control (Edges)\"] + [p[:25] + \"...\" for p in prompts]\n",
    "display_images_grid(all_images, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "ControlNet added an extra condition to the generation process:\n",
    "\n",
    "1. **Normal SDXL**: Text embedding → Guide denoising\n",
    "2. **With ControlNet**: Text embedding + Edge features → Guide denoising\n",
    "\n",
    "The `controlnet_conditioning_scale` (0.0-1.0) controls how strictly to follow the edges:\n",
    "- 0.0 = Ignore edges completely\n",
    "- 0.5 = Soft guidance\n",
    "- 1.0 = Strict adherence to edges\n",
    "\n",
    "Notice how all three images maintain the house structure but with completely different styles!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Custom ControlNet\n",
    "\n",
    "Create your own control image and generate variations!\n",
    "\n",
    "1. Draw a simple scene (a robot, a car, or anything with clear edges)\n",
    "2. Extract edges with `get_canny_edges()`\n",
    "3. Generate different styles with the same structure\n",
    "\n",
    "<details>\n",
    "<summary>Hint: Creating Control Images</summary>\n",
    "\n",
    "```python\n",
    "# You can also load an existing image\n",
    "my_image = Image.open(\"/path/to/image.jpg\").resize((1024, 1024))\n",
    "my_edges = get_canny_edges(my_image)\n",
    "\n",
    "# Or download from URL\n",
    "import requests\n",
    "from io import BytesIO\n",
    "response = requests.get(\"https://example.com/image.jpg\")\n",
    "my_image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Try creating your own control image and generating variations!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Flux - State of the Art (Optional)\n",
    "\n",
    "Flux is a newer model that produces incredibly detailed and coherent images. It requires more memory (~24GB) but DGX Spark handles it easily!\n",
    "\n",
    "**Note**: Flux.1-dev requires accepting the license on Hugging Face.\n",
    "\n",
    "For this demo, we'll use the schnell (fast) version which is permissively licensed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up ControlNet first\n",
    "del controlnet_pipe\n",
    "del controlnet\n",
    "clear_gpu_memory()\n",
    "print(f\"Memory after cleanup: {get_memory_usage()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import FluxPipeline\n",
    "\n",
    "print(\"Loading Flux.1-schnell...\")\n",
    "print(\"This model is larger and may take a minute to load...\")\n",
    "print(f\"Memory before: {get_memory_usage()}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "flux_pipe = FluxPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-schnell\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "flux_pipe = flux_pipe.to(\"cuda\")\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"\\nFlux loaded in {load_time:.1f} seconds!\")\n",
    "print(f\"Memory after: {get_memory_usage()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_flux(\n",
    "    prompt: str,\n",
    "    num_steps: int = 4,  # Flux.schnell is optimized for 4 steps\n",
    "    seed: Optional[int] = None,\n",
    "    width: int = 1024,\n",
    "    height: int = 1024\n",
    ") -> Image.Image:\n",
    "    \"\"\"\n",
    "    Generate an image using Flux.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Text description\n",
    "        num_steps: Inference steps (4 for schnell)\n",
    "        seed: Random seed\n",
    "        width: Image width\n",
    "        height: Image height\n",
    "        \n",
    "    Returns:\n",
    "        Generated image\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "    else:\n",
    "        generator = None\n",
    "    \n",
    "    print(f\"Generating with Flux...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        result = flux_pipe(\n",
    "            prompt=prompt,\n",
    "            num_inference_steps=num_steps,\n",
    "            generator=generator,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            guidance_scale=0.0  # Flux.schnell doesn't use guidance\n",
    "        )\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"Generated in {generation_time:.1f} seconds\")\n",
    "    \n",
    "    return result.images[0]\n",
    "\n",
    "print(\"generate_flux() function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Flux with the same prompt we used earlier\n",
    "flux_prompt = \"A majestic mountain landscape at sunset, golden hour lighting, \\\n",
    "photorealistic, 8k resolution, dramatic clouds, peaceful lake reflection\"\n",
    "\n",
    "flux_image = generate_flux(flux_prompt, seed=42)\n",
    "display(flux_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flux excels at complex scenes and text rendering\n",
    "complex_prompts = [\n",
    "    \"A bookshelf filled with colorful books, each spine has readable titles like 'Adventure' and 'Mystery', cozy reading nook\",\n",
    "    \"A futuristic city with flying cars, holographic advertisements showing 'WELCOME 2050', neon lights\"\n",
    "]\n",
    "\n",
    "print(\"Testing Flux with complex prompts...\\n\")\n",
    "\n",
    "flux_images = []\n",
    "for i, prompt in enumerate(complex_prompts):\n",
    "    print(f\"\\n[{i+1}/2] {prompt[:50]}...\")\n",
    "    img = generate_flux(prompt, seed=42+i)\n",
    "    flux_images.append(img)\n",
    "\n",
    "display_images_grid(flux_images, [\"Bookshelf\", \"Future City\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Using Wrong Image Dimensions\n",
    "\n",
    "```python\n",
    "# Wrong - SDXL trained on 1024x1024\n",
    "image = generate_image(prompt, width=512, height=512)  # Lower quality\n",
    "\n",
    "# Right - use native resolution\n",
    "image = generate_image(prompt, width=1024, height=1024)  # Best quality\n",
    "\n",
    "# Also good - SDXL supports these aspect ratios:\n",
    "# 1024x1024, 1152x896, 896x1152, 1216x832, 832x1216, 1344x768, 768x1344\n",
    "```\n",
    "\n",
    "### Mistake 2: Guidance Scale Too High\n",
    "\n",
    "```python\n",
    "# Wrong - oversaturated, artifacts\n",
    "image = generate_image(prompt, guidance_scale=30)  # Too high!\n",
    "\n",
    "# Right - balanced generation\n",
    "image = generate_image(prompt, guidance_scale=7.5)  # Good default\n",
    "```\n",
    "\n",
    "### Mistake 3: Not Using Negative Prompts\n",
    "\n",
    "```python\n",
    "# Wrong - may get unwanted artifacts\n",
    "image = generate_image(\"portrait of a person\")\n",
    "\n",
    "# Right - explicitly avoid problems\n",
    "image = generate_image(\n",
    "    \"portrait of a person\",\n",
    "    negative_prompt=\"ugly, deformed, bad anatomy, extra limbs\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Mistake 4: Forgetting to Set Seed for Reproducibility\n",
    "\n",
    "```python\n",
    "# Wrong - can't reproduce the result\n",
    "image = generate_image(prompt)  # Random seed each time\n",
    "\n",
    "# Right - reproducible results\n",
    "image = generate_image(prompt, seed=42)  # Same image every time\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- How diffusion models generate images from noise\n",
    "- How to use SDXL for high-quality text-to-image generation\n",
    "- How to craft effective prompts and negative prompts\n",
    "- How to use ControlNet for guided generation\n",
    "- How to use Flux for state-of-the-art results\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Diffusion = Denoising**: Models learn to remove noise step by step\n",
    "2. **Prompts matter**: More descriptive = better results\n",
    "3. **Guidance scale**: 7-10 is usually the sweet spot\n",
    "4. **ControlNet**: Add structure while keeping creative freedom\n",
    "5. **Seeds**: Use them for reproducibility!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "### Build an Image Variation Generator\n",
    "\n",
    "Create a function that:\n",
    "1. Takes a base prompt and a list of style modifiers\n",
    "2. Generates the same scene in multiple styles\n",
    "3. Uses the same seed for each to maintain consistency\n",
    "4. Returns a grid of all variations\n",
    "\n",
    "Example:\n",
    "```python\n",
    "base = \"A dragon flying over a castle\"\n",
    "styles = [\"oil painting\", \"anime style\", \"photorealistic\", \"pixel art\"]\n",
    "variations = generate_style_variations(base, styles)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE\n",
    "\n",
    "def generate_style_variations(base_prompt: str, styles: List[str], seed: int = 42) -> List[Image.Image]:\n",
    "    \"\"\"\n",
    "    Generate the same scene in multiple artistic styles.\n",
    "    \n",
    "    Args:\n",
    "        base_prompt: The core scene description\n",
    "        styles: List of style modifiers\n",
    "        seed: Random seed for consistency\n",
    "        \n",
    "    Returns:\n",
    "        List of generated images\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [SDXL Paper: High-Resolution Image Synthesis](https://arxiv.org/abs/2307.01952)\n",
    "- [Diffusers Library Documentation](https://huggingface.co/docs/diffusers)\n",
    "- [ControlNet Paper](https://arxiv.org/abs/2302.05543)\n",
    "- [Flux Technical Report](https://blackforestlabs.ai/)\n",
    "- [Prompt Engineering Guide for SD](https://stable-diffusion-art.com/prompt-guide/)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "if 'flux_pipe' in dir():\n",
    "    del flux_pipe\n",
    "if 'sdxl_pipe' in dir():\n",
    "    del sdxl_pipe\n",
    "if 'controlnet_pipe' in dir():\n",
    "    del controlnet_pipe\n",
    "\n",
    "clear_gpu_memory()\n",
    "print(f\"Final memory state: {get_memory_usage()}\")\n",
    "print(\"\\nNotebook complete! Ready for the next task.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}