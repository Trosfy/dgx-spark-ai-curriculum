{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 4.1.1: Vision-Language Models Demo\n\n**Module:** 4.1 - Multimodal AI  \n**Time:** 2 hours  \n**Difficulty:** ⭐⭐⭐\n\n---\n\n## Learning Objectives\n\nBy the end of this notebook, you will:\n- [ ] Understand how vision-language models work at a high level\n- [ ] Use LLaVA to analyze and describe images\n- [ ] Use Qwen2-VL for advanced visual understanding\n- [ ] Compare different VLM capabilities\n- [ ] Build a simple image Q&A application\n\n---\n\n## Prerequisites\n\n- Completed: Module 3.6 (AI Agents)\n- Knowledge of: Transformers, attention mechanisms, LLMs\n- Running in: NGC PyTorch container with transformers installed"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Real-World Context\n\nVision-Language Models (VLMs) are transforming how we interact with AI:\n\n**Industry Applications:**\n- **Medical Imaging**: \"What abnormalities do you see in this X-ray?\"\n- **Accessibility**: Describing images for visually impaired users\n- **E-commerce**: \"Find products similar to this photo\"\n- **Security**: \"Is there anything suspicious in this surveillance footage?\"\n- **Education**: \"Explain the diagram in this textbook\"\n\n**Why DGX Spark?**\n- Qwen2-VL-7B: ~18GB - fits easily in 128GB unified memory\n- Qwen3-VL-8B: ~20GB - runs with room to spare (2025 model)\n- No need for expensive cloud APIs - run everything locally!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: How Do Vision-Language Models Work?\n",
    "\n",
    "> **Imagine you're a translator who can speak two languages: \"Picture\" and \"English\".**\n",
    ">\n",
    "> When someone shows you a photo, you first describe what you see in the picture using your \"Picture\" vocabulary (that's the **vision encoder** - usually CLIP). You take mental notes: \"I see a cat, it's orange, it's sitting on a blue couch...\"\n",
    ">\n",
    "> Then, you translate these notes into English so you can talk about the picture (that's the **language model** part). But you need a special translator's notebook to connect your Picture-notes to English-notes (that's the **projection layer**).\n",
    ">\n",
    "> When someone asks \"What color is the cat?\", you check your Picture-notes, find the color information, and use your English skills to say \"The cat is orange!\"\n",
    ">\n",
    "> **In AI terms:** \n",
    "> 1. **Vision Encoder** (CLIP/SigLIP): Converts images to embeddings\n",
    "> 2. **Projection Layer**: Aligns image embeddings to text embedding space\n",
    "> 3. **Language Model**: Generates text responses using both image and text context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment Setup\n",
    "\n",
    "Let's start by setting up our environment and checking our GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if not already installed)\n",
    "# Run this only once\n",
    "# !pip install transformers>=4.45.0 accelerate bitsandbytes pillow requests -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import time\n",
    "import warnings\n",
    "from IPython.display import display\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability and memory\n",
    "print(\"=\" * 50)\n",
    "print(\"GPU Configuration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Total Memory: {total_memory:.1f} GB\")\n",
    "    print(f\"Currently Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"Available: {total_memory - allocated:.1f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available - VLMs will be very slow on CPU!\")\n",
    "    \n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory cache - essential when switching between models.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    print(\"GPU memory cleared!\")\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "        return f\"Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\"\n",
    "    return \"No GPU available\"\n",
    "\n",
    "def load_image_from_url(url: str) -> Image.Image:\n",
    "    \"\"\"Load an image from a URL.\"\"\"\n",
    "    response = requests.get(url, timeout=10)\n",
    "    image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    return image\n",
    "\n",
    "print(\"Utility functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 2: Understanding VLM Architecture\n\n### The Three Components\n\n```\n┌─────────────────┐     ┌──────────────────┐     ┌─────────────────┐\n│  Vision Encoder │────▶│ Projection Layer │────▶│ Language Model  │\n│    (CLIP/SigLIP)│     │   (Linear/MLP)   │     │  (Qwen/LLaMA)   │\n└─────────────────┘     └──────────────────┘     └─────────────────┘\n        │                        │                       │\n   Image → Tokens          Align Spaces           Generate Text\n  (224x224 patches)      (Vision → Text)        (Answer questions)\n```\n\n### Why This Matters\n\n| Component | What It Does | Size Impact |\n|-----------|--------------|-------------|\n| Vision Encoder | \"Sees\" the image, creates embeddings | ~400M params |\n| Projection | Translates vision → language space | ~10M params |\n| Language Model | Reasons and generates responses | 7B-72B params |\n\nThe language model dominates the size, which is why:\n- Qwen2-VL-7B ≈ 18GB\n- Qwen3-VL-8B ≈ 20GB (2025 recommended model)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 3: Qwen3-VL - Modern Vision-Language Model\n\nQwen3-VL (2025) is the latest vision-language model with impressive capabilities:\n- **Dynamic Resolution**: Handles images of any size without fixed cropping\n- **Video Understanding**: Can process video frames\n- **Multilingual**: Strong support for Chinese and English\n- **Document Understanding**: Great at reading text in images\n- **Improved Reasoning**: Better at spatial and compositional understanding\n\nLet's load it and try it out!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "\n",
    "# Clear any existing models from memory\n",
    "clear_gpu_memory()\n",
    "\n",
    "print(\"Loading LLaVA-1.6-Vicuna-7B...\")\n",
    "print(f\"Memory before: {get_memory_usage()}\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Load model and processor\n",
    "model_id = \"llava-hf/llava-v1.6-vicuna-7b-hf\"\n",
    "\n",
    "llava_processor = LlavaNextProcessor.from_pretrained(model_id)\n",
    "\n",
    "llava_model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 for Blackwell\n",
    "    device_map=\"auto\",           # Automatically map to GPU\n",
    "    low_cpu_mem_usage=True       # Optimize memory during loading\n",
    ")\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"\\nModel loaded in {load_time:.1f} seconds!\")\n",
    "print(f\"Memory after: {get_memory_usage()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample image\n",
    "# Using a royalty-free image from Unsplash\n",
    "image_url = \"https://images.unsplash.com/photo-1518791841217-8f162f1e1131?w=800\"\n",
    "\n",
    "try:\n",
    "    sample_image = load_image_from_url(image_url)\n",
    "    # Resize for display\n",
    "    display_image = sample_image.copy()\n",
    "    display_image.thumbnail((400, 400))\n",
    "    display(display_image)\n",
    "    print(f\"Image size: {sample_image.size}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load image from URL: {e}\")\n",
    "    print(\"Creating a simple test image instead...\")\n",
    "    # Create a simple gradient image as fallback\n",
    "    import numpy as np\n",
    "    arr = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "    arr[:, :, 0] = np.linspace(0, 255, 224).astype(np.uint8)  # Red gradient\n",
    "    arr[:, :, 2] = np.linspace(255, 0, 224).astype(np.uint8)  # Blue gradient\n",
    "    sample_image = Image.fromarray(arr)\n",
    "    display(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llava(image: Image.Image, question: str, max_tokens: int = 300) -> str:\n",
    "    \"\"\"\n",
    "    Ask LLaVA a question about an image.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image to analyze\n",
    "        question: Question to ask about the image\n",
    "        max_tokens: Maximum tokens in response\n",
    "        \n",
    "    Returns:\n",
    "        Model's response as a string\n",
    "    \"\"\"\n",
    "    # Format the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": question}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    prompt = llava_processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    \n",
    "    # Process inputs\n",
    "    inputs = llava_processor(images=image, text=prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(llava_model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate response\n",
    "    start_time = time.time()\n",
    "    with torch.inference_mode():\n",
    "        output = llava_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    generation_time = time.time() - start_time\n",
    "    \n",
    "    # Decode response\n",
    "    response = llava_processor.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the assistant's response\n",
    "    if \"ASSISTANT:\" in response:\n",
    "        response = response.split(\"ASSISTANT:\")[-1].strip()\n",
    "    \n",
    "    print(f\"\\n[Generated in {generation_time:.1f}s]\")\n",
    "    return response\n",
    "\n",
    "print(\"ask_llava() function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's ask LLaVA about our image!\n",
    "question1 = \"What do you see in this image? Describe it in detail.\"\n",
    "\n",
    "print(f\"Question: {question1}\")\n",
    "print(\"-\" * 50)\n",
    "response1 = ask_llava(sample_image, question1)\n",
    "print(f\"\\nLLaVA: {response1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a follow-up question\n",
    "question2 = \"What mood or atmosphere does this image convey?\"\n",
    "\n",
    "print(f\"Question: {question2}\")\n",
    "print(\"-\" * 50)\n",
    "response2 = ask_llava(sample_image, question2)\n",
    "print(f\"\\nLLaVA: {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask about specific details\n",
    "question3 = \"What colors are most prominent in this image?\"\n",
    "\n",
    "print(f\"Question: {question3}\")\n",
    "print(\"-\" * 50)\n",
    "response3 = ask_llava(sample_image, question3)\n",
    "print(f\"\\nLLaVA: {response3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "1. **Image Processing**: The vision encoder (CLIP ViT-L/14) converted the image into 576 visual tokens\n",
    "2. **Token Alignment**: The projection layer aligned these tokens with the language model's embedding space\n",
    "3. **Text Generation**: The language model (Vicuna-7B) generated a response based on both the visual tokens and your question\n",
    "\n",
    "Notice the generation time - this includes both image processing and text generation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Image Q&A\n",
    "\n",
    "Try asking different types of questions about the image:\n",
    "\n",
    "1. **Descriptive**: \"What objects are in this image?\"\n",
    "2. **Analytical**: \"What might have happened just before this photo was taken?\"\n",
    "3. **Creative**: \"Write a short story inspired by this image.\"\n",
    "4. **Practical**: \"If this were a product photo, what would be a good caption?\"\n",
    "\n",
    "<details>\n",
    "<summary>Hint: Try loading your own image!</summary>\n",
    "\n",
    "```python\n",
    "# Load from file\n",
    "my_image = Image.open(\"/path/to/your/image.jpg\")\n",
    "\n",
    "# Or from URL\n",
    "my_image = load_image_from_url(\"https://example.com/image.jpg\")\n",
    "\n",
    "# Then ask questions\n",
    "response = ask_llava(my_image, \"What do you see?\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Try different questions or load your own image!\n",
    "\n",
    "my_question = \"What would be a good title for this image if it were a painting?\"\n",
    "\n",
    "print(f\"Question: {my_question}\")\n",
    "print(\"-\" * 50)\n",
    "my_response = ask_llava(sample_image, my_question)\n",
    "print(f\"\\nLLaVA: {my_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Qwen2-VL - Advanced Vision Understanding\n",
    "\n",
    "Qwen2-VL is a state-of-the-art VLM from Alibaba with impressive capabilities:\n",
    "- **Dynamic Resolution**: Handles images of any size without fixed cropping\n",
    "- **Video Understanding**: Can process video frames\n",
    "- **Multilingual**: Strong support for Chinese and English\n",
    "- **Document Understanding**: Great at reading text in images\n",
    "\n",
    "Let's compare it with LLaVA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's clean up LLaVA to free memory\n",
    "del llava_model\n",
    "del llava_processor\n",
    "clear_gpu_memory()\n",
    "print(f\"Memory after cleanup: {get_memory_usage()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "print(\"Loading Qwen2-VL-7B-Instruct...\")\n",
    "print(f\"Memory before: {get_memory_usage()}\")\n",
    "start_time = time.time()\n",
    "\n",
    "qwen_model_id = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "\n",
    "qwen_processor = AutoProcessor.from_pretrained(qwen_model_id)\n",
    "\n",
    "qwen_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    qwen_model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"\\nModel loaded in {load_time:.1f} seconds!\")\n",
    "print(f\"Memory after: {get_memory_usage()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_qwen(image: Image.Image, question: str, max_tokens: int = 300) -> str:\n",
    "    \"\"\"\n",
    "    Ask Qwen2-VL a question about an image.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image to analyze\n",
    "        question: Question to ask about the image\n",
    "        max_tokens: Maximum tokens in response\n",
    "        \n",
    "    Returns:\n",
    "        Model's response as a string\n",
    "    \"\"\"\n",
    "    # Format the conversation for Qwen2-VL\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": question}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    text = qwen_processor.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Process inputs\n",
    "    inputs = qwen_processor(\n",
    "        text=[text],\n",
    "        images=[image],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = inputs.to(qwen_model.device)\n",
    "    \n",
    "    # Generate response\n",
    "    start_time = time.time()\n",
    "    with torch.inference_mode():\n",
    "        output_ids = qwen_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    generation_time = time.time() - start_time\n",
    "    \n",
    "    # Decode - get only the new tokens\n",
    "    generated_ids = output_ids[:, inputs.input_ids.shape[1]:]\n",
    "    response = qwen_processor.batch_decode(\n",
    "        generated_ids, \n",
    "        skip_special_tokens=True, \n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "    \n",
    "    print(f\"\\n[Generated in {generation_time:.1f}s]\")\n",
    "    return response\n",
    "\n",
    "print(\"ask_qwen() function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try Qwen2-VL with the same image\n",
    "print(f\"Question: {question1}\")\n",
    "print(\"-\" * 50)\n",
    "qwen_response = ask_qwen(sample_image, question1)\n",
    "print(f\"\\nQwen2-VL: {qwen_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qwen2-VL's Special Abilities\n",
    "\n",
    "Qwen2-VL excels at:\n",
    "1. **OCR/Text Reading**: It can read text in images very accurately\n",
    "2. **Document Understanding**: Great with charts, tables, and diagrams\n",
    "3. **Spatial Reasoning**: Better understanding of object positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test OCR capabilities with a document-style image\n",
    "# We'll create a simple image with text\n",
    "\n",
    "from PIL import ImageDraw, ImageFont\n",
    "\n",
    "# Create a simple document image\n",
    "doc_image = Image.new('RGB', (400, 200), color='white')\n",
    "draw = ImageDraw.Draw(doc_image)\n",
    "\n",
    "# Add some text (using default font)\n",
    "try:\n",
    "    # Try to use a nice font if available\n",
    "    font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 20)\n",
    "except:\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "text_content = \"\"\"Meeting Notes - Dec 2024\n",
    "1. Project deadline: March 15\n",
    "2. Budget: $50,000\n",
    "3. Team size: 5 people\"\"\"\n",
    "\n",
    "draw.text((10, 10), text_content, fill='black', font=font)\n",
    "\n",
    "display(doc_image)\n",
    "print(\"\\nDocument image created for OCR test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test OCR capabilities\n",
    "ocr_question = \"Read all the text in this image and summarize the key information.\"\n",
    "\n",
    "print(f\"Question: {ocr_question}\")\n",
    "print(\"-\" * 50)\n",
    "ocr_response = ask_qwen(doc_image, ocr_question)\n",
    "print(f\"\\nQwen2-VL: {ocr_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Comparing VLM Capabilities\n",
    "\n",
    "Let's create a systematic comparison of different VLM tasks.\n",
    "\n",
    "### Task Categories\n",
    "\n",
    "| Task | Description | Best Model |\n",
    "|------|-------------|------------|\n",
    "| Image Description | General scene understanding | Both good |\n",
    "| Object Detection | Identifying specific objects | Similar |\n",
    "| OCR/Text Reading | Reading text in images | Qwen2-VL |\n",
    "| Document Understanding | Charts, tables, diagrams | Qwen2-VL |\n",
    "| Counting | Counting objects accurately | Challenging for both |\n",
    "| Spatial Reasoning | Understanding positions | Qwen2-VL slightly better |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test with a more complex scene\n",
    "# Using a street scene image\n",
    "street_url = \"https://images.unsplash.com/photo-1480714378408-67cf0d13bc1b?w=800\"\n",
    "\n",
    "try:\n",
    "    street_image = load_image_from_url(street_url)\n",
    "    display_img = street_image.copy()\n",
    "    display_img.thumbnail((400, 400))\n",
    "    display(display_img)\n",
    "except Exception as e:\n",
    "    print(f\"Could not load street image: {e}\")\n",
    "    street_image = sample_image  # Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test spatial reasoning\n",
    "spatial_question = \"Describe the layout of this scene from left to right. What objects are in the foreground vs background?\"\n",
    "\n",
    "print(f\"Question: {spatial_question}\")\n",
    "print(\"-\" * 50)\n",
    "spatial_response = ask_qwen(street_image, spatial_question)\n",
    "print(f\"\\nQwen2-VL: {spatial_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Building an Image Q&A Application\n",
    "\n",
    "Let's put it all together into a reusable application class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageQA:\n",
    "    \"\"\"\n",
    "    A simple Image Question-Answering application using VLMs.\n",
    "    \n",
    "    Example usage:\n",
    "        qa = ImageQA(model_name=\"qwen\")\n",
    "        image = Image.open(\"photo.jpg\")\n",
    "        answer = qa.ask(image, \"What do you see?\")\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"qwen\"):\n",
    "        \"\"\"\n",
    "        Initialize the Image QA system.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Either \"qwen\" or \"llava\"\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        self.conversation_history = []\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the selected model.\"\"\"\n",
    "        if self.model is not None:\n",
    "            print(\"Model already loaded!\")\n",
    "            return\n",
    "            \n",
    "        clear_gpu_memory()\n",
    "        \n",
    "        if self.model_name == \"qwen\":\n",
    "            from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "            model_id = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "            self.processor = AutoProcessor.from_pretrained(model_id)\n",
    "            self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "                model_id,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "        else:\n",
    "            from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "            model_id = \"llava-hf/llava-v1.6-vicuna-7b-hf\"\n",
    "            self.processor = LlavaNextProcessor.from_pretrained(model_id)\n",
    "            self.model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "                model_id,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "        \n",
    "        print(f\"{self.model_name.upper()} model loaded!\")\n",
    "        \n",
    "    def ask(self, image: Image.Image, question: str, max_tokens: int = 300) -> str:\n",
    "        \"\"\"\n",
    "        Ask a question about an image.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image to analyze\n",
    "            question: Question to ask\n",
    "            max_tokens: Maximum response length\n",
    "            \n",
    "        Returns:\n",
    "            Model's response\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            self.load_model()\n",
    "            \n",
    "        if self.model_name == \"qwen\":\n",
    "            return self._ask_qwen(image, question, max_tokens)\n",
    "        else:\n",
    "            return self._ask_llava(image, question, max_tokens)\n",
    "            \n",
    "    def _ask_qwen(self, image, question, max_tokens):\n",
    "        messages = [{\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": question}\n",
    "        ]}]\n",
    "        \n",
    "        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = self.processor(text=[text], images=[image], padding=True, return_tensors=\"pt\")\n",
    "        inputs = inputs.to(self.model.device)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            output_ids = self.model.generate(**inputs, max_new_tokens=max_tokens)\n",
    "        \n",
    "        generated_ids = output_ids[:, inputs.input_ids.shape[1]:]\n",
    "        return self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    def _ask_llava(self, image, question, max_tokens):\n",
    "        conversation = [{\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": question}\n",
    "        ]}]\n",
    "        \n",
    "        prompt = self.processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "        inputs = self.processor(images=image, text=prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            output = self.model.generate(**inputs, max_new_tokens=max_tokens)\n",
    "        \n",
    "        response = self.processor.decode(output[0], skip_special_tokens=True)\n",
    "        if \"ASSISTANT:\" in response:\n",
    "            response = response.split(\"ASSISTANT:\")[-1].strip()\n",
    "        return response\n",
    "    \n",
    "    def analyze_image(self, image: Image.Image) -> dict:\n",
    "        \"\"\"\n",
    "        Perform comprehensive analysis of an image.\n",
    "        \n",
    "        Returns a dictionary with:\n",
    "        - description: General description\n",
    "        - objects: Objects detected\n",
    "        - mood: Mood/atmosphere\n",
    "        - text: Any text found in the image\n",
    "        \"\"\"\n",
    "        analysis = {}\n",
    "        \n",
    "        print(\"Analyzing image...\")\n",
    "        analysis['description'] = self.ask(image, \"Describe this image in 2-3 sentences.\")\n",
    "        analysis['objects'] = self.ask(image, \"List the main objects visible in this image.\")\n",
    "        analysis['mood'] = self.ask(image, \"What mood or feeling does this image convey? Answer in one sentence.\")\n",
    "        analysis['text'] = self.ask(image, \"Is there any text visible in this image? If yes, what does it say? If no, just say 'No text found'.\")\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Free GPU memory.\"\"\"\n",
    "        if self.model is not None:\n",
    "            del self.model\n",
    "            del self.processor\n",
    "            self.model = None\n",
    "            self.processor = None\n",
    "            clear_gpu_memory()\n",
    "            \n",
    "print(\"ImageQA class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the existing model first\n",
    "del qwen_model\n",
    "del qwen_processor\n",
    "clear_gpu_memory()\n",
    "\n",
    "# Test our ImageQA class\n",
    "qa = ImageQA(model_name=\"qwen\")\n",
    "\n",
    "# Run comprehensive analysis\n",
    "analysis = qa.analyze_image(sample_image)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"IMAGE ANALYSIS RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in analysis.items():\n",
    "    print(f\"\\n{key.upper()}:\")\n",
    "    print(f\"  {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Clearing GPU Memory Between Models\n",
    "\n",
    "```python\n",
    "# Wrong - loading a second model without clearing the first\n",
    "model1 = load_llava()\n",
    "model2 = load_qwen()  # OOM error!\n",
    "\n",
    "# Right - always clear memory first\n",
    "model1 = load_llava()\n",
    "del model1\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "model2 = load_qwen()  # Works!\n",
    "```\n",
    "\n",
    "### Mistake 2: Forgetting to Move Inputs to GPU\n",
    "\n",
    "```python\n",
    "# Wrong - inputs stay on CPU\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n",
    "output = model.generate(**inputs)  # Error: tensors on different devices\n",
    "\n",
    "# Right - move inputs to model's device\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "output = model.generate(**inputs)  # Works!\n",
    "```\n",
    "\n",
    "### Mistake 3: Using Wrong Image Format\n",
    "\n",
    "```python\n",
    "# Wrong - passing file path instead of PIL Image\n",
    "response = ask_llava(\"path/to/image.jpg\", \"What is this?\")  # Error!\n",
    "\n",
    "# Right - load as PIL Image first\n",
    "image = Image.open(\"path/to/image.jpg\").convert('RGB')\n",
    "response = ask_llava(image, \"What is this?\")  # Works!\n",
    "```\n",
    "\n",
    "### Mistake 4: Expecting Perfect Counting\n",
    "\n",
    "```python\n",
    "# Unrealistic expectation\n",
    "# VLMs are not great at precise counting!\n",
    "response = ask_llava(crowd_photo, \"Exactly how many people are in this image?\")\n",
    "# Response might be \"approximately 50 people\" when there are 47\n",
    "\n",
    "# Better approach - use ranges or approximate language\n",
    "response = ask_llava(crowd_photo, \"Are there more or fewer than 100 people visible?\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- How VLMs combine vision encoders with language models\n",
    "- How to use LLaVA for general image understanding\n",
    "- How to use Qwen2-VL for advanced tasks including OCR\n",
    "- How to build a reusable Image Q&A application\n",
    "- Common pitfalls and how to avoid them\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **VLMs = Vision Encoder + Projection + LLM** - Understanding this architecture helps you choose the right model\n",
    "2. **Model size matters** - 7B models are fast but less capable than 13B+ models\n",
    "3. **Memory management is crucial** - Always clear GPU memory when switching models\n",
    "4. **Different strengths** - LLaVA is great for general chat, Qwen2-VL excels at documents and OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "### Build a Multi-Image Comparison Tool\n",
    "\n",
    "Create a function that:\n",
    "1. Takes two images as input\n",
    "2. Describes each image separately\n",
    "3. Compares and contrasts the two images\n",
    "4. Suggests which image is better for a given purpose\n",
    "\n",
    "**Hint**: You'll need to ask the VLM about each image, then ask it to compare based on the descriptions.\n",
    "\n",
    "<details>\n",
    "<summary>Solution Approach</summary>\n",
    "\n",
    "1. Analyze each image separately\n",
    "2. Concatenate the images side by side into one image\n",
    "3. Ask the VLM to compare \"the left image\" vs \"the right image\"\n",
    "4. Or use the text descriptions to prompt an LLM for comparison\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE\n",
    "\n",
    "def compare_images(image1: Image.Image, image2: Image.Image, purpose: str = \"social media post\") -> dict:\n",
    "    \"\"\"\n",
    "    Compare two images and recommend which is better for a given purpose.\n",
    "    \n",
    "    Args:\n",
    "        image1: First image\n",
    "        image2: Second image  \n",
    "        purpose: What the images will be used for\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with comparison results\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [LLaVA Paper: Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)\n",
    "- [Qwen2-VL Technical Report](https://arxiv.org/abs/2409.12191)\n",
    "- [CLIP Paper: Learning Transferable Visual Models](https://arxiv.org/abs/2103.00020)\n",
    "- [Hugging Face VLM Guide](https://huggingface.co/docs/transformers/main/en/tasks/visual_question_answering)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "if 'qa' in dir():\n",
    "    qa.cleanup()\n",
    "\n",
    "clear_gpu_memory()\n",
    "print(f\"Final memory state: {get_memory_usage()}\")\n",
    "print(\"\\nNotebook complete! Ready for the next task.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}