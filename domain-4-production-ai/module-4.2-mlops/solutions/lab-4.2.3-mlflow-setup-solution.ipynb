{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 15.3 Solution: MLflow Experiment Tracking\n",
    "\n",
    "This notebook provides solutions for the MLflow exercise.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Solution: Complete Experiment Tracking Workflow\n",
    "\n",
    "**Task:** Create a complete experiment tracking workflow.\n",
    "\n",
    "Requirements:\n",
    "1. Create a new experiment\n",
    "2. Run 5+ training simulations with different hyperparameters\n",
    "3. Log parameters, metrics, and artifacts\n",
    "4. Query for the best run\n",
    "5. Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup MLflow\n",
    "NOTEBOOK_DIR = Path(os.getcwd())\n",
    "MLFLOW_DIR = str((NOTEBOOK_DIR / \"../mlflow\").resolve())\n",
    "os.makedirs(MLFLOW_DIR, exist_ok=True)\n",
    "\n",
    "mlflow.set_tracking_uri(f\"file://{MLFLOW_DIR}\")\n",
    "print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a new experiment\n",
    "EXPERIMENT_NAME = \"my-first-experiment\"\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "\n",
    "if experiment is None:\n",
    "    experiment_id = mlflow.create_experiment(\n",
    "        EXPERIMENT_NAME,\n",
    "        tags={\n",
    "            \"project\": \"module-15-solution\",\n",
    "            \"author\": \"student\",\n",
    "            \"purpose\": \"hyperparameter-exploration\"\n",
    "        }\n",
    "    )\n",
    "    print(f\"‚úÖ Created experiment: {EXPERIMENT_NAME}\")\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id\n",
    "    print(f\"‚úÖ Using existing experiment: {EXPERIMENT_NAME}\")\n",
    "\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define hyperparameter grid for 5+ simulations\n",
    "\n",
    "HYPERPARAM_GRID = [\n",
    "    {\"learning_rate\": 1e-5, \"batch_size\": 8, \"dropout\": 0.1, \"warmup_steps\": 100},\n",
    "    {\"learning_rate\": 1e-4, \"batch_size\": 8, \"dropout\": 0.1, \"warmup_steps\": 100},\n",
    "    {\"learning_rate\": 1e-4, \"batch_size\": 16, \"dropout\": 0.2, \"warmup_steps\": 200},\n",
    "    {\"learning_rate\": 1e-4, \"batch_size\": 32, \"dropout\": 0.1, \"warmup_steps\": 100},\n",
    "    {\"learning_rate\": 1e-3, \"batch_size\": 16, \"dropout\": 0.2, \"warmup_steps\": 50},\n",
    "    {\"learning_rate\": 5e-4, \"batch_size\": 16, \"dropout\": 0.15, \"warmup_steps\": 150},\n",
    "]\n",
    "\n",
    "print(f\"üìã Planned {len(HYPERPARAM_GRID)} experiments:\")\n",
    "for i, hp in enumerate(HYPERPARAM_GRID):\n",
    "    print(f\"   {i+1}. lr={hp['learning_rate']}, bs={hp['batch_size']}, dropout={hp['dropout']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_training(hyperparams: dict, n_epochs: int = 10) -> dict:\n",
    "    \"\"\"\n",
    "    Simulate a training run with given hyperparameters.\n",
    "    Returns metrics for each epoch and final results.\n",
    "    \"\"\"\n",
    "    lr = hyperparams['learning_rate']\n",
    "    bs = hyperparams['batch_size']\n",
    "    dropout = hyperparams['dropout']\n",
    "    warmup = hyperparams['warmup_steps']\n",
    "    \n",
    "    # Simulate training dynamics\n",
    "    # Higher LR = faster convergence but more variance\n",
    "    # Larger batch = more stable\n",
    "    # Higher dropout = slower convergence but better generalization\n",
    "    \n",
    "    lr_factor = 1.0 + (lr - 1e-4) * 5000  # LR effect\n",
    "    bs_factor = 1.0 - (bs - 16) * 0.005   # Batch size effect\n",
    "    dropout_penalty = dropout * 0.5       # Dropout slows learning\n",
    "    \n",
    "    history = []\n",
    "    base_loss = 2.0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Add some randomness\n",
    "        noise = random.gauss(0, 0.02 * (1.5 - lr_factor * 0.1))\n",
    "        \n",
    "        # Training loss decreases\n",
    "        train_loss = base_loss * (0.8 ** (epoch * lr_factor * bs_factor - dropout_penalty)) + noise\n",
    "        train_loss = max(0.05, train_loss)\n",
    "        \n",
    "        # Validation loss with slight gap\n",
    "        val_loss = train_loss * (1.1 + dropout * 0.2) + noise * 0.5\n",
    "        \n",
    "        # Accuracy improves\n",
    "        accuracy = min(0.98, 0.5 + epoch * 0.05 * lr_factor - dropout_penalty * 0.1 + noise)\n",
    "        \n",
    "        history.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"accuracy\": accuracy\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"history\": history,\n",
    "        \"final_train_loss\": history[-1][\"train_loss\"],\n",
    "        \"final_val_loss\": history[-1][\"val_loss\"],\n",
    "        \"final_accuracy\": history[-1][\"accuracy\"],\n",
    "        \"best_accuracy\": max(h[\"accuracy\"] for h in history)\n",
    "    }\n",
    "\n",
    "# Test simulation\n",
    "test_result = simulate_training(HYPERPARAM_GRID[0])\n",
    "print(f\"Test simulation: acc={test_result['final_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_plot(history: list, output_path: str) -> None:\n",
    "    \"\"\"Create and save a training progress plot.\"\"\"\n",
    "    epochs = [h['epoch'] for h in history]\n",
    "    train_losses = [h['train_loss'] for h in history]\n",
    "    val_losses = [h['val_loss'] for h in history]\n",
    "    accuracies = [h['accuracy'] for h in history]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(epochs, train_losses, 'b-', label='Train Loss')\n",
    "    ax1.plot(epochs, val_losses, 'r--', label='Val Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Loss Curves')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.plot(epochs, accuracies, 'g-', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Accuracy')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=100)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Run all experiments with full logging\n",
    "\n",
    "print(\"\\nüî¨ Running hyperparameter sweep...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "run_ids = []\n",
    "\n",
    "for i, hyperparams in enumerate(HYPERPARAM_GRID):\n",
    "    run_name = f\"run-{i+1}_lr{hyperparams['learning_rate']}_bs{hyperparams['batch_size']}\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        # Log hyperparameters\n",
    "        mlflow.log_params(hyperparams)\n",
    "        mlflow.log_param(\"n_epochs\", 10)\n",
    "        mlflow.log_param(\"model_type\", \"simulated\")\n",
    "        \n",
    "        # Add tags\n",
    "        mlflow.set_tag(\"run_index\", str(i + 1))\n",
    "        mlflow.set_tag(\"sweep_id\", \"sweep-001\")\n",
    "        \n",
    "        # Run training simulation\n",
    "        results = simulate_training(hyperparams)\n",
    "        \n",
    "        # Log metrics for each epoch\n",
    "        for h in results['history']:\n",
    "            mlflow.log_metrics({\n",
    "                \"train_loss\": h['train_loss'],\n",
    "                \"val_loss\": h['val_loss'],\n",
    "                \"accuracy\": h['accuracy']\n",
    "            }, step=h['epoch'])\n",
    "        \n",
    "        # Log final metrics\n",
    "        mlflow.log_metrics({\n",
    "            \"final_train_loss\": results['final_train_loss'],\n",
    "            \"final_val_loss\": results['final_val_loss'],\n",
    "            \"final_accuracy\": results['final_accuracy'],\n",
    "            \"best_accuracy\": results['best_accuracy']\n",
    "        })\n",
    "        \n",
    "        # Create and log artifact (training plot)\n",
    "        plot_path = f\"/tmp/training_plot_{i+1}.png\"\n",
    "        create_training_plot(results['history'], plot_path)\n",
    "        mlflow.log_artifact(plot_path, \"plots\")\n",
    "        \n",
    "        # Log config as JSON artifact\n",
    "        config_path = f\"/tmp/config_{i+1}.json\"\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump({\"hyperparams\": hyperparams, \"results\": results}, f, indent=2)\n",
    "        mlflow.log_artifact(config_path, \"configs\")\n",
    "        \n",
    "        run_ids.append(run.info.run_id)\n",
    "        \n",
    "        print(f\"Run {i+1}/{len(HYPERPARAM_GRID)}: \"\n",
    "              f\"lr={hyperparams['learning_rate']}, bs={hyperparams['batch_size']} \"\n",
    "              f\"‚Üí acc={results['final_accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Completed {len(HYPERPARAM_GRID)} experiments!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Query for best run\n",
    "\n",
    "runs_df = mlflow.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    filter_string=\"\",\n",
    "    order_by=[\"metrics.final_accuracy DESC\"]\n",
    ")\n",
    "\n",
    "print(\"\\nüìä All Runs (sorted by accuracy):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "display_cols = [\n",
    "    'run_id', 'params.learning_rate', 'params.batch_size', 'params.dropout',\n",
    "    'metrics.final_accuracy', 'metrics.final_val_loss'\n",
    "]\n",
    "available_cols = [c for c in display_cols if c in runs_df.columns]\n",
    "\n",
    "print(runs_df[available_cols].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best run details\n",
    "best_run = runs_df.iloc[0]\n",
    "\n",
    "print(\"\\nüèÜ BEST RUN:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Run ID: {best_run['run_id'][:16]}...\")\n",
    "print(f\"\\nHyperparameters:\")\n",
    "print(f\"  Learning Rate: {best_run.get('params.learning_rate', 'N/A')}\")\n",
    "print(f\"  Batch Size: {best_run.get('params.batch_size', 'N/A')}\")\n",
    "print(f\"  Dropout: {best_run.get('params.dropout', 'N/A')}\")\n",
    "print(f\"  Warmup Steps: {best_run.get('params.warmup_steps', 'N/A')}\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Final Accuracy: {best_run.get('metrics.final_accuracy', 'N/A'):.4f}\")\n",
    "print(f\"  Final Val Loss: {best_run.get('metrics.final_val_loss', 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Visualize results\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Filter runs with data\n",
    "viz_df = runs_df[runs_df['params.learning_rate'].notna()].copy()\n",
    "viz_df['lr'] = viz_df['params.learning_rate'].astype(float)\n",
    "viz_df['bs'] = viz_df['params.batch_size'].astype(float)\n",
    "viz_df['dropout'] = viz_df['params.dropout'].astype(float)\n",
    "\n",
    "# 1. Learning Rate vs Accuracy\n",
    "ax1 = axes[0, 0]\n",
    "scatter = ax1.scatter(viz_df['lr'], viz_df['metrics.final_accuracy'] * 100, \n",
    "                      c=viz_df['bs'], cmap='viridis', s=150)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('Learning Rate')\n",
    "ax1.set_ylabel('Final Accuracy (%)')\n",
    "ax1.set_title('Learning Rate vs Accuracy')\n",
    "plt.colorbar(scatter, ax=ax1, label='Batch Size')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Batch Size vs Validation Loss\n",
    "ax2 = axes[0, 1]\n",
    "scatter2 = ax2.scatter(viz_df['bs'], viz_df['metrics.final_val_loss'],\n",
    "                       c=viz_df['dropout'], cmap='coolwarm', s=150)\n",
    "ax2.set_xlabel('Batch Size')\n",
    "ax2.set_ylabel('Final Validation Loss')\n",
    "ax2.set_title('Batch Size vs Val Loss')\n",
    "plt.colorbar(scatter2, ax=ax2, label='Dropout')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Accuracy Distribution\n",
    "ax3 = axes[1, 0]\n",
    "accuracies = viz_df['metrics.final_accuracy'].values * 100\n",
    "ax3.hist(accuracies, bins=10, edgecolor='black', alpha=0.7)\n",
    "ax3.axvline(x=accuracies.max(), color='green', linestyle='--', label=f'Best: {accuracies.max():.1f}%')\n",
    "ax3.axvline(x=accuracies.mean(), color='blue', linestyle='--', label=f'Mean: {accuracies.mean():.1f}%')\n",
    "ax3.set_xlabel('Final Accuracy (%)')\n",
    "ax3.set_ylabel('Count')\n",
    "ax3.set_title('Accuracy Distribution')\n",
    "ax3.legend()\n",
    "\n",
    "# 4. Hyperparameter Importance (simple correlation)\n",
    "ax4 = axes[1, 1]\n",
    "correlations = {\n",
    "    'Learning Rate': viz_df['lr'].corr(viz_df['metrics.final_accuracy']),\n",
    "    'Batch Size': viz_df['bs'].corr(viz_df['metrics.final_accuracy']),\n",
    "    'Dropout': viz_df['dropout'].corr(viz_df['metrics.final_accuracy'])\n",
    "}\n",
    "colors = ['green' if c > 0 else 'red' for c in correlations.values()]\n",
    "ax4.barh(list(correlations.keys()), list(correlations.values()), color=colors)\n",
    "ax4.set_xlabel('Correlation with Accuracy')\n",
    "ax4.set_title('Hyperparameter Importance')\n",
    "ax4.axvline(x=0, color='black', linestyle='-')\n",
    "ax4.set_xlim(-1, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{MLFLOW_DIR}/sweep_analysis.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìÅ Analysis saved to {MLFLOW_DIR}/sweep_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Structured Experiments**: Named experiments group related runs\n",
    "2. **Complete Logging**: Log params, metrics over time, and artifacts\n",
    "3. **Easy Querying**: MLflow's search API makes finding best runs simple\n",
    "4. **Visualization**: Combine MLflow data with matplotlib for insights\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To view these results in the MLflow UI:\n",
    "\n",
    "```bash\n",
    "mlflow ui --backend-store-uri ../mlflow --host 0.0.0.0 --port 5000\n",
    "```\n",
    "\n",
    "Then open http://localhost:5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n‚úÖ Solution complete!\")\n",
    "print(f\"\\nTo view results in MLflow UI, run:\")\n",
    "print(f\"   mlflow ui --backend-store-uri {MLFLOW_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
