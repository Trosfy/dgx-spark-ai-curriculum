{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4.3 Solutions: MLOps & Experiment Tracking\n",
    "\n",
    "This notebook contains solutions to the exercises and challenges from Module 4.3.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [MLflow Setup Exercise](#mlflow-exercise)\n",
    "2. [W&B Learning Rate Scheduler Comparison](#wandb-exercise)\n",
    "3. [Custom Evaluation Pipeline](#evaluation-exercise)\n",
    "4. [Drift Monitoring Simulation](#drift-exercise)\n",
    "5. [Model Versioning Workflow](#registry-exercise)\n",
    "6. [Reproducibility Pipeline](#reproducibility-exercise)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. MLflow Setup Exercise <a id=\"mlflow-exercise\"></a>\n",
    "\n",
    "**Task:** Compare different optimization algorithms using MLflow tracking.\n",
    "\n",
    "This solution demonstrates:\n",
    "- Creating experiments and runs\n",
    "- Logging parameters, metrics, and models\n",
    "- Comparing runs programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Configure MLflow\n",
    "TRACKING_DIR = \"./mlruns\"\n",
    "os.makedirs(TRACKING_DIR, exist_ok=True)\n",
    "mlflow.set_tracking_uri(f\"file://{os.path.abspath(TRACKING_DIR)}\")\n",
    "\n",
    "# Create synthetic data\n",
    "n_samples = 1000\n",
    "n_features = 20\n",
    "X = torch.randn(n_samples, n_features)\n",
    "y = (X @ torch.randn(n_features) > 0).float()\n",
    "train_X, val_X = X[:800], X[800:]\n",
    "train_y, val_y = y[:800], y[800:]\n",
    "\n",
    "# Simple classifier\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Optimizer Comparison\n",
    "\n",
    "mlflow.set_experiment(\"Optimizer-Comparison\")\n",
    "\n",
    "optimizers_to_try = [\n",
    "    (\"Adam\", lambda p: optim.Adam(p, lr=0.01)),\n",
    "    (\"SGD\", lambda p: optim.SGD(p, lr=0.01, momentum=0.9)),\n",
    "    (\"AdamW\", lambda p: optim.AdamW(p, lr=0.01)),\n",
    "    (\"RMSprop\", lambda p: optim.RMSprop(p, lr=0.01)),\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for opt_name, opt_fn in optimizers_to_try:\n",
    "    set_seed(42)  # Reset seed for fair comparison\n",
    "    \n",
    "    with mlflow.start_run(run_name=opt_name):\n",
    "        # Log parameters\n",
    "        mlflow.log_params({\n",
    "            \"optimizer\": opt_name,\n",
    "            \"learning_rate\": 0.01,\n",
    "            \"hidden_dim\": 64,\n",
    "            \"epochs\": 20\n",
    "        })\n",
    "        \n",
    "        # Create model and optimizer\n",
    "        model = SimpleClassifier(n_features, 64)\n",
    "        optimizer = opt_fn(model.parameters())\n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "        # Training loop\n",
    "        best_val_acc = 0\n",
    "        for epoch in range(20):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(train_X)\n",
    "            loss = criterion(outputs, train_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_out = model(val_X)\n",
    "                val_acc = ((val_out > 0.5).float() == val_y).float().mean().item()\n",
    "            \n",
    "            # Log metrics\n",
    "            mlflow.log_metrics({\n",
    "                \"train_loss\": loss.item(),\n",
    "                \"val_accuracy\": val_acc\n",
    "            }, step=epoch)\n",
    "            \n",
    "            best_val_acc = max(best_val_acc, val_acc)\n",
    "        \n",
    "        mlflow.log_metric(\"best_val_accuracy\", best_val_acc)\n",
    "        results.append((opt_name, best_val_acc))\n",
    "        print(f\"{opt_name}: {best_val_acc:.4f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\nRanked by accuracy:\")\n",
    "for opt_name, acc in sorted(results, key=lambda x: -x[1]):\n",
    "    print(f\"  {opt_name}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. W&B Learning Rate Scheduler Comparison <a id=\"wandb-exercise\"></a>\n",
    "\n",
    "**Task:** Compare different learning rate schedulers and visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Learning Rate Scheduler Comparison\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR, OneCycleLR\n",
    "\n",
    "# Since W&B requires authentication, we'll simulate the tracking\n",
    "class SimpleTracker:\n",
    "    \"\"\"Simple tracker for demonstration.\"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.logs = []\n",
    "    \n",
    "    def log(self, metrics):\n",
    "        self.logs.append(metrics)\n",
    "\n",
    "def train_with_scheduler(scheduler_name, scheduler_fn, epochs=30):\n",
    "    set_seed(42)\n",
    "    tracker = SimpleTracker(scheduler_name)\n",
    "    \n",
    "    model = SimpleClassifier(n_features, 64)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "    \n",
    "    # Create scheduler\n",
    "    if scheduler_name == \"step\":\n",
    "        scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    elif scheduler_name == \"cosine\":\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    elif scheduler_name == \"onecycle\":\n",
    "        scheduler = OneCycleLR(optimizer, max_lr=0.1, epochs=epochs, steps_per_epoch=1)\n",
    "    else:\n",
    "        scheduler = None\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(train_X), train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_acc = ((model(val_X) > 0.5).float() == val_y).float().mean().item()\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        tracker.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"loss\": loss.item(),\n",
    "            \"val_accuracy\": val_acc,\n",
    "            \"learning_rate\": current_lr\n",
    "        })\n",
    "        \n",
    "        best_acc = max(best_acc, val_acc)\n",
    "    \n",
    "    return tracker, best_acc\n",
    "\n",
    "# Run comparison\n",
    "schedulers = [\"step\", \"cosine\", \"onecycle\", \"none\"]\n",
    "scheduler_results = {}\n",
    "\n",
    "for sched in schedulers:\n",
    "    tracker, acc = train_with_scheduler(sched, None)\n",
    "    scheduler_results[sched] = {\"tracker\": tracker, \"accuracy\": acc}\n",
    "    print(f\"{sched}: {acc:.4f}\")\n",
    "\n",
    "print(\"\\nBest scheduler:\", max(scheduler_results.items(), key=lambda x: x[1]['accuracy'])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Custom Evaluation Pipeline <a id=\"evaluation-exercise\"></a>\n",
    "\n",
    "**Task:** Create a code generation evaluation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Code Generation Evaluation\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    metric_name: str\n",
    "    score: float\n",
    "    details: dict = None\n",
    "\n",
    "class CodeEvaluator:\n",
    "    \"\"\"Evaluator for code generation tasks.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_syntax(code: str) -> EvaluationResult:\n",
    "        \"\"\"Check if code has valid Python syntax.\"\"\"\n",
    "        try:\n",
    "            compile(code, '<string>', 'exec')\n",
    "            return EvaluationResult(\"syntax\", 1.0, {\"valid\": True})\n",
    "        except SyntaxError as e:\n",
    "            return EvaluationResult(\"syntax\", 0.0, {\"error\": str(e)})\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_docstring(code: str) -> EvaluationResult:\n",
    "        \"\"\"Check if code contains a docstring.\"\"\"\n",
    "        has_docstring = '\"\"\"' in code or \"'''\" in code\n",
    "        return EvaluationResult(\n",
    "            \"docstring\", \n",
    "            1.0 if has_docstring else 0.0,\n",
    "            {\"has_docstring\": has_docstring}\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_function_presence(code: str, required: List[str]) -> EvaluationResult:\n",
    "        \"\"\"Check if required functions are present.\"\"\"\n",
    "        found = sum(1 for fn in required if f\"def {fn}\" in code)\n",
    "        score = found / len(required) if required else 0\n",
    "        return EvaluationResult(\n",
    "            \"functions\",\n",
    "            score,\n",
    "            {\"found\": found, \"required\": len(required)}\n",
    "        )\n",
    "    \n",
    "    @staticmethod  \n",
    "    def check_type_hints(code: str) -> EvaluationResult:\n",
    "        \"\"\"Check for type hints.\"\"\"\n",
    "        has_hints = \"->\" in code or \": str\" in code or \": int\" in code\n",
    "        return EvaluationResult(\n",
    "            \"type_hints\",\n",
    "            1.0 if has_hints else 0.5,\n",
    "            {\"has_type_hints\": has_hints}\n",
    "        )\n",
    "    \n",
    "    def evaluate(self, code: str, required_functions: List[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Run all evaluations.\"\"\"\n",
    "        results = [\n",
    "            self.check_syntax(code),\n",
    "            self.check_docstring(code),\n",
    "            self.check_function_presence(code, required_functions or []),\n",
    "            self.check_type_hints(code)\n",
    "        ]\n",
    "        \n",
    "        scores = {r.metric_name: r.score for r in results}\n",
    "        scores[\"overall\"] = np.mean(list(scores.values()))\n",
    "        \n",
    "        return scores\n",
    "\n",
    "# Test the evaluator\n",
    "evaluator = CodeEvaluator()\n",
    "\n",
    "test_codes = [\n",
    "    # Good code\n",
    "    '''\n",
    "def calculate_sum(a: int, b: int) -> int:\n",
    "    \"\"\"Calculate the sum of two numbers.\"\"\"\n",
    "    return a + b\n",
    "''',\n",
    "    # Missing docstring\n",
    "    '''\n",
    "def calculate_product(a, b):\n",
    "    return a * b\n",
    "''',\n",
    "    # Invalid syntax\n",
    "    '''\n",
    "def broken(\n",
    "    return x\n",
    "'''\n",
    "]\n",
    "\n",
    "for i, code in enumerate(test_codes):\n",
    "    scores = evaluator.evaluate(code, required_functions=[\"calculate_sum\"])\n",
    "    print(f\"\\nCode {i+1}:\")\n",
    "    for metric, score in scores.items():\n",
    "        print(f\"  {metric}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Drift Monitoring Simulation <a id=\"drift-exercise\"></a>\n",
    "\n",
    "**Task:** Simulate data arriving over time with increasing drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Drift Simulation Over Time\n",
    "\n",
    "def generate_data_with_drift(n_samples, drift_intensity):\n",
    "    \"\"\"Generate data with controllable drift.\"\"\"\n",
    "    data = {\n",
    "        'feature1': np.random.normal(0 + drift_intensity, 1, n_samples),\n",
    "        'feature2': np.random.normal(5, 2, n_samples),\n",
    "        'feature3': np.random.poisson(3 + int(drift_intensity * 2), n_samples),\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df['target'] = (df['feature1'] > 0).astype(int)\n",
    "    df['prediction'] = df['target']  # Assume perfect predictions\n",
    "    return df\n",
    "\n",
    "class SimpleDriftMonitor:\n",
    "    \"\"\"Simple drift monitor for simulation.\"\"\"\n",
    "    \n",
    "    def __init__(self, reference_data):\n",
    "        self.reference = reference_data\n",
    "        self.history = []\n",
    "    \n",
    "    def check(self, current_data):\n",
    "        \"\"\"Check for drift using simple mean comparison.\"\"\"\n",
    "        drift_scores = {}\n",
    "        \n",
    "        for col in ['feature1', 'feature2', 'feature3']:\n",
    "            ref_mean = self.reference[col].mean()\n",
    "            cur_mean = current_data[col].mean()\n",
    "            ref_std = self.reference[col].std()\n",
    "            \n",
    "            # Normalized difference\n",
    "            drift_scores[col] = abs(cur_mean - ref_mean) / (ref_std + 1e-10)\n",
    "        \n",
    "        overall_drift = np.mean(list(drift_scores.values()))\n",
    "        \n",
    "        self.history.append({\n",
    "            'overall_drift': overall_drift,\n",
    "            **drift_scores\n",
    "        })\n",
    "        \n",
    "        return overall_drift\n",
    "\n",
    "# Simulate 30 days of data\n",
    "reference = generate_data_with_drift(1000, 0)\n",
    "monitor = SimpleDriftMonitor(reference)\n",
    "\n",
    "print(\"Simulating 30 days of data with increasing drift...\")\n",
    "print(\"Day | Drift Score | Alert\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for day in range(30):\n",
    "    # Drift increases over time\n",
    "    drift_intensity = day / 20  # 0 to 1.5\n",
    "    daily_data = generate_data_with_drift(100, drift_intensity)\n",
    "    \n",
    "    drift_score = monitor.check(daily_data)\n",
    "    \n",
    "    # Alert logic\n",
    "    if drift_score > 1.0:\n",
    "        alert = \"CRITICAL\"\n",
    "    elif drift_score > 0.5:\n",
    "        alert = \"WARNING\"\n",
    "    else:\n",
    "        alert = \"OK\"\n",
    "    \n",
    "    if day % 5 == 0 or alert != \"OK\":  # Print every 5 days or on alert\n",
    "        print(f\"{day:3d} | {drift_score:11.3f} | {alert}\")\n",
    "\n",
    "# Summary\n",
    "history_df = pd.DataFrame(monitor.history)\n",
    "print(f\"\\nMax drift observed: {history_df['overall_drift'].max():.3f}\")\n",
    "print(f\"Days with warnings: {(history_df['overall_drift'] > 0.5).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Model Versioning Workflow <a id=\"registry-exercise\"></a>\n",
    "\n",
    "**Task:** Implement a complete model versioning workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Complete Model Versioning Workflow\n",
    "\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "mlflow.set_experiment(\"Model-Versioning-Workflow\")\n",
    "client = MlflowClient()\n",
    "\n",
    "# Model configurations to try\n",
    "configs = [\n",
    "    {\"hidden_dim\": 32, \"lr\": 0.01, \"name\": \"small\"},\n",
    "    {\"hidden_dim\": 64, \"lr\": 0.005, \"name\": \"medium\"},\n",
    "    {\"hidden_dim\": 128, \"lr\": 0.001, \"name\": \"large\"},\n",
    "]\n",
    "\n",
    "model_results = []\n",
    "\n",
    "# Train and register all models\n",
    "for config in configs:\n",
    "    set_seed(42)\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"v-{config['name']}\"):\n",
    "        # Log config\n",
    "        mlflow.log_params(config)\n",
    "        \n",
    "        # Train\n",
    "        model = SimpleClassifier(n_features, config['hidden_dim'])\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "        for epoch in range(20):\n",
    "            model.train()\n",
    "            loss = criterion(model(train_X), train_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_acc = ((model(val_X) > 0.5).float() == val_y).float().mean().item()\n",
    "        \n",
    "        mlflow.log_metric(\"val_accuracy\", val_acc)\n",
    "        \n",
    "        # Register model\n",
    "        mlflow.pytorch.log_model(\n",
    "            model, \n",
    "            artifact_path=\"model\",\n",
    "            registered_model_name=\"VersionedClassifier\"\n",
    "        )\n",
    "        \n",
    "        model_results.append({\n",
    "            \"name\": config['name'],\n",
    "            \"run_id\": mlflow.active_run().info.run_id,\n",
    "            \"accuracy\": val_acc\n",
    "        })\n",
    "        \n",
    "        print(f\"{config['name']}: accuracy = {val_acc:.4f}\")\n",
    "\n",
    "# Find best model\n",
    "best = max(model_results, key=lambda x: x['accuracy'])\n",
    "print(f\"\\nBest model: {best['name']} (accuracy: {best['accuracy']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promotion workflow\n",
    "print(\"\\nPromotion Workflow:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    versions = client.search_model_versions(\"name='VersionedClassifier'\")\n",
    "    \n",
    "    # Find best version\n",
    "    best_version = None\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    for version in versions:\n",
    "        run = client.get_run(version.run_id)\n",
    "        acc = run.data.metrics.get('val_accuracy', 0)\n",
    "        if acc > best_accuracy:\n",
    "            best_accuracy = acc\n",
    "            best_version = version\n",
    "    \n",
    "    if best_version:\n",
    "        print(f\"Best version: {best_version.version} (accuracy: {best_accuracy:.4f})\")\n",
    "        \n",
    "        # Promote to staging\n",
    "        client.transition_model_version_stage(\n",
    "            name=\"VersionedClassifier\",\n",
    "            version=best_version.version,\n",
    "            stage=\"Staging\"\n",
    "        )\n",
    "        print(f\"Version {best_version.version} promoted to Staging\")\n",
    "        \n",
    "        # Simulate validation checks passing\n",
    "        print(\"Running validation checks...\")\n",
    "        print(\"  Accuracy check: PASSED\")\n",
    "        print(\"  Latency check: PASSED\")\n",
    "        \n",
    "        # Promote to production\n",
    "        client.transition_model_version_stage(\n",
    "            name=\"VersionedClassifier\",\n",
    "            version=best_version.version,\n",
    "            stage=\"Production\",\n",
    "            archive_existing_versions=True\n",
    "        )\n",
    "        print(f\"Version {best_version.version} promoted to Production!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Reproducibility Pipeline <a id=\"reproducibility-exercise\"></a>\n",
    "\n",
    "**Task:** Create a complete reproducible training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Complete Reproducibility Pipeline\n",
    "\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "class ReproducibleTrainer:\n",
    "    \"\"\"Trainer with built-in reproducibility.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.seed = config.get('seed', 42)\n",
    "        self.env_info = self._capture_environment()\n",
    "    \n",
    "    def _set_seeds(self):\n",
    "        \"\"\"Set all random seeds.\"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(self.seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    def _capture_environment(self):\n",
    "        \"\"\"Capture environment info.\"\"\"\n",
    "        return {\n",
    "            \"python\": sys.version.split()[0],\n",
    "            \"torch\": torch.__version__,\n",
    "            \"numpy\": np.__version__,\n",
    "            \"platform\": platform.system(),\n",
    "            \"cuda\": torch.version.cuda if torch.cuda.is_available() else None\n",
    "        }\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Run training with reproducibility.\"\"\"\n",
    "        self._set_seeds()\n",
    "        \n",
    "        # Create model\n",
    "        model = SimpleClassifier(\n",
    "            n_features, \n",
    "            self.config['hidden_dim']\n",
    "        )\n",
    "        optimizer = optim.Adam(model.parameters(), lr=self.config['lr'])\n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "        # Training\n",
    "        losses = []\n",
    "        for epoch in range(self.config['epochs']):\n",
    "            model.train()\n",
    "            loss = criterion(model(train_X), train_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_acc = ((model(val_X) > 0.5).float() == val_y).float().mean().item()\n",
    "            first_param = model.net[0].weight[0, 0].item()\n",
    "        \n",
    "        return {\n",
    "            \"final_loss\": losses[-1],\n",
    "            \"val_accuracy\": val_acc,\n",
    "            \"first_param\": first_param  # For verification\n",
    "        }\n",
    "    \n",
    "    def verify_reproducibility(self, n_runs=3):\n",
    "        \"\"\"Verify training is reproducible.\"\"\"\n",
    "        results = []\n",
    "        for i in range(n_runs):\n",
    "            result = self.train()\n",
    "            results.append(result)\n",
    "            print(f\"Run {i+1}: loss={result['final_loss']:.6f}, \"\n",
    "                  f\"acc={result['val_accuracy']:.4f}, \"\n",
    "                  f\"param={result['first_param']:.6f}\")\n",
    "        \n",
    "        # Check all runs are identical\n",
    "        all_same = all(\n",
    "            abs(r['first_param'] - results[0]['first_param']) < 1e-6\n",
    "            for r in results\n",
    "        )\n",
    "        \n",
    "        return all_same\n",
    "\n",
    "# Run reproducibility verification\n",
    "config = {\n",
    "    \"seed\": 42,\n",
    "    \"hidden_dim\": 64,\n",
    "    \"lr\": 0.01,\n",
    "    \"epochs\": 10\n",
    "}\n",
    "\n",
    "trainer = ReproducibleTrainer(config)\n",
    "\n",
    "print(\"Environment:\")\n",
    "for key, value in trainer.env_info.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nVerifying reproducibility with 3 runs:\")\n",
    "is_reproducible = trainer.verify_reproducibility()\n",
    "\n",
    "if is_reproducible:\n",
    "    print(\"\\n✅ PASSED: All runs produced identical results!\")\n",
    "else:\n",
    "    print(\"\\n❌ FAILED: Runs produced different results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This solutions notebook demonstrated:\n",
    "\n",
    "1. **MLflow Tracking**: Comparing optimizers with proper experiment tracking\n",
    "2. **W&B-style Tracking**: Learning rate scheduler comparison with logging\n",
    "3. **Custom Evaluation**: Building a code quality evaluator\n",
    "4. **Drift Detection**: Simulating and monitoring drift over time\n",
    "5. **Model Registry**: Complete versioning and promotion workflow\n",
    "6. **Reproducibility**: Building a fully reproducible training pipeline\n",
    "\n",
    "All these patterns are essential for production ML systems!\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Solutions notebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
