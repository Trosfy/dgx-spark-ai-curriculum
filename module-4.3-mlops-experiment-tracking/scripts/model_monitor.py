"""
Model Monitoring Utilities for Drift Detection and Performance Tracking.

This module provides tools for monitoring ML models in production,
including data drift detection, performance monitoring, and alerting.

Example:
    from model_monitor import ModelMonitor, DriftDetector

    # Initialize monitor
    monitor = ModelMonitor(reference_data, column_mapping)

    # Check for drift
    alerts = monitor.check(production_data)
    for alert in alerts:
        print(f"{alert.severity}: {alert.message}")
"""

from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, field
from datetime import datetime
import numpy as np
import pandas as pd
from enum import Enum


class AlertSeverity(Enum):
    """Severity levels for monitoring alerts."""
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"


@dataclass
class MonitoringAlert:
    """Alert generated by the monitoring system."""
    severity: AlertSeverity
    category: str
    message: str
    timestamp: datetime = field(default_factory=datetime.now)
    details: Dict = field(default_factory=dict)

    def to_dict(self) -> Dict:
        """Convert alert to dictionary."""
        return {
            "severity": self.severity.value,
            "category": self.category,
            "message": self.message,
            "timestamp": self.timestamp.isoformat(),
            "details": self.details
        }


class DriftDetector:
    """
    Statistical drift detection for numerical and categorical features.

    Example:
        detector = DriftDetector()
        drift_score, is_drifted = detector.detect_drift(
            reference_values,
            current_values
        )
    """

    def __init__(self, threshold: float = 0.1):
        """
        Initialize drift detector.

        Args:
            threshold: P-value threshold for drift detection
        """
        self.threshold = threshold

    def ks_test(self, reference: np.ndarray, current: np.ndarray) -> tuple:
        """
        Perform Kolmogorov-Smirnov test for numerical features.

        Args:
            reference: Reference distribution values
            current: Current distribution values

        Returns:
            tuple: (statistic, p_value, is_drifted)
        """
        from scipy import stats

        statistic, p_value = stats.ks_2samp(reference, current)
        is_drifted = p_value < self.threshold

        return statistic, p_value, is_drifted

    def chi2_test(self, reference: np.ndarray, current: np.ndarray) -> tuple:
        """
        Perform Chi-squared test for categorical features.

        Args:
            reference: Reference category counts
            current: Current category counts

        Returns:
            tuple: (statistic, p_value, is_drifted)
        """
        from scipy import stats

        # Get all unique categories
        all_categories = set(reference) | set(current)

        # Count frequencies
        ref_counts = pd.Series(reference).value_counts()
        cur_counts = pd.Series(current).value_counts()

        # Align categories
        ref_freq = [ref_counts.get(cat, 0) for cat in all_categories]
        cur_freq = [cur_counts.get(cat, 0) for cat in all_categories]

        # Normalize to same scale
        ref_freq = np.array(ref_freq) / sum(ref_freq) * len(current)
        cur_freq = np.array(cur_freq)

        # Chi-squared test
        statistic, p_value = stats.chisquare(cur_freq, f_exp=ref_freq)
        is_drifted = p_value < self.threshold

        return statistic, p_value, is_drifted

    def detect_drift(
        self,
        reference: np.ndarray,
        current: np.ndarray,
        feature_type: str = "numerical"
    ) -> tuple:
        """
        Detect drift in a feature.

        Args:
            reference: Reference values
            current: Current values
            feature_type: "numerical" or "categorical"

        Returns:
            tuple: (drift_score, is_drifted)
        """
        if feature_type == "numerical":
            statistic, p_value, is_drifted = self.ks_test(reference, current)
        else:
            statistic, p_value, is_drifted = self.chi2_test(reference, current)

        return statistic, is_drifted


class ModelMonitor:
    """
    Comprehensive model monitoring system.

    Monitors for:
    - Data drift
    - Model performance degradation
    - Data quality issues

    Example:
        monitor = ModelMonitor(
            reference_data=training_data,
            target_column="target",
            prediction_column="prediction"
        )

        alerts = monitor.check(production_data)
    """

    def __init__(
        self,
        reference_data: pd.DataFrame,
        target_column: Optional[str] = None,
        prediction_column: Optional[str] = None,
        numerical_features: Optional[List[str]] = None,
        categorical_features: Optional[List[str]] = None,
        drift_threshold: float = 0.3,
        accuracy_threshold: float = 0.8,
        missing_threshold: float = 0.05
    ):
        """
        Initialize the model monitor.

        Args:
            reference_data: Reference (training) data
            target_column: Name of the target column
            prediction_column: Name of the prediction column
            numerical_features: List of numerical feature names
            categorical_features: List of categorical feature names
            drift_threshold: Threshold for drift detection (share of drifted features)
            accuracy_threshold: Minimum acceptable accuracy
            missing_threshold: Maximum acceptable missing value ratio
        """
        self.reference_data = reference_data
        self.target_column = target_column
        self.prediction_column = prediction_column
        self.drift_threshold = drift_threshold
        self.accuracy_threshold = accuracy_threshold
        self.missing_threshold = missing_threshold

        # Auto-detect feature types if not provided
        if numerical_features is None:
            numerical_features = reference_data.select_dtypes(
                include=[np.number]
            ).columns.tolist()
            # Exclude target and prediction
            if target_column:
                numerical_features = [f for f in numerical_features if f != target_column]
            if prediction_column:
                numerical_features = [f for f in numerical_features if f != prediction_column]

        self.numerical_features = numerical_features
        self.categorical_features = categorical_features or []

        self.drift_detector = DriftDetector()
        self.history: List[Dict] = []

    def check(self, current_data: pd.DataFrame) -> List[MonitoringAlert]:
        """
        Run all monitoring checks on current data.

        Args:
            current_data: Current production data

        Returns:
            List of monitoring alerts
        """
        alerts = []
        check_results = {
            "timestamp": datetime.now().isoformat(),
            "n_samples": len(current_data)
        }

        # Check data drift
        drift_alerts, drift_results = self._check_data_drift(current_data)
        alerts.extend(drift_alerts)
        check_results["drift"] = drift_results

        # Check data quality
        quality_alerts, quality_results = self._check_data_quality(current_data)
        alerts.extend(quality_alerts)
        check_results["quality"] = quality_results

        # Check performance (if target available)
        if self.target_column and self.target_column in current_data.columns:
            perf_alerts, perf_results = self._check_performance(current_data)
            alerts.extend(perf_alerts)
            check_results["performance"] = perf_results

        self.history.append(check_results)
        return alerts

    def _check_data_drift(self, current_data: pd.DataFrame) -> tuple:
        """Check for data drift in features."""
        alerts = []
        drifted_features = []

        for feature in self.numerical_features:
            if feature not in current_data.columns:
                continue

            ref_values = self.reference_data[feature].dropna().values
            cur_values = current_data[feature].dropna().values

            if len(ref_values) == 0 or len(cur_values) == 0:
                continue

            drift_score, is_drifted = self.drift_detector.detect_drift(
                ref_values, cur_values, "numerical"
            )

            if is_drifted:
                drifted_features.append(feature)

        # Calculate drift share
        total_features = len(self.numerical_features) + len(self.categorical_features)
        drift_share = len(drifted_features) / total_features if total_features > 0 else 0

        if drift_share > self.drift_threshold:
            severity = AlertSeverity.CRITICAL if drift_share > 0.5 else AlertSeverity.WARNING
            alerts.append(MonitoringAlert(
                severity=severity,
                category="data_drift",
                message=f"Data drift detected in {drift_share:.1%} of features",
                details={
                    "drift_share": drift_share,
                    "drifted_features": drifted_features
                }
            ))

        return alerts, {
            "drift_share": drift_share,
            "drifted_features": drifted_features
        }

    def _check_data_quality(self, current_data: pd.DataFrame) -> tuple:
        """Check data quality issues."""
        alerts = []

        # Check missing values
        missing_ratio = current_data.isnull().mean().mean()

        if missing_ratio > self.missing_threshold:
            alerts.append(MonitoringAlert(
                severity=AlertSeverity.WARNING,
                category="data_quality",
                message=f"High missing value ratio: {missing_ratio:.1%}",
                details={"missing_ratio": missing_ratio}
            ))

        return alerts, {"missing_ratio": missing_ratio}

    def _check_performance(self, current_data: pd.DataFrame) -> tuple:
        """Check model performance."""
        alerts = []

        if self.prediction_column not in current_data.columns:
            return alerts, {}

        # Calculate accuracy
        predictions = current_data[self.prediction_column]
        targets = current_data[self.target_column]
        accuracy = (predictions == targets).mean()

        if accuracy < self.accuracy_threshold:
            severity = AlertSeverity.CRITICAL if accuracy < 0.6 else AlertSeverity.WARNING
            alerts.append(MonitoringAlert(
                severity=severity,
                category="performance",
                message=f"Model accuracy dropped to {accuracy:.1%}",
                details={
                    "accuracy": accuracy,
                    "threshold": self.accuracy_threshold
                }
            ))

        return alerts, {"accuracy": accuracy}

    def get_history(self) -> pd.DataFrame:
        """Get monitoring history as DataFrame."""
        if not self.history:
            return pd.DataFrame()

        rows = []
        for record in self.history:
            row = {
                "timestamp": record["timestamp"],
                "n_samples": record["n_samples"],
                "drift_share": record.get("drift", {}).get("drift_share", 0),
                "missing_ratio": record.get("quality", {}).get("missing_ratio", 0),
                "accuracy": record.get("performance", {}).get("accuracy", None)
            }
            rows.append(row)

        return pd.DataFrame(rows)


def main():
    """Example usage of ModelMonitor."""
    # Create sample data
    np.random.seed(42)

    # Reference data
    reference = pd.DataFrame({
        "feature1": np.random.normal(0, 1, 1000),
        "feature2": np.random.normal(5, 2, 1000),
        "target": np.random.randint(0, 2, 1000)
    })
    reference["prediction"] = reference["target"]

    # Current data with drift
    current = pd.DataFrame({
        "feature1": np.random.normal(0.5, 1, 200),  # Shifted mean
        "feature2": np.random.normal(5, 2, 200),
        "target": np.random.randint(0, 2, 200)
    })
    current["prediction"] = current["target"]

    # Initialize monitor
    monitor = ModelMonitor(
        reference_data=reference,
        target_column="target",
        prediction_column="prediction"
    )

    # Check for issues
    alerts = monitor.check(current)

    print("Monitoring Results:")
    print("=" * 50)
    for alert in alerts:
        print(f"[{alert.severity.value.upper()}] {alert.message}")

    print("\nHistory:")
    print(monitor.get_history())


if __name__ == "__main__":
    main()
