{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4.3.3: LLM Benchmark Suite\n",
    "\n",
    "**Module:** 4.3 - MLOps & Experiment Tracking  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐⭐ (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand the major LLM benchmarks and what they measure\n",
    "- [ ] Run standard benchmarks using lm-evaluation-harness\n",
    "- [ ] Interpret benchmark results correctly\n",
    "- [ ] Compare models using standardized evaluations\n",
    "- [ ] Avoid common benchmarking pitfalls\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Tasks 4.3.1-4.3.2 (MLflow/W&B Setup)\n",
    "- Knowledge of: LLM basics, transformers\n",
    "- Hardware: DGX Spark with at least 20GB free GPU memory\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "When Meta releases Llama 3.1, or when Mistral announces a new model, how do they prove it's better than competitors?\n",
    "\n",
    "**Benchmarks.**\n",
    "\n",
    "Every major AI lab uses standardized benchmarks to:\n",
    "- Compare models objectively\n",
    "- Track progress over time\n",
    "- Identify strengths and weaknesses\n",
    "- Justify research claims in papers\n",
    "\n",
    "Without proper benchmarking, you're just guessing if your model is good.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELI5: What are LLM Benchmarks?\n",
    "\n",
    "> **Imagine you're testing how smart different students are.**\n",
    ">\n",
    "> You could ask each student random questions, but that's not fair! One might get easy questions, another hard ones.\n",
    ">\n",
    "> Instead, you give everyone the **same standardized test** - like the SAT. Now you can compare fairly:\n",
    "> - Math section = reasoning ability\n",
    "> - Reading section = language understanding\n",
    "> - Writing section = generation quality\n",
    ">\n",
    "> **In AI terms:** Benchmarks are standardized tests for LLMs. MMLU tests knowledge, HellaSwag tests common sense, HumanEval tests coding. Same questions for every model, fair comparison.\n",
    "\n",
    "---\n",
    "\n",
    "## The Major LLM Benchmarks\n",
    "\n",
    "| Benchmark | What It Tests | # Questions | How It Works |\n",
    "|-----------|---------------|-------------|-------------|\n",
    "| **MMLU** | World knowledge | 14,042 | Multiple choice (57 subjects) |\n",
    "| **HellaSwag** | Common sense | 10,042 | Sentence completion |\n",
    "| **ARC** | Science reasoning | 7,787 | Multiple choice (grade school) |\n",
    "| **WinoGrande** | Pronoun resolution | 1,767 | Fill in the blank |\n",
    "| **TruthfulQA** | Factual accuracy | 817 | Multiple choice |\n",
    "| **GSM8K** | Math reasoning | 1,319 | Word problems |\n",
    "| **HumanEval** | Coding | 164 | Write Python functions |\n",
    "| **MT-Bench** | Chat quality | 80 | LLM-as-judge scoring |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting Up lm-evaluation-harness\n",
    "\n",
    "EleutherAI's [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) is the industry standard for LLM benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install lm-eval\n",
    "!pip install lm-eval -q\n",
    "\n",
    "import lm_eval\n",
    "print(f\"lm-eval version: {lm_eval.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available GPU memory before loading models\n",
    "import torch\n",
    "\n",
    "def print_gpu_memory():\n",
    "    \"\"\"Print current GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        cached = torch.cuda.memory_reserved(0) / 1024**3\n",
    "        free = total - cached\n",
    "        print(f\"GPU Memory:\")\n",
    "        print(f\"  Total:     {total:.1f} GB\")\n",
    "        print(f\"  Allocated: {allocated:.1f} GB\")\n",
    "        print(f\"  Cached:    {cached:.1f} GB\")\n",
    "        print(f\"  Free:      {free:.1f} GB\")\n",
    "    else:\n",
    "        print(\"No GPU available\")\n",
    "\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available benchmark tasks\n",
    "from lm_eval import tasks\n",
    "\n",
    "# Get all available tasks\n",
    "all_tasks = tasks.ALL_TASKS\n",
    "print(f\"Total available tasks: {len(all_tasks)}\")\n",
    "print(\"\\nPopular benchmarks:\")\n",
    "\n",
    "popular = ['mmlu', 'hellaswag', 'arc_easy', 'arc_challenge', 'winogrande', \n",
    "           'truthfulqa_mc', 'gsm8k', 'humaneval']\n",
    "for task in popular:\n",
    "    status = \"available\" if task in all_tasks else \"not found\"\n",
    "    print(f\"  - {task}: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Running Your First Benchmark\n",
    "\n",
    "Let's benchmark a small model first to understand the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear cache before loading model\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Starting benchmark run...\")\n",
    "print(\"This will:\")\n",
    "print(\"  1. Load the model (microsoft/phi-2)\")\n",
    "print(\"  2. Run HellaSwag benchmark (10k questions)\")\n",
    "print(\"  3. Report accuracy metrics\")\n",
    "print(\"\\nEstimated time: 10-15 minutes on DGX Spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_eval import evaluator\n",
    "from lm_eval.models.huggingface import HFLM\n",
    "import time\n",
    "\n",
    "def run_benchmark(\n",
    "    model_name: str,\n",
    "    tasks: list,\n",
    "    batch_size: int = 8,\n",
    "    limit: int = None,\n",
    "    dtype: str = \"bfloat16\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Run lm-eval benchmarks on a model.\n",
    "    \n",
    "    Args:\n",
    "        model_name: HuggingFace model ID\n",
    "        tasks: List of benchmark tasks to run\n",
    "        batch_size: Batch size for inference\n",
    "        limit: Limit number of examples (None for full benchmark)\n",
    "        dtype: Model dtype (bfloat16 recommended for DGX Spark)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Benchmark results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Benchmarking: {model_name}\")\n",
    "    print(f\"Tasks: {tasks}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    if limit:\n",
    "        print(f\"Limit: {limit} examples per task\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluator.simple_evaluate(\n",
    "        model=\"hf\",\n",
    "        model_args=f\"pretrained={model_name},dtype={dtype}\",\n",
    "        tasks=tasks,\n",
    "        batch_size=batch_size,\n",
    "        limit=limit,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RESULTS for {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for task_name, task_results in results['results'].items():\n",
    "        print(f\"\\n{task_name}:\")\n",
    "        for metric, value in task_results.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"  {metric}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"  {metric}: {value}\")\n",
    "    \n",
    "    print(f\"\\nTotal time: {elapsed/60:.1f} minutes\")\n",
    "    print_gpu_memory()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a quick benchmark with limited examples\n",
    "# Using limit=100 for demonstration (full run takes longer)\n",
    "\n",
    "quick_results = run_benchmark(\n",
    "    model_name=\"microsoft/phi-2\",\n",
    "    tasks=[\"hellaswag\"],\n",
    "    batch_size=8,\n",
    "    limit=100  # Remove this for full benchmark\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "The benchmark:\n",
    "1. Loaded the model onto GPU\n",
    "2. Fed it 100 HellaSwag questions (sentence completion)\n",
    "3. Calculated accuracy (how often it picked the right completion)\n",
    "\n",
    "**Note:** With `limit=100`, this is a quick approximation. For publishable results, run the full benchmark!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Benchmarks\n",
    "\n",
    "### HellaSwag: Common Sense Reasoning\n",
    "\n",
    "Example question:\n",
    "> **Context:** \"A woman is standing at a podium. She\"\n",
    "> \n",
    "> **Choices:**\n",
    "> - A) \"adjusts the microphone and begins speaking.\"\n",
    "> - B) \"starts doing jumping jacks.\"\n",
    "> - C) \"eats a sandwich loudly.\"\n",
    "> - D) \"falls asleep immediately.\"\n",
    ">\n",
    "> **Answer:** A\n",
    "\n",
    "Models need common sense to know what typically happens next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at actual HellaSwag examples\n",
    "from datasets import load_dataset\n",
    "\n",
    "hellaswag = load_dataset(\"hellaswag\", split=\"validation[:5]\")\n",
    "\n",
    "print(\"Sample HellaSwag questions:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, example in enumerate(hellaswag):\n",
    "    print(f\"\\nQuestion {i+1}:\")\n",
    "    print(f\"Activity: {example['activity_label']}\")\n",
    "    print(f\"Context: {example['ctx']}\")\n",
    "    print(f\"Endings:\")\n",
    "    for j, ending in enumerate(example['endings']):\n",
    "        marker = \"-> \" if j == int(example['label']) else \"   \"\n",
    "        print(f\"  {marker}{chr(65+j)}) {ending[:80]}...\" if len(ending) > 80 else f\"  {marker}{chr(65+j)}) {ending}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MMLU: Massive Multitask Language Understanding\n",
    "\n",
    "MMLU tests knowledge across 57 subjects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMLU subjects\n",
    "mmlu_subjects = [\n",
    "    # STEM\n",
    "    \"abstract_algebra\", \"anatomy\", \"astronomy\", \"computer_science\",\n",
    "    \"electrical_engineering\", \"machine_learning\", \"physics\",\n",
    "    # Humanities\n",
    "    \"philosophy\", \"world_history\", \"moral_scenarios\", \"us_history\",\n",
    "    # Social Sciences\n",
    "    \"economics\", \"psychology\", \"sociology\", \"political_science\",\n",
    "    # Other\n",
    "    \"clinical_knowledge\", \"medical_genetics\", \"professional_law\",\n",
    "    \"professional_accounting\", \"marketing\"\n",
    "]\n",
    "\n",
    "print(\"MMLU covers 57 subjects across 4 categories:\")\n",
    "print(f\"\\nSample subjects: {mmlu_subjects[:10]}...\")\n",
    "print(\"\\nDifficulty ranges from high school to professional level.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample MMLU question\n",
    "mmlu_sample = load_dataset(\"cais/mmlu\", \"computer_science\", split=\"test[:3]\")\n",
    "\n",
    "print(\"Sample MMLU Computer Science questions:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, example in enumerate(mmlu_sample):\n",
    "    print(f\"\\nQuestion {i+1}: {example['question']}\")\n",
    "    for j, choice in enumerate(example['choices']):\n",
    "        marker = \"-> \" if j == example['answer'] else \"   \"\n",
    "        print(f\"  {marker}{chr(65+j)}) {choice}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Comparing Multiple Models\n",
    "\n",
    "Let's benchmark multiple models and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to compare (small models for faster demo)\n",
    "models_to_test = [\n",
    "    \"microsoft/phi-2\",           # 2.7B params\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # 1.1B params\n",
    "]\n",
    "\n",
    "# Benchmarks to run\n",
    "benchmark_tasks = [\"hellaswag\", \"arc_easy\"]\n",
    "\n",
    "print(\"Model Comparison Plan:\")\n",
    "print(f\"  Models: {models_to_test}\")\n",
    "print(f\"  Benchmarks: {benchmark_tasks}\")\n",
    "print(f\"  Note: Using limit=50 for quick comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comparisons\n",
    "comparison_results = {}\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    # Clear GPU memory between models\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    try:\n",
    "        results = run_benchmark(\n",
    "            model_name=model_name,\n",
    "            tasks=benchmark_tasks,\n",
    "            batch_size=4,\n",
    "            limit=50  # Small limit for demo\n",
    "        )\n",
    "        comparison_results[model_name] = results\n",
    "    except Exception as e:\n",
    "        print(f\"Error benchmarking {model_name}: {e}\")\n",
    "        comparison_results[model_name] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, results in comparison_results.items():\n",
    "    if results is None:\n",
    "        continue\n",
    "    \n",
    "    row = {\"model\": model_name.split(\"/\")[-1]}\n",
    "    \n",
    "    for task_name, task_results in results['results'].items():\n",
    "        # Get accuracy metric (different tasks use different names)\n",
    "        acc_key = [k for k in task_results.keys() if 'acc' in k.lower()]\n",
    "        if acc_key:\n",
    "            row[task_name] = task_results[acc_key[0]]\n",
    "    \n",
    "    comparison_data.append(row)\n",
    "\n",
    "if comparison_data:\n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    print(df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No results to compare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Running Benchmarks from Command Line\n",
    "\n",
    "For production use, the command line is often more convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example command for full benchmark\n",
    "cli_command = \"\"\"\n",
    "# Full MMLU benchmark (takes ~2-3 hours for an 8B model)\n",
    "lm_eval --model hf \\\\\n",
    "    --model_args pretrained=meta-llama/Llama-3.1-8B,dtype=bfloat16 \\\\\n",
    "    --tasks mmlu \\\\\n",
    "    --batch_size 8 \\\\\n",
    "    --output_path ./results/llama-3.1-8b-mmlu\n",
    "\n",
    "# Quick benchmark suite (common tasks)\n",
    "lm_eval --model hf \\\\\n",
    "    --model_args pretrained=meta-llama/Llama-3.1-8B,dtype=bfloat16 \\\\\n",
    "    --tasks hellaswag,arc_easy,arc_challenge,winogrande \\\\\n",
    "    --batch_size 8 \\\\\n",
    "    --output_path ./results/llama-3.1-8b-quick\n",
    "\n",
    "# With logging to W&B\n",
    "lm_eval --model hf \\\\\n",
    "    --model_args pretrained=microsoft/phi-2,dtype=bfloat16 \\\\\n",
    "    --tasks hellaswag,mmlu \\\\\n",
    "    --batch_size 8 \\\\\n",
    "    --wandb_args project=llm-benchmarks,name=phi2-eval \\\\\n",
    "    --output_path ./results/phi2\n",
    "\"\"\"\n",
    "\n",
    "print(\"Command Line Examples:\")\n",
    "print(cli_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a quick CLI benchmark\n",
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=microsoft/phi-2,dtype=bfloat16 \\\n",
    "    --tasks arc_easy \\\n",
    "    --batch_size 4 \\\n",
    "    --limit 20 \\\n",
    "    --output_path ./results/phi2_quick_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Integrating with MLflow\n",
    "\n",
    "Let's log benchmark results to MLflow for tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import json\n",
    "\n",
    "def benchmark_with_mlflow(\n",
    "    model_name: str,\n",
    "    tasks: list,\n",
    "    batch_size: int = 8,\n",
    "    limit: int = None,\n",
    "    experiment_name: str = \"LLM-Benchmarks\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Run benchmarks and log results to MLflow.\n",
    "    \"\"\"\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"{model_name.split('/')[-1]}-benchmark\"):\n",
    "        # Log configuration\n",
    "        mlflow.log_params({\n",
    "            \"model_name\": model_name,\n",
    "            \"tasks\": \",\".join(tasks),\n",
    "            \"batch_size\": batch_size,\n",
    "            \"limit\": limit or \"full\",\n",
    "            \"dtype\": \"bfloat16\"\n",
    "        })\n",
    "        \n",
    "        # Run benchmark\n",
    "        results = evaluator.simple_evaluate(\n",
    "            model=\"hf\",\n",
    "            model_args=f\"pretrained={model_name},dtype=bfloat16\",\n",
    "            tasks=tasks,\n",
    "            batch_size=batch_size,\n",
    "            limit=limit,\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        \n",
    "        # Log metrics\n",
    "        for task_name, task_results in results['results'].items():\n",
    "            for metric, value in task_results.items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    mlflow.log_metric(f\"{task_name}_{metric}\", value)\n",
    "        \n",
    "        # Save full results as artifact\n",
    "        with open(\"benchmark_results.json\", \"w\") as f:\n",
    "            json.dump(results, f, indent=2, default=str)\n",
    "        mlflow.log_artifact(\"benchmark_results.json\")\n",
    "        \n",
    "        print(f\"Results logged to MLflow run: {mlflow.active_run().info.run_id}\")\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmark with MLflow logging\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "mlflow_results = benchmark_with_mlflow(\n",
    "    model_name=\"microsoft/phi-2\",\n",
    "    tasks=[\"arc_easy\"],\n",
    "    batch_size=4,\n",
    "    limit=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Understanding Benchmark Scores\n",
    "\n",
    "### What's a \"Good\" Score?\n",
    "\n",
    "| Benchmark | Random Baseline | Good (7B) | Excellent (70B+) | Human Level |\n",
    "|-----------|-----------------|-----------|------------------|-------------|\n",
    "| MMLU | 25% | 55-65% | 80-90% | ~90% |\n",
    "| HellaSwag | 25% | 75-85% | 90-95% | ~95% |\n",
    "| ARC-Easy | 25% | 75-85% | 90-95% | ~95% |\n",
    "| ARC-Challenge | 25% | 45-55% | 70-85% | ~80% |\n",
    "| WinoGrande | 50% | 70-75% | 85-90% | ~94% |\n",
    "| GSM8K | 0% | 30-50% | 70-90% | ~100% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference scores from model cards\n",
    "reference_scores = {\n",
    "    \"Model\": [\"Llama-2-7B\", \"Llama-2-70B\", \"Llama-3.1-8B\", \"Llama-3.1-70B\", \"GPT-4\"],\n",
    "    \"MMLU\": [45.3, 69.8, 66.6, 79.3, 86.4],\n",
    "    \"HellaSwag\": [77.2, 85.3, 80.1, 88.0, 95.3],\n",
    "    \"ARC-C\": [53.1, 68.0, 55.4, 68.8, 96.3],\n",
    "    \"WinoGrande\": [74.0, 80.5, 77.0, 85.3, 87.5],\n",
    "}\n",
    "\n",
    "ref_df = pd.DataFrame(reference_scores)\n",
    "print(\"Reference Benchmark Scores (from model cards):\")\n",
    "print(\"=\"*60)\n",
    "print(ref_df.to_string(index=False))\n",
    "print(\"\\nNote: These are normalized accuracy percentages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself\n",
    "\n",
    "Run a full benchmark comparison on two models of your choice.\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Try comparing:\n",
    "- A base model vs. its instruction-tuned version\n",
    "- Models from different families at similar sizes\n",
    "- A model before and after your fine-tuning\n",
    "\n",
    "Use `limit=None` for accurate results (will take longer).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Compare two models on at least 2 benchmarks\n",
    "\n",
    "# Suggested models:\n",
    "# - \"meta-llama/Llama-3.2-1B\" (1B)\n",
    "# - \"meta-llama/Llama-3.2-3B\" (3B)\n",
    "# - \"microsoft/phi-2\" (2.7B)\n",
    "\n",
    "# Your comparison code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Cherry-Picking Benchmarks\n",
    "\n",
    "```python\n",
    "# Wrong - only reporting where your model does well\n",
    "results = run_benchmark(model, tasks=[\"my_model_is_great_at_this\"])\n",
    "\n",
    "# Right - run standard benchmark suite\n",
    "results = run_benchmark(model, tasks=[\"mmlu\", \"hellaswag\", \"arc_challenge\", \"winogrande\"])\n",
    "```\n",
    "**Why:** Cherry-picking gives a misleading picture of model quality.\n",
    "\n",
    "### Mistake 2: Comparing Apples to Oranges\n",
    "\n",
    "```python\n",
    "# Wrong - different evaluation settings\n",
    "model_a_results = run_benchmark(model_a, batch_size=8, limit=100)\n",
    "model_b_results = run_benchmark(model_b, batch_size=1, limit=None)\n",
    "\n",
    "# Right - identical settings for fair comparison\n",
    "for model in [model_a, model_b]:\n",
    "    results = run_benchmark(model, batch_size=8, limit=None)\n",
    "```\n",
    "**Why:** Different batch sizes or limits can affect results.\n",
    "\n",
    "### Mistake 3: Using Limit for Published Results\n",
    "\n",
    "```python\n",
    "# Wrong for publication - limited examples\n",
    "results = run_benchmark(model, tasks=[\"mmlu\"], limit=100)\n",
    "print(f\"MMLU Score: {results}%\")  # Not statistically valid!\n",
    "\n",
    "# Right for publication - full benchmark\n",
    "results = run_benchmark(model, tasks=[\"mmlu\"], limit=None)\n",
    "```\n",
    "**Why:** Small samples have high variance and aren't reproducible.\n",
    "\n",
    "### Mistake 4: Ignoring Benchmark Leakage\n",
    "\n",
    "```python\n",
    "# Be aware: training data might contain benchmark questions!\n",
    "# This inflates scores artificially.\n",
    "\n",
    "# Good practice: report training data sources\n",
    "# Good practice: use held-out test sets\n",
    "# Good practice: report results on multiple benchmarks\n",
    "```\n",
    "**Why:** Some models are accidentally (or intentionally) trained on benchmark data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- What major LLM benchmarks measure (MMLU, HellaSwag, ARC, etc.)\n",
    "- How to run benchmarks using lm-evaluation-harness\n",
    "- How to compare models fairly\n",
    "- How to integrate benchmarks with MLflow tracking\n",
    "- Common benchmarking pitfalls to avoid\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "Create a benchmark dashboard that:\n",
    "1. Runs 5 standard benchmarks on a model\n",
    "2. Logs all results to MLflow\n",
    "3. Creates a radar chart comparing to reference scores\n",
    "4. Saves a summary report as an artifact\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)\n",
    "- [HELM Benchmark](https://crfm.stanford.edu/helm/) - Stanford's holistic evaluation\n",
    "- [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n",
    "- [Chatbot Arena](https://arena.lmsys.org/) - Human preference rankings\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Clean up temporary files\n",
    "if os.path.exists(\"benchmark_results.json\"):\n",
    "    os.remove(\"benchmark_results.json\")\n",
    "\n",
    "print(\"Cleanup complete!\")\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Standard benchmarks are great for comparing to other models, but what about your specific use case? The next notebook covers **custom evaluation** - creating task-specific metrics and using LLM-as-judge for nuanced assessment.\n",
    "\n",
    "**Continue to:** [04-custom-evaluation.ipynb](04-custom-evaluation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
