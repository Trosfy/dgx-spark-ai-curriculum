{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4.3.1: MLflow Setup for LLM Experiment Tracking\n",
    "\n",
    "**Module:** 4.3 - MLOps & Experiment Tracking  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐ (Beginner-Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand why experiment tracking is essential for ML projects\n",
    "- [ ] Set up and configure MLflow tracking server on DGX Spark\n",
    "- [ ] Log parameters, metrics, and artifacts from training runs\n",
    "- [ ] Navigate the MLflow UI to compare experiments\n",
    "- [ ] Use MLflow's autolog feature for PyTorch models\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Module 4.2 (AI Safety) or basic understanding of model training\n",
    "- Knowledge of: Python, basic ML training loops\n",
    "- Running inside NGC PyTorch container with port 5000 exposed\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "Picture this: You're working on fine-tuning an LLM for your company. After 50 experiments over 3 weeks, your boss asks:\n",
    "\n",
    "> \"Which configuration gave us the best results? And can you reproduce it?\"\n",
    "\n",
    "Without proper tracking, you're digging through terminal logs, trying to remember which hyperparameters you used, and hoping you saved that one good checkpoint somewhere.\n",
    "\n",
    "**MLflow solves this.** Companies like Databricks, Meta, and Microsoft use it to track thousands of experiments, ensuring every result is reproducible and comparable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELI5: What is Experiment Tracking?\n",
    "\n",
    "> **Imagine you're baking the perfect chocolate chip cookie.**\n",
    ">\n",
    "> You try different recipes: more sugar, less butter, different oven temperatures. But if you don't write down what you did each time, you'll forget which batch was the best!\n",
    ">\n",
    "> Experiment tracking is your **recipe book** for machine learning. Every time you train a model, you write down:\n",
    "> - What ingredients (hyperparameters) you used\n",
    "> - How it turned out (metrics like accuracy, loss)\n",
    "> - The actual cookie (the saved model)\n",
    ">\n",
    "> **In AI terms:** MLflow is like a lab notebook that automatically records everything about your ML experiments, so you can always find and reproduce your best results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting Up MLflow\n",
    "\n",
    "### Understanding MLflow Components\n",
    "\n",
    "MLflow has four main components:\n",
    "\n",
    "| Component | Purpose | Analogy |\n",
    "|-----------|---------|--------|\n",
    "| **Tracking** | Log experiments | Recipe book |\n",
    "| **Projects** | Package code | Recipe cards |\n",
    "| **Models** | Package models | Cookie tin |\n",
    "| **Registry** | Version models | Cookbook editions |\n",
    "\n",
    "Today we'll focus on **Tracking** - the foundation of everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's install and verify MLflow\n",
    "!pip install mlflow -q\n",
    "\n",
    "import mlflow\n",
    "print(f\"MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our DGX Spark environment\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the MLflow Tracking Server\n",
    "\n",
    "In production, you'd run the MLflow server as a separate process. For learning, we'll use MLflow's local file-based tracking.\n",
    "\n",
    "**Note:** To run the full UI server, execute this in a separate terminal:\n",
    "```bash\n",
    "mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri ./mlruns\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure MLflow to use local storage\n",
    "# In production, you'd use a remote tracking server\n",
    "\n",
    "MLFLOW_TRACKING_DIR = \"./mlruns\"\n",
    "os.makedirs(MLFLOW_TRACKING_DIR, exist_ok=True)\n",
    "\n",
    "# Set tracking URI to local directory\n",
    "mlflow.set_tracking_uri(f\"file://{os.path.abspath(MLFLOW_TRACKING_DIR)}\")\n",
    "\n",
    "print(f\"Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"\\nMLflow will store experiments in: {os.path.abspath(MLFLOW_TRACKING_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "We configured MLflow to store all experiment data locally. In a team setting, you'd point this to a shared server so everyone can see each other's experiments.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: Your First Tracked Experiment\n",
    "\n",
    "Let's track a simple training run. We'll create a toy classification problem to demonstrate the concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a toy dataset for demonstration\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic data\n",
    "n_samples = 1000\n",
    "n_features = 20\n",
    "\n",
    "X = torch.randn(n_samples, n_features)\n",
    "# True weights for our classification\n",
    "true_weights = torch.randn(n_features)\n",
    "y = (X @ true_weights > 0).float()\n",
    "\n",
    "# Split into train/val\n",
    "train_X, val_X = X[:800], X[800:]\n",
    "train_y, val_y = y[:800], y[800:]\n",
    "\n",
    "print(f\"Training samples: {len(train_X)}\")\n",
    "print(f\"Validation samples: {len(val_X)}\")\n",
    "print(f\"Features: {n_features}\")\n",
    "print(f\"Class balance: {y.mean():.2%} positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network\n",
    "class SimpleClassifier(nn.Module):\n",
    "    \"\"\"A simple 2-layer neural network for binary classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze(-1)\n",
    "\n",
    "# Quick test\n",
    "model = SimpleClassifier(n_features, 64)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Let's Track an Experiment!\n",
    "\n",
    "This is where MLflow shines. We'll log:\n",
    "- **Parameters**: hyperparameters like learning rate, hidden size\n",
    "- **Metrics**: loss, accuracy (tracked over time)\n",
    "- **Artifacts**: saved model, plots\n",
    "- **Tags**: metadata for organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_mlflow(\n",
    "    hidden_dim: int = 64,\n",
    "    learning_rate: float = 0.01,\n",
    "    dropout: float = 0.1,\n",
    "    epochs: int = 20,\n",
    "    batch_size: int = 32,\n",
    "    experiment_name: str = \"Binary-Classification-Demo\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a simple classifier with full MLflow tracking.\n",
    "    \n",
    "    This demonstrates the core MLflow logging capabilities.\n",
    "    \"\"\"\n",
    "    # Set or create experiment\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    # Start a new run\n",
    "    with mlflow.start_run(run_name=f\"hidden-{hidden_dim}-lr-{learning_rate}\"):\n",
    "        # Log parameters (the recipe)\n",
    "        mlflow.log_param(\"hidden_dim\", hidden_dim)\n",
    "        mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "        mlflow.log_param(\"dropout\", dropout)\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        \n",
    "        # Log tags (metadata)\n",
    "        mlflow.set_tag(\"model_type\", \"SimpleClassifier\")\n",
    "        mlflow.set_tag(\"developer\", \"SPARK\")\n",
    "        mlflow.set_tag(\"environment\", \"DGX Spark\")\n",
    "        \n",
    "        # Create model\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = SimpleClassifier(n_features, hidden_dim, dropout).to(device)\n",
    "        \n",
    "        # Log model architecture as artifact\n",
    "        with open(\"model_architecture.txt\", \"w\") as f:\n",
    "            f.write(str(model))\n",
    "        mlflow.log_artifact(\"model_architecture.txt\")\n",
    "        os.remove(\"model_architecture.txt\")\n",
    "        \n",
    "        # Setup training\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        train_dataset = TensorDataset(train_X.to(device), train_y.to(device))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Training loop\n",
    "        best_val_acc = 0.0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            epoch_loss = 0.0\n",
    "            \n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            avg_loss = epoch_loss / len(train_loader)\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(val_X.to(device))\n",
    "                val_preds = (val_outputs > 0.5).float()\n",
    "                val_acc = (val_preds == val_y.to(device)).float().mean().item()\n",
    "                val_loss = criterion(val_outputs, val_y.to(device)).item()\n",
    "            \n",
    "            # Log metrics with step number\n",
    "            mlflow.log_metrics({\n",
    "                \"train_loss\": avg_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_accuracy\": val_acc\n",
    "            }, step=epoch)\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"Epoch {epoch:2d}: train_loss={avg_loss:.4f}, val_acc={val_acc:.4f}\")\n",
    "        \n",
    "        # Log final metrics\n",
    "        mlflow.log_metric(\"best_val_accuracy\", best_val_acc)\n",
    "        \n",
    "        # Log the model\n",
    "        mlflow.pytorch.log_model(model, \"model\")\n",
    "        \n",
    "        print(f\"\\nRun completed! Best validation accuracy: {best_val_acc:.4f}\")\n",
    "        print(f\"Run ID: {mlflow.active_run().info.run_id}\")\n",
    "        \n",
    "        return model, best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run our first tracked experiment!\n",
    "model, acc = train_with_mlflow(\n",
    "    hidden_dim=64,\n",
    "    learning_rate=0.01,\n",
    "    epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "MLflow automatically:\n",
    "1. Created an experiment called \"Binary-Classification-Demo\"\n",
    "2. Started a run with our parameters logged\n",
    "3. Tracked loss and accuracy at each epoch\n",
    "4. Saved the trained model\n",
    "5. Generated a unique run ID for future reference\n",
    "\n",
    "Let's run a few more experiments with different configurations to see the power of comparison!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple experiments with different hyperparameters\n",
    "experiments = [\n",
    "    {\"hidden_dim\": 32, \"learning_rate\": 0.001, \"dropout\": 0.1},\n",
    "    {\"hidden_dim\": 128, \"learning_rate\": 0.01, \"dropout\": 0.2},\n",
    "    {\"hidden_dim\": 64, \"learning_rate\": 0.1, \"dropout\": 0.0},\n",
    "]\n",
    "\n",
    "results = []\n",
    "for config in experiments:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Running: {config}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    _, acc = train_with_mlflow(**config, epochs=20)\n",
    "    results.append((config, acc))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Summary of all experiments:\")\n",
    "print(\"=\"*50)\n",
    "for config, acc in results:\n",
    "    print(f\"Accuracy: {acc:.4f} | {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Exploring Experiments Programmatically\n",
    "\n",
    "The MLflow UI is great, but you can also query experiments with Python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all experiments\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "print(\"All experiments:\")\n",
    "for exp in client.search_experiments():\n",
    "    print(f\"  - {exp.name} (ID: {exp.experiment_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all runs from our experiment\n",
    "experiment = client.get_experiment_by_name(\"Binary-Classification-Demo\")\n",
    "\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    order_by=[\"metrics.best_val_accuracy DESC\"]\n",
    ")\n",
    "\n",
    "print(f\"Found {len(runs)} runs in 'Binary-Classification-Demo'\")\n",
    "print(\"\\nTop runs by validation accuracy:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for run in runs[:5]:\n",
    "    params = run.data.params\n",
    "    metrics = run.data.metrics\n",
    "    print(f\"Run: {run.info.run_id[:8]}... | \"\n",
    "          f\"Acc: {metrics.get('best_val_accuracy', 0):.4f} | \"\n",
    "          f\"hidden={params.get('hidden_dim', 'N/A')}, \"\n",
    "          f\"lr={params.get('learning_rate', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_run = runs[0]\n",
    "print(f\"Loading best model from run: {best_run.info.run_id}\")\n",
    "\n",
    "# Load model from MLflow\n",
    "best_model_uri = f\"runs:/{best_run.info.run_id}/model\"\n",
    "loaded_model = mlflow.pytorch.load_model(best_model_uri)\n",
    "\n",
    "# Test it\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    device = next(loaded_model.parameters()).device\n",
    "    test_output = loaded_model(val_X[:5].to(device))\n",
    "    print(f\"\\nSample predictions: {test_output.cpu().numpy().round(3)}\")\n",
    "    print(f\"Actual labels:      {val_y[:5].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: MLflow Autologging for PyTorch\n",
    "\n",
    "MLflow can automatically log metrics without modifying your training code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autologging for PyTorch\n",
    "mlflow.pytorch.autolog(\n",
    "    log_every_n_epoch=1,\n",
    "    log_models=True,\n",
    "    disable=False,\n",
    "    exclusive=False\n",
    ")\n",
    "\n",
    "print(\"PyTorch autologging enabled!\")\n",
    "print(\"MLflow will automatically log:\")\n",
    "print(\"  - Model parameters\")\n",
    "print(\"  - Training/validation metrics\")\n",
    "print(\"  - Model architecture\")\n",
    "print(\"  - Saved model artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate autologging with PyTorch Lightning (cleaner approach)\n",
    "# Note: For vanilla PyTorch, autolog works with some training frameworks\n",
    "\n",
    "# Let's create a simple example without Lightning\n",
    "mlflow.set_experiment(\"Autolog-Demo\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"autolog-example\"):\n",
    "    # Create model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SimpleClassifier(n_features, 64).to(device)\n",
    "    \n",
    "    # Manual training for now - autolog captures what it can\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        outputs = model(train_X.to(device))\n",
    "        loss = criterion(outputs, train_y.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # These will be captured by autolog\n",
    "        mlflow.log_metric(\"epoch_loss\", loss.item(), step=epoch)\n",
    "    \n",
    "    # Log the final model\n",
    "    mlflow.pytorch.log_model(model, \"autolog_model\")\n",
    "    print(\"Autolog run complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Tracking LLM Fine-Tuning (Preview)\n",
    "\n",
    "Let's see how you'd track a real LLM fine-tuning job. We'll use a mock example that shows the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_llm_finetuning_run():\n",
    "    \"\"\"\n",
    "    Demonstrates how to track an LLM fine-tuning experiment.\n",
    "    This is a mock example - replace with actual training code.\n",
    "    \"\"\"\n",
    "    mlflow.set_experiment(\"LLM-Finetuning-Demo\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"phi2-lora-r16\"):\n",
    "        # Log LLM-specific parameters\n",
    "        mlflow.log_params({\n",
    "            # Model config\n",
    "            \"base_model\": \"microsoft/phi-2\",\n",
    "            \"model_size\": \"2.7B\",\n",
    "            \n",
    "            # LoRA config\n",
    "            \"adapter_type\": \"LoRA\",\n",
    "            \"lora_r\": 16,\n",
    "            \"lora_alpha\": 32,\n",
    "            \"lora_dropout\": 0.05,\n",
    "            \"target_modules\": \"q_proj,v_proj,k_proj,o_proj\",\n",
    "            \n",
    "            # Training config\n",
    "            \"batch_size\": 4,\n",
    "            \"gradient_accumulation_steps\": 8,\n",
    "            \"effective_batch_size\": 32,\n",
    "            \"learning_rate\": 2e-4,\n",
    "            \"lr_scheduler\": \"cosine\",\n",
    "            \"warmup_ratio\": 0.03,\n",
    "            \"num_epochs\": 3,\n",
    "            \"max_seq_length\": 2048,\n",
    "            \n",
    "            # Quantization\n",
    "            \"quantization\": \"4-bit\",\n",
    "            \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
    "            \n",
    "            # Data\n",
    "            \"dataset\": \"custom_instruction_data\",\n",
    "            \"train_samples\": 10000,\n",
    "            \"val_samples\": 1000\n",
    "        })\n",
    "        \n",
    "        # Log DGX Spark environment info\n",
    "        mlflow.set_tags({\n",
    "            \"hardware\": \"DGX Spark\",\n",
    "            \"gpu_memory\": \"128GB Unified\",\n",
    "            \"gpu_type\": \"Blackwell GB10\",\n",
    "            \"container\": \"nvcr.io/nvidia/pytorch:25.11-py3\"\n",
    "        })\n",
    "        \n",
    "        # Simulate training metrics over epochs\n",
    "        import random\n",
    "        random.seed(42)\n",
    "        \n",
    "        base_loss = 2.5\n",
    "        for step in range(100):\n",
    "            # Simulate decreasing loss\n",
    "            train_loss = base_loss * (0.95 ** (step / 10)) + random.uniform(-0.05, 0.05)\n",
    "            val_loss = train_loss * 1.1 + random.uniform(-0.03, 0.03)\n",
    "            \n",
    "            mlflow.log_metrics({\n",
    "                \"train/loss\": train_loss,\n",
    "                \"val/loss\": val_loss,\n",
    "                \"train/learning_rate\": 2e-4 * max(0.1, 1 - step/100),\n",
    "                \"train/tokens_per_second\": 5000 + random.randint(-100, 100),\n",
    "                \"memory/gpu_allocated_gb\": 45 + random.uniform(-2, 2)\n",
    "            }, step=step)\n",
    "        \n",
    "        # Log final evaluation metrics\n",
    "        mlflow.log_metrics({\n",
    "            \"eval/final_loss\": 1.23,\n",
    "            \"eval/perplexity\": 3.42,\n",
    "            \"eval/accuracy\": 0.78,\n",
    "            \"training_time_hours\": 2.5\n",
    "        })\n",
    "        \n",
    "        print(\"Mock LLM fine-tuning run logged!\")\n",
    "        print(f\"Run ID: {mlflow.active_run().info.run_id}\")\n",
    "\n",
    "mock_llm_finetuning_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself\n",
    "\n",
    "Now it's your turn! Create an experiment that tracks different optimization algorithms.\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Try these optimizers: `Adam`, `SGD`, `AdamW`, and compare their convergence.\n",
    "Log the optimizer name as a parameter and track the loss curves.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create an experiment called \"Optimizer-Comparison\"\n",
    "# Try at least 3 different optimizers\n",
    "# Log the optimizer type, learning rate, and track training loss\n",
    "\n",
    "# Starter code:\n",
    "optimizers_to_try = [\n",
    "    (\"Adam\", optim.Adam),\n",
    "    (\"SGD\", optim.SGD),\n",
    "    (\"AdamW\", optim.AdamW),\n",
    "]\n",
    "\n",
    "# Your experiment code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Setting Experiment Name\n",
    "\n",
    "```python\n",
    "# Wrong - runs go to \"Default\" experiment\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_metric(\"loss\", 0.5)\n",
    "\n",
    "# Right - organized by experiment\n",
    "mlflow.set_experiment(\"My-Project\")\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_metric(\"loss\", 0.5)\n",
    "```\n",
    "**Why:** Without experiment names, all your runs get mixed together, making it impossible to find anything later.\n",
    "\n",
    "### Mistake 2: Forgetting to End Runs\n",
    "\n",
    "```python\n",
    "# Wrong - orphaned run if exception occurs\n",
    "mlflow.start_run()\n",
    "mlflow.log_metric(\"loss\", 0.5)\n",
    "# Forgot mlflow.end_run()!\n",
    "\n",
    "# Right - context manager handles cleanup\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_metric(\"loss\", 0.5)\n",
    "# Automatically ends run\n",
    "```\n",
    "**Why:** Orphaned runs can cause issues with subsequent experiments.\n",
    "\n",
    "### Mistake 3: Logging After Run Ends\n",
    "\n",
    "```python\n",
    "# Wrong\n",
    "with mlflow.start_run():\n",
    "    pass\n",
    "mlflow.log_metric(\"final_loss\", 0.3)  # Error!\n",
    "\n",
    "# Right\n",
    "with mlflow.start_run():\n",
    "    # ... training ...\n",
    "    mlflow.log_metric(\"final_loss\", 0.3)  # Inside context\n",
    "```\n",
    "**Why:** MLflow needs an active run to log to. All logging must happen inside the run context.\n",
    "\n",
    "### Mistake 4: Not Using Consistent Naming\n",
    "\n",
    "```python\n",
    "# Wrong - inconsistent metric names\n",
    "mlflow.log_metric(\"Loss\", 0.5)\n",
    "mlflow.log_metric(\"training_loss\", 0.4)\n",
    "mlflow.log_metric(\"LOSS\", 0.3)\n",
    "\n",
    "# Right - consistent naming convention\n",
    "mlflow.log_metric(\"train/loss\", 0.5)\n",
    "mlflow.log_metric(\"val/loss\", 0.4)\n",
    "mlflow.log_metric(\"test/loss\", 0.3)\n",
    "```\n",
    "**Why:** Inconsistent names make it impossible to compare runs in the UI.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- How to set up MLflow tracking for experiments\n",
    "- How to log parameters, metrics, and artifacts\n",
    "- How to query and compare runs programmatically\n",
    "- How to load saved models from previous runs\n",
    "- Best practices for LLM fine-tuning tracking\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "Create a hyperparameter search that:\n",
    "1. Uses a grid of learning rates: [0.001, 0.01, 0.1]\n",
    "2. Uses hidden dimensions: [32, 64, 128]\n",
    "3. Tracks all 9 combinations with MLflow\n",
    "4. Finds and loads the best model\n",
    "5. Creates a summary visualization\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)\n",
    "- [MLflow PyTorch Guide](https://mlflow.org/docs/latest/python_api/mlflow.pytorch.html)\n",
    "- [MLflow Best Practices](https://mlflow.org/docs/latest/tracking.html#organizing-runs-into-experiments)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Delete variables\n",
    "del model, loaded_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Cleanup complete!\")\n",
    "print(f\"GPU memory cached: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, we'll explore **Weights & Biases (W&B)** - another popular experiment tracking tool with powerful visualization features. You'll learn when to use MLflow vs W&B and how to integrate both into your workflow.\n",
    "\n",
    "**Continue to:** [02-wandb-integration.ipynb](02-wandb-integration.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
