{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4.3.5: Drift Detection with Evidently AI\n",
    "\n",
    "**Module:** 4.3 - MLOps & Experiment Tracking  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** â­â­â­ (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand the types of drift that affect ML models\n",
    "- [ ] Set up Evidently AI for model monitoring\n",
    "- [ ] Detect data drift and concept drift in production\n",
    "- [ ] Create monitoring dashboards and alerts\n",
    "- [ ] Build a drift monitoring pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Task 4.3.4 (Custom Evaluation)\n",
    "- Knowledge of: Basic statistics, model deployment concepts\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "Your model performed great during testing. You deployed it. Two months later, accuracy tanks. What happened?\n",
    "\n",
    "**Drift** happened.\n",
    "\n",
    "Real examples:\n",
    "- **COVID-19**: Fraud detection models failed when shopping patterns changed overnight\n",
    "- **Seasonal shifts**: Recommendation systems degrade when user preferences change\n",
    "- **Data pipeline issues**: A sensor starts returning values in different units\n",
    "\n",
    "Companies like Uber, Netflix, and Airbnb continuously monitor for drift because catching it early saves millions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELI5: What is Drift?\n",
    "\n",
    "> **Imagine you learned to drive in sunny California.**\n",
    ">\n",
    "> You're great at it! Clear roads, perfect weather, easy traffic.\n",
    ">\n",
    "> Then you move to Minnesota in winter. Suddenly:\n",
    "> - The roads are icy (different input data)\n",
    "> - Driving rules are different (different relationships)\n",
    "> - Your skills don't work as well (degraded performance)\n",
    ">\n",
    "> **In AI terms:** Drift is when the real-world data or relationships change compared to what your model was trained on. There are two main types:\n",
    "> - **Data drift**: The inputs look different (California â†’ Minnesota weather)\n",
    "> - **Concept drift**: The rules changed (summer driving â†’ winter driving)\n",
    "\n",
    "---\n",
    "\n",
    "## Types of Drift\n",
    "\n",
    "| Type | What Changes | Example | Detection |\n",
    "|------|-------------|---------|----------|\n",
    "| **Data Drift** | Input distribution | Users are younger now | Compare feature distributions |\n",
    "| **Concept Drift** | Inputâ†’Output relationship | \"Spam\" definition changed | Monitor prediction quality |\n",
    "| **Label Drift** | Output distribution | More positive reviews | Compare prediction distributions |\n",
    "| **Upstream Drift** | Data pipeline | New data format | Monitor data quality |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting Up Evidently AI\n",
    "\n",
    "Evidently AI is an open-source tool for ML monitoring and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Evidently\n",
    "!pip install evidently -q\n",
    "\n",
    "import evidently\n",
    "print(f\"Evidently version: {evidently.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Evidently imports\n",
    "from evidently import ColumnMapping\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import DataDriftPreset, DataQualityPreset\n",
    "from evidently.metrics import (\n",
    "    DataDriftTable,\n",
    "    DatasetDriftMetric,\n",
    "    ColumnDriftMetric,\n",
    "    DatasetSummaryMetric\n",
    ")\n",
    "\n",
    "print(\"Imports ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Creating Sample Data\n",
    "\n",
    "Let's simulate a real scenario: a sentiment classifier deployed in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(n_samples: int = 1000, seed: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate synthetic training data for a sentiment classifier.\n",
    "    \n",
    "    Features represent text statistics like:\n",
    "    - text_length: Number of characters\n",
    "    - word_count: Number of words\n",
    "    - avg_word_length: Average word length\n",
    "    - sentiment_keywords: Count of sentiment-related words\n",
    "    - exclamation_count: Number of exclamation marks\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    data = {\n",
    "        'text_length': np.random.normal(150, 50, n_samples).clip(20, 500),\n",
    "        'word_count': np.random.normal(30, 10, n_samples).clip(5, 100),\n",
    "        'avg_word_length': np.random.normal(5.5, 1.0, n_samples).clip(3, 10),\n",
    "        'sentiment_keywords': np.random.poisson(3, n_samples),\n",
    "        'exclamation_count': np.random.poisson(1, n_samples),\n",
    "        'question_marks': np.random.poisson(0.5, n_samples),\n",
    "        'uppercase_ratio': np.random.beta(2, 20, n_samples),\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Generate target based on features (simulated relationship)\n",
    "    score = (\n",
    "        0.3 * (df['sentiment_keywords'] > 2) +\n",
    "        0.2 * (df['exclamation_count'] > 0) +\n",
    "        0.2 * (df['text_length'] > 100) +\n",
    "        0.3 * np.random.random(n_samples)\n",
    "    )\n",
    "    df['target'] = (score > 0.5).astype(int)  # Binary: positive/negative\n",
    "    df['prediction'] = df['target']  # Perfect predictions during training\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate reference (training) data\n",
    "reference_data = generate_training_data(n_samples=2000)\n",
    "\n",
    "print(\"Reference (Training) Data:\")\n",
    "print(f\"  Shape: {reference_data.shape}\")\n",
    "print(f\"  Target distribution: {reference_data['target'].value_counts().to_dict()}\")\n",
    "reference_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_production_data(\n",
    "    n_samples: int = 500,\n",
    "    drift_type: str = \"none\",\n",
    "    drift_intensity: float = 0.5,\n",
    "    seed: int = 123\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate production data with optional drift.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of samples\n",
    "        drift_type: \"none\", \"data_drift\", \"concept_drift\", \"both\"\n",
    "        drift_intensity: How severe the drift is (0-1)\n",
    "        seed: Random seed\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Start with similar distribution to training\n",
    "    data = {\n",
    "        'text_length': np.random.normal(150, 50, n_samples).clip(20, 500),\n",
    "        'word_count': np.random.normal(30, 10, n_samples).clip(5, 100),\n",
    "        'avg_word_length': np.random.normal(5.5, 1.0, n_samples).clip(3, 10),\n",
    "        'sentiment_keywords': np.random.poisson(3, n_samples),\n",
    "        'exclamation_count': np.random.poisson(1, n_samples),\n",
    "        'question_marks': np.random.poisson(0.5, n_samples),\n",
    "        'uppercase_ratio': np.random.beta(2, 20, n_samples),\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Apply data drift\n",
    "    if drift_type in [\"data_drift\", \"both\"]:\n",
    "        # Shift distributions\n",
    "        df['text_length'] = df['text_length'] * (1 + drift_intensity * 0.5)  # Longer texts\n",
    "        df['exclamation_count'] = np.random.poisson(1 + 3 * drift_intensity, n_samples)  # More excited\n",
    "        df['uppercase_ratio'] = np.random.beta(2 + 5 * drift_intensity, 20, n_samples)  # More caps\n",
    "    \n",
    "    # Generate predictions (model's behavior)\n",
    "    score = (\n",
    "        0.3 * (df['sentiment_keywords'] > 2) +\n",
    "        0.2 * (df['exclamation_count'] > 0) +\n",
    "        0.2 * (df['text_length'] > 100) +\n",
    "        0.3 * np.random.random(n_samples)\n",
    "    )\n",
    "    df['prediction'] = (score > 0.5).astype(int)\n",
    "    \n",
    "    # True labels (may differ if concept drift)\n",
    "    if drift_type in [\"concept_drift\", \"both\"]:\n",
    "        # The relationship between features and target has changed\n",
    "        # Now uppercase_ratio matters more\n",
    "        new_score = (\n",
    "            0.1 * (df['sentiment_keywords'] > 2) +  # Less important now\n",
    "            0.1 * (df['exclamation_count'] > 0) +   # Less important now\n",
    "            0.4 * (df['uppercase_ratio'] > 0.1) +   # Now more important!\n",
    "            0.4 * np.random.random(n_samples)\n",
    "        )\n",
    "        df['target'] = (new_score > 0.5).astype(int)\n",
    "    else:\n",
    "        df['target'] = df['prediction']  # No concept drift means good predictions\n",
    "    \n",
    "    # Add some noise to predictions\n",
    "    noise_mask = np.random.random(n_samples) < (0.1 + drift_intensity * 0.2)\n",
    "    df.loc[noise_mask, 'prediction'] = 1 - df.loc[noise_mask, 'prediction']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate production data scenarios\n",
    "production_no_drift = generate_production_data(drift_type=\"none\")\n",
    "production_data_drift = generate_production_data(drift_type=\"data_drift\", drift_intensity=0.8)\n",
    "production_concept_drift = generate_production_data(drift_type=\"concept_drift\", drift_intensity=0.7)\n",
    "production_both = generate_production_data(drift_type=\"both\", drift_intensity=0.6)\n",
    "\n",
    "print(\"Production Data Scenarios Generated:\")\n",
    "print(f\"  No drift: {len(production_no_drift)} samples\")\n",
    "print(f\"  Data drift: {len(production_data_drift)} samples\")\n",
    "print(f\"  Concept drift: {len(production_concept_drift)} samples\")\n",
    "print(f\"  Both: {len(production_both)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Detecting Data Drift\n",
    "\n",
    "Let's use Evidently to compare the production data to our reference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column mapping\n",
    "column_mapping = ColumnMapping(\n",
    "    target='target',\n",
    "    prediction='prediction',\n",
    "    numerical_features=[\n",
    "        'text_length', 'word_count', 'avg_word_length',\n",
    "        'sentiment_keywords', 'exclamation_count',\n",
    "        'question_marks', 'uppercase_ratio'\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Column mapping configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_drift_report(\n",
    "    reference: pd.DataFrame,\n",
    "    current: pd.DataFrame,\n",
    "    column_mapping: ColumnMapping,\n",
    "    report_name: str = \"drift_report\"\n",
    ") -> Report:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive drift report.\n",
    "    \"\"\"\n",
    "    report = Report(metrics=[\n",
    "        DatasetDriftMetric(),\n",
    "        DataDriftTable(),\n",
    "    ])\n",
    "    \n",
    "    report.run(\n",
    "        reference_data=reference,\n",
    "        current_data=current,\n",
    "        column_mapping=column_mapping\n",
    "    )\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Test with no-drift scenario\n",
    "report_no_drift = run_drift_report(\n",
    "    reference_data, \n",
    "    production_no_drift, \n",
    "    column_mapping\n",
    ")\n",
    "\n",
    "print(\"No-drift report generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract drift results\n",
    "def extract_drift_summary(report: Report) -> dict:\n",
    "    \"\"\"Extract key drift metrics from report.\"\"\"\n",
    "    results = report.as_dict()\n",
    "    \n",
    "    summary = {}\n",
    "    \n",
    "    for metric in results.get('metrics', []):\n",
    "        metric_id = metric.get('metric', '')\n",
    "        result = metric.get('result', {})\n",
    "        \n",
    "        if 'DatasetDriftMetric' in metric_id:\n",
    "            summary['dataset_drift'] = result.get('dataset_drift', False)\n",
    "            summary['drift_share'] = result.get('drift_share', 0)\n",
    "            summary['n_drifted_features'] = result.get('number_of_drifted_columns', 0)\n",
    "        \n",
    "        if 'DataDriftTable' in metric_id:\n",
    "            summary['column_drifts'] = {}\n",
    "            for col_data in result.get('drift_by_columns', {}).values():\n",
    "                col_name = col_data.get('column_name', 'unknown')\n",
    "                summary['column_drifts'][col_name] = {\n",
    "                    'drifted': col_data.get('drift_detected', False),\n",
    "                    'drift_score': col_data.get('drift_score', 0),\n",
    "                    'stattest': col_data.get('stattest_name', 'unknown')\n",
    "                }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Check no-drift scenario\n",
    "summary_no_drift = extract_drift_summary(report_no_drift)\n",
    "print(\"No-Drift Scenario:\")\n",
    "print(f\"  Dataset drift detected: {summary_no_drift.get('dataset_drift', 'N/A')}\")\n",
    "print(f\"  Drift share: {summary_no_drift.get('drift_share', 0):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now check data drift scenario\n",
    "report_data_drift = run_drift_report(\n",
    "    reference_data,\n",
    "    production_data_drift,\n",
    "    column_mapping\n",
    ")\n",
    "\n",
    "summary_data_drift = extract_drift_summary(report_data_drift)\n",
    "\n",
    "print(\"Data Drift Scenario:\")\n",
    "print(f\"  Dataset drift detected: {summary_data_drift.get('dataset_drift', 'N/A')}\")\n",
    "print(f\"  Drift share: {summary_data_drift.get('drift_share', 0):.2%}\")\n",
    "print(f\"  Features drifted: {summary_data_drift.get('n_drifted_features', 0)}\")\n",
    "\n",
    "print(\"\\nPer-column drift:\")\n",
    "for col, info in summary_data_drift.get('column_drifts', {}).items():\n",
    "    if col in ['target', 'prediction']:\n",
    "        continue\n",
    "    status = \"DRIFTED\" if info['drifted'] else \"OK\"\n",
    "    print(f\"  {col}: {status} (score: {info['drift_score']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save HTML report\n",
    "report_data_drift.save_html(\"data_drift_report.html\")\n",
    "print(\"Report saved to data_drift_report.html\")\n",
    "print(\"Open in browser to see interactive visualizations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "Evidently compared the statistical distributions of each feature:\n",
    "- Used statistical tests (Kolmogorov-Smirnov for numerical features)\n",
    "- Calculated drift scores for each column\n",
    "- Flagged columns where the distribution significantly changed\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4: Data Quality Monitoring\n",
    "\n",
    "Beyond drift, we should monitor data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently.metrics import (\n",
    "    DatasetMissingValuesMetric,\n",
    "    ColumnQuantileMetric,\n",
    "    ColumnValueRangeMetric\n",
    ")\n",
    "\n",
    "def run_quality_report(\n",
    "    reference: pd.DataFrame,\n",
    "    current: pd.DataFrame,\n",
    "    column_mapping: ColumnMapping\n",
    ") -> Report:\n",
    "    \"\"\"\n",
    "    Generate a data quality report.\n",
    "    \"\"\"\n",
    "    report = Report(metrics=[\n",
    "        DatasetSummaryMetric(),\n",
    "        DatasetMissingValuesMetric(),\n",
    "    ])\n",
    "    \n",
    "    report.run(\n",
    "        reference_data=reference,\n",
    "        current_data=current,\n",
    "        column_mapping=column_mapping\n",
    "    )\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with quality issues\n",
    "production_bad_quality = production_no_drift.copy()\n",
    "\n",
    "# Introduce missing values\n",
    "production_bad_quality.loc[\n",
    "    np.random.random(len(production_bad_quality)) < 0.1, \n",
    "    'sentiment_keywords'\n",
    "] = np.nan\n",
    "\n",
    "# Introduce outliers\n",
    "production_bad_quality.loc[\n",
    "    np.random.random(len(production_bad_quality)) < 0.05,\n",
    "    'text_length'\n",
    "] = 10000  # Unreasonably long\n",
    "\n",
    "print(\"Introduced quality issues:\")\n",
    "print(f\"  Missing values: {production_bad_quality['sentiment_keywords'].isna().sum()}\")\n",
    "print(f\"  Outliers (text_length > 1000): {(production_bad_quality['text_length'] > 1000).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run quality report\n",
    "quality_report = run_quality_report(\n",
    "    reference_data,\n",
    "    production_bad_quality,\n",
    "    column_mapping\n",
    ")\n",
    "\n",
    "quality_report.save_html(\"data_quality_report.html\")\n",
    "print(\"Quality report saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Model Performance Monitoring\n",
    "\n",
    "Track how model performance changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently.metric_preset import ClassificationPreset\n",
    "from evidently.metrics import (\n",
    "    ClassificationQualityMetric,\n",
    "    ClassificationClassBalance,\n",
    "    ClassificationConfusionMatrix\n",
    ")\n",
    "\n",
    "def run_performance_report(\n",
    "    reference: pd.DataFrame,\n",
    "    current: pd.DataFrame,\n",
    "    column_mapping: ColumnMapping\n",
    ") -> Report:\n",
    "    \"\"\"\n",
    "    Monitor classification model performance.\n",
    "    \"\"\"\n",
    "    report = Report(metrics=[\n",
    "        ClassificationQualityMetric(),\n",
    "        ClassificationClassBalance(),\n",
    "        ClassificationConfusionMatrix()\n",
    "    ])\n",
    "    \n",
    "    report.run(\n",
    "        reference_data=reference,\n",
    "        current_data=current,\n",
    "        column_mapping=column_mapping\n",
    "    )\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance: no drift vs concept drift\n",
    "print(\"Performance with NO drift:\")\n",
    "perf_no_drift = run_performance_report(reference_data, production_no_drift, column_mapping)\n",
    "perf_results_no_drift = perf_no_drift.as_dict()\n",
    "\n",
    "# Extract accuracy\n",
    "for metric in perf_results_no_drift.get('metrics', []):\n",
    "    if 'ClassificationQuality' in metric.get('metric', ''):\n",
    "        result = metric.get('result', {}).get('current', {})\n",
    "        print(f\"  Accuracy: {result.get('accuracy', 'N/A'):.4f}\")\n",
    "        print(f\"  Precision: {result.get('precision', 'N/A'):.4f}\")\n",
    "        print(f\"  Recall: {result.get('recall', 'N/A'):.4f}\")\n",
    "        print(f\"  F1: {result.get('f1', 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPerformance with CONCEPT drift:\")\n",
    "perf_concept_drift = run_performance_report(reference_data, production_concept_drift, column_mapping)\n",
    "perf_results_concept = perf_concept_drift.as_dict()\n",
    "\n",
    "for metric in perf_results_concept.get('metrics', []):\n",
    "    if 'ClassificationQuality' in metric.get('metric', ''):\n",
    "        result = metric.get('result', {}).get('current', {})\n",
    "        print(f\"  Accuracy: {result.get('accuracy', 'N/A'):.4f}\")\n",
    "        print(f\"  Precision: {result.get('precision', 'N/A'):.4f}\")\n",
    "        print(f\"  Recall: {result.get('recall', 'N/A'):.4f}\")\n",
    "        print(f\"  F1: {result.get('f1', 'N/A'):.4f}\")\n",
    "\n",
    "print(\"\\nNotice the performance degradation due to concept drift!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Building a Monitoring Pipeline\n",
    "\n",
    "Let's create a reusable monitoring pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class MonitoringAlert:\n",
    "    \"\"\"Alert generated by monitoring.\"\"\"\n",
    "    severity: str  # \"info\", \"warning\", \"critical\"\n",
    "    category: str  # \"data_drift\", \"concept_drift\", \"quality\", \"performance\"\n",
    "    message: str\n",
    "    details: dict\n",
    "\n",
    "\n",
    "class ModelMonitor:\n",
    "    \"\"\"\n",
    "    Comprehensive model monitoring system.\n",
    "    \n",
    "    Example:\n",
    "        monitor = ModelMonitor(reference_data, column_mapping)\n",
    "        alerts = monitor.check(current_data)\n",
    "        for alert in alerts:\n",
    "            print(f\"{alert.severity}: {alert.message}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        reference_data: pd.DataFrame,\n",
    "        column_mapping: ColumnMapping,\n",
    "        drift_threshold: float = 0.3,\n",
    "        accuracy_threshold: float = 0.8,\n",
    "        missing_threshold: float = 0.05\n",
    "    ):\n",
    "        self.reference_data = reference_data\n",
    "        self.column_mapping = column_mapping\n",
    "        self.drift_threshold = drift_threshold\n",
    "        self.accuracy_threshold = accuracy_threshold\n",
    "        self.missing_threshold = missing_threshold\n",
    "        self.history: List[dict] = []\n",
    "    \n",
    "    def check(self, current_data: pd.DataFrame) -> List[MonitoringAlert]:\n",
    "        \"\"\"\n",
    "        Run all monitoring checks and return alerts.\n",
    "        \"\"\"\n",
    "        alerts = []\n",
    "        check_results = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"n_samples\": len(current_data)\n",
    "        }\n",
    "        \n",
    "        # Check data drift\n",
    "        drift_alerts, drift_results = self._check_data_drift(current_data)\n",
    "        alerts.extend(drift_alerts)\n",
    "        check_results[\"drift\"] = drift_results\n",
    "        \n",
    "        # Check data quality\n",
    "        quality_alerts, quality_results = self._check_data_quality(current_data)\n",
    "        alerts.extend(quality_alerts)\n",
    "        check_results[\"quality\"] = quality_results\n",
    "        \n",
    "        # Check performance (if labels available)\n",
    "        if 'target' in current_data.columns:\n",
    "            perf_alerts, perf_results = self._check_performance(current_data)\n",
    "            alerts.extend(perf_alerts)\n",
    "            check_results[\"performance\"] = perf_results\n",
    "        \n",
    "        self.history.append(check_results)\n",
    "        return alerts\n",
    "    \n",
    "    def _check_data_drift(self, current_data: pd.DataFrame) -> tuple:\n",
    "        \"\"\"Check for data drift.\"\"\"\n",
    "        alerts = []\n",
    "        \n",
    "        report = Report(metrics=[DatasetDriftMetric(), DataDriftTable()])\n",
    "        report.run(\n",
    "            reference_data=self.reference_data,\n",
    "            current_data=current_data,\n",
    "            column_mapping=self.column_mapping\n",
    "        )\n",
    "        \n",
    "        results = report.as_dict()\n",
    "        drift_share = 0\n",
    "        drifted_columns = []\n",
    "        \n",
    "        for metric in results.get('metrics', []):\n",
    "            if 'DatasetDriftMetric' in metric.get('metric', ''):\n",
    "                result = metric.get('result', {})\n",
    "                drift_share = result.get('drift_share', 0)\n",
    "                \n",
    "            if 'DataDriftTable' in metric.get('metric', ''):\n",
    "                for col_data in metric.get('result', {}).get('drift_by_columns', {}).values():\n",
    "                    if col_data.get('drift_detected', False):\n",
    "                        drifted_columns.append(col_data.get('column_name', 'unknown'))\n",
    "        \n",
    "        if drift_share > self.drift_threshold:\n",
    "            severity = \"critical\" if drift_share > 0.5 else \"warning\"\n",
    "            alerts.append(MonitoringAlert(\n",
    "                severity=severity,\n",
    "                category=\"data_drift\",\n",
    "                message=f\"Data drift detected in {drift_share:.1%} of features\",\n",
    "                details={\"drift_share\": drift_share, \"drifted_columns\": drifted_columns}\n",
    "            ))\n",
    "        \n",
    "        return alerts, {\"drift_share\": drift_share, \"drifted_columns\": drifted_columns}\n",
    "    \n",
    "    def _check_data_quality(self, current_data: pd.DataFrame) -> tuple:\n",
    "        \"\"\"Check data quality.\"\"\"\n",
    "        alerts = []\n",
    "        \n",
    "        # Check missing values\n",
    "        missing_ratio = current_data.isnull().mean().mean()\n",
    "        \n",
    "        if missing_ratio > self.missing_threshold:\n",
    "            alerts.append(MonitoringAlert(\n",
    "                severity=\"warning\",\n",
    "                category=\"quality\",\n",
    "                message=f\"High missing value ratio: {missing_ratio:.1%}\",\n",
    "                details={\"missing_ratio\": missing_ratio}\n",
    "            ))\n",
    "        \n",
    "        return alerts, {\"missing_ratio\": missing_ratio}\n",
    "    \n",
    "    def _check_performance(self, current_data: pd.DataFrame) -> tuple:\n",
    "        \"\"\"Check model performance.\"\"\"\n",
    "        alerts = []\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = (current_data['prediction'] == current_data['target']).mean()\n",
    "        \n",
    "        if accuracy < self.accuracy_threshold:\n",
    "            severity = \"critical\" if accuracy < 0.6 else \"warning\"\n",
    "            alerts.append(MonitoringAlert(\n",
    "                severity=severity,\n",
    "                category=\"performance\",\n",
    "                message=f\"Model accuracy dropped to {accuracy:.1%}\",\n",
    "                details={\"accuracy\": accuracy, \"threshold\": self.accuracy_threshold}\n",
    "            ))\n",
    "        \n",
    "        return alerts, {\"accuracy\": accuracy}\n",
    "    \n",
    "    def get_summary(self) -> pd.DataFrame:\n",
    "        \"\"\"Get historical monitoring summary.\"\"\"\n",
    "        if not self.history:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        rows = []\n",
    "        for record in self.history:\n",
    "            row = {\n",
    "                \"timestamp\": record[\"timestamp\"],\n",
    "                \"n_samples\": record[\"n_samples\"],\n",
    "                \"drift_share\": record.get(\"drift\", {}).get(\"drift_share\", 0),\n",
    "                \"missing_ratio\": record.get(\"quality\", {}).get(\"missing_ratio\", 0),\n",
    "                \"accuracy\": record.get(\"performance\", {}).get(\"accuracy\", None)\n",
    "            }\n",
    "            rows.append(row)\n",
    "        \n",
    "        return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the monitoring pipeline\n",
    "monitor = ModelMonitor(\n",
    "    reference_data=reference_data,\n",
    "    column_mapping=column_mapping,\n",
    "    drift_threshold=0.2,\n",
    "    accuracy_threshold=0.85\n",
    ")\n",
    "\n",
    "# Check various scenarios\n",
    "scenarios = [\n",
    "    (\"No drift\", production_no_drift),\n",
    "    (\"Data drift\", production_data_drift),\n",
    "    (\"Concept drift\", production_concept_drift),\n",
    "    (\"Both types\", production_both)\n",
    "]\n",
    "\n",
    "for name, data in scenarios:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Scenario: {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    alerts = monitor.check(data)\n",
    "    \n",
    "    if alerts:\n",
    "        for alert in alerts:\n",
    "            icon = {\"critical\": \"ðŸ”´\", \"warning\": \"ðŸŸ¡\", \"info\": \"ðŸ”µ\"}[alert.severity]\n",
    "            print(f\"  {icon} [{alert.severity.upper()}] {alert.message}\")\n",
    "    else:\n",
    "        print(\"  âœ… All checks passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View monitoring history\n",
    "summary = monitor.get_summary()\n",
    "print(\"\\nMonitoring History:\")\n",
    "print(summary.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself\n",
    "\n",
    "Create a monitoring dashboard that:\n",
    "1. Simulates data arriving over time (with increasing drift)\n",
    "2. Runs checks at regular intervals\n",
    "3. Plots drift metrics over time\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Use a loop to simulate time periods:\n",
    "```python\n",
    "for day in range(30):\n",
    "    drift_intensity = day / 30  # Increasing drift\n",
    "    data = generate_production_data(drift_intensity=drift_intensity)\n",
    "    alerts = monitor.check(data)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Simulate drift over time and visualize\n",
    "\n",
    "# Your monitoring simulation..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Having Good Reference Data\n",
    "\n",
    "```python\n",
    "# Wrong - using biased or small reference\n",
    "reference = training_data.sample(100)  # Too small!\n",
    "\n",
    "# Right - use representative reference data\n",
    "reference = training_data.sample(min(10000, len(training_data)))\n",
    "# Or use a dedicated holdout set\n",
    "```\n",
    "**Why:** Small or biased reference data leads to false drift alarms.\n",
    "\n",
    "### Mistake 2: Alerting on Every Drift\n",
    "\n",
    "```python\n",
    "# Wrong - alert on any statistical difference\n",
    "if p_value < 0.05:\n",
    "    send_alert(\"DRIFT DETECTED!\")  # Too sensitive\n",
    "\n",
    "# Right - set practical thresholds\n",
    "if drift_share > 0.3 and effect_size > 0.2:\n",
    "    send_alert(\"Significant drift detected\")\n",
    "```\n",
    "**Why:** Small statistical differences are normal. Alert on meaningful changes.\n",
    "\n",
    "### Mistake 3: Ignoring Concept Drift\n",
    "\n",
    "```python\n",
    "# Wrong - only monitoring inputs\n",
    "check_data_drift(inputs)\n",
    "\n",
    "# Right - also monitor performance\n",
    "check_data_drift(inputs)\n",
    "check_prediction_distribution(predictions)\n",
    "check_actual_performance(predictions, delayed_labels)\n",
    "```\n",
    "**Why:** Concept drift (changed relationships) won't show in input distributions.\n",
    "\n",
    "### Mistake 4: Not Automating Monitoring\n",
    "\n",
    "```python\n",
    "# Wrong - manual checks\n",
    "# \"I'll run the drift check next week...\"\n",
    "\n",
    "# Right - automated pipeline\n",
    "# Schedule with cron, Airflow, or similar\n",
    "@scheduled(cron=\"0 0 * * *\")  # Daily at midnight\n",
    "def run_monitoring():\n",
    "    data = get_last_24h_data()\n",
    "    alerts = monitor.check(data)\n",
    "    if alerts:\n",
    "        send_to_slack(alerts)\n",
    "```\n",
    "**Why:** Manual monitoring gets forgotten. Automate it.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- What data drift and concept drift are\n",
    "- How to use Evidently AI for drift detection\n",
    "- How to monitor data quality in production\n",
    "- How to track model performance over time\n",
    "- How to build a monitoring pipeline with alerts\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "Build a complete monitoring system that:\n",
    "1. Stores monitoring results in MLflow\n",
    "2. Creates a live dashboard using Streamlit or Gradio\n",
    "3. Sends Slack/email alerts when thresholds are exceeded\n",
    "4. Automatically triggers model retraining on severe drift\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Evidently AI Documentation](https://docs.evidentlyai.com/)\n",
    "- [Made with ML: Monitoring](https://madewithml.com/courses/mlops/monitoring/)\n",
    "- [Google MLOps: Monitoring](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "import os\n",
    "\n",
    "# Remove generated reports\n",
    "for f in [\"data_drift_report.html\", \"data_quality_report.html\"]:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "\n",
    "print(\"Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you can detect when your model needs updating, the next notebook covers **model versioning** - how to manage multiple model versions and safely roll back if needed.\n",
    "\n",
    "**Continue to:** [06-model-registry.ipynb](06-model-registry.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
