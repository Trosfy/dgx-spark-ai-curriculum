{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.1: NumPy Broadcasting Lab - SOLUTIONS\n",
    "\n",
    "**Module:** 2 - Python for AI/ML  \n",
    "\n",
    "This notebook contains solutions to all exercises from the NumPy Broadcasting Lab.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"Solutions Notebook for NumPy Broadcasting Lab\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Zero-Mean Normalization\n",
    "\n",
    "**Task:** Normalize each row to have zero mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - Exercise 1\n",
    "np.random.seed(42)\n",
    "data = np.random.randn(5, 4)\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(f\"\\nRow means before: {data.mean(axis=1).round(4)}\")\n",
    "\n",
    "# Solution: Use keepdims=True to maintain shape for broadcasting\n",
    "row_means = data.mean(axis=1, keepdims=True)  # Shape: (5, 1)\n",
    "normalized = data - row_means\n",
    "\n",
    "print(f\"\\nRow means shape: {row_means.shape}\")\n",
    "print(f\"Row means after: {normalized.mean(axis=1).round(10)}\")\n",
    "print(\"\\n✅ All row means are now ~0!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Solution (without keepdims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Use reshape or np.newaxis\n",
    "row_means_v2 = data.mean(axis=1)[:, np.newaxis]  # (5,) -> (5, 1)\n",
    "normalized_v2 = data - row_means_v2\n",
    "\n",
    "print(f\"Same result? {np.allclose(normalized, normalized_v2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Cosine Similarity Matrix\n",
    "\n",
    "**Task:** Compute cosine similarity between all pairs of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - Exercise 2\n",
    "np.random.seed(42)\n",
    "embeddings = np.random.randn(100, 64).astype(np.float32)\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Step 1: Compute norm of each embedding\n",
    "norms = np.linalg.norm(embeddings, axis=1, keepdims=True)  # Shape: (100, 1)\n",
    "print(f\"Norms shape: {norms.shape}\")\n",
    "\n",
    "# Step 2: Normalize embeddings to unit length\n",
    "normalized_embeddings = embeddings / norms\n",
    "\n",
    "# Verify unit length\n",
    "print(f\"Normalized embedding norms: {np.linalg.norm(normalized_embeddings, axis=1)[:5].round(4)}\")\n",
    "\n",
    "# Step 3: Compute similarity matrix using @ operator\n",
    "# For unit vectors, cosine similarity = dot product\n",
    "similarity_matrix = normalized_embeddings @ normalized_embeddings.T\n",
    "\n",
    "print(f\"\\nSimilarity matrix shape: {similarity_matrix.shape}\")\n",
    "print(f\"Diagonal (self-similarity): {similarity_matrix.diagonal()[:5].round(4)}\")\n",
    "print(f\"All diagonal values are 1.0? {np.allclose(similarity_matrix.diagonal(), 1.0)}\")\n",
    "\n",
    "# Sample off-diagonal similarities\n",
    "print(f\"\\nSample similarities [0, 1:5]: {similarity_matrix[0, 1:5].round(4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: One-liner solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compact one-liner\n",
    "def cosine_similarity_matrix(X):\n",
    "    \"\"\"Compute pairwise cosine similarity.\"\"\"\n",
    "    X_norm = X / np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    return X_norm @ X_norm.T\n",
    "\n",
    "sim_v2 = cosine_similarity_matrix(embeddings)\n",
    "print(f\"Same result? {np.allclose(similarity_matrix, sim_v2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge: Mini Neural Network Forward Pass\n",
    "\n",
    "**Task:** Implement a 2-layer network using only broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - Challenge\n",
    "np.random.seed(42)\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation function.\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Numerically stable softmax.\"\"\"\n",
    "    x_max = x.max(axis=-1, keepdims=True)\n",
    "    exp_x = np.exp(x - x_max)\n",
    "    return exp_x / exp_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "def forward(x, w1, b1, w2, b2):\n",
    "    \"\"\"\n",
    "    Two-layer neural network forward pass.\n",
    "    \n",
    "    Args:\n",
    "        x: Input (batch_size, 784)\n",
    "        w1: First layer weights (784, 256)\n",
    "        b1: First layer bias (256,)\n",
    "        w2: Second layer weights (256, 10)\n",
    "        b2: Second layer bias (10,)\n",
    "    \n",
    "    Returns:\n",
    "        Probability distribution over 10 classes (batch_size, 10)\n",
    "    \"\"\"\n",
    "    # Layer 1: Linear + ReLU\n",
    "    z1 = x @ w1 + b1  # (batch, 784) @ (784, 256) + (256,) -> (batch, 256)\n",
    "    h = relu(z1)\n",
    "    \n",
    "    # Layer 2: Linear + Softmax\n",
    "    z2 = h @ w2 + b2  # (batch, 256) @ (256, 10) + (10,) -> (batch, 10)\n",
    "    probs = softmax(z2)\n",
    "    \n",
    "    return probs\n",
    "\n",
    "# Test the network\n",
    "batch_size = 32\n",
    "x = np.random.randn(batch_size, 784).astype(np.float32)\n",
    "w1 = np.random.randn(784, 256).astype(np.float32) * 0.01\n",
    "b1 = np.zeros(256, dtype=np.float32)\n",
    "w2 = np.random.randn(256, 10).astype(np.float32) * 0.01\n",
    "b2 = np.zeros(10, dtype=np.float32)\n",
    "\n",
    "# Forward pass\n",
    "probs = forward(x, w1, b1, w2, b2)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {probs.shape}\")\n",
    "print(f\"\\nProbabilities sum to 1? {np.allclose(probs.sum(axis=1), 1.0)}\")\n",
    "print(f\"\\nSample output (first batch item): {probs[0].round(4)}\")\n",
    "print(f\"Predicted class: {np.argmax(probs[0])}\")\n",
    "\n",
    "print(\"\\n✅ Mini neural network forward pass complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended Solution: With Intermediate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_verbose(x, w1, b1, w2, b2):\n",
    "    \"\"\"Forward pass with intermediate value inspection.\"\"\"\n",
    "    print(\"Step-by-step forward pass:\")\n",
    "    print(f\"  Input x:      {x.shape}\")\n",
    "    \n",
    "    # Layer 1\n",
    "    z1 = x @ w1\n",
    "    print(f\"  x @ w1:       {z1.shape}\")\n",
    "    \n",
    "    z1 = z1 + b1  # Broadcasting: (batch, 256) + (256,)\n",
    "    print(f\"  + b1:         {z1.shape}\")\n",
    "    \n",
    "    h = relu(z1)\n",
    "    print(f\"  ReLU:         {h.shape}, active neurons: {(h > 0).sum() / h.size * 100:.1f}%\")\n",
    "    \n",
    "    # Layer 2\n",
    "    z2 = h @ w2 + b2\n",
    "    print(f\"  h @ w2 + b2:  {z2.shape}\")\n",
    "    \n",
    "    probs = softmax(z2)\n",
    "    print(f\"  Softmax:      {probs.shape}\")\n",
    "    \n",
    "    return probs\n",
    "\n",
    "_ = forward_verbose(x, w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Use `keepdims=True`** when you need to broadcast back to original shape\n",
    "2. **Cosine similarity** becomes a simple matrix multiply after normalization\n",
    "3. **Neural network layers** are just matrix multiplies + broadcasting for bias\n",
    "4. **ReLU and softmax** work element-wise and preserve shapes\n",
    "\n",
    "---\n",
    "\n",
    "**End of Solutions**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
