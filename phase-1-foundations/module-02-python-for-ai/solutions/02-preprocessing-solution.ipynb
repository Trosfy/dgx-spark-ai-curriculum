{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.2: Dataset Preprocessing Pipeline - SOLUTIONS\n",
    "\n",
    "**Module:** 2 - Python for AI/ML  \n",
    "\n",
    "This notebook contains solutions to all exercises from the Preprocessing Pipeline Lab.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Solutions Notebook for Preprocessing Pipeline\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the sample data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "data = {\n",
    "    'age': np.random.randint(18, 80, n_samples).astype(float),\n",
    "    'income': np.random.lognormal(10.5, 0.5, n_samples),\n",
    "    'credit_score': np.random.randint(300, 850, n_samples).astype(float),\n",
    "    'years_employed': np.random.exponential(5, n_samples),\n",
    "    'education': np.random.choice(\n",
    "        ['High School', 'Bachelor', 'Master', 'PhD', None], \n",
    "        n_samples, \n",
    "        p=[0.3, 0.35, 0.2, 0.1, 0.05]\n",
    "    ),\n",
    "    'employment_type': np.random.choice(\n",
    "        ['Full-time', 'Part-time', 'Self-employed', 'Unemployed'],\n",
    "        n_samples,\n",
    "        p=[0.6, 0.15, 0.15, 0.1]\n",
    "    ),\n",
    "    'default': np.random.choice([0, 1], n_samples, p=[0.85, 0.15])\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add missing values\n",
    "df.loc[np.random.choice(n_samples, 50, replace=False), 'age'] = np.nan\n",
    "df.loc[np.random.choice(n_samples, 80, replace=False), 'income'] = np.nan\n",
    "df.loc[np.random.choice(n_samples, 30, replace=False), 'credit_score'] = np.nan\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nMissing values:\\n{df.isnull().sum()[df.isnull().sum() > 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Group-Based Imputation\n",
    "\n",
    "**Task:** Impute missing income using the median income for each education level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - Exercise 1\n",
    "df_group_imputed = df.copy()\n",
    "\n",
    "# First, fill education missing values (we need groups to exist)\n",
    "education_mode = df_group_imputed['education'].mode()[0]\n",
    "df_group_imputed['education'].fillna(education_mode, inplace=True)\n",
    "\n",
    "# Show median income by education BEFORE imputation\n",
    "print(\"Median income by education (before imputation):\")\n",
    "print(df.groupby('education')['income'].median().round(0))\n",
    "\n",
    "# Method 1: Using groupby + transform + fillna\n",
    "df_group_imputed['income'] = df_group_imputed.groupby('education')['income'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "\n",
    "print(f\"\\nMissing values after imputation: {df_group_imputed['income'].isna().sum()}\")\n",
    "\n",
    "# Verify by checking a sample\n",
    "print(\"\\nSample of imputed data (first 5 originally missing):\")\n",
    "original_missing = df[df['income'].isna()].head(5).index\n",
    "print(df_group_imputed.loc[original_missing, ['education', 'income']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Method: Manual Group Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: More explicit approach\n",
    "df_alt = df.copy()\n",
    "df_alt['education'].fillna(education_mode, inplace=True)\n",
    "\n",
    "# Calculate group medians\n",
    "group_medians = df_alt.groupby('education')['income'].median()\n",
    "print(\"Group medians:\")\n",
    "print(group_medians.round(0))\n",
    "\n",
    "# Fill missing values\n",
    "for education_level in df_alt['education'].unique():\n",
    "    mask = (df_alt['education'] == education_level) & (df_alt['income'].isna())\n",
    "    df_alt.loc[mask, 'income'] = group_medians[education_level]\n",
    "\n",
    "print(f\"\\nMissing after: {df_alt['income'].isna().sum()}\")\n",
    "print(f\"Same result? {np.allclose(df_group_imputed['income'], df_alt['income'], equal_nan=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Extended Preprocessor with Log Transform\n",
    "\n",
    "**Task:** Add log transformation capability to the Preprocessor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - Exercise 2: Extended Preprocessor\n",
    "\n",
    "class PreprocessorWithLog:\n",
    "    \"\"\"\n",
    "    Extended Preprocessor with log transformation support.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        numeric_features,\n",
    "        categorical_features=None,\n",
    "        ordinal_mappings=None,\n",
    "        scaling='standard',\n",
    "        impute_strategy='median',\n",
    "        log_features=None  # NEW PARAMETER\n",
    "    ):\n",
    "        self.numeric_features = numeric_features\n",
    "        self.categorical_features = categorical_features or []\n",
    "        self.ordinal_mappings = ordinal_mappings or {}\n",
    "        self.scaling = scaling\n",
    "        self.impute_strategy = impute_strategy\n",
    "        self.log_features = log_features or []  # NEW\n",
    "        \n",
    "        self.numeric_stats_ = {}\n",
    "        self.categorical_values_ = {}\n",
    "        self.scale_params_ = {}\n",
    "        self._is_fitted = False\n",
    "        \n",
    "    def fit(self, df):\n",
    "        # Learn imputation values\n",
    "        for col in self.numeric_features:\n",
    "            if self.impute_strategy == 'median':\n",
    "                self.numeric_stats_[col] = df[col].median()\n",
    "            else:\n",
    "                self.numeric_stats_[col] = df[col].mean()\n",
    "        \n",
    "        for col in self.categorical_features:\n",
    "            mode_values = df[col].mode()\n",
    "            self.categorical_values_[col] = {\n",
    "                'mode': mode_values[0] if len(mode_values) > 0 else 'Unknown',\n",
    "                'categories': sorted(df[col].dropna().unique())\n",
    "            }\n",
    "        \n",
    "        # Prepare data for scale param calculation\n",
    "        df_temp = df.copy()\n",
    "        for col in self.numeric_features:\n",
    "            df_temp[col].fillna(self.numeric_stats_[col], inplace=True)\n",
    "        \n",
    "        # Apply log transform BEFORE computing scale params\n",
    "        for col in self.log_features:\n",
    "            if col in df_temp.columns:\n",
    "                df_temp[col] = np.log1p(np.maximum(df_temp[col], 0))\n",
    "        \n",
    "        # Learn scaling parameters\n",
    "        X = df_temp[self.numeric_features].values\n",
    "        \n",
    "        if self.scaling == 'standard':\n",
    "            self.scale_params_ = {\n",
    "                'center': X.mean(axis=0),\n",
    "                'scale': X.std(axis=0)\n",
    "            }\n",
    "        elif self.scaling == 'minmax':\n",
    "            self.scale_params_ = {\n",
    "                'center': X.min(axis=0),\n",
    "                'scale': X.max(axis=0) - X.min(axis=0)\n",
    "            }\n",
    "        \n",
    "        # Prevent division by zero\n",
    "        self.scale_params_['scale'] = np.where(\n",
    "            self.scale_params_['scale'] == 0, 1, self.scale_params_['scale']\n",
    "        )\n",
    "        \n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        if not self._is_fitted:\n",
    "            raise ValueError(\"Not fitted!\")\n",
    "        \n",
    "        result = df.copy()\n",
    "        \n",
    "        # 1. Impute numeric\n",
    "        for col in self.numeric_features:\n",
    "            result[col].fillna(self.numeric_stats_[col], inplace=True)\n",
    "        \n",
    "        # 2. Impute categorical\n",
    "        for col in self.categorical_features:\n",
    "            result[col].fillna(self.categorical_values_[col]['mode'], inplace=True)\n",
    "        \n",
    "        # 3. Apply log transform (NEW!)\n",
    "        for col in self.log_features:\n",
    "            if col in result.columns:\n",
    "                result[f'log_{col}'] = np.log1p(np.maximum(result[col], 0))\n",
    "        \n",
    "        # 4. Ordinal encoding\n",
    "        for col, mapping in self.ordinal_mappings.items():\n",
    "            result[f'{col}_encoded'] = result[col].map(mapping).fillna(-1)\n",
    "            result = result.drop(col, axis=1)\n",
    "        \n",
    "        # 5. One-hot encoding\n",
    "        for col in self.categorical_features:\n",
    "            if col in self.ordinal_mappings:\n",
    "                continue\n",
    "            for cat in self.categorical_values_[col]['categories']:\n",
    "                result[f'{col}_{cat}'] = (result[col] == cat).astype(int)\n",
    "            result = result.drop(col, axis=1)\n",
    "        \n",
    "        # 6. Scale numeric (including log-transformed)\n",
    "        if self.scaling:\n",
    "            for i, col in enumerate(self.numeric_features):\n",
    "                # If this column was log-transformed, scale the log version\n",
    "                if col in self.log_features:\n",
    "                    target_col = f'log_{col}'\n",
    "                else:\n",
    "                    target_col = col\n",
    "                \n",
    "                result[target_col] = (\n",
    "                    (result[target_col] - self.scale_params_['center'][i]) / \n",
    "                    self.scale_params_['scale'][i]\n",
    "                )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def fit_transform(self, df):\n",
    "        return self.fit(df).transform(df)\n",
    "\n",
    "print(\"Extended Preprocessor with log transform defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the extended preprocessor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "preprocessor = PreprocessorWithLog(\n",
    "    numeric_features=['age', 'income', 'credit_score', 'years_employed'],\n",
    "    categorical_features=['education', 'employment_type'],\n",
    "    ordinal_mappings={\n",
    "        'education': {'High School': 0, 'Bachelor': 1, 'Master': 2, 'PhD': 3}\n",
    "    },\n",
    "    scaling='standard',\n",
    "    log_features=['income']  # Apply log transform to income\n",
    ")\n",
    "\n",
    "train_processed = preprocessor.fit_transform(train_df)\n",
    "test_processed = preprocessor.transform(test_df)\n",
    "\n",
    "print(\"Processed columns:\")\n",
    "print(train_processed.columns.tolist())\n",
    "\n",
    "print(f\"\\nlog_income statistics:\")\n",
    "print(train_processed['log_income'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge: Titanic Dataset Preprocessing\n",
    "\n",
    "Complete preprocessing pipeline for the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - Titanic Challenge\n",
    "import seaborn as sns\n",
    "\n",
    "# Load Titanic data\n",
    "titanic = sns.load_dataset('titanic')\n",
    "print(f\"Titanic dataset shape: {titanic.shape}\")\n",
    "print(f\"\\nColumns: {titanic.columns.tolist()}\")\n",
    "print(f\"\\nMissing values:\\n{titanic.isnull().sum()[titanic.isnull().sum() > 0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "df_titanic = titanic.copy()\n",
    "\n",
    "# 1. Extract title from name\n",
    "# Note: 'name' column is not in seaborn's version, so we'll skip this\n",
    "# If using Kaggle version:\n",
    "# df_titanic['title'] = df_titanic['name'].str.extract(' ([A-Za-z]+)\\.')\n",
    "\n",
    "# 2. Create family size\n",
    "df_titanic['family_size'] = df_titanic['sibsp'] + df_titanic['parch'] + 1\n",
    "\n",
    "# 3. Is traveling alone?\n",
    "df_titanic['is_alone'] = (df_titanic['family_size'] == 1).astype(int)\n",
    "\n",
    "# 4. Fare per person\n",
    "df_titanic['fare_per_person'] = df_titanic['fare'] / df_titanic['family_size']\n",
    "\n",
    "print(\"New features created:\")\n",
    "print(df_titanic[['family_size', 'is_alone', 'fare_per_person']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "\n",
    "# Age: Impute with median by class and sex\n",
    "df_titanic['age'] = df_titanic.groupby(['pclass', 'sex'])['age'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "\n",
    "# Embarked: Impute with mode\n",
    "df_titanic['embarked'].fillna(df_titanic['embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Deck (from cabin): Extract first letter, fill unknown\n",
    "# df_titanic['deck'] = df_titanic['deck'].fillna('Unknown')\n",
    "\n",
    "print(f\"Missing values after imputation:\")\n",
    "print(df_titanic.isnull().sum()[df_titanic.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "\n",
    "# Sex: Binary encoding\n",
    "df_titanic['sex_encoded'] = (df_titanic['sex'] == 'male').astype(int)\n",
    "\n",
    "# Embarked: One-hot\n",
    "embarked_dummies = pd.get_dummies(df_titanic['embarked'], prefix='embarked', dtype=int)\n",
    "df_titanic = pd.concat([df_titanic, embarked_dummies], axis=1)\n",
    "\n",
    "# Class: Already numeric (1, 2, 3)\n",
    "\n",
    "print(\"Encoded features:\")\n",
    "print(df_titanic[['sex_encoded', 'embarked_C', 'embarked_Q', 'embarked_S']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numeric features\n",
    "numeric_cols = ['age', 'fare', 'fare_per_person', 'family_size']\n",
    "\n",
    "# Standard scaling\n",
    "for col in numeric_cols:\n",
    "    mean = df_titanic[col].mean()\n",
    "    std = df_titanic[col].std()\n",
    "    df_titanic[f'{col}_scaled'] = (df_titanic[col] - mean) / std\n",
    "\n",
    "print(\"Scaled features statistics:\")\n",
    "print(df_titanic[[f'{c}_scaled' for c in numeric_cols]].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final feature set for modeling\n",
    "feature_columns = [\n",
    "    'pclass',\n",
    "    'sex_encoded',\n",
    "    'age_scaled',\n",
    "    'fare_per_person_scaled',\n",
    "    'family_size',\n",
    "    'is_alone',\n",
    "    'embarked_C',\n",
    "    'embarked_Q',\n",
    "    'embarked_S'\n",
    "]\n",
    "\n",
    "X = df_titanic[feature_columns]\n",
    "y = df_titanic['survived']\n",
    "\n",
    "print(f\"Final feature matrix shape: {X.shape}\")\n",
    "print(f\"\\nFeatures:\\n{X.columns.tolist()}\")\n",
    "print(f\"\\nTarget distribution:\\n{y.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Group-based imputation** is more intelligent than global imputation\n",
    "2. **Log transforms** help with skewed distributions like income\n",
    "3. **Feature engineering** (family_size, is_alone) can be very valuable\n",
    "4. **Always fit on training data only** to prevent data leakage\n",
    "\n",
    "---\n",
    "\n",
    "**End of Solutions**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
