{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.4: Einsum Mastery - SOLUTIONS\n",
    "\n",
    "**Module:** 2 - Python for AI/ML  \n",
    "\n",
    "This notebook contains solutions to all exercises from the Einsum Mastery Lab.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Solutions Notebook for Einsum Mastery\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Batch Outer Product\n",
    "\n",
    "**Task:** Compute the outer product for each pair of vectors in a batch.\n",
    "- Input: Two tensors of shape `(batch, dim)`\n",
    "- Output: Shape `(batch, dim, dim)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - Exercise 1\n",
    "np.random.seed(42)\n",
    "a = np.random.randn(16, 64)  # 16 vectors of dim 64\n",
    "b = np.random.randn(16, 64)  # 16 vectors of dim 64\n",
    "\n",
    "# Solution: 'bi,bj->bij'\n",
    "# - a has indices (b, i)\n",
    "# - b has indices (b, j)\n",
    "# - No index is summed over (both appear in output)\n",
    "# - Result has indices (b, i, j)\n",
    "batch_outer = np.einsum('bi,bj->bij', a, b)\n",
    "\n",
    "print(f\"a shape: {a.shape}\")\n",
    "print(f\"b shape: {b.shape}\")\n",
    "print(f\"Result shape: {batch_outer.shape}\")\n",
    "\n",
    "# Verify against loop version\n",
    "expected = np.array([np.outer(a[i], b[i]) for i in range(16)])\n",
    "print(f\"\\nCorrect? {np.allclose(batch_outer, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Solution\n",
    "\n",
    "```\n",
    "'bi,bj->bij'\n",
    "\n",
    "a[b, i]  ×  b[b, j]  =  result[b, i, j]\n",
    "\n",
    "For each batch element b:\n",
    "  For each i and j:\n",
    "    result[b, i, j] = a[b, i] * b[b, j]\n",
    "```\n",
    "\n",
    "This is exactly the outer product definition, applied to each batch element!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Bilinear Form\n",
    "\n",
    "**Task:** Compute $x^T A y$ for batches of vectors and a shared matrix.\n",
    "- x: shape `(batch, m)`\n",
    "- A: shape `(m, n)` (shared across batch)\n",
    "- y: shape `(batch, n)`\n",
    "- Result: shape `(batch,)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - Exercise 2\n",
    "np.random.seed(42)\n",
    "x = np.random.randn(32, 64)   # (batch=32, m=64)\n",
    "A = np.random.randn(64, 128)  # (m=64, n=128)\n",
    "y = np.random.randn(32, 128)  # (batch=32, n=128)\n",
    "\n",
    "# Solution: 'bm,mn,bn->b'\n",
    "# - x has indices (b, m)\n",
    "# - A has indices (m, n)\n",
    "# - y has indices (b, n)\n",
    "# - m and n are summed over (don't appear in output)\n",
    "# - b is preserved\n",
    "bilinear = np.einsum('bm,mn,bn->b', x, A, y)\n",
    "\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"A shape: {A.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Result shape: {bilinear.shape}\")\n",
    "\n",
    "# Verify against loop version\n",
    "expected = np.array([x[i] @ A @ y[i] for i in range(32)])\n",
    "print(f\"\\nCorrect? {np.allclose(bilinear, expected)}\")\n",
    "\n",
    "# Sample values\n",
    "print(f\"\\nFirst 5 values: {bilinear[:5].round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Solution\n",
    "\n",
    "The bilinear form $x^T A y$ expands to:\n",
    "$$\\sum_m \\sum_n x_m \\cdot A_{mn} \\cdot y_n$$\n",
    "\n",
    "In einsum notation: `'bm,mn,bn->b'`\n",
    "\n",
    "```\n",
    "For each batch b:\n",
    "  result[b] = sum over m and n of: x[b,m] * A[m,n] * y[b,n]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Two-Step Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Break into two steps\n",
    "\n",
    "# Step 1: Compute A @ y for each batch element: (m, n) @ (b, n)^T -> (b, m)\n",
    "# Using einsum: 'mn,bn->bm'\n",
    "Ay = np.einsum('mn,bn->bm', A, y)\n",
    "print(f\"Ay shape: {Ay.shape}\")\n",
    "\n",
    "# Step 2: Compute x · (Ay) for each batch element\n",
    "# Using einsum: 'bm,bm->b'\n",
    "bilinear_v2 = np.einsum('bm,bm->b', x, Ay)\n",
    "\n",
    "print(f\"Same result? {np.allclose(bilinear, bilinear_v2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Multi-Head Attention with Einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete multi-head attention implementation\n",
    "def softmax(x, axis=-1):\n",
    "    x_max = x.max(axis=axis, keepdims=True)\n",
    "    exp_x = np.exp(x - x_max)\n",
    "    return exp_x / exp_x.sum(axis=axis, keepdims=True)\n",
    "\n",
    "def multi_head_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Multi-head scaled dot-product attention using einsum.\n",
    "    \n",
    "    Args:\n",
    "        Q: Query tensor (batch, heads, seq_q, dim)\n",
    "        K: Key tensor (batch, heads, seq_k, dim)\n",
    "        V: Value tensor (batch, heads, seq_k, dim)\n",
    "        mask: Optional attention mask (batch, heads, seq_q, seq_k)\n",
    "    \n",
    "    Returns:\n",
    "        output: (batch, heads, seq_q, dim)\n",
    "        attention_weights: (batch, heads, seq_q, seq_k)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # Compute attention scores\n",
    "    # Q: (b, h, sq, d) @ K^T: (b, h, d, sk) -> (b, h, sq, sk)\n",
    "    scores = np.einsum('bhqd,bhkd->bhqk', Q, K) / np.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if mask is not None:\n",
    "        scores = np.where(mask == 0, -1e9, scores)\n",
    "    \n",
    "    # Softmax\n",
    "    weights = softmax(scores, axis=-1)\n",
    "    \n",
    "    # Apply to values\n",
    "    # weights: (b, h, sq, sk) @ V: (b, h, sk, d) -> (b, h, sq, d)\n",
    "    output = np.einsum('bhqk,bhkd->bhqd', weights, V)\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "# Test\n",
    "np.random.seed(42)\n",
    "batch, heads, seq_len, dim = 2, 4, 8, 16\n",
    "\n",
    "Q = np.random.randn(batch, heads, seq_len, dim).astype(np.float32)\n",
    "K = np.random.randn(batch, heads, seq_len, dim).astype(np.float32)\n",
    "V = np.random.randn(batch, heads, seq_len, dim).astype(np.float32)\n",
    "\n",
    "output, weights = multi_head_attention(Q, K, V)\n",
    "\n",
    "print(\"Multi-Head Attention Results:\")\n",
    "print(f\"  Q shape: {Q.shape}\")\n",
    "print(f\"  K shape: {K.shape}\")\n",
    "print(f\"  V shape: {V.shape}\")\n",
    "print(f\"  Output shape: {output.shape}\")\n",
    "print(f\"  Weights shape: {weights.shape}\")\n",
    "print(f\"  Weights sum to 1? {np.allclose(weights.sum(axis=-1), 1.0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With causal mask (for autoregressive models)\n",
    "causal_mask = np.triu(np.ones((seq_len, seq_len)), k=1)\n",
    "causal_mask = 1 - causal_mask  # 1 = attend, 0 = mask\n",
    "causal_mask = causal_mask[np.newaxis, np.newaxis, :, :]  # Add batch and head dims\n",
    "\n",
    "print(\"Causal mask (for token 5):\")\n",
    "print(causal_mask[0, 0, 4, :])  # Token 5 can attend to tokens 0-4\n",
    "\n",
    "output_causal, weights_causal = multi_head_attention(Q, K, V, mask=causal_mask)\n",
    "\n",
    "print(f\"\\nCausal attention weights for position 4:\")\n",
    "print(f\"  {weights_causal[0, 0, 4, :].round(3)}\")\n",
    "print(f\"  (positions 5-7 should be ~0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Complete Einsum Reference\n",
    "\n",
    "### Single Array Operations\n",
    "\n",
    "| Operation | Einsum | NumPy |\n",
    "|-----------|--------|-------|\n",
    "| Sum all | `'ij->'` | `np.sum(A)` |\n",
    "| Row sums | `'ij->i'` | `np.sum(A, axis=1)` |\n",
    "| Column sums | `'ij->j'` | `np.sum(A, axis=0)` |\n",
    "| Transpose | `'ij->ji'` | `A.T` |\n",
    "| Diagonal | `'ii->i'` | `np.diag(A)` |\n",
    "| Trace | `'ii->'` | `np.trace(A)` |\n",
    "\n",
    "### Two Array Operations\n",
    "\n",
    "| Operation | Einsum | NumPy |\n",
    "|-----------|--------|-------|\n",
    "| Dot product | `'i,i->'` | `np.dot(a, b)` |\n",
    "| Outer product | `'i,j->ij'` | `np.outer(a, b)` |\n",
    "| Matrix multiply | `'ik,kj->ij'` | `A @ B` |\n",
    "| Matrix-vector | `'ij,j->i'` | `A @ v` |\n",
    "| Element-wise | `'ij,ij->ij'` | `A * B` |\n",
    "\n",
    "### Batch Operations\n",
    "\n",
    "| Operation | Einsum |\n",
    "|-----------|--------|\n",
    "| Batch matmul | `'bik,bkj->bij'` |\n",
    "| Batch outer | `'bi,bj->bij'` |\n",
    "| Batch dot | `'bi,bi->b'` |\n",
    "| Attention scores | `'bhsd,bhtd->bhst'` |\n",
    "| Attention apply | `'bhst,bhtd->bhsd'` |\n",
    "\n",
    "---\n",
    "\n",
    "**End of Solutions**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
