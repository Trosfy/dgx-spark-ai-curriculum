{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.4: Einsum Mastery\n",
    "\n",
    "**Module:** 2 - Python for AI/ML  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** â­â­â­\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand Einstein summation notation (einsum)\n",
    "- [ ] Implement common operations using einsum\n",
    "- [ ] Build attention score computation using einsum\n",
    "- [ ] Compare einsum with explicit implementations\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Prerequisites\n",
    "\n",
    "- Completed: Tasks 2.1-2.3\n",
    "- Knowledge of: Matrix multiplication, broadcasting\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ Real-World Context\n",
    "\n",
    "**Why learn einsum?**\n",
    "\n",
    "Einsum is the Swiss Army knife of tensor operations:\n",
    "- **Transformers** use einsum for attention computation\n",
    "- **PyTorch and TensorFlow** both support einsum natively\n",
    "- **Research papers** often use Einstein notation\n",
    "\n",
    "Once you master einsum, you can:\n",
    "- Read cutting-edge ML research more easily\n",
    "- Write cleaner, more expressive code\n",
    "- Implement complex operations in a single line\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§’ ELI5: What is Einsum?\n",
    "\n",
    "> **Imagine you're giving directions for a treasure hunt...** ðŸ—ºï¸\n",
    ">\n",
    "> Instead of saying:\n",
    "> \"Take the first row of map A, multiply it by the first column of map B,\n",
    "> add all the numbers, put it in position (1,1) of the result... repeat for every position\"\n",
    ">\n",
    "> You can just say:\n",
    "> \"For each i,j in the result, sum over k: A[i,k] Ã— B[k,j]\"\n",
    ">\n",
    "> Or even shorter: `'ik,kj->ij'`\n",
    ">\n",
    "> **In AI terms:** Einsum lets you describe tensor operations using index notation.\n",
    "> Indices that appear on both sides get matched, indices that disappear get summed over.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Welcome to Einsum Mastery! ðŸ§®\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "def time_function(func, *args, n_runs=5):\n",
    "    \"\"\"Time a function over multiple runs.\"\"\"\n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.perf_counter()\n",
    "        result = func(*args)\n",
    "        times.append(time.perf_counter() - start)\n",
    "    return np.mean(times), result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Einsum Basics\n",
    "\n",
    "### The Einsum Notation\n",
    "\n",
    "```\n",
    "np.einsum('input_indices -> output_indices', arrays)\n",
    "```\n",
    "\n",
    "Rules:\n",
    "1. Each input array gets labels for its dimensions\n",
    "2. Indices that appear in inputs but not output are **summed over**\n",
    "3. Indices in output define the result shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start simple!\n",
    "\n",
    "# Example 1: Sum all elements\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "# 'ij->' means: take all i,j indices, no output indices = sum everything\n",
    "result_einsum = np.einsum('ij->', A)\n",
    "result_sum = np.sum(A)\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(f\"\\nSum all elements:\")\n",
    "print(f\"  np.einsum('ij->', A) = {result_einsum}\")\n",
    "print(f\"  np.sum(A)            = {result_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Sum along an axis\n",
    "\n",
    "# Sum over j (columns) = row sums\n",
    "row_sums_einsum = np.einsum('ij->i', A)\n",
    "row_sums_numpy = np.sum(A, axis=1)\n",
    "\n",
    "print(f\"Sum over columns (row sums):\")\n",
    "print(f\"  np.einsum('ij->i', A) = {row_sums_einsum}\")\n",
    "print(f\"  np.sum(A, axis=1)     = {row_sums_numpy}\")\n",
    "\n",
    "# Sum over i (rows) = column sums\n",
    "col_sums_einsum = np.einsum('ij->j', A)\n",
    "col_sums_numpy = np.sum(A, axis=0)\n",
    "\n",
    "print(f\"\\nSum over rows (column sums):\")\n",
    "print(f\"  np.einsum('ij->j', A) = {col_sums_einsum}\")\n",
    "print(f\"  np.sum(A, axis=0)     = {col_sums_numpy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Transpose\n",
    "\n",
    "# 'ij->ji' swaps the indices\n",
    "transpose_einsum = np.einsum('ij->ji', A)\n",
    "transpose_numpy = A.T\n",
    "\n",
    "print(\"Original A:\")\n",
    "print(A)\n",
    "print(f\"\\nTranspose with einsum ('ij->ji'):\")\n",
    "print(transpose_einsum)\n",
    "print(f\"\\nMatches A.T? {np.allclose(transpose_einsum, transpose_numpy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Diagonal elements\n",
    "\n",
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# 'ii->i' extracts diagonal (where row index = column index)\n",
    "diag_einsum = np.einsum('ii->i', A)\n",
    "diag_numpy = np.diag(A)\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(f\"\\nDiagonal with einsum ('ii->i'): {diag_einsum}\")\n",
    "print(f\"Diagonal with np.diag:          {diag_numpy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Trace (sum of diagonal)\n",
    "\n",
    "# 'ii->' means: take diagonal elements (ii), sum them all (no output)\n",
    "trace_einsum = np.einsum('ii->', A)\n",
    "trace_numpy = np.trace(A)\n",
    "\n",
    "print(f\"Trace with einsum ('ii->'): {trace_einsum}\")\n",
    "print(f\"Trace with np.trace:        {trace_numpy}\")\n",
    "print(f\"\\nðŸ’¡ The trace is 1+5+9 = {1+5+9}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” Einsum Pattern Cheat Sheet (Part 1)\n",
    "\n",
    "| Operation | Einsum | NumPy Equivalent |\n",
    "|-----------|--------|------------------|\n",
    "| Sum all | `'ij->'` | `np.sum(A)` |\n",
    "| Row sums | `'ij->i'` | `np.sum(A, axis=1)` |\n",
    "| Col sums | `'ij->j'` | `np.sum(A, axis=0)` |\n",
    "| Transpose | `'ij->ji'` | `A.T` |\n",
    "| Diagonal | `'ii->i'` | `np.diag(A)` |\n",
    "| Trace | `'ii->'` | `np.trace(A)` |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Two-Array Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Dot product (vectors)\n",
    "\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "# 'i,i->' means: multiply corresponding elements (same index i), sum all\n",
    "dot_einsum = np.einsum('i,i->', a, b)\n",
    "dot_numpy = np.dot(a, b)\n",
    "\n",
    "print(f\"a = {a}\")\n",
    "print(f\"b = {b}\")\n",
    "print(f\"\\nDot product with einsum ('i,i->'): {dot_einsum}\")\n",
    "print(f\"Dot product with np.dot:           {dot_numpy}\")\n",
    "print(f\"\\nðŸ’¡ 1Ã—4 + 2Ã—5 + 3Ã—6 = {1*4 + 2*5 + 3*6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 7: Outer product\n",
    "\n",
    "# 'i,j->ij' means: for each combination of i and j, multiply a[i] Ã— b[j]\n",
    "outer_einsum = np.einsum('i,j->ij', a, b)\n",
    "outer_numpy = np.outer(a, b)\n",
    "\n",
    "print(f\"Outer product with einsum ('i,j->ij'):\")\n",
    "print(outer_einsum)\n",
    "print(f\"\\nMatches np.outer? {np.allclose(outer_einsum, outer_numpy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 8: Matrix multiplication!\n",
    "\n",
    "A = np.random.randn(3, 4)\n",
    "B = np.random.randn(4, 5)\n",
    "\n",
    "# 'ik,kj->ij' means:\n",
    "# - A has indices (i,k)\n",
    "# - B has indices (k,j)  \n",
    "# - k appears in both but not in output â†’ sum over k\n",
    "# - Result has indices (i,j)\n",
    "matmul_einsum = np.einsum('ik,kj->ij', A, B)\n",
    "matmul_numpy = A @ B\n",
    "\n",
    "print(f\"A shape: {A.shape}\")\n",
    "print(f\"B shape: {B.shape}\")\n",
    "print(f\"Result shape: {matmul_einsum.shape}\")\n",
    "print(f\"\\nMatches A @ B? {np.allclose(matmul_einsum, matmul_numpy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 9: Element-wise multiplication\n",
    "\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "# 'ij,ij->ij' means: same indices in, same indices out = element-wise\n",
    "hadamard_einsum = np.einsum('ij,ij->ij', A, B)\n",
    "hadamard_numpy = A * B\n",
    "\n",
    "print(\"Element-wise multiplication:\")\n",
    "print(hadamard_einsum)\n",
    "print(f\"\\nMatches A * B? {np.allclose(hadamard_einsum, hadamard_numpy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” Einsum Pattern Cheat Sheet (Part 2)\n",
    "\n",
    "| Operation | Einsum | NumPy Equivalent |\n",
    "|-----------|--------|------------------|\n",
    "| Dot product | `'i,i->'` | `np.dot(a, b)` |\n",
    "| Outer product | `'i,j->ij'` | `np.outer(a, b)` |\n",
    "| Matrix multiply | `'ik,kj->ij'` | `A @ B` |\n",
    "| Element-wise | `'ij,ij->ij'` | `A * B` |\n",
    "| Matrix-vector | `'ij,j->i'` | `A @ v` |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Batch Operations (This is where it gets powerful!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 10: Batch matrix multiplication\n",
    "\n",
    "batch_size = 32\n",
    "A = np.random.randn(batch_size, 64, 128)  # 32 matrices of 64Ã—128\n",
    "B = np.random.randn(batch_size, 128, 64)  # 32 matrices of 128Ã—64\n",
    "\n",
    "# 'bik,bkj->bij' means:\n",
    "# - b is batch dimension (preserved)\n",
    "# - ik,kj->ij is matrix multiply (k summed over)\n",
    "batch_matmul_einsum = np.einsum('bik,bkj->bij', A, B)\n",
    "batch_matmul_numpy = A @ B\n",
    "\n",
    "print(f\"A shape: {A.shape}\")\n",
    "print(f\"B shape: {B.shape}\")\n",
    "print(f\"Result shape: {batch_matmul_einsum.shape}\")\n",
    "print(f\"\\nMatches A @ B? {np.allclose(batch_matmul_einsum, batch_matmul_numpy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 11: Batch dot product (useful for similarity scores)\n",
    "\n",
    "# Two sets of vectors: compute dot product for corresponding pairs\n",
    "queries = np.random.randn(32, 128)  # 32 query vectors\n",
    "keys = np.random.randn(32, 128)     # 32 key vectors\n",
    "\n",
    "# 'bi,bi->b' means: for each b, compute dot product of vectors\n",
    "batch_dot_einsum = np.einsum('bi,bi->b', queries, keys)\n",
    "batch_dot_loop = np.array([np.dot(q, k) for q, k in zip(queries, keys)])\n",
    "\n",
    "print(f\"Queries shape: {queries.shape}\")\n",
    "print(f\"Keys shape: {keys.shape}\")\n",
    "print(f\"Result shape: {batch_dot_einsum.shape}\")\n",
    "print(f\"\\nFirst 5 dot products: {batch_dot_einsum[:5].round(2)}\")\n",
    "print(f\"Matches loop? {np.allclose(batch_dot_einsum, batch_dot_loop)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 12: Broadcasting in einsum\n",
    "\n",
    "# Single matrix applied to batch of vectors\n",
    "W = np.random.randn(256, 128)       # Weight matrix\n",
    "X = np.random.randn(32, 128)        # Batch of 32 input vectors\n",
    "\n",
    "# 'oi,bi->bo' means:\n",
    "# - W has indices (o,i)\n",
    "# - X has indices (b,i)\n",
    "# - i summed over, result is (b,o)\n",
    "# This is X @ W.T\n",
    "result_einsum = np.einsum('oi,bi->bo', W, X)\n",
    "result_numpy = X @ W.T\n",
    "\n",
    "print(f\"W shape: {W.shape}\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"Result shape: {result_einsum.shape}\")\n",
    "print(f\"\\nMatches X @ W.T? {np.allclose(result_einsum, result_numpy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Attention Score Computation ðŸ”¥\n",
    "\n",
    "### ðŸ§’ ELI5: What is Attention?\n",
    "\n",
    "> **Imagine reading a story...** ðŸ“–\n",
    ">\n",
    "> When you read \"The cat sat on the mat because **it** was tired\", \n",
    "> your brain automatically knows \"it\" refers to \"cat\", not \"mat\".\n",
    ">\n",
    "> **Attention** gives AI the same ability! For each word, it computes\n",
    "> \"how much should I pay attention to every other word?\"\n",
    ">\n",
    "> The formula: $\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention dimensions (like in a Transformer)\n",
    "batch_size = 8\n",
    "num_heads = 12\n",
    "seq_length = 64\n",
    "head_dim = 64\n",
    "\n",
    "# Q, K, V tensors: (batch, heads, seq_len, head_dim)\n",
    "Q = np.random.randn(batch_size, num_heads, seq_length, head_dim).astype(np.float32)\n",
    "K = np.random.randn(batch_size, num_heads, seq_length, head_dim).astype(np.float32)\n",
    "V = np.random.randn(batch_size, num_heads, seq_length, head_dim).astype(np.float32)\n",
    "\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"K shape: {K.shape}\")\n",
    "print(f\"V shape: {V.shape}\")\n",
    "print(f\"\\nðŸ’¡ This represents {batch_size} sequences, {num_heads} attention heads,\")\n",
    "print(f\"   each sequence has {seq_length} tokens, each head dimension is {head_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute attention scores = Q @ K.T\n",
    "# Shape: (batch, heads, seq, seq)\n",
    "\n",
    "# Using einsum: 'bhsd,bhtd->bhst'\n",
    "# - Q: (b)atch, (h)eads, (s)eq, (d)im\n",
    "# - K: (b)atch, (h)eads, (t)arget_seq, (d)im\n",
    "# - d summed over â†’ result is (b, h, s, t)\n",
    "scores_einsum = np.einsum('bhsd,bhtd->bhst', Q, K)\n",
    "\n",
    "# Using matmul (need to transpose K's last two dims)\n",
    "scores_matmul = Q @ K.transpose(0, 1, 3, 2)  # Transpose last two dims\n",
    "\n",
    "print(f\"Attention scores shape: {scores_einsum.shape}\")\n",
    "print(f\"Matches matmul version? {np.allclose(scores_einsum, scores_matmul)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Scale by sqrt(d_k)\n",
    "scale = np.sqrt(head_dim)\n",
    "scaled_scores = scores_einsum / scale\n",
    "\n",
    "print(f\"Scale factor: {scale}\")\n",
    "print(f\"Scaled scores shape: {scaled_scores.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply softmax\n",
    "def softmax(x, axis=-1):\n",
    "    \"\"\"Numerically stable softmax.\"\"\"\n",
    "    x_max = x.max(axis=axis, keepdims=True)\n",
    "    exp_x = np.exp(x - x_max)\n",
    "    return exp_x / exp_x.sum(axis=axis, keepdims=True)\n",
    "\n",
    "attention_weights = softmax(scaled_scores, axis=-1)\n",
    "\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"Weights sum to 1? {np.allclose(attention_weights.sum(axis=-1), 1.0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Apply attention to values = Attn @ V\n",
    "# Shape: (batch, heads, seq, head_dim)\n",
    "\n",
    "# Using einsum: 'bhst,bhtd->bhsd'\n",
    "# - Attention: (b, h, s, t)\n",
    "# - V: (b, h, t, d)\n",
    "# - t summed over â†’ result is (b, h, s, d)\n",
    "output_einsum = np.einsum('bhst,bhtd->bhsd', attention_weights, V)\n",
    "\n",
    "# Using matmul\n",
    "output_matmul = attention_weights @ V\n",
    "\n",
    "print(f\"Output shape: {output_einsum.shape}\")\n",
    "print(f\"Matches matmul version? {np.allclose(output_einsum, output_matmul)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete attention function!\n",
    "\n",
    "def attention_einsum(Q, K, V):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention using einsum.\n",
    "    \n",
    "    Args:\n",
    "        Q: Query tensor (batch, heads, seq, dim)\n",
    "        K: Key tensor (batch, heads, seq, dim)\n",
    "        V: Value tensor (batch, heads, seq, dim)\n",
    "    \n",
    "    Returns:\n",
    "        Output tensor (batch, heads, seq, dim)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # Compute attention scores: Q @ K.T / sqrt(d_k)\n",
    "    scores = np.einsum('bhsd,bhtd->bhst', Q, K) / np.sqrt(d_k)\n",
    "    \n",
    "    # Apply softmax\n",
    "    weights = softmax(scores, axis=-1)\n",
    "    \n",
    "    # Apply to values\n",
    "    output = np.einsum('bhst,bhtd->bhsd', weights, V)\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "# Test it!\n",
    "output, weights = attention_einsum(Q, K, V)\n",
    "print(f\"âœ… Attention output shape: {output.shape}\")\n",
    "print(f\"   Attention weights shape: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_matmul(Q, K, V):\n",
    "    \"\"\"Attention using standard matmul.\"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = (Q @ K.transpose(0, 1, 3, 2)) / np.sqrt(d_k)\n",
    "    weights = softmax(scores, axis=-1)\n",
    "    output = weights @ V\n",
    "    return output, weights\n",
    "\n",
    "# Time both versions\n",
    "einsum_time, _ = time_function(attention_einsum, Q, K, V, n_runs=10)\n",
    "matmul_time, _ = time_function(attention_matmul, Q, K, V, n_runs=10)\n",
    "\n",
    "print(f\"Einsum version: {einsum_time*1000:.2f} ms\")\n",
    "print(f\"Matmul version: {matmul_time*1000:.2f} ms\")\n",
    "print(f\"\\nðŸ’¡ Both are similar because NumPy optimizes both patterns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ‹ Try It Yourself: Exercises\n",
    "\n",
    "### Exercise 1: Batch Outer Product\n",
    "\n",
    "Compute the outer product for each pair of vectors in a batch.\n",
    "\n",
    "- Input: Two tensors of shape `(batch, dim)`\n",
    "- Output: Shape `(batch, dim, dim)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "np.random.seed(42)\n",
    "a = np.random.randn(16, 64)  # 16 vectors of dim 64\n",
    "b = np.random.randn(16, 64)  # 16 vectors of dim 64\n",
    "\n",
    "# TODO: Use einsum to compute batch outer product\n",
    "# batch_outer = np.einsum('??,??->???', a, b)\n",
    "\n",
    "# Uncomment to verify:\n",
    "# print(f\"Result shape: {batch_outer.shape}\")  # Should be (16, 64, 64)\n",
    "# expected = np.array([np.outer(a[i], b[i]) for i in range(16)])\n",
    "# print(f\"Correct? {np.allclose(batch_outer, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Hint for Exercise 1</summary>\n",
    "\n",
    "For outer product of vectors: `'i,j->ij'`\n",
    "\n",
    "Add batch dimension b: `'bi,bj->bij'`\n",
    "\n",
    "</details>\n",
    "\n",
    "### Exercise 2: Bilinear Form\n",
    "\n",
    "Compute $x^T A y$ for batches of vectors and a shared matrix.\n",
    "\n",
    "- x: shape `(batch, m)`\n",
    "- A: shape `(m, n)` (shared across batch)\n",
    "- y: shape `(batch, n)`\n",
    "- Result: shape `(batch,)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2\n",
    "np.random.seed(42)\n",
    "x = np.random.randn(32, 64)\n",
    "A = np.random.randn(64, 128)\n",
    "y = np.random.randn(32, 128)\n",
    "\n",
    "# TODO: Compute x^T A y using einsum\n",
    "# bilinear = np.einsum('...', x, A, y)\n",
    "\n",
    "# Uncomment to verify:\n",
    "# print(f\"Result shape: {bilinear.shape}\")  # Should be (32,)\n",
    "# expected = np.array([x[i] @ A @ y[i] for i in range(32)])\n",
    "# print(f\"Correct? {np.allclose(bilinear, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Hint for Exercise 2</summary>\n",
    "\n",
    "Break it down:\n",
    "1. x has indices (b, m)\n",
    "2. A has indices (m, n)\n",
    "3. y has indices (b, n)\n",
    "4. m and n should be summed over\n",
    "\n",
    "Answer: `'bm,mn,bn->b'`\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš ï¸ Common Mistakes\n",
    "\n",
    "### Mistake 1: Wrong index order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randn(3, 4)\n",
    "B = np.random.randn(4, 5)\n",
    "\n",
    "# âŒ Wrong: indices don't match for matmul\n",
    "# result = np.einsum('ij,jk->ik', A, B)  # This would work\n",
    "# result = np.einsum('ij,kj->ik', A, B)  # This is different! (A @ B.T)\n",
    "\n",
    "# âœ… Right: match the shared dimension\n",
    "result = np.einsum('ij,jk->ik', A, B)  # Standard matmul\n",
    "print(f\"Result shape: {result.shape}\")\n",
    "print(f\"\\nðŸ’¡ Always check which indices are being summed over!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Forgetting optimize=True for large operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For complex einsum with many tensors, use optimize=True\n",
    "# This lets NumPy find the optimal contraction order\n",
    "\n",
    "A = np.random.randn(10, 20)\n",
    "B = np.random.randn(20, 30)\n",
    "C = np.random.randn(30, 40)\n",
    "\n",
    "# âœ… Good: let NumPy optimize\n",
    "result = np.einsum('ij,jk,kl->il', A, B, C, optimize=True)\n",
    "print(f\"Result shape: {result.shape}\")\n",
    "print(f\"\\nðŸ’¡ optimize=True is important for 3+ tensor contractions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- âœ… Basic einsum operations (sum, transpose, diagonal, trace)\n",
    "- âœ… Two-tensor operations (dot, outer, matmul)\n",
    "- âœ… Batch operations for ML workloads\n",
    "- âœ… Implementing attention with einsum\n",
    "\n",
    "## ðŸ“‹ Complete Einsum Reference\n",
    "\n",
    "| Operation | Einsum | Explicit |\n",
    "|-----------|--------|----------|\n",
    "| Sum | `'ij->'` | `np.sum(A)` |\n",
    "| Transpose | `'ij->ji'` | `A.T` |\n",
    "| Diagonal | `'ii->i'` | `np.diag(A)` |\n",
    "| Trace | `'ii->'` | `np.trace(A)` |\n",
    "| Dot product | `'i,i->'` | `np.dot(a,b)` |\n",
    "| Outer product | `'i,j->ij'` | `np.outer(a,b)` |\n",
    "| Matrix multiply | `'ik,kj->ij'` | `A @ B` |\n",
    "| Batch matmul | `'bik,bkj->bij'` | `A @ B` |\n",
    "| Attention scores | `'bhsd,bhtd->bhst'` | `Q @ K.T` |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“– Further Reading\n",
    "\n",
    "- [Einstein Summation in NumPy](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html)\n",
    "- [Einsum is All You Need (Blog)](https://rockt.github.io/2018/04/30/einsum)\n",
    "- [Attention Is All You Need (Paper)](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del Q, K, V, output, weights\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… Memory cleaned up!\")\n",
    "print(\"\\nðŸŽ‰ Congratulations! You've mastered einsum!\")\n",
    "print(\"   Next up: Task 2.5 - Profiling Exercise\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
