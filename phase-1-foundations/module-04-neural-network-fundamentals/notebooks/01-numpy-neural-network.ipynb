{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4.1: NumPy Neural Network from Scratch\n",
    "\n",
    "**Module:** 4 - Neural Network Fundamentals  \n",
    "**Time:** 4 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê (Challenging but rewarding!)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand how neural networks learn through forward and backward passes\n",
    "- [ ] Implement a fully-connected (Linear) layer from scratch\n",
    "- [ ] Implement ReLU activation with proper gradient computation\n",
    "- [ ] Implement Softmax + Cross-Entropy for classification\n",
    "- [ ] Build a complete training loop with SGD optimizer\n",
    "- [ ] Train a network on MNIST to >95% accuracy\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Module 2 (NumPy proficiency)\n",
    "- Completed: Module 3 (Matrix calculus, chain rule)\n",
    "- Knowledge of: Matrix multiplication, derivatives\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**Why build from scratch when PyTorch exists?**\n",
    "\n",
    "Understanding the internals helps you:\n",
    "1. **Debug effectively** - When training fails, you know where to look\n",
    "2. **Optimize performance** - Knowing bottlenecks helps you speed things up\n",
    "3. **Design new architectures** - Innovation requires understanding primitives\n",
    "4. **Interview prep** - Top AI companies ask about these fundamentals\n",
    "\n",
    "Many successful ML engineers at Google, OpenAI, and Anthropic can implement neural networks from scratch. It's a rite of passage!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßí ELI5: How Neural Networks Learn\n",
    "\n",
    "> **Imagine you're playing a game of \"Hot and Cold\" with a friend who's hiding.**\n",
    ">\n",
    "> You take a step in some direction. Your friend says \"warmer\" or \"colder.\" Based on that feedback, you adjust your direction and step size. Over many steps, you find your friend!\n",
    ">\n",
    "> **A neural network works the same way:**\n",
    "> 1. **Forward Pass**: Make a prediction (take a step)\n",
    "> 2. **Loss Calculation**: Check how wrong we were (\"warmer\" or \"colder\")\n",
    "> 3. **Backward Pass**: Figure out which direction to adjust (where did I go wrong?)\n",
    "> 4. **Weight Update**: Take a small step in the right direction\n",
    ">\n",
    "> **The magic of backpropagation** is that it can trace blame back through many layers. If the output is wrong, it figures out how much each neuron in each layer contributed to that mistake.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add scripts directory to path\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(os.getcwd()), 'scripts'))\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('default')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding the Forward Pass\n",
    "\n",
    "### What happens in a neural network layer?\n",
    "\n",
    "A single neuron computes:\n",
    "$$y = \\sigma(w_1 x_1 + w_2 x_2 + ... + w_n x_n + b) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b)$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{x}$ = input vector\n",
    "- $\\mathbf{w}$ = weight vector (learnable)\n",
    "- $b$ = bias (learnable)\n",
    "- $\\sigma$ = activation function (adds non-linearity)\n",
    "\n",
    "For a whole layer with many neurons, we use matrix multiplication:\n",
    "$$\\mathbf{Y} = \\sigma(\\mathbf{X} \\mathbf{W} + \\mathbf{b})$$\n",
    "\n",
    "Let's visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's trace through a simple example\n",
    "print(\"üîç Tracing a Forward Pass\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Input: batch of 2 samples, each with 3 features\n",
    "X = np.array([\n",
    "    [1.0, 2.0, 3.0],   # Sample 1\n",
    "    [4.0, 5.0, 6.0]    # Sample 2\n",
    "])\n",
    "print(f\"Input X (2 samples, 3 features):\")\n",
    "print(X)\n",
    "\n",
    "# Weights: 3 input features -> 4 output neurons\n",
    "W = np.array([\n",
    "    [0.1, 0.2, 0.3, 0.4],\n",
    "    [0.5, 0.6, 0.7, 0.8],\n",
    "    [0.9, 1.0, 1.1, 1.2]\n",
    "])\n",
    "print(f\"\\nWeights W (3 inputs -> 4 neurons):\")\n",
    "print(W)\n",
    "\n",
    "# Bias: one per output neuron\n",
    "b = np.array([0.1, 0.2, 0.3, 0.4])\n",
    "print(f\"\\nBias b (4 neurons): {b}\")\n",
    "\n",
    "# Linear transformation\n",
    "Z = X @ W + b\n",
    "print(f\"\\nLinear output Z = X @ W + b:\")\n",
    "print(Z)\n",
    "print(f\"Shape: {Z.shape}\")\n",
    "\n",
    "# ReLU activation\n",
    "A = np.maximum(0, Z)\n",
    "print(f\"\\nAfter ReLU activation:\")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "1. We took a batch of 2 input samples (each with 3 features)\n",
    "2. Matrix multiplication `X @ W` transformed 3 features ‚Üí 4 neurons\n",
    "3. Adding bias shifted each neuron's output\n",
    "4. ReLU removed negative values (keeping only positive \"activations\")\n",
    "\n",
    "**Key insight:** The weights and biases are the \"learnable parameters\" that we'll adjust during training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Implementing the Linear Layer\n",
    "\n",
    "Now let's create a proper `Linear` layer class with both forward and backward passes.\n",
    "\n",
    "### The Math Behind Backpropagation\n",
    "\n",
    "Given: $Z = XW + b$ and gradient from above: $\\frac{\\partial L}{\\partial Z}$\n",
    "\n",
    "We need to compute:\n",
    "- $\\frac{\\partial L}{\\partial W} = X^T \\cdot \\frac{\\partial L}{\\partial Z}$ (to update weights)\n",
    "- $\\frac{\\partial L}{\\partial b} = \\sum \\frac{\\partial L}{\\partial Z}$ (to update bias)\n",
    "- $\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Z} \\cdot W^T$ (to pass to previous layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \"\"\"\n",
    "    Fully-connected linear layer.\n",
    "    \n",
    "    ELI5: A Linear layer is like a voting committee. Each input feature\n",
    "    is a voter, and each weight represents how much we trust that voter.\n",
    "    The bias is a starting opinion before any votes come in.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int, init: str = 'he'):\n",
    "        \"\"\"\n",
    "        Initialize the linear layer.\n",
    "        \n",
    "        Args:\n",
    "            in_features: Number of input features\n",
    "            out_features: Number of output features (neurons)\n",
    "            init: Initialization method ('he' for ReLU, 'xavier' for tanh/sigmoid)\n",
    "        \"\"\"\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Initialize weights using He initialization (optimal for ReLU)\n",
    "        if init == 'he':\n",
    "            std = np.sqrt(2.0 / in_features)\n",
    "        else:  # xavier\n",
    "            std = np.sqrt(2.0 / (in_features + out_features))\n",
    "            \n",
    "        self.W = np.random.randn(in_features, out_features) * std\n",
    "        self.b = np.zeros(out_features)\n",
    "        \n",
    "        # Cache for backward pass\n",
    "        self.cache = {}\n",
    "        \n",
    "        # Gradients\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass: Z = X @ W + b\n",
    "        \n",
    "        Args:\n",
    "            X: Input of shape (batch_size, in_features)\n",
    "            \n",
    "        Returns:\n",
    "            Output of shape (batch_size, out_features)\n",
    "        \"\"\"\n",
    "        # Save input for backward pass\n",
    "        self.cache['X'] = X\n",
    "        \n",
    "        # Linear transformation\n",
    "        Z = X @ self.W + self.b\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients.\n",
    "        \n",
    "        Args:\n",
    "            dZ: Gradient from above, shape (batch_size, out_features)\n",
    "            \n",
    "        Returns:\n",
    "            Gradient to pass to previous layer, shape (batch_size, in_features)\n",
    "        \"\"\"\n",
    "        X = self.cache['X']\n",
    "        batch_size = X.shape[0]\n",
    "        \n",
    "        # Gradient for weights: X^T @ dZ, averaged over batch\n",
    "        self.dW = X.T @ dZ / batch_size\n",
    "        \n",
    "        # Gradient for bias: sum over batch dimension\n",
    "        self.db = np.mean(dZ, axis=0)\n",
    "        \n",
    "        # Gradient for input (to pass to previous layer)\n",
    "        dX = dZ @ self.W.T\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "    def __call__(self, X: np.ndarray) -> np.ndarray:\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Linear layer\n",
    "print(\"üß™ Testing Linear Layer\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create layer: 784 inputs -> 256 outputs (like first layer for MNIST)\n",
    "layer = Linear(784, 256)\n",
    "print(f\"Layer: {layer.in_features} -> {layer.out_features}\")\n",
    "print(f\"Weight shape: {layer.W.shape}\")\n",
    "print(f\"Bias shape: {layer.b.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "X = np.random.randn(32, 784)  # Batch of 32 images\n",
    "Z = layer(X)\n",
    "print(f\"\\nForward pass: ({32}, {784}) -> {Z.shape}\")\n",
    "\n",
    "# Backward pass\n",
    "dZ = np.random.randn(32, 256)  # Gradient from above\n",
    "dX = layer.backward(dZ)\n",
    "print(f\"Backward pass: gradient shape = {dX.shape}\")\n",
    "print(f\"Weight gradient shape: {layer.dW.shape}\")\n",
    "print(f\"Bias gradient shape: {layer.db.shape}\")\n",
    "\n",
    "print(\"\\n‚úÖ Linear layer works correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Implementing ReLU Activation\n",
    "\n",
    "### Why do we need activation functions?\n",
    "\n",
    "Without activations, stacking linear layers would just be one big linear transformation:\n",
    "$$XW_1W_2 = X(W_1W_2) = XW_{combined}$$\n",
    "\n",
    "Activations add **non-linearity**, allowing the network to learn complex patterns.\n",
    "\n",
    "### ReLU: The Most Popular Activation\n",
    "\n",
    "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
    "\n",
    "**Derivative:**\n",
    "$$\\frac{d}{dx}\\text{ReLU}(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{if } x \\leq 0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit activation.\n",
    "    \n",
    "    ELI5: ReLU is like a one-way valve. Positive values flow through\n",
    "    unchanged, but negative values get blocked (become zero).\n",
    "    This creates \"sparsity\" - many neurons output zero, which is efficient!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass: A = max(0, Z)\n",
    "        \"\"\"\n",
    "        self.cache['Z'] = Z\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def backward(self, dA: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass: gradient is 1 where Z > 0, else 0.\n",
    "        \"\"\"\n",
    "        Z = self.cache['Z']\n",
    "        # Gradient flows through where Z was positive\n",
    "        dZ = dA * (Z > 0).astype(float)\n",
    "        return dZ\n",
    "    \n",
    "    def __call__(self, Z: np.ndarray) -> np.ndarray:\n",
    "        return self.forward(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ReLU\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# ReLU function\n",
    "x = np.linspace(-3, 3, 100)\n",
    "relu = ReLU()\n",
    "y = relu(x)\n",
    "\n",
    "axes[0].plot(x, y, 'b-', linewidth=2)\n",
    "axes[0].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[0].axvline(x=0, color='k', linewidth=0.5)\n",
    "axes[0].set_title('ReLU(x) = max(0, x)', fontsize=12)\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('ReLU(x)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# ReLU gradient\n",
    "grad = np.ones_like(x)\n",
    "dZ = relu.backward(grad)\n",
    "\n",
    "axes[1].plot(x, dZ, 'r-', linewidth=2)\n",
    "axes[1].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[1].axvline(x=0, color='k', linewidth=0.5)\n",
    "axes[1].set_title(\"ReLU'(x) = 1 if x > 0, else 0\", fontsize=12)\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel(\"Gradient\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Key insight: ReLU passes gradient through only for positive inputs!\")\n",
    "print(\"   This is why 'dead ReLU' can happen - neurons that always output 0 never learn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Implementing Softmax + Cross-Entropy Loss\n",
    "\n",
    "For classification, we need:\n",
    "1. **Softmax**: Convert raw scores (logits) to probabilities\n",
    "2. **Cross-Entropy**: Measure how different our predictions are from true labels\n",
    "\n",
    "### Softmax\n",
    "$$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$$\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "$$L = -\\sum_i y_i \\log(\\hat{y}_i)$$\n",
    "\n",
    "Where $y$ is one-hot encoded true labels and $\\hat{y}$ is softmax output.\n",
    "\n",
    "### The Beautiful Gradient\n",
    "When combined, the gradient is simply:\n",
    "$$\\frac{\\partial L}{\\partial z} = \\hat{y} - y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"\n",
    "    Softmax activation for classification.\n",
    "    \n",
    "    ELI5: Softmax is like converting exam scores to class rankings.\n",
    "    It makes sure all probabilities sum to 1 (100%), with higher\n",
    "    scores getting higher probabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute softmax probabilities.\n",
    "        \n",
    "        Note: We subtract max for numerical stability (prevents overflow).\n",
    "        \"\"\"\n",
    "        # Subtract max for numerical stability\n",
    "        Z_shifted = Z - np.max(Z, axis=1, keepdims=True)\n",
    "        exp_Z = np.exp(Z_shifted)\n",
    "        probs = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "        self.cache['probs'] = probs\n",
    "        return probs\n",
    "    \n",
    "    def __call__(self, Z: np.ndarray) -> np.ndarray:\n",
    "        return self.forward(Z)\n",
    "\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    \"\"\"\n",
    "    Cross-entropy loss for classification.\n",
    "    \n",
    "    ELI5: Cross-entropy measures how \"surprised\" we are by the predictions.\n",
    "    - Confident correct prediction: Low surprise, low loss\n",
    "    - Confident WRONG prediction: High surprise, high loss\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, epsilon: float = 1e-10):\n",
    "        self.epsilon = epsilon  # Prevent log(0)\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, probs: np.ndarray, targets: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute cross-entropy loss.\n",
    "        \n",
    "        Args:\n",
    "            probs: Softmax probabilities, shape (batch_size, num_classes)\n",
    "            targets: True labels as indices, shape (batch_size,)\n",
    "            \n",
    "        Returns:\n",
    "            Scalar loss value\n",
    "        \"\"\"\n",
    "        batch_size = probs.shape[0]\n",
    "        \n",
    "        # Clip probabilities for numerical stability\n",
    "        probs_clipped = np.clip(probs, self.epsilon, 1 - self.epsilon)\n",
    "        \n",
    "        # Get probability of correct class for each sample\n",
    "        correct_probs = probs_clipped[np.arange(batch_size), targets]\n",
    "        \n",
    "        # Negative log likelihood\n",
    "        loss = -np.mean(np.log(correct_probs))\n",
    "        \n",
    "        # Save for backward\n",
    "        self.cache['probs'] = probs\n",
    "        self.cache['targets'] = targets\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute gradient of loss with respect to logits (pre-softmax).\n",
    "        \n",
    "        The beautiful result: gradient = softmax_output - one_hot_targets\n",
    "        \"\"\"\n",
    "        probs = self.cache['probs']\n",
    "        targets = self.cache['targets']\n",
    "        batch_size = probs.shape[0]\n",
    "        \n",
    "        # Start with softmax output\n",
    "        grad = probs.copy()\n",
    "        \n",
    "        # Subtract 1 from the correct class probability\n",
    "        grad[np.arange(batch_size), targets] -= 1\n",
    "        \n",
    "        return grad\n",
    "    \n",
    "    def __call__(self, probs: np.ndarray, targets: np.ndarray) -> float:\n",
    "        return self.forward(probs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Softmax + CrossEntropy\n",
    "print(\"üß™ Testing Softmax + Cross-Entropy\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Logits for 3 samples, 4 classes\n",
    "logits = np.array([\n",
    "    [2.0, 1.0, 0.1, 0.5],   # Confident about class 0\n",
    "    [0.1, 0.2, 3.0, 0.1],   # Confident about class 2\n",
    "    [1.0, 1.0, 1.0, 1.0]    # Uncertain (uniform)\n",
    "])\n",
    "targets = np.array([0, 2, 1])  # True classes\n",
    "\n",
    "print(\"Logits:\")\n",
    "print(logits)\n",
    "print(f\"\\nTrue targets: {targets}\")\n",
    "\n",
    "# Forward pass\n",
    "softmax = Softmax()\n",
    "probs = softmax(logits)\n",
    "print(f\"\\nSoftmax probabilities:\")\n",
    "print(probs)\n",
    "print(f\"Sum per sample: {probs.sum(axis=1)}\")\n",
    "\n",
    "# Loss\n",
    "loss_fn = CrossEntropyLoss()\n",
    "loss = loss_fn(probs, targets)\n",
    "print(f\"\\nCross-entropy loss: {loss:.4f}\")\n",
    "\n",
    "# Gradient\n",
    "grad = loss_fn.backward()\n",
    "print(f\"\\nGradient (probs - one_hot):\")\n",
    "print(grad)\n",
    "\n",
    "print(\"\\nüí° Notice: The gradient is small for correct predictions, large for wrong ones!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Building the Complete Network\n",
    "\n",
    "Now let's put it all together into a Multi-Layer Perceptron (MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron for classification.\n",
    "    \n",
    "    Architecture: Input -> Linear -> ReLU -> Linear -> ReLU -> Linear -> Softmax\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes: List[int]):\n",
    "        \"\"\"\n",
    "        Initialize the MLP.\n",
    "        \n",
    "        Args:\n",
    "            layer_sizes: List of layer sizes, e.g., [784, 256, 128, 10]\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "        \n",
    "        # Create layers\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "            # Add ReLU for all layers except the last\n",
    "            if i < len(layer_sizes) - 2:\n",
    "                self.activations.append(ReLU())\n",
    "        \n",
    "        # Softmax for output\n",
    "        self.softmax = Softmax()\n",
    "        \n",
    "        print(f\"Created MLP: {' -> '.join(map(str, layer_sizes))}\")\n",
    "        print(f\"Total parameters: {self.count_parameters():,}\")\n",
    "    \n",
    "    def count_parameters(self) -> int:\n",
    "        \"\"\"Count total trainable parameters.\"\"\"\n",
    "        total = 0\n",
    "        for layer in self.layers:\n",
    "            total += layer.W.size + layer.b.size\n",
    "        return total\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \"\"\"\n",
    "        # First n-1 layers: Linear + ReLU\n",
    "        out = X\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            out = self.layers[i](out)\n",
    "            out = self.activations[i](out)\n",
    "        \n",
    "        # Last layer: Linear + Softmax\n",
    "        out = self.layers[-1](out)\n",
    "        out = self.softmax(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, grad: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Backward pass through the network.\n",
    "        \"\"\"\n",
    "        # Last layer (no ReLU backward needed, softmax+CE gradient already computed)\n",
    "        grad = self.layers[-1].backward(grad)\n",
    "        \n",
    "        # Hidden layers in reverse order\n",
    "        for i in range(len(self.layers) - 2, -1, -1):\n",
    "            grad = self.activations[i].backward(grad)\n",
    "            grad = self.layers[i].backward(grad)\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Get class predictions (argmax of probabilities).\"\"\"\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "    \n",
    "    def __call__(self, X: np.ndarray) -> np.ndarray:\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the MLP\n",
    "print(\"üß™ Testing MLP\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create network: 784 -> 256 -> 128 -> 10\n",
    "model = MLP([784, 256, 128, 10])\n",
    "\n",
    "# Test forward pass\n",
    "X_test = np.random.randn(32, 784)\n",
    "output = model(X_test)\n",
    "print(f\"\\nInput shape: {X_test.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output sums to 1: {np.allclose(output.sum(axis=1), 1)}\")\n",
    "\n",
    "# Test backward pass\n",
    "targets = np.random.randint(0, 10, 32)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "loss = loss_fn(output, targets)\n",
    "grad = loss_fn.backward()\n",
    "model.backward(grad)\n",
    "\n",
    "print(f\"\\nLoss: {loss:.4f}\")\n",
    "print(f\"Gradients computed for all layers: {all(l.dW is not None for l in model.layers)}\")\n",
    "\n",
    "print(\"\\n‚úÖ MLP works correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Implementing SGD Optimizer\n",
    "\n",
    "The optimizer updates weights based on gradients:\n",
    "$$W = W - \\eta \\cdot \\frac{\\partial L}{\\partial W}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent optimizer.\n",
    "    \n",
    "    ELI5: SGD is like taking small steps downhill while blindfolded.\n",
    "    You feel which way is down (gradient) and take a small step.\n",
    "    The learning rate controls how big your steps are.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 0.01):\n",
    "        self.lr = learning_rate\n",
    "    \n",
    "    def step(self, model: MLP) -> None:\n",
    "        \"\"\"\n",
    "        Update all weights in the model.\n",
    "        \"\"\"\n",
    "        for layer in model.layers:\n",
    "            layer.W -= self.lr * layer.dW\n",
    "            layer.b -= self.lr * layer.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Loading MNIST Dataset\n",
    "\n",
    "MNIST is the \"Hello World\" of deep learning - handwritten digits 0-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path: str = '../data') -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load MNIST dataset.\n",
    "    \n",
    "    Returns:\n",
    "        X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    import gzip\n",
    "    import urllib.request\n",
    "    \n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    base_url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = {\n",
    "        'train_images': 'train-images-idx3-ubyte.gz',\n",
    "        'train_labels': 'train-labels-idx1-ubyte.gz',\n",
    "        'test_images': 't10k-images-idx3-ubyte.gz',\n",
    "        'test_labels': 't10k-labels-idx1-ubyte.gz'\n",
    "    }\n",
    "    \n",
    "    def download_if_needed(filename: str) -> str:\n",
    "        filepath = os.path.join(path, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            urllib.request.urlretrieve(base_url + filename, filepath)\n",
    "        return filepath\n",
    "    \n",
    "    def load_images(filepath: str) -> np.ndarray:\n",
    "        with gzip.open(filepath, 'rb') as f:\n",
    "            f.read(16)  # Skip header\n",
    "            data = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "            return data.reshape(-1, 784).astype(np.float32) / 255.0\n",
    "    \n",
    "    def load_labels(filepath: str) -> np.ndarray:\n",
    "        with gzip.open(filepath, 'rb') as f:\n",
    "            f.read(8)  # Skip header\n",
    "            return np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    \n",
    "    X_train = load_images(download_if_needed(files['train_images']))\n",
    "    y_train = load_labels(download_if_needed(files['train_labels']))\n",
    "    X_test = load_images(download_if_needed(files['test_images']))\n",
    "    y_test = load_labels(download_if_needed(files['test_labels']))\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "print(\"üìÇ Loading MNIST dataset...\")\n",
    "X_train, y_train, X_test, y_test = load_mnist()\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"Input dimensions: {X_train.shape[1]} (28x28 flattened)\")\n",
    "print(f\"Classes: {np.unique(y_train)}\")\n",
    "\n",
    "# Visualize some samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_train[i].reshape(28, 28), cmap='gray')\n",
    "    ax.set_title(f'Label: {y_train[i]}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Sample MNIST Digits', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Training Loop\n",
    "\n",
    "Now we'll put everything together and train our network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(X: np.ndarray, y: np.ndarray, batch_size: int, shuffle: bool = True):\n",
    "    \"\"\"\n",
    "    Create mini-batches from data.\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.arange(n_samples)\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        batch_idx = indices[start:end]\n",
    "        yield X[batch_idx], y[batch_idx]\n",
    "\n",
    "\n",
    "def compute_accuracy(model: MLP, X: np.ndarray, y: np.ndarray, batch_size: int = 256) -> float:\n",
    "    \"\"\"\n",
    "    Compute accuracy on a dataset.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X_batch, y_batch in create_batches(X, y, batch_size, shuffle=False):\n",
    "        predictions = model.predict(X_batch)\n",
    "        correct += np.sum(predictions == y_batch)\n",
    "        total += len(y_batch)\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: MLP,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    epochs: int = 10,\n",
    "    batch_size: int = 64,\n",
    "    learning_rate: float = 0.01\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Train the model and return training history.\n",
    "    \"\"\"\n",
    "    optimizer = SGD(learning_rate)\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_acc': [],\n",
    "        'epoch_time': []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Configuration:\")\n",
    "    print(f\"  - Epochs: {epochs}\")\n",
    "    print(f\"  - Batch size: {batch_size}\")\n",
    "    print(f\"  - Learning rate: {learning_rate}\")\n",
    "    print(f\"  - Training samples: {X_train.shape[0]:,}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        # Training\n",
    "        for X_batch, y_batch in create_batches(X_train, y_train, batch_size):\n",
    "            # Forward pass\n",
    "            output = model(X_batch)\n",
    "            loss = loss_fn(output, y_batch)\n",
    "            epoch_loss += loss\n",
    "            n_batches += 1\n",
    "            \n",
    "            # Backward pass\n",
    "            grad = loss_fn.backward()\n",
    "            model.backward(grad)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step(model)\n",
    "        \n",
    "        # Compute metrics\n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        train_acc = compute_accuracy(model, X_train, y_train)\n",
    "        test_acc = compute_accuracy(model, X_test, y_test)\n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(avg_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        history['epoch_time'].append(epoch_time)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch + 1:2d}/{epochs} | \"\n",
    "              f\"Loss: {avg_loss:.4f} | \"\n",
    "              f\"Train Acc: {train_acc:.2%} | \"\n",
    "              f\"Test Acc: {test_acc:.2%} | \"\n",
    "              f\"Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üéâ Training complete!\")\n",
    "    print(f\"   Final Test Accuracy: {history['test_acc'][-1]:.2%}\")\n",
    "    print(f\"   Total Training Time: {sum(history['epoch_time']):.2f}s\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model!\n",
    "print(\"üöÄ Building and Training Neural Network from Scratch!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create model: 784 -> 256 -> 128 -> 10\n",
    "model = MLP([784, 256, 128, 10])\n",
    "\n",
    "# Train!\n",
    "history = train_model(\n",
    "    model, \n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.1  # Higher LR for faster convergence\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Visualizing Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(history['train_loss'], 'b-', linewidth=2, label='Training Loss')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training Loss Over Time', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "# Accuracy curves\n",
    "axes[1].plot(history['train_acc'], 'b-', linewidth=2, label='Train Accuracy')\n",
    "axes[1].plot(history['test_acc'], 'r--', linewidth=2, label='Test Accuracy')\n",
    "axes[1].axhline(y=0.95, color='g', linestyle=':', linewidth=2, label='95% Target')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Accuracy Over Time', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final report\n",
    "target_achieved = history['test_acc'][-1] >= 0.95\n",
    "if target_achieved:\n",
    "    print(\"\\nüéâ SUCCESS! You achieved >95% accuracy on MNIST!\")\n",
    "else:\n",
    "    print(f\"\\nüìà Current accuracy: {history['test_acc'][-1]:.2%}\")\n",
    "    print(\"   Try: more epochs, higher learning rate, or larger hidden layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
    "\n",
    "# Get random test samples\n",
    "indices = np.random.choice(len(X_test), 10, replace=False)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    idx = indices[i]\n",
    "    image = X_test[idx].reshape(28, 28)\n",
    "    true_label = y_test[idx]\n",
    "    \n",
    "    # Get prediction\n",
    "    pred_probs = model(X_test[idx:idx+1])[0]\n",
    "    pred_label = np.argmax(pred_probs)\n",
    "    confidence = pred_probs[pred_label]\n",
    "    \n",
    "    # Display\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    color = 'green' if pred_label == true_label else 'red'\n",
    "    ax.set_title(f'Pred: {pred_label} ({confidence:.0%})\\nTrue: {true_label}', \n",
    "                 color=color, fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Model Predictions on Test Set', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting to normalize input data\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - raw pixel values (0-255)\n",
    "X_train = load_images(...)  # Values 0-255\n",
    "\n",
    "# ‚úÖ Right - normalized to [0, 1]\n",
    "X_train = load_images(...) / 255.0\n",
    "```\n",
    "**Why:** Large input values cause large activations and exploding gradients.\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 2: Wrong gradient shape\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - forgetting to average over batch\n",
    "self.dW = X.T @ dZ  # Shape correct, but values too large\n",
    "\n",
    "# ‚úÖ Right - average over batch\n",
    "self.dW = X.T @ dZ / batch_size\n",
    "```\n",
    "**Why:** Without averaging, larger batch sizes would have larger gradients.\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 3: Numerical instability in softmax\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - can cause overflow\n",
    "exp_Z = np.exp(Z)\n",
    "probs = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "\n",
    "# ‚úÖ Right - subtract max for stability\n",
    "Z_shifted = Z - np.max(Z, axis=1, keepdims=True)\n",
    "exp_Z = np.exp(Z_shifted)\n",
    "probs = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "```\n",
    "**Why:** exp(1000) = overflow, but exp(1000-1000) = exp(0) = 1.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Experiment with Architecture\n",
    "\n",
    "Try different network architectures and see how they affect accuracy:\n",
    "- Deeper: `[784, 512, 256, 128, 64, 10]`\n",
    "- Wider: `[784, 512, 512, 10]`\n",
    "- Simpler: `[784, 128, 10]`\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Just change the layer_sizes list when creating the MLP.\n",
    "Deeper networks may need lower learning rates!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Try a different architecture\n",
    "# model_v2 = MLP([784, ???, 10])\n",
    "# history_v2 = train_model(model_v2, X_train, y_train, X_test, y_test, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Learning Rate Exploration\n",
    "\n",
    "Try learning rates: 0.001, 0.01, 0.1, 1.0\n",
    "\n",
    "Which one works best? Which ones fail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Compare learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've accomplished something remarkable! You:\n",
    "\n",
    "- ‚úÖ Built a neural network from scratch using only NumPy\n",
    "- ‚úÖ Implemented forward and backward passes\n",
    "- ‚úÖ Created Linear, ReLU, Softmax, and CrossEntropy components\n",
    "- ‚úÖ Trained on real data and achieved >95% accuracy\n",
    "- ‚úÖ Visualized training progress and predictions\n",
    "\n",
    "**Key Insights:**\n",
    "1. Neural networks are just matrix multiplication + non-linearity\n",
    "2. Backpropagation traces gradients through the chain rule\n",
    "3. Proper initialization and normalization are crucial\n",
    "4. The softmax + cross-entropy gradient is beautifully simple\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Advanced Challenge:** Implement momentum in the SGD optimizer.\n",
    "\n",
    "Momentum helps escape local minima and speeds up training:\n",
    "$$v = \\beta \\cdot v - \\eta \\cdot \\nabla L$$\n",
    "$$W = W + v$$\n",
    "\n",
    "Where $\\beta$ is typically 0.9.\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [Neural Networks and Deep Learning (Nielsen)](http://neuralnetworksanddeeplearning.com/) - Free online book\n",
    "- [CS231n: Backpropagation](https://cs231n.github.io/optimization-2/) - Stanford course notes\n",
    "- [3Blue1Brown: Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) - Excellent visualizations\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "import gc\n",
    "\n",
    "# Clear large variables if needed\n",
    "# del X_train, y_train, X_test, y_test, model\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")\n",
    "print(\"\\nüéØ Next: Proceed to notebook 02-activation-function-study.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
