{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4.4: Normalization Comparison\n",
    "\n",
    "**Module:** 4 - Neural Network Fundamentals  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** â­â­â­ (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Implement BatchNorm from scratch (forward + backward)\n",
    "- [ ] Implement LayerNorm from scratch\n",
    "- [ ] Understand RMSNorm (used in modern LLMs)\n",
    "- [ ] Compare training stability and convergence\n",
    "- [ ] Know when to use each normalization technique\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Prerequisites\n",
    "\n",
    "- Completed: Notebooks 01-03\n",
    "- Knowledge of: Mean, variance, neural network training\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ Real-World Context\n",
    "\n",
    "**Normalization is essential in modern deep learning:**\n",
    "\n",
    "- **Batch Normalization (2015)**: Revolutionized CNN training, enabled 10x deeper networks\n",
    "- **Layer Normalization (2016)**: Made transformers possible by removing batch dependency\n",
    "- **RMSNorm (2019)**: Powers LLaMA, Mistral, and modern LLMs - faster than LayerNorm!\n",
    "\n",
    "Without normalization, training deep networks would be nearly impossible.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§’ ELI5: What is Normalization?\n",
    "\n",
    "> **Imagine you're comparing test scores from different countries.**\n",
    ">\n",
    "> Country A scores: 80-100 (high average, narrow range)  \n",
    "> Country B scores: 20-80 (lower average, wide range)  \n",
    ">\n",
    "> How do you compare fairly? **You normalize!**\n",
    "> 1. Subtract the mean (center at zero)\n",
    "> 2. Divide by standard deviation (make scales comparable)\n",
    ">\n",
    "> Now all scores are on the same scale, centered around 0 with similar spread.\n",
    ">\n",
    "> **In neural networks**, activations can have wildly different scales across layers. Normalization keeps everything \"fair\" so learning happens smoothly.\n",
    ">\n",
    "> The difference between BatchNorm and LayerNorm is just **which scores we compare**:\n",
    "> - **BatchNorm**: Compare students across schools (normalize across batch)\n",
    "> - **LayerNorm**: Compare each student across subjects (normalize across features)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(os.getcwd()), 'scripts'))\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('default')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Implementing Batch Normalization\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "For a batch of activations, BatchNorm computes:\n",
    "\n",
    "1. **Mean** across batch: $\\mu = \\frac{1}{m} \\sum_i x_i$\n",
    "2. **Variance** across batch: $\\sigma^2 = \\frac{1}{m} \\sum_i (x_i - \\mu)^2$\n",
    "3. **Normalize**: $\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$\n",
    "4. **Scale and shift**: $y_i = \\gamma \\hat{x}_i + \\beta$\n",
    "\n",
    "Where $\\gamma$ and $\\beta$ are learnable parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm:\n",
    "    \"\"\"\n",
    "    Batch Normalization layer.\n",
    "    \n",
    "    Normalizes across the batch dimension.\n",
    "    \n",
    "    Key insight: During training, use batch statistics.\n",
    "                 During inference, use running (moving average) statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_features: int, momentum: float = 0.1, epsilon: float = 1e-5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_features: Number of features (neurons in previous layer)\n",
    "            momentum: For running statistics update (0 = no update, 1 = replace)\n",
    "            epsilon: For numerical stability\n",
    "        \"\"\"\n",
    "        self.num_features = num_features\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Learnable parameters\n",
    "        self.gamma = np.ones(num_features)   # Scale\n",
    "        self.beta = np.zeros(num_features)   # Shift\n",
    "        \n",
    "        # Running statistics for inference\n",
    "        self.running_mean = np.zeros(num_features)\n",
    "        self.running_var = np.ones(num_features)\n",
    "        \n",
    "        # Gradients\n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "        \n",
    "        # Cache\n",
    "        self.cache = {}\n",
    "        self.training = True\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input of shape (batch_size, num_features)\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            # Compute batch statistics\n",
    "            batch_mean = np.mean(x, axis=0)\n",
    "            batch_var = np.var(x, axis=0)\n",
    "            \n",
    "            # Normalize\n",
    "            x_centered = x - batch_mean\n",
    "            std = np.sqrt(batch_var + self.epsilon)\n",
    "            x_norm = x_centered / std\n",
    "            \n",
    "            # Update running statistics\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
    "            \n",
    "            # Cache for backward\n",
    "            self.cache['x'] = x\n",
    "            self.cache['x_norm'] = x_norm\n",
    "            self.cache['mean'] = batch_mean\n",
    "            self.cache['var'] = batch_var\n",
    "            self.cache['std'] = std\n",
    "            self.cache['x_centered'] = x_centered\n",
    "        else:\n",
    "            # Use running statistics during inference\n",
    "            x_norm = (x - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "        \n",
    "        # Scale and shift\n",
    "        out = self.gamma * x_norm + self.beta\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass.\n",
    "        \n",
    "        This is the tricky part! The gradient flows through the normalization,\n",
    "        which depends on batch statistics.\n",
    "        \"\"\"\n",
    "        x = self.cache['x']\n",
    "        x_norm = self.cache['x_norm']\n",
    "        x_centered = self.cache['x_centered']\n",
    "        std = self.cache['std']\n",
    "        var = self.cache['var']\n",
    "        m = x.shape[0]  # batch size\n",
    "        \n",
    "        # Gradients of learnable parameters\n",
    "        self.dgamma = np.sum(dout * x_norm, axis=0)\n",
    "        self.dbeta = np.sum(dout, axis=0)\n",
    "        \n",
    "        # Gradient through scale\n",
    "        dx_norm = dout * self.gamma\n",
    "        \n",
    "        # Gradient through normalization (this is the complex part)\n",
    "        # See: https://kevinzakka.github.io/2016/09/14/batch_normalization/\n",
    "        dvar = np.sum(dx_norm * x_centered * -0.5 * (var + self.epsilon) ** (-1.5), axis=0)\n",
    "        dmean = np.sum(dx_norm * -1 / std, axis=0) + dvar * np.mean(-2 * x_centered, axis=0)\n",
    "        \n",
    "        dx = dx_norm / std + dvar * 2 * x_centered / m + dmean / m\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test BatchNorm\n",
    "print(\"ğŸ§ª Testing Batch Normalization\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create input with non-zero mean and non-unit variance\n",
    "x = np.random.randn(32, 256) * 5 + 3  # mean~3, std~5\n",
    "print(f\"Input stats:\")\n",
    "print(f\"  Mean: {x.mean():.2f} (should be ~3)\")\n",
    "print(f\"  Std:  {x.std():.2f} (should be ~5)\")\n",
    "\n",
    "# Apply BatchNorm\n",
    "bn = BatchNorm(256)\n",
    "out = bn(x)\n",
    "\n",
    "print(f\"\\nOutput stats (per feature, averaged across batch):\")\n",
    "print(f\"  Mean: {out.mean(axis=0).mean():.6f} (should be ~0)\")\n",
    "print(f\"  Std:  {out.std(axis=0).mean():.2f} (should be ~1)\")\n",
    "\n",
    "# Test backward\n",
    "dout = np.random.randn(32, 256)\n",
    "dx = bn.backward(dout)\n",
    "print(f\"\\nBackward pass: dx shape = {dx.shape}\")\n",
    "print(f\"Gamma gradient shape: {bn.dgamma.shape}\")\n",
    "\n",
    "print(\"\\nâœ… BatchNorm test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Implementing Layer Normalization\n",
    "\n",
    "### Key Difference from BatchNorm\n",
    "\n",
    "- **BatchNorm**: Normalize across batch dimension (features are normalized together across samples)\n",
    "- **LayerNorm**: Normalize across feature dimension (each sample is normalized independently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    \"\"\"\n",
    "    Layer Normalization.\n",
    "    \n",
    "    Normalizes across the feature dimension (not batch).\n",
    "    \n",
    "    Key advantages over BatchNorm:\n",
    "    - Works with batch_size=1\n",
    "    - Same behavior in training and inference\n",
    "    - Perfect for transformers and RNNs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, normalized_shape: int, epsilon: float = 1e-5):\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Learnable parameters\n",
    "        self.gamma = np.ones(normalized_shape)\n",
    "        self.beta = np.zeros(normalized_shape)\n",
    "        \n",
    "        # Gradients\n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "        \n",
    "        # Cache\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Normalize each sample independently across features.\n",
    "        \"\"\"\n",
    "        # Compute mean and variance across features (axis=-1)\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        var = np.var(x, axis=-1, keepdims=True)\n",
    "        \n",
    "        # Normalize\n",
    "        std = np.sqrt(var + self.epsilon)\n",
    "        x_norm = (x - mean) / std\n",
    "        \n",
    "        # Cache for backward\n",
    "        self.cache['x'] = x\n",
    "        self.cache['x_norm'] = x_norm\n",
    "        self.cache['mean'] = mean\n",
    "        self.cache['var'] = var\n",
    "        self.cache['std'] = std\n",
    "        \n",
    "        # Scale and shift\n",
    "        out = self.gamma * x_norm + self.beta\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass.\n",
    "        \"\"\"\n",
    "        x = self.cache['x']\n",
    "        x_norm = self.cache['x_norm']\n",
    "        mean = self.cache['mean']\n",
    "        var = self.cache['var']\n",
    "        std = self.cache['std']\n",
    "        n = x.shape[-1]  # Number of features\n",
    "        \n",
    "        # Gradients of learnable parameters\n",
    "        self.dgamma = np.sum(dout * x_norm, axis=0)\n",
    "        self.dbeta = np.sum(dout, axis=0)\n",
    "        \n",
    "        # Gradient through scale\n",
    "        dx_norm = dout * self.gamma\n",
    "        \n",
    "        # Gradient through normalization\n",
    "        x_centered = x - mean\n",
    "        dvar = np.sum(dx_norm * x_centered * -0.5 * (var + self.epsilon) ** (-1.5), axis=-1, keepdims=True)\n",
    "        dmean = np.sum(dx_norm * -1 / std, axis=-1, keepdims=True) + dvar * np.mean(-2 * x_centered, axis=-1, keepdims=True)\n",
    "        \n",
    "        dx = dx_norm / std + dvar * 2 * x_centered / n + dmean / n\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LayerNorm\n",
    "print(\"ğŸ§ª Testing Layer Normalization\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "x = np.random.randn(32, 256) * 5 + 3\n",
    "\n",
    "ln = LayerNorm(256)\n",
    "out = ln(x)\n",
    "\n",
    "print(f\"Output stats (per sample, should be normalized):\")\n",
    "print(f\"  Mean per sample (first 5): {out.mean(axis=-1)[:5]}\")\n",
    "print(f\"  Std per sample (first 5):  {out.std(axis=-1)[:5]}\")\n",
    "\n",
    "# Test backward\n",
    "dout = np.random.randn(32, 256)\n",
    "dx = ln.backward(dout)\n",
    "print(f\"\\nBackward pass: dx shape = {dx.shape}\")\n",
    "\n",
    "print(\"\\nâœ… LayerNorm test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Understanding RMSNorm\n",
    "\n",
    "### The Simplification\n",
    "\n",
    "RMSNorm removes the mean-centering step:\n",
    "\n",
    "$$\\text{RMSNorm}(x) = \\frac{x}{\\text{RMS}(x)} \\cdot \\gamma$$\n",
    "\n",
    "Where $\\text{RMS}(x) = \\sqrt{\\frac{1}{n}\\sum_i x_i^2}$\n",
    "\n",
    "**Why it works:** For transformers, the mean-centering provides marginal benefit but adds computation. LLaMA showed RMSNorm works just as well and is faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm:\n",
    "    \"\"\"\n",
    "    Root Mean Square Layer Normalization.\n",
    "    \n",
    "    Used in LLaMA, Mistral, and other modern LLMs.\n",
    "    Faster than LayerNorm (no mean computation), similar performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, normalized_shape: int, epsilon: float = 1e-5):\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Only gamma (no beta in RMSNorm)\n",
    "        self.gamma = np.ones(normalized_shape)\n",
    "        \n",
    "        # Gradients\n",
    "        self.dgamma = None\n",
    "        \n",
    "        # Cache\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \"\"\"\n",
    "        # Compute RMS\n",
    "        rms = np.sqrt(np.mean(x ** 2, axis=-1, keepdims=True) + self.epsilon)\n",
    "        \n",
    "        # Normalize\n",
    "        x_norm = x / rms\n",
    "        \n",
    "        # Cache\n",
    "        self.cache['x'] = x\n",
    "        self.cache['rms'] = rms\n",
    "        self.cache['x_norm'] = x_norm\n",
    "        \n",
    "        # Scale (no shift in RMSNorm)\n",
    "        return self.gamma * x_norm\n",
    "    \n",
    "    def backward(self, dout: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass.\n",
    "        \"\"\"\n",
    "        x = self.cache['x']\n",
    "        rms = self.cache['rms']\n",
    "        x_norm = self.cache['x_norm']\n",
    "        n = x.shape[-1]\n",
    "        \n",
    "        # Gradient of gamma\n",
    "        self.dgamma = np.sum(dout * x_norm, axis=0)\n",
    "        \n",
    "        # Gradient through scale\n",
    "        dx_norm = dout * self.gamma\n",
    "        \n",
    "        # Gradient through RMS normalization\n",
    "        drms = np.sum(dx_norm * x * -1 / (rms ** 2), axis=-1, keepdims=True)\n",
    "        dx_sq = drms * 0.5 / rms / n\n",
    "        \n",
    "        dx = dx_norm / rms + 2 * dx_sq * x\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RMSNorm\n",
    "print(\"ğŸ§ª Testing RMSNorm\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "x = np.random.randn(32, 256) * 5 + 3\n",
    "\n",
    "rms = RMSNorm(256)\n",
    "out = rms(x)\n",
    "\n",
    "# Check RMS of output\n",
    "rms_values = np.sqrt(np.mean(out ** 2, axis=-1))\n",
    "print(f\"RMS per sample (should be ~1): {rms_values[:5]}\")\n",
    "print(f\"Note: Mean is NOT centered (that's the point of RMSNorm)\")\n",
    "print(f\"Mean per sample: {out.mean(axis=-1)[:5]}\")\n",
    "\n",
    "print(\"\\nâœ… RMSNorm test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Visual Comparison\n",
    "\n",
    "Let's visualize what each normalization does to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input with interesting statistics\n",
    "np.random.seed(42)\n",
    "x = np.random.randn(1000, 64) * 3 + 2  # Shifted and scaled\n",
    "\n",
    "# Apply each normalization\n",
    "bn = BatchNorm(64)\n",
    "ln = LayerNorm(64)\n",
    "rmsn = RMSNorm(64)\n",
    "\n",
    "out_bn = bn(x)\n",
    "out_ln = ln(x)\n",
    "out_rms = rmsn(x)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# Row 1: Distribution of a single feature across batch\n",
    "feature_idx = 0\n",
    "\n",
    "axes[0, 0].hist(x[:, feature_idx], bins=30, alpha=0.7, color='gray')\n",
    "axes[0, 0].set_title(f'Original (Feature {feature_idx})')\n",
    "axes[0, 0].set_xlabel('Value')\n",
    "\n",
    "axes[0, 1].hist(out_bn[:, feature_idx], bins=30, alpha=0.7, color='blue')\n",
    "axes[0, 1].set_title(f'After BatchNorm (Feature {feature_idx})')\n",
    "axes[0, 1].axvline(0, color='red', linestyle='--')\n",
    "\n",
    "axes[0, 2].hist(out_ln[:, feature_idx], bins=30, alpha=0.7, color='green')\n",
    "axes[0, 2].set_title(f'After LayerNorm (Feature {feature_idx})')\n",
    "\n",
    "axes[0, 3].hist(out_rms[:, feature_idx], bins=30, alpha=0.7, color='purple')\n",
    "axes[0, 3].set_title(f'After RMSNorm (Feature {feature_idx})')\n",
    "\n",
    "# Row 2: Distribution of a single sample across features\n",
    "sample_idx = 0\n",
    "\n",
    "axes[1, 0].hist(x[sample_idx], bins=30, alpha=0.7, color='gray')\n",
    "axes[1, 0].set_title(f'Original (Sample {sample_idx})')\n",
    "axes[1, 0].set_xlabel('Value')\n",
    "\n",
    "axes[1, 1].hist(out_bn[sample_idx], bins=30, alpha=0.7, color='blue')\n",
    "axes[1, 1].set_title(f'After BatchNorm (Sample {sample_idx})')\n",
    "\n",
    "axes[1, 2].hist(out_ln[sample_idx], bins=30, alpha=0.7, color='green')\n",
    "axes[1, 2].set_title(f'After LayerNorm (Sample {sample_idx})')\n",
    "axes[1, 2].axvline(0, color='red', linestyle='--')\n",
    "\n",
    "axes[1, 3].hist(out_rms[sample_idx], bins=30, alpha=0.7, color='purple')\n",
    "axes[1, 3].set_title(f'After RMSNorm (Sample {sample_idx})')\n",
    "\n",
    "plt.suptitle('Normalization Effects', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Key observations:\")\n",
    "print(\"   Row 1: BatchNorm normalizes across batch (each FEATURE is centered)\")\n",
    "print(\"   Row 2: LayerNorm normalizes across features (each SAMPLE is centered)\")\n",
    "print(\"   RMSNorm: Similar to LayerNorm but without centering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Training Comparison\n",
    "\n",
    "Let's compare training with different normalization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "import gzip\n",
    "import urllib.request\n",
    "\n",
    "def load_mnist(path='../data'):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    base_url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = {\n",
    "        'train_images': 'train-images-idx3-ubyte.gz',\n",
    "        'train_labels': 'train-labels-idx1-ubyte.gz',\n",
    "        'test_images': 't10k-images-idx3-ubyte.gz',\n",
    "        'test_labels': 't10k-labels-idx1-ubyte.gz'\n",
    "    }\n",
    "    \n",
    "    def download(filename):\n",
    "        filepath = os.path.join(path, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            urllib.request.urlretrieve(base_url + filename, filepath)\n",
    "        return filepath\n",
    "    \n",
    "    def load_images(fp):\n",
    "        with gzip.open(fp, 'rb') as f:\n",
    "            f.read(16)\n",
    "            return np.frombuffer(f.read(), dtype=np.uint8).reshape(-1, 784).astype(np.float32) / 255.0\n",
    "    \n",
    "    def load_labels(fp):\n",
    "        with gzip.open(fp, 'rb') as f:\n",
    "            f.read(8)\n",
    "            return np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    \n",
    "    return (load_images(download(files['train_images'])),\n",
    "            load_labels(download(files['train_labels'])),\n",
    "            load_images(download(files['test_images'])),\n",
    "            load_labels(download(files['test_labels'])))\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_mnist()\n",
    "X_train, y_train = X_train[:5000], y_train[:5000]  # Subset for speed\n",
    "print(f\"Training samples: {len(X_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedMLP:\n",
    "    \"\"\"\n",
    "    MLP with configurable normalization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes: List[int], norm_type: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            layer_sizes: e.g., [784, 256, 128, 10]\n",
    "            norm_type: 'batch', 'layer', 'rms', or None\n",
    "        \"\"\"\n",
    "        self.norm_type = norm_type\n",
    "        self.training = True\n",
    "        \n",
    "        self.layers = []\n",
    "        self.norms = []\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            W = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2.0 / layer_sizes[i])\n",
    "            b = np.zeros(layer_sizes[i + 1])\n",
    "            self.layers.append({'W': W, 'b': b, 'cache': {}, 'dW': None, 'db': None})\n",
    "            \n",
    "            # Add normalization for hidden layers\n",
    "            if i < len(layer_sizes) - 2 and norm_type:\n",
    "                if norm_type == 'batch':\n",
    "                    self.norms.append(BatchNorm(layer_sizes[i + 1]))\n",
    "                elif norm_type == 'layer':\n",
    "                    self.norms.append(LayerNorm(layer_sizes[i + 1]))\n",
    "                elif norm_type == 'rms':\n",
    "                    self.norms.append(RMSNorm(layer_sizes[i + 1]))\n",
    "            else:\n",
    "                self.norms.append(None)\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        out = X\n",
    "        \n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            layer['cache']['X'] = out\n",
    "            out = out @ layer['W'] + layer['b']\n",
    "            layer['cache']['Z'] = out\n",
    "            \n",
    "            # Apply normalization if present\n",
    "            if self.norms[i] is not None:\n",
    "                if hasattr(self.norms[i], 'training'):\n",
    "                    self.norms[i].training = self.training\n",
    "                out = self.norms[i](out)\n",
    "            \n",
    "            # ReLU\n",
    "            out = np.maximum(0, out)\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers[-1]['cache']['X'] = out\n",
    "        out = out @ self.layers[-1]['W'] + self.layers[-1]['b']\n",
    "        \n",
    "        # Softmax\n",
    "        out_shifted = out - np.max(out, axis=1, keepdims=True)\n",
    "        exp_out = np.exp(out_shifted)\n",
    "        self.probs = exp_out / np.sum(exp_out, axis=1, keepdims=True)\n",
    "        \n",
    "        return self.probs\n",
    "    \n",
    "    def backward(self, targets: np.ndarray, lr: float):\n",
    "        batch_size = len(targets)\n",
    "        grad = self.probs.copy()\n",
    "        grad[np.arange(batch_size), targets] -= 1\n",
    "        \n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            layer = self.layers[i]\n",
    "            X = layer['cache']['X']\n",
    "            \n",
    "            layer['dW'] = X.T @ grad / batch_size\n",
    "            layer['db'] = np.mean(grad, axis=0)\n",
    "            grad = grad @ layer['W'].T\n",
    "            \n",
    "            if i > 0:\n",
    "                Z = self.layers[i - 1]['cache']['Z']\n",
    "                \n",
    "                # ReLU backward\n",
    "                grad = grad * (Z > 0).astype(float)\n",
    "                \n",
    "                # Normalization backward\n",
    "                if self.norms[i - 1] is not None:\n",
    "                    grad = self.norms[i - 1].backward(grad)\n",
    "            \n",
    "            layer['W'] -= lr * layer['dW']\n",
    "            layer['b'] -= lr * layer['db']\n",
    "            \n",
    "            # Update norm parameters\n",
    "            if i > 0 and self.norms[i - 1] is not None:\n",
    "                norm = self.norms[i - 1]\n",
    "                norm.gamma -= lr * norm.dgamma\n",
    "                if hasattr(norm, 'beta') and norm.dbeta is not None:\n",
    "                    norm.beta -= lr * norm.dbeta\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        self.training = False\n",
    "        probs = self.forward(X)\n",
    "        self.training = True\n",
    "        return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(norm_type, epochs=20, lr=0.1):\n",
    "    np.random.seed(42)\n",
    "    model = NormalizedMLP([784, 256, 128, 10], norm_type=norm_type)\n",
    "    \n",
    "    history = {'loss': [], 'acc': []}\n",
    "    batch_size = 64\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        indices = np.random.permutation(len(X_train))\n",
    "        epoch_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for start in range(0, len(X_train), batch_size):\n",
    "            batch_idx = indices[start:start + batch_size]\n",
    "            X_batch = X_train[batch_idx]\n",
    "            y_batch = y_train[batch_idx]\n",
    "            \n",
    "            probs = model.forward(X_batch)\n",
    "            loss = -np.mean(np.log(probs[np.arange(len(y_batch)), y_batch] + 1e-10))\n",
    "            model.backward(y_batch, lr)\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            n_batches += 1\n",
    "        \n",
    "        acc = np.mean(model.predict(X_test[:1000]) == y_test[:1000])\n",
    "        history['loss'].append(epoch_loss / n_batches)\n",
    "        history['acc'].append(acc)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with each normalization type\n",
    "print(\"ğŸ‹ï¸ Training with different normalization methods...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "norm_types = [None, 'batch', 'layer', 'rms']\n",
    "results = {}\n",
    "\n",
    "for norm_type in norm_types:\n",
    "    name = norm_type if norm_type else 'None'\n",
    "    start = time.time()\n",
    "    history = train_and_evaluate(norm_type, epochs=20)\n",
    "    elapsed = time.time() - start\n",
    "    results[name] = history\n",
    "    print(f\"{name:10s} | Final Acc: {history['acc'][-1]:.2%} | Time: {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = {'None': 'gray', 'batch': 'blue', 'layer': 'green', 'rms': 'purple'}\n",
    "\n",
    "for name, history in results.items():\n",
    "    axes[0].plot(history['loss'], color=colors[name], linewidth=2, label=name)\n",
    "    axes[1].plot(history['acc'], color=colors[name], linewidth=2, label=name)\n",
    "\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Test Accuracy')\n",
    "axes[1].set_title('Test Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"                    NORMALIZATION COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Method       â”‚ Normalizes Over     â”‚ Best For                                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ BatchNorm    â”‚ Batch dimension     â”‚ CNNs, large batches, computer vision    â”‚\n",
    "â”‚              â”‚ (across samples)    â”‚ Different behavior train vs inference   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ LayerNorm    â”‚ Feature dimension   â”‚ Transformers, RNNs, NLP                 â”‚\n",
    "â”‚              â”‚ (within each sample)â”‚ Same behavior train and inference       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ RMSNorm      â”‚ Feature dimension   â”‚ Modern LLMs (LLaMA, Mistral)            â”‚\n",
    "â”‚              â”‚ (no mean centering) â”‚ Faster than LayerNorm, similar quality  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Key Decision Factors:\n",
    "1. Batch Size: Small/variable â†’ LayerNorm or RMSNorm\n",
    "2. Architecture: CNN â†’ BatchNorm, Transformer â†’ LayerNorm/RMSNorm\n",
    "3. Speed Priority: RMSNorm (no mean computation)\n",
    "4. Train/Inference Consistency: LayerNorm or RMSNorm\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš ï¸ Common Mistakes\n",
    "\n",
    "### Mistake 1: Using BatchNorm with batch_size=1\n",
    "\n",
    "```python\n",
    "# âŒ Wrong - BatchNorm needs multiple samples for statistics\n",
    "bn = BatchNorm(256)\n",
    "out = bn(single_sample)  # Only one sample!\n",
    "\n",
    "# âœ… Right - Use LayerNorm for single samples\n",
    "ln = LayerNorm(256)\n",
    "out = ln(single_sample)\n",
    "```\n",
    "\n",
    "### Mistake 2: Forgetting to switch training mode\n",
    "\n",
    "```python\n",
    "# âŒ Wrong - Using batch statistics during inference\n",
    "model.eval()  # But BatchNorm inside still uses batch stats!\n",
    "\n",
    "# âœ… Right - Set all BatchNorm layers to eval mode\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, BatchNorm):\n",
    "        layer.training = False\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "\n",
    "- âœ… Implemented BatchNorm with forward and backward passes\n",
    "- âœ… Implemented LayerNorm from scratch\n",
    "- âœ… Understood RMSNorm and why LLMs use it\n",
    "- âœ… Compared training stability with different normalizations\n",
    "- âœ… Know when to use each technique\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– Further Reading\n",
    "\n",
    "- [Batch Normalization Paper](https://arxiv.org/abs/1502.03167)\n",
    "- [Layer Normalization Paper](https://arxiv.org/abs/1607.06450)\n",
    "- [RMSNorm Paper](https://arxiv.org/abs/1910.07467)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… Cleanup complete!\")\n",
    "print(\"\\nğŸ¯ Next: Proceed to notebook 05-training-diagnostics-lab.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
