{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4.2 Solution: Activation Function Study\n",
    "\n",
    "This notebook contains solutions to the exercises from Task 4.2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Implement PReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PReLU:\n",
    "    \"\"\"\n",
    "    Parametric ReLU - the slope for negative values is learned.\n",
    "    \n",
    "    PReLU(x) = x if x > 0 else alpha * x\n",
    "    \n",
    "    Where alpha is a learnable parameter!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha_init: float = 0.01, lr: float = 0.01):\n",
    "        self.alpha = alpha_init  # Learnable parameter\n",
    "        self.lr = lr  # Learning rate for alpha\n",
    "        self.cache = {}\n",
    "        self.name = f'PReLU(alpha={alpha_init})'\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        self.cache['x'] = x\n",
    "        return np.where(x > 0, x, self.alpha * x)\n",
    "    \n",
    "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass.\n",
    "        \n",
    "        Computes gradients for:\n",
    "        1. Input x (to pass to previous layer)\n",
    "        2. Alpha parameter (to update it)\n",
    "        \"\"\"\n",
    "        x = self.cache['x']\n",
    "        \n",
    "        # Gradient w.r.t. input\n",
    "        grad_input = np.where(x > 0, 1.0, self.alpha) * grad_output\n",
    "        \n",
    "        # Gradient w.r.t. alpha: sum of (grad_output * x) where x < 0\n",
    "        dalpha = np.sum(grad_output * x * (x < 0))\n",
    "        \n",
    "        # Update alpha\n",
    "        self.alpha -= self.lr * dalpha\n",
    "        \n",
    "        return grad_input\n",
    "    \n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        return self.forward(x)\n",
    "\n",
    "# Test PReLU\n",
    "print(\"Testing PReLU:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "prelu = PReLU(alpha_init=0.01, lr=0.001)\n",
    "\n",
    "# Test forward\n",
    "x = np.array([[-2, -1, 0, 1, 2]], dtype=float)\n",
    "out = prelu(x)\n",
    "print(f\"Input:  {x[0]}\")\n",
    "print(f\"Output: {out[0]}\")\n",
    "print(f\"Initial alpha: {prelu.alpha}\")\n",
    "\n",
    "# Test backward (simulate learning)\n",
    "grad = np.ones_like(x)\n",
    "for i in range(100):\n",
    "    prelu.forward(x)\n",
    "    prelu.backward(grad)\n",
    "\n",
    "print(f\"\\nAlpha after 100 updates: {prelu.alpha:.4f}\")\n",
    "print(\"(Alpha should have increased since we want stronger gradients for negative inputs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PReLU with different alphas\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "x = np.linspace(-3, 3, 200)\n",
    "alphas = [0.0, 0.1, 0.25, 0.5]\n",
    "\n",
    "for alpha in alphas:\n",
    "    prelu = PReLU(alpha_init=alpha)\n",
    "    y = prelu(x.copy())\n",
    "    axes[0].plot(x, y, linewidth=2, label=f'alpha={alpha}')\n",
    "\n",
    "axes[0].axhline(0, color='k', linewidth=0.5)\n",
    "axes[0].axvline(0, color='k', linewidth=0.5)\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('PReLU(x)')\n",
    "axes[0].set_title('PReLU with Different Alpha Values')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Compare with ReLU and LeakyReLU\n",
    "relu = np.maximum(0, x)\n",
    "leaky_relu = np.where(x > 0, x, 0.1 * x)\n",
    "prelu_out = PReLU(0.25)(x.copy())\n",
    "\n",
    "axes[1].plot(x, relu, linewidth=2, label='ReLU')\n",
    "axes[1].plot(x, leaky_relu, linewidth=2, label='LeakyReLU (0.1)')\n",
    "axes[1].plot(x, prelu_out, linewidth=2, label='PReLU (learned)')\n",
    "axes[1].axhline(0, color='k', linewidth=0.5)\n",
    "axes[1].axvline(0, color='k', linewidth=0.5)\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('Activation')\n",
    "axes[1].set_title('ReLU vs LeakyReLU vs PReLU')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Deep Sigmoid vs ReLU Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepMLP:\n",
    "    \"\"\"MLP with configurable activation for gradient analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes, activation='relu'):\n",
    "        self.layers = []\n",
    "        self.activation = activation\n",
    "        self.gradient_norms = []\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            W = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2.0 / layer_sizes[i])\n",
    "            b = np.zeros(layer_sizes[i + 1])\n",
    "            self.layers.append({'W': W, 'b': b, 'cache': {}})\n",
    "    \n",
    "    def _activate(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        else:  # sigmoid\n",
    "            return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def _activate_grad(self, x, grad):\n",
    "        if self.activation == 'relu':\n",
    "            return grad * (x > 0)\n",
    "        else:  # sigmoid\n",
    "            s = 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "            return grad * s * (1 - s)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = X\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            layer['cache']['X'] = out\n",
    "            out = out @ layer['W'] + layer['b']\n",
    "            layer['cache']['Z'] = out\n",
    "            out = self._activate(out)\n",
    "        \n",
    "        self.layers[-1]['cache']['X'] = out\n",
    "        out = out @ self.layers[-1]['W'] + self.layers[-1]['b']\n",
    "        out_shifted = out - np.max(out, axis=1, keepdims=True)\n",
    "        exp_out = np.exp(out_shifted)\n",
    "        self.probs = exp_out / np.sum(exp_out, axis=1, keepdims=True)\n",
    "        return self.probs\n",
    "    \n",
    "    def backward(self, targets, lr):\n",
    "        batch_size = len(targets)\n",
    "        grad = self.probs.copy()\n",
    "        grad[np.arange(batch_size), targets] -= 1\n",
    "        \n",
    "        layer_grads = []\n",
    "        \n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            layer = self.layers[i]\n",
    "            X = layer['cache']['X']\n",
    "            \n",
    "            dW = X.T @ grad / batch_size\n",
    "            layer_grads.append(np.linalg.norm(dW))\n",
    "            \n",
    "            grad = grad @ layer['W'].T\n",
    "            \n",
    "            if i > 0:\n",
    "                Z = self.layers[i - 1]['cache']['Z']\n",
    "                grad = self._activate_grad(Z, grad)\n",
    "            \n",
    "            layer['W'] -= lr * dW\n",
    "        \n",
    "        self.gradient_norms.append(layer_grads[::-1])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.forward(X), axis=1)\n",
    "\n",
    "# Load data\n",
    "import gzip\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "def load_mnist(path='../data'):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    base_url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = ['train-images-idx3-ubyte.gz', 'train-labels-idx1-ubyte.gz',\n",
    "             't10k-images-idx3-ubyte.gz', 't10k-labels-idx1-ubyte.gz']\n",
    "    for f in files:\n",
    "        fp = os.path.join(path, f)\n",
    "        if not os.path.exists(fp): urllib.request.urlretrieve(base_url + f, fp)\n",
    "    def load_img(fp): \n",
    "        with gzip.open(fp) as f: f.read(16); return np.frombuffer(f.read(), np.uint8).reshape(-1,784).astype(np.float32)/255\n",
    "    def load_lbl(fp): \n",
    "        with gzip.open(fp) as f: f.read(8); return np.frombuffer(f.read(), np.uint8)\n",
    "    return (load_img(os.path.join(path, files[0])), load_lbl(os.path.join(path, files[1])),\n",
    "            load_img(os.path.join(path, files[2])), load_lbl(os.path.join(path, files[3])))\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare 10-layer networks with Sigmoid vs ReLU\n",
    "print(\"10-Layer Network Comparison: Sigmoid vs ReLU\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Deep architecture (10 layers)\n",
    "arch = [784, 256, 256, 256, 256, 256, 256, 256, 128, 10]\n",
    "\n",
    "# Train Sigmoid network\n",
    "np.random.seed(42)\n",
    "model_sigmoid = DeepMLP(arch, activation='sigmoid')\n",
    "X_subset = X_train[:1000]\n",
    "y_subset = y_train[:1000]\n",
    "\n",
    "for epoch in range(3):\n",
    "    for start in range(0, len(X_subset), 64):\n",
    "        model_sigmoid.forward(X_subset[start:start+64])\n",
    "        model_sigmoid.backward(y_subset[start:start+64], 0.1)\n",
    "\n",
    "acc_sigmoid = np.mean(model_sigmoid.predict(X_test[:1000]) == y_test[:1000])\n",
    "\n",
    "# Train ReLU network\n",
    "np.random.seed(42)\n",
    "model_relu = DeepMLP(arch, activation='relu')\n",
    "\n",
    "for epoch in range(3):\n",
    "    for start in range(0, len(X_subset), 64):\n",
    "        model_relu.forward(X_subset[start:start+64])\n",
    "        model_relu.backward(y_subset[start:start+64], 0.1)\n",
    "\n",
    "acc_relu = np.mean(model_relu.predict(X_test[:1000]) == y_test[:1000])\n",
    "\n",
    "print(f\"Sigmoid Network Accuracy: {acc_sigmoid:.2%}\")\n",
    "print(f\"ReLU Network Accuracy: {acc_relu:.2%}\")\n",
    "\n",
    "# Compare gradient norms\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Get final gradient norms per layer\n",
    "sigmoid_grads = np.mean(model_sigmoid.gradient_norms[-10:], axis=0)\n",
    "relu_grads = np.mean(model_relu.gradient_norms[-10:], axis=0)\n",
    "\n",
    "x = np.arange(len(sigmoid_grads))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, sigmoid_grads, width, label='Sigmoid', color='red', alpha=0.7)\n",
    "axes[0].bar(x + width/2, relu_grads, width, label='ReLU', color='green', alpha=0.7)\n",
    "axes[0].set_xlabel('Layer (0 = closest to input)')\n",
    "axes[0].set_ylabel('Gradient Norm')\n",
    "axes[0].set_title('Gradient Magnitude by Layer')\n",
    "axes[0].legend()\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Show ratio\n",
    "ratio = relu_grads / (sigmoid_grads + 1e-10)\n",
    "axes[1].bar(x, ratio, color='blue', alpha=0.7)\n",
    "axes[1].axhline(1, color='red', linestyle='--', label='Equal')\n",
    "axes[1].set_xlabel('Layer')\n",
    "axes[1].set_ylabel('ReLU / Sigmoid Gradient Ratio')\n",
    "axes[1].set_title('How Much Stronger ReLU Gradients Are')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ’¡ Key Insight:\")\n",
    "print(f\"   ReLU gradients in early layers are {ratio[0]:.0f}x stronger than Sigmoid!\")\n",
    "print(f\"   This explains why deep Sigmoid networks are hard to train.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **PReLU** learns the negative slope, potentially better than fixed LeakyReLU\n",
    "2. **Sigmoid gradients vanish** exponentially in deep networks\n",
    "3. **ReLU maintains gradient flow** allowing training of very deep networks\n",
    "4. Modern activations (GELU, SiLU) combine smoothness with good gradient flow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
