{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4.5 Solution: Training Diagnostics Lab\n",
    "\n",
    "This notebook contains solutions to the exercises from Task 4.5.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple\n",
    "import time\n",
    "import os\n",
    "import gzip\n",
    "import urllib.request\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('default')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST for experiments\n",
    "def load_mnist(path='../data'):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    base_url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = {\n",
    "        'train_images': 'train-images-idx3-ubyte.gz',\n",
    "        'train_labels': 'train-labels-idx1-ubyte.gz',\n",
    "        'test_images': 't10k-images-idx3-ubyte.gz',\n",
    "        'test_labels': 't10k-labels-idx1-ubyte.gz'\n",
    "    }\n",
    "    \n",
    "    def download(filename):\n",
    "        filepath = os.path.join(path, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            urllib.request.urlretrieve(base_url + filename, filepath)\n",
    "        return filepath\n",
    "    \n",
    "    def load_images(fp):\n",
    "        with gzip.open(fp, 'rb') as f:\n",
    "            f.read(16)\n",
    "            return np.frombuffer(f.read(), dtype=np.uint8).reshape(-1, 784).astype(np.float32) / 255.0\n",
    "    \n",
    "    def load_labels(fp):\n",
    "        with gzip.open(fp, 'rb') as f:\n",
    "            f.read(8)\n",
    "            return np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    \n",
    "    return (load_images(download(files['train_images'])),\n",
    "            load_labels(download(files['train_labels'])),\n",
    "            load_images(download(files['test_images'])),\n",
    "            load_labels(download(files['test_labels'])))\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_mnist()\n",
    "print(f\"Loaded {len(X_train)} training samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise Solution: Diagnose These Training Curves\n",
    "\n",
    "### Problem A: Learning Rate Too High (Loss Exploding)\n",
    "\n",
    "**Symptoms:**\n",
    "- Loss increases exponentially\n",
    "- Training is completely unstable\n",
    "- Model outputs become NaN\n",
    "\n",
    "**Diagnosis:** The learning rate is so high that weight updates overshoot the optimum massively, causing the loss to explode.\n",
    "\n",
    "**Fix:** Decrease learning rate by 10-100x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Problem A\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "x = np.arange(20)\n",
    "y_exploding = np.exp(x * 0.3) + np.random.randn(20) * 10\n",
    "\n",
    "ax.plot(y_exploding, 'r-', linewidth=2, label='LR too high')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Problem A: Learning Rate Too High - Loss Exploding')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Diagnosis: Learning rate too high\")\n",
    "print(\"Fix: Decrease LR by 10-100x (e.g., from 10.0 to 0.1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Problem B: Learning Rate Too Low (Loss Stuck)\n",
    "\n",
    "**Symptoms:**\n",
    "- Loss barely decreases\n",
    "- Training takes extremely long\n",
    "- Loss appears flat after many epochs\n",
    "\n",
    "**Diagnosis:** The learning rate is so small that weight updates are negligible. The model is learning, but at a glacial pace.\n",
    "\n",
    "**Fix:** Increase learning rate by 10-100x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Problem B\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "x = np.arange(20)\n",
    "y_stuck = 2.3 + np.random.randn(20) * 0.01\n",
    "\n",
    "ax.plot(y_stuck, 'b-', linewidth=2, label='LR too low')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Problem B: Learning Rate Too Low - Loss Stuck')\n",
    "ax.set_ylim(0, 5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Diagnosis: Learning rate too low\")\n",
    "print(\"Fix: Increase LR by 10-100x (e.g., from 0.0001 to 0.01 or 0.1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Problem C: Overfitting\n",
    "\n",
    "**Symptoms:**\n",
    "- Training loss decreases steadily\n",
    "- Validation loss decreases initially, then starts increasing\n",
    "- Gap between train and validation loss grows\n",
    "\n",
    "**Diagnosis:** The model is memorizing the training data instead of learning generalizable patterns. It performs well on training data but poorly on unseen data.\n",
    "\n",
    "**Fixes:**\n",
    "1. Add regularization (L2, Dropout)\n",
    "2. Get more training data\n",
    "3. Use data augmentation\n",
    "4. Reduce model complexity\n",
    "5. Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Problem C\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "x = np.arange(20)\n",
    "y_train_loss = 2.3 - np.log(x + 1) * 0.8 + np.random.randn(20) * 0.05\n",
    "y_val_loss = 2.3 - np.log(x + 1) * 0.3 + x * 0.05 + np.random.randn(20) * 0.05\n",
    "\n",
    "ax.plot(y_train_loss, 'b-', linewidth=2, label='Train Loss')\n",
    "ax.plot(y_val_loss, 'r-', linewidth=2, label='Validation Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Problem C: Overfitting - Train/Val Divergence')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "# Mark the divergence point\n",
    "ax.axvline(x=8, color='orange', linestyle='--', alpha=0.7, label='Divergence point')\n",
    "ax.annotate('Overfitting starts here', xy=(8, 1.5), xytext=(12, 1.8),\n",
    "            arrowprops=dict(arrowstyle='->', color='orange'),\n",
    "            fontsize=10, color='orange')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Diagnosis: Overfitting\")\n",
    "print(\"Fixes:\")\n",
    "print(\"  1. Add L2 regularization (weight decay)\")\n",
    "print(\"  2. Add Dropout layers\")\n",
    "print(\"  3. Get more training data\")\n",
    "print(\"  4. Use early stopping at the divergence point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Problem D: Learning Rate Too High (Oscillating)\n",
    "\n",
    "**Symptoms:**\n",
    "- Loss oscillates up and down\n",
    "- No steady decrease\n",
    "- Training is unstable but doesn't explode\n",
    "\n",
    "**Diagnosis:** The learning rate is high enough to cause oscillation around the optimum, but not so high that it explodes. The model keeps overshooting.\n",
    "\n",
    "**Fixes:**\n",
    "1. Decrease learning rate by 2-10x\n",
    "2. Use learning rate scheduling\n",
    "3. Increase batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Problem D\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "x = np.arange(20)\n",
    "y_oscillating = 1.5 + np.sin(x * 0.5) * 0.5 + np.random.randn(20) * 0.3\n",
    "\n",
    "ax.plot(y_oscillating, 'g-', linewidth=2, label='LR causing oscillation')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Problem D: Learning Rate Too High - Oscillating')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Diagnosis: Learning rate too high (oscillating)\")\n",
    "print(\"Fixes:\")\n",
    "print(\"  1. Decrease LR by 2-10x\")\n",
    "print(\"  2. Use learning rate scheduling (reduce on plateau)\")\n",
    "print(\"  3. Increase batch size for more stable gradients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Quick Reference Table\n",
    "\n",
    "| Symptom | Diagnosis | Fix |\n",
    "|---------|-----------|-----|\n",
    "| Loss explodes to Inf/NaN | LR too high | Decrease LR by 10-100x |\n",
    "| Loss barely moves | LR too low | Increase LR by 10-100x |\n",
    "| Loss oscillates | LR too high | Decrease LR by 2-10x |\n",
    "| Train loss down, val loss up | Overfitting | Add regularization, more data |\n",
    "| Both losses high, not moving | Underfitting | Bigger model, train longer |\n",
    "| Can't overfit one batch | Bug in code | Debug forward/backward |\n",
    "| Early layer gradients = 0 | Vanishing gradients | Use ReLU, He init |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Implementing the Fixes\n",
    "\n",
    "### Fix 1: Learning Rate Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP:\n",
    "    \"\"\"Simple MLP for demonstrating LR effects.\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes: List[int]):\n",
    "        self.layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            W = np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float32) * np.sqrt(2.0 / layer_sizes[i])\n",
    "            b = np.zeros(layer_sizes[i + 1], dtype=np.float32)\n",
    "            self.layers.append({'W': W, 'b': b, 'cache': {}})\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        out = X\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            layer['cache']['X'] = out\n",
    "            out = out @ layer['W'] + layer['b']\n",
    "            layer['cache']['Z'] = out\n",
    "            out = np.maximum(0, out)  # ReLU\n",
    "        \n",
    "        self.layers[-1]['cache']['X'] = out\n",
    "        out = out @ self.layers[-1]['W'] + self.layers[-1]['b']\n",
    "        \n",
    "        out_shifted = out - np.max(out, axis=1, keepdims=True)\n",
    "        exp_out = np.exp(out_shifted)\n",
    "        self.probs = exp_out / np.sum(exp_out, axis=1, keepdims=True)\n",
    "        return self.probs\n",
    "    \n",
    "    def backward(self, targets: np.ndarray, lr: float) -> None:\n",
    "        batch_size = len(targets)\n",
    "        grad = self.probs.copy()\n",
    "        grad[np.arange(batch_size), targets] -= 1\n",
    "        \n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            layer = self.layers[i]\n",
    "            X = layer['cache']['X']\n",
    "            \n",
    "            dW = X.T @ grad / batch_size\n",
    "            db = np.mean(grad, axis=0)\n",
    "            grad = grad @ layer['W'].T\n",
    "            \n",
    "            if i > 0:\n",
    "                Z = self.layers[i - 1]['cache']['Z']\n",
    "                grad = grad * (Z > 0)\n",
    "            \n",
    "            layer['W'] -= lr * dW\n",
    "            layer['b'] -= lr * db\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return np.argmax(self.forward(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_lr(lr: float, epochs: int = 20) -> Dict[str, List[float]]:\n",
    "    \"\"\"Train model with given learning rate.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    model = SimpleMLP([784, 256, 128, 10])\n",
    "    history = {'loss': [], 'acc': []}\n",
    "    \n",
    "    X_sub, y_sub = X_train[:5000], y_train[:5000]\n",
    "    batch_size = 64\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        indices = np.random.permutation(len(X_sub))\n",
    "        epoch_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for start in range(0, len(X_sub), batch_size):\n",
    "            batch_idx = indices[start:start + batch_size]\n",
    "            X_batch = X_sub[batch_idx]\n",
    "            y_batch = y_sub[batch_idx]\n",
    "            \n",
    "            probs = model.forward(X_batch)\n",
    "            loss = -np.mean(np.log(probs[np.arange(len(y_batch)), y_batch] + 1e-10))\n",
    "            \n",
    "            if np.isnan(loss) or loss > 100:\n",
    "                loss = 100\n",
    "            \n",
    "            model.backward(y_batch, lr)\n",
    "            epoch_loss += loss\n",
    "            n_batches += 1\n",
    "        \n",
    "        acc = np.mean(model.predict(X_test[:1000]) == y_test[:1000])\n",
    "        history['loss'].append(epoch_loss / n_batches)\n",
    "        history['acc'].append(acc)\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Compare different learning rates\n",
    "print(\"Training with different learning rates...\")\n",
    "lr_high = train_with_lr(10.0)\n",
    "lr_good = train_with_lr(0.1)\n",
    "lr_low = train_with_lr(0.0001)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LR comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(lr_high['loss'], 'r-', linewidth=2, alpha=0.7, label='LR=10.0 (too high)')\n",
    "axes[0].plot(lr_good['loss'], 'g-', linewidth=2, label='LR=0.1 (good)')\n",
    "axes[0].plot(lr_low['loss'], 'b-', linewidth=2, alpha=0.7, label='LR=0.0001 (too low)')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Curves: Learning Rate Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim(0, 10)\n",
    "\n",
    "axes[1].plot(lr_high['acc'], 'r-', linewidth=2, alpha=0.7, label='LR=10.0 (too high)')\n",
    "axes[1].plot(lr_good['acc'], 'g-', linewidth=2, label='LR=0.1 (good)')\n",
    "axes[1].plot(lr_low['acc'], 'b-', linewidth=2, alpha=0.7, label='LR=0.0001 (too low)')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Curves: Learning Rate Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  LR=10.0:   Loss={lr_high['loss'][-1]:.4f}, Acc={lr_high['acc'][-1]:.2%}\")\n",
    "print(f\"  LR=0.1:    Loss={lr_good['loss'][-1]:.4f}, Acc={lr_good['acc'][-1]:.2%}\")\n",
    "print(f\"  LR=0.0001: Loss={lr_low['loss'][-1]:.4f}, Acc={lr_low['acc'][-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Always start with the 'overfit one batch' test** - if your model can't memorize 32 samples, there's a bug in your code.\n",
    "\n",
    "2. **Learning rate is usually the first thing to check** - try 10x higher or 10x lower to diagnose.\n",
    "\n",
    "3. **Vanishing gradients are sneaky** - use ReLU activation and proper He initialization.\n",
    "\n",
    "4. **Overfitting is obvious from train/val divergence** - add regularization (L2, Dropout) or get more data.\n",
    "\n",
    "5. **Track gradient magnitudes** - if early layers have tiny gradients, you have vanishing gradient problem.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
