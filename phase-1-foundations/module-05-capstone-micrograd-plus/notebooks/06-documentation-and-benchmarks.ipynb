{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5.6: Documentation and Benchmarks\n",
    "\n",
    "**Module:** 5 - Phase 1 Capstone: MicroGrad+  \n",
    "**Time:** 1 hour  \n",
    "**Difficulty:** â­â­\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand how to write good API documentation\n",
    "- [ ] Compare MicroGrad+ performance against PyTorch\n",
    "- [ ] Learn why PyTorch is faster and what optimizations it uses\n",
    "- [ ] Complete the Phase 1 Capstone!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Prerequisites\n",
    "\n",
    "- Completed: Tasks 5.1-5.5\n",
    "- Knowledge of: Python documentation practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport time\nimport sys\nfrom pathlib import Path\n\ndef _find_module_root():\n    \"\"\"Find the module root directory containing micrograd_plus.\"\"\"\n    current = Path.cwd()\n    for parent in [current] + list(current.parents):\n        if (parent / 'micrograd_plus' / '__init__.py').exists():\n            return str(parent)\n    # Fallback to parent directory\n    return str(Path.cwd().parent)\n\nsys.path.insert(0, _find_module_root())\n\nfrom micrograd_plus import (\n    Tensor, Linear, ReLU, Dropout,\n    CrossEntropyLoss, MSELoss, Adam, SGD, Sequential\n)\nfrom micrograd_plus.utils import set_seed\n\nset_seed(42)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: API Documentation\n",
    "\n",
    "Good documentation makes your code usable by others (and your future self!). Let's review the key components of our MicroGrad+ API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the module documentation\n",
    "import micrograd_plus\n",
    "\n",
    "print(\"MicroGrad+ Module Documentation\")\n",
    "print(\"=\" * 60)\n",
    "print(micrograd_plus.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check individual class documentation\n",
    "print(\"Tensor Class Documentation\")\n",
    "print(\"=\" * 60)\n",
    "print(Tensor.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of good docstring format\n",
    "print(\"Linear Layer Documentation\")\n",
    "print(\"=\" * 60)\n",
    "print(Linear.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation Best Practices\n",
    "\n",
    "1. **Module docstring**: Explain what the module does and provide a quick example\n",
    "2. **Class docstring**: Describe purpose, args, attributes, and usage\n",
    "3. **Method docstring**: Document args, returns, raises, and provide examples\n",
    "4. **Type hints**: Use them for better IDE support and clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Benchmarking Against PyTorch\n",
    "\n",
    "Let's compare MicroGrad+ performance against PyTorch. Spoiler: PyTorch will be much faster - and that's okay! The goal is to understand why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if PyTorch is available\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    PYTORCH_AVAILABLE = True\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"PyTorch not available. Skipping PyTorch benchmarks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_micrograd(batch_size, in_features, hidden, out_features, num_iterations):\n",
    "    \"\"\"Benchmark MicroGrad+ forward and backward pass.\"\"\"\n",
    "    set_seed(42)\n",
    "    \n",
    "    # Create model\n",
    "    model = Sequential(\n",
    "        Linear(in_features, hidden),\n",
    "        ReLU(),\n",
    "        Linear(hidden, hidden),\n",
    "        ReLU(),\n",
    "        Linear(hidden, out_features)\n",
    "    )\n",
    "    \n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Create data\n",
    "    X = np.random.randn(batch_size, in_features).astype(np.float32)\n",
    "    y = np.random.randint(0, out_features, batch_size)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        x_tensor = Tensor(X, requires_grad=True)\n",
    "        logits = model(x_tensor)\n",
    "        loss = loss_fn(logits, Tensor(y))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.time()\n",
    "    for _ in range(num_iterations):\n",
    "        x_tensor = Tensor(X, requires_grad=True)\n",
    "        logits = model(x_tensor)\n",
    "        loss = loss_fn(logits, Tensor(y))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    return elapsed / num_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE:\n",
    "    def benchmark_pytorch(batch_size, in_features, hidden, out_features, num_iterations, device='cpu'):\n",
    "        \"\"\"Benchmark PyTorch forward and backward pass.\"\"\"\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        # Create model\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, out_features)\n",
    "        ).to(device)\n",
    "        \n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Create data\n",
    "        X = torch.randn(batch_size, in_features, device=device)\n",
    "        y = torch.randint(0, out_features, (batch_size,), device=device)\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(5):\n",
    "            logits = model(X)\n",
    "            loss = loss_fn(logits, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if device == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        # Benchmark\n",
    "        start = time.time()\n",
    "        for _ in range(num_iterations):\n",
    "            logits = model(X)\n",
    "            loss = loss_fn(logits, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if device == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        return elapsed / num_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmarks\n",
    "print(\"Benchmark: Forward + Backward + Optimizer Step\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "configs = [\n",
    "    (32, 784, 256, 10, 100),   # Small: MNIST-like\n",
    "    (64, 784, 512, 10, 50),    # Medium\n",
    "    (128, 1024, 512, 100, 20), # Large\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for batch_size, in_features, hidden, out_features, iters in configs:\n",
    "    print(f\"\\nConfig: batch={batch_size}, input={in_features}, hidden={hidden}, output={out_features}\")\n",
    "    \n",
    "    # MicroGrad+\n",
    "    mg_time = benchmark_micrograd(batch_size, in_features, hidden, out_features, iters)\n",
    "    print(f\"  MicroGrad+: {mg_time*1000:.2f} ms/iteration\")\n",
    "    \n",
    "    if PYTORCH_AVAILABLE:\n",
    "        # PyTorch CPU\n",
    "        pt_cpu_time = benchmark_pytorch(batch_size, in_features, hidden, out_features, iters, 'cpu')\n",
    "        print(f\"  PyTorch CPU: {pt_cpu_time*1000:.2f} ms/iteration\")\n",
    "        \n",
    "        ratio_cpu = mg_time / pt_cpu_time\n",
    "        print(f\"  Ratio (MicroGrad+ / PyTorch CPU): {ratio_cpu:.1f}x\")\n",
    "        \n",
    "        # PyTorch GPU (if available)\n",
    "        if torch.cuda.is_available():\n",
    "            pt_gpu_time = benchmark_pytorch(batch_size, in_features, hidden, out_features, iters, 'cuda')\n",
    "            print(f\"  PyTorch GPU: {pt_gpu_time*1000:.2f} ms/iteration\")\n",
    "            ratio_gpu = mg_time / pt_gpu_time\n",
    "            print(f\"  Ratio (MicroGrad+ / PyTorch GPU): {ratio_gpu:.1f}x\")\n",
    "        \n",
    "        results.append({\n",
    "            'config': f'{batch_size}x{in_features}',\n",
    "            'micrograd': mg_time * 1000,\n",
    "            'pytorch_cpu': pt_cpu_time * 1000,\n",
    "            'ratio': ratio_cpu\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize benchmark results\n",
    "if PYTORCH_AVAILABLE and results:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    configs = [r['config'] for r in results]\n",
    "    mg_times = [r['micrograd'] for r in results]\n",
    "    pt_times = [r['pytorch_cpu'] for r in results]\n",
    "    ratios = [r['ratio'] for r in results]\n",
    "    \n",
    "    # Time comparison\n",
    "    x = np.arange(len(configs))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0].bar(x - width/2, mg_times, width, label='MicroGrad+', color='blue')\n",
    "    axes[0].bar(x + width/2, pt_times, width, label='PyTorch CPU', color='orange')\n",
    "    axes[0].set_ylabel('Time (ms)')\n",
    "    axes[0].set_title('Execution Time Comparison')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(configs)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ratio\n",
    "    axes[1].bar(configs, ratios, color='green')\n",
    "    axes[1].set_ylabel('Slowdown Factor (x times slower)')\n",
    "    axes[1].set_title('MicroGrad+ vs PyTorch CPU')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Why is PyTorch Faster?\n",
    "\n",
    "PyTorch is typically 10-100x faster than our implementation. Here's why:\n",
    "\n",
    "### 1. Compiled Operations (C++/CUDA)\n",
    "```\n",
    "MicroGrad+: Pure Python + NumPy\n",
    "PyTorch: C++ kernels, CUDA for GPU\n",
    "```\n",
    "\n",
    "### 2. Optimized Memory Layout\n",
    "```\n",
    "MicroGrad+: NumPy arrays (general purpose)\n",
    "PyTorch: Custom tensor layout optimized for ML\n",
    "```\n",
    "\n",
    "### 3. Operator Fusion\n",
    "```\n",
    "MicroGrad+: Each operation creates new array\n",
    "PyTorch: Fuses operations to reduce memory traffic\n",
    "```\n",
    "\n",
    "### 4. Parallelization\n",
    "```\n",
    "MicroGrad+: Single-threaded (mostly)\n",
    "PyTorch: Multi-threaded CPU, massively parallel GPU\n",
    "```\n",
    "\n",
    "### 5. Autograd Optimization\n",
    "```\n",
    "MicroGrad+: Python closures for backward functions\n",
    "PyTorch: Compiled autograd engine with memory optimization\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate why GPU is so fast\n",
    "print(\"Why GPUs are so fast for neural networks:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "Matrix multiplication example: (1000 x 1000) @ (1000 x 1000)\n",
    "\n",
    "Operations needed: 1000 * 1000 * 1000 = 1 billion multiply-adds\n",
    "\n",
    "CPU (8 cores, 3 GHz):\n",
    "  - ~24 billion ops/sec\n",
    "  - Time: ~40 ms\n",
    "\n",
    "GPU (6144 CUDA cores on DGX Spark):\n",
    "  - ~200+ TFLOPS (FP8/FP16)\n",
    "  - Time: ~5 Î¼s (microseconds!)\n",
    "\n",
    "The GPU wins because:\n",
    "1. Thousands of simple cores vs dozens of complex cores\n",
    "2. High memory bandwidth (designed for throughput)\n",
    "3. Operations optimized for data parallelism\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: What You've Accomplished\n",
    "\n",
    "Let's summarize what you built in this capstone project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count lines of code in MicroGrad+\n",
    "import os\n",
    "\n",
    "def count_lines(filepath):\n",
    "    \"\"\"Count non-empty, non-comment lines.\"\"\"\n",
    "    count = 0\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            stripped = line.strip()\n",
    "            if stripped and not stripped.startswith('#'):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "package_dir = '../micrograd_plus'\n",
    "total_lines = 0\n",
    "\n",
    "print(\"MicroGrad+ Package Statistics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for filename in os.listdir(package_dir):\n",
    "    if filename.endswith('.py'):\n",
    "        filepath = os.path.join(package_dir, filename)\n",
    "        lines = count_lines(filepath)\n",
    "        total_lines += lines\n",
    "        print(f\"  {filename:20s}: {lines:4d} lines\")\n",
    "\n",
    "print(f\"  {'TOTAL':20s}: {total_lines:4d} lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of what was built\n",
    "print(\"\"\"\n",
    "ðŸŽ‰ CONGRATULATIONS! You've completed the Phase 1 Capstone! ðŸŽ‰\n",
    "\n",
    "You built MicroGrad+, featuring:\n",
    "\n",
    "âœ… Tensor Class with Autograd\n",
    "   - Automatic differentiation (reverse-mode)\n",
    "   - Broadcasting support\n",
    "   - 15+ differentiable operations\n",
    "\n",
    "âœ… Neural Network Layers\n",
    "   - Linear (fully connected)\n",
    "   - ReLU, Sigmoid, Tanh activations\n",
    "   - Softmax for classification\n",
    "   - Dropout for regularization\n",
    "   - BatchNorm for normalization\n",
    "\n",
    "âœ… Loss Functions\n",
    "   - MSE Loss for regression\n",
    "   - Cross-Entropy Loss for classification\n",
    "\n",
    "âœ… Optimizers\n",
    "   - SGD with momentum\n",
    "   - Adam (adaptive learning rates)\n",
    "\n",
    "âœ… Training Utilities\n",
    "   - Sequential container\n",
    "   - DataLoader for batching\n",
    "   - Training and evaluation loops\n",
    "\n",
    "âœ… Testing Suite\n",
    "   - Unit tests for all components\n",
    "   - Gradient checking against numerical gradients\n",
    "\n",
    "âœ… Real Application\n",
    "   - Trained on MNIST with >95% accuracy!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: What's Next?\n",
    "\n",
    "In **Phase 2: Intermediate**, you'll use PyTorch to:\n",
    "\n",
    "1. **Work with real hardware**\n",
    "   - Use the DGX Spark GPU\n",
    "   - Leverage 128GB unified memory\n",
    "   - See 10-100x speedups!\n",
    "\n",
    "2. **Build advanced architectures**\n",
    "   - Convolutional Neural Networks (CNNs)\n",
    "   - Recurrent Neural Networks (RNNs)\n",
    "   - Transformers and Attention\n",
    "\n",
    "3. **Handle real-world data**\n",
    "   - Image classification (CIFAR-10, ImageNet)\n",
    "   - Natural Language Processing\n",
    "   - Transfer learning\n",
    "\n",
    "4. **Scale to larger models**\n",
    "   - Fine-tuning pre-trained models\n",
    "   - Efficient training techniques\n",
    "   - Model optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Phase 1 Complete!\n",
    "\n",
    "You now have:\n",
    "- Deep understanding of how neural networks work internally\n",
    "- Your own working autograd engine\n",
    "- Solid foundation in Python, NumPy, and software engineering\n",
    "- Practical experience training real models\n",
    "\n",
    "These fundamentals will serve you well as you move to more advanced topics!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "print(\"Phase 1 Capstone Complete! ðŸŽ“\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}