{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.4: Compatibility Matrix\n",
    "\n",
    "**Module:** 1 - DGX Spark Platform Mastery  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand why some tools don't work on DGX Spark\n",
    "- [ ] Research and document 20+ AI/ML tool compatibility\n",
    "- [ ] Learn workarounds for partially supported tools\n",
    "- [ ] Create a comprehensive reference matrix\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Task 1.1-1.3\n",
    "- Basic understanding of ARM64 architecture\n",
    "- Familiarity with common AI/ML tools\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "You've just received your DGX Spark. Excitedly, you try to install your favorite tools:\n",
    "\n",
    "```bash\n",
    "pip install torch  # ‚ùå Fails - wrong architecture\n",
    "pip install vllm   # ‚ùå Fails - x86_64 only wheels\n",
    "```\n",
    "\n",
    "This frustration is common! The DGX Spark uses ARM64 (aarch64) architecture with a cutting-edge Blackwell GPU. Many tools simply haven't caught up yet.\n",
    "\n",
    "This notebook helps you navigate the ecosystem and find what works.\n",
    "\n",
    "---\n",
    "\n",
    "## üßí ELI5: Why Don't Some Tools Work?\n",
    "\n",
    "> **Imagine you have a special game console that plays a new type of game disc...**\n",
    ">\n",
    "> Most games are made for the old disc format (x86). Your new console (DGX Spark with ARM64) needs games made specifically for it.\n",
    ">\n",
    "> Some game makers have already made new versions (NVIDIA with NGC containers). Others are still working on it. And some old games might never get ported.\n",
    ">\n",
    "> **In AI terms:** Software must be compiled for ARM64 + CUDA 13 + Blackwell GPU. This is a rare combination that many projects don't support yet.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Compatibility Challenges\n",
    "\n",
    "### Why Things Break\n",
    "\n",
    "Three main reasons why tools may not work on DGX Spark:\n",
    "\n",
    "1. **Architecture (ARM64 vs x86_64)**\n",
    "   - Most Python wheels are pre-compiled for x86_64\n",
    "   - ARM64 requires separate compilation\n",
    "   - Many projects don't publish ARM64 wheels\n",
    "\n",
    "2. **CUDA Version (13.x)**\n",
    "   - DGX Spark uses CUDA 13.0+\n",
    "   - Many tools only support CUDA 11.x or 12.x\n",
    "   - CUDA versions are NOT backward compatible\n",
    "\n",
    "3. **GPU Architecture (Blackwell)**\n",
    "   - Blackwell (SM 100) is brand new\n",
    "   - Older CUDA code may not include Blackwell support\n",
    "   - Some optimizations require architecture-specific code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's verify our platform details\n",
    "import platform\n",
    "import subprocess\n",
    "\n",
    "print(\"DGX Spark Platform Details\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Architecture: {platform.machine()}\")\n",
    "print(f\"OS: {platform.system()} {platform.release()}\")\n",
    "print(f\"Python: {platform.python_version()}\")\n",
    "\n",
    "# Get CUDA version\n",
    "try:\n",
    "    result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n",
    "    for line in result.stdout.split('\\n'):\n",
    "        if 'CUDA Version' in line:\n",
    "            cuda_ver = line.split('CUDA Version:')[1].split()[0]\n",
    "            print(f\"CUDA: {cuda_ver}\")\n",
    "            break\n",
    "except Exception:\n",
    "    print(\"CUDA: Unable to determine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Compatibility Categories\n",
    "\n",
    "We'll categorize tools into these support levels:\n",
    "\n",
    "| Level | Icon | Meaning |\n",
    "|-------|------|----------|\n",
    "| Full Support | ‚úÖ | Works out of the box |\n",
    "| NGC Required | üê≥ | Works via NGC containers |\n",
    "| Partial | ‚ö†Ô∏è | Works with limitations/workarounds |\n",
    "| Not Compatible | ‚ùå | Does not work currently |\n",
    "| Untested | ‚ùì | Needs testing |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define compatibility data structure\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List\n",
    "from enum import Enum\n",
    "\n",
    "class CompatLevel(Enum):\n",
    "    FULL = \"‚úÖ Full Support\"\n",
    "    NGC = \"üê≥ NGC Required\"\n",
    "    PARTIAL = \"‚ö†Ô∏è Partial\"\n",
    "    NO = \"‚ùå Not Compatible\"\n",
    "    UNTESTED = \"‚ùì Untested\"\n",
    "\n",
    "@dataclass\n",
    "class ToolCompatibility:\n",
    "    \"\"\"Compatibility information for an AI/ML tool.\"\"\"\n",
    "    name: str\n",
    "    category: str\n",
    "    level: CompatLevel\n",
    "    notes: str\n",
    "    workaround: Optional[str] = None\n",
    "    tested_version: Optional[str] = None\n",
    "    url: Optional[str] = None\n",
    "\n",
    "print(\"Compatibility data structure defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: The Compatibility Matrix\n",
    "\n",
    "Here's the comprehensive compatibility matrix for DGX Spark. This is based on testing and community reports as of 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive compatibility database\n",
    "\n",
    "compatibility_matrix = [\n",
    "    # ========== Deep Learning Frameworks ==========\n",
    "    ToolCompatibility(\n",
    "        name=\"PyTorch\",\n",
    "        category=\"Deep Learning Framework\",\n",
    "        level=CompatLevel.NGC,\n",
    "        notes=\"Must use NGC container. pip install does NOT work on ARM64+CUDA.\",\n",
    "        workaround=\"docker pull nvcr.io/nvidia/pytorch:25.11-py3\",\n",
    "        tested_version=\"2.5+\",\n",
    "        url=\"https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"TensorFlow\",\n",
    "        category=\"Deep Learning Framework\",\n",
    "        level=CompatLevel.NGC,\n",
    "        notes=\"Use NGC container for GPU support.\",\n",
    "        workaround=\"docker pull nvcr.io/nvidia/tensorflow:25.11-tf2-py3\",\n",
    "        tested_version=\"2.16+\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"JAX\",\n",
    "        category=\"Deep Learning Framework\",\n",
    "        level=CompatLevel.NGC,\n",
    "        notes=\"NGC container available with CUDA support.\",\n",
    "        workaround=\"docker pull nvcr.io/nvidia/jax:25.11-py3\"\n",
    "    ),\n",
    "    \n",
    "    # ========== LLM Inference ==========\n",
    "    ToolCompatibility(\n",
    "        name=\"Ollama\",\n",
    "        category=\"LLM Inference\",\n",
    "        level=CompatLevel.FULL,\n",
    "        notes=\"Native ARM64 support. Pre-installed on DGX OS. Excellent performance.\",\n",
    "        tested_version=\"0.3+\",\n",
    "        url=\"https://ollama.ai\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"llama.cpp\",\n",
    "        category=\"LLM Inference\",\n",
    "        level=CompatLevel.FULL,\n",
    "        notes=\"Native ARM64+CUDA support. Compile with CUDA flags.\",\n",
    "        workaround=\"cmake -B build -DGGML_CUDA=ON && cmake --build build\",\n",
    "        url=\"https://github.com/ggerganov/llama.cpp\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"vLLM\",\n",
    "        category=\"LLM Inference\",\n",
    "        level=CompatLevel.PARTIAL,\n",
    "        notes=\"Experimental ARM64 support. May require building from source.\",\n",
    "        workaround=\"Build from source with ARM64+CUDA flags\",\n",
    "        url=\"https://github.com/vllm-project/vllm\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"TensorRT-LLM\",\n",
    "        category=\"LLM Inference\",\n",
    "        level=CompatLevel.PARTIAL,\n",
    "        notes=\"NVIDIA product - Blackwell support in development.\",\n",
    "        workaround=\"Use NGC container with latest TRT-LLM\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"Text Generation Inference (TGI)\",\n",
    "        category=\"LLM Inference\",\n",
    "        level=CompatLevel.PARTIAL,\n",
    "        notes=\"HuggingFace server. ARM64 Docker image available.\",\n",
    "        workaround=\"Use ARM64 Docker image\"\n",
    "    ),\n",
    "    \n",
    "    # ========== Model Libraries ==========\n",
    "    ToolCompatibility(\n",
    "        name=\"Hugging Face Transformers\",\n",
    "        category=\"Model Library\",\n",
    "        level=CompatLevel.NGC,\n",
    "        notes=\"Works inside NGC PyTorch container.\",\n",
    "        workaround=\"pip install transformers inside NGC container\",\n",
    "        tested_version=\"4.40+\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"Hugging Face Diffusers\",\n",
    "        category=\"Model Library\",\n",
    "        level=CompatLevel.NGC,\n",
    "        notes=\"Works inside NGC PyTorch container.\",\n",
    "        workaround=\"pip install diffusers inside NGC container\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"NVIDIA NeMo\",\n",
    "        category=\"Model Library\",\n",
    "        level=CompatLevel.NGC,\n",
    "        notes=\"Full support via NGC container.\",\n",
    "        workaround=\"docker pull nvcr.io/nvidia/nemo:25.11\",\n",
    "        url=\"https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"OpenAI API (client)\",\n",
    "        category=\"Model Library\",\n",
    "        level=CompatLevel.FULL,\n",
    "        notes=\"Pure Python - works everywhere.\",\n",
    "        tested_version=\"1.0+\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"LangChain\",\n",
    "        category=\"Model Library\",\n",
    "        level=CompatLevel.FULL,\n",
    "        notes=\"Pure Python - works everywhere. Use with Ollama.\",\n",
    "        tested_version=\"0.2+\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"LlamaIndex\",\n",
    "        category=\"Model Library\",\n",
    "        level=CompatLevel.FULL,\n",
    "        notes=\"Pure Python - works everywhere.\",\n",
    "        tested_version=\"0.10+\"\n",
    "    ),\n",
    "    \n",
    "    # ========== Training & Fine-tuning ==========\n",
    "    ToolCompatibility(\n",
    "        name=\"DeepSpeed\",\n",
    "        category=\"Training\",\n",
    "        level=CompatLevel.PARTIAL,\n",
    "        notes=\"Some features may not work. Use NGC container.\",\n",
    "        workaround=\"Use within NGC PyTorch container\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"PEFT (LoRA)\",\n",
    "        category=\"Training\",\n",
    "        level=CompatLevel.NGC,\n",
    "        notes=\"Works inside NGC container.\",\n",
    "        workaround=\"pip install peft inside NGC container\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"bitsandbytes\",\n",
    "        category=\"Training\",\n",
    "        level=CompatLevel.PARTIAL,\n",
    "        notes=\"4-bit/8-bit quantization. ARM64 support improving.\",\n",
    "        workaround=\"May need to build from source\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"Unsloth\",\n",
    "        category=\"Training\",\n",
    "        level=CompatLevel.UNTESTED,\n",
    "        notes=\"Fast fine-tuning. Needs testing on ARM64.\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"Axolotl\",\n",
    "        category=\"Training\",\n",
    "        level=CompatLevel.NGC,\n",
    "        notes=\"Fine-tuning framework. Use with NGC container.\"\n",
    "    ),\n",
    "    \n",
    "    # ========== Data Science ==========\n",
    "    ToolCompatibility(\n",
    "        name=\"RAPIDS (cuDF, cuML)\",\n",
    "        category=\"Data Science\",\n",
    "        level=CompatLevel.NGC,\n",
    "        notes=\"GPU-accelerated data science. Full NGC support.\",\n",
    "        workaround=\"docker pull nvcr.io/nvidia/rapidsai/base:25.11-cuda13.0-py3.11\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"Pandas\",\n",
    "        category=\"Data Science\",\n",
    "        level=CompatLevel.FULL,\n",
    "        notes=\"Pure Python - works everywhere.\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"NumPy\",\n",
    "        category=\"Data Science\",\n",
    "        level=CompatLevel.FULL,\n",
    "        notes=\"ARM64 wheels available.\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"Scikit-learn\",\n",
    "        category=\"Data Science\",\n",
    "        level=CompatLevel.FULL,\n",
    "        notes=\"ARM64 wheels available.\"\n",
    "    ),\n",
    "    \n",
    "    # ========== Vector Databases ==========\n",
    "    ToolCompatibility(\n",
    "        name=\"ChromaDB\",\n",
    "        category=\"Vector Database\",\n",
    "        level=CompatLevel.FULL,\n",
    "        notes=\"Pure Python with SQLite. Works everywhere.\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"FAISS\",\n",
    "        category=\"Vector Database\",\n",
    "        level=CompatLevel.NGC,\n",
    "        notes=\"GPU version needs NGC container.\",\n",
    "        workaround=\"Use faiss-gpu inside NGC container\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"Milvus\",\n",
    "        category=\"Vector Database\",\n",
    "        level=CompatLevel.PARTIAL,\n",
    "        notes=\"ARM64 Docker images available.\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"Qdrant\",\n",
    "        category=\"Vector Database\",\n",
    "        level=CompatLevel.FULL,\n",
    "        notes=\"ARM64 Docker images available.\"\n",
    "    ),\n",
    "    \n",
    "    # ========== Development Tools ==========\n",
    "    ToolCompatibility(\n",
    "        name=\"JupyterLab\",\n",
    "        category=\"Development\",\n",
    "        level=CompatLevel.FULL,\n",
    "        notes=\"Pre-installed on DGX OS.\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"VS Code\",\n",
    "        category=\"Development\",\n",
    "        level=CompatLevel.FULL,\n",
    "        notes=\"ARM64 version available.\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"Docker\",\n",
    "        category=\"Development\",\n",
    "        level=CompatLevel.FULL,\n",
    "        notes=\"Pre-installed with NVIDIA runtime.\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"Git\",\n",
    "        category=\"Development\",\n",
    "        level=CompatLevel.FULL,\n",
    "        notes=\"Pre-installed.\"\n",
    "    ),\n",
    "    \n",
    "    # ========== MLOps ==========\n",
    "    ToolCompatibility(\n",
    "        name=\"MLflow\",\n",
    "        category=\"MLOps\",\n",
    "        level=CompatLevel.FULL,\n",
    "        notes=\"Pure Python - works everywhere.\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"Weights & Biases\",\n",
    "        category=\"MLOps\",\n",
    "        level=CompatLevel.FULL,\n",
    "        notes=\"Pure Python - works everywhere.\"\n",
    "    ),\n",
    "    ToolCompatibility(\n",
    "        name=\"Triton Inference Server\",\n",
    "        category=\"MLOps\",\n",
    "        level=CompatLevel.NGC,\n",
    "        notes=\"Full NGC support.\",\n",
    "        workaround=\"docker pull nvcr.io/nvidia/tritonserver:25.11-py3\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(compatibility_matrix)} tools into compatibility matrix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Viewing the Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the compatibility matrix\n",
    "\n",
    "def display_matrix_by_category():\n",
    "    \"\"\"Display the compatibility matrix organized by category.\"\"\"\n",
    "    categories = {}\n",
    "    for tool in compatibility_matrix:\n",
    "        if tool.category not in categories:\n",
    "            categories[tool.category] = []\n",
    "        categories[tool.category].append(tool)\n",
    "    \n",
    "    for category, tools in categories.items():\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"  {category}\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "        print(f\"{'Tool':<30} {'Status':<20}\")\n",
    "        print(\"-\" * 50)\n",
    "        for tool in tools:\n",
    "            print(f\"{tool.name:<30} {tool.level.value:<20}\")\n",
    "\n",
    "display_matrix_by_category()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table\n",
    "def create_summary():\n",
    "    \"\"\"Create a summary of compatibility levels.\"\"\"\n",
    "    counts = {level: 0 for level in CompatLevel}\n",
    "    for tool in compatibility_matrix:\n",
    "        counts[tool.level] += 1\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"COMPATIBILITY SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    total = len(compatibility_matrix)\n",
    "    \n",
    "    for level, count in counts.items():\n",
    "        pct = count / total * 100\n",
    "        bar = \"‚ñà\" * int(pct / 5) + \"‚ñë\" * (20 - int(pct / 5))\n",
    "        print(f\"{level.value:<20} [{bar}] {count:>2} ({pct:.0f}%)\")\n",
    "    \n",
    "    print(f\"\\nTotal tools evaluated: {total}\")\n",
    "\n",
    "create_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Detailed Tool Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_tool(name: str):\n",
    "    \"\"\"Look up detailed information for a specific tool.\"\"\"\n",
    "    for tool in compatibility_matrix:\n",
    "        if name.lower() in tool.name.lower():\n",
    "            print(f\"\\n{'=' * 60}\")\n",
    "            print(f\"  {tool.name}\")\n",
    "            print(f\"{'=' * 60}\")\n",
    "            print(f\"Category:    {tool.category}\")\n",
    "            print(f\"Status:      {tool.level.value}\")\n",
    "            print(f\"Notes:       {tool.notes}\")\n",
    "            if tool.workaround:\n",
    "                print(f\"Workaround:  {tool.workaround}\")\n",
    "            if tool.tested_version:\n",
    "                print(f\"Tested Ver:  {tool.tested_version}\")\n",
    "            if tool.url:\n",
    "                print(f\"URL:         {tool.url}\")\n",
    "            return\n",
    "    print(f\"Tool '{name}' not found in matrix.\")\n",
    "\n",
    "# Example lookups\n",
    "lookup_tool(\"PyTorch\")\n",
    "lookup_tool(\"Ollama\")\n",
    "lookup_tool(\"vLLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself #1\n",
    "\n",
    "Look up a tool you use frequently. Is it compatible?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Try: `lookup_tool(\"transformers\")` or `lookup_tool(\"langchain\")`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Look up your favorite tool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Export the Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_markdown_table():\n",
    "    \"\"\"Export compatibility matrix as markdown table.\"\"\"\n",
    "    lines = [\n",
    "        \"# DGX Spark Compatibility Matrix\\n\",\n",
    "        \"Last updated: \" + __import__('datetime').datetime.now().strftime(\"%Y-%m-%d\") + \"\\n\",\n",
    "        \"\\n| Tool | Category | Status | Notes |\",\n",
    "        \"|------|----------|--------|-------|\"\n",
    "    ]\n",
    "    \n",
    "    for tool in sorted(compatibility_matrix, key=lambda x: (x.category, x.name)):\n",
    "        status = tool.level.value\n",
    "        notes = tool.notes[:50] + \"...\" if len(tool.notes) > 50 else tool.notes\n",
    "        lines.append(f\"| {tool.name} | {tool.category} | {status} | {notes} |\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Generate and display\n",
    "markdown_table = export_markdown_table()\n",
    "print(markdown_table[:2000])  # First 2000 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "with open(\"compatibility_matrix.md\", \"w\") as f:\n",
    "    f.write(export_markdown_table())\n",
    "\n",
    "print(\"‚úÖ Saved to compatibility_matrix.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as JSON for programmatic use\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def export_json():\n",
    "    \"\"\"Export compatibility matrix as JSON.\"\"\"\n",
    "    data = {\n",
    "        \"generated\": datetime.now().isoformat(),\n",
    "        \"platform\": \"NVIDIA DGX Spark\",\n",
    "        \"tools\": [\n",
    "            {\n",
    "                \"name\": t.name,\n",
    "                \"category\": t.category,\n",
    "                \"status\": t.level.name,\n",
    "                \"status_display\": t.level.value,\n",
    "                \"notes\": t.notes,\n",
    "                \"workaround\": t.workaround,\n",
    "                \"tested_version\": t.tested_version,\n",
    "                \"url\": t.url\n",
    "            }\n",
    "            for t in compatibility_matrix\n",
    "        ]\n",
    "    }\n",
    "    return data\n",
    "\n",
    "# Save JSON\n",
    "with open(\"compatibility_matrix.json\", \"w\") as f:\n",
    "    json.dump(export_json(), f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Saved to compatibility_matrix.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Quick Compatibility Checker\n",
    "\n",
    "Let's create a utility function to quickly check if a package can be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pip_compatibility(package_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Check if a pip package has ARM64 Linux wheels.\n",
    "    \n",
    "    Returns:\n",
    "        dict with compatibility info\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    import re\n",
    "    \n",
    "    result = {\n",
    "        \"package\": package_name,\n",
    "        \"has_arm64_wheel\": False,\n",
    "        \"has_any_wheel\": False,\n",
    "        \"is_pure_python\": False,\n",
    "        \"recommendation\": \"\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Query PyPI for package info\n",
    "        import urllib.request\n",
    "        import json\n",
    "        \n",
    "        url = f\"https://pypi.org/pypi/{package_name}/json\"\n",
    "        with urllib.request.urlopen(url, timeout=10) as response:\n",
    "            data = json.loads(response.read())\n",
    "        \n",
    "        # Check available wheels\n",
    "        urls = data.get(\"urls\", [])\n",
    "        \n",
    "        for url_info in urls:\n",
    "            filename = url_info.get(\"filename\", \"\")\n",
    "            \n",
    "            if filename.endswith(\".whl\"):\n",
    "                result[\"has_any_wheel\"] = True\n",
    "                \n",
    "                # Check for ARM64\n",
    "                if \"aarch64\" in filename or \"arm64\" in filename:\n",
    "                    result[\"has_arm64_wheel\"] = True\n",
    "                \n",
    "                # Check for pure Python (py3-none-any)\n",
    "                if \"py3-none-any\" in filename or \"py2.py3-none-any\" in filename:\n",
    "                    result[\"is_pure_python\"] = True\n",
    "        \n",
    "        # Make recommendation\n",
    "        if result[\"is_pure_python\"]:\n",
    "            result[\"recommendation\"] = \"‚úÖ Pure Python - should work directly\"\n",
    "        elif result[\"has_arm64_wheel\"]:\n",
    "            result[\"recommendation\"] = \"‚úÖ ARM64 wheel available - pip install should work\"\n",
    "        elif result[\"has_any_wheel\"]:\n",
    "            result[\"recommendation\"] = \"‚ö†Ô∏è No ARM64 wheel - may need NGC container or build from source\"\n",
    "        else:\n",
    "            result[\"recommendation\"] = \"‚ùì No wheels found - check documentation\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        result[\"recommendation\"] = f\"‚ùå Error checking package: {e}\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test some packages\n",
    "test_packages = [\"numpy\", \"pandas\", \"transformers\", \"torch\", \"langchain\"]\n",
    "\n",
    "print(\"Package Compatibility Check\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for pkg in test_packages:\n",
    "    result = check_pip_compatibility(pkg)\n",
    "    print(f\"\\n{result['package']}:\")\n",
    "    print(f\"  {result['recommendation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Community Resources\n",
    "\n",
    "Where to find help with compatibility issues:\n",
    "\n",
    "| Resource | URL | Purpose |\n",
    "|----------|-----|----------|\n",
    "| NVIDIA Forums | forums.developer.nvidia.com | Official support |\n",
    "| NGC Catalog | catalog.ngc.nvidia.com | Container images |\n",
    "| DGX Playbooks | build.nvidia.com/spark | Guides and examples |\n",
    "| GitHub Issues | Search \"aarch64\" or \"ARM64\" | Community solutions |\n",
    "\n",
    "### Key Tips for Finding Compatibility Info\n",
    "\n",
    "1. **Search GitHub Issues:** Many projects have ARM64 compatibility discussions\n",
    "2. **Check NGC First:** NVIDIA often provides pre-built containers\n",
    "3. **Look for Docker Images:** Multi-arch images often support ARM64\n",
    "4. **Check Recent Commits:** ARM64 support may be in development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Trying pip install for everything\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong - Will fail for many packages\n",
    "pip install torch transformers accelerate\n",
    "\n",
    "# ‚úÖ Right - Use NGC container first\n",
    "docker run --gpus all -it nvcr.io/nvidia/pytorch:25.11-py3 bash\n",
    "# Then inside container:\n",
    "pip install transformers accelerate\n",
    "```\n",
    "\n",
    "### Mistake 2: Assuming x86 Docker images work\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong - x86 image won't run on ARM64\n",
    "docker pull someproject/tool:latest  # May be x86 only!\n",
    "\n",
    "# ‚úÖ Right - Check for ARM64/multi-arch support\n",
    "docker manifest inspect someproject/tool:latest | grep arm64\n",
    "```\n",
    "\n",
    "### Mistake 3: Not checking NGC before building from source\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong - Waste time building\n",
    "git clone https://github.com/project/tool\n",
    "pip install -e .  # Hours of compilation...\n",
    "\n",
    "# ‚úÖ Right - Check NGC first\n",
    "# Search: catalog.ngc.nvidia.com\n",
    "docker pull nvcr.io/nvidia/tool:latest  # Done in minutes!\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ Why ARM64 + CUDA creates compatibility challenges\n",
    "- ‚úÖ How to categorize tool compatibility levels\n",
    "- ‚úÖ Created a comprehensive 30+ tool compatibility matrix\n",
    "- ‚úÖ How to check pip package ARM64 support\n",
    "- ‚úÖ Where to find help and workarounds\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "Add 5 more tools to the compatibility matrix that you use regularly. Research their ARM64/DGX Spark compatibility.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Check:\n",
    "- GitHub releases for ARM64 builds\n",
    "- Docker Hub for multi-arch images\n",
    "- PyPI for aarch64 wheels\n",
    "- NGC catalog for NVIDIA-optimized versions\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Add more tools to the matrix\n",
    "\n",
    "# Example:\n",
    "# my_new_tools = [\n",
    "#     ToolCompatibility(\n",
    "#         name=\"My Tool\",\n",
    "#         category=\"Category\",\n",
    "#         level=CompatLevel.FULL,\n",
    "#         notes=\"Works great!\"\n",
    "#     ),\n",
    "# ]\n",
    "# compatibility_matrix.extend(my_new_tools)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [NGC Container Catalog](https://catalog.ngc.nvidia.com/)\n",
    "- [PyPI Classifier for Platform](https://pypi.org/classifiers/)\n",
    "- [Docker Multi-Platform Images](https://docs.docker.com/build/building/multi-platform/)\n",
    "- [DGX Spark Playbooks](https://build.nvidia.com/spark)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Files created in this notebook:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"  - compatibility_matrix.md\")\n",
    "print(\"  - compatibility_matrix.json\")\n",
    "print(\"\\nThese are reference files - keep them!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ Great job completing Task 1.4: Compatibility Matrix!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nNext up: Task 1.5 - Ollama Benchmarking\")\n",
    "print(\"You'll benchmark LLM inference performance on your DGX Spark.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
