{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 3.4 Solutions: SVD for LoRA Intuition\n",
        "\n",
        "This notebook contains solutions for the exercises in the SVD for LoRA lab.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"SVD for LoRA Solutions\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reconstruct_low_rank(U, S, Vt, rank):\n",
        "    \"\"\"Reconstruct matrix using only top 'rank' singular values\"\"\"\n",
        "    return U[:, :rank] @ np.diag(S[:rank]) @ Vt[:rank, :]\n",
        "\n",
        "def relative_error(original, reconstructed):\n",
        "    \"\"\"Compute relative reconstruction error (Frobenius norm)\"\"\"\n",
        "    return np.linalg.norm(original - reconstructed) / np.linalg.norm(original)\n",
        "\n",
        "def create_low_rank_matrix(d=768, true_rank=64, noise_level=0.01):\n",
        "    \"\"\"Create a simulated neural network weight matrix with low effective rank\"\"\"\n",
        "    A = np.random.randn(d, true_rank) / np.sqrt(true_rank)\n",
        "    B = np.random.randn(true_rank, d) / np.sqrt(true_rank)\n",
        "    noise = np.random.randn(d, d) * noise_level\n",
        "    return A @ B + noise\n",
        "\n",
        "print(\"Helper functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise Solution: Find the Optimal Rank\n",
        "\n",
        "### ðŸ§’ ELI5: What We're Doing\n",
        "\n",
        "> **Imagine you're compressing a photo...**\n",
        ">\n",
        "> - Too much compression: Photo looks blurry (high error)\n",
        "> - Too little compression: File is still huge (no savings)\n",
        "> - Sweet spot: Photo looks great AND file is small!\n",
        ">\n",
        "> We're finding the \"sweet spot\" for matrix compression!\n",
        "\n",
        "### The Task\n",
        "\n",
        "Find the minimum rank needed to achieve less than 1% reconstruction error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_optimal_rank(W, target_error=0.01):\n",
        "    \"\"\"\n",
        "    Find minimum rank needed to achieve target reconstruction error.\n",
        "    \n",
        "    The math:\n",
        "    - SVD gives us W = U @ diag(S) @ Vt\n",
        "    - Low-rank approximation: W_r = U[:,:r] @ diag(S[:r]) @ Vt[:r,:]\n",
        "    - Error = ||W - W_r|| / ||W||\n",
        "    \n",
        "    We find the smallest r where error < target_error.\n",
        "    \n",
        "    Args:\n",
        "        W: Input matrix\n",
        "        target_error: Maximum acceptable relative error (default 1%)\n",
        "    \n",
        "    Returns:\n",
        "        Optimal rank\n",
        "    \"\"\"\n",
        "    # Step 1: Perform SVD\n",
        "    U, S, Vt = np.linalg.svd(W, full_matrices=False)\n",
        "    \n",
        "    # Step 2: Try each rank from 1 to full\n",
        "    for r in range(1, len(S) + 1):\n",
        "        # Reconstruct with rank r\n",
        "        W_approx = reconstruct_low_rank(U, S, Vt, r)\n",
        "        \n",
        "        # Compute error\n",
        "        error = relative_error(W, W_approx)\n",
        "        \n",
        "        # Check if we've achieved target\n",
        "        if error < target_error:\n",
        "            return r\n",
        "    \n",
        "    # If no rank achieves target, return full rank\n",
        "    return len(S)\n",
        "\n",
        "print(\"find_optimal_rank() function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on our simulated neural network weight\n",
        "d_model = 768  # Like BERT-base\n",
        "true_rank = 64  # Simulated effective rank\n",
        "\n",
        "W_neural = create_low_rank_matrix(d=d_model, true_rank=true_rank, noise_level=0.01)\n",
        "\n",
        "# Find optimal rank for 1% error\n",
        "optimal_r = find_optimal_rank(W_neural, target_error=0.01)\n",
        "\n",
        "print(f\"Matrix shape: {W_neural.shape}\")\n",
        "print(f\"Total parameters: {W_neural.size:,}\")\n",
        "print(f\"Simulated true rank: {true_rank}\")\n",
        "print(f\"\\nOptimal rank for <1% error: {optimal_r}\")\n",
        "print(f\"Parameters with low-rank: {2 * d_model * optimal_r:,}\")\n",
        "print(f\"Compression ratio: {W_neural.size / (2 * d_model * optimal_r):.1f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why Does the Optimal Rank Exceed the True Rank?\n",
        "\n",
        "We added noise to our simulated matrix! The noise has \"full rank\", so we need a bit more than the true rank to capture it.\n",
        "\n",
        "In real neural networks:\n",
        "- The effective rank is usually much smaller than the matrix dimension\n",
        "- But it's not perfectly low-rank due to training noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize error vs rank trade-off\n",
        "\n",
        "# Compute error for a range of ranks\n",
        "U, S, Vt = np.linalg.svd(W_neural, full_matrices=False)\n",
        "\n",
        "ranks = list(range(1, 150))\n",
        "errors = []\n",
        "for r in ranks:\n",
        "    W_approx = reconstruct_low_rank(U, S, Vt, r)\n",
        "    errors.append(relative_error(W_neural, W_approx) * 100)  # As percentage\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Linear scale\n",
        "axes[0].plot(ranks, errors, 'b-', linewidth=2)\n",
        "axes[0].axhline(y=1, color='red', linestyle='--', label='1% error threshold')\n",
        "axes[0].axvline(x=optimal_r, color='green', linestyle='--', \n",
        "               label=f'Optimal rank = {optimal_r}')\n",
        "axes[0].axvline(x=true_rank, color='orange', linestyle=':', \n",
        "               label=f'True rank = {true_rank}')\n",
        "axes[0].set_xlabel('Rank', fontsize=12)\n",
        "axes[0].set_ylabel('Reconstruction Error (%)', fontsize=12)\n",
        "axes[0].set_title('Error vs Rank Trade-off', fontsize=14)\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].set_xlim(0, 150)\n",
        "\n",
        "# Log scale for detail\n",
        "axes[1].semilogy(ranks, errors, 'b-', linewidth=2)\n",
        "axes[1].axhline(y=1, color='red', linestyle='--', label='1% error threshold')\n",
        "axes[1].axvline(x=optimal_r, color='green', linestyle='--', \n",
        "               label=f'Optimal rank = {optimal_r}')\n",
        "axes[1].set_xlabel('Rank', fontsize=12)\n",
        "axes[1].set_ylabel('Reconstruction Error (%) - Log Scale', fontsize=12)\n",
        "axes[1].set_title('Error vs Rank (Log Scale)', fontsize=14)\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].set_xlim(0, 150)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Bonus: Efficient Implementation Using Singular Values\n",
        "\n",
        "We can compute the error without actually reconstructing the matrix!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_optimal_rank_efficient(W, target_error=0.01):\n",
        "    \"\"\"\n",
        "    Find optimal rank efficiently using singular values directly.\n",
        "    \n",
        "    Key insight: The Frobenius norm of the error is:\n",
        "    ||W - W_r||_FÂ² = sum(S[r:]Â²)\n",
        "    \n",
        "    So we can compute error from singular values alone!\n",
        "    \n",
        "    This is MUCH faster for large matrices.\n",
        "    \"\"\"\n",
        "    # Step 1: Perform SVD (or just get singular values)\n",
        "    S = np.linalg.svd(W, compute_uv=False)\n",
        "    \n",
        "    # Total energy (Frobenius norm squared)\n",
        "    total_energy = np.sum(S ** 2)\n",
        "    \n",
        "    # Step 2: Compute cumulative energy from the BACK\n",
        "    # (energy NOT captured by keeping top r components)\n",
        "    residual_energy = np.cumsum(S[::-1] ** 2)[::-1]\n",
        "    \n",
        "    # Step 3: Compute relative error for each rank\n",
        "    # error_rÂ² = residual_energy[r] / total_energy\n",
        "    relative_errors = np.sqrt(residual_energy / total_energy)\n",
        "    \n",
        "    # Step 4: Find first rank where error < target\n",
        "    for r, error in enumerate(relative_errors, 1):\n",
        "        if error < target_error:\n",
        "            return r\n",
        "    \n",
        "    return len(S)\n",
        "\n",
        "# Verify it gives the same answer\n",
        "optimal_r_efficient = find_optimal_rank_efficient(W_neural, target_error=0.01)\n",
        "\n",
        "print(f\"Original method: rank = {optimal_r}\")\n",
        "print(f\"Efficient method: rank = {optimal_r_efficient}\")\n",
        "print(f\"Match: {optimal_r == optimal_r_efficient}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time comparison\n",
        "import time\n",
        "\n",
        "# Original method\n",
        "start = time.time()\n",
        "for _ in range(10):\n",
        "    _ = find_optimal_rank(W_neural, target_error=0.01)\n",
        "original_time = (time.time() - start) / 10\n",
        "\n",
        "# Efficient method\n",
        "start = time.time()\n",
        "for _ in range(10):\n",
        "    _ = find_optimal_rank_efficient(W_neural, target_error=0.01)\n",
        "efficient_time = (time.time() - start) / 10\n",
        "\n",
        "print(f\"Original method: {original_time*1000:.2f} ms per call\")\n",
        "print(f\"Efficient method: {efficient_time*1000:.2f} ms per call\")\n",
        "print(f\"Speedup: {original_time/efficient_time:.1f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Bonus 2: Testing on Different Matrix Types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on different types of matrices\n",
        "\n",
        "test_matrices = {\n",
        "    'Low-rank (r=16)': create_low_rank_matrix(768, true_rank=16, noise_level=0.005),\n",
        "    'Low-rank (r=64)': create_low_rank_matrix(768, true_rank=64, noise_level=0.005),\n",
        "    'Low-rank (r=128)': create_low_rank_matrix(768, true_rank=128, noise_level=0.005),\n",
        "    'Random (full rank)': np.random.randn(768, 768) / np.sqrt(768),\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, W in test_matrices.items():\n",
        "    optimal_r = find_optimal_rank_efficient(W, target_error=0.01)\n",
        "    full_params = W.size\n",
        "    lora_params = 2 * W.shape[0] * optimal_r\n",
        "    compression = full_params / lora_params\n",
        "    \n",
        "    results.append({\n",
        "        'name': name,\n",
        "        'optimal_rank': optimal_r,\n",
        "        'compression': compression\n",
        "    })\n",
        "\n",
        "print(\"Optimal Rank Analysis\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"{'Matrix Type':<25} {'Optimal Rank':<15} {'Compression':<15}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for r in results:\n",
        "    print(f\"{r['name']:<25} {r['optimal_rank']:<15} {r['compression']:.1f}x\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize singular value spectra for each matrix type\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "colors = plt.cm.tab10(np.linspace(0, 1, len(test_matrices)))\n",
        "\n",
        "for (name, W), color in zip(test_matrices.items(), colors):\n",
        "    S = np.linalg.svd(W, compute_uv=False)\n",
        "    S_normalized = S / S[0]  # Normalize by largest\n",
        "    \n",
        "    # Linear plot\n",
        "    axes[0].plot(S_normalized[:150], '-', color=color, linewidth=2, \n",
        "                label=name, alpha=0.8)\n",
        "    \n",
        "    # Log plot\n",
        "    axes[1].semilogy(S_normalized[:150], '-', color=color, linewidth=2, \n",
        "                    label=name, alpha=0.8)\n",
        "\n",
        "axes[0].set_xlabel('Singular Value Index', fontsize=12)\n",
        "axes[0].set_ylabel('Normalized Singular Value', fontsize=12)\n",
        "axes[0].set_title('Singular Value Decay (Linear)', fontsize=14)\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].set_xlabel('Singular Value Index', fontsize=12)\n",
        "axes[1].set_ylabel('Normalized Singular Value (Log)', fontsize=12)\n",
        "axes[1].set_title('Singular Value Decay (Log Scale)', fontsize=14)\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š Key Observations:\")\n",
        "print(\"  - Low-rank matrices: Sharp drop after true rank\")\n",
        "print(\"  - Random matrices: Slow, gradual decay (all components important)\")\n",
        "print(\"  - This is why LoRA works for neural networks!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Bonus 3: Memory Savings Calculator for Real Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_lora_savings(model_name, d_model, n_layers, lora_rank, \n",
        "                           apply_to=['q', 'v'], bytes_per_param=4):\n",
        "    \"\"\"\n",
        "    Calculate memory savings from using LoRA.\n",
        "    \n",
        "    Args:\n",
        "        model_name: Name for display\n",
        "        d_model: Hidden dimension\n",
        "        n_layers: Number of transformer layers\n",
        "        lora_rank: LoRA rank\n",
        "        apply_to: Which projections to apply LoRA to\n",
        "        bytes_per_param: Bytes per parameter (4 for fp32, 2 for fp16)\n",
        "    \"\"\"\n",
        "    # Standard attention has 4 projections: Q, K, V, O\n",
        "    # Each is d_model Ã— d_model\n",
        "    params_per_projection = d_model * d_model\n",
        "    \n",
        "    # Full fine-tuning\n",
        "    full_trainable = 4 * params_per_projection * n_layers\n",
        "    \n",
        "    # LoRA (only applied to specified projections)\n",
        "    lora_per_projection = 2 * d_model * lora_rank\n",
        "    lora_trainable = len(apply_to) * lora_per_projection * n_layers\n",
        "    \n",
        "    # Memory\n",
        "    full_memory_mb = full_trainable * bytes_per_param / 1e6\n",
        "    lora_memory_mb = lora_trainable * bytes_per_param / 1e6\n",
        "    \n",
        "    return {\n",
        "        'model': model_name,\n",
        "        'full_params': full_trainable,\n",
        "        'lora_params': lora_trainable,\n",
        "        'savings_percent': (1 - lora_trainable/full_trainable) * 100,\n",
        "        'full_memory_mb': full_memory_mb,\n",
        "        'lora_memory_mb': lora_memory_mb\n",
        "    }\n",
        "\n",
        "# Common models\n",
        "models = [\n",
        "    ('BERT-base', 768, 12),\n",
        "    ('BERT-large', 1024, 24),\n",
        "    ('GPT-2', 768, 12),\n",
        "    ('GPT-2 Medium', 1024, 24),\n",
        "    ('LLaMA-7B', 4096, 32),\n",
        "    ('LLaMA-13B', 5120, 40),\n",
        "    ('LLaMA-70B', 8192, 80),\n",
        "]\n",
        "\n",
        "# LoRA ranks to try\n",
        "lora_ranks = [8, 16, 32, 64]\n",
        "\n",
        "print(\"LoRA Memory Savings (rank=16, applied to Q and V)\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"{'Model':<15} {'Full Params':<15} {'LoRA Params':<15} {'Savings':<12} {'LoRA Size':<12}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for model_name, d_model, n_layers in models:\n",
        "    result = calculate_lora_savings(model_name, d_model, n_layers, lora_rank=16)\n",
        "    print(f\"{result['model']:<15} \"\n",
        "          f\"{result['full_params']:>13,} \"\n",
        "          f\"{result['lora_params']:>13,} \"\n",
        "          f\"{result['savings_percent']:>10.1f}% \"\n",
        "          f\"{result['lora_memory_mb']:>10.1f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize savings for LLaMA-7B with different ranks\n",
        "\n",
        "ranks = [4, 8, 16, 32, 64, 128, 256]\n",
        "savings_data = []\n",
        "\n",
        "for rank in ranks:\n",
        "    result = calculate_lora_savings('LLaMA-7B', 4096, 32, lora_rank=rank)\n",
        "    savings_data.append(result)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Savings percentage\n",
        "axes[0].bar([str(r) for r in ranks], \n",
        "           [s['savings_percent'] for s in savings_data],\n",
        "           color='green', alpha=0.7)\n",
        "axes[0].axhline(y=95, color='red', linestyle='--', label='95% threshold')\n",
        "axes[0].set_xlabel('LoRA Rank', fontsize=12)\n",
        "axes[0].set_ylabel('Memory Savings (%)', fontsize=12)\n",
        "axes[0].set_title('LLaMA-7B: LoRA Memory Savings by Rank', fontsize=14)\n",
        "axes[0].legend()\n",
        "axes[0].set_ylim(80, 100)\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# LoRA adapter size\n",
        "axes[1].bar([str(r) for r in ranks], \n",
        "           [s['lora_memory_mb'] for s in savings_data],\n",
        "           color='steelblue', alpha=0.7)\n",
        "axes[1].set_xlabel('LoRA Rank', fontsize=12)\n",
        "axes[1].set_ylabel('LoRA Adapter Size (MB)', fontsize=12)\n",
        "axes[1].set_title('LLaMA-7B: LoRA Adapter Size by Rank', fontsize=14)\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š LLaMA-7B LoRA Insights:\")\n",
        "print(f\"  - Full attention fine-tuning: {savings_data[0]['full_params']/1e9:.2f}B params\")\n",
        "print(f\"  - Rank 16 LoRA: {savings_data[2]['lora_params']/1e6:.1f}M params ({savings_data[2]['savings_percent']:.1f}% savings)\")\n",
        "print(f\"  - Rank 64 LoRA: {savings_data[4]['lora_params']/1e6:.1f}M params ({savings_data[4]['savings_percent']:.1f}% savings)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **Finding optimal rank** is straightforward:\n",
        "   - Perform SVD\n",
        "   - Try ranks from 1 to full\n",
        "   - Return first rank with error < threshold\n",
        "\n",
        "2. **Efficient implementation** uses singular values directly:\n",
        "   - Error can be computed from residual singular values\n",
        "   - No need to reconstruct the full matrix\n",
        "   - Much faster for large matrices\n",
        "\n",
        "3. **Low-rank matrices** require small rank for good approximation\n",
        "   - Neural network weights tend to be low-rank after training\n",
        "   - This is why LoRA works so well!\n",
        "\n",
        "4. **Real-world savings** are dramatic:\n",
        "   - LLaMA-7B: ~96% fewer trainable parameters with rank 16\n",
        "   - LoRA adapters are typically 10-100MB vs GB for full fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "print(\"\\nâœ… Solution notebook complete!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
