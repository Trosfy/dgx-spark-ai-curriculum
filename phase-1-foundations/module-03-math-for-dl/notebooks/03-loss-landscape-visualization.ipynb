{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.3: Loss Landscape Visualization\n",
    "\n",
    "**Module:** 3 - Mathematics for Deep Learning  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand what a loss landscape represents\n",
    "- [ ] Create 2D heatmap visualizations of loss\n",
    "- [ ] Create 3D surface plots of loss landscapes\n",
    "- [ ] Visualize optimization trajectories on loss surfaces\n",
    "- [ ] Identify local minima, saddle points, and flat regions\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Task 3.1-3.2\n",
    "- Knowledge of: Neural networks, gradient descent\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**Why visualize loss landscapes?**\n",
    "\n",
    "- **Debugging:** Why isn't my model learning? (Maybe stuck in local minimum)\n",
    "- **Architecture:** Wide vs deep networks have different landscapes\n",
    "- **Understanding:** Skip connections create smoother landscapes (why ResNet works!)\n",
    "\n",
    "**Famous insight:** Li et al. (2018) showed that ResNet has much smoother loss landscapes than plain networks, explaining their trainability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What is a Loss Landscape?\n",
    "\n",
    "> **Imagine you're exploring a foggy mountain range...**\n",
    ">\n",
    "> - The **height** at any point = how wrong your model is (loss)\n",
    "> - **Your position** on the map = your model's parameters\n",
    "> - The **lowest valleys** = best model parameters\n",
    "> - **Peaks and ridges** = bad parameters (high loss)\n",
    ">\n",
    "> When you train a model, you're:\n",
    "> 1. Standing somewhere on this mountain (initial parameters)\n",
    "> 2. Trying to find the lowest valley (minimize loss)\n",
    "> 3. You can only feel which way is DOWN (gradients)\n",
    "> 4. You take small steps hoping to reach the bottom\n",
    ">\n",
    "> **The challenge:** \n",
    "> - There might be multiple valleys (local minima)\n",
    "> - Some paths lead to dead ends (saddle points)\n",
    "> - Some valleys are narrow and hard to find\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"üöÄ Loss Landscape Visualization Lab\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding Loss Landscape Concepts\n",
    "\n",
    "Before we dive into neural networks, let's visualize different landscape features using simple 2D functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_landscape_3d(func, x_range, y_range, title, ax=None):\n",
    "    \"\"\"Create a 3D surface plot of a 2D function\"\"\"\n",
    "    x = np.linspace(*x_range, 100)\n",
    "    y = np.linspace(*y_range, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = func(X, Y)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(10, 7))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    surf = ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, \n",
    "                          linewidth=0, antialiased=True)\n",
    "    ax.set_xlabel('Parameter 1')\n",
    "    ax.set_ylabel('Parameter 2')\n",
    "    ax.set_zlabel('Loss')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "# Different landscape types\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "# 1. Single global minimum (ideal!)\n",
    "ax1 = fig.add_subplot(221, projection='3d')\n",
    "single_min = lambda x, y: x**2 + y**2\n",
    "plot_landscape_3d(single_min, (-3, 3), (-3, 3), \"1. Single Global Minimum\\n(Ideal - Convex)\", ax1)\n",
    "\n",
    "# 2. Multiple local minima\n",
    "ax2 = fig.add_subplot(222, projection='3d')\n",
    "multi_min = lambda x, y: np.sin(2*x) * np.cos(2*y) + 0.1*(x**2 + y**2)\n",
    "plot_landscape_3d(multi_min, (-3, 3), (-3, 3), \"2. Multiple Local Minima\\n(Non-convex)\", ax2)\n",
    "\n",
    "# 3. Saddle point\n",
    "ax3 = fig.add_subplot(223, projection='3d')\n",
    "saddle = lambda x, y: x**2 - y**2\n",
    "plot_landscape_3d(saddle, (-2, 2), (-2, 2), \"3. Saddle Point\\n(Goes down in x, up in y)\", ax3)\n",
    "\n",
    "# 4. Narrow valley (hard to optimize)\n",
    "ax4 = fig.add_subplot(224, projection='3d')\n",
    "narrow = lambda x, y: 0.1*x**2 + 10*y**2  # Very different scales!\n",
    "plot_landscape_3d(narrow, (-3, 3), (-3, 3), \"4. Narrow Valley\\n(Ill-conditioned)\", ax4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Four types of loss landscapes:\")\n",
    "print(\"1. Convex: One global minimum - easy to optimize!\")\n",
    "print(\"2. Non-convex: Multiple minima - might get stuck\")\n",
    "print(\"3. Saddle point: Gradient is zero but not minimum - deceiving\")\n",
    "print(\"4. Ill-conditioned: Different scales - causes zigzagging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Key Concepts\n",
    "\n",
    "| Feature | Description | Challenge |\n",
    "|---------|-------------|----------|\n",
    "| **Global Minimum** | Lowest point overall | The goal! |\n",
    "| **Local Minimum** | Lowest point nearby | Can trap optimizer |\n",
    "| **Saddle Point** | Flat in gradient, but not minimum | Gradient ~0 but not done |\n",
    "| **Narrow Valley** | Steep in some directions | Causes oscillation |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Creating a Neural Network for Visualization\n",
    "\n",
    "Let's create a simple network and visualize its actual loss landscape on a toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple dataset (2D classification)\n",
    "def create_moons_dataset(n_samples=200, noise=0.1):\n",
    "    \"\"\"Create a two-moons classification dataset\"\"\"\n",
    "    n_samples_per_class = n_samples // 2\n",
    "    \n",
    "    # First moon\n",
    "    theta1 = np.linspace(0, np.pi, n_samples_per_class)\n",
    "    X1 = np.column_stack([np.cos(theta1), np.sin(theta1)])\n",
    "    \n",
    "    # Second moon (shifted)\n",
    "    theta2 = np.linspace(0, np.pi, n_samples_per_class)\n",
    "    X2 = np.column_stack([1 - np.cos(theta2), 1 - np.sin(theta2) - 0.5])\n",
    "    \n",
    "    X = np.vstack([X1, X2]) + np.random.randn(n_samples, 2) * noise\n",
    "    y = np.array([0] * n_samples_per_class + [1] * n_samples_per_class)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X_np, y_np = create_moons_dataset(200, noise=0.15)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_data = torch.FloatTensor(X_np)\n",
    "y_data = torch.FloatTensor(y_np).unsqueeze(1)\n",
    "\n",
    "# Visualize the dataset\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_np[y_np==0, 0], X_np[y_np==0, 1], c='red', label='Class 0', alpha=0.6)\n",
    "plt.scatter(X_np[y_np==1, 0], X_np[y_np==1, 1], c='blue', label='Class 1', alpha=0.6)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Two Moons Dataset')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dataset: {X_np.shape[0]} samples, {X_np.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple network\n",
    "class SimpleNet(nn.Module):\n",
    "    \"\"\"A tiny network for loss landscape visualization\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size=4):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Create network and loss function\n",
    "model = SimpleNet(hidden_size=4)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "print(\"Network architecture:\")\n",
    "print(f\"  Input(2) ‚Üí Hidden({4}) ‚Üí Output(1)\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Visualizing a 2D Slice of the Loss Landscape\n",
    "\n",
    "Neural networks have many parameters, but we can visualize a 2D \"slice\" by:\n",
    "1. Picking a trained model's parameters as the \"center\"\n",
    "2. Choosing two random directions in parameter space\n",
    "3. Moving along these directions and measuring loss\n",
    "\n",
    "This technique was introduced by Li et al. (2018) in \"Visualizing the Loss Landscape of Neural Nets\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_as_vector(model):\n",
    "    \"\"\"Flatten all model parameters into a single vector\"\"\"\n",
    "    return torch.cat([p.data.view(-1) for p in model.parameters()])\n",
    "\n",
    "def set_params_from_vector(model, params_vector):\n",
    "    \"\"\"Set model parameters from a flattened vector\"\"\"\n",
    "    idx = 0\n",
    "    for p in model.parameters():\n",
    "        numel = p.numel()\n",
    "        p.data = params_vector[idx:idx+numel].view(p.shape)\n",
    "        idx += numel\n",
    "\n",
    "def compute_loss(model, X, y, criterion):\n",
    "    \"\"\"Compute loss without gradient tracking\"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X)\n",
    "        return criterion(outputs, y).item()\n",
    "\n",
    "def create_random_direction(model):\n",
    "    \"\"\"Create a random direction in parameter space (normalized per layer)\"\"\"\n",
    "    direction = []\n",
    "    for p in model.parameters():\n",
    "        d = torch.randn_like(p)\n",
    "        # Normalize by the parameter's norm (filter normalization)\n",
    "        d = d / (d.norm() + 1e-10) * p.norm()\n",
    "        direction.append(d.view(-1))\n",
    "    return torch.cat(direction)\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, train the model so we have a good center point\n",
    "model = SimpleNet(hidden_size=8)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "for epoch in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_data)\n",
    "    loss = criterion(outputs, y_data)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "print(f\"Training complete!\")\n",
    "print(f\"  Initial loss: {losses[0]:.4f}\")\n",
    "print(f\"  Final loss: {losses[-1]:.4f}\")\n",
    "\n",
    "# Plot training curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 2D loss landscape visualization\n",
    "\n",
    "# Save the trained parameters as center\n",
    "center_params = get_params_as_vector(model).clone()\n",
    "\n",
    "# Create two random directions\n",
    "torch.manual_seed(42)\n",
    "direction1 = create_random_direction(model)\n",
    "direction2 = create_random_direction(model)\n",
    "\n",
    "# Define the range to explore\n",
    "alpha_range = np.linspace(-1.0, 1.0, 51)\n",
    "beta_range = np.linspace(-1.0, 1.0, 51)\n",
    "\n",
    "# Compute loss for each point in the grid\n",
    "print(\"Computing loss landscape (this may take a moment)...\")\n",
    "loss_surface = np.zeros((len(beta_range), len(alpha_range)))\n",
    "\n",
    "for i, beta in enumerate(beta_range):\n",
    "    for j, alpha in enumerate(alpha_range):\n",
    "        # Move in the two directions from center\n",
    "        new_params = center_params + alpha * direction1 + beta * direction2\n",
    "        set_params_from_vector(model, new_params)\n",
    "        loss_surface[i, j] = compute_loss(model, X_data, y_data, criterion)\n",
    "\n",
    "# Restore original parameters\n",
    "set_params_from_vector(model, center_params)\n",
    "\n",
    "print(f\"Loss range: [{loss_surface.min():.4f}, {loss_surface.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize as 2D contour plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 2D Contour (heatmap)\n",
    "A, B = np.meshgrid(alpha_range, beta_range)\n",
    "contour = axes[0].contourf(A, B, loss_surface, levels=50, cmap='viridis')\n",
    "axes[0].contour(A, B, loss_surface, levels=20, colors='white', alpha=0.3, linewidths=0.5)\n",
    "axes[0].scatter([0], [0], color='red', s=100, marker='*', label='Trained model', zorder=5)\n",
    "axes[0].set_xlabel('Direction 1 (Œ±)')\n",
    "axes[0].set_ylabel('Direction 2 (Œ≤)')\n",
    "axes[0].set_title('Loss Landscape (2D Slice)')\n",
    "axes[0].legend()\n",
    "plt.colorbar(contour, ax=axes[0], label='Loss')\n",
    "\n",
    "# 3D Surface\n",
    "ax3d = fig.add_subplot(122, projection='3d')\n",
    "surf = ax3d.plot_surface(A, B, loss_surface, cmap='viridis', alpha=0.8)\n",
    "ax3d.scatter([0], [0], [loss_surface[len(beta_range)//2, len(alpha_range)//2]], \n",
    "            color='red', s=100, marker='*')\n",
    "ax3d.set_xlabel('Direction 1 (Œ±)')\n",
    "ax3d.set_ylabel('Direction 2 (Œ≤)')\n",
    "ax3d.set_zlabel('Loss')\n",
    "ax3d.set_title('Loss Landscape (3D Surface)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä The trained model (red star) sits in a valley!\")\n",
    "print(\"   This is what optimization achieved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "We visualized a 2D \"slice\" of the loss landscape:\n",
    "\n",
    "- **Center (red star):** Our trained model's parameters\n",
    "- **Axes:** Two random directions in parameter space\n",
    "- **Color:** Loss value (darker = lower = better)\n",
    "\n",
    "The trained model sits at a **low point** (valley) in the landscape!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Optimization Trajectory\n",
    "\n",
    "Let's watch how the optimizer navigates the loss landscape during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a fresh model and record the trajectory\n",
    "torch.manual_seed(123)\n",
    "model = SimpleNet(hidden_size=8)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)  # Use SGD for clearer trajectory\n",
    "\n",
    "# Record parameters at each step\n",
    "trajectory = [get_params_as_vector(model).clone()]\n",
    "losses = [compute_loss(model, X_data, y_data, criterion)]\n",
    "\n",
    "# Train and record\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_data)\n",
    "    loss = criterion(outputs, y_data)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    trajectory.append(get_params_as_vector(model).clone())\n",
    "    losses.append(loss.item())\n",
    "\n",
    "print(f\"Recorded {len(trajectory)} points along the trajectory\")\n",
    "print(f\"Loss: {losses[0]:.4f} ‚Üí {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA to project trajectory to 2D\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Stack all trajectory points\n",
    "traj_matrix = torch.stack(trajectory).numpy()\n",
    "\n",
    "# Fit PCA to find the 2 most important directions\n",
    "pca = PCA(n_components=2)\n",
    "traj_2d = pca.fit_transform(traj_matrix)\n",
    "\n",
    "print(f\"Variance explained by 2 PCs: {pca.explained_variance_ratio_.sum()*100:.1f}%\")\n",
    "\n",
    "# Create loss landscape along these PCA directions\n",
    "# Use final point as center\n",
    "center = traj_matrix[-1]\n",
    "pc1 = pca.components_[0]\n",
    "pc2 = pca.components_[1]\n",
    "\n",
    "# Determine range from trajectory\n",
    "margin = 0.5\n",
    "x_min, x_max = traj_2d[:, 0].min() - margin, traj_2d[:, 0].max() + margin\n",
    "y_min, y_max = traj_2d[:, 1].min() - margin, traj_2d[:, 1].max() + margin\n",
    "\n",
    "# Create grid\n",
    "x_range = np.linspace(x_min, x_max, 50)\n",
    "y_range = np.linspace(y_min, y_max, 50)\n",
    "\n",
    "# Compute loss surface\n",
    "print(\"Computing loss surface along PCA directions...\")\n",
    "loss_surface = np.zeros((len(y_range), len(x_range)))\n",
    "\n",
    "for i, y_val in enumerate(y_range):\n",
    "    for j, x_val in enumerate(x_range):\n",
    "        # Reconstruct parameters\n",
    "        params = center + x_val * pc1 + y_val * pc2\n",
    "        set_params_from_vector(model, torch.FloatTensor(params))\n",
    "        loss_surface[i, j] = compute_loss(model, X_data, y_data, criterion)\n",
    "\n",
    "# Restore original\n",
    "set_params_from_vector(model, torch.FloatTensor(traj_matrix[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trajectory on loss landscape\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 2D with trajectory\n",
    "X_grid, Y_grid = np.meshgrid(x_range, y_range)\n",
    "contour = axes[0].contourf(X_grid, Y_grid, loss_surface, levels=50, cmap='viridis')\n",
    "axes[0].contour(X_grid, Y_grid, loss_surface, levels=15, colors='white', alpha=0.3, linewidths=0.5)\n",
    "\n",
    "# Plot trajectory\n",
    "axes[0].plot(traj_2d[:, 0], traj_2d[:, 1], 'r-', linewidth=2, alpha=0.7, label='Optimization path')\n",
    "axes[0].scatter(traj_2d[0, 0], traj_2d[0, 1], color='red', s=150, marker='o', \n",
    "               edgecolors='white', linewidth=2, label='Start', zorder=5)\n",
    "axes[0].scatter(traj_2d[-1, 0], traj_2d[-1, 1], color='yellow', s=150, marker='*', \n",
    "               edgecolors='black', linewidth=2, label='End', zorder=5)\n",
    "\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "axes[0].set_title('Optimization Trajectory on Loss Landscape')\n",
    "axes[0].legend(loc='upper right')\n",
    "plt.colorbar(contour, ax=axes[0], label='Loss')\n",
    "\n",
    "# 3D with trajectory\n",
    "ax3d = fig.add_subplot(122, projection='3d')\n",
    "surf = ax3d.plot_surface(X_grid, Y_grid, loss_surface, cmap='viridis', alpha=0.6)\n",
    "\n",
    "# Plot trajectory in 3D\n",
    "traj_losses = np.array(losses)\n",
    "ax3d.plot(traj_2d[:, 0], traj_2d[:, 1], traj_losses, 'r-', linewidth=2, label='Path')\n",
    "ax3d.scatter([traj_2d[0, 0]], [traj_2d[0, 1]], [traj_losses[0]], \n",
    "            color='red', s=100, marker='o', label='Start')\n",
    "ax3d.scatter([traj_2d[-1, 0]], [traj_2d[-1, 1]], [traj_losses[-1]], \n",
    "            color='yellow', s=100, marker='*', label='End')\n",
    "\n",
    "ax3d.set_xlabel('PC1')\n",
    "ax3d.set_ylabel('PC2')\n",
    "ax3d.set_zlabel('Loss')\n",
    "ax3d.set_title('3D Optimization Path')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä The optimizer follows a path from high loss (start) to low loss (end)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Comparing Different Architectures\n",
    "\n",
    "Let's see how network width affects the loss landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_landscape_for_model(model, title, X_data, y_data, resolution=40):\n",
    "    \"\"\"Train model and visualize its loss landscape\"\"\"\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "    \n",
    "    # Train\n",
    "    for _ in range(300):\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(X_data), y_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Get center and directions\n",
    "    center = get_params_as_vector(model).clone()\n",
    "    torch.manual_seed(42)\n",
    "    dir1 = create_random_direction(model)\n",
    "    dir2 = create_random_direction(model)\n",
    "    \n",
    "    # Create landscape\n",
    "    alpha_range = np.linspace(-1, 1, resolution)\n",
    "    beta_range = np.linspace(-1, 1, resolution)\n",
    "    loss_surface = np.zeros((resolution, resolution))\n",
    "    \n",
    "    for i, beta in enumerate(beta_range):\n",
    "        for j, alpha in enumerate(alpha_range):\n",
    "            new_params = center + alpha * dir1 + beta * dir2\n",
    "            set_params_from_vector(model, new_params)\n",
    "            loss_surface[i, j] = compute_loss(model, X_data, y_data, criterion)\n",
    "    \n",
    "    set_params_from_vector(model, center)\n",
    "    \n",
    "    return loss_surface, alpha_range, beta_range\n",
    "\n",
    "# Compare narrow vs wide networks\n",
    "print(\"Creating landscape for narrow network (hidden=2)...\")\n",
    "torch.manual_seed(42)\n",
    "narrow_model = SimpleNet(hidden_size=2)\n",
    "narrow_surface, ar, br = visualize_landscape_for_model(narrow_model, \"Narrow\", X_data, y_data)\n",
    "\n",
    "print(\"Creating landscape for wide network (hidden=32)...\")\n",
    "torch.manual_seed(42)\n",
    "wide_model = SimpleNet(hidden_size=32)\n",
    "wide_surface, _, _ = visualize_landscape_for_model(wide_model, \"Wide\", X_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare landscapes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "A, B = np.meshgrid(ar, br)\n",
    "\n",
    "# Narrow network\n",
    "c1 = axes[0].contourf(A, B, narrow_surface, levels=50, cmap='viridis')\n",
    "axes[0].contour(A, B, narrow_surface, levels=15, colors='white', alpha=0.3)\n",
    "axes[0].scatter([0], [0], color='red', s=100, marker='*')\n",
    "axes[0].set_xlabel('Direction 1')\n",
    "axes[0].set_ylabel('Direction 2')\n",
    "axes[0].set_title(f'Narrow Network (hidden=2)\\n{sum(p.numel() for p in narrow_model.parameters())} params')\n",
    "plt.colorbar(c1, ax=axes[0], label='Loss')\n",
    "\n",
    "# Wide network\n",
    "c2 = axes[1].contourf(A, B, wide_surface, levels=50, cmap='viridis')\n",
    "axes[1].contour(A, B, wide_surface, levels=15, colors='white', alpha=0.3)\n",
    "axes[1].scatter([0], [0], color='red', s=100, marker='*')\n",
    "axes[1].set_xlabel('Direction 1')\n",
    "axes[1].set_ylabel('Direction 2')\n",
    "axes[1].set_title(f'Wide Network (hidden=32)\\n{sum(p.numel() for p in wide_model.parameters())} params')\n",
    "plt.colorbar(c2, ax=axes[1], label='Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observations:\")\n",
    "print(\"  - Wider networks often have smoother landscapes\")\n",
    "print(\"  - More parameters = more directions to escape local minima\")\n",
    "print(\"  - This is why overparameterized networks train well!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting Filter Normalization\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Random directions have different scales per layer\n",
    "direction = torch.randn_like(params)\n",
    "\n",
    "# ‚úÖ Right: Normalize each layer's direction by its parameter norm\n",
    "for p in model.parameters():\n",
    "    d = torch.randn_like(p)\n",
    "    d = d / d.norm() * p.norm()  # Scale to match layer\n",
    "```\n",
    "\n",
    "**Why:** Without normalization, some directions dominate others.\n",
    "\n",
    "### Mistake 2: Visualizing Too Small a Region\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Too zoomed in - miss the big picture\n",
    "alpha_range = np.linspace(-0.01, 0.01, 50)\n",
    "\n",
    "# ‚úÖ Right: Explore a meaningful range\n",
    "alpha_range = np.linspace(-1.0, 1.0, 50)\n",
    "```\n",
    "\n",
    "### Mistake 3: Not Training Before Visualization\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Visualizing around random initialization\n",
    "model = SimpleNet()\n",
    "visualize(model)  # Random point in landscape!\n",
    "\n",
    "# ‚úÖ Right: Train first, then visualize\n",
    "model = SimpleNet()\n",
    "train(model)\n",
    "visualize(model)  # Now we see the minimum!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise: Compare Optimizers on the Same Landscape\n",
    "\n",
    "Visualize the trajectories of SGD, SGD+Momentum, and Adam on the same loss landscape.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "1. Create a fresh model for each optimizer\n",
    "2. Use the same random seed for initialization\n",
    "3. Record trajectories for each\n",
    "4. Project all to the same PCA space\n",
    "5. Plot on the same contour\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Hint structure:\n",
    "# optimizers_to_compare = [\n",
    "#     ('SGD', torch.optim.SGD, {'lr': 0.5}),\n",
    "#     ('Momentum', torch.optim.SGD, {'lr': 0.5, 'momentum': 0.9}),\n",
    "#     ('Adam', torch.optim.Adam, {'lr': 0.05}),\n",
    "# ]\n",
    "# \n",
    "# trajectories = {}\n",
    "# for name, opt_class, opt_kwargs in optimizers_to_compare:\n",
    "#     torch.manual_seed(42)  # Same initialization!\n",
    "#     model = SimpleNet(hidden_size=8)\n",
    "#     optimizer = opt_class(model.parameters(), **opt_kwargs)\n",
    "#     # ... train and record trajectory\n",
    "\n",
    "print(\"Implement the comparison!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "\n",
    "- ‚úÖ **Loss landscapes** are high-dimensional surfaces the optimizer navigates\n",
    "- ‚úÖ **2D slices** let us visualize what's happening\n",
    "- ‚úÖ **Trajectories** show how optimization proceeds\n",
    "- ‚úÖ **Architecture** affects landscape smoothness\n",
    "- ‚úÖ Wider networks tend to have smoother, easier landscapes\n",
    "\n",
    "**Key insight:** The loss landscape is the \"terrain\" your optimizer explores. Understanding it helps you debug training!\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [Visualizing the Loss Landscape of Neural Nets](https://arxiv.org/abs/1712.09913) - The seminal paper\n",
    "- [Loss Surfaces, Mode Connectivity, and Fast Ensembling](https://arxiv.org/abs/1802.10026)\n",
    "- [Deep Double Descent](https://openai.com/blog/deep-double-descent/) - OpenAI blog post\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")\n",
    "print(\"\\n‚û°Ô∏è  Next: Task 3.4 - SVD for LoRA Intuition\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
