{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab E.4: Graph Classification\n",
    "\n",
    "**Module:** E - Graph Neural Networks  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** â­â­â­ (Intermediate-Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand graph-level vs node-level tasks\n",
    "- [ ] Implement graph pooling operations (mean, max, sum)\n",
    "- [ ] Build a complete graph classification pipeline\n",
    "- [ ] Train on MUTAG (molecular property prediction)\n",
    "- [ ] Implement attention-based pooling\n",
    "- [ ] Compare different pooling strategies\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Prerequisites\n",
    "\n",
    "- Completed: Lab E.2 (GCN from Scratch)\n",
    "- Completed: Lab E.3 (Graph Attention Networks)\n",
    "- Knowledge of: GNN message passing, PyTorch basics\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ Real-World Context\n",
    "\n",
    "**Graph Classification Applications:**\n",
    "\n",
    "| Domain | Graph | Classification Task |\n",
    "|--------|-------|--------------------|\n",
    "| Drug Discovery | Molecules | Toxic vs Non-toxic, Active vs Inactive |\n",
    "| Malware Detection | Program call graphs | Malicious vs Benign |\n",
    "| Social Networks | User interaction graphs | Bot vs Human accounts |\n",
    "| Protein Analysis | Protein structures | Enzyme function prediction |\n",
    "\n",
    "**The Challenge:** How do we go from node embeddings to a single prediction for the entire graph?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§’ ELI5: What Is Graph Classification?\n",
    "\n",
    "> **Imagine you're a doctor** looking at different types of bacteria under a microscope:\n",
    ">\n",
    "> **Node Classification** (Labs E.2-E.3):\n",
    "> - \"Is THIS specific cell part of the nucleus?\"\n",
    "> - You classify individual parts\n",
    ">\n",
    "> **Graph Classification** (This Lab):\n",
    "> - \"Is THIS ENTIRE bacterium dangerous?\"\n",
    "> - You classify the whole organism based on its structure\n",
    ">\n",
    "> **The Problem:** Each bacterium has a different number of cells (nodes)!\n",
    "> - Bacterium A: 50 cells\n",
    "> - Bacterium B: 200 cells\n",
    "> - How do we compare them with a fixed-size neural network?\n",
    ">\n",
    "> **The Solution: Pooling!**\n",
    "> - \"Summarize\" all cells into one fixed-size description\n",
    "> - Like taking an \"average\" or \"maximum\" across all cells\n",
    "> - Then classify that summary\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool, global_max_pool, global_add_pool\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MUTAG dataset\n",
    "# MUTAG contains 188 molecules labeled by mutagenicity\n",
    "dataset = TUDataset(root='/tmp/MUTAG', name='MUTAG')\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MUTAG DATASET\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Number of graphs: {len(dataset)}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")\n",
    "print(f\"Number of node features: {dataset.num_node_features}\")\n",
    "print(f\"Number of edge features: {dataset.num_edge_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset statistics\n",
    "num_nodes = [data.num_nodes for data in dataset]\n",
    "num_edges = [data.num_edges for data in dataset]\n",
    "\n",
    "print(f\"\\nðŸ“Š GRAPH STATISTICS\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Nodes per graph: min={min(num_nodes)}, max={max(num_nodes)}, avg={np.mean(num_nodes):.1f}\")\n",
    "print(f\"Edges per graph: min={min(num_edges)}, max={max(num_edges)}, avg={np.mean(num_edges):.1f}\")\n",
    "\n",
    "# Class distribution\n",
    "labels = [data.y.item() for data in dataset]\n",
    "print(f\"\\nðŸ“Š CLASS DISTRIBUTION\")\n",
    "print(\"-\" * 40)\n",
    "for c in range(dataset.num_classes):\n",
    "    count = labels.count(c)\n",
    "    print(f\"Class {c}: {count} graphs ({100*count/len(dataset):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few molecules\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Atom type to color mapping (for MUTAG)\n",
    "atom_colors = ['lightblue', 'orange', 'lightgreen', 'pink', 'yellow', 'cyan', 'magenta']\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    data = dataset[i]\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "    \n",
    "    # Get node colors from features\n",
    "    node_colors = [atom_colors[data.x[n].argmax().item() % len(atom_colors)] \n",
    "                   for n in range(data.num_nodes)]\n",
    "    \n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    nx.draw(G, pos, ax=ax, node_color=node_colors, \n",
    "            node_size=300, with_labels=False, edge_color='gray')\n",
    "    \n",
    "    label = \"Mutagenic\" if data.y.item() == 1 else \"Non-mutagenic\"\n",
    "    ax.set_title(f\"Molecule {i}\\n{label}\\n({data.num_nodes} atoms)\")\n",
    "\n",
    "plt.suptitle(\"Sample Molecules from MUTAG Dataset\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Each molecule is a graph where:\")\n",
    "print(\"   - Nodes = Atoms\")\n",
    "print(\"   - Edges = Chemical bonds\")\n",
    "print(\"   - Task = Predict if the molecule causes genetic mutations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding Graph Pooling\n",
    "\n",
    "### 2.1 The Pooling Problem\n",
    "\n",
    "After GNN layers, we have node embeddings:\n",
    "- Graph 1: 17 nodes â†’ [17, hidden_dim]\n",
    "- Graph 2: 23 nodes â†’ [23, hidden_dim]\n",
    "- Graph 3: 11 nodes â†’ [11, hidden_dim]\n",
    "\n",
    "We need a **fixed-size** representation for classification. This is where **pooling** comes in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the pooling concept\n",
    "\n",
    "def explain_pooling():\n",
    "    print(\"ðŸ”„ GRAPH POOLING EXPLAINED\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Example: 3 nodes with 4-dim embeddings\n",
    "    node_embeddings = torch.tensor([\n",
    "        [1.0, 2.0, 0.5, 3.0],  # Node 0\n",
    "        [0.5, 1.0, 2.0, 1.0],  # Node 1\n",
    "        [2.0, 0.0, 1.5, 2.0],  # Node 2\n",
    "    ])\n",
    "    \n",
    "    print(\"Node embeddings (3 nodes Ã— 4 dimensions):\")\n",
    "    for i, emb in enumerate(node_embeddings):\n",
    "        print(f\"  Node {i}: {emb.tolist()}\")\n",
    "    \n",
    "    print(\"\\nðŸ“Š POOLING OPERATIONS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Mean pooling\n",
    "    mean_pool = node_embeddings.mean(dim=0)\n",
    "    print(f\"MEAN pooling: {mean_pool.tolist()}\")\n",
    "    print(\"  â†’ Average of all nodes. Smooth representation.\")\n",
    "    \n",
    "    # Max pooling\n",
    "    max_pool = node_embeddings.max(dim=0)[0]\n",
    "    print(f\"\\nMAX pooling: {max_pool.tolist()}\")\n",
    "    print(\"  â†’ Maximum across nodes. Captures extreme features.\")\n",
    "    \n",
    "    # Sum pooling\n",
    "    sum_pool = node_embeddings.sum(dim=0)\n",
    "    print(f\"\\nSUM pooling: {sum_pool.tolist()}\")\n",
    "    print(\"  â†’ Sum of all nodes. Sensitive to graph size.\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Different pooling captures different aspects!\")\n",
    "    print(\"   Often, we combine multiple pooling methods.\")\n",
    "\n",
    "explain_pooling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Batching in PyG\n",
    "\n",
    "How does PyG handle multiple graphs of different sizes in a batch?\n",
    "\n",
    "**The `batch` tensor:** Maps each node to its graph index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader to see batching in action\n",
    "loader = DataLoader(dataset[:4], batch_size=4, shuffle=False)\n",
    "batch = next(iter(loader))\n",
    "\n",
    "print(\"ðŸ“¦ BATCHING IN PyG\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nBatch object: {batch}\")\n",
    "print(f\"\\nTotal nodes in batch: {batch.num_nodes}\")\n",
    "print(f\"Total edges in batch: {batch.num_edges}\")\n",
    "\n",
    "print(f\"\\nðŸ”‘ THE BATCH TENSOR\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"batch.batch shape: {batch.batch.shape}\")\n",
    "print(f\"batch.batch values: {batch.batch.tolist()}\")\n",
    "\n",
    "# Count nodes per graph\n",
    "for i in range(4):\n",
    "    n_nodes = (batch.batch == i).sum().item()\n",
    "    print(f\"Graph {i}: {n_nodes} nodes\")\n",
    "\n",
    "print(\"\\nðŸ’¡ The batch tensor tells us which graph each node belongs to!\")\n",
    "print(\"   This is crucial for pooling - we pool WITHIN each graph.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate how global pooling works with batch\n",
    "print(\"\\nðŸ”„ GLOBAL POOLING WITH BATCH\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Fake node embeddings (all ones for simplicity)\n",
    "fake_embeddings = torch.ones(batch.num_nodes, 4)\n",
    "\n",
    "print(f\"Input: {batch.num_nodes} nodes Ã— 4 dims\")\n",
    "\n",
    "# Global mean pooling\n",
    "pooled = global_mean_pool(fake_embeddings, batch.batch)\n",
    "print(f\"\\nAfter global_mean_pool: {pooled.shape}\")\n",
    "print(f\"Output: 4 graphs Ã— 4 dims\")\n",
    "print(f\"\\nPooled values: {pooled}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Each graph's nodes are averaged into ONE vector!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Building a Graph Classifier\n",
    "\n",
    "### 3.1 Basic Architecture\n",
    "\n",
    "```\n",
    "Input Graph â†’ GNN Layers â†’ Pooling â†’ MLP Classifier â†’ Class Prediction\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Neural Network for graph-level classification.\n",
    "    \n",
    "    Architecture:\n",
    "        GCN â†’ ReLU â†’ GCN â†’ ReLU â†’ GCN â†’ Pooling â†’ MLP â†’ Output\n",
    "    \n",
    "    Args:\n",
    "        num_features: Number of input node features\n",
    "        hidden_dim: Hidden layer dimension\n",
    "        num_classes: Number of output classes\n",
    "        pooling: Pooling method ('mean', 'max', 'sum', 'mean_max')\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_features: int, hidden_dim: int, num_classes: int,\n",
    "                 pooling: str = 'mean'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pooling = pooling\n",
    "        \n",
    "        # GNN layers\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Classifier MLP\n",
    "        pool_dim = hidden_dim * 2 if pooling == 'mean_max' else hidden_dim\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(pool_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Node features [num_nodes, num_features]\n",
    "            edge_index: Edge indices [2, num_edges]\n",
    "            batch: Batch assignment [num_nodes]\n",
    "        \n",
    "        Returns:\n",
    "            Class logits [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        # GNN layers\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        # Pooling\n",
    "        if self.pooling == 'mean':\n",
    "            x = global_mean_pool(x, batch)\n",
    "        elif self.pooling == 'max':\n",
    "            x = global_max_pool(x, batch)\n",
    "        elif self.pooling == 'sum':\n",
    "            x = global_add_pool(x, batch)\n",
    "        elif self.pooling == 'mean_max':\n",
    "            x_mean = global_mean_pool(x, batch)\n",
    "            x_max = global_max_pool(x, batch)\n",
    "            x = torch.cat([x_mean, x_max], dim=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling: {self.pooling}\")\n",
    "        \n",
    "        # Classify\n",
    "        return self.classifier(x)\n",
    "    \n",
    "    def get_graph_embedding(self, x, edge_index, batch):\n",
    "        \"\"\"Get graph embedding before classification.\"\"\"\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return global_mean_pool(x, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model = GraphClassifier(\n",
    "    num_features=dataset.num_node_features,\n",
    "    hidden_dim=64,\n",
    "    num_classes=dataset.num_classes,\n",
    "    pooling='mean_max'\n",
    ").to(device)\n",
    "\n",
    "print(\"Graph Classifier Architecture:\")\n",
    "print(\"=\" * 50)\n",
    "print(model)\n",
    "\n",
    "# Test forward pass\n",
    "batch = next(iter(DataLoader(dataset[:4], batch_size=4))).to(device)\n",
    "out = model(batch.x, batch.edge_index, batch.batch)\n",
    "\n",
    "print(f\"\\nTest batch: 4 graphs\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Output (logits): {out}\")\n",
    "\n",
    "# Parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Training the Graph Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test split\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset = dataset[:train_size]\n",
    "test_dataset = dataset[train_size:]\n",
    "\n",
    "print(f\"Training graphs: {len(train_dataset)}\")\n",
    "print(f\"Test graphs: {len(test_dataset)}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(batch.x, batch.edge_index, batch.batch)\n",
    "        loss = F.cross_entropy(out, batch.y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * batch.num_graphs\n",
    "    \n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    \"\"\"Evaluate model accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        out = model(batch.x, batch.edge_index, batch.batch)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == batch.y).sum().item()\n",
    "        total += batch.num_graphs\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model = GraphClassifier(\n",
    "    num_features=dataset.num_node_features,\n",
    "    hidden_dim=64,\n",
    "    num_classes=dataset.num_classes,\n",
    "    pooling='mean_max'\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "history = {'loss': [], 'train_acc': [], 'test_acc': []}\n",
    "\n",
    "print(\"Training Graph Classifier on MUTAG...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_test_acc = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    loss = train(model, train_loader, optimizer, device)\n",
    "    train_acc = evaluate(model, train_loader, device)\n",
    "    test_acc = evaluate(model, test_loader, device)\n",
    "    \n",
    "    history['loss'].append(loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    \n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        best_epoch = epoch\n",
    "    \n",
    "    if epoch % 10 == 0 or epoch == 99:\n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | \"\n",
    "              f\"Train: {train_acc:.4f} | Test: {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"ðŸŽ‰ Best test accuracy: {best_test_acc:.4f} at epoch {best_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['loss'], color='steelblue', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train', linewidth=2)\n",
    "axes[1].plot(history['test_acc'], label='Test', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Classification Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Comparing Pooling Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_pooling(pooling_type, num_runs=5):\n",
    "    \"\"\"Train model with specific pooling and return average accuracy.\"\"\"\n",
    "    accuracies = []\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        # Shuffle dataset\n",
    "        perm = torch.randperm(len(dataset))\n",
    "        shuffled = dataset[perm]\n",
    "        \n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        train_data = shuffled[:train_size]\n",
    "        test_data = shuffled[train_size:]\n",
    "        \n",
    "        train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "        test_loader = DataLoader(test_data, batch_size=32)\n",
    "        \n",
    "        model = GraphClassifier(\n",
    "            num_features=dataset.num_node_features,\n",
    "            hidden_dim=64,\n",
    "            num_classes=dataset.num_classes,\n",
    "            pooling=pooling_type\n",
    "        ).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        \n",
    "        best_acc = 0\n",
    "        for epoch in range(100):\n",
    "            train(model, train_loader, optimizer, device)\n",
    "            acc = evaluate(model, test_loader, device)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "        \n",
    "        accuracies.append(best_acc)\n",
    "    \n",
    "    return np.mean(accuracies), np.std(accuracies)\n",
    "\n",
    "# Compare pooling strategies\n",
    "print(\"Comparing Pooling Strategies (5 runs each)...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "pooling_types = ['mean', 'max', 'sum', 'mean_max']\n",
    "results = {}\n",
    "\n",
    "for pooling in pooling_types:\n",
    "    mean_acc, std_acc = train_with_pooling(pooling)\n",
    "    results[pooling] = (mean_acc, std_acc)\n",
    "    print(f\"{pooling:10s}: {mean_acc:.4f} Â± {std_acc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "best_pooling = max(results, key=lambda x: results[x][0])\n",
    "print(f\"ðŸ† Best pooling: {best_pooling} ({results[best_pooling][0]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize pooling comparison\n",
    "pooling_names = list(results.keys())\n",
    "means = [results[p][0] for p in pooling_names]\n",
    "stds = [results[p][1] for p in pooling_names]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(pooling_names, means, yerr=stds, capsize=5, \n",
    "               color=['steelblue', 'coral', 'seagreen', 'gold'],\n",
    "               edgecolor='black')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Pooling Strategy Comparison on MUTAG')\n",
    "plt.ylim(0.5, 1.0)\n",
    "\n",
    "# Add value labels\n",
    "for bar, mean, std in zip(bars, means, stds):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.02,\n",
    "             f'{mean:.3f}', ha='center', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” Analysis: Which Pooling Works Best?\n",
    "\n",
    "- **Mean pooling**: Captures the \"average\" property of nodes. Good for properties that depend on overall composition.\n",
    "\n",
    "- **Max pooling**: Captures the \"most extreme\" feature. Good when a single atom/feature determines the property.\n",
    "\n",
    "- **Sum pooling**: Sensitive to graph size. Larger molecules naturally have larger representations.\n",
    "\n",
    "- **Mean + Max**: Combines both - often the best choice in practice!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Attention-Based Pooling\n",
    "\n",
    "Instead of simple pooling, we can **learn** which nodes are important!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention-based graph pooling.\n",
    "    \n",
    "    Learns to weight nodes by importance:\n",
    "        attention_score = softmax(MLP(node_embedding))\n",
    "        graph_embedding = sum(attention_score * node_embedding)\n",
    "    \n",
    "    Args:\n",
    "        hidden_dim: Node embedding dimension\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Attention network\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, batch: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node embeddings [num_nodes, hidden_dim]\n",
    "            batch: Batch assignment [num_nodes]\n",
    "        \n",
    "        Returns:\n",
    "            Graph embeddings [batch_size, hidden_dim]\n",
    "            Attention weights [num_nodes] (for visualization)\n",
    "        \"\"\"\n",
    "        # Compute attention scores\n",
    "        attn_scores = self.attention(x).squeeze(-1)  # [num_nodes]\n",
    "        \n",
    "        # Softmax within each graph\n",
    "        from torch_geometric.utils import softmax\n",
    "        attn_weights = softmax(attn_scores, batch)  # [num_nodes]\n",
    "        \n",
    "        # Weighted sum\n",
    "        weighted = x * attn_weights.unsqueeze(-1)  # [num_nodes, hidden_dim]\n",
    "        out = global_add_pool(weighted, batch)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        return out, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphClassifierWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph classifier with attention-based pooling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_features: int, hidden_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # GNN layers\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Attention pooling\n",
    "        self.pool = AttentionPooling(hidden_dim)\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, edge_index, batch, return_attention=False):\n",
    "        # GNN\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        # Attention pooling\n",
    "        x, attn = self.pool(x, batch)\n",
    "        \n",
    "        # Classify\n",
    "        out = self.classifier(x)\n",
    "        \n",
    "        if return_attention:\n",
    "            return out, attn\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with attention pooling\n",
    "def train_attention(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index, batch.batch)\n",
    "        loss = F.cross_entropy(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch.num_graphs\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_attention(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        out = model(batch.x, batch.edge_index, batch.batch)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == batch.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "# Train\n",
    "model_attn = GraphClassifierWithAttention(\n",
    "    num_features=dataset.num_node_features,\n",
    "    hidden_dim=64,\n",
    "    num_classes=dataset.num_classes\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_attn.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Training with Attention Pooling...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "best_acc = 0\n",
    "for epoch in range(100):\n",
    "    loss = train_attention(model_attn, train_loader, optimizer, device)\n",
    "    test_acc = evaluate_attention(model_attn, test_loader, device)\n",
    "    \n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Test: {test_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Best test accuracy with attention: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights on a molecule\n",
    "@torch.no_grad()\n",
    "def visualize_molecule_attention(model, data, idx=0):\n",
    "    \"\"\"\n",
    "    Visualize which atoms the model pays attention to.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get single graph\n",
    "    graph = data[idx].to(device)\n",
    "    batch = torch.zeros(graph.num_nodes, dtype=torch.long, device=device)\n",
    "    \n",
    "    # Get prediction and attention\n",
    "    out, attn = model(graph.x, graph.edge_index, batch, return_attention=True)\n",
    "    pred = out.argmax().item()\n",
    "    attn = attn.cpu().numpy()\n",
    "    \n",
    "    # Normalize attention for visualization\n",
    "    attn_norm = (attn - attn.min()) / (attn.max() - attn.min() + 1e-8)\n",
    "    \n",
    "    # Create graph\n",
    "    G = to_networkx(graph.cpu(), to_undirected=True)\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    \n",
    "    # Draw\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Original molecule\n",
    "    atom_colors = ['lightblue', 'orange', 'lightgreen', 'pink', 'yellow', 'cyan', 'magenta']\n",
    "    node_colors = [atom_colors[graph.x[n].argmax().item() % len(atom_colors)] \n",
    "                   for n in range(graph.num_nodes)]\n",
    "    nx.draw(G, pos, ax=axes[0], node_color=node_colors,\n",
    "            node_size=500, with_labels=True, edge_color='gray')\n",
    "    axes[0].set_title(f\"Molecule (Ground Truth: {'Mutagenic' if graph.y.item() == 1 else 'Non-mutagenic'})\")\n",
    "    \n",
    "    # Attention visualization\n",
    "    node_colors_attn = plt.cm.Reds(attn_norm)\n",
    "    nx.draw(G, pos, ax=axes[1], node_color=attn_norm, cmap=plt.cm.Reds,\n",
    "            node_size=500 + 500 * attn_norm, with_labels=True, edge_color='gray')\n",
    "    axes[1].set_title(f\"Attention Weights (Prediction: {'Mutagenic' if pred == 1 else 'Non-mutagenic'})\")\n",
    "    \n",
    "    # Colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.Reds, norm=plt.Normalize(0, 1))\n",
    "    sm.set_array([])\n",
    "    plt.colorbar(sm, ax=axes[1], label='Attention Weight')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print top-attended atoms\n",
    "    print(f\"\\nðŸ“Š Top-3 attended atoms:\")\n",
    "    top_atoms = np.argsort(attn)[-3:][::-1]\n",
    "    for i, atom in enumerate(top_atoms):\n",
    "        print(f\"  {i+1}. Atom {atom}: attention = {attn[atom]:.4f}\")\n",
    "\n",
    "# Visualize a few molecules\n",
    "for idx in [0, 5, 10]:\n",
    "    visualize_molecule_attention(model_attn, test_dataset, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ‹ Try It Yourself: Exercise 1\n",
    "\n",
    "**Task:** Test on a different molecular dataset - PROTEINS.\n",
    "\n",
    "1. Load the PROTEINS dataset using `TUDataset(root='/tmp/PROTEINS', name='PROTEINS')`\n",
    "2. Train your graph classifier on it\n",
    "3. Compare pooling strategies\n",
    "4. Which works best for protein classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "\n",
    "# proteins = TUDataset(root='/tmp/PROTEINS', name='PROTEINS')\n",
    "# print(f\"PROTEINS: {len(proteins)} graphs, {proteins.num_classes} classes\")\n",
    "\n",
    "# Train and compare..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ‹ Try It Yourself: Exercise 2\n",
    "\n",
    "**Task:** Implement hierarchical pooling (TopK pooling).\n",
    "\n",
    "Instead of pooling all nodes at once, TopK pooling:\n",
    "1. Scores each node\n",
    "2. Keeps only the top-k% highest scoring nodes\n",
    "3. Creates a coarsened graph\n",
    "4. Repeats until we have one \"super-node\"\n",
    "\n",
    "Use PyG's `TopKPooling` layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "from torch_geometric.nn import TopKPooling\n",
    "\n",
    "# Hint:\n",
    "# pool = TopKPooling(hidden_dim, ratio=0.5)\n",
    "# x, edge_index, _, batch, _, _ = pool(x, edge_index, batch=batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš ï¸ Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting the batch tensor\n",
    "```python\n",
    "# âŒ Wrong: Pooling without batch (pools ALL nodes)\n",
    "x = global_mean_pool(x, None)\n",
    "\n",
    "# âœ… Right: Use batch to pool within each graph\n",
    "x = global_mean_pool(x, batch)\n",
    "```\n",
    "**Why:** Without batch, you get ONE output for ALL graphs!\n",
    "\n",
    "### Mistake 2: Wrong loss function for imbalanced data\n",
    "```python\n",
    "# âŒ Risky: Standard cross-entropy with imbalanced classes\n",
    "loss = F.cross_entropy(out, y)\n",
    "\n",
    "# âœ… Better: Use class weights\n",
    "weights = torch.tensor([1.0, 2.0])  # More weight to minority class\n",
    "loss = F.cross_entropy(out, y, weight=weights)\n",
    "```\n",
    "\n",
    "### Mistake 3: Data leakage in graph datasets\n",
    "```python\n",
    "# âŒ Wrong: Shuffling after split (some test graphs in train)\n",
    "train = dataset[:100]\n",
    "test = dataset[100:]\n",
    "train = train.shuffle()\n",
    "\n",
    "# âœ… Right: Shuffle first, then split\n",
    "dataset = dataset.shuffle()\n",
    "train = dataset[:100]\n",
    "test = dataset[100:]\n",
    "```\n",
    "\n",
    "### Mistake 4: Not moving batch to device\n",
    "```python\n",
    "# âŒ Wrong: Batch still on CPU\n",
    "for batch in loader:\n",
    "    out = model(batch.x, batch.edge_index, batch.batch)  # Error!\n",
    "\n",
    "# âœ… Right: Move entire batch to device\n",
    "for batch in loader:\n",
    "    batch = batch.to(device)\n",
    "    out = model(batch.x, batch.edge_index, batch.batch)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- âœ… The difference between node and graph classification\n",
    "- âœ… How graph pooling works (mean, max, sum)\n",
    "- âœ… How PyG batches multiple graphs (batch tensor)\n",
    "- âœ… How to build end-to-end graph classifiers\n",
    "- âœ… Attention-based pooling for interpretability\n",
    "- âœ… Which atoms matter for molecular property prediction!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Challenge (Optional)\n",
    "\n",
    "**Advanced Challenge:** Implement DiffPool (Differentiable Pooling).\n",
    "\n",
    "DiffPool learns a \"soft assignment\" of nodes to clusters, then pools within clusters:\n",
    "\n",
    "1. GNN computes node embeddings\n",
    "2. Another GNN computes cluster assignments (soft)\n",
    "3. Nodes are aggregated into clusters\n",
    "4. Repeat for hierarchical coarsening\n",
    "\n",
    "This is the state-of-the-art for graph classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Challenge: Simplified DiffPool\n",
    "\n",
    "class SimpleDiffPool(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified DiffPool layer.\n",
    "    \n",
    "    Learns soft cluster assignments.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, num_clusters):\n",
    "        super().__init__()\n",
    "        # Your code here!\n",
    "        # Hint: Use a GNN to predict cluster assignments\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Your code here!\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“– Further Reading\n",
    "\n",
    "- [How Powerful are GNNs?](https://arxiv.org/abs/1810.00826) - WL test and GIN\n",
    "- [DiffPool Paper](https://arxiv.org/abs/1806.08804) - Hierarchical pooling\n",
    "- [Graph Classification Survey](https://arxiv.org/abs/2006.16904) - Comprehensive overview\n",
    "- [MoleculeNet](https://moleculenet.org/) - More molecular datasets\n",
    "- [OGB Leaderboard](https://ogb.stanford.edu/) - Graph ML benchmarks\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "\n",
    "del model, model_attn\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory after cleanup: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
    "\n",
    "print(\"âœ… Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ“ Module E Complete!\n",
    "\n",
    "Congratulations! You've completed the Graph Neural Networks module!\n",
    "\n",
    "**What you've mastered:**\n",
    "- âœ… Graph data structures and PyTorch Geometric\n",
    "- âœ… Message passing neural networks (GCN)\n",
    "- âœ… Attention mechanisms for graphs (GAT)\n",
    "- âœ… Graph-level classification with pooling\n",
    "\n",
    "**Where to go from here:**\n",
    "1. **Drug Discovery**: Apply GNNs to molecular property prediction\n",
    "2. **Knowledge Graphs**: Combine with RAG for structured retrieval\n",
    "3. **Social Network Analysis**: Fraud detection, recommendation\n",
    "4. **Research**: Explore GraphTransformers, DiffPool, GIN\n",
    "\n",
    "**Remember:** GNNs are powerful when your data has **relational structure**. Not everything is a graph - but when it is, GNNs shine!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒŸ Final Tips\n",
    "\n",
    "> **Professor SPARK's GNN Wisdom:**\n",
    ">\n",
    "> 1. **Start simple**: GCN often works surprisingly well!\n",
    "> 2. **2-3 layers is enough**: More layers â†’ over-smoothing\n",
    "> 3. **Pool wisely**: mean+max often beats fancy methods\n",
    "> 4. **Attention is interpretable**: Use it to explain predictions\n",
    "> 5. **Scale matters**: For large graphs, use sampling (GraphSAGE)\n",
    "\n",
    "Happy graph learning! ðŸ“ŠðŸ”—ðŸ§ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
