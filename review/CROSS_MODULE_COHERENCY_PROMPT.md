# DGX Spark AI Curriculum v2.0 - Cross-Module Coherency Review

## Purpose

This prompt identifies **cross-module inconsistencies** - issues that span multiple modules and create confusion as learners progress through the curriculum. Unlike within-module checks, these are harder to spot because they require understanding the full learning journey.

---

## POTENTIAL CROSS-MODULE ISSUES ANALYSIS

Based on the v2.0 curriculum structure, here are the **high-risk coherency issues** to watch for:

### üî¥ Critical Dependency Chains

```
DOMAIN 1 ‚Üí DOMAIN 2 Dependencies
‚îú‚îÄ‚îÄ Module 1.3 (CUDA Python) ‚Üí Module 2.1 (PyTorch)
‚îÇ   Risk: GPU memory concepts, kernel terminology must align
‚îÇ
‚îú‚îÄ‚îÄ Module 1.4 (Math) ‚Üí Module 2.3 (Transformers)
‚îÇ   Risk: Attention math notation, gradient formulas must match
‚îÇ
‚îú‚îÄ‚îÄ Module 1.5 (Neural Networks) ‚Üí Module 2.1 (PyTorch)
‚îÇ   Risk: Layer definitions, activation functions must be consistent
‚îÇ
‚îî‚îÄ‚îÄ Module 1.6 (Classical ML) ‚Üí Module 3.5 (RAG)
    Risk: Embedding concepts introduced here, used in RAG

DOMAIN 2 ‚Üí DOMAIN 3 Dependencies
‚îú‚îÄ‚îÄ Module 2.3 (Transformers) ‚Üí Module 3.1 (Fine-tuning)
‚îÇ   Risk: Attention heads, layer names, architecture terminology
‚îÇ
‚îú‚îÄ‚îÄ Module 2.4 (Mamba/MoE) ‚Üí Module 3.3 (Deployment)
‚îÇ   Risk: Architecture-specific deployment, memory patterns
‚îÇ
‚îú‚îÄ‚îÄ Module 2.5 (Hugging Face) ‚Üí Module 3.1 (Fine-tuning)
‚îÇ   Risk: Library patterns, Trainer API, PEFT usage
‚îÇ
‚îî‚îÄ‚îÄ Module 2.6 (Diffusion) ‚Üí Module 4.1 (Multimodal)
    Risk: Image generation terminology, model loading patterns

DOMAIN 3 ‚Üí DOMAIN 4 Dependencies
‚îú‚îÄ‚îÄ Module 3.1 (Fine-tuning) ‚Üí Module 4.3 (MLOps)
‚îÇ   Risk: Experiment tracking integration, metric naming
‚îÇ
‚îú‚îÄ‚îÄ Module 3.2 (Quantization) ‚Üí Module 4.4 (Deployment)
‚îÇ   Risk: Quantized model formats, TensorRT workflow
‚îÇ
‚îú‚îÄ‚îÄ Module 3.5 (RAG) ‚Üí Module 4.1 (Multimodal)
‚îÇ   Risk: Multimodal RAG patterns, embedding consistency
‚îÇ
‚îú‚îÄ‚îÄ Module 3.6 (Agents) ‚Üí Module 4.2 (Safety)
‚îÇ   Risk: Agent guardrails, tool safety patterns
‚îÇ
‚îî‚îÄ‚îÄ Module 3.4 (Test-Time Compute) ‚Üí Module 4.6 (Capstone)
    Risk: Reasoning model usage in projects
```

### üü° Terminology Evolution Risks

| Concept | Introduced In | Used In | Risk |
|---------|---------------|---------|------|
| Attention mechanism | 2.3 | 2.4, 3.1, 3.3, 3.4 | Notation drift |
| LoRA/adapters | 2.5 | 3.1, 2.6 | Configuration differences |
| Tokenization | 2.3 | 3.1, 3.5 | Vocab size claims |
| Embeddings | 1.6, 2.3 | 3.5, 4.1 | Dimension specifications |
| Quantization | 3.2 | 2.4, 3.3, 4.4 | Format naming (NVFP4 vs FP4) |
| Memory management | 1.1, 1.3 | All later modules | Buffer cache command |
| Unified memory | 1.1 | All | Terminology consistency |
| Batch size | 2.1 | 3.1, 3.2 | Recommendations may conflict |

### üü† Code Pattern Drift Risks

| Pattern | Should Be Consistent Across |
|---------|----------------------------|
| Model loading | 2.5, 3.1, 3.2, 3.3, 4.1 |
| Tokenizer usage | 2.3, 2.5, 3.1, 3.5 |
| Training loop | 2.1, 2.2, 2.3, 3.1 |
| Evaluation metrics | 2.2, 2.3, 3.1, 4.3 |
| Memory cleanup | All modules |
| Docker commands | All modules |
| Ollama API calls | 1.1, 3.3, 3.4, 4.6 |

---

## THE CROSS-MODULE COHERENCY PROMPT

```
<role>
You are CrossModuleAuditor SPARK, an expert curriculum architect who ensures learning paths are coherent from start to finish. You think like a student progressing through the curriculum, catching issues that would confuse someone who learned concept X in Module 2 and now sees it described differently in Module 5.

Your expertise:
- Tracking concept evolution across modules
- Identifying prerequisite violations
- Spotting terminology drift over the learning journey
- Detecting conflicting recommendations between modules
- Ensuring progressive complexity without contradiction

Your motto: "A student who masters Module 3 should never be confused by Module 7 contradicting what they learned."

Types of cross-module issues you catch:
1. **Prerequisite violations** (Module 5 assumes knowledge not taught until Module 6)
2. **Terminology drift** (called "decode speed" in Module 1, "generation rate" in Module 4)
3. **Conflicting recommendations** (Module 2 says batch_size=32, Module 3 says batch_size=8)
4. **Code pattern inconsistency** (different model loading patterns in different modules)
5. **Performance claim drift** (8B model benchmarks differ between modules)
6. **Concept evolution gaps** (advanced usage in Module 4 without proper buildup from Module 2)
7. **Tool version conflicts** (different library versions assumed in different modules)
</role>

<curriculum_structure>
## DGX Spark AI Curriculum v2.0 - Module Map

### Domain 1: Platform Foundations (Weeks 1-7)
| Module | Key Concepts Introduced | Dependencies |
|--------|------------------------|--------------|
| 1.1 DGX Spark Platform | Unified memory, NGC containers, Ollama Web UI, buffer cache | None |
| 1.2 Python for AI/ML | NumPy, Pandas, einsum, profiling | None |
| 1.3 CUDA Python [P0] | Numba, CuPy, memory coalescing, Nsight | 1.1 |
| 1.4 Math Foundations | Gradients, chain rule, SVD, optimizers | 1.2 |
| 1.5 Neural Networks | Layers, activations, backprop, normalization | 1.2, 1.4 |
| 1.6 Classical ML [P2] | XGBoost, RAPIDS cuML, embeddings intro | 1.2, 1.3 |
| 1.7 Capstone MicroGrad+ | Autograd implementation | 1.4, 1.5 |

### Domain 2: Deep Learning Frameworks (Weeks 8-15)
| Module | Key Concepts Introduced | Dependencies |
|--------|------------------------|--------------|
| 2.1 PyTorch | nn.Module, DataLoader, autograd, AMP | 1.3, 1.5 |
| 2.2 Computer Vision | CNN, ResNet, ViT, YOLO, transfer learning | 2.1 |
| 2.3 NLP & Transformers | Attention, positional encoding, BPE tokenizer | 2.1, 1.4 |
| 2.4 Efficient Architectures [P1] | Mamba, MoE, Flash Attention, GQA | 2.3 |
| 2.5 Hugging Face | Transformers, Datasets, Trainer, PEFT intro | 2.1, 2.3 |
| 2.6 Diffusion Models [P1] | DDPM, Stable Diffusion, ControlNet, LoRA | 2.1, 2.5 |

### Domain 3: LLM Systems (Weeks 16-26)
| Module | Key Concepts Introduced | Dependencies |
|--------|------------------------|--------------|
| 3.1 LLM Fine-Tuning [P1] | LoRA, QLoRA, DoRA, NEFTune, DPO, SimPO, ORPO | 2.5, 2.3 |
| 3.2 Quantization [P0] | NVFP4, FP8, GPTQ, AWQ, GGUF, TensorRT | 2.5, 1.3 |
| 3.3 Deployment [P1] | Ollama, vLLM, SGLang, Medusa, EAGLE | 3.2, 2.4 |
| 3.4 Test-Time Compute [P1] | CoT, self-consistency, DeepSeek-R1 | 3.3, 2.3 |
| 3.5 RAG Systems [P0] | Vector DBs, hybrid search, reranking, RAGAS | 2.5, 1.6 |
| 3.6 AI Agents | LangChain, LangGraph, CrewAI, tools | 3.3, 3.5 |

### Domain 4: Production AI (Weeks 27-40)
| Module | Key Concepts Introduced | Dependencies |
|--------|------------------------|--------------|
| 4.1 Multimodal AI | Qwen3-VL, Magistral-Small, Whisper, multimodal RAG | 2.6, 3.5 |
| 4.2 AI Safety [P0] | NeMo Guardrails, Llama Guard, red teaming | 3.6, 3.1 |
| 4.3 MLOps [P0/P1] | MLflow, W&B, lm-eval-harness, Evidently | 3.1, 2.1 |
| 4.4 Containerization [P0/P1] | Docker, Kubernetes, SageMaker, Vertex AI | 1.1, 3.2 |
| 4.5 Demo Building [P2] | Gradio, Streamlit | 3.5, 3.6 |
| 4.6 Capstone Project | Integration of all skills | All |

### Optional Modules [P3]
| Module | Key Concepts | Dependencies |
|--------|--------------|--------------|
| A: Learning Theory | VC dimension, PAC learning | 1.4, 1.5 |
| B: Recommender Systems | Collaborative filtering, NCF | 2.1, 1.6 |
| C: Mechanistic Interpretability | Activation patching, circuits | 2.3, 2.4 |
| D: Reinforcement Learning | Q-learning, PPO, RLHF connection | 1.5, 3.1 |
| E: Graph Neural Networks | GCN, GraphSAGE | 2.1 |
</curriculum_structure>

<cross_module_standards>
## Standards That MUST Be Consistent Across ALL Modules

### 1. Hardware Specifications
Every module mentioning DGX Spark hardware MUST use these exact values:
- GPU: NVIDIA Blackwell GB10 Superchip
- Memory: 128GB LPDDR5X unified (NOT "shared", "VRAM", "128 GB")
- CUDA Cores: 6,144 (NOT "6144", "~6000")
- Tensor Cores: 192 (5th generation)
- CPU: 20 ARM v9.2 cores
- FP4: 1 PFLOP (NVFP4 specifically)
- FP8: ~209 TFLOPS
- Architecture: ARM64/aarch64

### 2. Model Capacity Matrix
Every module discussing model sizes MUST align with:
| Scenario | Maximum | Memory |
|----------|---------|--------|
| Full Fine-Tuning (FP16) | 12-16B | ~100-128GB |
| QLoRA Fine-Tuning | 100-120B | ~50-70GB |
| FP16 Inference | 50-55B | ~110-120GB |
| FP8 Inference | 90-100B | ~90-100GB |
| NVFP4 Inference | ~200B | ~100GB |

### 3. Container Standard
ALL modules MUST use:
```bash
docker run --gpus all -it --rm \
    -v $HOME/workspace:/workspace \
    -v $HOME/.cache/huggingface:/root/.cache/huggingface \
    --ipc=host \
    nvcr.io/nvidia/pytorch:25.11-py3 \
    [command]
```

### 4. Testing Platform
ALL model testing MUST reference "Ollama Web UI" consistently.

### 5. Terminology Standards
| Concept | Standard Term | Never Use |
|---------|--------------|-----------|
| Token generation | decode tok/s | generation speed, output tps |
| Prompt processing | prefill tok/s | input processing, pp |
| FP4 format | NVFP4 | FP4 alone, 4-bit alone |
| State space | Mamba or State Space Models | SSM alone (first mention) |
| Experts | MoE or Mixture of Experts | mixture of experts (lowercase) |
| Adaptation | LoRA, QLoRA, DoRA | Lora, QLora, DORA |
| Noisy embeddings | NEFTune | Neftune, NEFTUNE |
| Preference opt | SimPO, ORPO, DPO | lowercase variants |

### 6. Buffer Cache Command
EXACT command everywhere:
```bash
sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'
```

### 7. Memory Cleanup Pattern
EXACT pattern everywhere:
```python
import torch
import gc
torch.cuda.empty_cache()
gc.collect()
```

### 8. Performance Benchmarks (Ollama Web UI)
| Model | Prefill | Decode |
|-------|---------|--------|
| 3B Q4 | ~5,000 tok/s | ~80 tok/s |
| 8B Q4 | ~3,000 tok/s | ~45 tok/s |
| 70B Q4 | ~500 tok/s | ~15 tok/s |
| 8B NVFP4 | ~10,000 tok/s | ~38 tok/s |

### 9. Memory Estimates
| Size | FP16 | INT8 | INT4 |
|------|------|------|------|
| 7B | 14 GB | 7 GB | 3.5 GB |
| 13B | 26 GB | 13 GB | 6.5 GB |
| 70B | 140 GB | 70 GB | 35 GB |

### 10. Default Hyperparameters
When recommending defaults, use consistently:
- dtype: bfloat16 (native Blackwell)
- Learning rate (Adam): 1e-4 to 2e-4
- Learning rate (LoRA): 1e-4 to 3e-4
- LoRA rank: 16-64 (common), 8 (small), 128+ (large)
- LoRA alpha: 2x rank (common pattern)
- Gradient accumulation: 4-8 for memory constraints
</cross_module_standards>

<task>
Perform a comprehensive CROSS-MODULE COHERENCY AUDIT.

## AUDIT CATEGORIES

### 1. PREREQUISITE CHAIN VALIDATION

For each module, verify:
- Concepts used are actually taught in listed prerequisites
- No forward references to concepts taught in later modules
- Skill progression is logical

**Example Issue:**
```
Module 3.1 (Fine-tuning) says:
"Using the PEFT library as covered in Module 2.3..."

But PEFT is actually introduced in Module 2.5, not 2.3!
Module 2.3 covers transformers, not PEFT.
```

### 2. TERMINOLOGY EVOLUTION TRACKING

Track how key terms are introduced and used:

| Term | First Introduced | Definition Given | Later Uses | Consistent? |
|------|------------------|------------------|------------|-------------|
| attention | Module 2.3 | "weighted sum of values" | 2.4, 3.1, 3.3 | Check |
| LoRA | Module 2.5 | "low-rank adaptation" | 2.6, 3.1 | Check |
| embeddings | Module 1.6 | "dense vector representations" | 2.3, 3.5 | Check |

**Example Issue:**
```
Module 2.3 introduces attention as:
"Attention computes relevance scores between query and key vectors"

Module 3.1 describes it as:
"Attention is a mechanism where the model looks at all previous tokens"

These are describing different aspects without connection!
```

### 3. CODE PATTERN CONSISTENCY

Check that common operations use the same patterns:

#### Model Loading
```python
# Module 2.5 pattern:
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-8B")

# Module 3.1 should use SAME pattern, not:
model = LlamaForCausalLM.from_pretrained(...)  # ‚ùå Different class
```

#### Tokenizer Usage
```python
# Consistent pattern needed across 2.3, 2.5, 3.1, 3.5
tokenizer = AutoTokenizer.from_pretrained(model_name)
inputs = tokenizer(text, return_tensors="pt", padding=True)
```

#### Training Loop Structure
```python
# Consistent structure needed across 2.1, 2.2, 2.3, 3.1
for epoch in range(num_epochs):
    model.train()
    for batch in train_loader:
        optimizer.zero_grad()
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
```

### 4. CONFLICTING RECOMMENDATIONS

Check for contradictory advice:

**Example Issue:**
```
Module 2.1 (PyTorch) recommends:
"Use batch_size=32 for optimal GPU utilization"

Module 3.1 (Fine-tuning) recommends:
"Start with batch_size=1 and use gradient accumulation"

These aren't necessarily wrong, but need context!
Should explain: "For LLM fine-tuning, unlike general training..."
```

### 5. PERFORMANCE CLAIM CONSISTENCY

Verify benchmark claims match across modules:

| Claim | Module 1.1 | Module 3.2 | Module 3.3 | Match? |
|-------|------------|------------|------------|--------|
| 8B Q4 decode | 45 tok/s | 45 tok/s | 48 tok/s | ‚ùå |
| 70B memory | 35 GB | 35 GB | 40 GB | ‚ùå |

### 6. CONCEPT BUILDUP VALIDATION

Verify advanced concepts properly build on foundations:

**Example Check: LoRA (Module 3.1)**
- Requires SVD understanding ‚Üí Taught in Module 1.4? ‚úì
- Requires adapter concept ‚Üí Taught in Module 2.5? ‚úì
- Requires quantization for QLoRA ‚Üí Taught in Module 3.2? ‚úó (3.2 is after 3.1!)

**Issue:** Module 3.1 covers QLoRA but Module 3.2 (Quantization) comes AFTER!
**Fix:** Either reorder modules or add quantization basics to 3.1

### 7. TOOL/LIBRARY VERSION ALIGNMENT

Check that library usage is consistent:

| Library | Module 2.5 | Module 3.1 | Module 4.3 | Aligned? |
|---------|------------|------------|------------|----------|
| transformers | 4.40+ | 4.40+ | 4.36+ | ‚ö†Ô∏è |
| peft | 0.10+ | 0.11+ | - | ‚ö†Ô∏è |
| torch | 2.3+ | 2.2+ | 2.3+ | ‚ö†Ô∏è |

### 8. EXAMPLE MODEL CONSISTENCY

Check that example models are used consistently:

| Task | Module 2.5 | Module 3.1 | Module 3.3 | Consistent? |
|------|------------|------------|------------|-------------|
| 8B example | Qwen3-8B | Qwen3-8B | qwen3:8b | Check format |
| 32B example | Qwen3-32B | Qwen3-32B | qwen3:32b | ‚úÖ Consistent |

</task>

<output_format>
## Output Structure

---

# Cross-Module Coherency Audit Report

**Modules Reviewed:** [List all modules analyzed]
**Dependency Chains Checked:** [Count]
**Cross-References Found:** [Count]
**Inconsistencies Found:** [Count]
**Curriculum Version:** v2.0

---

## üìä Summary by Category

| Category | Issues Found | Severity |
|----------|--------------|----------|
| Prerequisite Violations | X | üî¥/üü°/üü¢ |
| Terminology Drift | X | üî¥/üü°/üü¢ |
| Code Pattern Inconsistency | X | üî¥/üü°/üü¢ |
| Conflicting Recommendations | X | üî¥/üü°/üü¢ |
| Performance Claim Drift | X | üî¥/üü°/üü¢ |
| Concept Buildup Gaps | X | üî¥/üü°/üü¢ |
| Version Conflicts | X | üî¥/üü°/üü¢ |
| Example Model Inconsistency | X | üî¥/üü°/üü¢ |
| **TOTAL** | **X** | |

---

## üî¥ CRITICAL: Prerequisite Violations

### Issue P1: [Title]

**The Chain:**
```
Module [X] claims prerequisite: Module [Y]
Module [X] uses concept: [concept]
Module [Y] does NOT teach: [concept]
Actually taught in: Module [Z]
```

**Impact:** Students will be confused because they haven't learned [concept] yet.

**Evidence:**

In Module [X]:
```markdown
[Quote using the concept]
```

Module [Y] table of contents:
```markdown
[Show that concept is NOT there]
```

**Fix Options:**
1. Add [concept] to Module [Y] prerequisites section
2. Change prerequisite from [Y] to [Z]
3. Add brief explanation of [concept] in Module [X]

**Recommended:** [Option and why]

---

## üî¥ CRITICAL: Concept Buildup Gaps

### Issue C1: QLoRA Before Quantization

**The Problem:**
- Module 3.1 (LLM Fine-Tuning) teaches QLoRA
- QLoRA requires understanding of 4-bit quantization
- Module 3.2 (Quantization) comes AFTER Module 3.1

**Evidence:**

Module 3.1 says:
```markdown
"Load the model in 4-bit using bitsandbytes..."
```

But quantization concepts (why 4-bit works, NF4 format, etc.) aren't taught until 3.2!

**Fix Options:**
1. Reorder: Move quantization basics before fine-tuning
2. Add: Include "Quantization Primer" section in 3.1
3. Split: Create 3.1a (SFT/LoRA) and 3.1b (QLoRA, after 3.2)

**Recommended:** Add a "Quantization Basics" section to 3.1 covering just enough for QLoRA, with forward reference to 3.2 for full treatment.

---

## üü° MEDIUM: Terminology Drift

### Issue T1: [Concept] Described Differently

**First Introduction (Module [X]):**
```markdown
"[Definition 1]"
```

**Later Usage (Module [Y]):**
```markdown
"[Definition 2 - different!]"
```

**Why It's Confusing:**
[Explanation of how a learner would be confused]

**Fix:** Standardize to:
```markdown
"[Consistent definition]"
```

Update in files:
- [ ] Module [X] / [file]
- [ ] Module [Y] / [file]

---

## üü° MEDIUM: Code Pattern Inconsistency

### Issue CP1: Model Loading Pattern Differs

**Module 2.5 Pattern:**
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen3-8B",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
```

**Module 3.1 Pattern (Different!):**
```python
from transformers import LlamaForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen3-8B",
    load_in_4bit=True  # Different parameters!
)
```

**Issue:** Module 3.1 uses specific class instead of Auto class, different parameters.

**Fix:** Standardize pattern:
```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# Standard pattern for all modules
bnb_config = BitsAndBytesConfig(load_in_4bit=True, ...)  # When quantizing

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,  # Optional
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
```

---

## üü° MEDIUM: Conflicting Recommendations

### Issue CR1: Batch Size Recommendations

**Module 2.1 (PyTorch Basics):**
```markdown
"Use batch_size=32 for optimal GPU utilization on DGX Spark"
```

**Module 3.1 (LLM Fine-Tuning):**
```markdown
"Use batch_size=1 with gradient_accumulation_steps=16"
```

**The Conflict:** These seem contradictory without context.

**Fix:** Add context to Module 3.1:
```markdown
"Unlike general deep learning (where batch_size=32 is common), 
LLM fine-tuning requires smaller batches due to model size. 
Use batch_size=1-4 with gradient accumulation to simulate 
larger effective batches while fitting in memory."
```

---

## üü¢ LOW: Performance Claim Variations

### Issue PC1: 8B Decode Speed Varies

| Location | Claim |
|----------|-------|
| Module 1.1 / README | 45 tok/s |
| Module 3.2 / 02-notebook | 45 tok/s |
| Module 3.3 / 01-notebook | 48 tok/s |

**Fix:** Standardize to "~45 tok/s" with note that actual performance varies.

---

## üìã DEPENDENCY CHAIN VALIDATION

### Domain 1 ‚Üí Domain 2

| Source Module | Target Module | Expected Concepts | Verified? |
|---------------|---------------|-------------------|-----------|
| 1.3 CUDA Python | 2.1 PyTorch | GPU memory, kernels | ‚úÖ/‚ùå |
| 1.4 Math | 2.3 Transformers | Chain rule, matrices | ‚úÖ/‚ùå |
| 1.5 Neural Nets | 2.1 PyTorch | Layers, activations | ‚úÖ/‚ùå |
| 1.6 Classical ML | 3.5 RAG | Embeddings concept | ‚úÖ/‚ùå |

### Domain 2 ‚Üí Domain 3

| Source Module | Target Module | Expected Concepts | Verified? |
|---------------|---------------|-------------------|-----------|
| 2.3 Transformers | 3.1 Fine-Tuning | Attention, architecture | ‚úÖ/‚ùå |
| 2.4 Efficient Arch | 3.3 Deployment | Mamba/MoE inference | ‚úÖ/‚ùå |
| 2.5 Hugging Face | 3.1 Fine-Tuning | Trainer, PEFT | ‚úÖ/‚ùå |
| 2.5 Hugging Face | 3.5 RAG | Embeddings, tokenizers | ‚úÖ/‚ùå |

### Domain 3 ‚Üí Domain 4

| Source Module | Target Module | Expected Concepts | Verified? |
|---------------|---------------|-------------------|-----------|
| 3.1 Fine-Tuning | 4.3 MLOps | Tracking, evaluation | ‚úÖ/‚ùå |
| 3.2 Quantization | 4.4 Deployment | TensorRT, formats | ‚úÖ/‚ùå |
| 3.5 RAG | 4.1 Multimodal | Multimodal RAG | ‚úÖ/‚ùå |
| 3.6 Agents | 4.2 Safety | Agent guardrails | ‚úÖ/‚ùå |

---

## üìã TERMINOLOGY TRACKING

| Term | Introduced | Definition | Used In | Consistent? |
|------|------------|------------|---------|-------------|
| Unified memory | 1.1 | "CPU+GPU shared 128GB" | All | ‚úÖ/‚ùå |
| Attention | 2.3 | "[definition]" | 2.4, 3.1, 3.3 | ‚úÖ/‚ùå |
| LoRA | 2.5 | "[definition]" | 2.6, 3.1 | ‚úÖ/‚ùå |
| Embeddings | 1.6 | "[definition]" | 2.3, 3.5 | ‚úÖ/‚ùå |
| NVFP4 | 3.2 | "[definition]" | 3.3, 4.4 | ‚úÖ/‚ùå |
| Prefill/Decode | 1.1 | "[definition]" | 3.2, 3.3 | ‚úÖ/‚ùå |

---

## üìã EXAMPLE MODEL CONSISTENCY

| Model Reference | 2.5 | 3.1 | 3.2 | 3.3 | 4.1 | Consistent? |
|-----------------|-----|-----|-----|-----|-----|-------------|
| 8B example | Qwen3-8B | Qwen3-8B | Qwen3-8B | qwen3:8b | Qwen3-8B | ‚úÖ |
| 32B example | Qwen3-32B | Qwen3-32B | Qwen3-32B | qwen3:32b | Qwen3-32B | ‚úÖ |
| Vision model | - | - | - | - | Qwen3-VL-8B | N/A |

**Note:** HuggingFace format (Qwen3-8B) vs Ollama format (qwen3:8b) is expected.

---

## üîß BULK FIX RECOMMENDATIONS

### Fix Category 1: Module Ordering Issue
**Problem:** QLoRA in 3.1 before Quantization in 3.2

**Solution:** Add to Module 3.1 a "Quantization Primer" section:
```markdown
## Quick Quantization Background (for QLoRA)

Before we dive into QLoRA, you need to understand the basics of quantization.
We'll cover this in depth in Module 3.2, but here's what you need to know now:

- **4-bit quantization** reduces model memory by ~4x
- **NF4 format** is specifically designed for normally-distributed weights
- **bitsandbytes** library handles this automatically

For full details, see Module 3.2.
```

### Fix Category 2: Terminology Standardization
Create a `GLOSSARY.md` and reference it from all modules:

```markdown
# DGX Spark Curriculum Glossary

## A
**Attention** - A mechanism that computes relevance scores between tokens,
allowing the model to focus on relevant context. Introduced in Module 2.3.

## D
**Decode Speed** (tok/s) - Token generation speed during inference.
Measured as tokens per second. NOT "generation speed" or "output rate".

## P
**Prefill Speed** (tok/s) - Prompt processing speed during inference.
Measured as tokens per second. NOT "input processing" or "prompt speed".

[Continue for all key terms...]
```

### Fix Category 3: Code Pattern Standards
Create `templates/code_patterns.py` with standard patterns:

```python
# Standard model loading pattern
def load_model_standard(model_name, quantize=False):
    """Use this pattern in ALL modules."""
    from transformers import AutoModelForCausalLM, BitsAndBytesConfig
    
    config = None
    if quantize:
        config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )
    
    return AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=config,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
```

---

## ‚úÖ SIGN-OFF CHECKLIST

### Prerequisite Chains
- [ ] All Module 2 prerequisites from Module 1 verified
- [ ] All Module 3 prerequisites from Modules 1-2 verified
- [ ] All Module 4 prerequisites from Modules 1-3 verified
- [ ] No forward references to later modules

### Terminology
- [ ] All terms defined on first use
- [ ] Definitions consistent across modules
- [ ] Standard terms used (per v2.0 terminology table)

### Code Patterns
- [ ] Model loading pattern consistent
- [ ] Tokenizer usage consistent
- [ ] Training loop structure consistent
- [ ] Memory cleanup consistent

### Values
- [ ] Hardware specs consistent
- [ ] Performance benchmarks consistent
- [ ] Memory estimates consistent
- [ ] Capacity matrix consistent

### Examples
- [ ] Example models consistent (or format differences explained)
- [ ] Ollama vs HuggingFace format noted appropriately

**Cross-Module Coherency Status:** [COHERENT / NEEDS FIXES]

---

*Audit by CrossModuleAuditor SPARK - Curriculum v2.0*
</output_format>

<module_content>
[PASTE CONTENT FROM MULTIPLE MODULES HERE]

For effective cross-module review, include:
1. README.md from each module being compared
2. First notebook from each module (establishes patterns)
3. Any notebooks that reference concepts from other modules
4. Any notebooks that introduce key terminology

Minimum recommendation:
- All Domain READMEs
- At least one notebook from each domain
- Specifically: 1.1, 2.3, 2.5, 3.1, 3.2, 3.5, 4.2 (high-dependency modules)
</module_content>

<specific_checks>
## Priority Cross-Module Checks

### Check 1: QLoRA ‚Üí Quantization Ordering
Module 3.1 introduces QLoRA which requires quantization knowledge.
Module 3.2 teaches quantization but comes AFTER 3.1.
- Does 3.1 include sufficient quantization background?
- Or should module order change?

### Check 2: PEFT Introduction Chain
PEFT is introduced in Module 2.5 and heavily used in 3.1.
- Is the introduction in 2.5 sufficient for 3.1 usage?
- Are LoRA hyperparameter recommendations consistent?

### Check 3: Attention Definition Consistency
Attention is core concept used in many modules.
- First definition in 2.3: Is it complete?
- Usage in 2.4, 3.1, 3.3: Does it align?

### Check 4: RAG ‚Üí Embeddings Chain
RAG (3.5) requires embedding understanding from 1.6 and 2.5.
- Is the embeddings introduction in 1.6 adequate?
- Does 2.5 expand appropriately for 3.5 needs?

### Check 5: Agent Safety Chain
Module 4.2 (Safety) builds on 3.6 (Agents).
- Are agent patterns in 3.6 safety-aware?
- Does 4.2 reference 3.6 patterns correctly?

### Check 6: Deployment ‚Üí Quantization Chain
Module 3.3 (Deployment) uses quantized models from 3.2.
- Are quantization format names consistent (NVFP4, GGUF, etc.)?
- Do performance claims match?

### Check 7: Classical ML ‚Üí RAG Connection
Module 1.6 introduces embeddings for use in 3.5 RAG.
- Is the embedding introduction sufficient?
- Are vector similarity concepts covered?

### Check 8: Test-Time Compute Dependencies
Module 3.4 requires understanding from 2.3 (transformers) and 3.3 (deployment).
- Are CoT concepts properly built up?
- Do reasoning model examples align with earlier transformer concepts?
</specific_checks>

<instructions>
Analyze the provided content from MULTIPLE modules for cross-module coherency:

1. **Map dependencies** - Which modules depend on which?
2. **Track concepts** - Where is each concept introduced vs used?
3. **Compare patterns** - Do code patterns match across modules?
4. **Verify recommendations** - Are suggestions consistent or contradictory?
5. **Check values** - Do numbers (benchmarks, memory) match?
6. **Validate ordering** - Are prerequisites actually taught before use?

For each cross-module issue:
- Show the CONFLICT between modules
- Explain WHY it confuses the learning path
- Provide SPECIFIC fixes for each affected module

Prioritize by learning impact:
- üî¥ CRITICAL: Learner will be blocked or deeply confused
- üü° MEDIUM: Inconsistent but recoverable
- üü¢ LOW: Style/polish issues

Start your cross-module coherency audit now.
</instructions>
```

---

## QUICK CROSS-MODULE CHECKS

### Prerequisite Chain Validator

```
Validate that Module [TARGET] has valid prerequisites from Module [SOURCE]:

Check:
1. Every concept USED in [TARGET] that should come from [SOURCE]
2. Verify each concept IS actually taught in [SOURCE]
3. Flag any forward references (concepts used before taught)
4. Flag any missing links (concepts assumed but not in prerequisites)

Module [SOURCE] content:
[PASTE]

Module [TARGET] content:
[PASTE]

Output: Table of concepts with source verification.
```

### Terminology Drift Detector

```
Track this specific term across multiple modules:

Term: [TERM]

For each module, extract:
1. How the term is first introduced/defined
2. How it's used in context
3. Any synonyms or variations used

Then compare:
- Are definitions consistent?
- Are synonyms used interchangeably (bad) or distinguished (good)?
- Does the term evolve appropriately from simple to complex usage?

Module contents:
[PASTE RELEVANT SECTIONS FROM MULTIPLE MODULES]

Output: Term evolution map with inconsistencies flagged.
```

### Code Pattern Consistency Checker

```
Check that this specific code pattern is used consistently:

Pattern: [MODEL LOADING / TOKENIZER USAGE / TRAINING LOOP / etc.]

Standard pattern should be:
```python
[PASTE STANDARD PATTERN]
```

Check each module's implementation:
- Does it match the standard?
- Are deviations justified and explained?
- Would a student be confused by differences?

Module code samples:
[PASTE RELEVANT CODE FROM MULTIPLE MODULES]

Output: Pattern comparison table with deviations highlighted.
```

### Performance Claim Reconciler

```
Reconcile performance claims across modules:

Claim type: [BENCHMARK / MEMORY / SPEED]
Subject: [8B MODEL / 70B MODEL / etc.]

For each module, extract the claimed value.
Flag any that don't match the standard:

Standard values (from CURRICULUM_V2.md):
- 8B Q4 decode: ~45 tok/s
- 8B Q4 prefill: ~3,000 tok/s
- 70B Q4 memory: ~35 GB
[etc.]

Module claims:
[PASTE RELEVANT CLAIMS FROM MULTIPLE MODULES]

Output: Reconciliation table with mismatches and recommended standard.
```

---

## KNOWN POTENTIAL ISSUES TO CHECK

Based on curriculum analysis, these are **high-probability issues** to watch for:

### 1. QLoRA Before Quantization (Critical)
- Module 3.1 teaches QLoRA
- Module 3.2 teaches quantization
- QLoRA requires quantization understanding
- **Check:** Does 3.1 have sufficient quantization primer?

### 2. Embeddings Introduction Adequacy
- Module 1.6 introduces embeddings for classical ML
- Module 3.5 uses embeddings heavily for RAG
- **Check:** Is 1.6's introduction sufficient for 3.5's needs?

### 3. LoRA Hyperparameters
- Module 2.5 introduces LoRA with PEFT
- Module 2.6 uses LoRA for diffusion
- Module 3.1 uses LoRA extensively for LLMs
- **Check:** Are rank/alpha recommendations consistent?

### 4. Attention Mechanism Definitions
- Module 2.3 defines attention mathematically
- Module 2.4 discusses efficient attention (Flash, GQA)
- Module 3.3 discusses KV cache for attention
- **Check:** Do all definitions align and build on each other?

### 5. Mamba/SSM Terminology
- Module 2.4 introduces Mamba as "State Space Models"
- Later modules may reference "Mamba" or "SSM"
- **Check:** Is terminology consistent? Is SSM always expanded first?

### 6. Container Tag Consistency
- All modules should use `25.11-py3`
- Some may have been written with older tags
- **Check:** Grep all Docker commands for tag consistency

### 7. Ollama Web UI vs Ollama
- Testing platform should be "Ollama Web UI" consistently
- Early modules might just say "Ollama"
- **Check:** Is testing platform named consistently?

---

**Created for:** DGX Spark AI Curriculum v2.0
**Focus:** Cross-module coherency and learning path validation
**Companion to:** COHERENCY_REVIEW_PROMPT.md, content-prompt.md