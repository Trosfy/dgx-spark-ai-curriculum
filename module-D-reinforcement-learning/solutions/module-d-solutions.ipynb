{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module D: Reinforcement Learning - Complete Solutions\n",
    "\n",
    "This notebook contains solutions to all exercises and challenges from Module D.\n",
    "\n",
    "**Contents:**\n",
    "1. Lab D.1 Solutions: MDP and Value Iteration\n",
    "2. Lab D.2 Solutions: Q-Learning\n",
    "3. Lab D.3 Solutions: Deep Q-Networks\n",
    "4. Lab D.4 Solutions: Policy Gradients and PPO\n",
    "5. Lab D.5 Solutions: RLHF Concepts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "import random\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except ImportError:\n",
    "    !pip install gymnasium -q\n",
    "    import gymnasium as gym\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lab D.1 Solutions: MDP and Value Iteration\n",
    "\n",
    "### Exercise: Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldMDP:\n",
    "    \"\"\"Simple 4x4 grid world.\"\"\"\n",
    "    \n",
    "    def __init__(self, grid_size: int = 4, slip_prob: float = 0.0):\n",
    "        self.grid_size = grid_size\n",
    "        self.n_states = grid_size ** 2\n",
    "        self.n_actions = 4\n",
    "        self.slip_prob = slip_prob\n",
    "        self.start_state = 0\n",
    "        self.goal_state = self.n_states - 1\n",
    "        self.gamma = 0.99\n",
    "        self.action_deltas = {0: (-1, 0), 1: (0, 1), 2: (1, 0), 3: (0, -1)}\n",
    "    \n",
    "    def state_to_pos(self, state):\n",
    "        return (state // self.grid_size, state % self.grid_size)\n",
    "    \n",
    "    def pos_to_state(self, row, col):\n",
    "        return row * self.grid_size + col\n",
    "    \n",
    "    def get_next_state(self, state, action):\n",
    "        row, col = self.state_to_pos(state)\n",
    "        dr, dc = self.action_deltas[action]\n",
    "        new_row = max(0, min(self.grid_size - 1, row + dr))\n",
    "        new_col = max(0, min(self.grid_size - 1, col + dc))\n",
    "        return self.pos_to_state(new_row, new_col)\n",
    "    \n",
    "    def get_reward(self, state, action, next_state):\n",
    "        if next_state == self.goal_state:\n",
    "            return 1.0\n",
    "        return -0.01\n",
    "    \n",
    "    def get_transition_probs(self, state, action):\n",
    "        if self.slip_prob == 0:\n",
    "            return {self.get_next_state(state, action): 1.0}\n",
    "        probs = {}\n",
    "        for a in range(self.n_actions):\n",
    "            next_s = self.get_next_state(state, a)\n",
    "            if a == action:\n",
    "                prob = 1.0 - self.slip_prob + self.slip_prob / self.n_actions\n",
    "            else:\n",
    "                prob = self.slip_prob / self.n_actions\n",
    "            probs[next_s] = probs.get(next_s, 0) + prob\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Policy Iteration\n",
    "\n",
    "def policy_evaluation(mdp, policy, threshold=1e-6):\n",
    "    \"\"\"\n",
    "    Evaluate a policy to get V^œÄ.\n",
    "    \n",
    "    Given a fixed policy, compute the value of each state under that policy.\n",
    "    \"\"\"\n",
    "    V = np.zeros(mdp.n_states)\n",
    "    \n",
    "    while True:\n",
    "        V_new = np.zeros(mdp.n_states)\n",
    "        \n",
    "        for s in range(mdp.n_states):\n",
    "            if s == mdp.goal_state:\n",
    "                continue  # Terminal state\n",
    "            \n",
    "            # Use the policy's action (not max!)\n",
    "            a = policy[s]\n",
    "            transitions = mdp.get_transition_probs(s, a)\n",
    "            \n",
    "            # Expected value under policy\n",
    "            V_new[s] = sum(\n",
    "                prob * (mdp.get_reward(s, a, ns) + mdp.gamma * V[ns])\n",
    "                for ns, prob in transitions.items()\n",
    "            )\n",
    "        \n",
    "        # Check convergence\n",
    "        if np.max(np.abs(V_new - V)) < threshold:\n",
    "            break\n",
    "        V = V_new\n",
    "    \n",
    "    return V\n",
    "\n",
    "\n",
    "def policy_improvement(mdp, V):\n",
    "    \"\"\"\n",
    "    Improve policy greedily with respect to V.\n",
    "    \n",
    "    For each state, find the action that maximizes expected value.\n",
    "    \"\"\"\n",
    "    policy = np.zeros(mdp.n_states, dtype=int)\n",
    "    \n",
    "    for s in range(mdp.n_states):\n",
    "        if s == mdp.goal_state:\n",
    "            continue\n",
    "        \n",
    "        # Find best action\n",
    "        action_values = []\n",
    "        for a in range(mdp.n_actions):\n",
    "            transitions = mdp.get_transition_probs(s, a)\n",
    "            q = sum(\n",
    "                prob * (mdp.get_reward(s, a, ns) + mdp.gamma * V[ns])\n",
    "                for ns, prob in transitions.items()\n",
    "            )\n",
    "            action_values.append(q)\n",
    "        \n",
    "        policy[s] = np.argmax(action_values)\n",
    "    \n",
    "    return policy\n",
    "\n",
    "\n",
    "def policy_iteration(mdp):\n",
    "    \"\"\"\n",
    "    Find optimal policy using Policy Iteration.\n",
    "    \n",
    "    Alternates between:\n",
    "    1. Policy Evaluation: Compute V^œÄ\n",
    "    2. Policy Improvement: Make policy greedy w.r.t. V^œÄ\n",
    "    \"\"\"\n",
    "    # Start with random policy\n",
    "    policy = np.random.randint(0, mdp.n_actions, size=mdp.n_states)\n",
    "    \n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        \n",
    "        # Policy Evaluation\n",
    "        V = policy_evaluation(mdp, policy)\n",
    "        \n",
    "        # Policy Improvement\n",
    "        new_policy = policy_improvement(mdp, V)\n",
    "        \n",
    "        # Check if policy changed\n",
    "        if np.array_equal(policy, new_policy):\n",
    "            print(f\"‚úÖ Policy Iteration converged in {iteration} iterations!\")\n",
    "            break\n",
    "        \n",
    "        policy = new_policy\n",
    "    \n",
    "    return V, policy\n",
    "\n",
    "\n",
    "# Test it\n",
    "mdp = GridWorldMDP(grid_size=4)\n",
    "V_pi, policy_pi = policy_iteration(mdp)\n",
    "\n",
    "print(\"\\nOptimal Value Function:\")\n",
    "print(V_pi.reshape(4, 4).round(3))\n",
    "\n",
    "action_names = ['Up', 'Right', 'Down', 'Left']\n",
    "print(\"\\nOptimal Policy:\")\n",
    "print(np.array([action_names[a] for a in policy_pi]).reshape(4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Obstacle Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Grid with Obstacles\n",
    "\n",
    "class ObstacleGridMDP(GridWorldMDP):\n",
    "    \"\"\"\n",
    "    Grid world with obstacles (impassable cells).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, grid_size: int = 6, obstacles: List[int] = None):\n",
    "        super().__init__(grid_size, slip_prob=0.0)\n",
    "        \n",
    "        # Default obstacles create a wall pattern\n",
    "        if obstacles is None:\n",
    "            self.obstacles = {8, 9, 14, 15, 20, 26, 27}\n",
    "        else:\n",
    "            self.obstacles = set(obstacles)\n",
    "        \n",
    "        print(f\"Grid: {grid_size}x{grid_size}\")\n",
    "        print(f\"Obstacles at: {sorted(self.obstacles)}\")\n",
    "    \n",
    "    def get_next_state(self, state: int, action: int) -> int:\n",
    "        \"\"\"Modified to handle obstacles.\"\"\"\n",
    "        # Get intended next state\n",
    "        next_state = super().get_next_state(state, action)\n",
    "        \n",
    "        # If it's an obstacle, stay in current state (bounce off)\n",
    "        if next_state in self.obstacles:\n",
    "            return state\n",
    "        \n",
    "        return next_state\n",
    "    \n",
    "    def visualize(self, V=None, policy=None):\n",
    "        \"\"\"Visualize the grid with obstacles.\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        \n",
    "        # Draw grid\n",
    "        for i in range(self.grid_size + 1):\n",
    "            ax.axhline(y=i, color='black', linewidth=1)\n",
    "            ax.axvline(x=i, color='black', linewidth=1)\n",
    "        \n",
    "        # Color cells\n",
    "        if V is not None:\n",
    "            V_2d = V.reshape(self.grid_size, self.grid_size)\n",
    "            ax.imshow(V_2d, cmap='RdYlGn',\n",
    "                     extent=[0, self.grid_size, self.grid_size, 0])\n",
    "        \n",
    "        # Mark obstacles, start, goal\n",
    "        arrow_map = {0: '‚Üë', 1: '‚Üí', 2: '‚Üì', 3: '‚Üê'}\n",
    "        for s in range(self.n_states):\n",
    "            row, col = self.state_to_pos(s)\n",
    "            x, y = col + 0.5, row + 0.5\n",
    "            \n",
    "            if s in self.obstacles:\n",
    "                ax.add_patch(plt.Rectangle((col, row), 1, 1, color='black'))\n",
    "                ax.text(x, y, 'üß±', ha='center', va='center', fontsize=16)\n",
    "            elif s == self.goal_state:\n",
    "                ax.text(x, y, 'üéØ', ha='center', va='center', fontsize=20)\n",
    "            elif s == self.start_state:\n",
    "                ax.text(x, y, 'üöÄ', ha='center', va='center', fontsize=20)\n",
    "            elif policy is not None:\n",
    "                ax.text(x, y, arrow_map[policy[s]], ha='center', va='center',\n",
    "                       fontsize=20, fontweight='bold')\n",
    "        \n",
    "        ax.set_xlim(0, self.grid_size)\n",
    "        ax.set_ylim(self.grid_size, 0)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_title('Grid World with Obstacles')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Solve it\n",
    "obstacle_mdp = ObstacleGridMDP(grid_size=6)\n",
    "V_obs, policy_obs = policy_iteration(obstacle_mdp)\n",
    "obstacle_mdp.visualize(V_obs, policy_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lab D.2 Solutions: Q-Learning\n",
    "\n",
    "### Exercise: Hyperparameter Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Q-Learning with various learning rates\n",
    "\n",
    "def q_learning(env, n_episodes=2000, alpha=0.1, gamma=0.99,\n",
    "               epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "    \"\"\"\n",
    "    Tabular Q-learning implementation.\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    rewards_history = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            # Epsilon-greedy action selection\n",
    "            if np.random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Q-learning update\n",
    "            best_next = np.max(Q[next_state]) if not done else 0\n",
    "            td_target = reward + gamma * best_next\n",
    "            td_error = td_target - Q[state, action]\n",
    "            Q[state, action] += alpha * td_error\n",
    "            \n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rewards_history.append(total_reward)\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    \n",
    "    return Q, rewards_history\n",
    "\n",
    "\n",
    "# Compare learning rates\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "\n",
    "learning_rates = [0.01, 0.1, 0.5, 0.9]\n",
    "results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"Testing lr = {lr}...\")\n",
    "    Q, rewards = q_learning(env, n_episodes=1500, alpha=lr)\n",
    "    \n",
    "    # Evaluate\n",
    "    successes = 0\n",
    "    for _ in range(500):\n",
    "        state, _ = env.reset()\n",
    "        for _ in range(100):\n",
    "            action = np.argmax(Q[state])\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            if terminated or truncated:\n",
    "                successes += reward\n",
    "                break\n",
    "    \n",
    "    results[lr] = successes / 500\n",
    "    print(f\"  Success rate: {results[lr]:.1%}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(range(len(learning_rates)), list(results.values()), alpha=0.7)\n",
    "plt.xticks(range(len(learning_rates)), [str(lr) for lr in learning_rates])\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.title('Q-Learning: Effect of Learning Rate')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Double Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Double Q-Learning\n",
    "\n",
    "def double_q_learning(env, n_episodes=5000, alpha=0.1, gamma=0.99,\n",
    "                      epsilon=1.0, epsilon_decay=0.9995, epsilon_min=0.05):\n",
    "    \"\"\"\n",
    "    Double Q-learning to reduce overestimation bias.\n",
    "    \n",
    "    Uses two Q-tables:\n",
    "    - Q1 selects, Q2 evaluates (and vice versa)\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    Q1 = np.zeros((n_states, n_actions))\n",
    "    Q2 = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    rewards_history = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            # Epsilon-greedy using sum of Q-tables\n",
    "            if np.random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(Q1[state] + Q2[state])\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Double Q-learning update\n",
    "            if np.random.random() < 0.5:\n",
    "                # Update Q1, evaluate with Q2\n",
    "                if done:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    best_action = np.argmax(Q1[next_state])\n",
    "                    target = reward + gamma * Q2[next_state, best_action]\n",
    "                Q1[state, action] += alpha * (target - Q1[state, action])\n",
    "            else:\n",
    "                # Update Q2, evaluate with Q1\n",
    "                if done:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    best_action = np.argmax(Q2[next_state])\n",
    "                    target = reward + gamma * Q1[next_state, best_action]\n",
    "                Q2[state, action] += alpha * (target - Q2[state, action])\n",
    "            \n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rewards_history.append(total_reward)\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    \n",
    "    return (Q1 + Q2) / 2, rewards_history\n",
    "\n",
    "\n",
    "# Test on slippery FrozenLake\n",
    "slippery_env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "print(\"Training Double Q-Learning on Slippery FrozenLake...\")\n",
    "Q_double, rewards_double = double_q_learning(slippery_env, n_episodes=10000)\n",
    "\n",
    "# Evaluate\n",
    "successes = 0\n",
    "for _ in range(1000):\n",
    "    state, _ = slippery_env.reset()\n",
    "    for _ in range(100):\n",
    "        action = np.argmax(Q_double[state])\n",
    "        state, reward, terminated, truncated, _ = slippery_env.step(action)\n",
    "        if terminated or truncated:\n",
    "            successes += reward\n",
    "            break\n",
    "\n",
    "print(f\"\\nDouble Q-Learning Success Rate: {successes/10:.1%}\")\n",
    "\n",
    "slippery_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lab D.3 Solutions: Deep Q-Networks\n",
    "\n",
    "### Exercise: Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Double DQN\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            torch.FloatTensor(np.array(states)).to(device),\n",
    "            torch.LongTensor(actions).to(device),\n",
    "            torch.FloatTensor(rewards).to(device),\n",
    "            torch.FloatTensor(np.array(next_states)).to(device),\n",
    "            torch.FloatTensor(dones).to(device)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class DoubleDQNAgent:\n",
    "    \"\"\"\n",
    "    Double DQN Agent.\n",
    "    \n",
    "    Key difference from standard DQN:\n",
    "    - Use Q-network to SELECT best action\n",
    "    - Use target network to EVALUATE that action\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99,\n",
    "                 buffer_size=50000, batch_size=64, target_update_freq=100):\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        \n",
    "        self.q_network = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_network = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.buffer = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_end = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.train_steps = 0\n",
    "    \n",
    "    def select_action(self, state, training=True):\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            return self.q_network(state_tensor).argmax().item()\n",
    "    \n",
    "    def train_step(self):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Double DQN target\n",
    "        with torch.no_grad():\n",
    "            # SELECT best action using Q-network\n",
    "            best_actions = self.q_network(next_states).argmax(dim=1)\n",
    "            # EVALUATE using target network\n",
    "            next_q = self.target_network(next_states).gather(1, best_actions.unsqueeze(1)).squeeze(1)\n",
    "            target_q = rewards + self.gamma * next_q * (1 - dones)\n",
    "        \n",
    "        loss = F.mse_loss(current_q, target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.train_steps += 1\n",
    "        \n",
    "        if self.train_steps % self.target_update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "# Train Double DQN on CartPole\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "agent = DoubleDQNAgent(\n",
    "    state_dim=env.observation_space.shape[0],\n",
    "    action_dim=env.action_space.n\n",
    ")\n",
    "\n",
    "print(\"Training Double DQN on CartPole...\")\n",
    "rewards_history = []\n",
    "\n",
    "for episode in range(300):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(500):\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        agent.buffer.push(state, action, reward, next_state, done)\n",
    "        agent.train_step()\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    rewards_history.append(total_reward)\n",
    "    \n",
    "    if (episode + 1) % 50 == 0:\n",
    "        avg = np.mean(rewards_history[-50:])\n",
    "        print(f\"Episode {episode + 1}: Avg Reward = {avg:.1f}\")\n",
    "\n",
    "print(f\"\\nFinal average (last 50): {np.mean(rewards_history[-50:]):.1f}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lab D.4 Solutions: Policy Gradients and PPO\n",
    "\n",
    "### Complete PPO Implementation with All Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Full PPO Implementation\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.shared(x)\n",
    "        return self.actor(features), self.critic(features)\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99,\n",
    "                 gae_lambda=0.95, clip_epsilon=0.2, value_coef=0.5,\n",
    "                 entropy_coef=0.01, n_epochs=10, batch_size=64):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.network = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        \n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            probs, value = self.network(state_tensor)\n",
    "        \n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        self.states.append(state)\n",
    "        self.actions.append(action.item())\n",
    "        self.log_probs.append(dist.log_prob(action).item())\n",
    "        self.values.append(value.item())\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def store(self, reward, done):\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def compute_gae(self, next_value):\n",
    "        values = self.values + [next_value]\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        \n",
    "        for t in reversed(range(len(self.rewards))):\n",
    "            delta = self.rewards[t] + self.gamma * values[t+1] * (1 - self.dones[t]) - values[t]\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1 - self.dones[t]) * gae\n",
    "            advantages.insert(0, gae)\n",
    "        \n",
    "        advantages = np.array(advantages)\n",
    "        returns = advantages + np.array(self.values)\n",
    "        \n",
    "        return advantages, returns\n",
    "    \n",
    "    def update(self, next_state):\n",
    "        with torch.no_grad():\n",
    "            next_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "            _, next_value = self.network(next_tensor)\n",
    "            next_value = next_value.item()\n",
    "        \n",
    "        advantages, returns = self.compute_gae(next_value)\n",
    "        \n",
    "        states = torch.FloatTensor(np.array(self.states)).to(device)\n",
    "        actions = torch.LongTensor(self.actions).to(device)\n",
    "        old_log_probs = torch.FloatTensor(self.log_probs).to(device)\n",
    "        advantages = torch.FloatTensor(advantages).to(device)\n",
    "        returns = torch.FloatTensor(returns).to(device)\n",
    "        \n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        for _ in range(self.n_epochs):\n",
    "            indices = np.random.permutation(len(states))\n",
    "            \n",
    "            for start in range(0, len(states), self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                idx = indices[start:end]\n",
    "                \n",
    "                probs, values = self.network(states[idx])\n",
    "                values = values.squeeze()\n",
    "                \n",
    "                dist = Categorical(probs)\n",
    "                new_log_probs = dist.log_prob(actions[idx])\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                ratio = (new_log_probs - old_log_probs[idx]).exp()\n",
    "                \n",
    "                surr1 = ratio * advantages[idx]\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages[idx]\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                value_loss = F.mse_loss(values, returns[idx])\n",
    "                \n",
    "                loss = policy_loss + self.value_coef * value_loss - self.entropy_coef * entropy\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "        \n",
    "        # Clear buffers\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "\n",
    "# Train PPO on LunarLander\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "agent = PPO(\n",
    "    state_dim=env.observation_space.shape[0],\n",
    "    action_dim=env.action_space.n\n",
    ")\n",
    "\n",
    "print(\"Training PPO on LunarLander...\")\n",
    "rewards_history = []\n",
    "episodes = 0\n",
    "rollout_length = 2048\n",
    "state, _ = env.reset()\n",
    "current_reward = 0\n",
    "\n",
    "while episodes < 500:\n",
    "    for _ in range(rollout_length):\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        agent.store(reward, done)\n",
    "        current_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            rewards_history.append(current_reward)\n",
    "            episodes += 1\n",
    "            current_reward = 0\n",
    "            state, _ = env.reset()\n",
    "            \n",
    "            if episodes % 50 == 0:\n",
    "                avg = np.mean(rewards_history[-50:])\n",
    "                print(f\"Episode {episodes}: Avg Reward = {avg:.1f}\")\n",
    "            \n",
    "            if episodes >= 500:\n",
    "                break\n",
    "        else:\n",
    "            state = next_state\n",
    "    \n",
    "    if len(agent.states) > 0:\n",
    "        agent.update(state)\n",
    "\n",
    "print(f\"\\nFinal average (last 50): {np.mean(rewards_history[-50:]):.1f}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lab D.5 Solutions: RLHF Concepts\n",
    "\n",
    "### DPO Loss Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: DPO Loss (Conceptual Implementation)\n",
    "\n",
    "def dpo_loss(\n",
    "    policy_chosen_logps: torch.Tensor,\n",
    "    policy_rejected_logps: torch.Tensor,\n",
    "    reference_chosen_logps: torch.Tensor,\n",
    "    reference_rejected_logps: torch.Tensor,\n",
    "    beta: float = 0.1\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Direct Preference Optimization (DPO) loss.\n",
    "    \n",
    "    DPO optimizes preferences directly without a reward model.\n",
    "    \n",
    "    Loss = -log(sigmoid(beta * (log_ratio_chosen - log_ratio_rejected)))\n",
    "    \n",
    "    where log_ratio = log(policy(y|x)) - log(reference(y|x))\n",
    "    \n",
    "    Args:\n",
    "        policy_chosen_logps: Log probs of chosen responses under policy\n",
    "        policy_rejected_logps: Log probs of rejected responses under policy\n",
    "        reference_chosen_logps: Log probs of chosen under reference (frozen)\n",
    "        reference_rejected_logps: Log probs of rejected under reference\n",
    "        beta: Temperature parameter (higher = more conservative)\n",
    "    \n",
    "    Returns:\n",
    "        DPO loss (scalar)\n",
    "    \"\"\"\n",
    "    # Compute log ratios\n",
    "    chosen_log_ratio = policy_chosen_logps - reference_chosen_logps\n",
    "    rejected_log_ratio = policy_rejected_logps - reference_rejected_logps\n",
    "    \n",
    "    # DPO loss\n",
    "    logits = beta * (chosen_log_ratio - rejected_log_ratio)\n",
    "    loss = -F.logsigmoid(logits).mean()\n",
    "    \n",
    "    # Useful metrics\n",
    "    with torch.no_grad():\n",
    "        chosen_rewards = beta * chosen_log_ratio\n",
    "        rejected_rewards = beta * rejected_log_ratio\n",
    "        reward_margin = (chosen_rewards - rejected_rewards).mean()\n",
    "        accuracy = (chosen_log_ratio > rejected_log_ratio).float().mean()\n",
    "    \n",
    "    return loss, {\n",
    "        'reward_margin': reward_margin.item(),\n",
    "        'accuracy': accuracy.item()\n",
    "    }\n",
    "\n",
    "\n",
    "# Demo\n",
    "batch_size = 8\n",
    "\n",
    "# Simulated log probabilities\n",
    "policy_chosen = torch.randn(batch_size) - 1  # Log probs are negative\n",
    "policy_rejected = torch.randn(batch_size) - 1.5\n",
    "ref_chosen = torch.randn(batch_size) - 1\n",
    "ref_rejected = torch.randn(batch_size) - 1\n",
    "\n",
    "loss, metrics = dpo_loss(policy_chosen, policy_rejected, ref_chosen, ref_rejected)\n",
    "\n",
    "print(\"DPO Loss Demo:\")\n",
    "print(f\"  Loss: {loss.item():.4f}\")\n",
    "print(f\"  Reward Margin: {metrics['reward_margin']:.4f}\")\n",
    "print(f\"  Accuracy: {metrics['accuracy']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This solutions notebook covered:\n",
    "\n",
    "1. **Lab D.1**: Policy Iteration and Obstacle Grid Worlds\n",
    "2. **Lab D.2**: Q-Learning hyperparameter tuning and Double Q-Learning\n",
    "3. **Lab D.3**: Double DQN implementation\n",
    "4. **Lab D.4**: Complete PPO implementation with GAE\n",
    "5. **Lab D.5**: DPO loss for RLHF alternatives\n",
    "\n",
    "Key takeaways:\n",
    "- RL is about learning from interaction to maximize reward\n",
    "- Value-based methods (Q-learning, DQN) learn action values\n",
    "- Policy-based methods (REINFORCE, PPO) directly optimize the policy\n",
    "- PPO is the backbone of RLHF for training LLMs\n",
    "- DPO provides a simpler alternative to full RLHF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
