{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab C.1: TransformerLens Setup - SOLUTIONS\n",
    "\n",
    "This notebook contains solutions to all exercises from Lab C.1.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformer_lens import HookedTransformer\n",
    "import plotly.express as px\n",
    "import gc\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Different Prompts\n",
    "\n",
    "Analyze different prompts: \"The capital of Germany is\", \"The opposite of hot is\", \"Einstein developed the theory of\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Analyze multiple prompts\n",
    "\n",
    "prompts = [\n",
    "    \"The capital of Germany is\",\n",
    "    \"The opposite of hot is\",\n",
    "    \"Einstein developed the theory of\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Tokenize and run\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    logits, cache = model.run_with_cache(tokens)\n",
    "    \n",
    "    # Get top predictions\n",
    "    probs = torch.softmax(logits[0, -1, :], dim=-1)\n",
    "    top_probs, top_indices = torch.topk(probs, 5)\n",
    "    \n",
    "    print(\"\\nTop 5 predictions:\")\n",
    "    for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
    "        token = model.tokenizer.decode(idx.item())\n",
    "        print(f\"  {i+1}. '{token}': {prob.item():.2%}\")\n",
    "    \n",
    "    # Clean up\n",
    "    del cache\n",
    "\n",
    "# Discussion:\n",
    "# - \"The capital of Germany is\" -> Should predict \" Berlin\"\n",
    "# - \"The opposite of hot is\" -> Should predict \" cold\"\n",
    "# - \"Einstein developed the theory of\" -> Should predict \" relativity\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Attention Pattern Analysis\n",
    "\n",
    "For the IOI prompt, find heads that attend to \"John\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Find heads attending to John\n",
    "\n",
    "prompt = \"John and Mary went to the store. John gave a book to\"\n",
    "tokens = model.to_tokens(prompt)\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "\n",
    "# Print tokens to find John's position\n",
    "token_strs = model.to_str_tokens(tokens)\n",
    "print(\"Tokens:\")\n",
    "for i, t in enumerate(token_strs):\n",
    "    print(f\"  {i}: '{t}'\")\n",
    "\n",
    "# John appears at position 0 (first occurrence) and position 9 (second occurrence)\n",
    "john_pos_1 = 0  # First John\n",
    "john_pos_2 = 9  # Second John\n",
    "last_pos = -1   # Final position\n",
    "\n",
    "# Get attention from last position to first John\n",
    "attention_to_john = np.zeros((model.cfg.n_layers, model.cfg.n_heads))\n",
    "\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    pattern = cache[\"pattern\", layer][0]  # [n_heads, seq, seq]\n",
    "    attention_to_john[layer, :] = pattern[:, last_pos, john_pos_1].detach().cpu().numpy()\n",
    "\n",
    "# Visualize\n",
    "fig = px.imshow(\n",
    "    attention_to_john,\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\", \"color\": \"Attention\"},\n",
    "    color_continuous_scale=\"Blues\",\n",
    "    title=f\"Attention from last position to 'John' (position {john_pos_1})\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Find top heads\n",
    "print(\"\\nTop 5 heads attending to first 'John':\")\n",
    "flat = attention_to_john.flatten()\n",
    "top_idx = np.argsort(flat)[-5:][::-1]\n",
    "for idx in top_idx:\n",
    "    layer = idx // model.cfg.n_heads\n",
    "    head = idx % model.cfg.n_heads\n",
    "    print(f\"  L{layer}H{head}: {attention_to_john[layer, head]:.3f}\")\n",
    "\n",
    "del cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Layer Comparison\n",
    "\n",
    "Compare logit lens between math (\"1 + 1 =\") and factual (\"Eiffel Tower\") knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Compare different types of knowledge\n",
    "\n",
    "def logit_lens_first_correct(model, prompt, expected_answer):\n",
    "    \"\"\"Find at which layer the correct answer first appears.\"\"\"\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    logits, cache = model.run_with_cache(tokens)\n",
    "    \n",
    "    W_U = model.W_U\n",
    "    answer_token = model.to_single_token(expected_answer)\n",
    "    \n",
    "    results = []\n",
    "    for layer in range(model.cfg.n_layers + 1):\n",
    "        if layer == 0:\n",
    "            resid = cache[\"resid_pre\", 0][0, -1, :]\n",
    "        else:\n",
    "            resid = cache[\"resid_post\", layer - 1][0, -1, :]\n",
    "        \n",
    "        resid_normed = model.ln_final(resid)\n",
    "        logits_layer = resid_normed @ W_U\n",
    "        probs = torch.softmax(logits_layer, dim=-1)\n",
    "        \n",
    "        # Check if correct answer is top-1\n",
    "        top_token = logits_layer.argmax().item()\n",
    "        is_correct = (top_token == answer_token)\n",
    "        prob = probs[answer_token].item()\n",
    "        \n",
    "        results.append({\n",
    "            'layer': layer,\n",
    "            'is_correct': is_correct,\n",
    "            'prob': prob,\n",
    "            'top_pred': model.tokenizer.decode(top_token)\n",
    "        })\n",
    "    \n",
    "    del cache\n",
    "    \n",
    "    # Find first layer where correct\n",
    "    first_correct = None\n",
    "    for r in results:\n",
    "        if r['is_correct']:\n",
    "            first_correct = r['layer']\n",
    "            break\n",
    "    \n",
    "    return results, first_correct\n",
    "\n",
    "# Test both prompts\n",
    "math_prompt = \"1 + 1 =\"\n",
    "math_answer = \" 2\"\n",
    "\n",
    "factual_prompt = \"The Eiffel Tower is in\"\n",
    "factual_answer = \" Paris\"\n",
    "\n",
    "math_results, math_first = logit_lens_first_correct(model, math_prompt, math_answer)\n",
    "factual_results, factual_first = logit_lens_first_correct(model, factual_prompt, factual_answer)\n",
    "\n",
    "print(f\"Math prompt: '{math_prompt}'\")\n",
    "print(f\"  Expected: '{math_answer}'\")\n",
    "print(f\"  First correct at layer: {math_first}\")\n",
    "\n",
    "print(f\"\\nFactual prompt: '{factual_prompt}'\")\n",
    "print(f\"  Expected: '{factual_answer}'\")\n",
    "print(f\"  First correct at layer: {factual_first}\")\n",
    "\n",
    "# Plot comparison\n",
    "layers = [r['layer'] for r in math_results]\n",
    "math_probs = [r['prob'] for r in math_results]\n",
    "factual_probs = [r['prob'] for r in factual_results]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(layers, math_probs, 'b-o', label=f\"Math: '{math_prompt}' → '{math_answer}'\")\n",
    "plt.plot(layers, factual_probs, 'r-o', label=f\"Factual: '{factual_prompt}' → '{factual_answer}'\")\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Probability of Correct Answer')\n",
    "plt.title('Logit Lens: Math vs Factual Knowledge')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConclusion:\")\n",
    "print(\"Math knowledge often emerges later in the network (layers 8-10)\")\n",
    "print(\"Factual knowledge sometimes emerges earlier, depending on frequency in training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Find Induction Heads (Preview)\n",
    "\n",
    "Find heads that complete \"Harry Potter...Harry\" → \" Potter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge Solution: Find induction heads\n",
    "\n",
    "prompt = \"Harry Potter is a wizard. Harry\"\n",
    "tokens = model.to_tokens(prompt)\n",
    "token_strs = model.to_str_tokens(tokens)\n",
    "\n",
    "print(\"Tokens:\")\n",
    "for i, t in enumerate(token_strs):\n",
    "    print(f\"  {i}: '{t}'\")\n",
    "\n",
    "# Run model\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "\n",
    "# For induction, we look for heads that attend from the last position\n",
    "# to the position AFTER the first \"Harry\" - that would be \"Potter\"\n",
    "# First \"Harry\" is at position 0, so we want attention to position 1 (\" Potter\")\n",
    "\n",
    "harry_pos = 0  # First \"Harry\"\n",
    "after_harry = 1  # \" Potter\"\n",
    "last_harry = len(token_strs) - 1  # Last \"Harry\"\n",
    "\n",
    "print(f\"\\nLooking for attention from '{token_strs[last_harry]}' (pos {last_harry})\")\n",
    "print(f\"to '{token_strs[after_harry]}' (pos {after_harry})\")\n",
    "\n",
    "# Get attention to position after first Harry\n",
    "induction_attention = np.zeros((model.cfg.n_layers, model.cfg.n_heads))\n",
    "\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    pattern = cache[\"pattern\", layer][0]\n",
    "    induction_attention[layer, :] = pattern[:, last_harry, after_harry].detach().cpu().numpy()\n",
    "\n",
    "# Visualize\n",
    "fig = px.imshow(\n",
    "    induction_attention,\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\", \"color\": \"Attention\"},\n",
    "    color_continuous_scale=\"YlOrRd\",\n",
    "    title=\"Induction-like Attention: Last 'Harry' → ' Potter'\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Find top heads\n",
    "print(\"\\nTop 5 induction-like heads:\")\n",
    "flat = induction_attention.flatten()\n",
    "top_idx = np.argsort(flat)[-5:][::-1]\n",
    "for idx in top_idx:\n",
    "    layer = idx // model.cfg.n_heads\n",
    "    head = idx % model.cfg.n_heads\n",
    "    print(f\"  L{layer}H{head}: {induction_attention[layer, head]:.3f}\")\n",
    "\n",
    "# Verify prediction\n",
    "probs = torch.softmax(logits[0, -1, :], dim=-1)\n",
    "potter_token = model.to_single_token(\" Potter\")\n",
    "print(f\"\\nProbability of ' Potter': {probs[potter_token].item():.2%}\")\n",
    "\n",
    "del cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
