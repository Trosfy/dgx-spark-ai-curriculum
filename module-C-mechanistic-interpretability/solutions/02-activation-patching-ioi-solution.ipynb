{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab C.2: Activation Patching on IOI - SOLUTIONS\n",
    "\n",
    "This notebook contains solutions to all exercises from Lab C.2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from transformer_lens import HookedTransformer\n",
    "import gc\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: MLP Patching\n",
    "\n",
    "Patch MLP layers instead of attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: MLP Patching\n",
    "\n",
    "# Setup IOI example\n",
    "clean_prompt = \"John and Mary went to the store. John gave a book to\"\n",
    "corrupted_prompt = \"John and Mary went to the store. Mary gave a book to\"\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_prompt)\n",
    "corrupted_tokens = model.to_tokens(corrupted_prompt)\n",
    "\n",
    "answer_token = model.to_single_token(\" Mary\")\n",
    "wrong_token = model.to_single_token(\" John\")\n",
    "\n",
    "# Get baselines\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "\n",
    "def compute_logit_diff(logits):\n",
    "    return (logits[0, -1, answer_token] - logits[0, -1, wrong_token]).item()\n",
    "\n",
    "clean_diff = compute_logit_diff(clean_logits)\n",
    "corrupted_diff = compute_logit_diff(corrupted_logits)\n",
    "\n",
    "print(f\"Clean logit diff: {clean_diff:.2f}\")\n",
    "print(f\"Corrupted logit diff: {corrupted_diff:.2f}\")\n",
    "\n",
    "# Patch MLP layers\n",
    "mlp_effects = []\n",
    "\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    def patch_mlp(activation, hook, layer=layer):\n",
    "        return corrupted_cache[f\"blocks.{layer}.hook_mlp_out\"]\n",
    "    \n",
    "    hook_name = f\"blocks.{layer}.hook_mlp_out\"\n",
    "    patched_logits = model.run_with_hooks(\n",
    "        clean_tokens,\n",
    "        fwd_hooks=[(hook_name, patch_mlp)]\n",
    "    )\n",
    "    \n",
    "    patched_diff = compute_logit_diff(patched_logits)\n",
    "    effect = (clean_diff - patched_diff) / (clean_diff - corrupted_diff + 1e-10)\n",
    "    mlp_effects.append(effect)\n",
    "\n",
    "# Visualize\n",
    "fig = px.bar(\n",
    "    x=list(range(model.cfg.n_layers)),\n",
    "    y=mlp_effects,\n",
    "    title=\"MLP Patching Effects on IOI\",\n",
    "    labels={\"x\": \"Layer\", \"y\": \"Patching Effect\"}\n",
    ")\n",
    "fig.add_hline(y=0, line_dash=\"dash\")\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nMost important MLP layers:\")\n",
    "for layer in np.argsort(np.abs(mlp_effects))[-5:][::-1]:\n",
    "    print(f\"  Layer {layer}: effect = {mlp_effects[layer]:.3f}\")\n",
    "\n",
    "del clean_cache, corrupted_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Position-Specific Patching\n",
    "\n",
    "Patch only specific positions to find where important information flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Position-Specific Patching\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_prompt)\n",
    "corrupted_tokens = model.to_tokens(corrupted_prompt)\n",
    "\n",
    "# Get caches\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "\n",
    "clean_diff = compute_logit_diff(clean_logits)\n",
    "corrupted_diff = compute_logit_diff(corrupted_logits)\n",
    "\n",
    "# Print tokens to identify positions\n",
    "token_strs = model.to_str_tokens(clean_tokens)\n",
    "print(\"Token positions:\")\n",
    "for i, t in enumerate(token_strs):\n",
    "    print(f\"  {i}: '{t}'\")\n",
    "\n",
    "# Patch specific positions at a key layer (layer 9)\n",
    "layer = 9\n",
    "position_effects = []\n",
    "\n",
    "for pos in range(len(token_strs)):\n",
    "    def patch_position(activation, hook, pos=pos):\n",
    "        activation[:, pos, :] = corrupted_cache[hook.name][:, pos, :]\n",
    "        return activation\n",
    "    \n",
    "    hook_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "    patched_logits = model.run_with_hooks(\n",
    "        clean_tokens,\n",
    "        fwd_hooks=[(hook_name, patch_position)]\n",
    "    )\n",
    "    \n",
    "    patched_diff = compute_logit_diff(patched_logits)\n",
    "    effect = (clean_diff - patched_diff) / (clean_diff - corrupted_diff + 1e-10)\n",
    "    position_effects.append(effect)\n",
    "\n",
    "# Visualize\n",
    "fig = px.bar(\n",
    "    x=token_strs,\n",
    "    y=position_effects,\n",
    "    title=f\"Position-Specific Patching at Layer {layer}\",\n",
    "    labels={\"x\": \"Token\", \"y\": \"Patching Effect\"}\n",
    ")\n",
    "fig.update_layout(xaxis_tickangle=45)\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nMost important positions:\")\n",
    "for pos in np.argsort(np.abs(position_effects))[-5:][::-1]:\n",
    "    print(f\"  Position {pos} ('{token_strs[pos]}'): effect = {position_effects[pos]:.3f}\")\n",
    "\n",
    "del clean_cache, corrupted_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Attention Pattern Analysis for Name Movers\n",
    "\n",
    "Visualize attention patterns of identified name mover heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Visualize name mover head attention\n",
    "\n",
    "# Known name mover heads from IOI paper: L9H9, L9H6, L10H0\n",
    "name_mover_heads = [(9, 9), (9, 6), (10, 0)]\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_prompt)\n",
    "_, cache = model.run_with_cache(clean_tokens)\n",
    "token_strs = model.to_str_tokens(clean_tokens)\n",
    "\n",
    "# Visualize each name mover head\n",
    "for layer, head in name_mover_heads:\n",
    "    pattern = cache[\"pattern\", layer][0, head].detach().cpu().numpy()\n",
    "    \n",
    "    fig = px.imshow(\n",
    "        pattern,\n",
    "        labels={\"x\": \"Key\", \"y\": \"Query\", \"color\": \"Attention\"},\n",
    "        x=token_strs,\n",
    "        y=token_strs,\n",
    "        color_continuous_scale=\"Blues\",\n",
    "        title=f\"Name Mover Head L{layer}H{head}\"\n",
    "    )\n",
    "    fig.update_layout(width=600, height=500)\n",
    "    fig.show()\n",
    "    \n",
    "    # Check attention from last position to names\n",
    "    mary_pos = 2  # Position of \"Mary\"\n",
    "    john_pos = 0  # Position of \"John\"\n",
    "    last_pos = len(token_strs) - 1\n",
    "    \n",
    "    print(f\"L{layer}H{head} - Last token attention:\")\n",
    "    print(f\"  To 'Mary' (pos {mary_pos}): {pattern[last_pos, mary_pos]:.3f}\")\n",
    "    print(f\"  To 'John' (pos {john_pos}): {pattern[last_pos, john_pos]:.3f}\")\n",
    "    print()\n",
    "\n",
    "print(\"Name mover heads attend strongly to the indirect object (Mary)\")\n",
    "print(\"from the final position where the prediction happens.\")\n",
    "\n",
    "del cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
