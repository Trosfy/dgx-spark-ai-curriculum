{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab C.1: TransformerLens Setup and Exploration\n",
    "\n",
    "**Module:** C - Mechanistic Interpretability  \n",
    "**Time:** 1.5 hours  \n",
    "**Difficulty:** ⭐⭐⭐ (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Install and configure TransformerLens for DGX Spark\n",
    "- [ ] Load GPT-2 Small and explore its architecture\n",
    "- [ ] Run inference and cache all internal activations\n",
    "- [ ] Visualize attention patterns and understand what they mean\n",
    "- [ ] Examine the residual stream and how information flows through the model\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Module 2.3 (NLP & Transformers)\n",
    "- Knowledge of: Attention mechanism basics, Python\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "When a language model gives a wrong or harmful answer, wouldn't it be amazing to *look inside* and see why? That's exactly what mechanistic interpretability enables. Companies like Anthropic, OpenAI, and DeepMind are investing heavily in this research to:\n",
    "\n",
    "- **Debug model failures** - Why did GPT say that?\n",
    "- **Ensure safety** - Can we detect deceptive reasoning?\n",
    "- **Verify capabilities** - Is the model actually \"understanding\" or just pattern matching?\n",
    "\n",
    "TransformerLens is the Swiss Army knife of interpretability research, giving us X-ray vision into transformers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELI5: What is Mechanistic Interpretability?\n",
    "\n",
    "> **Imagine you have a magical calculator** that always gives the right answer to any math problem. Pretty cool, right? But here's the thing - you don't know HOW it works inside. Is it actually doing math? Or did someone just put a huge lookup table inside with every possible question and answer?\n",
    ">\n",
    "> **Opening up the calculator** to see the gears, circuits, and mechanisms inside - that's mechanistic interpretability! We're not just asking \"does it work?\" but \"HOW does it work?\"\n",
    ">\n",
    "> **For neural networks**, this means looking at:\n",
    "> - Which neurons fire for which concepts?\n",
    "> - How does information flow from input to output?\n",
    "> - Are there identifiable \"circuits\" that do specific tasks?\n",
    ">\n",
    "> **The amazing discovery**: Neural networks develop surprisingly human-interpretable internal structures! They're not just a mess of numbers - they build organized systems for handling different tasks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup\n",
    "\n",
    "Let's get TransformerLens installed and verify our DGX Spark environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if not already installed)\n",
    "# Run this cell only once!\n",
    "\n",
    "# TransformerLens - our main interpretability library\n",
    "# !pip install transformer-lens\n",
    "\n",
    "# Visualization libraries\n",
    "# !pip install plotly kaleido\n",
    "\n",
    "# CircuitsVis for interactive visualizations (optional)\n",
    "# !pip install circuitsvis\n",
    "\n",
    "print(\"Packages ready! Let's do some interpretability research.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "# TransformerLens\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set up plotting\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check DGX Spark memory - we have 128GB unified memory!\n",
    "def print_memory_stats():\n",
    "    \"\"\"Print GPU memory statistics.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        free = total - reserved\n",
    "        \n",
    "        print(f\"GPU Memory:\")\n",
    "        print(f\"  Total:     {total:.1f} GB\")\n",
    "        print(f\"  Allocated: {allocated:.1f} GB\")\n",
    "        print(f\"  Reserved:  {reserved:.1f} GB\")\n",
    "        print(f\"  Free:      {free:.1f} GB\")\n",
    "    else:\n",
    "        print(\"No GPU available\")\n",
    "\n",
    "print_memory_stats()\n",
    "\n",
    "# DGX Spark Advantage: With 128GB, we can easily cache activations\n",
    "# for multiple model runs simultaneously!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "We verified our environment. On DGX Spark, you should see ~128GB total memory. This is *huge* for interpretability work where we need to cache every activation in the model!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Loading GPT-2 Small with TransformerLens\n",
    "\n",
    "### ELI5: What is TransformerLens?\n",
    "\n",
    "> **Think of TransformerLens like X-ray glasses** for neural networks. When you normally run a model, you give it input and get output - like a black box. But TransformerLens lets you see EVERYTHING happening inside:\n",
    ">\n",
    "> - Every attention pattern (who's paying attention to whom?)\n",
    "> - Every hidden state (what's the model \"thinking\" at each step?)\n",
    "> - Every MLP activation (what features is it detecting?)\n",
    ">\n",
    "> It's like having a slow-motion camera for each neuron!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 Small (124M parameters)\n",
    "# This model is perfect for interpretability - big enough to be interesting,\n",
    "# small enough to fully analyze\n",
    "\n",
    "print(\"Loading GPT-2 Small...\")\n",
    "print(\"(This may take a minute on first run as it downloads the weights)\\n\")\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the model architecture\n",
    "print(\"GPT-2 Small Architecture:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Number of layers:      {model.cfg.n_layers}\")\n",
    "print(f\"Number of heads:       {model.cfg.n_heads}\")\n",
    "print(f\"Model dimension:       {model.cfg.d_model}\")\n",
    "print(f\"Head dimension:        {model.cfg.d_head}\")\n",
    "print(f\"MLP dimension:         {model.cfg.d_mlp}\")\n",
    "print(f\"Vocabulary size:       {model.cfg.d_vocab}\")\n",
    "print(f\"Context window:        {model.cfg.n_ctx}\")\n",
    "print(f\"Total parameters:      {model.cfg.n_params / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Architecture Numbers to Remember\n",
    "\n",
    "| Component | Value | What it means |\n",
    "|-----------|-------|---------------|\n",
    "| 12 layers | Depth of processing | More layers = more abstract reasoning |\n",
    "| 12 heads per layer | Parallel attention patterns | Different heads learn different patterns |\n",
    "| 768 model dimension | Size of \"thought\" vectors | Each position represented by 768 numbers |\n",
    "| 64 head dimension | Per-head size | 768 ÷ 12 = 64 |\n",
    "| 144 total heads | 12 × 12 | Each potentially does something different! |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Running Inference and Caching Activations\n",
    "\n",
    "### ELI5: What are Activations?\n",
    "\n",
    "> **Imagine following a package through a factory**. At each station, the package changes - things are added, modified, inspected. \"Activations\" are like taking a photo of the package at every single station.\n",
    ">\n",
    "> For transformers:\n",
    "> - **Input** → \"The cat sat on the\"\n",
    "> - **Station 1 (Embedding)** → Numbers representing each word\n",
    "> - **Station 2 (Attention Layer 1)** → Words start paying attention to each other\n",
    "> - **Station 3 (MLP Layer 1)** → Features are detected\n",
    "> - ... (repeat for 12 layers) ...\n",
    "> - **Output** → \"mat\" (prediction)\n",
    ">\n",
    "> **Caching activations** = saving photos at every station so we can analyze them later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run a simple prompt and cache ALL activations\n",
    "prompt = \"The capital of France is\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "tokens = model.to_tokens(prompt)\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Token IDs: {tokens[0].tolist()}\")\n",
    "print(f\"Tokens: {model.to_str_tokens(prompt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model and cache EVERYTHING\n",
    "# This is where TransformerLens shines!\n",
    "\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"  - Batch size: {logits.shape[0]}\")\n",
    "print(f\"  - Sequence length: {logits.shape[1]}\")\n",
    "print(f\"  - Vocabulary size: {logits.shape[2]}\")\n",
    "\n",
    "print(f\"\\nNumber of cached activations: {len(cache)}\")\n",
    "print(f\"\\nSample of cached activation names:\")\n",
    "for i, key in enumerate(list(cache.keys())[:10]):\n",
    "    print(f\"  {key}\")\n",
    "print(\"  ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What did the model predict?\n",
    "# Get probabilities for the last position\n",
    "last_token_logits = logits[0, -1, :]  # [vocab_size]\n",
    "probs = torch.softmax(last_token_logits, dim=-1)\n",
    "\n",
    "# Top 10 predictions\n",
    "top_k = 10\n",
    "top_probs, top_indices = torch.topk(probs, top_k)\n",
    "\n",
    "print(f\"Top {top_k} predictions for '{prompt}___':\")\n",
    "print(\"=\" * 50)\n",
    "for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
    "    token = model.tokenizer.decode(idx.item())\n",
    "    print(f\"{i+1}. '{token}': {prob.item():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "The model correctly predicts \" Paris\" with high confidence. But *how* does it know this? That's what we'll investigate!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Exploring the Residual Stream\n",
    "\n",
    "### ELI5: The Residual Stream View\n",
    "\n",
    "> **Think of the residual stream as a highway**. Each token starts as a car on this highway. As the car travels:\n",
    ">\n",
    "> - **Attention layers** are like billboards that can add information to your car based on what other cars are carrying\n",
    "> - **MLP layers** are like rest stops that can transform your cargo\n",
    ">\n",
    "> **The key insight**: Information is *added* to the stream, not replaced! Each layer contributes its piece, building up the final answer.\n",
    ">\n",
    "> This is why it's called \"residual\" - each layer adds a \"residue\" of new information to what's already there.\n",
    "\n",
    "![Residual Stream](https://raw.githubusercontent.com/neelnanda-io/TransformerLens/main/docs/residual_stream.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the residual stream at different layers\n",
    "# \"resid_post\" means the residual stream AFTER that layer\n",
    "\n",
    "def get_residual_norms(cache, position=-1):\n",
    "    \"\"\"Get L2 norms of residual stream at each layer.\"\"\"\n",
    "    norms = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        resid = cache[\"resid_post\", layer][0, position, :]  # [d_model]\n",
    "        norms.append(resid.norm().item())\n",
    "    return norms\n",
    "\n",
    "# Get norms for the last token position (where prediction happens)\n",
    "norms = get_residual_norms(cache, position=-1)\n",
    "\n",
    "# Plot\n",
    "fig = px.line(\n",
    "    x=list(range(model.cfg.n_layers)),\n",
    "    y=norms,\n",
    "    title=\"Residual Stream Norm Across Layers (Last Token)\",\n",
    "    labels={\"x\": \"Layer\", \"y\": \"L2 Norm\"}\n",
    ")\n",
    "fig.update_traces(mode=\"lines+markers\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Residual Norm Plot\n",
    "\n",
    "The increasing norm shows that each layer is **adding** information to the residual stream. The model builds up its understanding layer by layer until it has enough information to predict \"Paris\".\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare residual streams for different positions\n",
    "tokens_list = model.to_str_tokens(prompt)\n",
    "\n",
    "# Get norms for each position in the sequence\n",
    "fig = go.Figure()\n",
    "\n",
    "for pos in range(len(tokens_list)):\n",
    "    norms = get_residual_norms(cache, position=pos)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(model.cfg.n_layers)),\n",
    "        y=norms,\n",
    "        name=f\"'{tokens_list[pos]}'\",\n",
    "        mode=\"lines+markers\"\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Residual Stream Norms for Each Token Position\",\n",
    "    xaxis_title=\"Layer\",\n",
    "    yaxis_title=\"L2 Norm\",\n",
    "    legend_title=\"Token\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "Notice how different tokens have different residual stream patterns:\n",
    "- Some tokens accumulate more information (larger norms)\n",
    "- The last position typically has the largest norm (it needs to predict the next token)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Visualizing Attention Patterns\n",
    "\n",
    "### ELI5: Attention Patterns\n",
    "\n",
    "> **Attention is like asking questions**. When processing the word \"capital\", the model might ask:\n",
    "> - \"What kind of capital?\" → looks at \"France\"\n",
    "> - \"What sentence structure?\" → looks at \"The\"\n",
    ">\n",
    "> **Each attention head** is like a specialist asking different questions:\n",
    "> - Head 1 might look for grammatical structure\n",
    "> - Head 2 might look for semantic meaning\n",
    "> - Head 3 might just copy the previous word\n",
    ">\n",
    "> **Attention patterns** show us WHO is paying attention to WHOM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attention patterns from the cache\n",
    "# Shape: [batch, n_heads, seq_len, seq_len]\n",
    "\n",
    "layer = 5  # Middle layer\n",
    "attention_pattern = cache[\"pattern\", layer][0]  # [n_heads, seq, seq]\n",
    "\n",
    "print(f\"Attention pattern shape: {attention_pattern.shape}\")\n",
    "print(f\"  - {attention_pattern.shape[0]} heads\")\n",
    "print(f\"  - {attention_pattern.shape[1]} query positions\")\n",
    "print(f\"  - {attention_pattern.shape[2]} key positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_head(cache, layer, head, tokens, title=None):\n",
    "    \"\"\"Create an attention pattern heatmap for a specific head.\"\"\"\n",
    "    pattern = cache[\"pattern\", layer][0, head].detach().cpu().numpy()\n",
    "    token_strs = model.to_str_tokens(tokens)\n",
    "    \n",
    "    fig = px.imshow(\n",
    "        pattern,\n",
    "        labels={\"x\": \"Key (Source)\", \"y\": \"Query (Destination)\", \"color\": \"Attention\"},\n",
    "        x=token_strs,\n",
    "        y=token_strs,\n",
    "        color_continuous_scale=\"Blues\",\n",
    "        title=title or f\"Layer {layer}, Head {head}\"\n",
    "    )\n",
    "    fig.update_layout(width=600, height=500)\n",
    "    return fig\n",
    "\n",
    "# Let's look at a few different attention heads\n",
    "fig = plot_attention_head(cache, layer=0, head=0, tokens=tokens)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Read Attention Heatmaps\n",
    "\n",
    "- **Rows** = Query positions (\"the word doing the looking\")\n",
    "- **Columns** = Key positions (\"the word being looked at\")\n",
    "- **Color intensity** = How much attention (brighter = more attention)\n",
    "\n",
    "So if position 4 (\"France\") has high attention to position 1 (\"capital\"), that means when processing \"France\", the model is gathering information from \"capital\".\n",
    "\n",
    "Due to causal masking, each position can only attend to itself and previous positions (lower triangle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize all heads in a layer to see their diversity\n",
    "def plot_all_heads_in_layer(cache, layer, tokens):\n",
    "    \"\"\"Plot all attention heads in a layer.\"\"\"\n",
    "    token_strs = model.to_str_tokens(tokens)\n",
    "    n_heads = model.cfg.n_heads\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=4,\n",
    "        subplot_titles=[f\"Head {h}\" for h in range(n_heads)],\n",
    "        vertical_spacing=0.1,\n",
    "        horizontal_spacing=0.05\n",
    "    )\n",
    "    \n",
    "    for head in range(n_heads):\n",
    "        pattern = cache[\"pattern\", layer][0, head].detach().cpu().numpy()\n",
    "        row = head // 4 + 1\n",
    "        col = head % 4 + 1\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Heatmap(\n",
    "                z=pattern,\n",
    "                x=token_strs,\n",
    "                y=token_strs,\n",
    "                colorscale=\"Blues\",\n",
    "                showscale=(head == 0)\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"All Attention Heads in Layer {layer}\",\n",
    "        height=800,\n",
    "        width=1200\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "fig = plot_all_heads_in_layer(cache, layer=0, tokens=tokens)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Head Diversity\n",
    "\n",
    "Notice how different heads learn different patterns:\n",
    "- **Some attend to previous tokens** (diagonal pattern offset by 1)\n",
    "- **Some attend to the beginning** (column on the left)\n",
    "- **Some attend broadly** (more uniform distribution)\n",
    "\n",
    "This specialization is what makes multi-head attention powerful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try It Yourself: Compare early vs. late layers\n",
    "# Which layer shows more specialized attention patterns?\n",
    "\n",
    "# Uncomment to see layer 11 (the last layer):\n",
    "# fig = plot_all_heads_in_layer(cache, layer=11, tokens=tokens)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: The Logit Lens\n",
    "\n",
    "### ELI5: What is the Logit Lens?\n",
    "\n",
    "> **Imagine you're writing an essay through multiple drafts**. The logit lens is like peeking at your answer after each draft:\n",
    ">\n",
    "> - **Draft 1 (Layer 1)**: \"Hmm... maybe the answer is... 'city'?\"\n",
    "> - **Draft 3 (Layer 3)**: \"Getting warmer... 'European'?\"\n",
    "> - **Draft 6 (Layer 6)**: \"Aha! 'Paris'!\"\n",
    "> - **Final Draft (Layer 12)**: \"Definitely 'Paris' (95% confident)\"\n",
    ">\n",
    "> The logit lens lets us see the model's \"work in progress\" at each layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_lens(model, cache, position=-1, top_k=5):\n",
    "    \"\"\"\n",
    "    Apply the logit lens to see predictions at each layer.\n",
    "    \n",
    "    This passes intermediate residual streams through the unembedding\n",
    "    to see what the model \"would predict\" at each layer.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Get unembedding matrix\n",
    "    W_U = model.W_U  # [d_model, vocab]\n",
    "    \n",
    "    for layer in range(model.cfg.n_layers + 1):\n",
    "        # Get residual stream at this layer\n",
    "        if layer == 0:\n",
    "            resid = cache[\"resid_pre\", 0][0, position, :]  # Before any layers\n",
    "        else:\n",
    "            resid = cache[\"resid_post\", layer - 1][0, position, :]  # After layer\n",
    "        \n",
    "        # Apply final layer norm\n",
    "        resid_normed = model.ln_final(resid)\n",
    "        \n",
    "        # Get logits\n",
    "        logits = resid_normed @ W_U\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Get top predictions\n",
    "        top_probs, top_indices = torch.topk(probs, top_k)\n",
    "        \n",
    "        layer_results = {\n",
    "            \"layer\": layer,\n",
    "            \"predictions\": [\n",
    "                (model.tokenizer.decode(idx.item()), prob.item())\n",
    "                for idx, prob in zip(top_indices, top_probs)\n",
    "            ]\n",
    "        }\n",
    "        results.append(layer_results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run logit lens\n",
    "lens_results = logit_lens(model, cache, position=-1, top_k=5)\n",
    "\n",
    "# Display results\n",
    "print(f\"Logit Lens Results for: '{prompt}'\")\n",
    "print(\"=\" * 60)\n",
    "for result in lens_results:\n",
    "    layer = result[\"layer\"]\n",
    "    top_pred, top_prob = result[\"predictions\"][0]\n",
    "    print(f\"Layer {layer:2d}: '{top_pred}' ({top_prob:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize logit lens as a heatmap\n",
    "def plot_logit_lens(results, target_tokens=None):\n",
    "    \"\"\"Plot logit lens results.\"\"\"\n",
    "    if target_tokens is None:\n",
    "        # Get all tokens that appear in top-1 at any layer\n",
    "        target_tokens = list(set(\n",
    "            r[\"predictions\"][0][0] for r in results\n",
    "        ))[:10]  # Limit to 10\n",
    "    \n",
    "    # Build probability matrix\n",
    "    n_layers = len(results)\n",
    "    prob_matrix = np.zeros((len(target_tokens), n_layers))\n",
    "    \n",
    "    for layer_idx, result in enumerate(results):\n",
    "        all_preds = {tok: prob for tok, prob in result[\"predictions\"]}\n",
    "        for tok_idx, tok in enumerate(target_tokens):\n",
    "            prob_matrix[tok_idx, layer_idx] = all_preds.get(tok, 0)\n",
    "    \n",
    "    fig = px.imshow(\n",
    "        prob_matrix,\n",
    "        labels={\"x\": \"Layer\", \"y\": \"Token\", \"color\": \"Probability\"},\n",
    "        y=target_tokens,\n",
    "        color_continuous_scale=\"YlOrRd\",\n",
    "        title=\"Logit Lens: Token Probabilities at Each Layer\"\n",
    "    )\n",
    "    fig.update_layout(width=800, height=400)\n",
    "    return fig\n",
    "\n",
    "# Include Paris and some alternatives\n",
    "fig = plot_logit_lens(lens_results, target_tokens=[\" Paris\", \" France\", \" the\", \" a\", \" London\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Logit Lens Results\n",
    "\n",
    "This shows us the **development of the prediction**:\n",
    "- Early layers might predict generic tokens\n",
    "- Middle layers start forming the correct answer\n",
    "- Later layers become confident in \"Paris\"\n",
    "\n",
    "This reveals which layers are doing the \"heavy lifting\" for this task!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Identifying Important Heads\n",
    "\n",
    "### ELI5: Which Heads Matter?\n",
    "\n",
    "> **Imagine you're making a cake with 144 helpers** (one for each attention head). Some helpers are crucial:\n",
    "> - \"Add flour\" helper - essential!\n",
    "> - \"Stir the bowl\" helper - very important!\n",
    "> - \"Play music\" helper - nice but not necessary\n",
    ">\n",
    "> We want to find which \"helpers\" (attention heads) are critical for getting the right answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find which heads contribute most to predicting \"Paris\"\n",
    "# We'll look at which heads attend strongly from the last position to \"France\"\n",
    "\n",
    "paris_token = model.to_single_token(\" Paris\")\n",
    "france_position = 3  # \"France\" is at position 3\n",
    "last_position = -1\n",
    "\n",
    "# Get attention from last position to \"France\" for all heads\n",
    "attention_to_france = np.zeros((model.cfg.n_layers, model.cfg.n_heads))\n",
    "\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    pattern = cache[\"pattern\", layer][0]  # [n_heads, seq, seq]\n",
    "    attention_to_france[layer, :] = pattern[:, last_position, france_position].detach().cpu().numpy()\n",
    "\n",
    "# Plot\n",
    "fig = px.imshow(\n",
    "    attention_to_france,\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\", \"color\": \"Attention\"},\n",
    "    color_continuous_scale=\"Blues\",\n",
    "    title=f\"Attention from Last Position to 'France' (position {france_position})\"\n",
    ")\n",
    "fig.update_layout(width=800, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the heads with highest attention to \"France\"\n",
    "top_k = 10\n",
    "flat_attention = attention_to_france.flatten()\n",
    "top_indices = np.argsort(flat_attention)[-top_k:][::-1]\n",
    "\n",
    "print(f\"Top {top_k} heads attending to 'France' from the last position:\")\n",
    "print(\"=\" * 50)\n",
    "for idx in top_indices:\n",
    "    layer = idx // model.cfg.n_heads\n",
    "    head = idx % model.cfg.n_heads\n",
    "    attn = flat_attention[idx]\n",
    "    print(f\"Layer {layer:2d}, Head {head:2d}: {attn:.2%} attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "We found the attention heads that \"look at France\" when predicting the next token. These heads are likely involved in:\n",
    "- Moving information about \"France\" to the final position\n",
    "- Connecting \"capital of France\" to \"Paris\"\n",
    "\n",
    "This is the beginning of **circuit discovery** - finding the components responsible for specific behaviors!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try It Yourself\n",
    "\n",
    "Now it's your turn to explore! Complete these exercises to deepen your understanding.\n",
    "\n",
    "### Exercise 1: Different Prompts\n",
    "Try running the analysis on different prompts:\n",
    "- \"The capital of Germany is\"\n",
    "- \"The opposite of hot is\"\n",
    "- \"Einstein developed the theory of\"\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Use `model.to_tokens(\"your prompt\")` and `model.run_with_cache(tokens)` to get a new cache, then reuse the analysis functions.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Try a different prompt and analyze it\n",
    "\n",
    "# your_prompt = \"...\"\n",
    "# your_tokens = model.to_tokens(your_prompt)\n",
    "# your_logits, your_cache = model.run_with_cache(your_tokens)\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Attention Pattern Analysis\n",
    "For the prompt \"John and Mary went to the store. John gave a book to\", find which heads attend from the last position to the first mention of \"John\".\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "First tokenize the prompt and print `model.to_str_tokens(prompt)` to find John's position. Then modify the attention analysis code to look at that position.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Find heads that attend to \"John\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Layer Comparison\n",
    "Compare the logit lens predictions between:\n",
    "- \"1 + 1 =\" (expects \" 2\")\n",
    "- \"The Eiffel Tower is in\" (expects \" Paris\")\n",
    "\n",
    "Which type of knowledge appears earlier in the network?\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Run the logit_lens function on both prompts and compare at which layer the correct answer first becomes the top prediction.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "# Compare logit lens for different types of knowledge\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting Batch Dimension\n",
    "```python\n",
    "# Wrong: Assuming no batch dimension\n",
    "pattern = cache[\"pattern\", 5]  # Shape: [batch, heads, seq, seq]\n",
    "pattern[0, 0]  # Error if you forget [0]\n",
    "\n",
    "# Correct: Always index batch first\n",
    "pattern = cache[\"pattern\", 5][0]  # [heads, seq, seq]\n",
    "```\n",
    "**Why:** TransformerLens always includes a batch dimension, even for single examples.\n",
    "\n",
    "### Mistake 2: Off-by-One in Layers\n",
    "```python\n",
    "# Wrong: Thinking there are 12 layers numbered 1-12\n",
    "resid = cache[\"resid_post\", 12]  # Error! Only 0-11\n",
    "\n",
    "# Correct: Layers are 0-indexed\n",
    "resid = cache[\"resid_post\", 11]  # Last layer\n",
    "```\n",
    "**Why:** Python uses 0-indexing. Layer 0 is the first layer, layer 11 is the last (for GPT-2 Small).\n",
    "\n",
    "### Mistake 3: Not Clearing GPU Memory\n",
    "```python\n",
    "# Wrong: Running many experiments without cleanup\n",
    "for prompt in many_prompts:\n",
    "    logits, cache = model.run_with_cache(tokens)  # Memory accumulates!\n",
    "\n",
    "# Correct: Clear cache periodically\n",
    "for prompt in many_prompts:\n",
    "    logits, cache = model.run_with_cache(tokens)\n",
    "    # Process cache...\n",
    "    del cache\n",
    "    torch.cuda.empty_cache()\n",
    "```\n",
    "**Why:** Each cache can be large. On DGX Spark you have 128GB, but it's still good practice!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- How to set up TransformerLens on DGX Spark\n",
    "- How to load models and cache activations\n",
    "- What the residual stream is and how information flows through it\n",
    "- How to visualize and interpret attention patterns\n",
    "- How to use the logit lens to see predictions at each layer\n",
    "- How to identify which attention heads might be important\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge (Optional)\n",
    "\n",
    "**Advanced: Find Induction Heads**\n",
    "\n",
    "Induction heads are one of the most important circuits discovered in transformers. They complete patterns like:\n",
    "- \"[A][B]...[A]\" → \"[B]\"\n",
    "\n",
    "For example: \"Harry Potter...Harry\" → \" Potter\"\n",
    "\n",
    "Can you find induction heads in GPT-2 Small by:\n",
    "1. Creating a prompt with repeated tokens\n",
    "2. Looking for heads that attend strongly to the position *after* the previous occurrence\n",
    "\n",
    "This will be covered in detail in Lab C.3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Find induction heads\n",
    "# Hint: Try \"Harry Potter... Harry\" and look for heads attending to \"Potter\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [TransformerLens Documentation](https://neelnanda-io.github.io/TransformerLens/)\n",
    "- [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html)\n",
    "- [ARENA Curriculum - Interpretability](https://arena3-chapter1-transformer-interp.streamlit.app/)\n",
    "- [Neel Nanda's YouTube](https://www.youtube.com/c/NeelNanda) - Excellent mech interp tutorials\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Let's free up GPU memory before moving to the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "del cache, logits\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Memory cleared!\")\n",
    "print_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "In **Lab C.2**, we'll learn **Activation Patching** - a powerful technique to determine *which* components are causally responsible for model behavior. Instead of just observing attention patterns, we'll actively intervene to prove which parts matter!\n",
    "\n",
    "**Next:** [Lab C.2: Activation Patching on IOI](02-activation-patching-ioi.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
