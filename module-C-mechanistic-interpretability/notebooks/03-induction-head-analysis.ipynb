{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab C.3: Induction Head Analysis\n",
    "\n",
    "**Module:** C - Mechanistic Interpretability  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐⭐⭐ (Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand what induction heads are and why they matter\n",
    "- [ ] Create datasets that test induction behavior\n",
    "- [ ] Identify induction heads through attention pattern analysis\n",
    "- [ ] Verify induction head composition with previous token heads\n",
    "- [ ] Measure the impact of ablating induction heads\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Lab C.1 and C.2\n",
    "- Knowledge of: Activation patching, attention patterns\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "**Induction heads are arguably the most important circuit discovered in transformers.**\n",
    "\n",
    "They explain:\n",
    "- **In-context learning**: How models learn from examples in the prompt\n",
    "- **Few-shot learning**: Why giving examples helps the model\n",
    "- **Pattern completion**: Predicting what comes next in repeated sequences\n",
    "\n",
    "Understanding induction heads gives insight into one of the core mechanisms that makes large language models powerful!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELI5: What are Induction Heads?\n",
    "\n",
    "> **Imagine you're playing a pattern game**: Someone says \"Apple, Banana, Apple, ___\"\n",
    ">\n",
    "> **You know the answer is \"Banana\"!** Why? Because:\n",
    "> 1. You notice \"Apple\" appeared before\n",
    "> 2. You remember what came after \"Apple\" last time (\"Banana\")\n",
    "> 3. You predict \"Banana\" will come again\n",
    ">\n",
    "> **That's exactly what induction heads do!** They complete patterns by:\n",
    "> 1. Finding where the current token appeared before\n",
    "> 2. Looking at what came after it\n",
    "> 3. Predicting that same thing will come again\n",
    ">\n",
    "> **The formal pattern:** `[A][B]...[A]` → `[B]`\n",
    ">\n",
    "> **Real examples:**\n",
    "> - \"Harry Potter...Harry\" → \" Potter\"\n",
    "> - \"New York City...New York\" → \" City\"\n",
    "> - \"def foo():...def\" → \" foo\" (in code!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELI5: The Induction Circuit\n",
    "\n",
    "> **Induction heads don't work alone!** They need a partner:\n",
    ">\n",
    "> **1. Previous Token Head (Layer 1-2)**\n",
    "> - Job: \"Copy information about token[i] to position[i+1]\"\n",
    "> - After \"Apple\" at position 0, position 1 now \"knows\" about Apple\n",
    ">\n",
    "> **2. Induction Head (Layer 5-6+)**  \n",
    "> - Job: \"Find positions that know about my current token\"\n",
    "> - When at the second \"Apple\" (position 2), looks for positions that \"know\" Apple\n",
    "> - Finds position 1 (which got Apple info from previous token head)\n",
    "> - Copies what's at position 1 → \"Banana\"!\n",
    ">\n",
    "> **It's a two-step dance:**\n",
    "> 1. Previous token head moves info forward: `[Apple] → [Banana gets Apple's info]`\n",
    "> 2. Induction head uses that info: `[Apple] attends to [position with Apple info]`\n",
    "\n",
    "```\n",
    "Position:    0       1       2       3\n",
    "Token:     Apple   Banana  Apple    ???\n",
    "                     ↑               |\n",
    "                     └───────────────┘\n",
    "                   Induction head attends here!\n",
    "                   (Finds \"Banana\" to predict)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "\n",
    "# TransformerLens\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Loaded GPT-2 Small: {model.cfg.n_layers} layers, {model.cfg.n_heads} heads/layer\")\n",
    "print(f\"Total heads: {model.cfg.n_layers * model.cfg.n_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Creating Induction Test Data\n",
    "\n",
    "To find induction heads, we need data that tests the `[A][B]...[A]` → `[B]` pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_repeated_sequence(seq_len: int = 25, seed: int = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a sequence of random tokens, then repeat it.\n",
    "    \n",
    "    This creates the pattern: [A][B][C]...[A][B][C]...\n",
    "    Induction heads should predict each token based on what followed it before.\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Length of the first half (will be doubled)\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Token tensor of shape [1, seq_len * 2]\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    # Use tokens in a safe range (avoid special tokens)\n",
    "    first_half = torch.randint(1000, 10000, (1, seq_len), device=model.cfg.device)\n",
    "    \n",
    "    # Repeat the sequence\n",
    "    repeated = torch.cat([first_half, first_half], dim=1)\n",
    "    \n",
    "    return repeated\n",
    "\n",
    "# Create example\n",
    "repeated_tokens = create_repeated_sequence(seq_len=10, seed=42)\n",
    "print(f\"Token shape: {repeated_tokens.shape}\")\n",
    "print(f\"First half:  {repeated_tokens[0, :10].tolist()}\")\n",
    "print(f\"Second half: {repeated_tokens[0, 10:].tolist()}\")\n",
    "print(f\"\\nAre they equal? {torch.equal(repeated_tokens[0, :10], repeated_tokens[0, 10:])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the model does well on this task\n",
    "def test_induction_accuracy(model, seq_len: int = 20, n_tests: int = 5):\n",
    "    \"\"\"Test how well the model predicts the next token in repeated sequences.\"\"\"\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_tested = 0\n",
    "    \n",
    "    for seed in range(n_tests):\n",
    "        tokens = create_repeated_sequence(seq_len, seed=seed)\n",
    "        \n",
    "        # Get model predictions\n",
    "        logits = model(tokens)\n",
    "        predictions = logits[0, :-1, :].argmax(dim=-1)  # Predict next token\n",
    "        \n",
    "        # In the second half, each prediction should match what followed in first half\n",
    "        # Position i in second half (i >= seq_len) should predict tokens[i+1-seq_len]\n",
    "        for pos in range(seq_len, 2 * seq_len - 1):\n",
    "            predicted = predictions[pos].item()\n",
    "            actual = tokens[0, pos + 1].item()  # What actually comes next\n",
    "            expected = tokens[0, pos + 1 - seq_len].item()  # What came after in first half\n",
    "            \n",
    "            # These should all be the same (repeated sequence)\n",
    "            assert actual == expected, \"Bug in test setup\"\n",
    "            \n",
    "            if predicted == actual:\n",
    "                total_correct += 1\n",
    "            total_tested += 1\n",
    "    \n",
    "    accuracy = total_correct / total_tested\n",
    "    print(f\"Induction accuracy: {accuracy:.1%} ({total_correct}/{total_tested})\")\n",
    "    return accuracy\n",
    "\n",
    "accuracy = test_induction_accuracy(model, seq_len=20, n_tests=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "The model shows strong induction behavior! When it sees `[A][B]...[A]`, it predicts `[B]` with high accuracy. This confirms induction heads are present and active.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Finding Induction Heads Through Attention Patterns\n",
    "\n",
    "Induction heads have a distinctive attention pattern: when at position `i` in the second half, they attend to position `i - seq_len + 1` (the position *after* where this token appeared before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_induction_scores(\n",
    "    model: HookedTransformer,\n",
    "    seq_len: int = 25,\n",
    "    n_samples: int = 10\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute induction scores for all attention heads.\n",
    "    \n",
    "    Induction score = average attention to the \"induction target\" position.\n",
    "    For position i in second half, the target is position (i - seq_len + 1).\n",
    "    \n",
    "    Returns:\n",
    "        Array of shape [n_layers, n_heads] with induction scores.\n",
    "    \"\"\"\n",
    "    n_layers = model.cfg.n_layers\n",
    "    n_heads = model.cfg.n_heads\n",
    "    scores = np.zeros((n_layers, n_heads))\n",
    "    \n",
    "    for sample in range(n_samples):\n",
    "        # Create repeated sequence\n",
    "        tokens = create_repeated_sequence(seq_len, seed=sample)\n",
    "        \n",
    "        # Get attention patterns\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        \n",
    "        for layer in range(n_layers):\n",
    "            pattern = cache[\"pattern\", layer][0]  # [n_heads, seq, seq]\n",
    "            \n",
    "            for head in range(n_heads):\n",
    "                # For each position in second half, check attention to induction target\n",
    "                induction_attn = 0\n",
    "                count = 0\n",
    "                \n",
    "                for pos in range(seq_len, 2 * seq_len):\n",
    "                    # Induction target: position after where this token appeared before\n",
    "                    target = pos - seq_len + 1\n",
    "                    if target > 0:  # Valid position\n",
    "                        induction_attn += pattern[head, pos, target].item()\n",
    "                        count += 1\n",
    "                \n",
    "                scores[layer, head] += induction_attn / count if count > 0 else 0\n",
    "        \n",
    "        # Clean up\n",
    "        del cache\n",
    "    \n",
    "    scores /= n_samples\n",
    "    return scores\n",
    "\n",
    "print(\"Computing induction scores (this may take a minute)...\")\n",
    "induction_scores = compute_induction_scores(model, seq_len=25, n_samples=10)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize induction scores\n",
    "fig = px.imshow(\n",
    "    induction_scores,\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\", \"color\": \"Induction Score\"},\n",
    "    color_continuous_scale=\"YlOrRd\",\n",
    "    title=\"Induction Scores Across All Attention Heads\"\n",
    ")\n",
    "fig.update_layout(width=800, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top induction heads\n",
    "print(\"Top Induction Heads:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "flat_scores = induction_scores.flatten()\n",
    "sorted_indices = np.argsort(flat_scores)[::-1]\n",
    "\n",
    "top_induction_heads = []\n",
    "for i, idx in enumerate(sorted_indices[:10]):\n",
    "    layer = idx // model.cfg.n_heads\n",
    "    head = idx % model.cfg.n_heads\n",
    "    score = induction_scores[layer, head]\n",
    "    print(f\"{i+1}. L{layer}H{head}: score = {score:.3f}\")\n",
    "    if score > 0.15:  # Threshold for \"real\" induction heads\n",
    "        top_induction_heads.append((layer, head, score))\n",
    "\n",
    "print(f\"\\nFound {len(top_induction_heads)} strong induction heads (score > 0.15)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Induction Scores\n",
    "\n",
    "- **High scores (>0.15)**: Strong induction heads that consistently attend to the \"answer\" position\n",
    "- **Medium scores (0.05-0.15)**: Partial induction behavior, may be backup heads\n",
    "- **Low scores (<0.05)**: Not induction heads\n",
    "\n",
    "Typically, induction heads appear in **middle layers (4-7)** of GPT-2 Small.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing Induction Head Attention Patterns\n",
    "\n",
    "Let's look at the attention patterns of our identified induction heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a specific repeated sequence for visualization\n",
    "seq_len = 10\n",
    "tokens = create_repeated_sequence(seq_len, seed=123)\n",
    "\n",
    "# Get attention patterns\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "\n",
    "# Convert to string tokens for labels\n",
    "token_strs = [f\"{i}:{model.tokenizer.decode(t.item())[:6]}\" \n",
    "              for i, t in enumerate(tokens[0])]\n",
    "print(\"Token labels:\")\n",
    "print(token_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top induction head\n",
    "if top_induction_heads:\n",
    "    layer, head, score = top_induction_heads[0]\n",
    "    \n",
    "    pattern = cache[\"pattern\", layer][0, head].detach().cpu().numpy()\n",
    "    \n",
    "    fig = px.imshow(\n",
    "        pattern,\n",
    "        labels={\"x\": \"Key (Source)\", \"y\": \"Query (Attending)\", \"color\": \"Attention\"},\n",
    "        x=token_strs,\n",
    "        y=token_strs,\n",
    "        color_continuous_scale=\"Blues\",\n",
    "        title=f\"Induction Head L{layer}H{head} Attention Pattern (score={score:.3f})\"\n",
    "    )\n",
    "    fig.update_layout(width=700, height=600)\n",
    "    \n",
    "    # Add annotation showing expected induction pattern\n",
    "    # Positions 10-19 should attend to positions 1-10\n",
    "    for i in range(seq_len, 2 * seq_len):\n",
    "        target = i - seq_len + 1\n",
    "        if target > 0:\n",
    "            fig.add_shape(\n",
    "                type=\"rect\",\n",
    "                x0=target-0.5, x1=target+0.5,\n",
    "                y0=i-0.5, y1=i+0.5,\n",
    "                line=dict(color=\"red\", width=1)\n",
    "            )\n",
    "    \n",
    "    fig.show()\n",
    "    print(\"Red boxes: Expected induction attention targets\")\n",
    "else:\n",
    "    print(\"No strong induction heads found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple induction heads\n",
    "if len(top_induction_heads) >= 2:\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=min(3, len(top_induction_heads)),\n",
    "        subplot_titles=[f\"L{l}H{h} (score={s:.2f})\" for l, h, s in top_induction_heads[:3]]\n",
    "    )\n",
    "    \n",
    "    for i, (layer, head, score) in enumerate(top_induction_heads[:3]):\n",
    "        pattern = cache[\"pattern\", layer][0, head].detach().cpu().numpy()\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Heatmap(\n",
    "                z=pattern,\n",
    "                colorscale=\"Blues\",\n",
    "                showscale=(i == 0)\n",
    "            ),\n",
    "            row=1, col=i+1\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Top 3 Induction Heads Comparison\",\n",
    "        height=400,\n",
    "        width=1000\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to Look For\n",
    "\n",
    "In induction head patterns, you should see:\n",
    "- **Diagonal stripe in bottom-right**: Positions in second half attending to offset in first half\n",
    "- **Offset of 1**: They attend to position `(i - seq_len + 1)`, not `(i - seq_len)`\n",
    "\n",
    "This offset is crucial - it's what makes them look at *what came after* the repeated token!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Finding Previous Token Heads\n",
    "\n",
    "Induction heads need previous token heads to work. Let's find those too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prev_token_scores(\n",
    "    model: HookedTransformer,\n",
    "    seq_len: int = 50,\n",
    "    n_samples: int = 10\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute previous token head scores.\n",
    "    \n",
    "    Previous token score = average attention to position (i-1).\n",
    "    \"\"\"\n",
    "    n_layers = model.cfg.n_layers\n",
    "    n_heads = model.cfg.n_heads\n",
    "    scores = np.zeros((n_layers, n_heads))\n",
    "    \n",
    "    for sample in range(n_samples):\n",
    "        # Random tokens (don't need repeated for this)\n",
    "        tokens = torch.randint(1000, 10000, (1, seq_len), device=model.cfg.device)\n",
    "        \n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        \n",
    "        for layer in range(n_layers):\n",
    "            pattern = cache[\"pattern\", layer][0]  # [n_heads, seq, seq]\n",
    "            \n",
    "            for head in range(n_heads):\n",
    "                # Average attention to previous position\n",
    "                prev_attn = 0\n",
    "                for pos in range(1, seq_len):\n",
    "                    prev_attn += pattern[head, pos, pos - 1].item()\n",
    "                \n",
    "                scores[layer, head] += prev_attn / (seq_len - 1)\n",
    "        \n",
    "        del cache\n",
    "    \n",
    "    scores /= n_samples\n",
    "    return scores\n",
    "\n",
    "print(\"Computing previous token head scores...\")\n",
    "prev_token_scores = compute_prev_token_scores(model)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize previous token scores\n",
    "fig = px.imshow(\n",
    "    prev_token_scores,\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\", \"color\": \"Prev Token Score\"},\n",
    "    color_continuous_scale=\"Greens\",\n",
    "    title=\"Previous Token Head Scores\"\n",
    ")\n",
    "fig.update_layout(width=800, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top previous token heads\n",
    "print(\"Top Previous Token Heads:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "flat_scores = prev_token_scores.flatten()\n",
    "sorted_indices = np.argsort(flat_scores)[::-1]\n",
    "\n",
    "top_prev_heads = []\n",
    "for i, idx in enumerate(sorted_indices[:10]):\n",
    "    layer = idx // model.cfg.n_heads\n",
    "    head = idx % model.cfg.n_heads\n",
    "    score = prev_token_scores[layer, head]\n",
    "    print(f\"{i+1}. L{layer}H{head}: score = {score:.3f}\")\n",
    "    if score > 0.3 and layer < 4:  # Previous token heads should be in early layers\n",
    "        top_prev_heads.append((layer, head, score))\n",
    "\n",
    "print(f\"\\nFound {len(top_prev_heads)} previous token heads (early layers, score > 0.3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Head Types\n",
    "\n",
    "- **Previous Token Heads**: Appear in early layers (0-2), attend to position i-1\n",
    "- **Induction Heads**: Appear in later layers (4-7), attend to position i-seq_len+1\n",
    "\n",
    "This layer ordering is crucial - previous token heads must run first to set up the information that induction heads use!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Verifying Composition\n",
    "\n",
    "Let's verify that induction heads actually read from previous token heads through K-composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_k_composition(\n",
    "    model: HookedTransformer,\n",
    "    source_layer: int,\n",
    "    source_head: int,\n",
    "    target_layer: int,\n",
    "    target_head: int\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute K-composition score between two heads.\n",
    "    \n",
    "    K-composition: Does source head's output get used by target head's keys?\n",
    "    \n",
    "    Higher score = source head writes vectors that target head uses for key matching.\n",
    "    \"\"\"\n",
    "    if target_layer <= source_layer:\n",
    "        return 0.0  # Can't compose backwards\n",
    "    \n",
    "    # Get weight matrices\n",
    "    W_O = model.W_O[source_layer, source_head]  # [d_head, d_model]\n",
    "    W_V = model.W_V[source_layer, source_head]  # [d_model, d_head]\n",
    "    W_K = model.W_K[target_layer, target_head]  # [d_model, d_head]\n",
    "    \n",
    "    # OV circuit: what does source head write to residual stream?\n",
    "    OV = W_V @ W_O  # [d_model, d_model]\n",
    "    \n",
    "    # K-composition: how much does target's K read from OV output?\n",
    "    # We measure: ||W_K^T @ OV|| / (||W_K^T|| * ||OV||)\n",
    "    composition = (W_K.T @ OV).norm().item()\n",
    "    normalized = composition / (W_K.T.norm().item() * OV.norm().item() + 1e-10)\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "# Compute composition between all prev token and induction head pairs\n",
    "if top_prev_heads and top_induction_heads:\n",
    "    print(\"K-Composition Scores (Previous Token → Induction):\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for src_layer, src_head, src_score in top_prev_heads[:3]:\n",
    "        for tgt_layer, tgt_head, tgt_score in top_induction_heads[:3]:\n",
    "            comp = compute_k_composition(model, src_layer, src_head, tgt_layer, tgt_head)\n",
    "            print(f\"L{src_layer}H{src_head} (prev) → L{tgt_layer}H{tgt_head} (ind): {comp:.3f}\")\n",
    "else:\n",
    "    print(\"Need both previous token and induction heads to compute composition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Composition Scores\n",
    "\n",
    "High K-composition means the induction head uses the previous token head's output for its key matching. This is exactly what we expect from the induction circuit!\n",
    "\n",
    "- **High scores (>0.1)**: Strong composition, likely part of induction circuit\n",
    "- **Low scores (<0.05)**: Weak composition, less likely to be partners\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Ablation Study\n",
    "\n",
    "Let's verify induction heads are actually important by ablating (removing) them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablate_heads(\n",
    "    model: HookedTransformer,\n",
    "    tokens: torch.Tensor,\n",
    "    heads_to_ablate: List[Tuple[int, int]],\n",
    "    method: str = \"zero\"\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Ablate specific heads and return logits.\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model\n",
    "        tokens: Input tokens\n",
    "        heads_to_ablate: List of (layer, head) tuples to ablate\n",
    "        method: 'zero' to zero out, 'mean' to replace with mean\n",
    "    \n",
    "    Returns:\n",
    "        Logits after ablation\n",
    "    \"\"\"\n",
    "    def ablate_hook(activation, hook, layer, head):\n",
    "        # activation shape: [batch, seq, n_heads, d_head]\n",
    "        if method == \"zero\":\n",
    "            activation[:, :, head, :] = 0\n",
    "        elif method == \"mean\":\n",
    "            activation[:, :, head, :] = activation[:, :, head, :].mean()\n",
    "        return activation\n",
    "    \n",
    "    # Create hooks for all heads to ablate\n",
    "    hooks = []\n",
    "    for layer, head in heads_to_ablate:\n",
    "        hook_name = f\"blocks.{layer}.attn.hook_z\"\n",
    "        hook_fn = lambda act, hook, l=layer, h=head: ablate_hook(act, hook, l, h)\n",
    "        hooks.append((hook_name, hook_fn))\n",
    "    \n",
    "    return model.run_with_hooks(tokens, fwd_hooks=hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_induction_loss(\n",
    "    model: HookedTransformer,\n",
    "    seq_len: int = 25,\n",
    "    heads_to_ablate: Optional[List[Tuple[int, int]]] = None,\n",
    "    n_samples: int = 5\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Measure loss on induction task, optionally with ablations.\n",
    "    \n",
    "    Returns:\n",
    "        (loss, accuracy) tuple\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    for seed in range(n_samples):\n",
    "        tokens = create_repeated_sequence(seq_len, seed=seed)\n",
    "        \n",
    "        if heads_to_ablate:\n",
    "            logits = ablate_heads(model, tokens, heads_to_ablate)\n",
    "        else:\n",
    "            logits = model(tokens)\n",
    "        \n",
    "        # Compute loss on second half predictions\n",
    "        for pos in range(seq_len, 2 * seq_len - 1):\n",
    "            target = tokens[0, pos + 1]\n",
    "            pred_logits = logits[0, pos, :]\n",
    "            \n",
    "            # Cross-entropy loss\n",
    "            loss = torch.nn.functional.cross_entropy(\n",
    "                pred_logits.unsqueeze(0), target.unsqueeze(0)\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Accuracy\n",
    "            if pred_logits.argmax() == target:\n",
    "                total_correct += 1\n",
    "            \n",
    "            total_count += 1\n",
    "    \n",
    "    return total_loss / total_count, total_correct / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline without ablation\n",
    "baseline_loss, baseline_acc = measure_induction_loss(model)\n",
    "print(f\"Baseline (no ablation):\")\n",
    "print(f\"  Loss: {baseline_loss:.3f}\")\n",
    "print(f\"  Accuracy: {baseline_acc:.1%}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablate induction heads\n",
    "if top_induction_heads:\n",
    "    induction_head_list = [(l, h) for l, h, _ in top_induction_heads]\n",
    "    \n",
    "    ablated_loss, ablated_acc = measure_induction_loss(\n",
    "        model, heads_to_ablate=induction_head_list\n",
    "    )\n",
    "    \n",
    "    print(f\"With induction heads ablated ({len(induction_head_list)} heads):\")\n",
    "    print(f\"  Loss: {ablated_loss:.3f} (change: {ablated_loss - baseline_loss:+.3f})\")\n",
    "    print(f\"  Accuracy: {ablated_acc:.1%} (change: {(ablated_acc - baseline_acc)*100:+.1f}%)\")\n",
    "    print()\n",
    "    print(f\"Loss increase: {(ablated_loss / baseline_loss - 1) * 100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: ablate random heads\n",
    "random.seed(42)\n",
    "random_heads = [(random.randint(0, 11), random.randint(0, 11)) \n",
    "                for _ in range(len(top_induction_heads) if top_induction_heads else 3)]\n",
    "\n",
    "random_loss, random_acc = measure_induction_loss(\n",
    "    model, heads_to_ablate=random_heads\n",
    ")\n",
    "\n",
    "print(f\"With random heads ablated ({len(random_heads)} heads):\")\n",
    "print(f\"  Loss: {random_loss:.3f} (change: {random_loss - baseline_loss:+.3f})\")\n",
    "print(f\"  Accuracy: {random_acc:.1%} (change: {(random_acc - baseline_acc)*100:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Ablation Results\n",
    "\n",
    "If induction heads are truly important:\n",
    "- Ablating them should **significantly increase loss** and **decrease accuracy**\n",
    "- Ablating random heads should have **much smaller effect**\n",
    "\n",
    "This provides causal evidence that these heads are responsible for induction!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try It Yourself\n",
    "\n",
    "### Exercise 1: Visualize Previous Token Head Patterns\n",
    "Visualize the attention patterns of the top previous token heads. Do they show the expected diagonal pattern?\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Use the same visualization code as for induction heads. Previous token heads should show a diagonal stripe one position below the main diagonal.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Ablate Previous Token Heads\n",
    "What happens to induction performance if you ablate the previous token heads instead of the induction heads?\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "If induction heads depend on previous token heads, ablating the latter should also hurt induction performance - maybe even more!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Natural Language Induction\n",
    "Test induction on natural language: \"Harry Potter is a wizard. Harry\" → \"Potter\"?\n",
    "Do the same induction heads activate for this more realistic example?\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Tokenize the natural language example and check attention patterns. Real text may trigger different/additional heads due to semantic content.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Wrong Induction Target Position\n",
    "```python\n",
    "# Wrong: Looking at position (i - seq_len)\n",
    "target = pos - seq_len  # This is where the token APPEARED\n",
    "\n",
    "# Correct: Looking at position (i - seq_len + 1)\n",
    "target = pos - seq_len + 1  # This is what FOLLOWED that token\n",
    "```\n",
    "**Why:** Induction heads look at what *came after* the repeated token, not the token itself!\n",
    "\n",
    "### Mistake 2: Confusing Induction with Copying\n",
    "```python\n",
    "# Copying head: \"ABC...\" → attends to \"A\", \"B\", \"C\"\n",
    "# Induction head: \"ABC...A\" → attends to \"B\" (what came after A)\n",
    "```\n",
    "**Why:** Induction is specifically about pattern completion, not simple copying.\n",
    "\n",
    "### Mistake 3: Layer Order Confusion\n",
    "```python\n",
    "# Wrong: Expecting induction heads in layer 0\n",
    "# Induction heads need previous token heads to run first!\n",
    "\n",
    "# Correct ordering:\n",
    "# Layers 0-2: Previous token heads\n",
    "# Layers 4-7: Induction heads\n",
    "```\n",
    "**Why:** Induction requires two-step composition that needs layer separation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- What induction heads are and how they work\n",
    "- How to create test data for induction behavior\n",
    "- How to identify induction heads through attention patterns\n",
    "- The role of previous token heads in the induction circuit\n",
    "- How to verify importance through ablation studies\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge (Optional)\n",
    "\n",
    "**Induction Head Formation**\n",
    "\n",
    "The original Anthropic paper discovered that induction heads form around step 2 billion during training. Can you:\n",
    "\n",
    "1. Load a smaller model (GPT-2 or similar) at different training checkpoints\n",
    "2. Measure induction scores at each checkpoint\n",
    "3. Find when induction heads \"turn on\"\n",
    "\n",
    "This is advanced but reveals fascinating training dynamics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Training dynamics\n",
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html) - The original Anthropic paper\n",
    "- [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html) - Foundation for circuit analysis\n",
    "- [The Induction Heads Story (LessWrong)](https://www.lesswrong.com/posts/TvrfY4c9eaGLeyDkE/induction-heads-illustrated) - Illustrated guide\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "del cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Memory cleared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "In **Lab C.4**, we'll explore **Sparse Autoencoders (SAEs)** - a cutting-edge technique for extracting interpretable features from neural networks. SAEs can find individual concepts like \"Python code\", \"questions\", or \"French text\" hidden in model activations!\n",
    "\n",
    "**Next:** [Lab C.4: Feature Extraction with SAEs](04-feature-extraction-saes.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
