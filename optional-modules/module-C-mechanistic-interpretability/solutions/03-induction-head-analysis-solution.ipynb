{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab C.3: Induction Head Analysis - SOLUTIONS\n",
    "\n",
    "This notebook contains solutions to all exercises from Lab C.3.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from transformer_lens import HookedTransformer\n",
    "import gc\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Visualize Previous Token Head Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Visualize previous token heads\n",
    "\n",
    "# Compute previous token scores\n",
    "def compute_prev_token_scores(model, seq_len=30, n_samples=5):\n",
    "    scores = np.zeros((model.cfg.n_layers, model.cfg.n_heads))\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        tokens = torch.randint(1000, 10000, (1, seq_len), device=\"cuda\")\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        \n",
    "        for layer in range(model.cfg.n_layers):\n",
    "            pattern = cache[\"pattern\", layer][0]\n",
    "            for head in range(model.cfg.n_heads):\n",
    "                # Average attention to previous position\n",
    "                prev_attn = sum(pattern[head, pos, pos-1].item() \n",
    "                              for pos in range(1, seq_len)) / (seq_len - 1)\n",
    "                scores[layer, head] += prev_attn\n",
    "        del cache\n",
    "    \n",
    "    return scores / n_samples\n",
    "\n",
    "prev_scores = compute_prev_token_scores(model)\n",
    "\n",
    "# Find top previous token heads\n",
    "top_prev_heads = []\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    for head in range(model.cfg.n_heads):\n",
    "        if prev_scores[layer, head] > 0.3 and layer < 4:\n",
    "            top_prev_heads.append((layer, head, prev_scores[layer, head]))\n",
    "\n",
    "top_prev_heads.sort(key=lambda x: -x[2])\n",
    "print(\"Top previous token heads:\")\n",
    "for l, h, s in top_prev_heads[:5]:\n",
    "    print(f\"  L{l}H{h}: {s:.3f}\")\n",
    "\n",
    "# Visualize attention pattern of top previous token head\n",
    "if top_prev_heads:\n",
    "    layer, head, _ = top_prev_heads[0]\n",
    "    \n",
    "    tokens = torch.randint(1000, 10000, (1, 15), device=\"cuda\")\n",
    "    _, cache = model.run_with_cache(tokens)\n",
    "    \n",
    "    pattern = cache[\"pattern\", layer][0, head].cpu().numpy()\n",
    "    \n",
    "    fig = px.imshow(\n",
    "        pattern,\n",
    "        labels={\"x\": \"Key\", \"y\": \"Query\", \"color\": \"Attention\"},\n",
    "        color_continuous_scale=\"Greens\",\n",
    "        title=f\"Previous Token Head L{layer}H{head}\"\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    print(\"\\nNotice the diagonal stripe one position below the main diagonal.\")\n",
    "    print(\"This shows each position attending to the previous position.\")\n",
    "    \n",
    "    del cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Ablate Previous Token Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Ablate previous token heads and measure induction impact\n",
    "\n",
    "def create_repeated_sequence(seq_len=20, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    first_half = torch.randint(1000, 10000, (1, seq_len), device=\"cuda\")\n",
    "    return torch.cat([first_half, first_half], dim=1)\n",
    "\n",
    "def measure_induction_accuracy(model, heads_to_ablate=None, n_samples=5, seq_len=20):\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    for seed in range(n_samples):\n",
    "        tokens = create_repeated_sequence(seq_len, seed=seed)\n",
    "        \n",
    "        if heads_to_ablate:\n",
    "            hooks = []\n",
    "            for layer, head in heads_to_ablate:\n",
    "                def ablate(act, hook, h=head):\n",
    "                    act[:, :, h, :] = 0\n",
    "                    return act\n",
    "                hooks.append((f\"blocks.{layer}.attn.hook_z\", ablate))\n",
    "            logits = model.run_with_hooks(tokens, fwd_hooks=hooks)\n",
    "        else:\n",
    "            logits = model(tokens)\n",
    "        \n",
    "        predictions = logits[0, :-1, :].argmax(dim=-1)\n",
    "        \n",
    "        for pos in range(seq_len, 2 * seq_len - 1):\n",
    "            if predictions[pos] == tokens[0, pos + 1]:\n",
    "                total_correct += 1\n",
    "            total_count += 1\n",
    "    \n",
    "    return total_correct / total_count\n",
    "\n",
    "# Baseline\n",
    "baseline_acc = measure_induction_accuracy(model)\n",
    "print(f\"Baseline induction accuracy: {baseline_acc:.1%}\")\n",
    "\n",
    "# Ablate previous token heads\n",
    "prev_heads = [(l, h) for l, h, _ in top_prev_heads[:3]] if top_prev_heads else [(1, 5), (2, 2)]\n",
    "ablated_acc = measure_induction_accuracy(model, heads_to_ablate=prev_heads)\n",
    "print(f\"With previous token heads ablated: {ablated_acc:.1%}\")\n",
    "print(f\"Accuracy drop: {(baseline_acc - ablated_acc)*100:.1f}%\")\n",
    "\n",
    "print(\"\\nConclusion: Ablating previous token heads significantly hurts induction!\")\n",
    "print(\"This confirms they're essential partners to induction heads.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Natural Language Induction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Test induction on natural language\n",
    "\n",
    "natural_prompts = [\n",
    "    (\"Harry Potter is a famous wizard. Harry\", \" Potter\"),\n",
    "    (\"New York City is very large. New York\", \" City\"),\n",
    "    (\"The quick brown fox jumped. The quick brown\", \" fox\"),\n",
    "]\n",
    "\n",
    "for prompt, expected in natural_prompts:\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    logits = model(tokens)\n",
    "    \n",
    "    # Get prediction\n",
    "    probs = torch.softmax(logits[0, -1, :], dim=-1)\n",
    "    top_token = logits[0, -1, :].argmax().item()\n",
    "    pred = model.tokenizer.decode(top_token)\n",
    "    \n",
    "    expected_token = model.to_single_token(expected)\n",
    "    expected_prob = probs[expected_token].item()\n",
    "    \n",
    "    match = \"✓\" if pred.strip() == expected.strip() else \"✗\"\n",
    "    print(f\"{match} '{prompt}' → predicted '{pred}' (expected '{expected}')\")\n",
    "    print(f\"   Probability of expected: {expected_prob:.2%}\")\n",
    "\n",
    "# Analyze attention for a specific example\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Analyzing: 'Harry Potter...Harry'\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "prompt = \"Harry Potter is a wizard. Harry\"\n",
    "tokens = model.to_tokens(prompt)\n",
    "token_strs = model.to_str_tokens(tokens)\n",
    "_, cache = model.run_with_cache(tokens)\n",
    "\n",
    "print(\"Tokens:\")\n",
    "for i, t in enumerate(token_strs):\n",
    "    print(f\"  {i}: '{t}'\")\n",
    "\n",
    "# Find position of first Harry (0) and Potter (1)\n",
    "# Last Harry is at position 7\n",
    "last_harry_pos = len(token_strs) - 1\n",
    "potter_pos = 1\n",
    "\n",
    "# Check known induction heads (from GPT-2 Small: L5H5, L6H9, etc.)\n",
    "known_induction = [(5, 5), (5, 1), (6, 9)]\n",
    "\n",
    "print(f\"\\nAttention from last Harry (pos {last_harry_pos}) to Potter (pos {potter_pos}):\")\n",
    "for layer, head in known_induction:\n",
    "    pattern = cache[\"pattern\", layer][0, head]\n",
    "    attn = pattern[last_harry_pos, potter_pos].item()\n",
    "    print(f\"  L{layer}H{head}: {attn:.3f}\")\n",
    "\n",
    "print(\"\\nInduction heads attend to the token AFTER the previous occurrence,\")\n",
    "print(\"which is 'Potter' - this enables them to complete the pattern!\")\n",
    "\n",
    "del cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
