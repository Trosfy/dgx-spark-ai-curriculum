{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab C.4: Feature Extraction with SAEs - SOLUTIONS\n",
    "\n",
    "This notebook contains solutions to all exercises from Lab C.4.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from transformer_lens import HookedTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAE class (from main notebook)\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, d_model, n_features):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_features = n_features\n",
    "        self.encoder = nn.Linear(d_model, n_features, bias=True)\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(d_model))\n",
    "        nn.init.kaiming_uniform_(self.encoder.weight, nonlinearity='relu')\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return F.relu(self.encoder(x))\n",
    "    \n",
    "    def decode(self, features):\n",
    "        return F.linear(features, self.encoder.weight.T, self.decoder_bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.encode(x)\n",
    "        return self.decode(features), features\n",
    "    \n",
    "    def compute_loss(self, x, sparsity_coef=1e-3):\n",
    "        reconstructed, features = self.forward(x)\n",
    "        recon_loss = F.mse_loss(reconstructed, x)\n",
    "        sparsity_loss = features.abs().mean()\n",
    "        return recon_loss + sparsity_coef * sparsity_loss, {\n",
    "            'recon': recon_loss.item(),\n",
    "            'sparsity': sparsity_loss.item(),\n",
    "            'n_active': (features > 0).sum(dim=-1).float().mean().item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Different Sparsity Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Compare different sparsity coefficients\n",
    "\n",
    "# Collect activations\n",
    "prompts = [\n",
    "    \"The capital of France is Paris.\",\n",
    "    \"def fibonacci(n): return n if n <= 1 else\",\n",
    "    \"What is the meaning of life?\",\n",
    "    \"I am so happy today!\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\"\n",
    "] * 10  # Repeat for more data\n",
    "\n",
    "activations = []\n",
    "layer = 6\n",
    "with torch.no_grad():\n",
    "    for prompt in prompts:\n",
    "        tokens = model.to_tokens(prompt)\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        activations.append(cache[\"resid_post\", layer][0])\n",
    "        del cache\n",
    "\n",
    "activations = torch.cat(activations, dim=0)\n",
    "print(f\"Activations: {activations.shape}\")\n",
    "\n",
    "# Train with different sparsity levels\n",
    "sparsity_coefs = [1e-2, 1e-3, 1e-4]\n",
    "results = {}\n",
    "\n",
    "for coef in sparsity_coefs:\n",
    "    print(f\"\\nTraining with sparsity_coef={coef}\")\n",
    "    sae = SparseAutoencoder(model.cfg.d_model, model.cfg.d_model * 4).to(device)\n",
    "    optimizer = torch.optim.Adam(sae.parameters(), lr=1e-3)\n",
    "    \n",
    "    dataset = TensorDataset(activations)\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        for batch in loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss, _ = sae.compute_loss(batch[0], sparsity_coef=coef)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Evaluate\n",
    "    with torch.no_grad():\n",
    "        _, stats = sae.compute_loss(activations, sparsity_coef=coef)\n",
    "    \n",
    "    results[coef] = stats\n",
    "    print(f\"  Recon loss: {stats['recon']:.4f}\")\n",
    "    print(f\"  Active features: {stats['n_active']:.1f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Summary:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Sparsity Coef':<15} {'Recon Loss':<12} {'Active Features':<15}\")\n",
    "for coef, stats in results.items():\n",
    "    print(f\"{coef:<15} {stats['recon']:<12.4f} {stats['n_active']:<15.1f}\")\n",
    "\n",
    "print(\"\\nConclusion:\")\n",
    "print(\"- Higher sparsity = fewer active features but worse reconstruction\")\n",
    "print(\"- Lower sparsity = more active features, potentially less interpretable\")\n",
    "print(\"- Sweet spot is typically 1e-4 to 1e-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Feature Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Find co-activating features\n",
    "\n",
    "# Train a fresh SAE\n",
    "sae = SparseAutoencoder(model.cfg.d_model, model.cfg.d_model * 4).to(device)\n",
    "optimizer = torch.optim.Adam(sae.parameters(), lr=1e-3)\n",
    "\n",
    "for _ in range(50):\n",
    "    for batch in DataLoader(TensorDataset(activations), batch_size=64, shuffle=True):\n",
    "        optimizer.zero_grad()\n",
    "        loss, _ = sae.compute_loss(batch[0], sparsity_coef=5e-4)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Get feature activations\n",
    "with torch.no_grad():\n",
    "    features = sae.encode(activations).cpu().numpy()  # [n_samples, n_features]\n",
    "\n",
    "# Compute correlation matrix (for active features only)\n",
    "feature_active = (features > 0).mean(axis=0) > 0.01  # Features active >1% of time\n",
    "active_indices = np.where(feature_active)[0]\n",
    "\n",
    "if len(active_indices) > 0:\n",
    "    active_features = features[:, active_indices]\n",
    "    correlation = np.corrcoef(active_features.T)\n",
    "    \n",
    "    # Find highly correlated pairs (excluding self-correlation)\n",
    "    np.fill_diagonal(correlation, 0)\n",
    "    \n",
    "    # Top 10 most correlated pairs\n",
    "    flat_corr = correlation.flatten()\n",
    "    top_idx = np.argsort(flat_corr)[-20:][::-1]  # Top 20 (pairs appear twice)\n",
    "    \n",
    "    print(\"Top correlated feature pairs:\")\n",
    "    seen = set()\n",
    "    for idx in top_idx:\n",
    "        i = idx // len(active_indices)\n",
    "        j = idx % len(active_indices)\n",
    "        if i < j and (i, j) not in seen:\n",
    "            feat_i = active_indices[i]\n",
    "            feat_j = active_indices[j]\n",
    "            print(f\"  Feature {feat_i} <-> Feature {feat_j}: corr = {correlation[i, j]:.3f}\")\n",
    "            seen.add((i, j))\n",
    "            if len(seen) >= 10:\n",
    "                break\n",
    "    \n",
    "    # Visualize correlation matrix\n",
    "    if len(active_indices) <= 100:  # Only if manageable size\n",
    "        fig = px.imshow(\n",
    "            correlation,\n",
    "            title=\"Feature Correlation Matrix\",\n",
    "            color_continuous_scale=\"RdBu_r\",\n",
    "            color_continuous_midpoint=0\n",
    "        )\n",
    "        fig.update_layout(width=600, height=600)\n",
    "        fig.show()\n",
    "else:\n",
    "    print(\"No features active frequently enough for correlation analysis\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"Features that co-activate might represent related concepts\")\n",
    "print(\"or be part of the same underlying 'circuit' in the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Different Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Compare SAEs on early vs late layers\n",
    "\n",
    "layers_to_compare = [2, 10]  # Early and late\n",
    "layer_saes = {}\n",
    "\n",
    "for layer in layers_to_compare:\n",
    "    print(f\"\\nLayer {layer}:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Collect activations\n",
    "    layer_acts = []\n",
    "    with torch.no_grad():\n",
    "        for prompt in prompts:\n",
    "            tokens = model.to_tokens(prompt)\n",
    "            _, cache = model.run_with_cache(tokens)\n",
    "            layer_acts.append(cache[\"resid_post\", layer][0])\n",
    "            del cache\n",
    "    layer_acts = torch.cat(layer_acts, dim=0)\n",
    "    \n",
    "    # Train SAE\n",
    "    sae = SparseAutoencoder(model.cfg.d_model, model.cfg.d_model * 4).to(device)\n",
    "    optimizer = torch.optim.Adam(sae.parameters(), lr=1e-3)\n",
    "    \n",
    "    for _ in range(50):\n",
    "        for batch in DataLoader(TensorDataset(layer_acts), batch_size=64, shuffle=True):\n",
    "            optimizer.zero_grad()\n",
    "            loss, _ = sae.compute_loss(batch[0], sparsity_coef=5e-4)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    layer_saes[layer] = sae\n",
    "    \n",
    "    # Analyze features\n",
    "    with torch.no_grad():\n",
    "        features = sae.encode(layer_acts)\n",
    "    \n",
    "    # Find most active features\n",
    "    mean_acts = features.mean(dim=0)\n",
    "    top_features = mean_acts.argsort(descending=True)[:5]\n",
    "    \n",
    "    print(f\"Top 5 most active features: {top_features.tolist()}\")\n",
    "    print(f\"Average active features: {(features > 0).sum(dim=-1).float().mean():.1f}\")\n",
    "\n",
    "# Compare what activates features in each layer\n",
    "test_prompts = {\n",
    "    \"Code\": \"def hello_world():\",\n",
    "    \"Question\": \"What is the capital?\",\n",
    "    \"Emotion\": \"I am very happy!\",\n",
    "    \"Factual\": \"The moon orbits Earth.\"\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Feature activation comparison:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, prompt in test_prompts.items():\n",
    "    print(f\"\\n'{prompt}' ({name}):\")\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    _, cache = model.run_with_cache(tokens)\n",
    "    \n",
    "    for layer, sae in layer_saes.items():\n",
    "        acts = cache[\"resid_post\", layer][0, -1, :]  # Last token\n",
    "        with torch.no_grad():\n",
    "            features = sae.encode(acts.unsqueeze(0))\n",
    "        \n",
    "        n_active = (features > 0).sum().item()\n",
    "        max_feat = features.argmax().item()\n",
    "        max_val = features.max().item()\n",
    "        \n",
    "        print(f\"  Layer {layer}: {n_active} active features, strongest=F{max_feat} ({max_val:.2f})\")\n",
    "    \n",
    "    del cache\n",
    "\n",
    "print(\"\\nConclusion:\")\n",
    "print(\"- Early layers often capture more syntactic/positional features\")\n",
    "print(\"- Later layers capture more semantic/conceptual features\")\n",
    "print(\"- This aligns with the 'hierarchy of abstraction' in deep networks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del activations, sae, layer_saes\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
