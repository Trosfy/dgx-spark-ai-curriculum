{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab C.4: Feature Extraction with SAEs\n",
    "\n",
    "**Module:** C - Mechanistic Interpretability  \n",
    "**Time:** 2.5 hours  \n",
    "**Difficulty:** ⭐⭐⭐⭐⭐ (Expert)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand why sparse autoencoders (SAEs) are powerful for interpretability\n",
    "- [ ] Implement a simple SAE architecture\n",
    "- [ ] Train an SAE on model activations\n",
    "- [ ] Extract and interpret learned features\n",
    "- [ ] Attempt activation steering using discovered features\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Labs C.1-C.3\n",
    "- Knowledge of: Attention patterns, residual stream, activation patching\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "**The Superposition Problem**: Neural networks pack way more concepts into their neurons than there are neurons! This is called *superposition* - a single neuron might respond to \"dogs\", \"wheels\", and \"the color blue\" simultaneously.\n",
    "\n",
    "**Sparse Autoencoders (SAEs)** are a breakthrough technique for \"unpacking\" these superposed representations into individual, interpretable features. Anthropic's work on [Towards Monosemanticity](https://transformer-circuits.pub/2023/monosemantic-features/index.html) showed SAEs can find millions of interpretable features in Claude.\n",
    "\n",
    "**Applications:**\n",
    "- Finding safety-relevant features (deception, manipulation)\n",
    "- Understanding model reasoning\n",
    "- Steering model behavior precisely\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELI5: What are Sparse Autoencoders?\n",
    "\n",
    "> **Imagine a radio signal** that's a mix of many songs playing at once. Each song is a \"feature\", and they're all superposed (overlapping) in the signal.\n",
    ">\n",
    "> **A regular listener** hears a jumbled mess - the raw signal is hard to interpret.\n",
    ">\n",
    "> **A Sparse Autoencoder** is like a magical music separator:\n",
    "> 1. It takes the mixed signal\n",
    "> 2. Expands it into many \"channels\" (more than the original dimensions)\n",
    "> 3. Forces most channels to be silent (sparsity)\n",
    "> 4. Each active channel plays ONE song clearly!\n",
    ">\n",
    "> **For neural networks:**\n",
    "> - Input: 768-dimensional residual stream (mixed signals)\n",
    "> - Expand to: 768 × 4 = 3072 features (many channels)\n",
    "> - Sparsity: Only ~10-50 features active at once\n",
    "> - Output: Each feature represents ONE concept (\"Python\", \"negative sentiment\", \"question\", etc.)\n",
    "\n",
    "```\n",
    "Superposed Activations      SAE Features\n",
    "      [Mixed]         →    [Dog][Code][French][ ][ ][Happy]\n",
    "     (768-dim)              (3072-dim, mostly zeros)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELI5: Why Sparsity Matters\n",
    "\n",
    "> **Without sparsity**: The SAE just learns to copy inputs. Every feature fires a little, giving us nothing interpretable.\n",
    ">\n",
    "> **With sparsity**: The SAE is forced to \"choose\" which few features to activate. This encourages each feature to specialize in one specific concept!\n",
    ">\n",
    "> **The math**: We add an L1 penalty: `loss = reconstruction_error + λ * ||features||₁`\n",
    ">\n",
    "> L1 pushes features toward zero, so only the most important ones survive!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "# TransformerLens\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    device=device\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model.cfg.n_layers} layers, d_model={model.cfg.d_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Implementing a Sparse Autoencoder\n",
    "\n",
    "Let's build an SAE from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse Autoencoder for extracting interpretable features.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input: residual stream activations [batch, d_model]\n",
    "    - Encoder: Linear + ReLU → sparse feature activations [batch, n_features]\n",
    "    - Decoder: Linear → reconstructed activations [batch, d_model]\n",
    "    \n",
    "    The encoder expansion (n_features > d_model) allows representing more\n",
    "    features than dimensions. Sparsity ensures only a few activate at once.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_features: int,\n",
    "        tied_weights: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Dimension of input activations (e.g., 768 for GPT-2 Small)\n",
    "            n_features: Number of sparse features (typically 2-8x d_model)\n",
    "            tied_weights: If True, decoder weights = encoder weights transposed\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_features = n_features\n",
    "        self.tied_weights = tied_weights\n",
    "        \n",
    "        # Encoder: d_model → n_features\n",
    "        self.encoder = nn.Linear(d_model, n_features, bias=True)\n",
    "        \n",
    "        # Decoder: n_features → d_model\n",
    "        if tied_weights:\n",
    "            # Decoder shares weights with encoder (transposed)\n",
    "            self.decoder_bias = nn.Parameter(torch.zeros(d_model))\n",
    "        else:\n",
    "            self.decoder = nn.Linear(n_features, d_model, bias=True)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights using Kaiming initialization.\"\"\"\n",
    "        nn.init.kaiming_uniform_(self.encoder.weight, nonlinearity='relu')\n",
    "        nn.init.zeros_(self.encoder.bias)\n",
    "        \n",
    "        if not self.tied_weights:\n",
    "            nn.init.kaiming_uniform_(self.decoder.weight, nonlinearity='linear')\n",
    "            nn.init.zeros_(self.decoder.bias)\n",
    "    \n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode input to sparse features.\n",
    "        \n",
    "        Args:\n",
    "            x: Input activations [batch, d_model]\n",
    "        \n",
    "        Returns:\n",
    "            Sparse feature activations [batch, n_features]\n",
    "        \"\"\"\n",
    "        return F.relu(self.encoder(x))  # ReLU ensures non-negative, sparse\n",
    "    \n",
    "    def decode(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Decode features back to original space.\n",
    "        \n",
    "        Args:\n",
    "            features: Sparse features [batch, n_features]\n",
    "        \n",
    "        Returns:\n",
    "            Reconstructed activations [batch, d_model]\n",
    "        \"\"\"\n",
    "        if self.tied_weights:\n",
    "            # Use encoder weight transposed\n",
    "            return F.linear(features, self.encoder.weight.T, self.decoder_bias)\n",
    "        else:\n",
    "            return self.decoder(features)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Full forward pass.\n",
    "        \n",
    "        Returns:\n",
    "            (reconstructed, features) tuple\n",
    "        \"\"\"\n",
    "        features = self.encode(x)\n",
    "        reconstructed = self.decode(features)\n",
    "        return reconstructed, features\n",
    "    \n",
    "    def compute_loss(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        sparsity_coef: float = 1e-3\n",
    "    ) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Compute SAE loss with sparsity penalty.\n",
    "        \n",
    "        Loss = MSE(x, reconstructed) + sparsity_coef * L1(features)\n",
    "        \n",
    "        Args:\n",
    "            x: Input activations\n",
    "            sparsity_coef: Weight for L1 sparsity penalty\n",
    "        \n",
    "        Returns:\n",
    "            (total_loss, loss_dict) tuple\n",
    "        \"\"\"\n",
    "        reconstructed, features = self.forward(x)\n",
    "        \n",
    "        # Reconstruction loss (MSE)\n",
    "        recon_loss = F.mse_loss(reconstructed, x)\n",
    "        \n",
    "        # Sparsity loss (L1)\n",
    "        sparsity_loss = features.abs().mean()\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = recon_loss + sparsity_coef * sparsity_loss\n",
    "        \n",
    "        # Compute sparsity statistics\n",
    "        with torch.no_grad():\n",
    "            sparsity = (features > 0).float().mean().item()  # Fraction of non-zero\n",
    "            n_active = (features > 0).sum(dim=-1).float().mean().item()  # Avg active features\n",
    "        \n",
    "        loss_dict = {\n",
    "            'total': total_loss.item(),\n",
    "            'recon': recon_loss.item(),\n",
    "            'sparsity': sparsity_loss.item(),\n",
    "            'frac_nonzero': sparsity,\n",
    "            'n_active': n_active\n",
    "        }\n",
    "        \n",
    "        return total_loss, loss_dict\n",
    "\n",
    "# Test SAE\n",
    "sae = SparseAutoencoder(\n",
    "    d_model=model.cfg.d_model,  # 768\n",
    "    n_features=model.cfg.d_model * 4,  # 3072 features\n",
    "    tied_weights=True\n",
    ").to(device)\n",
    "\n",
    "print(f\"SAE created:\")\n",
    "print(f\"  Input dim: {sae.d_model}\")\n",
    "print(f\"  Features: {sae.n_features}\")\n",
    "print(f\"  Expansion: {sae.n_features / sae.d_model}x\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in sae.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "test_input = torch.randn(32, model.cfg.d_model, device=device)\n",
    "reconstructed, features = sae(test_input)\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Features shape: {features.shape}\")\n",
    "print(f\"Reconstructed shape: {reconstructed.shape}\")\n",
    "print(f\"\\nFeature statistics:\")\n",
    "print(f\"  Min: {features.min().item():.4f}\")\n",
    "print(f\"  Max: {features.max().item():.4f}\")\n",
    "print(f\"  Fraction > 0: {(features > 0).float().mean().item():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Collecting Training Data\n",
    "\n",
    "We need diverse activations from the model to train our SAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample prompts for diverse activations\n",
    "SAMPLE_PROMPTS = [\n",
    "    # Factual\n",
    "    \"The capital of France is Paris.\",\n",
    "    \"Water boils at 100 degrees Celsius.\",\n",
    "    \"The Earth orbits around the Sun.\",\n",
    "    \n",
    "    # Code\n",
    "    \"def fibonacci(n):\\n    if n <= 1:\",\n",
    "    \"import numpy as np\\narray = np.zeros((10, 10))\",\n",
    "    \"for i in range(10):\\n    print(i)\",\n",
    "    \n",
    "    # Questions\n",
    "    \"What is the meaning of life?\",\n",
    "    \"How do neural networks learn?\",\n",
    "    \"Why is the sky blue?\",\n",
    "    \n",
    "    # Stories\n",
    "    \"Once upon a time, there was a brave knight who\",\n",
    "    \"In a galaxy far, far away,\",\n",
    "    \"The detective examined the mysterious letter carefully.\",\n",
    "    \n",
    "    # Emotions\n",
    "    \"I am so happy today!\",\n",
    "    \"This is absolutely terrible.\",\n",
    "    \"I feel uncertain about the future.\",\n",
    "    \n",
    "    # Instructions\n",
    "    \"Please summarize the following text:\",\n",
    "    \"Translate this to French:\",\n",
    "    \"Explain like I'm five:\",\n",
    "    \n",
    "    # Math\n",
    "    \"2 + 2 = 4, 3 + 3 = 6, 4 + 4 =\",\n",
    "    \"The derivative of x^2 is 2x.\",\n",
    "    \n",
    "    # More diverse content\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Breaking news: Scientists discover new species\",\n",
    "    \"Recipe: Mix flour, eggs, and sugar together.\",\n",
    "    \"Hello! How are you today?\",\n",
    "    \"ERROR: Connection refused on port 8080\",\n",
    "    \"According to the latest research,\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_activations(\n",
    "    model: HookedTransformer,\n",
    "    prompts: List[str],\n",
    "    layer: int,\n",
    "    position: str = \"all\"  # \"all\", \"last\", or \"random\"\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Collect residual stream activations from model.\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model\n",
    "        prompts: List of text prompts\n",
    "        layer: Which layer to extract from\n",
    "        position: Which positions to keep\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of activations [n_samples, d_model]\n",
    "    \"\"\"\n",
    "    all_activations = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for prompt in tqdm(prompts, desc=\"Collecting activations\"):\n",
    "            tokens = model.to_tokens(prompt)\n",
    "            _, cache = model.run_with_cache(tokens)\n",
    "            \n",
    "            # Get residual stream after this layer\n",
    "            resid = cache[\"resid_post\", layer][0]  # [seq, d_model]\n",
    "            \n",
    "            if position == \"all\":\n",
    "                all_activations.append(resid)\n",
    "            elif position == \"last\":\n",
    "                all_activations.append(resid[-1:])  # Keep batch dim\n",
    "            elif position == \"random\":\n",
    "                idx = torch.randint(0, resid.shape[0], (1,))\n",
    "                all_activations.append(resid[idx])\n",
    "            \n",
    "            del cache\n",
    "    \n",
    "    return torch.cat(all_activations, dim=0)\n",
    "\n",
    "# Collect activations from middle layer\n",
    "layer = 6  # Middle layer of GPT-2 Small\n",
    "activations = collect_activations(model, SAMPLE_PROMPTS, layer=layer, position=\"all\")\n",
    "\n",
    "print(f\"Collected activations: {activations.shape}\")\n",
    "print(f\"Activation stats:\")\n",
    "print(f\"  Mean: {activations.mean().item():.4f}\")\n",
    "print(f\"  Std: {activations.std().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create more training data by generating random tokens too\n",
    "def generate_random_activations(\n",
    "    model: HookedTransformer,\n",
    "    n_batches: int = 50,\n",
    "    seq_len: int = 20,\n",
    "    layer: int = 6\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Generate activations from random token sequences.\"\"\"\n",
    "    all_activations = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(n_batches), desc=\"Generating random activations\"):\n",
    "            # Random tokens (avoiding special tokens)\n",
    "            tokens = torch.randint(1000, 40000, (1, seq_len), device=device)\n",
    "            _, cache = model.run_with_cache(tokens)\n",
    "            \n",
    "            resid = cache[\"resid_post\", layer][0]\n",
    "            all_activations.append(resid)\n",
    "            \n",
    "            del cache\n",
    "    \n",
    "    return torch.cat(all_activations, dim=0)\n",
    "\n",
    "# Generate additional random activations\n",
    "random_activations = generate_random_activations(model, n_batches=50, layer=layer)\n",
    "print(f\"Random activations: {random_activations.shape}\")\n",
    "\n",
    "# Combine all activations\n",
    "all_activations = torch.cat([activations, random_activations], dim=0)\n",
    "print(f\"\\nTotal activations: {all_activations.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Training the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sae(\n",
    "    sae: SparseAutoencoder,\n",
    "    activations: torch.Tensor,\n",
    "    n_epochs: int = 100,\n",
    "    batch_size: int = 256,\n",
    "    lr: float = 1e-3,\n",
    "    sparsity_coef: float = 1e-3\n",
    ") -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Train the SAE on collected activations.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of loss histories\n",
    "    \"\"\"\n",
    "    # Create dataloader\n",
    "    dataset = TensorDataset(activations)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(sae.parameters(), lr=lr)\n",
    "    \n",
    "    # History\n",
    "    history = {\n",
    "        'total': [], 'recon': [], 'sparsity': [],\n",
    "        'frac_nonzero': [], 'n_active': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(n_epochs), desc=\"Training SAE\"):\n",
    "        epoch_losses = {k: [] for k in history.keys()}\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            x = batch[0]  # [batch, d_model]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss, loss_dict = sae.compute_loss(x, sparsity_coef=sparsity_coef)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            for k, v in loss_dict.items():\n",
    "                epoch_losses[k].append(v)\n",
    "        \n",
    "        # Record epoch averages\n",
    "        for k in history.keys():\n",
    "            history[k].append(np.mean(epoch_losses[k]))\n",
    "        \n",
    "        # Print progress every 20 epochs\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}: recon={history['recon'][-1]:.4f}, \"\n",
    "                  f\"sparsity={history['frac_nonzero'][-1]:.2%}, \"\n",
    "                  f\"n_active={history['n_active'][-1]:.1f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize SAE for fresh training\n",
    "sae = SparseAutoencoder(\n",
    "    d_model=model.cfg.d_model,\n",
    "    n_features=model.cfg.d_model * 4,  # 4x expansion\n",
    "    tied_weights=True\n",
    ").to(device)\n",
    "\n",
    "# Train\n",
    "history = train_sae(\n",
    "    sae,\n",
    "    all_activations,\n",
    "    n_epochs=100,\n",
    "    batch_size=256,\n",
    "    lr=1e-3,\n",
    "    sparsity_coef=5e-4  # Tune this for desired sparsity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "axes[0, 0].plot(history['recon'])\n",
    "axes[0, 0].set_title('Reconstruction Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "\n",
    "axes[0, 1].plot(history['sparsity'])\n",
    "axes[0, 1].set_title('Sparsity Loss (L1)')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "\n",
    "axes[1, 0].plot(history['frac_nonzero'])\n",
    "axes[1, 0].set_title('Fraction of Features Active')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylim(0, 0.5)\n",
    "\n",
    "axes[1, 1].plot(history['n_active'])\n",
    "axes[1, 1].set_title('Avg Number of Active Features')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal statistics:\")\n",
    "print(f\"  Reconstruction loss: {history['recon'][-1]:.4f}\")\n",
    "print(f\"  Active features: {history['n_active'][-1]:.1f} / {sae.n_features}\")\n",
    "print(f\"  Sparsity: {history['frac_nonzero'][-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Analyzing Learned Features\n",
    "\n",
    "Now let's explore what features the SAE learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_activating_examples(\n",
    "    sae: SparseAutoencoder,\n",
    "    model: HookedTransformer,\n",
    "    prompts: List[str],\n",
    "    layer: int,\n",
    "    feature_idx: int,\n",
    "    top_k: int = 5\n",
    ") -> List[Tuple[str, int, float]]:\n",
    "    \"\"\"\n",
    "    Find examples that most activate a specific feature.\n",
    "    \n",
    "    Returns:\n",
    "        List of (prompt, token_position, activation_value) tuples\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for prompt in prompts:\n",
    "            tokens = model.to_tokens(prompt)\n",
    "            _, cache = model.run_with_cache(tokens)\n",
    "            \n",
    "            resid = cache[\"resid_post\", layer][0]  # [seq, d_model]\n",
    "            features = sae.encode(resid)  # [seq, n_features]\n",
    "            \n",
    "            # Get activations for this feature\n",
    "            feature_acts = features[:, feature_idx].cpu().numpy()\n",
    "            \n",
    "            # Get token strings\n",
    "            token_strs = model.to_str_tokens(tokens)\n",
    "            \n",
    "            for pos, act in enumerate(feature_acts):\n",
    "                if act > 0:\n",
    "                    results.append((prompt, token_strs[pos], pos, act))\n",
    "            \n",
    "            del cache\n",
    "    \n",
    "    # Sort by activation and return top k\n",
    "    results.sort(key=lambda x: -x[3])\n",
    "    return results[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most active features overall\n",
    "def find_most_active_features(\n",
    "    sae: SparseAutoencoder,\n",
    "    activations: torch.Tensor,\n",
    "    top_k: int = 20\n",
    ") -> List[Tuple[int, float, float]]:\n",
    "    \"\"\"\n",
    "    Find features that activate most frequently and strongly.\n",
    "    \n",
    "    Returns:\n",
    "        List of (feature_idx, mean_activation, frequency) tuples\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        features = sae.encode(activations)  # [n_samples, n_features]\n",
    "        \n",
    "        # Mean activation when active\n",
    "        mean_acts = (features * (features > 0).float()).sum(dim=0) / (features > 0).float().sum(dim=0).clamp(min=1)\n",
    "        \n",
    "        # Frequency of activation\n",
    "        frequency = (features > 0).float().mean(dim=0)\n",
    "        \n",
    "        # Combined score\n",
    "        scores = mean_acts * frequency\n",
    "        \n",
    "        top_indices = torch.argsort(scores, descending=True)[:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append((\n",
    "                idx.item(),\n",
    "                mean_acts[idx].item(),\n",
    "                frequency[idx].item()\n",
    "            ))\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Find most active features\n",
    "top_features = find_most_active_features(sae, all_activations, top_k=20)\n",
    "\n",
    "print(\"Most Active Features:\")\n",
    "print(\"=\" * 50)\n",
    "for idx, mean_act, freq in top_features:\n",
    "    print(f\"Feature {idx:4d}: mean_act={mean_act:.3f}, freq={freq:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze specific features\n",
    "def analyze_feature(feature_idx: int):\n",
    "    \"\"\"Analyze what a specific feature responds to.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FEATURE {feature_idx}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Find top activating examples\n",
    "    top_examples = find_top_activating_examples(\n",
    "        sae, model, SAMPLE_PROMPTS, layer=layer,\n",
    "        feature_idx=feature_idx, top_k=10\n",
    "    )\n",
    "    \n",
    "    if top_examples:\n",
    "        print(\"\\nTop activating tokens:\")\n",
    "        for prompt, token, pos, act in top_examples:\n",
    "            print(f\"  [{act:.3f}] '{token}' in \\\"{prompt[:40]}...\\\"\")\n",
    "    else:\n",
    "        print(\"  (No strong activations found in sample prompts)\")\n",
    "\n",
    "# Analyze a few top features\n",
    "for idx, _, _ in top_features[:5]:\n",
    "    analyze_feature(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature activations on a specific prompt\n",
    "def visualize_feature_activations(\n",
    "    prompt: str,\n",
    "    top_k_features: int = 10\n",
    "):\n",
    "    \"\"\"Visualize which features activate for each token in a prompt.\"\"\"\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    token_strs = model.to_str_tokens(tokens)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        resid = cache[\"resid_post\", layer][0]\n",
    "        features = sae.encode(resid).cpu().numpy()  # [seq, n_features]\n",
    "    \n",
    "    # Find features with highest max activation\n",
    "    max_acts = features.max(axis=0)\n",
    "    top_feature_indices = np.argsort(max_acts)[-top_k_features:][::-1]\n",
    "    \n",
    "    # Create heatmap\n",
    "    selected_features = features[:, top_feature_indices].T  # [k_features, seq]\n",
    "    \n",
    "    fig = px.imshow(\n",
    "        selected_features,\n",
    "        labels={\"x\": \"Token\", \"y\": \"Feature\", \"color\": \"Activation\"},\n",
    "        x=token_strs,\n",
    "        y=[f\"F{i}\" for i in top_feature_indices],\n",
    "        color_continuous_scale=\"YlOrRd\",\n",
    "        title=f\"Feature Activations for: \\\"{prompt}\\\"\"\n",
    "    )\n",
    "    fig.update_layout(width=800, height=400)\n",
    "    fig.show()\n",
    "\n",
    "# Visualize for different prompts\n",
    "visualize_feature_activations(\"def fibonacci(n):\\n    if n <= 1:\")\n",
    "visualize_feature_activations(\"The capital of France is Paris.\")\n",
    "visualize_feature_activations(\"I am so happy today!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 6: Activation Steering\n\nCan we modify model behavior by adding/subtracting feature directions?\n\n### Text Generation with TransformerLens\n\nBefore we steer, let's understand how to generate text with TransformerLens:\n\n```python\n# model.generate() - Autoregressive text generation\n# Key parameters:\n#   - input: Token tensor [batch, seq] to start from\n#   - max_new_tokens: How many new tokens to generate\n#   - do_sample: If False, use greedy decoding (pick highest probability)\n#                If True, sample from distribution (more creative)\n#   - temperature: Controls randomness when sampling (higher = more random)\n\ntokens = model.to_tokens(\"Hello world\")\noutput = model.generate(tokens, max_new_tokens=10, do_sample=False)\ntext = model.tokenizer.decode(output[0])\n```\n\nThis is the baseline we'll compare steering against!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steer_with_feature(\n",
    "    model: HookedTransformer,\n",
    "    sae: SparseAutoencoder,\n",
    "    prompt: str,\n",
    "    feature_idx: int,\n",
    "    strength: float = 5.0,\n",
    "    layer: int = 6\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate text while adding a feature direction.\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model\n",
    "        sae: Trained SAE\n",
    "        prompt: Input prompt\n",
    "        feature_idx: Which feature to add\n",
    "        strength: How much to add\n",
    "        layer: Which layer to intervene\n",
    "    \n",
    "    Returns:\n",
    "        Generated text\n",
    "    \"\"\"\n",
    "    # Get the feature direction from the decoder\n",
    "    with torch.no_grad():\n",
    "        # Create one-hot feature vector\n",
    "        feature_vec = torch.zeros(sae.n_features, device=device)\n",
    "        feature_vec[feature_idx] = 1.0\n",
    "        \n",
    "        # Get direction in residual stream space\n",
    "        direction = sae.decode(feature_vec.unsqueeze(0))[0]  # [d_model]\n",
    "        direction = direction / direction.norm() * strength\n",
    "    \n",
    "    # Define steering hook\n",
    "    def steering_hook(activation, hook):\n",
    "        # Add direction to all positions\n",
    "        return activation + direction\n",
    "    \n",
    "    # Generate with steering\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    \n",
    "    generated = []\n",
    "    for _ in range(20):  # Generate 20 tokens\n",
    "        with torch.no_grad():\n",
    "            logits = model.run_with_hooks(\n",
    "                tokens,\n",
    "                fwd_hooks=[(f\"blocks.{layer}.hook_resid_post\", steering_hook)]\n",
    "            )\n",
    "        \n",
    "        # Sample next token\n",
    "        next_token = logits[0, -1, :].argmax().unsqueeze(0).unsqueeze(0)\n",
    "        tokens = torch.cat([tokens, next_token], dim=1)\n",
    "        generated.append(model.tokenizer.decode(next_token[0].item()))\n",
    "    \n",
    "    return prompt + \"\".join(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare generation with and without steering\n",
    "prompt = \"The weather today is\"\n",
    "\n",
    "# Normal generation\n",
    "tokens = model.to_tokens(prompt)\n",
    "with torch.no_grad():\n",
    "    normal_output = model.generate(\n",
    "        tokens, max_new_tokens=20, do_sample=False\n",
    "    )\n",
    "normal_text = model.tokenizer.decode(normal_output[0])\n",
    "\n",
    "print(\"Normal generation:\")\n",
    "print(f\"  {normal_text}\")\n",
    "print()\n",
    "\n",
    "# Try steering with a few different features\n",
    "for feature_idx in [top_features[0][0], top_features[5][0], top_features[10][0]]:\n",
    "    steered_text = steer_with_feature(\n",
    "        model, sae, prompt, feature_idx,\n",
    "        strength=10.0, layer=layer\n",
    "    )\n",
    "    print(f\"Steered with feature {feature_idx}:\")\n",
    "    print(f\"  {steered_text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Steering Results\n",
    "\n",
    "If a feature represents a coherent concept, steering should produce semantically consistent changes. However:\n",
    "\n",
    "- Not all features are interpretable (some are polysemantic)\n",
    "- Steering strength needs tuning (too much = incoherent)\n",
    "- This is a simplified SAE - real SAEs are much larger\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try It Yourself\n",
    "\n",
    "### Exercise 1: Different Sparsity Levels\n",
    "Train SAEs with different sparsity coefficients (1e-2, 1e-3, 1e-4). How does this affect:\n",
    "- Reconstruction quality?\n",
    "- Number of active features?\n",
    "- Interpretability?\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Higher sparsity coefficient = fewer active features = potentially more monosemantic but higher reconstruction error.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 2: Feature Similarity\nFind pairs of features that often co-activate. What might this tell us about their relationship?\n\n<details>\n<summary>Hint</summary>\n\nCompute the correlation matrix of feature activations across all samples. High correlation = features that activate together.\n\n```python\n# Get all feature activations\nwith torch.no_grad():\n    features = sae.encode(all_activations).cpu().numpy()  # [n_samples, n_features]\n\n# Option 1: Using numpy's corrcoef\n# np.corrcoef computes Pearson correlation coefficients\n# It takes a 2D array where each row is a variable, columns are observations\ncorrelation_matrix = np.corrcoef(features.T)  # [n_features, n_features]\n\n# Option 2: Using pandas (more memory efficient for large matrices)\nimport pandas as pd\ndf_features = pd.DataFrame(features)\ncorrelation_matrix = df_features.corr().values\n\n# Find highly correlated pairs (excluding self-correlation on diagonal)\nthreshold = 0.5\nhigh_corr_pairs = []\nfor i in range(len(correlation_matrix)):\n    for j in range(i+1, len(correlation_matrix)):\n        if abs(correlation_matrix[i, j]) > threshold:\n            high_corr_pairs.append((i, j, correlation_matrix[i, j]))\n```\n</details>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Different Layers\n",
    "Train SAEs on early (layer 2) and late (layer 10) layers. Are features more abstract in later layers?\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Early layers often capture syntactic features (punctuation, parts of speech). Later layers capture semantic features (topics, concepts).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Insufficient Training Data\n",
    "```python\n",
    "# Wrong: Training on a few prompts\n",
    "activations = collect_activations(model, [\"Hello world\"], layer=6)\n",
    "\n",
    "# Correct: Diverse, large-scale data\n",
    "activations = collect_activations(model, many_diverse_prompts, layer=6)\n",
    "```\n",
    "**Why:** SAEs need diverse data to learn general features, not prompt-specific patterns.\n",
    "\n",
    "### Mistake 2: Wrong Sparsity Balance\n",
    "```python\n",
    "# Wrong: Too high sparsity\n",
    "loss = recon + 1.0 * sparsity  # Almost no features activate!\n",
    "\n",
    "# Correct: Balanced sparsity\n",
    "loss = recon + 1e-4 * sparsity  # ~50-200 active features\n",
    "```\n",
    "**Why:** Need enough active features to reconstruct, but sparse enough to be interpretable.\n",
    "\n",
    "### Mistake 3: Confusing Features with Neurons\n",
    "```python\n",
    "# SAE features ≠ model neurons!\n",
    "# Features are learned directions in activation space\n",
    "# They may not correspond to any single neuron\n",
    "```\n",
    "**Why:** SAEs extract *directions*, which are linear combinations of neurons.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- Why sparse autoencoders are useful for interpretability\n",
    "- How to implement and train an SAE\n",
    "- How to collect diverse activations for training\n",
    "- How to analyze and interpret learned features\n",
    "- How to steer model behavior using feature directions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge (Optional)\n",
    "\n",
    "**Replicate Anthropic's Feature Analysis**\n",
    "\n",
    "Anthropic's \"Towards Monosemanticity\" paper trained SAEs with millions of features. Try:\n",
    "\n",
    "1. Increase to 8x or 16x expansion\n",
    "2. Train on much more data (Wikipedia, code, etc.)\n",
    "3. Find specific interpretable features:\n",
    "   - \"Python code\"\n",
    "   - \"Questions\"\n",
    "   - \"Negative sentiment\"\n",
    "   - \"First-person narrative\"\n",
    "\n",
    "This requires more compute but is very rewarding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Large-scale SAE training\n",
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Towards Monosemanticity](https://transformer-circuits.pub/2023/monosemantic-features/index.html) - Anthropic's landmark paper\n",
    "- [Scaling Monosemanticity](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html) - SAEs on Claude\n",
    "- [Neuronpedia](https://www.neuronpedia.org/) - Interactive feature explorer\n",
    "- [SAE Lens](https://github.com/jbloomAus/SAELens) - Production SAE training library\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "del sae, all_activations, activations, random_activations\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Memory cleared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed Module C: Mechanistic Interpretability! You now have hands-on experience with:\n",
    "\n",
    "1. **TransformerLens** - The core interpretability toolkit\n",
    "2. **Activation Patching** - Finding causal mechanisms\n",
    "3. **Induction Heads** - A fundamental circuit for in-context learning\n",
    "4. **Sparse Autoencoders** - Extracting interpretable features\n",
    "\n",
    "These are the same techniques used at top AI labs like Anthropic, DeepMind, and OpenAI!\n",
    "\n",
    "### Where to Go From Here\n",
    "\n",
    "1. **Contribute to open research**: Check out [200 Concrete Problems](https://www.alignmentforum.org/posts/LbrPTJ4fmABEdEnLf/200-concrete-open-problems-in-mechanistic-interpretability)\n",
    "2. **Explore Neuronpedia**: Analyze pre-trained SAE features\n",
    "3. **Join the community**: MATS, ARENA, and AI Safety Camp programs\n",
    "4. **Apply to your projects**: Use these tools to understand your own models\n",
    "\n",
    "Good luck on your interpretability journey!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}