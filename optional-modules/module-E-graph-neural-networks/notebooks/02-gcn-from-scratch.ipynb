{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab E.2: Graph Convolutional Networks from Scratch\n",
    "\n",
    "**Module:** E - Graph Neural Networks  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê (Intermediate-Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand the message passing framework for GNNs\n",
    "- [ ] Implement a Graph Convolutional Network layer from scratch\n",
    "- [ ] Build and train a 2-layer GCN for node classification\n",
    "- [ ] Achieve >80% accuracy on the Cora dataset\n",
    "- [ ] Visualize learned node embeddings with t-SNE\n",
    "- [ ] Compare scratch implementation with PyG's optimized version\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Lab E.1 (PyTorch Geometric Setup)\n",
    "- Knowledge of: PyTorch nn.Module, matrix operations, basic neural networks\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**Graph Convolutional Networks power:**\n",
    "\n",
    "- **Google Maps:** Predicting traffic and ETA using road networks\n",
    "- **Drug Discovery:** Predicting if a molecule will be effective medicine\n",
    "- **Fraud Detection:** Identifying suspicious transaction patterns\n",
    "- **Recommendation Systems:** Pinterest's PinSage recommends billions of items\n",
    "\n",
    "GCNs are the foundation of modern graph learning. Understanding them deeply will help you tackle any graph problem.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßí ELI5: What Are Graph Convolutions?\n",
    "\n",
    "> **Imagine you're at a school party** where everyone wears a colored shirt (that's their \"feature\").\n",
    ">\n",
    "> To figure out what group someone belongs to, you could:\n",
    "> 1. Look at their own shirt color (their features)\n",
    "> 2. Look at the shirt colors of their friends (neighbor features)\n",
    "> 3. Mix these together to get a better picture\n",
    ">\n",
    "> **A Graph Convolution does exactly this:**\n",
    "> 1. Each person looks at their own features\n",
    "> 2. Collects features from friends (neighbors)\n",
    "> 3. Averages everything together\n",
    "> 4. Transforms the result through a learnable filter\n",
    ">\n",
    "> **After doing this once**, you know about your direct friends.\n",
    "> **After doing it twice**, you know about friends-of-friends.\n",
    "> **After three times**, you have a good sense of your local \"neighborhood.\"\n",
    ">\n",
    "> **In AI terms:** This is called *message passing*. Each node sends messages to neighbors, receives messages from neighbors, and updates itself. The \"messages\" are learned representations, and the GCN learns what information to extract and share.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Cora dataset\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"CORA DATASET\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Nodes: {data.num_nodes}\")\n",
    "print(f\"Edges: {data.num_edges}\")\n",
    "print(f\"Features per node: {dataset.num_features}\")\n",
    "print(f\"Classes: {dataset.num_classes}\")\n",
    "print(f\"Training nodes: {data.train_mask.sum().item()}\")\n",
    "print(f\"Validation nodes: {data.val_mask.sum().item()}\")\n",
    "print(f\"Test nodes: {data.test_mask.sum().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The Message Passing Framework\n",
    "\n",
    "### 2.1 Understanding Message Passing\n",
    "\n",
    "Every Graph Neural Network follows the **Message Passing** paradigm:\n",
    "\n",
    "```\n",
    "For each node i:\n",
    "    1. MESSAGE:   Compute messages from all neighbors j\n",
    "    2. AGGREGATE: Combine messages (sum, mean, max, etc.)\n",
    "    3. UPDATE:    Update node i's representation\n",
    "```\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$h_i^{(l+1)} = \\text{UPDATE}\\left(h_i^{(l)}, \\text{AGGREGATE}\\left(\\{\\text{MESSAGE}(h_i^{(l)}, h_j^{(l)}, e_{ij}) : j \\in \\mathcal{N}(i)\\}\\right)\\right)$$\n",
    "\n",
    "Where:\n",
    "- $h_i^{(l)}$ = representation of node $i$ at layer $l$\n",
    "- $\\mathcal{N}(i)$ = neighbors of node $i$\n",
    "- $e_{ij}$ = edge features (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize message passing with a simple example\n",
    "\n",
    "def visualize_message_passing():\n",
    "    \"\"\"\n",
    "    Demonstrates message passing on a tiny graph.\n",
    "    \"\"\"\n",
    "    print(\"üîÑ MESSAGE PASSING EXAMPLE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Tiny graph: 4 nodes, simple connections\n",
    "    #   0 -- 1\n",
    "    #   |    |\n",
    "    #   2 -- 3\n",
    "    \n",
    "    # Node features (2D for simplicity)\n",
    "    features = torch.tensor([\n",
    "        [1.0, 0.0],  # Node 0\n",
    "        [0.0, 1.0],  # Node 1\n",
    "        [1.0, 1.0],  # Node 2\n",
    "        [0.5, 0.5],  # Node 3\n",
    "    ])\n",
    "    \n",
    "    # Edges (undirected)\n",
    "    edges = [(0, 1), (1, 0), (0, 2), (2, 0), \n",
    "             (1, 3), (3, 1), (2, 3), (3, 2)]\n",
    "    \n",
    "    print(\"Initial node features:\")\n",
    "    for i, f in enumerate(features):\n",
    "        print(f\"  Node {i}: {f.tolist()}\")\n",
    "    \n",
    "    print(\"\\nüì® Round 1: Each node collects neighbor messages\")\n",
    "    \n",
    "    # For each node, aggregate neighbor features (mean)\n",
    "    neighbors = {\n",
    "        0: [1, 2],\n",
    "        1: [0, 3],\n",
    "        2: [0, 3],\n",
    "        3: [1, 2]\n",
    "    }\n",
    "    \n",
    "    new_features = []\n",
    "    for node in range(4):\n",
    "        # Collect neighbor features\n",
    "        neighbor_feats = torch.stack([features[n] for n in neighbors[node]])\n",
    "        \n",
    "        # Include self (self-loop)\n",
    "        all_feats = torch.cat([features[node].unsqueeze(0), neighbor_feats])\n",
    "        \n",
    "        # Aggregate: mean\n",
    "        aggregated = all_feats.mean(dim=0)\n",
    "        new_features.append(aggregated)\n",
    "        \n",
    "        print(f\"  Node {node}:\")\n",
    "        print(f\"    Neighbors: {neighbors[node]}\")\n",
    "        print(f\"    Neighbor features: {neighbor_feats.tolist()}\")\n",
    "        print(f\"    After aggregation: {aggregated.tolist()}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ After 1 round, each node knows about its direct neighbors!\")\n",
    "    print(\"\\nüí° With 2 rounds, nodes would know about 2-hop neighbors.\")\n",
    "\n",
    "visualize_message_passing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "We simulated one round of message passing:\n",
    "\n",
    "1. Each node collected features from its neighbors\n",
    "2. Combined them with its own feature (self-loop)\n",
    "3. Computed the mean as the new representation\n",
    "\n",
    "**Key insight:** After this process:\n",
    "- Nodes 0 and 2 have similar representations (they share neighbor 0 and 2)\n",
    "- Nodes 1 and 3 have similar representations (they share neighbor 1 and 3)\n",
    "\n",
    "This is how GNNs create **structurally-aware embeddings**!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Implementing GCN from Scratch\n",
    "\n",
    "### 3.1 The GCN Layer Formula\n",
    "\n",
    "The Graph Convolutional Network (Kipf & Welling, 2017) uses this update rule:\n",
    "\n",
    "$$H^{(l+1)} = \\sigma\\left(\\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2} H^{(l)} W^{(l)}\\right)$$\n",
    "\n",
    "Where:\n",
    "- $\\tilde{A} = A + I$ (adjacency matrix with self-loops)\n",
    "- $\\tilde{D}$ = degree matrix of $\\tilde{A}$\n",
    "- $H^{(l)}$ = node features at layer $l$\n",
    "- $W^{(l)}$ = learnable weight matrix\n",
    "- $\\sigma$ = activation function (ReLU)\n",
    "\n",
    "### üßí ELI5: What's with all the D's and A's?\n",
    "\n",
    "> **Imagine you're averaging your friends' opinions:**\n",
    ">\n",
    "> - **Without normalization:** If you have 100 friends and I have 2, your \"average\" is way bigger (summing 100 vs 2 numbers)\n",
    "> - **With normalization:** We divide by the number of friends, so everyone's vote counts equally\n",
    ">\n",
    "> The $D^{-1/2}$ terms do exactly this - they normalize by degree so nodes with many neighbors don't dominate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayerScratch(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Convolutional Layer - implemented from scratch.\n",
    "    \n",
    "    This implements: H' = œÉ(D^(-1/2) √É D^(-1/2) H W)\n",
    "    \n",
    "    Where:\n",
    "    - √É = A + I (adjacency with self-loops)\n",
    "    - D = degree matrix\n",
    "    - H = node features\n",
    "    - W = learnable weights\n",
    "    - œÉ = activation (ReLU, applied externally)\n",
    "    \n",
    "    Args:\n",
    "        in_channels: Number of input features per node\n",
    "        out_channels: Number of output features per node\n",
    "    \n",
    "    Example:\n",
    "        >>> layer = GCNLayerScratch(1433, 64)\n",
    "        >>> x = torch.randn(2708, 1433)\n",
    "        >>> edge_index = ...  # [2, num_edges]\n",
    "        >>> out = layer(x, edge_index)  # [2708, 64]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        # Learnable weight matrix\n",
    "        self.weight = nn.Parameter(torch.Tensor(in_channels, out_channels))\n",
    "        \n",
    "        # Initialize weights using Xavier/Glorot initialization\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Node features [num_nodes, in_channels]\n",
    "            edge_index: Graph edges [2, num_edges]\n",
    "            \n",
    "        Returns:\n",
    "            Updated node features [num_nodes, out_channels]\n",
    "        \"\"\"\n",
    "        num_nodes = x.size(0)\n",
    "        \n",
    "        # Step 1: Add self-loops (A ‚Üí A + I)\n",
    "        edge_index_with_loops, _ = add_self_loops(edge_index, num_nodes=num_nodes)\n",
    "        \n",
    "        # Step 2: Compute degree normalization (D^(-1/2))\n",
    "        row, col = edge_index_with_loops\n",
    "        deg = degree(row, num_nodes=num_nodes, dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        \n",
    "        # Normalization coefficient for each edge\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        \n",
    "        # Step 3: Linear transformation (H @ W)\n",
    "        x = x @ self.weight\n",
    "        \n",
    "        # Step 4: Message passing (aggregate normalized neighbor features)\n",
    "        out = torch.zeros_like(x)\n",
    "        \n",
    "        # This is the \"slow\" way - we'll optimize later\n",
    "        for i, (src, dst) in enumerate(edge_index_with_loops.t()):\n",
    "            out[dst] += norm[i] * x[src]\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'GCNLayerScratch({self.in_channels}, {self.out_channels})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the layer\n",
    "layer = GCNLayerScratch(dataset.num_features, 64).to(device)\n",
    "print(f\"Layer: {layer}\")\n",
    "print(f\"Weight shape: {layer.weight.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "out = layer(data.x, data.edge_index)\n",
    "print(f\"\\nInput shape: {data.x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(\"\\n‚úÖ GCN layer working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Optimized GCN Layer with Scatter Operations\n",
    "\n",
    "The for-loop above is slow. Let's use vectorized scatter operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Optimized Graph Convolutional Layer using scatter operations.\n",
    "    \n",
    "    Same formula as GCNLayerScratch, but much faster!\n",
    "    Uses scatter_add for efficient message aggregation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        # Learnable parameters\n",
    "        self.weight = nn.Parameter(torch.Tensor(in_channels, out_channels))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Initialize weights.\"\"\"\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Optimized forward pass using scatter operations.\n",
    "        \"\"\"\n",
    "        num_nodes = x.size(0)\n",
    "        \n",
    "        # Step 1: Add self-loops\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)\n",
    "        \n",
    "        # Step 2: Compute normalization\n",
    "        row, col = edge_index\n",
    "        deg = degree(row, num_nodes=num_nodes, dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        \n",
    "        # Step 3: Linear transformation\n",
    "        x = x @ self.weight\n",
    "        \n",
    "        # Step 4: Efficient message passing with scatter_add\n",
    "        # For each edge (src ‚Üí dst), add normalized src feature to dst\n",
    "        out = torch.zeros_like(x)\n",
    "        src_features = x[row] * norm.view(-1, 1)  # Normalized source features\n",
    "        out.scatter_add_(0, col.view(-1, 1).expand_as(src_features), src_features)\n",
    "        \n",
    "        # Step 5: Add bias\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'GCNLayer({self.in_channels}, {self.out_channels})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare speed: slow vs optimized\n",
    "layer_slow = GCNLayerScratch(dataset.num_features, 64).to(device)\n",
    "layer_fast = GCNLayer(dataset.num_features, 64).to(device)\n",
    "\n",
    "# Warm-up\n",
    "_ = layer_slow(data.x, data.edge_index)\n",
    "_ = layer_fast(data.x, data.edge_index)\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "\n",
    "# Time slow version\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    _ = layer_slow(data.x, data.edge_index)\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "slow_time = (time.time() - start) / 10\n",
    "\n",
    "# Time fast version\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    _ = layer_fast(data.x, data.edge_index)\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "fast_time = (time.time() - start) / 10\n",
    "\n",
    "print(f\"Slow (loop) version: {slow_time*1000:.2f} ms per forward pass\")\n",
    "print(f\"Fast (scatter) version: {fast_time*1000:.2f} ms per forward pass\")\n",
    "print(f\"\\nSpeedup: {slow_time/fast_time:.1f}x faster! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Building the Complete GCN Model\n",
    "\n",
    "Now let's stack multiple GCN layers to create a full model for node classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-layer Graph Convolutional Network for node classification.\n",
    "    \n",
    "    Architecture:\n",
    "        Input ‚Üí GCN Layer 1 ‚Üí ReLU ‚Üí Dropout ‚Üí GCN Layer 2 ‚Üí Output\n",
    "    \n",
    "    Args:\n",
    "        num_features: Number of input features per node\n",
    "        num_classes: Number of output classes\n",
    "        hidden_dim: Hidden layer dimension (default: 64)\n",
    "        dropout: Dropout probability (default: 0.5)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_features: int, num_classes: int, \n",
    "                 hidden_dim: int = 64, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = GCNLayer(num_features, hidden_dim)\n",
    "        self.conv2 = GCNLayer(hidden_dim, num_classes)\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Node features [num_nodes, num_features]\n",
    "            edge_index: Graph edges [2, num_edges]\n",
    "            \n",
    "        Returns:\n",
    "            Class logits [num_nodes, num_classes]\n",
    "        \"\"\"\n",
    "        # Layer 1: GCN + ReLU + Dropout\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Layer 2: GCN (no activation - raw logits)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_embeddings(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get intermediate node embeddings (after layer 1).\n",
    "        Useful for visualization.\n",
    "        \"\"\"\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = GCN(\n",
    "    num_features=dataset.num_features,\n",
    "    num_classes=dataset.num_classes,\n",
    "    hidden_dim=64,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "print(\"GCN Model Architecture:\")\n",
    "print(\"=\" * 50)\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Training the GCN\n",
    "\n",
    "### 5.1 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \n",
    "    Returns:\n",
    "        Training loss\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data.x, data.edge_index)\n",
    "    \n",
    "    # Compute loss ONLY on training nodes\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data):\n",
    "    \"\"\"\n",
    "    Evaluate on train/val/test sets.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_acc, val_acc, test_acc)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "    \n",
    "    # Compute accuracy for each split\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        correct = pred[mask].eq(data.y[mask]).sum().item()\n",
    "        acc = correct / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    \n",
    "    return tuple(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "model = GCN(\n",
    "    num_features=dataset.num_features,\n",
    "    num_classes=dataset.num_classes,\n",
    "    hidden_dim=64,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': [],\n",
    "    'test_acc': []\n",
    "}\n",
    "\n",
    "print(\"Training GCN on Cora...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_val_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(200):\n",
    "    loss = train(model, data, optimizer)\n",
    "    train_acc, val_acc, test_acc = evaluate(model, data)\n",
    "    \n",
    "    # Save history\n",
    "    history['loss'].append(loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    \n",
    "    # Track best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch\n",
    "        best_test_acc = test_acc\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 20 == 0 or epoch == 199:\n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | \"\n",
    "              f\"Train: {train_acc:.4f} | Val: {val_acc:.4f} | Test: {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"üéâ Best validation accuracy: {best_val_acc:.4f} at epoch {best_epoch}\")\n",
    "print(f\"üìä Test accuracy at best val: {best_test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(history['loss'], color='steelblue', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[1].plot(history['train_acc'], label='Train', linewidth=2)\n",
    "axes[1].plot(history['val_acc'], label='Validation', linewidth=2)\n",
    "axes[1].plot(history['test_acc'], label='Test', linewidth=2)\n",
    "axes[1].axhline(y=0.8, color='red', linestyle='--', label='80% target')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Curves')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if best_test_acc >= 0.8:\n",
    "    print(\"\\nüéâ Congratulations! You achieved >80% test accuracy!\")\n",
    "else:\n",
    "    print(f\"\\nüìà Try tuning hyperparameters. Current: {best_test_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 6: Visualizing Learned Embeddings\n\nLet's see what the GCN learned by visualizing node embeddings with t-SNE:\n\n### Understanding t-SNE for Visualization\n\n**t-SNE (t-Distributed Stochastic Neighbor Embedding)** is a dimensionality reduction technique perfect for visualization:\n\n- Reduces high-dimensional data (64D embeddings) to 2D for plotting\n- Preserves local structure: similar points stay close together\n- From **scikit-learn** (`sklearn`), a popular ML library\n\n**Key Parameters:**\n- `n_components`: Output dimensions (usually 2 for visualization)\n- `perplexity`: Balance between local/global structure (typically 5-50)\n- `random_state`: For reproducibility"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# scikit-learn (sklearn) is the standard Python ML library\n# TSNE is used for visualizing high-dimensional data in 2D\nfrom sklearn.manifold import TSNE\n\n# Get embeddings from trained model\nmodel.eval()\nwith torch.no_grad():\n    embeddings = model.get_embeddings(data.x, data.edge_index)\n    embeddings = embeddings.cpu().numpy()  # TSNE needs NumPy arrays\n    labels = data.y.cpu().numpy()\n\nprint(f\"Embedding shape: {embeddings.shape}\")\nprint(\"Running t-SNE (this may take a minute)...\")\n\n# Create TSNE object and fit-transform embeddings\n# fit_transform(): fits the model and transforms data in one step\ntsne = TSNE(n_components=2, random_state=42, perplexity=30)\nembeddings_2d = tsne.fit_transform(embeddings)\n\nprint(\"t-SNE complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot embeddings colored by class\n",
    "class_names = ['Case-Based', 'Genetic Alg.', 'Neural Nets', 'Probabilistic', \n",
    "               'Reinfortic', 'Rule Learn.', 'Theory']\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "scatter = plt.scatter(\n",
    "    embeddings_2d[:, 0], \n",
    "    embeddings_2d[:, 1], \n",
    "    c=labels, \n",
    "    cmap='tab10',\n",
    "    alpha=0.7,\n",
    "    s=20\n",
    ")\n",
    "\n",
    "# Create legend\n",
    "handles, _ = scatter.legend_elements()\n",
    "plt.legend(handles, class_names, loc='upper right', title='Paper Topic')\n",
    "\n",
    "plt.title('GCN Node Embeddings (t-SNE Visualization)', fontsize=14)\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Notice how papers of the same class cluster together!\")\n",
    "print(\"   The GCN learned to create separable representations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: Original features vs Learned embeddings\n",
    "print(\"Comparing Original Features vs GCN Embeddings...\")\n",
    "\n",
    "# t-SNE on original features\n",
    "original_features = data.x.cpu().numpy()\n",
    "tsne_original = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "original_2d = tsne_original.fit_transform(original_features)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Original features\n",
    "axes[0].scatter(original_2d[:, 0], original_2d[:, 1], \n",
    "                c=labels, cmap='tab10', alpha=0.7, s=20)\n",
    "axes[0].set_title('Original Bag-of-Words Features', fontsize=12)\n",
    "axes[0].set_xlabel('t-SNE Dimension 1')\n",
    "axes[0].set_ylabel('t-SNE Dimension 2')\n",
    "\n",
    "# GCN embeddings\n",
    "axes[1].scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                c=labels, cmap='tab10', alpha=0.7, s=20)\n",
    "axes[1].set_title('GCN Learned Embeddings', fontsize=12)\n",
    "axes[1].set_xlabel('t-SNE Dimension 1')\n",
    "axes[1].set_ylabel('t-SNE Dimension 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç The GCN embeddings show MUCH clearer class separation!\")\n",
    "print(\"   This is because GCN uses both features AND graph structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Comparison with PyG's Optimized Implementation\n",
    "\n",
    "Let's compare our implementation with PyTorch Geometric's optimized version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class PyGGCN(nn.Module):\n",
    "    \"\"\"\n",
    "    GCN using PyTorch Geometric's optimized layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, num_classes, hidden_dim=64, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, num_classes)\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Create PyG model\n",
    "pyg_model = PyGGCN(\n",
    "    num_features=dataset.num_features,\n",
    "    num_classes=dataset.num_classes,\n",
    "    hidden_dim=64\n",
    ").to(device)\n",
    "\n",
    "print(\"PyG GCN Model:\")\n",
    "print(pyg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare speed\n",
    "our_model = GCN(dataset.num_features, dataset.num_classes).to(device)\n",
    "\n",
    "# Warm-up\n",
    "for _ in range(10):\n",
    "    _ = our_model(data.x, data.edge_index)\n",
    "    _ = pyg_model(data.x, data.edge_index)\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "\n",
    "# Time our implementation\n",
    "n_runs = 100\n",
    "start = time.time()\n",
    "for _ in range(n_runs):\n",
    "    _ = our_model(data.x, data.edge_index)\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "our_time = (time.time() - start) / n_runs * 1000\n",
    "\n",
    "# Time PyG implementation\n",
    "start = time.time()\n",
    "for _ in range(n_runs):\n",
    "    _ = pyg_model(data.x, data.edge_index)\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "pyg_time = (time.time() - start) / n_runs * 1000\n",
    "\n",
    "print(\"Speed Comparison:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Our implementation: {our_time:.3f} ms per forward pass\")\n",
    "print(f\"PyG implementation: {pyg_time:.3f} ms per forward pass\")\n",
    "print(f\"\\nPyG is {our_time/pyg_time:.1f}x faster (highly optimized C++ backend)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PyG model and compare accuracy\n",
    "pyg_model = PyGGCN(dataset.num_features, dataset.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(pyg_model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "for epoch in range(200):\n",
    "    pyg_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = pyg_model(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate\n",
    "pyg_model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = pyg_model(data.x, data.edge_index).argmax(dim=1)\n",
    "    pyg_test_acc = (pred[data.test_mask] == data.y[data.test_mask]).float().mean().item()\n",
    "\n",
    "print(f\"\\nAccuracy Comparison:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Our implementation: {best_test_acc:.4f}\")\n",
    "print(f\"PyG implementation: {pyg_test_acc:.4f}\")\n",
    "print(\"\\n‚úÖ Both achieve similar accuracy - our implementation is correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself: Exercise 1\n",
    "\n",
    "**Task:** Experiment with different hidden dimensions.\n",
    "\n",
    "Train GCNs with hidden_dim = [16, 32, 64, 128, 256] and compare:\n",
    "1. Training time\n",
    "2. Final test accuracy\n",
    "3. Number of parameters\n",
    "\n",
    "Which hidden dimension works best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "# Hint: Create a loop over hidden dimensions\n",
    "\n",
    "hidden_dims = [16, 32, 64, 128, 256]\n",
    "results = []\n",
    "\n",
    "for hidden_dim in hidden_dims:\n",
    "    # Create model\n",
    "    # model = GCN(..., hidden_dim=hidden_dim, ...)\n",
    "    \n",
    "    # Train for 200 epochs\n",
    "    # ...\n",
    "    \n",
    "    # Record: hidden_dim, num_params, test_acc, train_time\n",
    "    pass\n",
    "\n",
    "# Plot results\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "```python\n",
    "for hidden_dim in hidden_dims:\n",
    "    model = GCN(dataset.num_features, dataset.num_classes, \n",
    "                hidden_dim=hidden_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(200):\n",
    "        train(model, data, optimizer)\n",
    "    train_time = time.time() - start\n",
    "    \n",
    "    _, _, test_acc = evaluate(model, data)\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    results.append((hidden_dim, n_params, test_acc, train_time))\n",
    "    print(f\"hidden_dim={hidden_dim}: {test_acc:.4f} acc, {n_params} params\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself: Exercise 2\n",
    "\n",
    "**Task:** Try a 3-layer GCN.\n",
    "\n",
    "Deeper GCNs can sometimes perform worse due to \"over-smoothing\" - all nodes become too similar!\n",
    "\n",
    "1. Modify the GCN class to have 3 layers\n",
    "2. Train and compare accuracy\n",
    "3. Do you see over-smoothing?\n",
    "\n",
    "**Hint:** Look at the t-SNE visualization - do classes still separate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "\n",
    "class GCN3Layer(nn.Module):\n",
    "    \"\"\"Three-layer GCN.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, num_classes, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        # TODO: Add conv1, conv2, conv3\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # TODO: Implement forward pass\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Training on all nodes instead of just training mask\n",
    "```python\n",
    "# ‚ùå Wrong: Using ALL labels for loss\n",
    "loss = F.cross_entropy(out, data.y)\n",
    "\n",
    "# ‚úÖ Right: Only use training nodes\n",
    "loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "```\n",
    "**Why:** Test nodes should never influence training - that's data leakage!\n",
    "\n",
    "### Mistake 2: Forgetting to add self-loops\n",
    "```python\n",
    "# ‚ùå Wrong: No self-loops\n",
    "# Each node can't see its own features!\n",
    "\n",
    "# ‚úÖ Right: Add self-loops before message passing\n",
    "edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)\n",
    "```\n",
    "**Why:** Without self-loops, a node only sees neighbors, not itself.\n",
    "\n",
    "### Mistake 3: Wrong normalization\n",
    "```python\n",
    "# ‚ùå Wrong: No normalization (high-degree nodes dominate)\n",
    "out[dst] += x[src]\n",
    "\n",
    "# ‚úÖ Right: Symmetric normalization\n",
    "norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "out[dst] += norm * x[src]\n",
    "```\n",
    "**Why:** High-degree nodes would have much larger representations otherwise.\n",
    "\n",
    "### Mistake 4: Too many layers (over-smoothing)\n",
    "```python\n",
    "# ‚ùå Risky: Many GCN layers\n",
    "# After ~6 layers, all nodes look the same!\n",
    "for i in range(10):\n",
    "    x = gcn_layer(x, edge_index)\n",
    "\n",
    "# ‚úÖ Safe: 2-3 layers for most tasks\n",
    "x = self.conv1(x, edge_index)\n",
    "x = F.relu(x)\n",
    "x = self.conv2(x, edge_index)\n",
    "```\n",
    "**Why:** Each GCN layer averages neighbors - too many layers makes everything similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ The message passing framework (Message ‚Üí Aggregate ‚Üí Update)\n",
    "- ‚úÖ How to implement GCN layers from scratch\n",
    "- ‚úÖ The importance of normalization and self-loops\n",
    "- ‚úÖ How to train GCNs for node classification\n",
    "- ‚úÖ Visualizing learned embeddings with t-SNE\n",
    "- ‚úÖ The over-smoothing problem with deep GCNs\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Advanced Challenge:** Implement residual connections to fight over-smoothing.\n",
    "\n",
    "Add skip connections like in ResNet:\n",
    "```\n",
    "h' = h + GCN(h)  # Instead of h' = GCN(h)\n",
    "```\n",
    "\n",
    "Does this allow deeper GCNs to work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Challenge: ResGCN\n",
    "\n",
    "class ResGCN(nn.Module):\n",
    "    \"\"\"GCN with residual connections.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, num_classes, hidden_dim=64, num_layers=4):\n",
    "        super().__init__()\n",
    "        # Your code here!\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # Your code here!\n",
    "        # Remember: h' = h + GCN(h)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [GCN Paper](https://arxiv.org/abs/1609.02907) - Original 2017 paper by Kipf & Welling\n",
    "- [Understanding Convolutions on Graphs](https://distill.pub/2021/understanding-gnns/) - Interactive visualization\n",
    "- [PyG Documentation](https://pytorch-geometric.readthedocs.io/) - Official docs\n",
    "- [Over-smoothing in GNNs](https://arxiv.org/abs/2006.13318) - Analysis of the problem\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "\n",
    "del model, pyg_model, our_model\n",
    "del embeddings, embeddings_2d, original_2d\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory after cleanup: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚è≠Ô∏è Next Steps\n",
    "\n",
    "GCNs treat all neighbors equally. But shouldn't some neighbors be more important than others?\n",
    "\n",
    "**In Lab E.3: Graph Attention Networks**, you'll:\n",
    "- Learn about attention mechanisms for graphs\n",
    "- Implement GAT layers that learn neighbor importance\n",
    "- Visualize which neighbors get the most attention\n",
    "- Achieve even better accuracy than GCN!\n",
    "\n",
    "Let's add attention to our graphs! üëÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}