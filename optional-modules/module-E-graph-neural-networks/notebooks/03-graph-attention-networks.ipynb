{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab E.3: Graph Attention Networks (GAT)\n",
    "\n",
    "**Module:** E - Graph Neural Networks  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê (Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand the limitations of GCN's equal neighbor weighting\n",
    "- [ ] Implement the Graph Attention mechanism from scratch\n",
    "- [ ] Build a multi-head Graph Attention Network\n",
    "- [ ] Train GAT on Cora and compare to GCN\n",
    "- [ ] Visualize attention weights to understand what the model learns\n",
    "- [ ] Analyze which edges get high attention scores\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Lab E.2 (GCN from Scratch)\n",
    "- Knowledge of: Attention mechanisms (helpful but not required)\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**Why do we need attention on graphs?**\n",
    "\n",
    "In GCN, all neighbors contribute equally. But in reality:\n",
    "- In social networks, your best friend's opinion matters more than an acquaintance's\n",
    "- In citation networks, a highly relevant paper is more important than a tangential one\n",
    "- In molecules, certain atom connections are more important for chemical properties\n",
    "\n",
    "**Graph Attention Networks learn to weight neighbors differently** based on their relevance to the task!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßí ELI5: What Is Graph Attention?\n",
    "\n",
    "> **Imagine you're at a party asking for movie recommendations:**\n",
    ">\n",
    "> With GCN (equal weighting):\n",
    "> - You ask 10 people and give everyone's opinion equal weight\n",
    "> - Your movie-buff friend counts the same as your tone-deaf uncle\n",
    ">\n",
    "> With GAT (attention weighting):\n",
    "> - You **learn** who gives good recommendations\n",
    "> - Your movie-buff friend gets 50% weight, uncle gets 2%\n",
    "> - The weights are learned from experience!\n",
    ">\n",
    "> **The magic:** GAT doesn't just average neighbors - it learns which neighbors are important for each specific task.\n",
    ">\n",
    "> **In AI terms:** For each pair of connected nodes, GAT computes an \"attention score\" that represents how important node j is to node i. These scores are learned during training!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import add_self_loops, softmax\n",
    "import time\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load Cora\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "print(f\"\\nCora: {data.num_nodes} nodes, {data.num_edges} edges\")\n",
    "print(f\"Features: {dataset.num_features}, Classes: {dataset.num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding the Attention Mechanism\n",
    "\n",
    "### 2.1 The GAT Formula\n",
    "\n",
    "For each pair of connected nodes $(i, j)$, GAT computes:\n",
    "\n",
    "**Step 1: Compute raw attention scores**\n",
    "$$e_{ij} = \\text{LeakyReLU}(\\mathbf{a}^T [\\mathbf{W}h_i \\| \\mathbf{W}h_j])$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{W}$ = learnable weight matrix\n",
    "- $\\mathbf{a}$ = learnable attention vector\n",
    "- $\\|$ = concatenation\n",
    "- LeakyReLU = activation (allows negative values)\n",
    "\n",
    "**Step 2: Normalize with softmax over neighbors**\n",
    "$$\\alpha_{ij} = \\text{softmax}_j(e_{ij}) = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}(i)} \\exp(e_{ik})}$$\n",
    "\n",
    "**Step 3: Weighted aggregation**\n",
    "$$h'_i = \\sigma\\left(\\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij} \\mathbf{W}h_j\\right)$$\n",
    "\n",
    "### üßí ELI5: Breaking Down Attention\n",
    "\n",
    "> Think of it as a **compatibility score**:\n",
    ">\n",
    "> 1. Transform features: \"Convert everyone's profile to a common format\"\n",
    "> 2. Compute compatibility: \"How well do these two profiles match?\"\n",
    "> 3. Normalize: \"Turn scores into percentages (must sum to 100%)\"\n",
    "> 4. Aggregate: \"Weight everyone's input by their importance percentage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention mechanism with a simple example\n",
    "\n",
    "def visualize_attention_concept():\n",
    "    \"\"\"\n",
    "    Demonstrate attention computation on a tiny graph.\n",
    "    \"\"\"\n",
    "    print(\"üîç ATTENTION MECHANISM EXAMPLE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Simple graph: Node 0 connected to nodes 1, 2, 3\n",
    "    # Node 0 features\n",
    "    h0 = torch.tensor([1.0, 0.5])\n",
    "    \n",
    "    # Neighbor features\n",
    "    h1 = torch.tensor([0.8, 0.6])  # Similar to h0\n",
    "    h2 = torch.tensor([0.2, 0.1])  # Different from h0\n",
    "    h3 = torch.tensor([0.9, 0.4])  # Very similar to h0\n",
    "    \n",
    "    print(\"Node 0 features:\", h0.tolist())\n",
    "    print(\"Neighbor 1 features:\", h1.tolist(), \"(similar)\")\n",
    "    print(\"Neighbor 2 features:\", h2.tolist(), \"(different)\")\n",
    "    print(\"Neighbor 3 features:\", h3.tolist(), \"(very similar)\")\n",
    "    \n",
    "    # Simple attention: dot product similarity\n",
    "    print(\"\\nüìä Computing attention scores (dot product similarity):\")\n",
    "    \n",
    "    e01 = torch.dot(h0, h1)\n",
    "    e02 = torch.dot(h0, h2)\n",
    "    e03 = torch.dot(h0, h3)\n",
    "    \n",
    "    print(f\"  e(0,1) = {e01.item():.3f}\")\n",
    "    print(f\"  e(0,2) = {e02.item():.3f}\")\n",
    "    print(f\"  e(0,3) = {e03.item():.3f}\")\n",
    "    \n",
    "    # Softmax normalization\n",
    "    scores = torch.tensor([e01, e02, e03])\n",
    "    attention = F.softmax(scores, dim=0)\n",
    "    \n",
    "    print(\"\\nüìä Normalized attention weights (softmax):\")\n",
    "    print(f\"  Œ±(0,1) = {attention[0].item():.3f} ({attention[0].item()*100:.1f}%)\")\n",
    "    print(f\"  Œ±(0,2) = {attention[1].item():.3f} ({attention[1].item()*100:.1f}%)\")\n",
    "    print(f\"  Œ±(0,3) = {attention[2].item():.3f} ({attention[2].item()*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nüí° Notice: Similar neighbors get MORE attention!\")\n",
    "    print(\"   Node 3 (very similar) gets ~\" + f\"{attention[2].item()*100:.0f}%\")\n",
    "    print(\"   Node 2 (different) gets only ~\" + f\"{attention[1].item()*100:.0f}%\")\n",
    "\n",
    "visualize_attention_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Implementing GAT from Scratch\n",
    "\n",
    "### 3.1 Single-Head Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayerScratch(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Attention Layer - Single Head Implementation.\n",
    "    \n",
    "    Implements:\n",
    "        e_ij = LeakyReLU(a^T [Wh_i || Wh_j])\n",
    "        Œ±_ij = softmax_j(e_ij)\n",
    "        h'_i = Œ£_j Œ±_ij * Wh_j\n",
    "    \n",
    "    Args:\n",
    "        in_channels: Input feature dimension\n",
    "        out_channels: Output feature dimension\n",
    "        dropout: Dropout probability for attention coefficients\n",
    "        negative_slope: LeakyReLU negative slope\n",
    "    \n",
    "    Example:\n",
    "        >>> layer = GATLayerScratch(1433, 64)\n",
    "        >>> out, attention = layer(x, edge_index, return_attention=True)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, \n",
    "                 dropout: float = 0.6, negative_slope: float = 0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.dropout = dropout\n",
    "        self.negative_slope = negative_slope\n",
    "        \n",
    "        # Linear transformation W: [in_channels] -> [out_channels]\n",
    "        self.W = nn.Linear(in_channels, out_channels, bias=False)\n",
    "        \n",
    "        # Attention mechanism: a ‚àà R^(2*out_channels)\n",
    "        # We split into a_left and a_right for efficiency\n",
    "        self.a_left = nn.Parameter(torch.Tensor(out_channels, 1))\n",
    "        self.a_right = nn.Parameter(torch.Tensor(out_channels, 1))\n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Initialize parameters with Xavier/Glorot.\"\"\"\n",
    "        nn.init.xavier_uniform_(self.W.weight)\n",
    "        nn.init.xavier_uniform_(self.a_left)\n",
    "        nn.init.xavier_uniform_(self.a_right)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, \n",
    "                return_attention: bool = False):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Node features [num_nodes, in_channels]\n",
    "            edge_index: Graph edges [2, num_edges]\n",
    "            return_attention: If True, also return attention weights\n",
    "            \n",
    "        Returns:\n",
    "            Updated node features [num_nodes, out_channels]\n",
    "            (Optional) Attention weights [num_edges]\n",
    "        \"\"\"\n",
    "        num_nodes = x.size(0)\n",
    "        \n",
    "        # Step 1: Add self-loops\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)\n",
    "        \n",
    "        # Step 2: Linear transformation: Wh for all nodes\n",
    "        Wh = self.W(x)  # [num_nodes, out_channels]\n",
    "        \n",
    "        # Step 3: Compute attention scores for each edge\n",
    "        src, dst = edge_index  # Source and destination nodes\n",
    "        \n",
    "        # e_ij = LeakyReLU(a_left^T * Wh_i + a_right^T * Wh_j)\n",
    "        # This is equivalent to a^T [Wh_i || Wh_j] but more efficient\n",
    "        e_left = (Wh @ self.a_left).squeeze(-1)   # [num_nodes]\n",
    "        e_right = (Wh @ self.a_right).squeeze(-1)  # [num_nodes]\n",
    "        \n",
    "        # Attention scores for each edge\n",
    "        e = e_left[src] + e_right[dst]  # [num_edges]\n",
    "        e = self.leaky_relu(e)\n",
    "        \n",
    "        # Step 4: Softmax over neighbors (for each destination node)\n",
    "        # PyG's softmax groups by destination node\n",
    "        alpha = softmax(e, dst, num_nodes=num_nodes)\n",
    "        \n",
    "        # Apply dropout to attention weights\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Step 5: Weighted aggregation\n",
    "        out = torch.zeros_like(Wh)\n",
    "        src_features = Wh[src] * alpha.view(-1, 1)  # Weighted source features\n",
    "        out.scatter_add_(0, dst.view(-1, 1).expand_as(src_features), src_features)\n",
    "        \n",
    "        if return_attention:\n",
    "            return out, (edge_index, alpha)\n",
    "        return out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'GATLayerScratch({self.in_channels}, {self.out_channels})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the layer\n",
    "layer = GATLayerScratch(dataset.num_features, 64).to(device)\n",
    "print(f\"Layer: {layer}\")\n",
    "\n",
    "# Forward pass with attention\n",
    "out, (edge_idx, attn) = layer(data.x, data.edge_index, return_attention=True)\n",
    "\n",
    "print(f\"\\nInput shape: {data.x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Attention weights shape: {attn.shape}\")\n",
    "print(f\"Attention weights sum per node: {attn.sum().item():.1f} (should be ~{data.num_nodes})\")\n",
    "print(\"\\n‚úÖ GAT layer working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Multi-Head Attention\n",
    "\n",
    "Just like in Transformers, using multiple attention \"heads\" helps the model capture different types of relationships.\n",
    "\n",
    "**Multi-head attention:**\n",
    "$$h'_i = \\Big\\|_{k=1}^{K} \\sigma\\left(\\sum_{j \\in \\mathcal{N}(i)} \\alpha^k_{ij} \\mathbf{W}^k h_j\\right)$$\n",
    "\n",
    "Where $\\|$ means concatenation of $K$ heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGATLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Graph Attention Layer.\n",
    "    \n",
    "    Runs multiple attention heads in parallel, then concatenates\n",
    "    (or averages) the results.\n",
    "    \n",
    "    Args:\n",
    "        in_channels: Input feature dimension\n",
    "        out_channels: Output dimension PER HEAD\n",
    "        heads: Number of attention heads (default: 8)\n",
    "        concat: If True, concatenate heads. If False, average them.\n",
    "        dropout: Dropout probability\n",
    "    \n",
    "    Output dimension:\n",
    "        - If concat=True: heads * out_channels\n",
    "        - If concat=False: out_channels\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, \n",
    "                 heads: int = 8, concat: bool = True, dropout: float = 0.6):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Create multiple attention heads\n",
    "        self.attention_heads = nn.ModuleList([\n",
    "            GATLayerScratch(in_channels, out_channels, dropout=dropout)\n",
    "            for _ in range(heads)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor,\n",
    "                return_attention: bool = False):\n",
    "        \"\"\"\n",
    "        Forward pass with multiple heads.\n",
    "        \"\"\"\n",
    "        # Run all heads in parallel\n",
    "        if return_attention:\n",
    "            head_outputs = []\n",
    "            head_attentions = []\n",
    "            for head in self.attention_heads:\n",
    "                out, attn = head(x, edge_index, return_attention=True)\n",
    "                head_outputs.append(out)\n",
    "                head_attentions.append(attn)\n",
    "        else:\n",
    "            head_outputs = [head(x, edge_index) for head in self.attention_heads]\n",
    "        \n",
    "        # Combine heads\n",
    "        if self.concat:\n",
    "            # Concatenate: [num_nodes, heads * out_channels]\n",
    "            out = torch.cat(head_outputs, dim=-1)\n",
    "        else:\n",
    "            # Average: [num_nodes, out_channels]\n",
    "            out = torch.stack(head_outputs, dim=0).mean(dim=0)\n",
    "        \n",
    "        if return_attention:\n",
    "            return out, head_attentions\n",
    "        return out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'MultiHeadGATLayer({self.in_channels}, {self.out_channels}, heads={self.heads})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-head attention\n",
    "mh_layer = MultiHeadGATLayer(dataset.num_features, 8, heads=8, concat=True).to(device)\n",
    "print(f\"Layer: {mh_layer}\")\n",
    "\n",
    "out = mh_layer(data.x, data.edge_index)\n",
    "print(f\"\\nInput: {data.x.shape}\")\n",
    "print(f\"Output: {out.shape} (8 heads √ó 8 dims = 64)\")\n",
    "print(\"\\n‚úÖ Multi-head GAT working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 4: Building the Complete GAT Model\n\n### Activation Functions: ELU vs ReLU\n\nThe GAT paper uses **ELU** (Exponential Linear Unit) instead of ReLU:\n\n| Activation | Formula | Advantage |\n|------------|---------|-----------|\n| ReLU | max(0, x) | Simple, fast |\n| ELU | x if x > 0, else Œ±(e^x - 1) | Smoother, can output negatives |\n\n**Why ELU for GAT?** ELU allows negative outputs, which can be important when attention weights vary significantly. The original GAT paper found ELU worked better than ReLU.\n\n```python\n# In PyTorch:\nx = F.relu(x)   # Standard ReLU\nx = F.elu(x)    # ELU (default Œ±=1.0)\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-layer Graph Attention Network for node classification.\n",
    "    \n",
    "    Architecture:\n",
    "        Input ‚Üí Multi-Head GAT (8 heads, concat) ‚Üí ELU ‚Üí Dropout \n",
    "              ‚Üí GAT (1 head, no concat) ‚Üí Output\n",
    "    \n",
    "    Args:\n",
    "        num_features: Input feature dimension\n",
    "        num_classes: Number of output classes\n",
    "        hidden_dim: Hidden dimension per head (default: 8)\n",
    "        heads: Number of attention heads (default: 8)\n",
    "        dropout: Dropout probability (default: 0.6)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_features: int, num_classes: int,\n",
    "                 hidden_dim: int = 8, heads: int = 8, dropout: float = 0.6):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Layer 1: Multi-head attention with concatenation\n",
    "        self.gat1 = MultiHeadGATLayer(\n",
    "            in_channels=num_features,\n",
    "            out_channels=hidden_dim,\n",
    "            heads=heads,\n",
    "            concat=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Layer 2: Single-head attention (for classification)\n",
    "        self.gat2 = MultiHeadGATLayer(\n",
    "            in_channels=hidden_dim * heads,  # Output of layer 1\n",
    "            out_channels=num_classes,\n",
    "            heads=1,\n",
    "            concat=False,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor,\n",
    "                return_attention: bool = False):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \"\"\"\n",
    "        # Input dropout\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Layer 1 + ELU activation\n",
    "        if return_attention:\n",
    "            x, attn1 = self.gat1(x, edge_index, return_attention=True)\n",
    "        else:\n",
    "            x = self.gat1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Layer 2 (no activation - raw logits)\n",
    "        if return_attention:\n",
    "            x, attn2 = self.gat2(x, edge_index, return_attention=True)\n",
    "            return x, (attn1, attn2)\n",
    "        \n",
    "        x = self.gat2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "    def get_embeddings(self, x: torch.Tensor, edge_index: torch.Tensor):\n",
    "        \"\"\"Get intermediate embeddings (after layer 1).\"\"\"\n",
    "        x = self.gat1(x, edge_index)\n",
    "        return F.elu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = GAT(\n",
    "    num_features=dataset.num_features,\n",
    "    num_classes=dataset.num_classes,\n",
    "    hidden_dim=8,\n",
    "    heads=8,\n",
    "    dropout=0.6\n",
    ").to(device)\n",
    "\n",
    "print(\"GAT Model Architecture:\")\n",
    "print(\"=\" * 50)\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Training GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data):\n",
    "    \"\"\"Evaluate on train/val/test.\"\"\"\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "    \n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        correct = pred[mask].eq(data.y[mask]).sum().item()\n",
    "        accs.append(correct / mask.sum().item())\n",
    "    return tuple(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "model = GAT(\n",
    "    num_features=dataset.num_features,\n",
    "    num_classes=dataset.num_classes,\n",
    "    hidden_dim=8,\n",
    "    heads=8,\n",
    "    dropout=0.6\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "# Training history\n",
    "history = {'loss': [], 'train_acc': [], 'val_acc': [], 'test_acc': []}\n",
    "\n",
    "print(\"Training GAT on Cora...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_val_acc = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(300):\n",
    "    loss = train(model, data, optimizer)\n",
    "    train_acc, val_acc, test_acc = evaluate(model, data)\n",
    "    \n",
    "    history['loss'].append(loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_test_acc = test_acc\n",
    "        best_epoch = epoch\n",
    "    \n",
    "    if epoch % 50 == 0 or epoch == 299:\n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | \"\n",
    "              f\"Train: {train_acc:.4f} | Val: {val_acc:.4f} | Test: {test_acc:.4f}\")\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"üéâ Best validation accuracy: {best_val_acc:.4f} at epoch {best_epoch}\")\n",
    "print(f\"üìä Test accuracy at best val: {best_test_acc:.4f}\")\n",
    "print(f\"‚è±Ô∏è Training time: {train_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['loss'], color='steelblue', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train', linewidth=2)\n",
    "axes[1].plot(history['val_acc'], label='Validation', linewidth=2)\n",
    "axes[1].plot(history['test_acc'], label='Test', linewidth=2)\n",
    "axes[1].axhline(y=0.81, color='red', linestyle='--', label='81% target')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Curves')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Visualizing Attention Weights\n",
    "\n",
    "This is the exciting part - let's see which neighbors the model pays attention to!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attention weights from trained model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _, (layer1_attn, layer2_attn) = model(data.x, data.edge_index, return_attention=True)\n",
    "\n",
    "# Layer 1 has 8 heads, let's look at the first head\n",
    "edge_index_with_loops, alpha = layer1_attn[0]  # First head\n",
    "edge_index_np = edge_index_with_loops.cpu().numpy()\n",
    "alpha_np = alpha.cpu().numpy()\n",
    "\n",
    "print(f\"Edge index shape: {edge_index_np.shape}\")\n",
    "print(f\"Attention weights shape: {alpha_np.shape}\")\n",
    "print(f\"\\nAttention statistics:\")\n",
    "print(f\"  Min: {alpha_np.min():.4f}\")\n",
    "print(f\"  Max: {alpha_np.max():.4f}\")\n",
    "print(f\"  Mean: {alpha_np.mean():.4f}\")\n",
    "print(f\"  Std: {alpha_np.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention for a specific node\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "def visualize_node_attention(node_id, data, edge_index, alpha, top_k=10):\n",
    "    \"\"\"\n",
    "    Visualize attention weights for a specific node.\n",
    "    \n",
    "    Args:\n",
    "        node_id: The node to analyze\n",
    "        data: PyG data object\n",
    "        edge_index: Edge index tensor [2, num_edges]\n",
    "        alpha: Attention weights [num_edges]\n",
    "        top_k: Number of top neighbors to highlight\n",
    "    \"\"\"\n",
    "    # Find edges where node_id is the destination (receiving attention)\n",
    "    edge_index_np = edge_index.cpu().numpy() if torch.is_tensor(edge_index) else edge_index\n",
    "    alpha_np = alpha.cpu().numpy() if torch.is_tensor(alpha) else alpha\n",
    "    \n",
    "    mask = edge_index_np[1] == node_id\n",
    "    neighbors = edge_index_np[0][mask]\n",
    "    neighbor_attentions = alpha_np[mask]\n",
    "    \n",
    "    # Sort by attention\n",
    "    sorted_idx = np.argsort(neighbor_attentions)[::-1]\n",
    "    \n",
    "    print(f\"\\nüîç Attention Analysis for Node {node_id}\")\n",
    "    print(f\"   Label: {data.y[node_id].item()}\")\n",
    "    print(f\"   Number of neighbors: {len(neighbors)}\")\n",
    "    print(\"\\n   Top-5 Attended Neighbors:\")\n",
    "    print(\"   \" + \"-\" * 40)\n",
    "    \n",
    "    labels = data.y.cpu().numpy()\n",
    "    node_label = labels[node_id]\n",
    "    \n",
    "    for i, idx in enumerate(sorted_idx[:5]):\n",
    "        neighbor = neighbors[idx]\n",
    "        attn = neighbor_attentions[idx]\n",
    "        neighbor_label = labels[neighbor]\n",
    "        same_class = \"‚úì\" if neighbor_label == node_label else \"‚úó\"\n",
    "        print(f\"   {i+1}. Node {neighbor}: Œ±={attn:.4f} (class {neighbor_label}) {same_class}\")\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Create subgraph with node and its neighbors\n",
    "    all_nodes = [node_id] + list(neighbors)\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(all_nodes)\n",
    "    \n",
    "    for i, (neighbor, attn) in enumerate(zip(neighbors, neighbor_attentions)):\n",
    "        G.add_edge(neighbor, node_id, weight=attn)\n",
    "    \n",
    "    pos = nx.spring_layout(G, seed=42, k=2)\n",
    "    \n",
    "    # Draw nodes\n",
    "    node_colors = [labels[n] for n in all_nodes]\n",
    "    node_sizes = [1000 if n == node_id else 500 for n in all_nodes]\n",
    "    \n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=all_nodes, \n",
    "                          node_color=node_colors, cmap=plt.cm.Set3,\n",
    "                          node_size=node_sizes, alpha=0.8)\n",
    "    \n",
    "    # Draw edges with width proportional to attention\n",
    "    edges = G.edges()\n",
    "    weights = [G[u][v]['weight'] * 5 for u, v in edges]  # Scale for visibility\n",
    "    \n",
    "    nx.draw_networkx_edges(G, pos, edgelist=edges, width=weights,\n",
    "                          alpha=0.7, edge_color='gray',\n",
    "                          arrows=True, arrowsize=15)\n",
    "    \n",
    "    # Labels\n",
    "    nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "    \n",
    "    ax.set_title(f\"Attention to Node {node_id}\\n(Edge width = attention weight)\")\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return neighbors, neighbor_attentions\n",
    "\n",
    "# Analyze a few nodes\n",
    "visualize_node_attention(0, data, edge_index_with_loops, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze: Do nodes pay more attention to same-class neighbors?\n",
    "\n",
    "labels = data.y.cpu().numpy()\n",
    "edge_index_np = edge_index_with_loops.cpu().numpy()\n",
    "\n",
    "# For each edge, check if source and destination have same label\n",
    "src_labels = labels[edge_index_np[0]]\n",
    "dst_labels = labels[edge_index_np[1]]\n",
    "same_class = src_labels == dst_labels\n",
    "\n",
    "# Compare attention weights for same-class vs different-class edges\n",
    "same_class_attn = alpha_np[same_class].mean()\n",
    "diff_class_attn = alpha_np[~same_class].mean()\n",
    "\n",
    "print(\"\\nüìä ATTENTION PATTERN ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Mean attention to SAME-class neighbors: {same_class_attn:.4f}\")\n",
    "print(f\"Mean attention to DIFFERENT-class neighbors: {diff_class_attn:.4f}\")\n",
    "print(f\"\\nRatio: {same_class_attn/diff_class_attn:.2f}x more attention to same class!\")\n",
    "\n",
    "# Plot distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(alpha_np[same_class], bins=50, alpha=0.7, label='Same class', density=True)\n",
    "axes[0].hist(alpha_np[~same_class], bins=50, alpha=0.7, label='Different class', density=True)\n",
    "axes[0].set_xlabel('Attention Weight')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].set_title('Distribution of Attention Weights')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot([alpha_np[same_class], alpha_np[~same_class]], \n",
    "                labels=['Same Class', 'Different Class'])\n",
    "axes[1].set_ylabel('Attention Weight')\n",
    "axes[1].set_title('Attention by Class Relationship')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° The model learned to pay MORE attention to same-class neighbors!\")\n",
    "print(\"   This helps it propagate useful information for classification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Comparison with GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "\n",
    "# Train GCN for comparison\n",
    "class PyGGCN(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class PyGGAT(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_dim=8, heads=8):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(num_features, hidden_dim, heads=heads, dropout=0.6)\n",
    "        self.conv2 = GATConv(hidden_dim * heads, num_classes, heads=1, \n",
    "                            concat=False, dropout=0.6)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "def train_model(Model, num_epochs=200, **kwargs):\n",
    "    \"\"\"Train a model and return best test accuracy.\"\"\"\n",
    "    model = Model(\n",
    "        num_features=dataset.num_features,\n",
    "        num_classes=dataset.num_classes,\n",
    "        **kwargs\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "    \n",
    "    best_val = 0\n",
    "    best_test = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(data.x, data.edge_index).argmax(dim=1)\n",
    "            val_acc = (pred[data.val_mask] == data.y[data.val_mask]).float().mean().item()\n",
    "            test_acc = (pred[data.test_mask] == data.y[data.test_mask]).float().mean().item()\n",
    "            \n",
    "            if val_acc > best_val:\n",
    "                best_val = val_acc\n",
    "                best_test = test_acc\n",
    "    \n",
    "    return best_test, sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Compare models\n",
    "print(\"Model Comparison on Cora\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "gcn_acc, gcn_params = train_model(PyGGCN, hidden_dim=64)\n",
    "print(f\"GCN:  {gcn_acc:.4f} accuracy, {gcn_params:,} params\")\n",
    "\n",
    "gat_acc, gat_params = train_model(PyGGAT, hidden_dim=8, heads=8)\n",
    "print(f\"GAT:  {gat_acc:.4f} accuracy, {gat_params:,} params\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "if gat_acc > gcn_acc:\n",
    "    print(f\"üéâ GAT outperforms GCN by {(gat_acc - gcn_acc)*100:.1f}%!\")\n",
    "else:\n",
    "    print(f\"üìä GCN outperforms GAT by {(gcn_acc - gat_acc)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nüí° GAT uses {gat_params/gcn_params:.1f}x {'more' if gat_params > gcn_params else 'fewer'} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself: Exercise 1\n",
    "\n",
    "**Task:** Experiment with different numbers of attention heads.\n",
    "\n",
    "Train GAT models with heads = [1, 2, 4, 8, 16] and compare:\n",
    "1. Test accuracy\n",
    "2. Number of parameters\n",
    "3. Training time\n",
    "\n",
    "Is more heads always better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "\n",
    "head_counts = [1, 2, 4, 8, 16]\n",
    "results = []\n",
    "\n",
    "for heads in head_counts:\n",
    "    # Train GAT with this number of heads\n",
    "    # Record accuracy, params, time\n",
    "    pass\n",
    "\n",
    "# Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "```python\n",
    "for heads in head_counts:\n",
    "    start = time.time()\n",
    "    acc, params = train_model(PyGGAT, num_epochs=200, hidden_dim=8, heads=heads)\n",
    "    train_time = time.time() - start\n",
    "    results.append((heads, acc, params, train_time))\n",
    "    print(f\"Heads={heads}: {acc:.4f} acc, {params} params, {train_time:.1f}s\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself: Exercise 2\n",
    "\n",
    "**Task:** Find the most \"important\" edges in the graph based on attention.\n",
    "\n",
    "1. Compute attention weights for all edges across all heads\n",
    "2. Find the top-10 edges with highest attention\n",
    "3. Are these edges within the same class or between classes?\n",
    "4. What does this tell us about the model's strategy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "\n",
    "# Get attention from all 8 heads in layer 1\n",
    "# Average attention across heads\n",
    "# Find top-10 edges\n",
    "# Analyze same-class vs different-class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Softmax over wrong dimension\n",
    "```python\n",
    "# ‚ùå Wrong: Softmax over all edges\n",
    "alpha = F.softmax(e, dim=0)  # All attention sums to 1\n",
    "\n",
    "# ‚úÖ Right: Softmax over neighbors of each node\n",
    "from torch_geometric.utils import softmax\n",
    "alpha = softmax(e, dst, num_nodes=num_nodes)  # Per-node normalization\n",
    "```\n",
    "**Why:** Each node's attention weights should sum to 1 independently.\n",
    "\n",
    "### Mistake 2: Not using dropout on attention\n",
    "```python\n",
    "# ‚ùå Missing: No dropout on attention\n",
    "out = (alpha.view(-1, 1) * src_features).scatter_add(...)\n",
    "\n",
    "# ‚úÖ Right: Apply dropout to attention weights\n",
    "alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "out = (alpha.view(-1, 1) * src_features).scatter_add(...)\n",
    "```\n",
    "**Why:** Attention dropout prevents overfitting to specific neighbors.\n",
    "\n",
    "### Mistake 3: Wrong concatenation in multi-head\n",
    "```python\n",
    "# ‚ùå Wrong: Concatenating along wrong dimension\n",
    "out = torch.cat(head_outputs, dim=0)  # Stacks nodes!\n",
    "\n",
    "# ‚úÖ Right: Concatenate along feature dimension\n",
    "out = torch.cat(head_outputs, dim=-1)  # [num_nodes, heads * dim]\n",
    "```\n",
    "**Why:** Multi-head concatenation should combine features, not duplicate nodes.\n",
    "\n",
    "### Mistake 4: Using LeakyReLU with wrong negative slope\n",
    "```python\n",
    "# ‚ùå Unusual: Standard ReLU (all negatives become 0)\n",
    "e = F.relu(e_left + e_right)\n",
    "\n",
    "# ‚úÖ Standard: LeakyReLU with negative_slope=0.2\n",
    "e = F.leaky_relu(e_left + e_right, negative_slope=0.2)\n",
    "```\n",
    "**Why:** LeakyReLU allows negative attention scores, which get very small (not zero) after softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ Why attention is better than equal weighting (learn neighbor importance)\n",
    "- ‚úÖ The GAT attention formula (e_ij, softmax, weighted aggregation)\n",
    "- ‚úÖ Multi-head attention (capture different relationship types)\n",
    "- ‚úÖ How to visualize and interpret attention weights\n",
    "- ‚úÖ GAT pays more attention to same-class neighbors!\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Advanced Challenge:** Implement GATv2.\n",
    "\n",
    "The original GAT computes attention as:\n",
    "```\n",
    "e_ij = LeakyReLU(a_left * Wh_i + a_right * Wh_j)\n",
    "```\n",
    "\n",
    "GATv2 (Brody et al., 2021) uses:\n",
    "```\n",
    "e_ij = a^T * LeakyReLU(W * [h_i || h_j])\n",
    "```\n",
    "\n",
    "The LeakyReLU is applied AFTER concatenation, giving more expressive attention.\n",
    "\n",
    "Implement GATv2 and compare to GAT on Cora!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Challenge: GATv2\n",
    "\n",
    "class GATv2Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    GATv2: Improved attention mechanism.\n",
    "    \n",
    "    Key difference: LeakyReLU is applied AFTER concatenation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # Your code here!\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # Your code here!\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [GAT Paper](https://arxiv.org/abs/1710.10903) - Original 2018 paper\n",
    "- [GATv2 Paper](https://arxiv.org/abs/2105.14491) - Improved attention (2021)\n",
    "- [Attention in Graphs Survey](https://arxiv.org/abs/2202.13060) - Comprehensive review\n",
    "- [PyG GATConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GATConv)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "\n",
    "del model, layer, mh_layer\n",
    "del layer1_attn, layer2_attn, alpha, edge_index_with_loops\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory after cleanup: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚è≠Ô∏è Next Steps\n",
    "\n",
    "So far we've classified **nodes**. But what about classifying entire **graphs**?\n",
    "\n",
    "**In Lab E.4: Graph Classification**, you'll:\n",
    "- Learn about graph-level pooling operations\n",
    "- Build classifiers for molecules and social networks\n",
    "- Implement mean, max, and attention-based pooling\n",
    "- Predict molecular properties on the MUTAG dataset!\n",
    "\n",
    "Let's classify some molecules! üß™"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}