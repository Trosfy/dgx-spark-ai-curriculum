{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab E.1: PyTorch Geometric Setup\n",
    "\n",
    "**Module:** E - Graph Neural Networks  \n",
    "**Time:** 1 hour  \n",
    "**Difficulty:** â­â­ (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand why graphs are a powerful data structure for AI\n",
    "- [ ] Install and configure PyTorch Geometric on DGX Spark\n",
    "- [ ] Load and explore the Cora citation network dataset\n",
    "- [ ] Visualize graph structure using NetworkX\n",
    "- [ ] Understand PyG Data objects and their components\n",
    "- [ ] Create train/validation/test splits for graph data\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Prerequisites\n",
    "\n",
    "- Completed: Module 1.5 (Neural Networks Fundamentals)\n",
    "- Completed: Module 2.1 (PyTorch Essentials)\n",
    "- Knowledge of: Basic Python, NumPy, PyTorch tensors\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ Real-World Context\n",
    "\n",
    "**Why do we need Graph Neural Networks?**\n",
    "\n",
    "Not all data fits into neat grids (like images) or sequences (like text). Consider:\n",
    "\n",
    "| Domain | Nodes | Edges | Task |\n",
    "|--------|-------|-------|------|\n",
    "| Social Networks | People | Friendships | Recommend new friends |\n",
    "| Drug Discovery | Atoms | Chemical bonds | Predict molecule properties |\n",
    "| Citation Networks | Papers | Citations | Classify paper topics |\n",
    "| Fraud Detection | Accounts | Transactions | Identify suspicious patterns |\n",
    "| Knowledge Graphs | Entities | Relationships | Answer complex questions |\n",
    "\n",
    "**Industry Impact:**\n",
    "- **Pinterest** uses GNNs for visual recommendations (PinSage)\n",
    "- **Google Maps** uses GNNs for ETA prediction\n",
    "- **Drug companies** use GNNs to discover new medicines (AlphaFold)\n",
    "- **Financial institutions** use GNNs to detect fraud rings\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§’ ELI5: What Are Graphs?\n",
    "\n",
    "> **Imagine you're at a birthday party** with all your friends. Now imagine drawing a picture where:\n",
    "> - Each person is a **dot** (we call these *nodes*)\n",
    "> - Each friendship is a **line connecting dots** (we call these *edges*)\n",
    ">\n",
    "> Now you have a **graph**! It shows who knows who.\n",
    ">\n",
    "> Here's the cool part: If you want to predict who might become friends next, you can look at people who have **many friends in common**. Two people who both know Alice, Bob, and Charlie will probably become friends too!\n",
    ">\n",
    "> **Graph Neural Networks do exactly this** - they learn patterns from connections to make predictions. To understand a person (node), look at who they're connected to (neighbors). Then look at neighbors of neighbors. This creates a rich understanding of someone's \"position\" in the social network.\n",
    ">\n",
    "> **In AI terms:** GNNs use *message passing* - each node sends messages to its neighbors, collects messages from neighbors, and updates its understanding. After a few rounds of this \"telephone game,\" every node knows about its local neighborhood.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup\n",
    "\n",
    "### 1.1 Installing PyTorch Geometric\n",
    "\n",
    "PyTorch Geometric (PyG) is the most popular library for Graph Neural Networks. It provides:\n",
    "- Efficient graph data structures\n",
    "- Pre-built GNN layers (GCN, GAT, GraphSAGE, etc.)\n",
    "- Popular graph datasets\n",
    "- Mini-batch training utilities for large graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check our environment\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch Geometric and dependencies\n",
    "# This works on DGX Spark (ARM64 + CUDA)\n",
    "!pip install -q torch-geometric\n",
    "!pip install -q networkx matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "import torch_geometric\n",
    "print(f\"PyTorch Geometric version: {torch_geometric.__version__}\")\n",
    "print(\"âœ… PyG installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” What Just Happened?\n",
    "\n",
    "We installed:\n",
    "1. **torch-geometric**: The main PyG library with GNN implementations\n",
    "2. **networkx**: For graph visualization and analysis\n",
    "3. **matplotlib**: For plotting\n",
    "\n",
    "> **DGX Spark Note:** PyG works great on our ARM64 architecture! The library auto-detects CUDA and uses GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding Graph Data Structures\n",
    "\n",
    "### 2.1 How Computers Represent Graphs\n",
    "\n",
    "There are two main ways to store a graph:\n",
    "\n",
    "**1. Adjacency Matrix** - A square matrix where entry (i,j) = 1 if nodes i and j are connected\n",
    "```\n",
    "     A  B  C  D\n",
    "A [[ 0, 1, 1, 0 ],\n",
    "B  [ 1, 0, 1, 1 ],\n",
    "C  [ 1, 1, 0, 0 ],\n",
    "D  [ 0, 1, 0, 0 ]]\n",
    "```\n",
    "- âœ… Easy matrix operations\n",
    "- âŒ Wastes memory for sparse graphs (most real graphs are sparse!)\n",
    "\n",
    "**2. Edge List** - Just list the connections\n",
    "```\n",
    "[[A, B], [A, C], [B, C], [B, D]]\n",
    "```\n",
    "- âœ… Memory efficient for sparse graphs\n",
    "- âœ… PyG uses this format!\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Create a simple graph: A triangle (A-B-C) with D connected to B\n",
    "# Node indices: A=0, B=1, C=2, D=3\n",
    "\n",
    "# Method 1: Adjacency Matrix (dense)\n",
    "adj_matrix = torch.tensor([\n",
    "    [0, 1, 1, 0],  # A connects to B, C\n",
    "    [1, 0, 1, 1],  # B connects to A, C, D\n",
    "    [1, 1, 0, 0],  # C connects to A, B\n",
    "    [0, 1, 0, 0],  # D connects to B\n",
    "], dtype=torch.float)\n",
    "\n",
    "print(\"Adjacency Matrix (dense):\")\n",
    "print(adj_matrix)\n",
    "print(f\"Memory: {adj_matrix.numel() * 4} bytes\\n\")\n",
    "\n",
    "# Method 2: Edge Index (sparse) - PyG format\n",
    "# Format: [[source nodes], [target nodes]]\n",
    "edge_index = torch.tensor([\n",
    "    [0, 0, 1, 1, 1, 2, 2, 3],  # source nodes\n",
    "    [1, 2, 0, 2, 3, 0, 1, 1],  # target nodes\n",
    "], dtype=torch.long)\n",
    "\n",
    "print(\"Edge Index (sparse - PyG format):\")\n",
    "print(edge_index)\n",
    "print(f\"Memory: {edge_index.numel() * 8} bytes\")\n",
    "print(f\"\\nEdges: {edge_index.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§’ ELI5: Edge Index Format\n",
    "\n",
    "> Think of it like a phone book for friendships:\n",
    "> - First row: Who's calling (source)\n",
    "> - Second row: Who's receiving (target)\n",
    ">\n",
    "> So `[[0, 1], [1, 2]]` means: \"0 calls 1\" and \"1 calls 2\"\n",
    ">\n",
    "> For **undirected** graphs (like friendships), we list both directions:\n",
    "> \"0 calls 1\" AND \"1 calls 0\" (both are friends with each other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize our simple graph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create NetworkX graph from edge index\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(['A', 'B', 'C', 'D'])\n",
    "G.add_edges_from([('A', 'B'), ('A', 'C'), ('B', 'C'), ('B', 'D')])\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "nx.draw(G, pos, with_labels=True, node_color='lightblue', \n",
    "        node_size=2000, font_size=20, font_weight='bold',\n",
    "        edge_color='gray', width=2)\n",
    "plt.title(\"Our Simple Graph: A Triangle + One Extra Node\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "print(f\"Average degree: {sum(dict(G.degree()).values()) / G.number_of_nodes():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: The Cora Citation Network\n",
    "\n",
    "### 3.1 What is Cora?\n",
    "\n",
    "Cora is the \"MNIST of graph learning\" - a classic benchmark dataset.\n",
    "\n",
    "- **Nodes**: 2,708 scientific papers\n",
    "- **Edges**: 10,556 citation links (paper A cites paper B)\n",
    "- **Features**: 1,433-dimensional bag-of-words for each paper\n",
    "- **Labels**: 7 categories (Neural Networks, Case-Based, etc.)\n",
    "- **Task**: Predict the category of each paper based on its content AND citation network\n",
    "\n",
    "Why is this interesting? A paper's topic is revealed not just by its words, but by **what papers cite it and what papers it cites**. GNNs can leverage this network structure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "import os\n",
    "\n",
    "# Download and load Cora dataset\n",
    "# The Planetoid class handles downloading and preprocessing\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"CORA DATASET OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Number of graphs: {len(dataset)}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")\n",
    "print(f\"Number of node features: {dataset.num_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the single graph in the dataset\n",
    "data = dataset[0]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"GRAPH STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Number of nodes: {data.num_nodes:,}\")\n",
    "print(f\"Number of edges: {data.num_edges:,}\")\n",
    "print(f\"Average node degree: {data.num_edges / data.num_nodes:.2f}\")\n",
    "print(f\"Has isolated nodes: {data.has_isolated_nodes()}\")\n",
    "print(f\"Has self-loops: {data.has_self_loops()}\")\n",
    "print(f\"Is undirected: {data.is_undirected()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Exploring the PyG Data Object\n",
    "\n",
    "PyG uses a special `Data` object to store graphs. Let's examine its components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"DATA OBJECT STRUCTURE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nData object: {data}\")\n",
    "print(\"\\nAvailable attributes:\")\n",
    "for key in data.keys():\n",
    "    print(f\"  - {key}: {data[key].shape if hasattr(data[key], 'shape') else type(data[key]).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine each component in detail\n",
    "\n",
    "print(\"\\nðŸ“Š NODE FEATURES (data.x)\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Shape: {data.x.shape}\")\n",
    "print(f\"Dtype: {data.x.dtype}\")\n",
    "print(f\"Description: {data.num_nodes} papers, each with {data.num_features} word features\")\n",
    "print(f\"Example (first 10 features of first paper): {data.x[0, :10]}\")\n",
    "print(f\"Non-zero features in first paper: {(data.x[0] > 0).sum().item()}\")\n",
    "\n",
    "print(\"\\nðŸ”— EDGE INDEX (data.edge_index)\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Shape: {data.edge_index.shape}\")\n",
    "print(f\"Dtype: {data.edge_index.dtype}\")\n",
    "print(f\"Description: [2, num_edges] - source and target node indices\")\n",
    "print(f\"First 5 edges:\")\n",
    "for i in range(5):\n",
    "    src, dst = data.edge_index[0, i].item(), data.edge_index[1, i].item()\n",
    "    print(f\"  Paper {src} â†’ cites â†’ Paper {dst}\")\n",
    "\n",
    "print(\"\\nðŸ·ï¸ LABELS (data.y)\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Shape: {data.y.shape}\")\n",
    "print(f\"Dtype: {data.y.dtype}\")\n",
    "print(f\"Number of classes: {data.y.max().item() + 1}\")\n",
    "print(f\"Class distribution:\")\n",
    "for c in range(dataset.num_classes):\n",
    "    count = (data.y == c).sum().item()\n",
    "    print(f\"  Class {c}: {count} papers ({100*count/data.num_nodes:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the train/val/test masks\n",
    "print(\"\\nðŸŽ­ DATA SPLITS (Masks)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(f\"Training mask shape: {data.train_mask.shape}\")\n",
    "print(f\"Training nodes: {data.train_mask.sum().item()} ({100*data.train_mask.sum().item()/data.num_nodes:.1f}%)\")\n",
    "print(f\"Validation nodes: {data.val_mask.sum().item()} ({100*data.val_mask.sum().item()/data.num_nodes:.1f}%)\")\n",
    "print(f\"Test nodes: {data.test_mask.sum().item()} ({100*data.test_mask.sum().item()/data.num_nodes:.1f}%)\")\n",
    "\n",
    "# Show how masks work\n",
    "print(\"\\nðŸ“ How masks work:\")\n",
    "print(f\"First 10 mask values: {data.train_mask[:10]}\")\n",
    "print(f\"\\nTo get training labels: data.y[data.train_mask]\")\n",
    "print(f\"Training labels shape: {data.y[data.train_mask].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” What Just Happened?\n",
    "\n",
    "We explored the Cora dataset structure:\n",
    "\n",
    "| Component | Shape | Description |\n",
    "|-----------|-------|-------------|\n",
    "| `data.x` | [2708, 1433] | Node features (bag-of-words) |\n",
    "| `data.edge_index` | [2, 10556] | Edge connections (citations) |\n",
    "| `data.y` | [2708] | Node labels (7 classes) |\n",
    "| `data.train_mask` | [2708] | Boolean mask for training nodes |\n",
    "| `data.val_mask` | [2708] | Boolean mask for validation nodes |\n",
    "| `data.test_mask` | [2708] | Boolean mask for test nodes |\n",
    "\n",
    "> **Key Insight:** The standard Cora split uses only **140 training nodes** (20 per class) out of 2,708 total! This is a *semi-supervised* setting - we predict labels for nodes using very few labeled examples, leveraging the graph structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Visualizing the Citation Network\n",
    "\n",
    "Let's create some visualizations to understand the graph structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_networkx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Convert to NetworkX for visualization\n",
    "# Note: We convert to undirected for cleaner visualization\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "print(f\"NetworkX graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a small subgraph (full graph is too large to display clearly)\n",
    "# Let's take nodes 0-100 and their connections\n",
    "\n",
    "subgraph_nodes = list(range(100))\n",
    "subgraph = G.subgraph(subgraph_nodes)\n",
    "\n",
    "# Get labels for coloring\n",
    "node_colors = data.y[subgraph_nodes].numpy()\n",
    "\n",
    "# Create a nice color map\n",
    "class_names = ['Case-Based', 'Genetic Alg.', 'Neural Nets', 'Probabilistic', \n",
    "               'Reinfortic', 'Rule Learn.', 'Theory']\n",
    "cmap = plt.cm.Set3\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "pos = nx.spring_layout(subgraph, seed=42, k=2)\n",
    "nx.draw(subgraph, pos, \n",
    "        node_color=node_colors, \n",
    "        cmap=cmap,\n",
    "        node_size=100, \n",
    "        with_labels=False,\n",
    "        edge_color='lightgray',\n",
    "        alpha=0.8,\n",
    "        width=0.5)\n",
    "\n",
    "# Add legend\n",
    "handles = [mpatches.Patch(color=cmap(i/6), label=class_names[i]) for i in range(7)]\n",
    "plt.legend(handles=handles, loc='upper left', fontsize=10)\n",
    "plt.title(\"Cora Subgraph: First 100 Papers (Colored by Topic)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze graph statistics\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\nðŸ“ˆ GRAPH STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Degree distribution\n",
    "degrees = [d for n, d in G.degree()]\n",
    "print(f\"\\nDegree Statistics:\")\n",
    "print(f\"  Min degree: {min(degrees)}\")\n",
    "print(f\"  Max degree: {max(degrees)}\")\n",
    "print(f\"  Mean degree: {np.mean(degrees):.2f}\")\n",
    "print(f\"  Median degree: {np.median(degrees):.1f}\")\n",
    "\n",
    "# Clustering coefficient\n",
    "avg_clustering = nx.average_clustering(G)\n",
    "print(f\"\\nClustering coefficient: {avg_clustering:.4f}\")\n",
    "print(\"  (How likely neighbors of a node are also connected)\")\n",
    "\n",
    "# Connected components\n",
    "n_components = nx.number_connected_components(G)\n",
    "print(f\"\\nNumber of connected components: {n_components}\")\n",
    "\n",
    "# Plot degree distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(degrees, bins=50, color='steelblue', edgecolor='white')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Degree Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(degrees, bins=50, color='steelblue', edgecolor='white', log=True)\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Count (log scale)')\n",
    "plt.title('Degree Distribution (Log Scale)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Notice the long tail - a few papers are highly cited (hub nodes)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most connected papers\n",
    "degree_dict = dict(G.degree())\n",
    "sorted_nodes = sorted(degree_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nðŸŒŸ TOP 10 MOST CITED PAPERS\")\n",
    "print(\"-\" * 40)\n",
    "for node_id, degree in sorted_nodes[:10]:\n",
    "    label = data.y[node_id].item()\n",
    "    print(f\"Paper {node_id}: {degree} citations (Class: {class_names[label]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§’ ELI5: What Do These Statistics Mean?\n",
    "\n",
    "> **Degree** = Number of friends. Most papers have few citations, but some \"famous\" papers have many.\n",
    ">\n",
    "> **Clustering Coefficient** = If your friends are also friends with each other. In Cora, it's moderate - papers in the same field tend to cite each other.\n",
    ">\n",
    "> **Connected Component** = A group where everyone can reach everyone through some path. Cora is mostly one big component - science is connected!\n",
    ">\n",
    "> **Power Law Distribution** = A few papers get LOTS of citations (famous works), while most papers get only a few. This is typical in real networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Working with PyG Data Objects\n",
    "\n",
    "### 5.1 Moving Data to GPU\n",
    "\n",
    "On DGX Spark, we have 128GB of unified memory - plenty of room for graph data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory before loading\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory before: {torch.cuda.memory_allocated() / 1e6:.1f} MB allocated\")\n",
    "\n",
    "# Move data to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device)\n",
    "\n",
    "print(f\"\\nData moved to: {device}\")\n",
    "print(f\"Node features device: {data.x.device}\")\n",
    "print(f\"Edge index device: {data.edge_index.device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU Memory after: {torch.cuda.memory_allocated() / 1e6:.1f} MB allocated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory usage breakdown\n",
    "def calculate_memory(tensor):\n",
    "    \"\"\"Calculate memory usage of a tensor in MB.\"\"\"\n",
    "    return tensor.numel() * tensor.element_size() / 1e6\n",
    "\n",
    "print(\"\\nðŸ’¾ MEMORY USAGE BREAKDOWN\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Node features (x): {calculate_memory(data.x):.2f} MB\")\n",
    "print(f\"Edge index: {calculate_memory(data.edge_index):.2f} MB\")\n",
    "print(f\"Labels (y): {calculate_memory(data.y):.4f} MB\")\n",
    "\n",
    "total_mb = calculate_memory(data.x) + calculate_memory(data.edge_index) + calculate_memory(data.y)\n",
    "print(f\"\\nTotal: {total_mb:.2f} MB\")\n",
    "print(f\"\\nâœ… Tiny compared to our 128GB unified memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Creating Custom Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default Cora split is very small (140 training nodes)\n",
    "# Let's create a custom split with more training data\n",
    "\n",
    "def create_custom_split(data, train_ratio=0.6, val_ratio=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Create custom train/val/test split for node classification.\n",
    "    \n",
    "    Args:\n",
    "        data: PyG Data object\n",
    "        train_ratio: Fraction of nodes for training\n",
    "        val_ratio: Fraction of nodes for validation\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Modified data with new masks\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    n_nodes = data.num_nodes\n",
    "    n_train = int(n_nodes * train_ratio)\n",
    "    n_val = int(n_nodes * val_ratio)\n",
    "    \n",
    "    # Shuffle node indices\n",
    "    perm = torch.randperm(n_nodes)\n",
    "    \n",
    "    # Create masks\n",
    "    train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "    \n",
    "    train_mask[perm[:n_train]] = True\n",
    "    val_mask[perm[n_train:n_train + n_val]] = True\n",
    "    test_mask[perm[n_train + n_val:]] = True\n",
    "    \n",
    "    # Update data object\n",
    "    data.train_mask = train_mask.to(data.x.device)\n",
    "    data.val_mask = val_mask.to(data.x.device)\n",
    "    data.test_mask = test_mask.to(data.x.device)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Create 60/20/20 split\n",
    "data_custom = create_custom_split(data.clone(), train_ratio=0.6, val_ratio=0.2)\n",
    "\n",
    "print(\"Custom Split (60/20/20):\")\n",
    "print(f\"  Training: {data_custom.train_mask.sum().item()} nodes\")\n",
    "print(f\"  Validation: {data_custom.val_mask.sum().item()} nodes\")\n",
    "print(f\"  Test: {data_custom.test_mask.sum().item()} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ‹ Try It Yourself: Exercise 1\n",
    "\n",
    "**Task:** Load and explore the **CiteSeer** dataset (another citation network).\n",
    "\n",
    "1. Load CiteSeer using `Planetoid(root='/tmp/CiteSeer', name='CiteSeer')`\n",
    "2. Print the number of nodes, edges, features, and classes\n",
    "3. Compare CiteSeer to Cora - which is larger?\n",
    "4. Visualize a subgraph of 50 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "# Hint: The code is almost identical to what we did for Cora\n",
    "\n",
    "# Step 1: Load CiteSeer\n",
    "# citeseer = Planetoid(root='/tmp/CiteSeer', name='CiteSeer')\n",
    "\n",
    "# Step 2: Print statistics\n",
    "# ...\n",
    "\n",
    "# Step 3: Compare to Cora\n",
    "# ...\n",
    "\n",
    "# Step 4: Visualize subgraph\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Hint</summary>\n",
    "\n",
    "```python\n",
    "# Load CiteSeer\n",
    "citeseer = Planetoid(root='/tmp/CiteSeer', name='CiteSeer')\n",
    "cs_data = citeseer[0]\n",
    "\n",
    "# Compare\n",
    "print(f\"CiteSeer: {cs_data.num_nodes} nodes, {cs_data.num_edges} edges\")\n",
    "print(f\"Cora: {data.num_nodes} nodes, {data.num_edges} edges\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ‹ Try It Yourself: Exercise 2\n",
    "\n",
    "**Task:** Analyze the neighborhood of a specific node.\n",
    "\n",
    "1. Pick node 0 (or any node you like)\n",
    "2. Find all its neighbors (directly connected nodes)\n",
    "3. What are the labels of the neighbors? Are they mostly the same class?\n",
    "4. This pattern (similar labels for neighbors) is called **homophily** - crucial for GNNs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "# Hint: Use edge_index to find neighbors\n",
    "\n",
    "# Step 1: Pick a node\n",
    "target_node = 0\n",
    "\n",
    "# Step 2: Find neighbors (nodes that have an edge to/from target_node)\n",
    "# edge_index[0] = source nodes, edge_index[1] = target nodes\n",
    "# ...\n",
    "\n",
    "# Step 3: Get labels of neighbors\n",
    "# ...\n",
    "\n",
    "# Step 4: Calculate homophily for this node\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Hint</summary>\n",
    "\n",
    "```python\n",
    "# Move data to CPU for easier manipulation\n",
    "edge_index_cpu = data.edge_index.cpu()\n",
    "labels_cpu = data.y.cpu()\n",
    "\n",
    "# Find neighbors (where target_node is the source)\n",
    "mask = edge_index_cpu[0] == target_node\n",
    "neighbors = edge_index_cpu[1][mask]\n",
    "\n",
    "# Get labels\n",
    "target_label = labels_cpu[target_node]\n",
    "neighbor_labels = labels_cpu[neighbors]\n",
    "\n",
    "# Calculate homophily\n",
    "same_label = (neighbor_labels == target_label).sum()\n",
    "homophily = same_label / len(neighbors)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš ï¸ Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting edge direction\n",
    "```python\n",
    "# âŒ Wrong: Only checking one direction\n",
    "neighbors = edge_index[1][edge_index[0] == node]\n",
    "\n",
    "# âœ… Right: For undirected graphs, check both directions\n",
    "out_neighbors = edge_index[1][edge_index[0] == node]\n",
    "in_neighbors = edge_index[0][edge_index[1] == node]\n",
    "all_neighbors = torch.unique(torch.cat([out_neighbors, in_neighbors]))\n",
    "```\n",
    "**Why:** In undirected graphs stored as directed edges, each edge appears twice.\n",
    "\n",
    "### Mistake 2: Using dense operations on large graphs\n",
    "```python\n",
    "# âŒ Wrong: Creates huge dense matrix\n",
    "adj_matrix = torch.zeros(num_nodes, num_nodes)\n",
    "for i, j in edge_index.t():\n",
    "    adj_matrix[i, j] = 1\n",
    "\n",
    "# âœ… Right: Use sparse operations\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "adj_matrix = to_dense_adj(edge_index)  # Only if you really need it!\n",
    "# Better: Work with edge_index directly\n",
    "```\n",
    "**Why:** A graph with 1M nodes would need 1MÃ—1M = 1TB for a dense matrix!\n",
    "\n",
    "### Mistake 3: Not using masks correctly\n",
    "```python\n",
    "# âŒ Wrong: Training on all data\n",
    "loss = criterion(model(data.x, data.edge_index), data.y)\n",
    "\n",
    "# âœ… Right: Only train on labeled training nodes\n",
    "out = model(data.x, data.edge_index)\n",
    "loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "```\n",
    "**Why:** Test nodes should never be used during training - that's cheating!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- âœ… Why graphs are important (social networks, molecules, citations)\n",
    "- âœ… How graphs are represented (edge index vs adjacency matrix)\n",
    "- âœ… How to install and use PyTorch Geometric\n",
    "- âœ… The structure of PyG Data objects (x, edge_index, y, masks)\n",
    "- âœ… How to visualize graphs with NetworkX\n",
    "- âœ… Key graph statistics (degree, clustering, connectivity)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Challenge (Optional)\n",
    "\n",
    "**Advanced Challenge:** Calculate the **graph-wide homophily** score.\n",
    "\n",
    "Homophily = fraction of edges that connect nodes of the same class.\n",
    "\n",
    "- High homophily (>0.7): Similar nodes are connected â†’ GNNs work great!\n",
    "- Low homophily (<0.3): Dissimilar nodes are connected â†’ Need special architectures\n",
    "\n",
    "Calculate homophily for Cora. Is it high or low? What does this mean for GNN performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Challenge: Calculate homophily\n",
    "# Hint: For each edge (i, j), check if y[i] == y[j]\n",
    "\n",
    "# Your code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“– Further Reading\n",
    "\n",
    "- [PyTorch Geometric Documentation](https://pytorch-geometric.readthedocs.io/) - Official docs\n",
    "- [CS224W: Machine Learning with Graphs](http://web.stanford.edu/class/cs224w/) - Stanford course\n",
    "- [A Gentle Introduction to GNNs](https://distill.pub/2021/gnn-intro/) - Distill article\n",
    "- [Graph Representation Learning Book](https://www.cs.mcgill.ca/~wlh/grl_book/) - Free online textbook\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "\n",
    "# Delete large objects\n",
    "del G, subgraph\n",
    "\n",
    "# Clear cache\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory after cleanup: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
    "\n",
    "print(\"âœ… Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## â­ï¸ Next Steps\n",
    "\n",
    "Now that you understand graph data and PyG basics, you're ready to build your first Graph Neural Network!\n",
    "\n",
    "**In Lab E.2: GCN from Scratch**, you'll:\n",
    "- Implement the Graph Convolutional Network layer\n",
    "- Understand the message passing paradigm\n",
    "- Train a GCN to classify papers in Cora\n",
    "- Achieve >80% accuracy with just a few lines of code!\n",
    "\n",
    "Let's go! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
