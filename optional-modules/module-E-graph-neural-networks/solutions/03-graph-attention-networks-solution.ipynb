{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab E.3 Solutions: Graph Attention Networks\n",
    "\n",
    "Complete solutions to all exercises in Lab E.3.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Different Number of Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_dim=8, heads=8):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(num_features, hidden_dim, heads=heads, dropout=0.6)\n",
    "        self.conv2 = GATConv(hidden_dim * heads, num_classes, heads=1, \n",
    "                            concat=False, dropout=0.6)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "def train_gat(heads, epochs=300):\n",
    "    model = GAT(dataset.num_features, dataset.num_classes, \n",
    "                hidden_dim=8, heads=heads).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "    \n",
    "    start = time.time()\n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(data.x, data.edge_index).argmax(dim=1)\n",
    "            acc = (pred[data.test_mask] == data.y[data.test_mask]).float().mean().item()\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "    \n",
    "    train_time = time.time() - start\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    return best_acc, n_params, train_time\n",
    "\n",
    "# Compare different head counts\n",
    "head_counts = [1, 2, 4, 8, 16]\n",
    "results = []\n",
    "\n",
    "print(\"Comparing Number of Attention Heads\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Heads':<10} {'Test Acc':<12} {'Params':<15} {'Time (s)':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for heads in head_counts:\n",
    "    acc, params, t = train_gat(heads)\n",
    "    results.append((heads, acc, params, t))\n",
    "    print(f\"{heads:<10} {acc:<12.4f} {params:<15,} {t:<10.2f}\")\n",
    "\n",
    "best = max(results, key=lambda x: x[1])\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"ğŸ† Best: heads={best[0]} with {best[1]:.4f} accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "heads = [r[0] for r in results]\n",
    "accs = [r[1] for r in results]\n",
    "params = [r[2] for r in results]\n",
    "times = [r[3] for r in results]\n",
    "\n",
    "axes[0].plot(heads, accs, 'o-', color='steelblue', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Heads')\n",
    "axes[0].set_ylabel('Test Accuracy')\n",
    "axes[0].set_title('Accuracy vs Heads')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(heads, [p/1000 for p in params], 'o-', color='coral', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Number of Heads')\n",
    "axes[1].set_ylabel('Parameters (thousands)')\n",
    "axes[1].set_title('Parameters vs Heads')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(heads, times, 'o-', color='seagreen', linewidth=2, markersize=8)\n",
    "axes[2].set_xlabel('Number of Heads')\n",
    "axes[2].set_ylabel('Training Time (s)')\n",
    "axes[2].set_title('Time vs Heads')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Key Insights:\")\n",
    "print(\"   - 8 heads is the standard choice (from original paper)\")\n",
    "print(\"   - More heads = more parameters and training time\")\n",
    "print(\"   - Diminishing returns beyond ~8 heads for Cora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Find Most Important Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "# Train a GAT model that returns attention\n",
    "class GATWithAttention(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(num_features, 8, heads=8, dropout=0.6)\n",
    "        self.conv2 = GATConv(64, num_classes, heads=1, concat=False, dropout=0.6)\n",
    "    \n",
    "    def forward(self, x, edge_index, return_attention=False):\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        \n",
    "        if return_attention:\n",
    "            x, (edge_idx1, attn1) = self.conv1(x, edge_index, return_attention_weights=True)\n",
    "        else:\n",
    "            x = self.conv1(x, edge_index)\n",
    "        \n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        \n",
    "        if return_attention:\n",
    "            x, (edge_idx2, attn2) = self.conv2(x, edge_index, return_attention_weights=True)\n",
    "            return x, (attn1, attn2, edge_idx1)\n",
    "        \n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "# Train model\n",
    "model = GATWithAttention(dataset.num_features, dataset.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "for epoch in range(300):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attention weights\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out, (attn1, attn2, edge_index) = model(data.x, data.edge_index, return_attention=True)\n",
    "\n",
    "# Average attention across 8 heads\n",
    "avg_attn = attn1.mean(dim=1).cpu().numpy()  # [num_edges]\n",
    "edge_index_np = edge_index.cpu().numpy()\n",
    "labels = data.y.cpu().numpy()\n",
    "\n",
    "# Find top-10 edges\n",
    "top_10_idx = np.argsort(avg_attn)[-10:][::-1]\n",
    "\n",
    "print(\"Top-10 Edges by Attention Weight\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Rank':<6} {'Edge':<20} {'Attention':<12} {'Same Class?':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "same_count = 0\n",
    "for i, idx in enumerate(top_10_idx):\n",
    "    src = edge_index_np[0, idx]\n",
    "    dst = edge_index_np[1, idx]\n",
    "    attn = avg_attn[idx]\n",
    "    same_class = labels[src] == labels[dst]\n",
    "    if same_class:\n",
    "        same_count += 1\n",
    "    print(f\"{i+1:<6} {src} â†’ {dst:<12} {attn:<12.4f} {'Yes âœ“' if same_class else 'No âœ—':<12}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Same-class edges in top-10: {same_count}/10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze overall: Do high-attention edges tend to be same-class?\n",
    "src_labels = labels[edge_index_np[0]]\n",
    "dst_labels = labels[edge_index_np[1]]\n",
    "same_class = src_labels == dst_labels\n",
    "\n",
    "# Compare attention distributions\n",
    "same_class_attn = avg_attn[same_class].mean()\n",
    "diff_class_attn = avg_attn[~same_class].mean()\n",
    "\n",
    "print(\"\\nğŸ“Š Overall Attention Pattern Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Mean attention to SAME-class: {same_class_attn:.4f}\")\n",
    "print(f\"Mean attention to DIFF-class: {diff_class_attn:.4f}\")\n",
    "print(f\"\\nRatio: {same_class_attn/diff_class_attn:.2f}x more attention to same class!\")\n",
    "print(\"\\nğŸ’¡ The model learns to focus on same-class neighbors!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Solution: GATv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import add_self_loops, softmax\n",
    "\n",
    "class GATv2Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    GATv2: Dynamic attention (Brody et al., 2021)\n",
    "    \n",
    "    Key difference from GAT:\n",
    "    - GAT:   e_ij = LeakyReLU(a_l * Wh_i + a_r * Wh_j)\n",
    "    - GATv2: e_ij = a * LeakyReLU(W * [h_i || h_j])\n",
    "    \n",
    "    LeakyReLU after concatenation = more expressive!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, heads=1, dropout=0.6):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Linear transformation for concatenated features\n",
    "        self.W = nn.Linear(2 * in_channels, heads * out_channels, bias=False)\n",
    "        \n",
    "        # Attention vector (applied AFTER LeakyReLU)\n",
    "        self.a = nn.Parameter(torch.Tensor(heads, out_channels))\n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        nn.init.xavier_uniform_(self.a)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        N = x.size(0)\n",
    "        H, C = self.heads, self.out_channels\n",
    "        \n",
    "        # Add self-loops\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=N)\n",
    "        src, dst = edge_index\n",
    "        \n",
    "        # Concatenate source and destination features\n",
    "        x_cat = torch.cat([x[src], x[dst]], dim=-1)  # [E, 2*in]\n",
    "        \n",
    "        # Transform and apply LeakyReLU\n",
    "        x_cat = self.W(x_cat).view(-1, H, C)  # [E, H, C]\n",
    "        x_cat = self.leaky_relu(x_cat)\n",
    "        \n",
    "        # Attention scores\n",
    "        e = (x_cat * self.a).sum(dim=-1)  # [E, H]\n",
    "        \n",
    "        # Softmax over neighbors\n",
    "        alpha = softmax(e, dst, num_nodes=N)\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Aggregation (using original transformed features)\n",
    "        Wx = nn.Linear(x.size(1), H * C, bias=False).to(x.device)(x)\n",
    "        Wx = Wx.view(N, H, C)\n",
    "        \n",
    "        out = torch.zeros(N, H, C, device=x.device)\n",
    "        src_feat = Wx[src] * alpha.unsqueeze(-1)\n",
    "        out.scatter_add_(0, dst.view(-1, 1, 1).expand(-1, H, C), src_feat)\n",
    "        \n",
    "        return out.view(N, H * C)\n",
    "\n",
    "print(\"GATv2 layer implemented!\")\n",
    "print(\"\\nğŸ’¡ GATv2 is more expressive because:\")\n",
    "print(\"   - Original GAT: attention is 'static' (depends only on features)\")\n",
    "print(\"   - GATv2: attention is 'dynamic' (depends on feature interactions)\")\n",
    "print(\"   - This allows the same node pair to have different attention\")\n",
    "print(\"     depending on the learned transformation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **8 heads is the sweet spot** for most tasks (original paper default)\n",
    "2. **GAT learns to attend more to same-class neighbors** - task-aware attention!\n",
    "3. **GATv2 is more expressive** due to dynamic attention computation\n",
    "4. **Attention visualization** helps interpret what the model learns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
