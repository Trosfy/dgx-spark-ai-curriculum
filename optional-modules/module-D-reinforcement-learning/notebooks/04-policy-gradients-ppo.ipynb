{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab D.4: Policy Gradients and PPO\n",
    "\n",
    "**Module:** D - Reinforcement Learning (Optional)\n",
    "**Time:** 2.5-3 hours\n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand why policy gradients work differently from value-based methods\n",
    "- [ ] Implement the REINFORCE algorithm from scratch\n",
    "- [ ] Build an Actor-Critic architecture\n",
    "- [ ] Implement Proximal Policy Optimization (PPO)\n",
    "- [ ] Understand why PPO is used for RLHF in LLMs\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Lab D.3 (Deep Q-Networks)\n",
    "- Knowledge of: PyTorch, neural networks, gradients\n",
    "- Understanding of: Probability distributions, log probabilities\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**Why Policy Gradients for LLMs?**\n",
    "\n",
    "DQN outputs Q-values for each action. But for LLMs:\n",
    "- Action space = entire vocabulary (~50,000+ tokens!)\n",
    "- Computing Q(s, a) for every token is expensive\n",
    "- We already have a policy: the LLM outputs token probabilities!\n",
    "\n",
    "**Solution: Directly optimize the policy (the LLM itself)**\n",
    "\n",
    "This is exactly what RLHF does:\n",
    "- The LLM is the policy œÄ(a|s)\n",
    "- PPO adjusts the LLM to generate better responses\n",
    "- ChatGPT, Claude, and other assistants use this approach!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: Policy Gradients vs Value-Based Methods\n",
    "\n",
    "> **Value-based (DQN)**: \"Let me figure out how good each action is, then pick the best one.\"\n",
    "> - Like a chess player calculating the value of each possible move\n",
    "> - Works well when you can evaluate all actions\n",
    ">\n",
    "> **Policy-based**: \"Let me directly learn which actions to take in each situation.\"\n",
    "> - Like a tennis player developing muscle memory\n",
    "> - Don't evaluate every option, just learn good habits\n",
    ">\n",
    "> **The key insight of policy gradients:**\n",
    "> - If an action led to high reward, make it MORE likely\n",
    "> - If an action led to low reward, make it LESS likely\n",
    "> - Adjust probabilities proportionally to how good the outcome was\n",
    ">\n",
    "> **In AI terms:** Policy gradient methods directly optimize the policy parameters by computing the gradient of expected reward with respect to those parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - run this first!\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "from collections import deque\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Gymnasium\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except ImportError:\n",
    "    !pip install gymnasium -q\n",
    "    import gymnasium as gym\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"üöÄ Module D.4: Policy Gradients and PPO\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use CartPole again, plus LunarLander for a harder challenge\n",
    "env_cartpole = gym.make(\"CartPole-v1\")\n",
    "env_lunar = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "print(\"üéÆ Environments:\")\n",
    "print(f\"\\nCartPole-v1:\")\n",
    "print(f\"   State: {env_cartpole.observation_space.shape}\")\n",
    "print(f\"   Actions: {env_cartpole.action_space.n}\")\n",
    "\n",
    "print(f\"\\nLunarLander-v2:\")\n",
    "print(f\"   State: {env_lunar.observation_space.shape}\")\n",
    "print(f\"   Actions: {env_lunar.action_space.n}\")\n",
    "print(\"   (0=nothing, 1=left engine, 2=main engine, 3=right engine)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The Policy Gradient Theorem\n",
    "\n",
    "### The Math Behind Policy Gradients\n",
    "\n",
    "Our goal: Maximize expected return $J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[R]$\n",
    "\n",
    "The **Policy Gradient Theorem** tells us:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot Q^{\\pi}(s, a) \\right]$$\n",
    "\n",
    "In plain English:\n",
    "1. Sample actions from your policy\n",
    "2. Compute how much each action contributed to the outcome\n",
    "3. Increase probability of good actions, decrease for bad ones\n",
    "\n",
    "### The Log Probability Trick\n",
    "\n",
    "Why $\\log \\pi$ instead of $\\pi$?\n",
    "\n",
    "> üßí **ELI5**: If you got an A on a test, you should study MORE like you did. If you got an F, study LESS like that. The log trick lets us turn \"multiply probability\" into \"add to log probability\", which works better with gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: REINFORCE Algorithm\n",
    "\n",
    "REINFORCE is the simplest policy gradient algorithm:\n",
    "\n",
    "1. Run a complete episode, collecting (state, action, reward)\n",
    "2. Compute returns (total future reward) for each timestep\n",
    "3. Update: $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t$\n",
    "\n",
    "Where $G_t = \\sum_{k=t}^T \\gamma^{k-t} r_k$ is the return from timestep $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple policy network that outputs action probabilities.\n",
    "    \n",
    "    Unlike Q-network (outputs values), this outputs a probability distribution!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)  # Output probabilities!\n",
    "        )\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns action probabilities.\"\"\"\n",
    "        return self.network(state)\n",
    "    \n",
    "    def get_action(self, state: torch.Tensor) -> Tuple[int, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Sample an action from the policy.\n",
    "        \n",
    "        Returns:\n",
    "            action: The sampled action\n",
    "            log_prob: Log probability of that action (for gradient computation)\n",
    "        \"\"\"\n",
    "        probs = self.forward(state)\n",
    "        dist = Categorical(probs)  # Create categorical distribution\n",
    "        action = dist.sample()     # Sample an action\n",
    "        log_prob = dist.log_prob(action)  # Get log probability\n",
    "        return action.item(), log_prob\n",
    "\n",
    "# Test it\n",
    "policy = PolicyNetwork(4, 2).to(device)\n",
    "state = torch.randn(1, 4).to(device)\n",
    "probs = policy(state)\n",
    "print(f\"Action probabilities: {probs.detach().cpu().numpy()[0]}\")\n",
    "\n",
    "action, log_prob = policy.get_action(state)\n",
    "print(f\"Sampled action: {action}, log_prob: {log_prob.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEAgent:\n",
    "    \"\"\"\n",
    "    REINFORCE: Monte Carlo Policy Gradient.\n",
    "    \n",
    "    The simplest policy gradient algorithm.\n",
    "    Updates after each complete episode using total returns.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, \n",
    "                 lr: float = 1e-3, gamma: float = 0.99):\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.policy = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        # Episode storage\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        print(f\"ü§ñ REINFORCE Agent Initialized\")\n",
    "        print(f\"   Parameters: {sum(p.numel() for p in self.policy.parameters()):,}\")\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"Select action and store log probability.\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action, log_prob = self.policy.get_action(state_tensor)\n",
    "        self.log_probs.append(log_prob)\n",
    "        return action\n",
    "    \n",
    "    def store_reward(self, reward: float):\n",
    "        \"\"\"Store reward for current timestep.\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def compute_returns(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute discounted returns for each timestep.\n",
    "        \n",
    "        G_t = r_t + Œ≥*r_{t+1} + Œ≥¬≤*r_{t+2} + ...\n",
    "        \"\"\"\n",
    "        returns = []\n",
    "        G = 0\n",
    "        \n",
    "        # Work backwards from end of episode\n",
    "        for reward in reversed(self.rewards):\n",
    "            G = reward + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        returns = torch.tensor(returns, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Normalize returns (reduces variance!)\n",
    "        if len(returns) > 1:\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        return returns\n",
    "    \n",
    "    def update(self) -> float:\n",
    "        \"\"\"\n",
    "        Update policy using REINFORCE.\n",
    "        \n",
    "        Loss = -sum(log_prob * return)  (negative because we maximize)\n",
    "        \"\"\"\n",
    "        returns = self.compute_returns()\n",
    "        log_probs = torch.stack(self.log_probs)\n",
    "        \n",
    "        # Policy gradient loss\n",
    "        # We want to MAXIMIZE log_prob * return, so we MINIMIZE negative\n",
    "        loss = -(log_probs * returns).sum()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Clear episode storage\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "# Create agent\n",
    "reinforce_agent = REINFORCEAgent(\n",
    "    state_dim=env_cartpole.observation_space.shape[0],\n",
    "    action_dim=env_cartpole.action_space.n,\n",
    "    lr=1e-3,\n",
    "    gamma=0.99\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reinforce(env, agent, n_episodes: int = 1000, \n",
    "                    print_freq: int = 100) -> List[float]:\n",
    "    \"\"\"\n",
    "    Train REINFORCE agent.\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Run episode\n",
    "        while True:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.store_reward(reward)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Update at end of episode\n",
    "        agent.update()\n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % print_freq == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            print(f\"Episode {episode + 1:4d} | Avg Reward (last 100): {avg_reward:.1f}\")\n",
    "    \n",
    "    return episode_rewards\n",
    "\n",
    "# Train!\n",
    "print(\"\\nüèãÔ∏è Training REINFORCE on CartPole...\\n\")\n",
    "reinforce_rewards = train_reinforce(env_cartpole, reinforce_agent, n_episodes=1000)\n",
    "\n",
    "print(f\"\\n‚úÖ Final average (last 100): {np.mean(reinforce_rewards[-100:]):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(reinforce_rewards, alpha=0.3)\n",
    "window = 50\n",
    "smoothed = np.convolve(reinforce_rewards, np.ones(window)/window, mode='valid')\n",
    "plt.plot(range(window-1, len(reinforce_rewards)), smoothed, label=f'{window}-ep avg')\n",
    "plt.axhline(y=475, color='g', linestyle='--', label='Goal')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('REINFORCE Training')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Show variance in rewards\n",
    "chunk_size = 50\n",
    "n_chunks = len(reinforce_rewards) // chunk_size\n",
    "variances = [np.var(reinforce_rewards[i*chunk_size:(i+1)*chunk_size]) \n",
    "             for i in range(n_chunks)]\n",
    "plt.plot([i*chunk_size for i in range(n_chunks)], variances)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward Variance')\n",
    "plt.title('REINFORCE has HIGH VARIANCE')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem with REINFORCE: High Variance\n",
    "\n",
    "Notice the jagged learning curve! REINFORCE has **high variance** because:\n",
    "1. Uses Monte Carlo returns (waits until end of episode)\n",
    "2. Each episode has different outcomes due to randomness\n",
    "3. Credit assignment is noisy‚Äîwhich action was actually responsible?\n",
    "\n",
    "**Solution: Actor-Critic methods!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Actor-Critic\n",
    "\n",
    "The key insight: Use a **learned value function** as a baseline to reduce variance.\n",
    "\n",
    "- **Actor**: The policy œÄ(a|s) - decides what to do\n",
    "- **Critic**: The value function V(s) - evaluates how good states are\n",
    "\n",
    "Instead of using raw returns $G_t$, we use the **advantage**:\n",
    "\n",
    "$$A(s, a) = Q(s, a) - V(s) \\approx r + \\gamma V(s') - V(s)$$\n",
    "\n",
    "> üßí **ELI5**: Instead of asking \"Was that action good?\", we ask \"Was that action better than average?\" This is less noisy because we compare to a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined Actor-Critic Network.\n",
    "    \n",
    "    Shared feature extraction with separate heads for:\n",
    "    - Actor (policy): outputs action probabilities\n",
    "    - Critic (value): outputs state value V(s)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared feature extractor\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Actor head (policy)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Critic head (value function)\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            action_probs: Probability distribution over actions\n",
    "            value: State value V(s)\n",
    "        \"\"\"\n",
    "        features = self.shared(state)\n",
    "        action_probs = self.actor(features)\n",
    "        value = self.critic(features)\n",
    "        return action_probs, value\n",
    "    \n",
    "    def get_action_and_value(self, state: torch.Tensor) -> Tuple[int, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Sample action and get log prob + value.\"\"\"\n",
    "        probs, value = self.forward(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob, value\n",
    "\n",
    "# Test it\n",
    "ac_net = ActorCriticNetwork(4, 2).to(device)\n",
    "state = torch.randn(1, 4).to(device)\n",
    "probs, value = ac_net(state)\n",
    "print(f\"Action probs: {probs.detach().cpu().numpy()[0]}\")\n",
    "print(f\"State value: {value.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgent:\n",
    "    \"\"\"\n",
    "    Advantage Actor-Critic (A2C).\n",
    "    \n",
    "    Uses the advantage function to reduce variance in policy gradients.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int,\n",
    "                 lr: float = 3e-4, gamma: float = 0.99,\n",
    "                 value_coef: float = 0.5, entropy_coef: float = 0.01):\n",
    "        self.gamma = gamma\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        self.network = ActorCriticNetwork(state_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        \n",
    "        # Episode storage\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.entropies = []\n",
    "        \n",
    "        print(f\"ü§ñ Actor-Critic Agent Initialized\")\n",
    "        print(f\"   Parameters: {sum(p.numel() for p in self.network.parameters()):,}\")\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"Select action, store log prob, value, and entropy.\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        probs, value = self.network(state_tensor)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        self.log_probs.append(dist.log_prob(action))\n",
    "        self.values.append(value.squeeze())\n",
    "        self.entropies.append(dist.entropy())\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def store_reward(self, reward: float):\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def update(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Update using advantage.\n",
    "        \n",
    "        Advantage = actual return - predicted value\n",
    "        \"\"\"\n",
    "        # Compute returns\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for reward in reversed(self.rewards):\n",
    "            G = reward + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Stack tensors\n",
    "        log_probs = torch.stack(self.log_probs)\n",
    "        values = torch.stack(self.values)\n",
    "        entropies = torch.stack(self.entropies)\n",
    "        \n",
    "        # Compute advantages\n",
    "        advantages = returns - values.detach()\n",
    "        \n",
    "        # Normalize advantages (reduces variance)\n",
    "        if len(advantages) > 1:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Policy loss: -log_prob * advantage\n",
    "        policy_loss = -(log_probs * advantages).mean()\n",
    "        \n",
    "        # Value loss: MSE between predicted and actual returns\n",
    "        value_loss = F.mse_loss(values, returns)\n",
    "        \n",
    "        # Entropy bonus (encourages exploration)\n",
    "        entropy_loss = -entropies.mean()\n",
    "        \n",
    "        # Total loss\n",
    "        loss = policy_loss + self.value_coef * value_loss + self.entropy_coef * entropy_loss\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Clear storage\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.entropies = []\n",
    "        \n",
    "        return {\n",
    "            'policy_loss': policy_loss.item(),\n",
    "            'value_loss': value_loss.item(),\n",
    "            'entropy': -entropy_loss.item()\n",
    "        }\n",
    "\n",
    "# Create and train\n",
    "ac_agent = ActorCriticAgent(\n",
    "    state_dim=env_cartpole.observation_space.shape[0],\n",
    "    action_dim=env_cartpole.action_space.n\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_actor_critic(env, agent, n_episodes: int = 1000,\n",
    "                       print_freq: int = 100) -> List[float]:\n",
    "    \"\"\"Train Actor-Critic agent.\"\"\"\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.store_reward(reward)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        agent.update()\n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        if (episode + 1) % print_freq == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            print(f\"Episode {episode + 1:4d} | Avg Reward: {avg_reward:.1f}\")\n",
    "    \n",
    "    return episode_rewards\n",
    "\n",
    "print(\"\\nüèãÔ∏è Training Actor-Critic on CartPole...\\n\")\n",
    "ac_rewards = train_actor_critic(env_cartpole, ac_agent, n_episodes=1000)\n",
    "\n",
    "print(f\"\\n‚úÖ Final average: {np.mean(ac_rewards[-100:]):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare REINFORCE vs Actor-Critic\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "window = 50\n",
    "\n",
    "# Smooth both\n",
    "reinforce_smooth = np.convolve(reinforce_rewards, np.ones(window)/window, mode='valid')\n",
    "ac_smooth = np.convolve(ac_rewards, np.ones(window)/window, mode='valid')\n",
    "\n",
    "plt.plot(range(window-1, len(reinforce_rewards)), reinforce_smooth, \n",
    "         label='REINFORCE', alpha=0.8)\n",
    "plt.plot(range(window-1, len(ac_rewards)), ac_smooth, \n",
    "         label='Actor-Critic', alpha=0.8)\n",
    "\n",
    "plt.axhline(y=475, color='g', linestyle='--', label='Goal')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward (smoothed)')\n",
    "plt.title('REINFORCE vs Actor-Critic on CartPole')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Comparison:\")\n",
    "print(f\"   REINFORCE final avg:     {np.mean(reinforce_rewards[-100:]):.1f}\")\n",
    "print(f\"   Actor-Critic final avg:  {np.mean(ac_rewards[-100:]):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Proximal Policy Optimization (PPO)\n",
    "\n",
    "### The Stability Problem\n",
    "\n",
    "Policy gradient updates can be unstable:\n",
    "- Large updates might destroy the policy\n",
    "- Small updates might be too slow\n",
    "\n",
    "**PPO's Solution: Clipped Objective**\n",
    "\n",
    "Instead of directly maximizing $r(\\theta) \\cdot A$ where $r(\\theta) = \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{old}}(a|s)}$,\n",
    "\n",
    "PPO maximizes:\n",
    "\n",
    "$$L^{CLIP}(\\theta) = \\min\\left( r(\\theta) A, \\text{clip}(r(\\theta), 1-\\epsilon, 1+\\epsilon) A \\right)$$\n",
    "\n",
    "> üßí **ELI5**: \"Don't change too much at once!\" PPO puts guardrails on how much the policy can change in one update. If you try to change too much, it clips the update back.\n",
    "\n",
    "### Why PPO for LLMs?\n",
    "\n",
    "1. **Stable**: Doesn't destroy the model with one bad update\n",
    "2. **Sample efficient**: Can reuse data multiple times\n",
    "3. **Works with large models**: Scales to billions of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization (PPO).\n",
    "    \n",
    "    The algorithm behind RLHF for ChatGPT, Claude, etc.\n",
    "    \n",
    "    Key features:\n",
    "    - Clipped objective for stable updates\n",
    "    - Multiple epochs per rollout (sample efficient)\n",
    "    - Generalized Advantage Estimation (GAE)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int,\n",
    "                 lr: float = 3e-4,\n",
    "                 gamma: float = 0.99,\n",
    "                 gae_lambda: float = 0.95,\n",
    "                 clip_epsilon: float = 0.2,\n",
    "                 value_coef: float = 0.5,\n",
    "                 entropy_coef: float = 0.01,\n",
    "                 n_epochs: int = 10,\n",
    "                 batch_size: int = 64):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.network = ActorCriticNetwork(state_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        \n",
    "        # Rollout storage\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        \n",
    "        print(f\"ü§ñ PPO Agent Initialized\")\n",
    "        print(f\"   Clip epsilon: {clip_epsilon}\")\n",
    "        print(f\"   Epochs per update: {n_epochs}\")\n",
    "        print(f\"   GAE lambda: {gae_lambda}\")\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"Select action and store transition data.\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            probs, value = self.network(state_tensor)\n",
    "        \n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        self.states.append(state)\n",
    "        self.actions.append(action.item())\n",
    "        self.log_probs.append(dist.log_prob(action).item())\n",
    "        self.values.append(value.item())\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def store_transition(self, reward: float, done: bool):\n",
    "        \"\"\"Store reward and done flag.\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def compute_gae(self, next_value: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Compute Generalized Advantage Estimation.\n",
    "        \n",
    "        GAE smoothly interpolates between:\n",
    "        - TD(0): low variance, high bias (Œª=0)\n",
    "        - Monte Carlo: high variance, low bias (Œª=1)\n",
    "        \"\"\"\n",
    "        values = self.values + [next_value]\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        \n",
    "        # Work backwards\n",
    "        for t in reversed(range(len(self.rewards))):\n",
    "            delta = self.rewards[t] + self.gamma * values[t+1] * (1 - self.dones[t]) - values[t]\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1 - self.dones[t]) * gae\n",
    "            advantages.insert(0, gae)\n",
    "        \n",
    "        advantages = np.array(advantages)\n",
    "        returns = advantages + np.array(self.values)\n",
    "        \n",
    "        return advantages, returns\n",
    "    \n",
    "    def update(self, next_state: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        PPO update with clipped objective.\n",
    "        \n",
    "        This is the heart of PPO!\n",
    "        \"\"\"\n",
    "        # Get final value for GAE computation\n",
    "        with torch.no_grad():\n",
    "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "            _, next_value = self.network(next_state_tensor)\n",
    "            next_value = next_value.item()\n",
    "        \n",
    "        # Compute advantages\n",
    "        advantages, returns = self.compute_gae(next_value)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(np.array(self.states)).to(device)\n",
    "        actions = torch.LongTensor(self.actions).to(device)\n",
    "        old_log_probs = torch.FloatTensor(self.log_probs).to(device)\n",
    "        advantages = torch.FloatTensor(advantages).to(device)\n",
    "        returns = torch.FloatTensor(returns).to(device)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # PPO update: multiple epochs over same data\n",
    "        total_policy_loss = 0\n",
    "        total_value_loss = 0\n",
    "        total_entropy = 0\n",
    "        n_updates = 0\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            # Mini-batch updates\n",
    "            indices = np.random.permutation(len(states))\n",
    "            \n",
    "            for start in range(0, len(states), self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                batch_idx = indices[start:end]\n",
    "                \n",
    "                batch_states = states[batch_idx]\n",
    "                batch_actions = actions[batch_idx]\n",
    "                batch_old_log_probs = old_log_probs[batch_idx]\n",
    "                batch_advantages = advantages[batch_idx]\n",
    "                batch_returns = returns[batch_idx]\n",
    "                \n",
    "                # Get current policy outputs\n",
    "                probs, values = self.network(batch_states)\n",
    "                dist = Categorical(probs)\n",
    "                new_log_probs = dist.log_prob(batch_actions)\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                # Probability ratio\n",
    "                ratio = (new_log_probs - batch_old_log_probs).exp()\n",
    "                \n",
    "                # Clipped surrogate objective\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, \n",
    "                                    1 - self.clip_epsilon, \n",
    "                                    1 + self.clip_epsilon) * batch_advantages\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                # Value loss\n",
    "                value_loss = F.mse_loss(values.squeeze(), batch_returns)\n",
    "                \n",
    "                # Total loss\n",
    "                loss = policy_loss + self.value_coef * value_loss - self.entropy_coef * entropy\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_policy_loss += policy_loss.item()\n",
    "                total_value_loss += value_loss.item()\n",
    "                total_entropy += entropy.item()\n",
    "                n_updates += 1\n",
    "        \n",
    "        # Clear rollout storage\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        \n",
    "        return {\n",
    "            'policy_loss': total_policy_loss / n_updates,\n",
    "            'value_loss': total_value_loss / n_updates,\n",
    "            'entropy': total_entropy / n_updates\n",
    "        }\n",
    "\n",
    "# Create PPO agent\n",
    "ppo_agent = PPOAgent(\n",
    "    state_dim=env_cartpole.observation_space.shape[0],\n",
    "    action_dim=env_cartpole.action_space.n,\n",
    "    lr=3e-4,\n",
    "    clip_epsilon=0.2,\n",
    "    n_epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(env, agent, n_episodes: int = 500,\n",
    "              rollout_length: int = 2048,\n",
    "              print_freq: int = 50) -> List[float]:\n",
    "    \"\"\"\n",
    "    Train PPO agent.\n",
    "    \n",
    "    PPO collects a \"rollout\" of experiences, then updates multiple times on it.\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    current_reward = 0\n",
    "    state, _ = env.reset()\n",
    "    steps = 0\n",
    "    episodes_done = 0\n",
    "    \n",
    "    while episodes_done < n_episodes:\n",
    "        # Collect rollout\n",
    "        for _ in range(rollout_length):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.store_transition(reward, done)\n",
    "            current_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                episode_rewards.append(current_reward)\n",
    "                episodes_done += 1\n",
    "                current_reward = 0\n",
    "                state, _ = env.reset()\n",
    "                \n",
    "                if episodes_done % print_freq == 0:\n",
    "                    avg_reward = np.mean(episode_rewards[-100:])\n",
    "                    print(f\"Episode {episodes_done:4d} | Avg Reward: {avg_reward:.1f}\")\n",
    "                \n",
    "                if episodes_done >= n_episodes:\n",
    "                    break\n",
    "            else:\n",
    "                state = next_state\n",
    "        \n",
    "        # Update policy\n",
    "        if len(agent.states) > 0:\n",
    "            agent.update(state)\n",
    "    \n",
    "    return episode_rewards\n",
    "\n",
    "print(\"\\nüèãÔ∏è Training PPO on CartPole...\\n\")\n",
    "ppo_rewards = train_ppo(env_cartpole, ppo_agent, n_episodes=500)\n",
    "\n",
    "print(f\"\\n‚úÖ Final average: {np.mean(ppo_rewards[-100:]):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three methods\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "window = 50\n",
    "\n",
    "# Limit to same number of episodes for fair comparison\n",
    "n_eps = min(len(reinforce_rewards), len(ac_rewards), len(ppo_rewards))\n",
    "\n",
    "reinforce_smooth = np.convolve(reinforce_rewards[:n_eps], np.ones(window)/window, mode='valid')\n",
    "ac_smooth = np.convolve(ac_rewards[:n_eps], np.ones(window)/window, mode='valid')\n",
    "ppo_smooth = np.convolve(ppo_rewards[:n_eps], np.ones(window)/window, mode='valid')\n",
    "\n",
    "plt.plot(range(window-1, n_eps), reinforce_smooth, label='REINFORCE', alpha=0.8)\n",
    "plt.plot(range(window-1, n_eps), ac_smooth, label='Actor-Critic', alpha=0.8)\n",
    "plt.plot(range(window-1, n_eps), ppo_smooth, label='PPO', alpha=0.8, linewidth=2)\n",
    "\n",
    "plt.axhline(y=475, color='g', linestyle='--', label='Goal (475)')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward (smoothed)')\n",
    "plt.title('Policy Gradient Methods Comparison on CartPole')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Final Performance (last 100 episodes):\")\n",
    "print(f\"   REINFORCE:     {np.mean(reinforce_rewards[-100:]):6.1f}\")\n",
    "print(f\"   Actor-Critic:  {np.mean(ac_rewards[-100:]):6.1f}\")\n",
    "print(f\"   PPO:           {np.mean(ppo_rewards[-100:]):6.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: PPO on LunarLander (Harder Environment)\n",
    "\n",
    "Let's test PPO on a more challenging environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PPO on LunarLander\n",
    "ppo_lunar = PPOAgent(\n",
    "    state_dim=env_lunar.observation_space.shape[0],\n",
    "    action_dim=env_lunar.action_space.n,\n",
    "    lr=3e-4,\n",
    "    gamma=0.99,\n",
    "    clip_epsilon=0.2,\n",
    "    n_epochs=10\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Training PPO on LunarLander (this takes longer)...\\n\")\n",
    "lunar_rewards = train_ppo(env_lunar, ppo_lunar, n_episodes=1000, print_freq=100)\n",
    "\n",
    "print(f\"\\n‚úÖ Final average: {np.mean(lunar_rewards[-100:]):.1f}\")\n",
    "print(f\"   (Goal: >200 for 'solved')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LunarLander training\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(lunar_rewards, alpha=0.3)\n",
    "window = 50\n",
    "smoothed = np.convolve(lunar_rewards, np.ones(window)/window, mode='valid')\n",
    "plt.plot(range(window-1, len(lunar_rewards)), smoothed, label=f'{window}-ep avg')\n",
    "plt.axhline(y=200, color='g', linestyle='--', label='Solved (200)')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('PPO on LunarLander')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Histogram of final rewards\n",
    "plt.hist(lunar_rewards[-100:], bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(x=200, color='g', linestyle='--', label='Solved')\n",
    "plt.xlabel('Reward')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Reward Distribution (Last 100 Episodes)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Normalizing Advantages\n",
    "\n",
    "```python\n",
    "# ‚ùå Raw advantages can have huge variance\n",
    "loss = -(log_probs * advantages).mean()\n",
    "\n",
    "# ‚úÖ Normalize advantages\n",
    "advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "loss = -(log_probs * advantages).mean()\n",
    "```\n",
    "\n",
    "### Mistake 2: Wrong Probability Ratio\n",
    "\n",
    "```python\n",
    "# ‚ùå Subtracting probabilities (wrong!)\n",
    "ratio = new_probs - old_probs\n",
    "\n",
    "# ‚úÖ Ratio of probabilities (in log space for stability)\n",
    "ratio = (new_log_probs - old_log_probs).exp()\n",
    "```\n",
    "\n",
    "### Mistake 3: Forgetting to Detach Old Log Probs\n",
    "\n",
    "```python\n",
    "# ‚ùå Computing gradients through old policy\n",
    "old_log_probs = dist.log_prob(action)  # Still attached to graph!\n",
    "\n",
    "# ‚úÖ Detach when storing\n",
    "old_log_probs = dist.log_prob(action).detach()  # or .item()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ **Policy Gradient Theorem**: Directly optimize the policy\n",
    "- ‚úÖ **REINFORCE**: Simple but high variance\n",
    "- ‚úÖ **Actor-Critic**: Use value function as baseline\n",
    "- ‚úÖ **Advantage Function**: \"How much better than average?\"\n",
    "- ‚úÖ **PPO**: Stable training with clipped objective\n",
    "- ‚úÖ **GAE**: Balance bias-variance in advantage estimation\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Connection to RLHF\n",
    "\n",
    "PPO is the algorithm used in RLHF for LLMs. Here's how it maps:\n",
    "\n",
    "| PPO Concept | RLHF Application |\n",
    "|-------------|------------------|\n",
    "| Policy œÄ(a\\|s) | The LLM generating tokens |\n",
    "| State s | Prompt + tokens generated so far |\n",
    "| Action a | Next token to generate |\n",
    "| Reward | Reward model score for complete response |\n",
    "| Clipped objective | Prevents LLM from changing too much |\n",
    "| KL penalty | Extra constraint to stay close to original model |\n",
    "\n",
    "In the next lab, we'll see exactly how this works with the TRL library!\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [PPO Paper](https://arxiv.org/abs/1707.06347) - Original PPO algorithm\n",
    "- [GAE Paper](https://arxiv.org/abs/1506.02438) - Generalized Advantage Estimation\n",
    "- [Spinning Up PPO](https://spinningup.openai.com/en/latest/algorithms/ppo.html) - Great explanation\n",
    "- [The 37 Implementation Details of PPO](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/) - Deep dive\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close environments\n",
    "env_cartpole.close()\n",
    "env_lunar.close()\n",
    "\n",
    "# Clear GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Notebook complete! Ready for Lab D.5: RLHF for Language Models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
