{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab D.2: Tabular Q-Learning\n",
    "\n",
    "**Module:** D - Reinforcement Learning (Optional)\n",
    "**Time:** 1.5-2 hours\n",
    "**Difficulty:** â­â­â­â˜†â˜†\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand why model-free learning is powerful\n",
    "- [ ] Implement the Q-learning algorithm from scratch\n",
    "- [ ] Master exploration vs exploitation with Îµ-greedy\n",
    "- [ ] Solve the FrozenLake environment\n",
    "- [ ] Visualize Q-table evolution during training\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Prerequisites\n",
    "\n",
    "- Completed: Lab D.1 (Markov Decision Processes)\n",
    "- Understanding of: Value functions, Bellman equation\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ Real-World Context\n",
    "\n",
    "**The Problem with Value Iteration:**\n",
    "\n",
    "In Lab D.1, we needed to know the transition probabilities $P(s'|s,a)$ to solve the MDP. But in the real world:\n",
    "\n",
    "- ðŸŽ® **Games**: We don't know how the game physics work exactly\n",
    "- ðŸ¤– **Robotics**: We can't perfectly model friction, slip, sensor noise\n",
    "- ðŸ’¬ **LLMs**: We don't know how humans will respond to generated text\n",
    "\n",
    "**Q-learning's magic:** Learn the optimal policy just by interacting with the environmentâ€”no model needed!\n",
    "\n",
    "> This is called **model-free reinforcement learning**, and it's the foundation of how we train AI systems in complex environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ§’ ELI5: What is Q-Learning?\n",
    "\n",
    "> **Imagine you're learning to navigate a new city without a map.** ðŸ—ºï¸\n",
    ">\n",
    "> Every day, you try different routes to get from home to work:\n",
    "> - You take a left... and get stuck in traffic (bad!)\n",
    "> - You take a right... and find a shortcut (good!)\n",
    "> - Over time, you learn which turns work best\n",
    ">\n",
    "> You're building a mental \"Q-table\" in your head:\n",
    "> - \"At the big intersection, turn RIGHT â†’ usually good\"\n",
    "> - \"At the coffee shop, go STRAIGHT â†’ sometimes good\"\n",
    ">\n",
    "> **The Q-learning update rule is simple:**\n",
    "> \"If I tried something and it turned out better than expected, I should do it more often.\"\n",
    ">\n",
    "> **In AI terms:** Q-learning updates action values based on experience. When we take action $a$ in state $s$, we observe the reward and update our Q-value to be closer to the actual return we received."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 1: Setup and Environment\n\nWe'll use **Gymnasium** (the successor to OpenAI Gym), specifically the **FrozenLake** environment:\n\n```\nSFFF      S: Start\nFHFH      F: Frozen (safe)\nFFFH      H: Hole (game over!)\nHFFG      G: Goal\n```\n\nThe agent must navigate from S to G without falling in holes!\n\n### Understanding the Gymnasium API\n\nGymnasium provides a standard interface for RL environments. Here are the key components:\n\n| Component | Description |\n|-----------|-------------|\n| `gym.make(env_name)` | Creates an environment instance |\n| `env.observation_space` | Describes the state space (`.n` gives number of discrete states) |\n| `env.action_space` | Describes the action space (`.n` gives number of actions) |\n| `env.reset()` | Resets environment, returns `(initial_state, info_dict)` |\n| `env.step(action)` | Takes action, returns `(next_state, reward, terminated, truncated, info)` |\n| `env.action_space.sample()` | Returns a random valid action |\n\n**Key return values from `env.step()`:**\n- `next_state`: The state after taking the action\n- `reward`: Immediate reward received\n- `terminated`: True if episode ended naturally (goal/failure)\n- `truncated`: True if episode was cut short (time limit)\n- `info`: Additional diagnostic information (often unused)\n\n**Note on byte strings:** The environment map uses byte strings (e.g., `b'H'` for Hole). To compare, use `== b'H'` or decode with `.decode('utf-8')`.\n\n### Key NumPy Functions for Q-Learning\n\nWe'll use these NumPy functions throughout the lab:\n\n- `np.argmax(array)`: Returns the **index** of the maximum value. Essential for finding the best action!\n- `np.random.randint(low, high)`: Returns a random integer in range [low, high)\n- `np.convolve(array, kernel, mode)`: Used for smoothing data in visualizations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - run this first!\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Install gymnasium if needed\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except ImportError:\n",
    "    print(\"Installing gymnasium...\")\n",
    "    !pip install gymnasium -q\n",
    "    import gymnasium as gym\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"ðŸš€ Module D.2: Tabular Q-Learning\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Gymnasium version: {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the FrozenLake environment\n",
    "# is_slippery=False makes it deterministic (easier to learn!)\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "\n",
    "print(\"ðŸ§Š FrozenLake Environment\")\n",
    "print(f\"   States: {env.observation_space.n}\")\n",
    "print(f\"   Actions: {env.action_space.n}\")\n",
    "print(\"   Action mapping: 0=Left, 1=Down, 2=Right, 3=Up\")\n",
    "print()\n",
    "\n",
    "# Visualize the map\n",
    "print(\"   Map Layout:\")\n",
    "desc = env.unwrapped.desc\n",
    "for row in desc:\n",
    "    print(\"   \" + \"\".join([c.decode('utf-8') for c in row]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the environment\n",
    "\n",
    "def explore_environment(env, n_episodes: int = 3):\n",
    "    \"\"\"Run a few random episodes to understand the dynamics.\"\"\"\n",
    "    \n",
    "    action_names = ['Left', 'Down', 'Right', 'Up']\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        print(f\"\\nðŸŽ® Episode {episode + 1}:\")\n",
    "        print(f\"   Start state: {state}\")\n",
    "        \n",
    "        total_reward = 0\n",
    "        steps = []\n",
    "        \n",
    "        for step in range(10):  # Max 10 steps\n",
    "            action = env.action_space.sample()  # Random action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            steps.append(f\"{state}â†’{action_names[action]}â†’{next_state}\")\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                if reward == 1:\n",
    "                    print(f\"   ðŸŽ‰ Reached goal in {step + 1} steps!\")\n",
    "                else:\n",
    "                    print(f\"   ðŸ’€ Fell in hole at step {step + 1}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"   Path: {' | '.join(steps[:5])}{'...' if len(steps) > 5 else ''}\")\n",
    "        print(f\"   Total reward: {total_reward}\")\n",
    "\n",
    "explore_environment(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” Key Observations\n",
    "\n",
    "1. **Sparse reward**: Only +1 for reaching the goal, 0 otherwise\n",
    "2. **Terminal states**: Holes and goal both end the episode\n",
    "3. **Random policy**: Very unlikely to reach the goal by chance!\n",
    "\n",
    "We need a smarter approach: **Q-learning**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The Q-Learning Algorithm\n",
    "\n",
    "### The Update Rule\n",
    "\n",
    "After taking action $a$ in state $s$, observing reward $r$ and landing in state $s'$:\n",
    "\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]$$\n",
    "\n",
    "Let's break this down:\n",
    "\n",
    "| Term | Meaning |\n",
    "|------|--------|\n",
    "| $Q(s, a)$ | Current estimate of action value |\n",
    "| $\\alpha$ | Learning rate (how fast we update) |\n",
    "| $r + \\gamma \\max Q(s', a')$ | **TD Target**: what we actually got |\n",
    "| $r + \\gamma \\max Q(s', a') - Q(s, a)$ | **TD Error**: how wrong we were |\n",
    "\n",
    "> ðŸ§’ **ELI5**: \"I thought this action was worth X, but I actually got Y. Let me adjust my estimate a little bit towards Y.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    Tabular Q-Learning Agent.\n",
    "    \n",
    "    This agent learns by storing Q-values in a table and updating them\n",
    "    based on experience using the TD learning rule.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_states: int, n_actions: int,\n",
    "                 learning_rate: float = 0.1,\n",
    "                 discount_factor: float = 0.99,\n",
    "                 epsilon_start: float = 1.0,\n",
    "                 epsilon_min: float = 0.01,\n",
    "                 epsilon_decay: float = 0.995):\n",
    "        \"\"\"\n",
    "        Initialize the Q-learning agent.\n",
    "        \n",
    "        Args:\n",
    "            n_states: Number of states in the environment\n",
    "            n_actions: Number of possible actions\n",
    "            learning_rate: Î± - how fast we update Q-values\n",
    "            discount_factor: Î³ - how much we value future rewards\n",
    "            epsilon_start: Initial exploration rate\n",
    "            epsilon_min: Minimum exploration rate\n",
    "            epsilon_decay: How fast epsilon decreases\n",
    "        \"\"\"\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        \n",
    "        # Exploration parameters\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        # Initialize Q-table with zeros\n",
    "        self.Q = np.zeros((n_states, n_actions))\n",
    "        \n",
    "        # For tracking learning progress\n",
    "        self.td_errors = []\n",
    "        \n",
    "        print(f\"ðŸ¤– Q-Learning Agent Initialized\")\n",
    "        print(f\"   Q-table shape: {self.Q.shape}\")\n",
    "        print(f\"   Learning rate: {self.lr}\")\n",
    "        print(f\"   Discount factor: {self.gamma}\")\n",
    "        print(f\"   Initial epsilon: {self.epsilon}\")\n",
    "    \n",
    "    def select_action(self, state: int, training: bool = True) -> int:\n",
    "        \"\"\"\n",
    "        Select an action using Îµ-greedy policy.\n",
    "        \n",
    "        During training: explore with probability Îµ\n",
    "        During evaluation: always exploit (take best action)\n",
    "        \"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            # Explore: random action\n",
    "            return np.random.randint(0, self.n_actions)\n",
    "        else:\n",
    "            # Exploit: best action according to Q-table\n",
    "            return np.argmax(self.Q[state])\n",
    "    \n",
    "    def update(self, state: int, action: int, reward: float, \n",
    "               next_state: int, done: bool) -> float:\n",
    "        \"\"\"\n",
    "        Update Q-value using the Q-learning rule.\n",
    "        \n",
    "        Returns the TD error (for tracking).\n",
    "        \"\"\"\n",
    "        # Current Q-value\n",
    "        current_q = self.Q[state, action]\n",
    "        \n",
    "        # TD Target: what we actually observed\n",
    "        if done:\n",
    "            # No future rewards after terminal state\n",
    "            td_target = reward\n",
    "        else:\n",
    "            # Reward + discounted best future value\n",
    "            td_target = reward + self.gamma * np.max(self.Q[next_state])\n",
    "        \n",
    "        # TD Error: how wrong our prediction was\n",
    "        td_error = td_target - current_q\n",
    "        \n",
    "        # Update Q-value\n",
    "        self.Q[state, action] = current_q + self.lr * td_error\n",
    "        \n",
    "        # Track for analysis\n",
    "        self.td_errors.append(abs(td_error))\n",
    "        \n",
    "        return td_error\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Reduce exploration rate over time.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def get_policy(self) -> np.ndarray:\n",
    "        \"\"\"Extract the greedy policy from Q-table.\"\"\"\n",
    "        return np.argmax(self.Q, axis=1)\n",
    "\n",
    "# Create our agent!\n",
    "agent = QLearningAgent(\n",
    "    n_states=env.observation_space.n,\n",
    "    n_actions=env.action_space.n\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Îµ-Greedy: Exploration vs Exploitation\n",
    "\n",
    "> ðŸ§’ **ELI5**: Should you go to your favorite restaurant (exploit what you know) or try a new place (explore to learn more)?\n",
    "\n",
    "- **Too much exploration**: Never commit to good actions\n",
    "- **Too much exploitation**: Miss better options\n",
    "\n",
    "Îµ-greedy is a simple solution:\n",
    "- With probability Îµ: take a random action (explore)\n",
    "- With probability 1-Îµ: take the best action (exploit)\n",
    "- Start with high Îµ (lots of exploration), decay over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize epsilon decay\n",
    "\n",
    "def plot_epsilon_decay(epsilon_start=1.0, epsilon_min=0.01, epsilon_decay=0.995, n_episodes=1000):\n",
    "    \"\"\"Show how epsilon decreases over training.\"\"\"\n",
    "    epsilons = []\n",
    "    epsilon = epsilon_start\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        epsilons.append(epsilon)\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(epsilons)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Epsilon (Exploration Rate)')\n",
    "    plt.title('Îµ-Greedy Exploration Decay')\n",
    "    plt.axhline(y=epsilon_min, color='r', linestyle='--', label=f'Min Îµ = {epsilon_min}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Find when we hit different exploration levels\n",
    "    for threshold in [0.5, 0.1, 0.05]:\n",
    "        episode = next((i for i, e in enumerate(epsilons) if e < threshold), n_episodes)\n",
    "        print(f\"   Îµ < {threshold}: Episode {episode}\")\n",
    "\n",
    "plot_epsilon_decay()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(env, agent, n_episodes: int = 2000, \n",
    "                     max_steps_per_episode: int = 100,\n",
    "                     verbose: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Train the Q-learning agent.\n",
    "    \n",
    "    Returns training statistics.\n",
    "    \"\"\"\n",
    "    rewards_per_episode = []\n",
    "    steps_per_episode = []\n",
    "    success_rate_history = []\n",
    "    q_table_snapshots = []  # Save Q-table periodically\n",
    "    \n",
    "    recent_successes = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        for step in range(max_steps_per_episode):\n",
    "            # Select action\n",
    "            action = agent.select_action(state, training=True)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Update Q-table\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Decay exploration\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # Track statistics\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        steps_per_episode.append(steps)\n",
    "        recent_successes.append(total_reward > 0)\n",
    "        \n",
    "        # Calculate success rate over last 100 episodes\n",
    "        if len(recent_successes) > 100:\n",
    "            recent_successes.pop(0)\n",
    "        success_rate = sum(recent_successes) / len(recent_successes)\n",
    "        success_rate_history.append(success_rate)\n",
    "        \n",
    "        # Save Q-table snapshot\n",
    "        if episode % 200 == 0:\n",
    "            q_table_snapshots.append((episode, agent.Q.copy()))\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose and (episode + 1) % 500 == 0:\n",
    "            print(f\"Episode {episode + 1:4d} | \"\n",
    "                  f\"Success Rate: {success_rate:.2%} | \"\n",
    "                  f\"Îµ: {agent.epsilon:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'rewards': rewards_per_episode,\n",
    "        'steps': steps_per_episode,\n",
    "        'success_rate': success_rate_history,\n",
    "        'q_snapshots': q_table_snapshots\n",
    "    }\n",
    "\n",
    "# Reset agent and train!\n",
    "agent = QLearningAgent(\n",
    "    n_states=env.observation_space.n,\n",
    "    n_actions=env.action_space.n,\n",
    "    learning_rate=0.8,     # High LR works well for deterministic env\n",
    "    discount_factor=0.95,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_min=0.01,\n",
    "    epsilon_decay=0.995\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ‹ï¸ Training Q-Learning Agent...\\n\")\n",
    "stats = train_q_learning(env, agent, n_episodes=2000)\n",
    "\n",
    "print(\"\\nâœ… Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Success rate over time\n",
    "ax = axes[0]\n",
    "ax.plot(stats['success_rate'])\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Success Rate (last 100)')\n",
    "ax.set_title('Learning Progress')\n",
    "ax.axhline(y=0.7, color='g', linestyle='--', label='Target: 70%')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Episode length\n",
    "ax = axes[1]\n",
    "window = 50\n",
    "smoothed_steps = np.convolve(stats['steps'], np.ones(window)/window, mode='valid')\n",
    "ax.plot(smoothed_steps)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Steps (smoothed)')\n",
    "ax.set_title('Episode Length')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# TD Error\n",
    "ax = axes[2]\n",
    "td_errors = agent.td_errors\n",
    "window = 100\n",
    "if len(td_errors) > window:\n",
    "    smoothed_td = np.convolve(td_errors, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(smoothed_td)\n",
    "ax.set_xlabel('Update Step')\n",
    "ax.set_ylabel('|TD Error| (smoothed)')\n",
    "ax.set_title('TD Error Over Time')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” What Just Happened?\n",
    "\n",
    "1. **Success rate** improves over time as the agent learns the optimal path\n",
    "2. **Episode length** stabilizes as the agent finds efficient routes\n",
    "3. **TD Error** decreases as Q-values converge to their true values\n",
    "\n",
    "The agent is learning from experience, without ever being told the rules of the game!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Visualizing the Learned Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_q_table(Q: np.ndarray, title: str = \"Q-Table\"):\n",
    "    \"\"\"\n",
    "    Visualize the Q-table as a heatmap with policy arrows.\n",
    "    \"\"\"\n",
    "    action_symbols = ['â†', 'â†“', 'â†’', 'â†‘']  # Left, Down, Right, Up\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Heatmap of Q-values\n",
    "    ax = axes[0]\n",
    "    im = ax.imshow(Q, cmap='RdYlGn', aspect='auto')\n",
    "    ax.set_xlabel('Action (0=L, 1=D, 2=R, 3=U)')\n",
    "    ax.set_ylabel('State')\n",
    "    ax.set_title(f'{title} - Q-Values')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Grid view with best actions\n",
    "    ax = axes[1]\n",
    "    V = np.max(Q, axis=1).reshape(4, 4)\n",
    "    policy = np.argmax(Q, axis=1).reshape(4, 4)\n",
    "    \n",
    "    im = ax.imshow(V, cmap='RdYlGn')\n",
    "    \n",
    "    # Add arrows and values\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            state = i * 4 + j\n",
    "            # Check if it's a hole or goal\n",
    "            if desc[i][j] == b'H':\n",
    "                ax.text(j, i, 'ðŸ•³ï¸', ha='center', va='center', fontsize=16)\n",
    "            elif desc[i][j] == b'G':\n",
    "                ax.text(j, i, 'ðŸŽ¯', ha='center', va='center', fontsize=16)\n",
    "            else:\n",
    "                ax.text(j, i - 0.2, action_symbols[policy[i, j]], \n",
    "                       ha='center', va='center', fontsize=20, fontweight='bold')\n",
    "                ax.text(j, i + 0.25, f'{V[i, j]:.2f}', \n",
    "                       ha='center', va='center', fontsize=9)\n",
    "    \n",
    "    ax.set_title(f'{title} - Optimal Policy')\n",
    "    ax.set_xticks(range(4))\n",
    "    ax.set_yticks(range(4))\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.colorbar(im, ax=ax, label='State Value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show the learned Q-table\n",
    "visualize_q_table(agent.Q, \"Learned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Q-table evolution over training\n",
    "\n",
    "fig, axes = plt.subplots(1, len(stats['q_snapshots']), figsize=(20, 4))\n",
    "\n",
    "for ax, (episode, Q_snap) in zip(axes, stats['q_snapshots']):\n",
    "    V = np.max(Q_snap, axis=1).reshape(4, 4)\n",
    "    im = ax.imshow(V, cmap='RdYlGn', vmin=0, vmax=1)\n",
    "    ax.set_title(f'Episode {episode}')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.suptitle('Q-Table Evolution During Training', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Evaluating the Learned Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, agent, n_episodes: int = 100) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the agent's policy (no exploration).\n",
    "    \n",
    "    Returns:\n",
    "        success_rate: Fraction of episodes that reached the goal\n",
    "        avg_steps: Average steps in successful episodes\n",
    "    \"\"\"\n",
    "    successes = 0\n",
    "    total_steps_in_successes = 0\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        steps = 0\n",
    "        \n",
    "        for step in range(100):\n",
    "            action = agent.select_action(state, training=False)  # No exploration!\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                if reward > 0:\n",
    "                    successes += 1\n",
    "                    total_steps_in_successes += steps\n",
    "                break\n",
    "    \n",
    "    success_rate = successes / n_episodes\n",
    "    avg_steps = total_steps_in_successes / max(1, successes)\n",
    "    \n",
    "    return success_rate, avg_steps\n",
    "\n",
    "# Evaluate!\n",
    "success_rate, avg_steps = evaluate_agent(env, agent, n_episodes=1000)\n",
    "\n",
    "print(\"\\nðŸ“Š Evaluation Results (1000 episodes):\")\n",
    "print(f\"   Success rate: {success_rate:.1%}\")\n",
    "print(f\"   Average steps (successful): {avg_steps:.1f}\")\n",
    "\n",
    "if success_rate >= 0.7:\n",
    "    print(\"\\nðŸŽ‰ Achieved >70% success rate! Lab objective complete!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Try adjusting hyperparameters or training longer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch a successful episode\n",
    "\n",
    "def demo_episode(env, agent, verbose: bool = True) -> bool:\n",
    "    \"\"\"Run and display one episode.\"\"\"\n",
    "    action_names = ['Left', 'Down', 'Right', 'Up']\n",
    "    \n",
    "    state, _ = env.reset()\n",
    "    if verbose:\n",
    "        print(f\"\\nðŸŽ® Demo Episode\")\n",
    "        print(f\"   Start: State {state}\")\n",
    "    \n",
    "    path = [state]\n",
    "    actions = []\n",
    "    \n",
    "    for step in range(20):\n",
    "        action = agent.select_action(state, training=False)\n",
    "        actions.append(action_names[action])\n",
    "        \n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        path.append(next_state)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   Path: {' â†’ '.join(map(str, path))}\")\n",
    "        print(f\"   Actions: {' â†’ '.join(actions)}\")\n",
    "        if reward > 0:\n",
    "            print(f\"   ðŸŽ‰ Reached goal in {len(actions)} steps!\")\n",
    "        else:\n",
    "            print(f\"   ðŸ’€ Failed!\")\n",
    "    \n",
    "    return reward > 0\n",
    "\n",
    "# Run a few demos\n",
    "for i in range(3):\n",
    "    demo_episode(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: The Slippery Version!\n",
    "\n",
    "Real FrozenLake has `is_slippery=True`â€”there's a 33% chance of slipping to an adjacent direction!\n",
    "\n",
    "This makes learning much harder because:\n",
    "1. The environment is stochastic\n",
    "2. Same action can lead to different outcomes\n",
    "3. Need more exploration to understand the dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create slippery environment\n",
    "slippery_env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True)\n",
    "\n",
    "# Train a new agent with adjusted hyperparameters\n",
    "slippery_agent = QLearningAgent(\n",
    "    n_states=slippery_env.observation_space.n,\n",
    "    n_actions=slippery_env.action_space.n,\n",
    "    learning_rate=0.1,      # Lower LR for noisy env\n",
    "    discount_factor=0.99,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_min=0.05,       # Keep some exploration\n",
    "    epsilon_decay=0.9995    # Slower decay\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ‹ï¸ Training on SLIPPERY FrozenLake...\\n\")\n",
    "slippery_stats = train_q_learning(slippery_env, slippery_agent, n_episodes=10000)\n",
    "\n",
    "# Evaluate\n",
    "success_rate, avg_steps = evaluate_agent(slippery_env, slippery_agent, n_episodes=1000)\n",
    "print(f\"\\nðŸ“Š Slippery Evaluation:\")\n",
    "print(f\"   Success rate: {success_rate:.1%}\")\n",
    "print(f\"   (Note: Random policy achieves ~1.5%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare deterministic vs slippery\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(stats['success_rate'], label='Deterministic', alpha=0.8)\n",
    "ax.plot(slippery_stats['success_rate'], label='Slippery', alpha=0.8)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Success Rate')\n",
    "ax.set_title('Learning Curves: Deterministic vs Slippery')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "visualize_q_table(slippery_agent.Q, \"Slippery\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight\n",
    "\n",
    "In slippery environments:\n",
    "- Learning is slower and noisier\n",
    "- Optimal success rate is lower (you can't always avoid holes)\n",
    "- The policy must account for uncertainty\n",
    "\n",
    "This is why RLHF for LLMs is challengingâ€”human preferences are noisy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš ï¸ Common Mistakes\n",
    "\n",
    "### Mistake 1: Too High Learning Rate\n",
    "\n",
    "```python\n",
    "# âŒ In stochastic environments, high LR causes oscillation\n",
    "lr = 0.9  # Too high for slippery env!\n",
    "\n",
    "# âœ… Use lower LR for noisy environments\n",
    "lr = 0.1  # More stable learning\n",
    "```\n",
    "\n",
    "### Mistake 2: Exploration Decay Too Fast\n",
    "\n",
    "```python\n",
    "# âŒ Stops exploring before finding good policies\n",
    "epsilon_decay = 0.9  # Reaches min in ~40 episodes!\n",
    "\n",
    "# âœ… Decay slowly enough to explore the space\n",
    "epsilon_decay = 0.995  # Takes ~1000 episodes to reach minimum\n",
    "```\n",
    "\n",
    "### Mistake 3: Forgetting Terminal States\n",
    "\n",
    "```python\n",
    "# âŒ Using max Q(s') when s' is terminal\n",
    "td_target = reward + gamma * np.max(Q[next_state])  # Wrong for done=True!\n",
    "\n",
    "# âœ… No future value for terminal states\n",
    "if done:\n",
    "    td_target = reward\n",
    "else:\n",
    "    td_target = reward + gamma * np.max(Q[next_state])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ‹ Try It Yourself: Hyperparameter Ablation\n",
    "\n",
    "How do different hyperparameters affect learning? Let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Compare different learning rates\n",
    "\n",
    "def run_ablation(env, learning_rates: list, n_episodes: int = 2000, n_trials: int = 3):\n",
    "    \"\"\"Run Q-learning with different learning rates and compare.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        print(f\"\\nðŸ“Š Testing lr = {lr}...\")\n",
    "        trial_success_rates = []\n",
    "        \n",
    "        for trial in range(n_trials):\n",
    "            agent = QLearningAgent(\n",
    "                n_states=env.observation_space.n,\n",
    "                n_actions=env.action_space.n,\n",
    "                learning_rate=lr,\n",
    "                discount_factor=0.99,\n",
    "                epsilon_start=1.0,\n",
    "                epsilon_min=0.01,\n",
    "                epsilon_decay=0.995\n",
    "            )\n",
    "            \n",
    "            # Train silently\n",
    "            _ = train_q_learning(env, agent, n_episodes=n_episodes, verbose=False)\n",
    "            \n",
    "            # Evaluate\n",
    "            success_rate, _ = evaluate_agent(env, agent, n_episodes=500)\n",
    "            trial_success_rates.append(success_rate)\n",
    "        \n",
    "        results[lr] = {\n",
    "            'mean': np.mean(trial_success_rates),\n",
    "            'std': np.std(trial_success_rates)\n",
    "        }\n",
    "        print(f\"   Success rate: {results[lr]['mean']:.1%} Â± {results[lr]['std']:.1%}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run ablation (this will take a few minutes)\n",
    "learning_rates = [0.01, 0.1, 0.5, 0.9]\n",
    "ablation_results = run_ablation(env, learning_rates, n_episodes=1500, n_trials=3)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 5))\n",
    "lrs = list(ablation_results.keys())\n",
    "means = [ablation_results[lr]['mean'] for lr in lrs]\n",
    "stds = [ablation_results[lr]['std'] for lr in lrs]\n",
    "\n",
    "plt.bar(range(len(lrs)), means, yerr=stds, capsize=5, alpha=0.7)\n",
    "plt.xticks(range(len(lrs)), [str(lr) for lr in lrs])\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.title('Q-Learning: Effect of Learning Rate')\n",
    "plt.axhline(y=0.7, color='g', linestyle='--', label='Target')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- âœ… **Model-free RL**: Learning without knowing transition probabilities\n",
    "- âœ… **Q-learning algorithm**: Update Q-values from experience\n",
    "- âœ… **TD Learning**: Bootstrap estimates from current values\n",
    "- âœ… **Îµ-Greedy**: Balance exploration and exploitation\n",
    "- âœ… **Hyperparameter tuning**: Learning rate, discount, exploration decay\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Challenge: Double Q-Learning\n",
    "\n",
    "Standard Q-learning tends to **overestimate** Q-values because it uses the same Q-table to both select and evaluate actions.\n",
    "\n",
    "**Double Q-learning** uses two Q-tables:\n",
    "1. Use Q1 to select the best action\n",
    "2. Use Q2 to evaluate that action (and vice versa)\n",
    "\n",
    "Can you implement it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Implement Double Q-Learning\n",
    "\n",
    "class DoubleQLearningAgent:\n",
    "    \"\"\"\n",
    "    Double Q-Learning to reduce overestimation bias.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_states: int, n_actions: int, lr: float = 0.1, gamma: float = 0.99):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Two Q-tables!\n",
    "        self.Q1 = np.zeros((n_states, n_actions))\n",
    "        self.Q2 = np.zeros((n_states, n_actions))\n",
    "        \n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "    \n",
    "    def select_action(self, state: int, training: bool = True) -> int:\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(0, self.n_actions)\n",
    "        # Use sum of both Q-tables for action selection\n",
    "        return np.argmax(self.Q1[state] + self.Q2[state])\n",
    "    \n",
    "    def update(self, state: int, action: int, reward: float, \n",
    "               next_state: int, done: bool):\n",
    "        \"\"\"\n",
    "        Double Q-learning update.\n",
    "        \n",
    "        Randomly update Q1 or Q2, using the other for evaluation.\n",
    "        \"\"\"\n",
    "        if np.random.random() < 0.5:\n",
    "            # Update Q1, use Q2 for evaluation\n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                # Select best action using Q1, but evaluate with Q2\n",
    "                best_action = np.argmax(self.Q1[next_state])\n",
    "                target = reward + self.gamma * self.Q2[next_state, best_action]\n",
    "            \n",
    "            self.Q1[state, action] += self.lr * (target - self.Q1[state, action])\n",
    "        else:\n",
    "            # Update Q2, use Q1 for evaluation\n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                best_action = np.argmax(self.Q2[next_state])\n",
    "                target = reward + self.gamma * self.Q1[next_state, best_action]\n",
    "            \n",
    "            self.Q2[state, action] += self.lr * (target - self.Q2[state, action])\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    @property\n",
    "    def Q(self):\n",
    "        \"\"\"Combined Q-table for evaluation.\"\"\"\n",
    "        return (self.Q1 + self.Q2) / 2\n",
    "\n",
    "# Test Double Q-Learning\n",
    "double_agent = DoubleQLearningAgent(\n",
    "    n_states=slippery_env.observation_space.n,\n",
    "    n_actions=slippery_env.action_space.n,\n",
    "    lr=0.1,\n",
    "    gamma=0.99\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ‹ï¸ Training Double Q-Learning on Slippery FrozenLake...\")\n",
    "\n",
    "# Custom training loop for DoubleQLearning\n",
    "for episode in range(10000):\n",
    "    state, _ = slippery_env.reset()\n",
    "    \n",
    "    for step in range(100):\n",
    "        action = double_agent.select_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = slippery_env.step(action)\n",
    "        done = terminated or truncated\n",
    "        double_agent.update(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    double_agent.decay_epsilon()\n",
    "\n",
    "# Evaluate\n",
    "successes = 0\n",
    "for _ in range(1000):\n",
    "    state, _ = slippery_env.reset()\n",
    "    for _ in range(100):\n",
    "        action = double_agent.select_action(state, training=False)\n",
    "        state, reward, terminated, truncated, _ = slippery_env.step(action)\n",
    "        if terminated or truncated:\n",
    "            successes += reward\n",
    "            break\n",
    "\n",
    "print(f\"\\nðŸ“Š Double Q-Learning Success Rate: {successes/10:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”— Connection to Deep Learning\n",
    "\n",
    "Tabular Q-learning works great for small state spaces (16 states in FrozenLake).\n",
    "\n",
    "But what about:\n",
    "- **Atari games**: 210 Ã— 160 pixels Ã— 128 colors = ~10^67 possible states!\n",
    "- **LLMs**: The state is the entire context window (potentially infinite!)\n",
    "\n",
    "**Solution: Use neural networks to approximate Q(s, a)**\n",
    "\n",
    "This is **Deep Q-Learning (DQN)**, which we'll cover in the next lab!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“– Further Reading\n",
    "\n",
    "- [Sutton & Barto, Chapter 6](http://incompleteideas.net/book/the-book-2nd.html) - Temporal-Difference Learning\n",
    "- [Q-learning Wikipedia](https://en.wikipedia.org/wiki/Q-learning)\n",
    "- [CleanRL Q-learning](https://github.com/vwxyzjn/cleanrl) - Production implementations\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close environments\n",
    "env.close()\n",
    "slippery_env.close()\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… Notebook complete! Ready for Lab D.3: Deep Q-Networks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}