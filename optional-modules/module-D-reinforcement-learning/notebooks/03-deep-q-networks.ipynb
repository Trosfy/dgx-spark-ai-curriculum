{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab D.3: Deep Q-Networks (DQN)\n",
    "\n",
    "**Module:** D - Reinforcement Learning (Optional)\n",
    "**Time:** 2-2.5 hours\n",
    "**Difficulty:** â­â­â­â­â˜†\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand why neural networks are needed for large state spaces\n",
    "- [ ] Implement a Q-network with PyTorch\n",
    "- [ ] Build an experience replay buffer\n",
    "- [ ] Implement target networks for training stability\n",
    "- [ ] Train DQN to solve CartPole (achieving >450 reward)\n",
    "- [ ] Ablate key components to understand their importance\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Prerequisites\n",
    "\n",
    "- Completed: Lab D.2 (Q-Learning)\n",
    "- Knowledge of: PyTorch basics (Module 2.1)\n",
    "- Understanding of: Neural networks, backpropagation\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ Real-World Context\n",
    "\n",
    "**The Problem with Tabular Q-Learning:**\n",
    "\n",
    "In Lab D.2, we stored Q-values in a table with one entry per (state, action) pair. This works for small, discrete state spaces.\n",
    "\n",
    "But consider:\n",
    "- **Atari**: 210Ã—160 pixels, 128 colors â†’ astronomical number of states\n",
    "- **Robotics**: Continuous sensor readings â†’ infinite state space\n",
    "- **LLMs**: Context = all previous tokens â†’ effectively infinite\n",
    "\n",
    "**The DQN breakthrough (2013):** Use a neural network to **approximate** $Q(s, a)$!\n",
    "\n",
    "DeepMind's DQN was the first to achieve human-level performance on Atari games, launching the deep reinforcement learning revolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ§’ ELI5: What is a Deep Q-Network?\n",
    "\n",
    "> **Imagine you have a Q-table so big you can't possibly fill it in.** ðŸ“Š\n",
    ">\n",
    "> Instead of memorizing every single combination (impossible!), you learn **patterns**:\n",
    "> - \"States that look like THIS tend to have high value\"\n",
    "> - \"When the enemy is close, dodging is usually good\"\n",
    ">\n",
    "> A neural network is a \"pattern recognizer\" that can generalize:\n",
    "> - It sees a few million examples\n",
    "> - It learns to predict Q-values for states it's never seen!\n",
    ">\n",
    "> **The magic:** Instead of $Q[s][a]$ = lookup in a table, we have $Q(s, a; \\theta)$ = neural network with parameters $\\theta$.\n",
    ">\n",
    "> **In AI terms:** DQN uses a neural network as a function approximator for the Q-function. The network takes a state as input and outputs Q-values for all possible actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup and CartPole Environment\n",
    "\n",
    "**CartPole**: Balance a pole on a cart by moving left or right.\n",
    "\n",
    "- **State**: 4 continuous values (cart position, cart velocity, pole angle, pole velocity)\n",
    "- **Actions**: 2 (push left, push right)\n",
    "- **Reward**: +1 for each timestep the pole is balanced\n",
    "- **Goal**: Score 500 (episode lasts 500 steps) or average >475 over 100 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - run this first!\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "from collections import deque\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Gymnasium\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except ImportError:\n",
    "    !pip install gymnasium -q\n",
    "    import gymnasium as gym\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device selection - prioritize GPU on DGX Spark\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"ðŸš€ Module D.3: Deep Q-Networks\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CartPole environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(\"ðŸŽ® CartPole-v1 Environment\")\n",
    "print(f\"   State space: {env.observation_space}\")\n",
    "print(f\"   State shape: {env.observation_space.shape}\")\n",
    "print(f\"   Action space: {env.action_space}\")\n",
    "print(f\"   Max episode steps: 500\")\n",
    "print()\n",
    "print(\"   State components:\")\n",
    "print(\"   [0] Cart Position   (-4.8 to 4.8)\")\n",
    "print(\"   [1] Cart Velocity   (-Inf to Inf)\")\n",
    "print(\"   [2] Pole Angle      (-0.42 rad to 0.42 rad)\")\n",
    "print(\"   [3] Pole Velocity   (-Inf to Inf)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what random play looks like\n",
    "\n",
    "def test_random_agent(env, n_episodes: int = 10):\n",
    "    \"\"\"See how random actions perform.\"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(500):\n",
    "            action = env.action_space.sample()  # Random action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "    \n",
    "    print(f\"Random Agent Performance ({n_episodes} episodes):\")\n",
    "    print(f\"   Mean reward: {np.mean(rewards):.1f}\")\n",
    "    print(f\"   Max reward:  {np.max(rewards):.1f}\")\n",
    "    print(f\"   (Goal: >475 average, ideally 500)\")\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "random_rewards = test_random_agent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Building the Q-Network\n",
    "\n",
    "Our neural network takes a **state** as input and outputs **Q-values for each action**:\n",
    "\n",
    "```\n",
    "State [4 dims] â†’ Hidden [64] â†’ Hidden [64] â†’ Q-values [2]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network for Q-value approximation.\n",
    "    \n",
    "    Takes state as input, outputs Q(s, a) for all actions.\n",
    "    \n",
    "    Architecture:\n",
    "        Input (state_dim) â†’ FC(64) â†’ ReLU â†’ FC(64) â†’ ReLU â†’ Output (action_dim)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        for layer in self.network:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.orthogonal_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            state: Shape (batch_size, state_dim) or (state_dim,)\n",
    "        \n",
    "        Returns:\n",
    "            Q-values for each action, shape (batch_size, action_dim)\n",
    "        \"\"\"\n",
    "        return self.network(state)\n",
    "\n",
    "# Create a test network\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "test_net = QNetwork(state_dim, action_dim).to(device)\n",
    "print(f\"ðŸ“ Q-Network Architecture:\")\n",
    "print(test_net)\n",
    "print(f\"\\nðŸ“Š Parameters: {sum(p.numel() for p in test_net.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the network\n",
    "state, _ = env.reset()\n",
    "state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)  # Add batch dim\n",
    "\n",
    "with torch.no_grad():\n",
    "    q_values = test_net(state_tensor)\n",
    "\n",
    "print(f\"Input state: {state}\")\n",
    "print(f\"Q-values: {q_values.cpu().numpy()[0]}\")\n",
    "print(f\"Best action: {q_values.argmax().item()} ({'Left' if q_values.argmax().item() == 0 else 'Right'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Experience Replay Buffer\n",
    "\n",
    "> ðŸ§’ **ELI5**: Imagine you're studying for an exam. Would you only review the last thing you learned? No! You'd go back and re-study earlier material too.\n",
    ">\n",
    "> Experience replay is like thatâ€”we store past experiences and randomly sample them for learning. This:\n",
    "> 1. **Breaks correlations**: Sequential experiences are highly correlated (bad for learning!)\n",
    "> 2. **Reuses data**: Each experience can be learned from multiple times\n",
    "> 3. **Smooths learning**: Don't forget old lessons when learning new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Experience Replay Buffer for DQN.\n",
    "    \n",
    "    Stores (state, action, reward, next_state, done) tuples.\n",
    "    Randomly samples batches for training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int = 100000):\n",
    "        \"\"\"\n",
    "        Initialize the replay buffer.\n",
    "        \n",
    "        Args:\n",
    "            capacity: Maximum number of experiences to store\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "    \n",
    "    def push(self, state: np.ndarray, action: int, reward: float,\n",
    "             next_state: np.ndarray, done: bool):\n",
    "        \"\"\"\n",
    "        Add an experience to the buffer.\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size: int) -> Tuple[torch.Tensor, ...]:\n",
    "        \"\"\"\n",
    "        Randomly sample a batch of experiences.\n",
    "        \n",
    "        Returns tensors ready for training.\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(np.array(states)).to(device),\n",
    "            torch.LongTensor(actions).to(device),\n",
    "            torch.FloatTensor(rewards).to(device),\n",
    "            torch.FloatTensor(np.array(next_states)).to(device),\n",
    "            torch.FloatTensor(dones).to(device)\n",
    "        )\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Test the buffer\n",
    "buffer = ReplayBuffer(capacity=10000)\n",
    "\n",
    "# Add some fake experiences\n",
    "for i in range(100):\n",
    "    fake_state = np.random.randn(4)\n",
    "    fake_action = random.randint(0, 1)\n",
    "    fake_reward = random.random()\n",
    "    fake_next_state = np.random.randn(4)\n",
    "    fake_done = random.random() < 0.1\n",
    "    buffer.push(fake_state, fake_action, fake_reward, fake_next_state, fake_done)\n",
    "\n",
    "print(f\"Buffer size: {len(buffer)}\")\n",
    "\n",
    "# Sample a batch\n",
    "states, actions, rewards, next_states, dones = buffer.sample(32)\n",
    "print(f\"Sampled batch shapes:\")\n",
    "print(f\"   States: {states.shape}\")\n",
    "print(f\"   Actions: {actions.shape}\")\n",
    "print(f\"   Rewards: {rewards.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: DQN Agent with Target Network\n",
    "\n",
    "### The Stability Problem\n",
    "\n",
    "In Q-learning, we update towards: $y = r + \\gamma \\max_{a'} Q(s', a')$\n",
    "\n",
    "But $Q$ is the network we're training! This creates a **moving target**.\n",
    "\n",
    "> ðŸ§’ **ELI5**: Imagine trying to hit a bullseye, but every time you throw a dart, someone moves the target. Hard to improve!\n",
    "\n",
    "**Solution: Target Network**\n",
    "- Keep a separate copy of the network for computing targets\n",
    "- Update the target network slowly (every N steps)\n",
    "- This gives a \"stable\" target to learn from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    Deep Q-Network Agent.\n",
    "    \n",
    "    Features:\n",
    "    - Neural network for Q-value approximation\n",
    "    - Experience replay buffer\n",
    "    - Target network for stable training\n",
    "    - Epsilon-greedy exploration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 state_dim: int,\n",
    "                 action_dim: int,\n",
    "                 hidden_dim: int = 64,\n",
    "                 lr: float = 1e-3,\n",
    "                 gamma: float = 0.99,\n",
    "                 buffer_size: int = 100000,\n",
    "                 batch_size: int = 64,\n",
    "                 target_update_freq: int = 100,\n",
    "                 epsilon_start: float = 1.0,\n",
    "                 epsilon_end: float = 0.01,\n",
    "                 epsilon_decay_steps: int = 10000):\n",
    "        \"\"\"\n",
    "        Initialize the DQN agent.\n",
    "        \"\"\"\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        \n",
    "        # Exploration parameters\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = (epsilon_start - epsilon_end) / epsilon_decay_steps\n",
    "        \n",
    "        # Q-Network (the one we train)\n",
    "        self.q_network = QNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "        \n",
    "        # Target Network (for stable targets)\n",
    "        self.target_network = QNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()  # Target never trains directly\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.buffer = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        # Tracking\n",
    "        self.train_steps = 0\n",
    "        self.losses = []\n",
    "        \n",
    "        print(f\"ðŸ¤– DQN Agent Initialized\")\n",
    "        print(f\"   Q-Network parameters: {sum(p.numel() for p in self.q_network.parameters()):,}\")\n",
    "        print(f\"   Target update frequency: every {target_update_freq} steps\")\n",
    "        print(f\"   Epsilon decay: {epsilon_start} â†’ {epsilon_end} over {epsilon_decay_steps} steps\")\n",
    "    \n",
    "    def select_action(self, state: np.ndarray, training: bool = True) -> int:\n",
    "        \"\"\"\n",
    "        Select an action using epsilon-greedy policy.\n",
    "        \"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_values = self.q_network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store an experience in the replay buffer.\"\"\"\n",
    "        self.buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    def train_step(self) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Perform one training step on a batch from the replay buffer.\n",
    "        \n",
    "        Returns the loss (or None if buffer too small).\n",
    "        \"\"\"\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample batch\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Current Q-values: Q(s, a) for the actions we took\n",
    "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Target Q-values: r + Î³ * max_a' Q_target(s', a')\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_network(next_states).max(dim=1)[0]\n",
    "            target_q = rewards + self.gamma * next_q * (1 - dones)\n",
    "        \n",
    "        # Loss: MSE between current and target Q-values\n",
    "        loss = F.mse_loss(current_q, target_q)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Track\n",
    "        self.train_steps += 1\n",
    "        self.losses.append(loss.item())\n",
    "        \n",
    "        # Update target network periodically\n",
    "        if self.train_steps % self.target_update_freq == 0:\n",
    "            self.update_target_network()\n",
    "        \n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon - self.epsilon_decay)\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy Q-network weights to target network.\"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "# Create our agent!\n",
    "agent = DQNAgent(\n",
    "    state_dim=env.observation_space.shape[0],\n",
    "    action_dim=env.action_space.n,\n",
    "    hidden_dim=64,\n",
    "    lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    buffer_size=50000,\n",
    "    batch_size=64,\n",
    "    target_update_freq=100,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay_steps=5000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Training DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(env, agent, n_episodes: int = 500, \n",
    "              max_steps: int = 500,\n",
    "              eval_freq: int = 50) -> Dict:\n",
    "    \"\"\"\n",
    "    Train the DQN agent.\n",
    "    \n",
    "    Returns training statistics.\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    eval_rewards = []\n",
    "    \n",
    "    best_eval_reward = 0\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Select and take action\n",
    "            action = agent.select_action(state, training=True)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Store experience\n",
    "            agent.store_experience(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Train\n",
    "            agent.train_step()\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(step + 1)\n",
    "        \n",
    "        # Periodic evaluation\n",
    "        if (episode + 1) % eval_freq == 0:\n",
    "            eval_reward = evaluate_agent(env, agent)\n",
    "            eval_rewards.append((episode, eval_reward))\n",
    "            \n",
    "            if eval_reward > best_eval_reward:\n",
    "                best_eval_reward = eval_reward\n",
    "            \n",
    "            # Print progress\n",
    "            recent_reward = np.mean(episode_rewards[-50:])\n",
    "            print(f\"Episode {episode + 1:4d} | \"\n",
    "                  f\"Recent Avg: {recent_reward:6.1f} | \"\n",
    "                  f\"Eval: {eval_reward:6.1f} | \"\n",
    "                  f\"Îµ: {agent.epsilon:.3f} | \"\n",
    "                  f\"Buffer: {len(agent.buffer):,}\")\n",
    "    \n",
    "    return {\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'episode_lengths': episode_lengths,\n",
    "        'eval_rewards': eval_rewards,\n",
    "        'losses': agent.losses,\n",
    "        'best_eval': best_eval_reward\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_agent(env, agent, n_episodes: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the agent's policy (no exploration).\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(500):\n",
    "            action = agent.select_action(state, training=False)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "    \n",
    "    return np.mean(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent!\n",
    "print(\"\\nðŸ‹ï¸ Training DQN on CartPole-v1...\\n\")\n",
    "\n",
    "stats = train_dqn(env, agent, n_episodes=400, eval_freq=50)\n",
    "\n",
    "print(f\"\\nâœ… Training Complete!\")\n",
    "print(f\"   Best evaluation reward: {stats['best_eval']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Episode rewards\n",
    "ax = axes[0, 0]\n",
    "ax.plot(stats['episode_rewards'], alpha=0.3, label='Raw')\n",
    "window = 50\n",
    "smoothed = np.convolve(stats['episode_rewards'], np.ones(window)/window, mode='valid')\n",
    "ax.plot(range(window-1, len(stats['episode_rewards'])), smoothed, label=f'{window}-episode avg')\n",
    "ax.axhline(y=475, color='g', linestyle='--', label='Target (475)')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('Training Rewards')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Evaluation rewards\n",
    "ax = axes[0, 1]\n",
    "if stats['eval_rewards']:\n",
    "    eval_eps, eval_rews = zip(*stats['eval_rewards'])\n",
    "    ax.plot(eval_eps, eval_rews, 'o-', markersize=8)\n",
    "    ax.axhline(y=450, color='g', linestyle='--', label='Goal (450)')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Evaluation Reward')\n",
    "ax.set_title('Evaluation Performance')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "ax = axes[1, 0]\n",
    "if stats['losses']:\n",
    "    window = 100\n",
    "    if len(stats['losses']) > window:\n",
    "        smoothed_loss = np.convolve(stats['losses'], np.ones(window)/window, mode='valid')\n",
    "        ax.plot(smoothed_loss)\n",
    "    else:\n",
    "        ax.plot(stats['losses'])\n",
    "ax.set_xlabel('Training Step')\n",
    "ax.set_ylabel('Loss (smoothed)')\n",
    "ax.set_title('Training Loss')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Episode lengths\n",
    "ax = axes[1, 1]\n",
    "ax.plot(stats['episode_lengths'], alpha=0.3)\n",
    "window = 50\n",
    "smoothed_lens = np.convolve(stats['episode_lengths'], np.ones(window)/window, mode='valid')\n",
    "ax.plot(range(window-1, len(stats['episode_lengths'])), smoothed_lens)\n",
    "ax.axhline(y=500, color='g', linestyle='--', label='Max (500)')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Episode Length')\n",
    "ax.set_title('Episode Length (longer = better balancing)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "final_reward = evaluate_agent(env, agent, n_episodes=100)\n",
    "print(f\"\\nðŸ“Š Final Evaluation (100 episodes):\")\n",
    "print(f\"   Mean Reward: {final_reward:.1f}\")\n",
    "\n",
    "if final_reward >= 450:\n",
    "    print(\"\\nðŸŽ‰ Achieved >450 average reward! Lab objective complete!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Try training longer or adjusting hyperparameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Ablation Study\n",
    "\n",
    "Let's see how important each component is by removing them one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgentNoReplay(DQNAgent):\n",
    "    \"\"\"\n",
    "    DQN without experience replay - trains on most recent experience only.\n",
    "    \"\"\"\n",
    "    \n",
    "    def train_step(self) -> Optional[float]:\n",
    "        if len(self.buffer) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Use only the most recent experience (no replay!)\n",
    "        state, action, reward, next_state, done = self.buffer.buffer[-1]\n",
    "        \n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action = torch.LongTensor([action]).to(device)\n",
    "        reward = torch.FloatTensor([reward]).to(device)\n",
    "        next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "        done = torch.FloatTensor([done]).to(device)\n",
    "        \n",
    "        # Standard DQN update (same as before)\n",
    "        current_q = self.q_network(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_network(next_state).max(dim=1)[0]\n",
    "            target_q = reward + self.gamma * next_q * (1 - done)\n",
    "        \n",
    "        loss = F.mse_loss(current_q, target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.train_steps += 1\n",
    "        self.losses.append(loss.item())\n",
    "        \n",
    "        if self.train_steps % self.target_update_freq == 0:\n",
    "            self.update_target_network()\n",
    "        \n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon - self.epsilon_decay)\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "class DQNAgentNoTarget(DQNAgent):\n",
    "    \"\"\"\n",
    "    DQN without target network - uses Q-network for targets.\n",
    "    \"\"\"\n",
    "    \n",
    "    def train_step(self) -> Optional[float]:\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "        \n",
    "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Use Q-network instead of target network!\n",
    "        with torch.no_grad():\n",
    "            next_q = self.q_network(next_states).max(dim=1)[0]  # Same network!\n",
    "            target_q = rewards + self.gamma * next_q * (1 - dones)\n",
    "        \n",
    "        loss = F.mse_loss(current_q, target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.train_steps += 1\n",
    "        self.losses.append(loss.item())\n",
    "        \n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon - self.epsilon_decay)\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ablation_study(env, n_episodes: int = 300, n_trials: int = 2):\n",
    "    \"\"\"\n",
    "    Compare full DQN vs ablated versions.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    configs = [\n",
    "        (\"Full DQN\", DQNAgent),\n",
    "        (\"No Replay\", DQNAgentNoReplay),\n",
    "        (\"No Target Net\", DQNAgentNoTarget),\n",
    "    ]\n",
    "    \n",
    "    for name, agent_class in configs:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing: {name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        trial_rewards = []\n",
    "        \n",
    "        for trial in range(n_trials):\n",
    "            print(f\"\\nTrial {trial + 1}/{n_trials}\")\n",
    "            \n",
    "            agent = agent_class(\n",
    "                state_dim=env.observation_space.shape[0],\n",
    "                action_dim=env.action_space.n,\n",
    "                hidden_dim=64,\n",
    "                lr=1e-3,\n",
    "                gamma=0.99,\n",
    "                epsilon_decay_steps=5000\n",
    "            )\n",
    "            \n",
    "            stats = train_dqn(env, agent, n_episodes=n_episodes, eval_freq=100)\n",
    "            \n",
    "            final_reward = evaluate_agent(env, agent, n_episodes=50)\n",
    "            trial_rewards.append(final_reward)\n",
    "            print(f\"   Trial {trial + 1} final reward: {final_reward:.1f}\")\n",
    "        \n",
    "        results[name] = {\n",
    "            'mean': np.mean(trial_rewards),\n",
    "            'std': np.std(trial_rewards),\n",
    "            'trials': trial_rewards\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run ablation (this takes a few minutes)\n",
    "print(\"\\nðŸ”¬ Running Ablation Study...\")\n",
    "ablation_results = run_ablation_study(env, n_episodes=200, n_trials=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ablation results\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "names = list(ablation_results.keys())\n",
    "means = [ablation_results[n]['mean'] for n in names]\n",
    "stds = [ablation_results[n]['std'] for n in names]\n",
    "\n",
    "bars = plt.bar(range(len(names)), means, yerr=stds, capsize=10, alpha=0.7,\n",
    "               color=['green', 'orange', 'red'])\n",
    "plt.xticks(range(len(names)), names, fontsize=12)\n",
    "plt.ylabel('Average Reward (100 episodes)', fontsize=12)\n",
    "plt.title('DQN Ablation Study: Importance of Each Component', fontsize=14)\n",
    "plt.axhline(y=450, color='green', linestyle='--', label='Goal (450)')\n",
    "plt.legend()\n",
    "\n",
    "# Add value labels\n",
    "for bar, mean, std in zip(bars, means, stds):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 10,\n",
    "             f'{mean:.0f}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Ablation Results:\")\n",
    "for name in names:\n",
    "    r = ablation_results[name]\n",
    "    print(f\"   {name:15s}: {r['mean']:6.1f} Â± {r['std']:5.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights from Ablation\n",
    "\n",
    "1. **Experience Replay** is crucial:\n",
    "   - Without it, learning is highly unstable\n",
    "   - Sequential experiences are correlated, breaking i.i.d. assumption\n",
    "\n",
    "2. **Target Network** stabilizes training:\n",
    "   - Without it, the \"moving target\" problem makes learning erratic\n",
    "   - Loss may look low but policy doesn't improve\n",
    "\n",
    "3. **Both components work together**:\n",
    "   - Replay provides diverse, decorrelated samples\n",
    "   - Target network provides stable learning signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš ï¸ Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Detaching Target\n",
    "\n",
    "```python\n",
    "# âŒ Gradients flow through target (unstable!)\n",
    "next_q = self.target_network(next_states).max(dim=1)[0]\n",
    "target_q = rewards + self.gamma * next_q * (1 - dones)\n",
    "loss.backward()  # Target affects Q-network gradients!\n",
    "\n",
    "# âœ… Detach target from gradient graph\n",
    "with torch.no_grad():\n",
    "    next_q = self.target_network(next_states).max(dim=1)[0]\n",
    "    target_q = rewards + self.gamma * next_q * (1 - dones)\n",
    "```\n",
    "\n",
    "### Mistake 2: Gathering Wrong Dimension\n",
    "\n",
    "```python\n",
    "# âŒ Wrong: actions shape mismatch\n",
    "q_values = self.q_network(states).gather(1, actions)  # Error!\n",
    "\n",
    "# âœ… Actions need to be 2D for gather\n",
    "q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "```\n",
    "\n",
    "### Mistake 3: Terminal State Value\n",
    "\n",
    "```python\n",
    "# âŒ Including future value for terminal states\n",
    "target_q = rewards + self.gamma * next_q  # Wrong for done=True!\n",
    "\n",
    "# âœ… Zero future value when done\n",
    "target_q = rewards + self.gamma * next_q * (1 - dones)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- âœ… **Function Approximation**: Using neural networks for large state spaces\n",
    "- âœ… **Q-Network**: Network that maps states to action values\n",
    "- âœ… **Experience Replay**: Breaking correlations by random sampling\n",
    "- âœ… **Target Networks**: Stable targets for training\n",
    "- âœ… **DQN Algorithm**: Combining all components for stable deep RL\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Challenge: Double DQN\n",
    "\n",
    "Standard DQN overestimates Q-values because it uses the same network to select AND evaluate actions.\n",
    "\n",
    "**Double DQN** decouples selection and evaluation:\n",
    "- Use Q-network to **select** the best action\n",
    "- Use target network to **evaluate** that action's value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Implement Double DQN\n",
    "\n",
    "class DoubleDQNAgent(DQNAgent):\n",
    "    \"\"\"\n",
    "    Double DQN to reduce overestimation bias.\n",
    "    \"\"\"\n",
    "    \n",
    "    def train_step(self) -> Optional[float]:\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Double DQN target\n",
    "        with torch.no_grad():\n",
    "            # Select best action using Q-network\n",
    "            best_actions = self.q_network(next_states).argmax(dim=1)\n",
    "            \n",
    "            # Evaluate that action using target network\n",
    "            next_q = self.target_network(next_states).gather(1, best_actions.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            target_q = rewards + self.gamma * next_q * (1 - dones)\n",
    "        \n",
    "        loss = F.mse_loss(current_q, target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.train_steps += 1\n",
    "        self.losses.append(loss.item())\n",
    "        \n",
    "        if self.train_steps % self.target_update_freq == 0:\n",
    "            self.update_target_network()\n",
    "        \n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon - self.epsilon_decay)\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "# Test Double DQN\n",
    "print(\"\\nðŸ‹ï¸ Training Double DQN...\")\n",
    "\n",
    "double_dqn_agent = DoubleDQNAgent(\n",
    "    state_dim=env.observation_space.shape[0],\n",
    "    action_dim=env.action_space.n,\n",
    ")\n",
    "\n",
    "double_stats = train_dqn(env, double_dqn_agent, n_episodes=300, eval_freq=100)\n",
    "\n",
    "final_reward = evaluate_agent(env, double_dqn_agent, n_episodes=100)\n",
    "print(f\"\\nðŸ“Š Double DQN Final Reward: {final_reward:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”— Connection to LLMs and RLHF\n",
    "\n",
    "DQN taught us key principles that apply to LLM training:\n",
    "\n",
    "| DQN Concept | RLHF Equivalent |\n",
    "|-------------|------------------|\n",
    "| Q-Network | The language model being fine-tuned |\n",
    "| Target Network | Reference model (KL constraint) |\n",
    "| Experience Replay | Training on batches of (prompt, response) pairs |\n",
    "| Epsilon-greedy | Temperature in sampling |\n",
    "\n",
    "However, LLMs use **policy gradient** methods (PPO) instead of value-based methods (DQN) because:\n",
    "1. Text generation has huge action space (vocabulary size)\n",
    "2. Policy methods directly optimize the generation distribution\n",
    "\n",
    "We'll learn PPO in the next lab!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“– Further Reading\n",
    "\n",
    "- [DQN Paper (2013)](https://arxiv.org/abs/1312.5602) - Original breakthrough\n",
    "- [Double DQN](https://arxiv.org/abs/1509.06461) - Reduces overestimation\n",
    "- [Dueling DQN](https://arxiv.org/abs/1511.06581) - Separate value and advantage\n",
    "- [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952) - Sample important transitions more\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "env.close()\n",
    "\n",
    "# Clear GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… Notebook complete! Ready for Lab D.4: Policy Gradients and PPO\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
