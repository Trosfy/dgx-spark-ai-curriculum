{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab D.1: Markov Decision Processes\n",
    "\n",
    "**Module:** D - Reinforcement Learning (Optional)\n",
    "**Time:** 1.5-2 hours\n",
    "**Difficulty:** ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand the components of a Markov Decision Process (MDP)\n",
    "- [ ] Implement state, action, reward, and transition functions\n",
    "- [ ] Calculate value functions using the Bellman equation\n",
    "- [ ] Find optimal policies using value iteration\n",
    "- [ ] Connect MDPs to real-world sequential decision problems\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Python programming fundamentals\n",
    "- Basic understanding of probability\n",
    "- NumPy array operations (Module 1.4)\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**Why MDPs matter for AI:**\n",
    "\n",
    "Every time ChatGPT generates a response, it makes a sequence of decisions‚Äîchoosing one word after another. Each word choice affects what comes next. This is exactly what MDPs model: **sequential decision-making where actions have consequences**.\n",
    "\n",
    "Real-world MDP applications:\n",
    "- ü§ñ **Robotics**: A robot deciding how to navigate a room\n",
    "- üéÆ **Games**: An AI learning to play chess or Atari\n",
    "- üí¨ **Chatbots**: RLHF training treats each token as a decision\n",
    "- üìà **Finance**: Portfolio optimization over time\n",
    "- üè• **Healthcare**: Treatment planning for patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What is a Markov Decision Process?\n",
    "\n",
    "> **Imagine you're a mouse in a maze looking for cheese.** üê≠üßÄ\n",
    ">\n",
    "> - **States (S)**: Where you are in the maze (each intersection is a \"state\")\n",
    "> - **Actions (A)**: Which direction you can go (up, down, left, right)\n",
    "> - **Rewards (R)**: You get +10 points for finding cheese, -1 for hitting a wall\n",
    "> - **Transitions (P)**: Sometimes the floor is slippery, so you might slip and go the wrong way!\n",
    "> - **Discount (Œ≥)**: Cheese now is better than cheese later (you're hungry!)\n",
    ">\n",
    "> The **Markov Property** is the key insight: \"Where you go next depends ONLY on where you are now, not on how you got there.\" It's like a GPS‚Äîit doesn't care about your journey so far, just your current location.\n",
    ">\n",
    "> **In AI terms:** An MDP is the mathematical framework for describing environments where an agent makes decisions, receives rewards, and tries to maximize total reward over time. It's the foundation of all reinforcement learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: MDP Components\n",
    "\n",
    "### The Five Elements of an MDP\n",
    "\n",
    "An MDP is defined by a tuple $(S, A, P, R, \\gamma)$:\n",
    "\n",
    "| Symbol | Name | Description |\n",
    "|--------|------|-------------|\n",
    "| $S$ | States | All possible situations the agent can be in |\n",
    "| $A$ | Actions | All possible moves the agent can make |\n",
    "| $P(s'\\|s, a)$ | Transition | Probability of reaching state $s'$ after taking action $a$ in state $s$ |\n",
    "| $R(s, a, s')$ | Reward | Immediate payoff for a transition |\n",
    "| $\\gamma$ | Discount | How much we value future rewards (0 to 1) |\n",
    "\n",
    "Let's implement each component!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - run this first!\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check we're running on DGX Spark\n",
    "print(\"üöÄ Module D.1: Markov Decision Processes\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldMDP:\n",
    "    \"\"\"\n",
    "    A simple 4x4 grid world MDP.\n",
    "    \n",
    "    The agent starts at top-left and must reach bottom-right (the goal).\n",
    "    Think of it as our mouse trying to find the cheese! üê≠\n",
    "    \n",
    "    Grid Layout:\n",
    "    +---+---+---+---+\n",
    "    | 0 | 1 | 2 | 3 |\n",
    "    +---+---+---+---+\n",
    "    | 4 | 5 | 6 | 7 |\n",
    "    +---+---+---+---+\n",
    "    | 8 | 9 |10 |11 |\n",
    "    +---+---+---+---+\n",
    "    |12 |13 |14 |15 | <- Goal!\n",
    "    +---+---+---+---+\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, grid_size: int = 4, slip_prob: float = 0.0):\n",
    "        \"\"\"\n",
    "        Initialize the grid world.\n",
    "        \n",
    "        Args:\n",
    "            grid_size: Size of the grid (default 4x4)\n",
    "            slip_prob: Probability of slipping to random adjacent cell (0.0 = deterministic)\n",
    "        \"\"\"\n",
    "        self.grid_size = grid_size\n",
    "        self.n_states = grid_size ** 2\n",
    "        self.n_actions = 4  # Up, Right, Down, Left\n",
    "        self.slip_prob = slip_prob\n",
    "        \n",
    "        # State indices\n",
    "        self.start_state = 0\n",
    "        self.goal_state = self.n_states - 1\n",
    "        \n",
    "        # Action mapping\n",
    "        self.action_names = ['Up', 'Right', 'Down', 'Left']\n",
    "        self.action_deltas = {\n",
    "            0: (-1, 0),  # Up\n",
    "            1: (0, 1),   # Right\n",
    "            2: (1, 0),   # Down\n",
    "            3: (0, -1)   # Left\n",
    "        }\n",
    "        \n",
    "        # Discount factor (how much we value future rewards)\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        print(f\"‚úÖ Created {grid_size}x{grid_size} Grid World MDP\")\n",
    "        print(f\"   States: {self.n_states}\")\n",
    "        print(f\"   Actions: {self.n_actions} ({', '.join(self.action_names)})\")\n",
    "        print(f\"   Start: State {self.start_state} | Goal: State {self.goal_state}\")\n",
    "        print(f\"   Slip probability: {slip_prob:.1%}\")\n",
    "    \n",
    "    def state_to_pos(self, state: int) -> Tuple[int, int]:\n",
    "        \"\"\"Convert state index to (row, col) position.\"\"\"\n",
    "        return (state // self.grid_size, state % self.grid_size)\n",
    "    \n",
    "    def pos_to_state(self, row: int, col: int) -> int:\n",
    "        \"\"\"Convert (row, col) position to state index.\"\"\"\n",
    "        return row * self.grid_size + col\n",
    "    \n",
    "    def get_next_state(self, state: int, action: int) -> int:\n",
    "        \"\"\"\n",
    "        Get the next state after taking an action (deterministic part).\n",
    "        \n",
    "        If the action would take us off the grid, we stay in place.\n",
    "        \"\"\"\n",
    "        row, col = self.state_to_pos(state)\n",
    "        dr, dc = self.action_deltas[action]\n",
    "        \n",
    "        # Apply action (stay in bounds)\n",
    "        new_row = max(0, min(self.grid_size - 1, row + dr))\n",
    "        new_col = max(0, min(self.grid_size - 1, col + dc))\n",
    "        \n",
    "        return self.pos_to_state(new_row, new_col)\n",
    "    \n",
    "    def get_reward(self, state: int, action: int, next_state: int) -> float:\n",
    "        \"\"\"\n",
    "        Get the reward for a transition.\n",
    "        \n",
    "        - +1.0 for reaching the goal\n",
    "        - -0.01 for each step (encourages efficiency)\n",
    "        \"\"\"\n",
    "        if next_state == self.goal_state:\n",
    "            return 1.0  # Found the cheese! üßÄ\n",
    "        else:\n",
    "            return -0.01  # Small cost for each step (time is valuable)\n",
    "    \n",
    "    def step(self, state: int, action: int) -> Tuple[int, float, bool]:\n",
    "        \"\"\"\n",
    "        Take an action in the environment.\n",
    "        \n",
    "        Returns:\n",
    "            next_state: The resulting state\n",
    "            reward: The reward received\n",
    "            done: Whether the episode is over (reached goal)\n",
    "        \"\"\"\n",
    "        # Handle slipping (stochastic transitions)\n",
    "        if self.slip_prob > 0 and np.random.random() < self.slip_prob:\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        \n",
    "        next_state = self.get_next_state(state, action)\n",
    "        reward = self.get_reward(state, action, next_state)\n",
    "        done = (next_state == self.goal_state)\n",
    "        \n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def get_transition_probs(self, state: int, action: int) -> Dict[int, float]:\n",
    "        \"\"\"\n",
    "        Get transition probabilities P(s'|s, a).\n",
    "        \n",
    "        Returns a dictionary mapping next_state -> probability\n",
    "        \"\"\"\n",
    "        if self.slip_prob == 0:\n",
    "            # Deterministic: only one possible next state\n",
    "            next_state = self.get_next_state(state, action)\n",
    "            return {next_state: 1.0}\n",
    "        else:\n",
    "            # Stochastic: could slip to any adjacent cell\n",
    "            probs = {}\n",
    "            for a in range(self.n_actions):\n",
    "                next_s = self.get_next_state(state, a)\n",
    "                if a == action:\n",
    "                    prob = 1.0 - self.slip_prob + self.slip_prob / self.n_actions\n",
    "                else:\n",
    "                    prob = self.slip_prob / self.n_actions\n",
    "                probs[next_s] = probs.get(next_s, 0) + prob\n",
    "            return probs\n",
    "    \n",
    "    def visualize_grid(self, values: Optional[np.ndarray] = None, \n",
    "                       policy: Optional[np.ndarray] = None,\n",
    "                       title: str = \"Grid World\"):\n",
    "        \"\"\"\n",
    "        Visualize the grid world, optionally with values or policy.\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        \n",
    "        # Draw grid\n",
    "        for i in range(self.grid_size + 1):\n",
    "            ax.axhline(y=i, color='black', linewidth=1)\n",
    "            ax.axvline(x=i, color='black', linewidth=1)\n",
    "        \n",
    "        # Color cells based on values\n",
    "        if values is not None:\n",
    "            values_2d = values.reshape(self.grid_size, self.grid_size)\n",
    "            im = ax.imshow(values_2d, cmap='RdYlGn', \n",
    "                          extent=[0, self.grid_size, self.grid_size, 0])\n",
    "            plt.colorbar(im, ax=ax, label='Value')\n",
    "        \n",
    "        # Draw policy arrows\n",
    "        arrow_map = {0: (0, 0.3), 1: (0.3, 0), 2: (0, -0.3), 3: (-0.3, 0)}\n",
    "        for s in range(self.n_states):\n",
    "            row, col = self.state_to_pos(s)\n",
    "            x, y = col + 0.5, row + 0.5\n",
    "            \n",
    "            if s == self.goal_state:\n",
    "                ax.text(x, y, 'üßÄ', fontsize=24, ha='center', va='center')\n",
    "            elif s == self.start_state:\n",
    "                ax.text(x, y, 'üê≠', fontsize=24, ha='center', va='center')\n",
    "            elif policy is not None:\n",
    "                dx, dy = arrow_map[policy[s]]\n",
    "                ax.arrow(x, y, dx, -dy, head_width=0.15, head_length=0.1, fc='blue', ec='blue')\n",
    "            \n",
    "            # Show state number\n",
    "            ax.text(x - 0.35, y - 0.35, str(s), fontsize=8, color='gray')\n",
    "        \n",
    "        ax.set_xlim(0, self.grid_size)\n",
    "        ax.set_ylim(self.grid_size, 0)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_title(title, fontsize=14)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create our grid world!\n",
    "mdp = GridWorldMDP(grid_size=4, slip_prob=0.0)\n",
    "mdp.visualize_grid(title=\"Our 4x4 Grid World (Mouse must find the cheese!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "We created a simple grid world where:\n",
    "- The **mouse** üê≠ starts at state 0 (top-left)\n",
    "- The **cheese** üßÄ is at state 15 (bottom-right)\n",
    "- The mouse can move in 4 directions\n",
    "- Each step costs -0.01 (encouraging the mouse to hurry!)\n",
    "- Finding cheese gives +1.0 reward\n",
    "\n",
    "This is a **deterministic** MDP‚Äîthe mouse always moves where it intends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the transition probabilities\n",
    "print(\"üìä Transition Probabilities from State 5:\\n\")\n",
    "\n",
    "state = 5\n",
    "for action in range(mdp.n_actions):\n",
    "    probs = mdp.get_transition_probs(state, action)\n",
    "    print(f\"  Action '{mdp.action_names[action]}':\")\n",
    "    for next_s, prob in probs.items():\n",
    "        row, col = mdp.state_to_pos(next_s)\n",
    "        print(f\"    ‚Üí State {next_s} (row {row}, col {col}) with probability {prob:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself: Simulate a Random Walk\n",
    "\n",
    "Let's see what happens when our mouse takes random actions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Complete this function to simulate an episode\n",
    "\n",
    "def run_random_episode(mdp: GridWorldMDP, max_steps: int = 100) -> Tuple[List[int], float]:\n",
    "    \"\"\"\n",
    "    Run one episode with random actions.\n",
    "    \n",
    "    Returns:\n",
    "        path: List of states visited\n",
    "        total_reward: Sum of all rewards\n",
    "    \"\"\"\n",
    "    state = mdp.start_state\n",
    "    path = [state]\n",
    "    total_reward = 0.0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # TODO: Choose a random action\n",
    "        action = np.random.randint(0, mdp.n_actions)  # Hint: use np.random.randint\n",
    "        \n",
    "        # TODO: Take the action using mdp.step()\n",
    "        next_state, reward, done = mdp.step(state, action)  # Hint: returns (next_state, reward, done)\n",
    "        \n",
    "        # Update tracking\n",
    "        path.append(next_state)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return path, total_reward\n",
    "\n",
    "# Run a few random episodes\n",
    "print(\"üé≤ Running 5 random episodes:\\n\")\n",
    "for i in range(5):\n",
    "    path, reward = run_random_episode(mdp)\n",
    "    print(f\"Episode {i+1}: {len(path)-1} steps, Reward: {reward:.3f}\")\n",
    "    if len(path) <= 10:\n",
    "        print(f\"   Path: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Value Functions\n",
    "\n",
    "### The Big Question: How Good is a State?\n",
    "\n",
    "The **Value Function** $V(s)$ tells us: \"If I'm in state $s$ and follow the best possible actions from here, how much total reward will I get?\"\n",
    "\n",
    "> üßí **ELI5**: Imagine you're playing a video game. Some positions are clearly better than others‚Äîyou're closer to the goal, or you have more options. The value function gives each position a \"score\" based on how good your future looks from there.\n",
    "\n",
    "### The Bellman Equation: The Magic Formula\n",
    "\n",
    "The value of a state depends on:\n",
    "1. The immediate reward you get\n",
    "2. The value of where you end up (discounted)\n",
    "\n",
    "$$V(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V(s') \\right]$$\n",
    "\n",
    "In plain English: **\"The value of being here = best action's (immediate reward + discounted future value)\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp: GridWorldMDP, threshold: float = 1e-6, max_iterations: int = 1000) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Solve the MDP using Value Iteration.\n",
    "    \n",
    "    This finds the optimal value function V*(s) and optimal policy œÄ*(s).\n",
    "    \n",
    "    Args:\n",
    "        mdp: The MDP to solve\n",
    "        threshold: Stop when values change less than this\n",
    "        max_iterations: Safety limit on iterations\n",
    "    \n",
    "    Returns:\n",
    "        V: Optimal value function\n",
    "        policy: Optimal policy (best action for each state)\n",
    "    \"\"\"\n",
    "    # Initialize values to zero\n",
    "    V = np.zeros(mdp.n_states)\n",
    "    \n",
    "    print(\"üîÑ Running Value Iteration...\")\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        V_new = np.zeros(mdp.n_states)\n",
    "        \n",
    "        for s in range(mdp.n_states):\n",
    "            # Goal state has value 0 (terminal)\n",
    "            if s == mdp.goal_state:\n",
    "                V_new[s] = 0\n",
    "                continue\n",
    "            \n",
    "            # Find best action using Bellman equation\n",
    "            action_values = []\n",
    "            for a in range(mdp.n_actions):\n",
    "                # Get transition probabilities\n",
    "                transitions = mdp.get_transition_probs(s, a)\n",
    "                \n",
    "                # Expected value = sum over all possible next states\n",
    "                expected_value = 0\n",
    "                for next_s, prob in transitions.items():\n",
    "                    reward = mdp.get_reward(s, a, next_s)\n",
    "                    expected_value += prob * (reward + mdp.gamma * V[next_s])\n",
    "                \n",
    "                action_values.append(expected_value)\n",
    "            \n",
    "            # Take the best action\n",
    "            V_new[s] = max(action_values)\n",
    "        \n",
    "        # Check for convergence\n",
    "        max_change = np.max(np.abs(V_new - V))\n",
    "        V = V_new\n",
    "        \n",
    "        if max_change < threshold:\n",
    "            print(f\"‚úÖ Converged after {iteration + 1} iterations!\")\n",
    "            break\n",
    "    \n",
    "    # Extract optimal policy from value function\n",
    "    policy = np.zeros(mdp.n_states, dtype=int)\n",
    "    for s in range(mdp.n_states):\n",
    "        if s == mdp.goal_state:\n",
    "            continue\n",
    "        \n",
    "        action_values = []\n",
    "        for a in range(mdp.n_actions):\n",
    "            transitions = mdp.get_transition_probs(s, a)\n",
    "            expected_value = sum(\n",
    "                prob * (mdp.get_reward(s, a, next_s) + mdp.gamma * V[next_s])\n",
    "                for next_s, prob in transitions.items()\n",
    "            )\n",
    "            action_values.append(expected_value)\n",
    "        \n",
    "        policy[s] = np.argmax(action_values)\n",
    "    \n",
    "    return V, policy\n",
    "\n",
    "# Solve our grid world!\n",
    "V, policy = value_iteration(mdp)\n",
    "\n",
    "print(\"\\nüìä Optimal Value Function:\")\n",
    "print(V.reshape(4, 4).round(3))\n",
    "\n",
    "print(\"\\nüß≠ Optimal Policy:\")\n",
    "policy_names = np.array([mdp.action_names[a] for a in policy]).reshape(4, 4)\n",
    "print(policy_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the solution\n",
    "mdp.visualize_grid(values=V, policy=policy, title=\"Optimal Value Function and Policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "1. **Value Iteration** computed $V^*(s)$ for every state\n",
    "2. States closer to the goal have higher values (greener)\n",
    "3. The **optimal policy** shows the best action at each state\n",
    "4. Notice: the policy creates a \"path\" straight to the goal!\n",
    "\n",
    "The mouse now knows exactly which way to go from any position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: The Discount Factor Œ≥\n",
    "\n",
    "> üßí **ELI5**: Would you rather have a cookie now or a cookie tomorrow? Most people prefer now! The discount factor $\\gamma$ captures this‚Äîit's how much we \"shrink\" future rewards.\n",
    "\n",
    "- $\\gamma = 0$: Only care about immediate reward (very short-sighted)\n",
    "- $\\gamma = 1$: Care equally about all future rewards (dangerous for infinite horizons!)\n",
    "- $\\gamma = 0.99$: Future matters, but immediate is still slightly better\n",
    "\n",
    "Let's see how $\\gamma$ affects our solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different discount factors\n",
    "gammas = [0.5, 0.9, 0.99, 0.999]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for ax, gamma in zip(axes, gammas):\n",
    "    # Create MDP with this gamma\n",
    "    test_mdp = GridWorldMDP(grid_size=4, slip_prob=0.0)\n",
    "    test_mdp.gamma = gamma\n",
    "    \n",
    "    # Solve it\n",
    "    V, policy = value_iteration(test_mdp)\n",
    "    \n",
    "    # Visualize\n",
    "    V_2d = V.reshape(4, 4)\n",
    "    im = ax.imshow(V_2d, cmap='RdYlGn')\n",
    "    ax.set_title(f'Œ≥ = {gamma}\\nV(0) = {V[0]:.4f}')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # Add text\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            ax.text(j, i, f'{V_2d[i,j]:.3f}', ha='center', va='center', fontsize=8)\n",
    "\n",
    "plt.suptitle('Effect of Discount Factor Œ≥ on Value Function', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "- With low $\\gamma$ (0.5), distant states barely see the goal's value\n",
    "- With high $\\gamma$ (0.999), value propagates all the way back\n",
    "- The starting state's value tells us: \"How good is it to start here?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Stochastic MDPs\n",
    "\n",
    "Real-world environments are noisy! Let's add some \"slipperiness\" to our floor:\n",
    "\n",
    "> When the mouse tries to move, there's a chance it slips and goes a random direction instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a slippery grid world (like FrozenLake!)\n",
    "slippery_mdp = GridWorldMDP(grid_size=4, slip_prob=0.3)\n",
    "\n",
    "print(\"\\nüìä Transition probabilities with 30% slip chance:\")\n",
    "print(\"\\nFrom State 5, trying to go Right:\")\n",
    "probs = slippery_mdp.get_transition_probs(5, 1)  # Action 1 = Right\n",
    "for next_s, prob in sorted(probs.items()):\n",
    "    print(f\"  ‚Üí State {next_s}: {prob:.1%} chance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the slippery grid world\n",
    "V_slippery, policy_slippery = value_iteration(slippery_mdp)\n",
    "\n",
    "# Compare deterministic vs stochastic\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Deterministic\n",
    "ax = axes[0]\n",
    "im = ax.imshow(V.reshape(4, 4), cmap='RdYlGn', vmin=0, vmax=1)\n",
    "ax.set_title('Deterministic (no slip)')\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        ax.text(j, i, f'{V[i*4+j]:.3f}', ha='center', va='center')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "# Stochastic\n",
    "ax = axes[1]\n",
    "im = ax.imshow(V_slippery.reshape(4, 4), cmap='RdYlGn', vmin=0, vmax=1)\n",
    "ax.set_title('Stochastic (30% slip)')\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        ax.text(j, i, f'{V_slippery[i*4+j]:.3f}', ha='center', va='center')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.colorbar(im, ax=axes)\n",
    "plt.suptitle('Value Functions: Deterministic vs Stochastic Worlds', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìâ Value of starting state:\")\n",
    "print(f\"   Deterministic: {V[0]:.4f}\")\n",
    "print(f\"   Stochastic:    {V_slippery[0]:.4f}\")\n",
    "print(f\"\\n   Uncertainty reduces value by {(1 - V_slippery[0]/V[0])*100:.1f}%!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight\n",
    "\n",
    "**Uncertainty is costly!** When the environment is unpredictable, the expected total reward decreases. This is why:\n",
    "\n",
    "- Robust policies try to avoid risky situations\n",
    "- In RLHF, we want the LLM to give consistent, reliable answers\n",
    "- Stochastic environments require more exploration to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Q-Values (Action Values)\n",
    "\n",
    "Sometimes we want to know: \"How good is it to take action $a$ in state $s$?\"\n",
    "\n",
    "This is the **Q-function** or **Action-Value function**:\n",
    "\n",
    "$$Q(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V(s')$$\n",
    "\n",
    "> üßí **ELI5**: If $V(s)$ tells you \"how good is this room?\", then $Q(s, a)$ tells you \"how good is taking this specific door from this room?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q_values(mdp: GridWorldMDP, V: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute Q(s, a) for all state-action pairs.\n",
    "    \n",
    "    Returns:\n",
    "        Q: Array of shape (n_states, n_actions)\n",
    "    \"\"\"\n",
    "    Q = np.zeros((mdp.n_states, mdp.n_actions))\n",
    "    \n",
    "    for s in range(mdp.n_states):\n",
    "        for a in range(mdp.n_actions):\n",
    "            transitions = mdp.get_transition_probs(s, a)\n",
    "            \n",
    "            Q[s, a] = sum(\n",
    "                prob * (mdp.get_reward(s, a, next_s) + mdp.gamma * V[next_s])\n",
    "                for next_s, prob in transitions.items()\n",
    "            )\n",
    "    \n",
    "    return Q\n",
    "\n",
    "# Compute Q-values for our deterministic MDP\n",
    "Q = compute_q_values(mdp, V)\n",
    "\n",
    "# Show Q-values for a specific state\n",
    "state = 5\n",
    "print(f\"üìä Q-values for State {state}:\")\n",
    "for a in range(mdp.n_actions):\n",
    "    print(f\"   Q({state}, {mdp.action_names[a]:>5}) = {Q[state, a]:.4f}\")\n",
    "\n",
    "print(f\"\\nüéØ Best action: {mdp.action_names[np.argmax(Q[state])]} (matches policy: {mdp.action_names[policy[state]]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Q-values as a heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "im = ax.imshow(Q, cmap='RdYlGn', aspect='auto')\n",
    "ax.set_xticks(range(mdp.n_actions))\n",
    "ax.set_xticklabels(mdp.action_names)\n",
    "ax.set_yticks(range(mdp.n_states))\n",
    "ax.set_yticklabels([f'State {s}' for s in range(mdp.n_states)])\n",
    "\n",
    "ax.set_xlabel('Action', fontsize=12)\n",
    "ax.set_ylabel('State', fontsize=12)\n",
    "ax.set_title('Q-Values: Q(s, a) for All State-Action Pairs', fontsize=14)\n",
    "\n",
    "plt.colorbar(im, label='Q-Value')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Q-values Matter\n",
    "\n",
    "1. **Policy Extraction**: $\\pi^*(s) = \\arg\\max_a Q(s, a)$\n",
    "2. **Learning**: We can learn $Q$ directly without knowing $P$ (Q-learning!)\n",
    "3. **RLHF**: The reward model essentially learns a Q-function over responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting the Discount Factor\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Not discounting future rewards\n",
    "V_new[s] = reward + V[next_state]  # Values will explode!\n",
    "\n",
    "# ‚úÖ Right: Apply discount factor\n",
    "V_new[s] = reward + gamma * V[next_state]\n",
    "```\n",
    "\n",
    "**Why:** Without discounting, infinite-horizon MDPs have infinite values.\n",
    "\n",
    "### Mistake 2: Updating with Old Values\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Using V in-place\n",
    "for s in range(n_states):\n",
    "    V[s] = max(...)  # Mixes old and new values!\n",
    "\n",
    "# ‚úÖ Right: Update to new array, then copy\n",
    "for s in range(n_states):\n",
    "    V_new[s] = max(...)\n",
    "V = V_new.copy()\n",
    "```\n",
    "\n",
    "**Why:** Bellman equation assumes all values are from same iteration.\n",
    "\n",
    "### Mistake 3: Wrong Transition Probabilities\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Probabilities don't sum to 1\n",
    "probs = {next_s: 0.5, other_s: 0.3}  # Sums to 0.8!\n",
    "\n",
    "# ‚úÖ Right: Ensure probabilities sum to 1\n",
    "probs = {next_s: 0.7, other_s: 0.3}  # Sums to 1.0\n",
    "```\n",
    "\n",
    "**Why:** Transition function must be a valid probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself: Larger Grid with Obstacles\n",
    "\n",
    "Create a 6x6 grid with obstacles (walls the mouse cannot pass through)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Implement a grid with obstacles\n",
    "\n",
    "class ObstacleGridMDP(GridWorldMDP):\n",
    "    \"\"\"\n",
    "    Grid world with obstacles (impassable cells).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, grid_size: int = 6, obstacles: List[int] = None):\n",
    "        super().__init__(grid_size, slip_prob=0.0)\n",
    "        \n",
    "        # TODO: Set obstacles (cells the agent cannot enter)\n",
    "        # Default obstacles create a wall-like pattern\n",
    "        if obstacles is None:\n",
    "            self.obstacles = {8, 9, 14, 15, 20, 26, 27}  # Example pattern\n",
    "        else:\n",
    "            self.obstacles = set(obstacles)\n",
    "        \n",
    "        print(f\"   Obstacles at: {sorted(self.obstacles)}\")\n",
    "    \n",
    "    def get_next_state(self, state: int, action: int) -> int:\n",
    "        \"\"\"Modified to handle obstacles.\"\"\"\n",
    "        # Get the intended next state\n",
    "        next_state = super().get_next_state(state, action)\n",
    "        \n",
    "        # TODO: If next_state is an obstacle, stay in current state\n",
    "        if next_state in self.obstacles:\n",
    "            return state  # Can't move into obstacle\n",
    "        \n",
    "        return next_state\n",
    "\n",
    "# Test it!\n",
    "obstacle_mdp = ObstacleGridMDP(grid_size=6)\n",
    "V_obs, policy_obs = value_iteration(obstacle_mdp)\n",
    "\n",
    "print(\"\\nüìä Value Function with Obstacles:\")\n",
    "print(V_obs.reshape(6, 6).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "The key is in `get_next_state()`: check if the intended next state is an obstacle. If so, return the current state (agent \"bounces\" off the wall).\n",
    "\n",
    "```python\n",
    "if next_state in self.obstacles:\n",
    "    return state  # Stay in place\n",
    "return next_state\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ **MDP Components**: States, Actions, Rewards, Transitions, Discount\n",
    "- ‚úÖ **The Markov Property**: Future depends only on present\n",
    "- ‚úÖ **Value Functions**: $V(s)$ tells us how good a state is\n",
    "- ‚úÖ **Bellman Equation**: The recursive relationship between values\n",
    "- ‚úÖ **Value Iteration**: Algorithm to find optimal policy\n",
    "- ‚úÖ **Q-values**: Action-value function for evaluating actions\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge: Policy Iteration\n",
    "\n",
    "Value iteration updates values until convergence. **Policy Iteration** is an alternative:\n",
    "\n",
    "1. Start with a random policy\n",
    "2. **Policy Evaluation**: Compute $V^\\pi$ for current policy\n",
    "3. **Policy Improvement**: Update policy greedily with respect to $V^\\pi$\n",
    "4. Repeat until policy doesn't change\n",
    "\n",
    "Can you implement it? Policy iteration often converges in fewer iterations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Implement Policy Iteration\n",
    "\n",
    "def policy_evaluation(mdp: GridWorldMDP, policy: np.ndarray, \n",
    "                      threshold: float = 1e-6) -> np.ndarray:\n",
    "    \"\"\"Evaluate a policy to get V^œÄ.\"\"\"\n",
    "    V = np.zeros(mdp.n_states)\n",
    "    \n",
    "    while True:\n",
    "        V_new = np.zeros(mdp.n_states)\n",
    "        for s in range(mdp.n_states):\n",
    "            if s == mdp.goal_state:\n",
    "                continue\n",
    "            \n",
    "            a = policy[s]  # Use policy's action (not max!)\n",
    "            transitions = mdp.get_transition_probs(s, a)\n",
    "            \n",
    "            V_new[s] = sum(\n",
    "                prob * (mdp.get_reward(s, a, ns) + mdp.gamma * V[ns])\n",
    "                for ns, prob in transitions.items()\n",
    "            )\n",
    "        \n",
    "        if np.max(np.abs(V_new - V)) < threshold:\n",
    "            break\n",
    "        V = V_new\n",
    "    \n",
    "    return V\n",
    "\n",
    "\n",
    "def policy_iteration(mdp: GridWorldMDP) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Find optimal policy using Policy Iteration.\"\"\"\n",
    "    # Start with random policy\n",
    "    policy = np.random.randint(0, mdp.n_actions, size=mdp.n_states)\n",
    "    \n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        \n",
    "        # Policy Evaluation\n",
    "        V = policy_evaluation(mdp, policy)\n",
    "        \n",
    "        # Policy Improvement\n",
    "        policy_stable = True\n",
    "        for s in range(mdp.n_states):\n",
    "            if s == mdp.goal_state:\n",
    "                continue\n",
    "            \n",
    "            old_action = policy[s]\n",
    "            \n",
    "            # Find best action\n",
    "            action_values = []\n",
    "            for a in range(mdp.n_actions):\n",
    "                transitions = mdp.get_transition_probs(s, a)\n",
    "                q = sum(\n",
    "                    prob * (mdp.get_reward(s, a, ns) + mdp.gamma * V[ns])\n",
    "                    for ns, prob in transitions.items()\n",
    "                )\n",
    "                action_values.append(q)\n",
    "            \n",
    "            policy[s] = np.argmax(action_values)\n",
    "            \n",
    "            if old_action != policy[s]:\n",
    "                policy_stable = False\n",
    "        \n",
    "        if policy_stable:\n",
    "            print(f\"‚úÖ Policy Iteration converged in {iteration} iterations!\")\n",
    "            break\n",
    "    \n",
    "    return V, policy\n",
    "\n",
    "# Test Policy Iteration\n",
    "V_pi, policy_pi = policy_iteration(mdp)\n",
    "\n",
    "print(\"\\nüìä Comparing methods:\")\n",
    "print(f\"   Value Iteration V(0):  {V[0]:.6f}\")\n",
    "print(f\"   Policy Iteration V(0): {V_pi[0]:.6f}\")\n",
    "print(f\"   Policies match: {np.array_equal(policy, policy_pi)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîó Connection to RLHF\n",
    "\n",
    "How does this connect to fine-tuning LLMs?\n",
    "\n",
    "| MDP Concept | RLHF Equivalent |\n",
    "|-------------|------------------|\n",
    "| State $s$ | Current context + tokens generated so far |\n",
    "| Action $a$ | Next token to generate |\n",
    "| Reward $R$ | Human preference or reward model score |\n",
    "| Policy $\\pi$ | The language model itself! |\n",
    "| Value $V(s)$ | Expected quality of response from this point |\n",
    "\n",
    "When we train ChatGPT with RLHF:\n",
    "1. The LLM is the **policy** (maps context ‚Üí token distribution)\n",
    "2. Each token generation is an **action**\n",
    "3. The reward model provides **rewards** for complete responses\n",
    "4. PPO optimizes the policy to maximize expected reward\n",
    "\n",
    "Understanding MDPs is the foundation for understanding how RLHF works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [Sutton & Barto, Chapter 3](http://incompleteideas.net/book/the-book-2nd.html) - Finite MDPs\n",
    "- [OpenAI Spinning Up: Key Concepts](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html)\n",
    "- [Bellman Equation Explained](https://en.wikipedia.org/wiki/Bellman_equation)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No GPU cleanup needed for this notebook (NumPy only)\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Notebook complete! Ready for Lab D.2: Q-Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Summary\n",
    "\n",
    "| Concept | Formula | Intuition |\n",
    "|---------|---------|------------|\n",
    "| Value Function | $V(s) = \\mathbb{E}[\\sum_t \\gamma^t R_t]$ | \"How good is this state?\" |\n",
    "| Q-Function | $Q(s,a) = R + \\gamma V(s')$ | \"How good is this action?\" |\n",
    "| Bellman Equation | $V(s) = \\max_a [R + \\gamma V(s')]$ | \"Value = best immediate + future\" |\n",
    "| Optimal Policy | $\\pi^*(s) = \\arg\\max_a Q(s,a)$ | \"Always take the best action\" |\n",
    "\n",
    "**Next:** In Lab D.2, we'll learn Q-learning‚Äîhow to find optimal policies **without knowing the transition probabilities**!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
