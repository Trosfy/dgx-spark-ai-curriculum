{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab D.5: RLHF - Reinforcement Learning for Language Models\n",
    "\n",
    "**Module:** D - Reinforcement Learning (Optional)\n",
    "**Time:** 2-2.5 hours\n",
    "**Difficulty:** â­â­â­â­â˜†\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand the complete RLHF pipeline\n",
    "- [ ] Learn how reward models work\n",
    "- [ ] Implement a simplified RLHF example with TRL\n",
    "- [ ] Understand KL divergence constraints\n",
    "- [ ] Compare RLHF to modern alternatives (DPO, ORPO)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Prerequisites\n",
    "\n",
    "- Completed: Labs D.1-D.4 (RL fundamentals through PPO)\n",
    "- Knowledge of: Transformers, language models\n",
    "- Understanding of: Policy gradients, PPO algorithm\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ Real-World Context\n",
    "\n",
    "**How ChatGPT Became Helpful**\n",
    "\n",
    "GPT-3 could continue text, but it wasn't \"helpful.\" Ask it a question and it might:\n",
    "- Continue with more questions\n",
    "- Give wrong information confidently\n",
    "- Be offensive or harmful\n",
    "\n",
    "**RLHF changed everything:**\n",
    "1. Train a **reward model** on human preferences\n",
    "2. Use **PPO** to fine-tune the LLM to maximize that reward\n",
    "3. Result: An LLM that tries to be helpful, harmless, and honest\n",
    "\n",
    "This is how ChatGPT, Claude, Gemini, and all modern assistants are trained!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ§’ ELI5: What is RLHF?\n",
    "\n",
    "> **Imagine you're training a dog to do tricks.** ðŸ•\n",
    ">\n",
    "> **Step 1: Teach the dog some tricks** (Supervised Fine-Tuning)\n",
    "> - You show the dog examples: \"Sit means put your butt down\"\n",
    "> - The dog learns to imitate what you showed\n",
    ">\n",
    "> **Step 2: Build a treat preference** (Reward Model)\n",
    "> - You show two versions of sitting: sloppy vs perfect\n",
    "> - You give treats for the better one\n",
    "> - Eventually the dog understands what \"good sitting\" means\n",
    ">\n",
    "> **Step 3: Let the dog practice** (PPO/RLHF)\n",
    "> - The dog tries sitting in different ways\n",
    "> - It gets treats based on how good the sit was\n",
    "> - It learns to sit perfectly to maximize treats!\n",
    ">\n",
    "> **The KL constraint:** \"Don't forget everything else while learning to sit!\"\n",
    "> - We penalize the dog if it becomes too different from its original behavior\n",
    "> - This keeps the dog well-rounded, not just a sitting machine\n",
    ">\n",
    "> **In AI terms:** RLHF fine-tunes an LLM using human feedback. We train a reward model on human preferences, then use PPO to optimize the LLM to generate responses that score highly on that reward model, while staying close to the original model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The RLHF Pipeline\n",
    "\n",
    "### Three Stages of RLHF\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Stage 1: SFT   â”‚ â”€â”€> â”‚ Stage 2: Reward â”‚ â”€â”€> â”‚  Stage 3: PPO   â”‚\n",
    "â”‚  Supervised     â”‚     â”‚     Model       â”‚     â”‚  Optimization   â”‚\n",
    "â”‚  Fine-Tuning    â”‚     â”‚   Training      â”‚     â”‚                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        â”‚                       â”‚                       â”‚\n",
    "        v                       v                       v\n",
    "   Base LLM learns         RM learns human         LLM maximizes\n",
    "   to follow format        preferences            reward score\n",
    "```\n",
    "\n",
    "Let's understand each stage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - run this first!\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Transformers\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "    from transformers import pipeline\n",
    "except ImportError:\n",
    "    print(\"Installing transformers...\")\n",
    "    !pip install transformers -q\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# TRL - Transformer Reinforcement Learning\n",
    "try:\n",
    "    import trl\n",
    "    from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "except ImportError:\n",
    "    print(\"Installing trl...\")\n",
    "    !pip install trl -q\n",
    "    import trl\n",
    "    from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"ðŸš€ Module D.5: RLHF for Language Models\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"TRL version: {trl.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Stage 1 - Supervised Fine-Tuning (SFT)\n",
    "\n",
    "Before RLHF, we fine-tune the base model on demonstration data:\n",
    "- Human-written examples of helpful responses\n",
    "- Question-answer pairs\n",
    "- Instruction-following examples\n",
    "\n",
    "This teaches the model the **format** of good responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: What SFT training data looks like\n",
    "\n",
    "sft_examples = [\n",
    "    {\n",
    "        \"prompt\": \"What is the capital of France?\",\n",
    "        \"response\": \"The capital of France is Paris. Paris is not only the capital but also the largest city in France, known for landmarks like the Eiffel Tower and the Louvre Museum.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Explain quantum computing in simple terms.\",\n",
    "        \"response\": \"Quantum computing uses quantum mechanics to process information differently than regular computers. While normal computers use bits (0 or 1), quantum computers use 'qubits' that can be both 0 and 1 at the same time. This allows them to solve certain problems much faster.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Write a haiku about programming.\",\n",
    "        \"response\": \"Code flows like water\\nBugs swim upstream, persistent\\nDebug brings the dawn\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"ðŸ“š SFT Training Examples:\\n\")\n",
    "for i, ex in enumerate(sft_examples):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Prompt: {ex['prompt']}\")\n",
    "    print(f\"  Response: {ex['response'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFT Training (Conceptual)\n",
    "\n",
    "```python\n",
    "# Pseudocode for SFT training\n",
    "for prompt, response in sft_dataset:\n",
    "    # Standard language modeling loss\n",
    "    input_ids = tokenizer(prompt + response)\n",
    "    \n",
    "    # Only compute loss on response tokens\n",
    "    logits = model(input_ids)\n",
    "    loss = cross_entropy(logits[len(prompt):], response_tokens)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "After SFT, the model can follow instructions but may not always give the \"best\" response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Stage 2 - Reward Model Training\n",
    "\n",
    "### The Preference Dataset\n",
    "\n",
    "Humans compare two responses and choose which is better:\n",
    "\n",
    "```\n",
    "Prompt: \"How do I make coffee?\"\n",
    "\n",
    "Response A: \"Boil water. Add coffee.\"\n",
    "Response B: \"Here's how to make great coffee:\n",
    "            1. Start with fresh, cold water\n",
    "            2. Use about 2 tablespoons of coffee per 6oz water\n",
    "            3. Heat water to 195-205Â°F (just below boiling)\n",
    "            4. Pour over grounds and steep for 4 minutes\n",
    "            5. Enjoy!\"\n",
    "\n",
    "Human choice: Response B is better âœ“\n",
    "```\n",
    "\n",
    "The reward model learns to predict these preferences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated preference data\n",
    "\n",
    "preference_data = [\n",
    "    {\n",
    "        \"prompt\": \"What's 2+2?\",\n",
    "        \"chosen\": \"2 + 2 = 4\",\n",
    "        \"rejected\": \"2+2 equals... um... let me think... 5? No wait, 4!\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Tell me a joke.\",\n",
    "        \"chosen\": \"Why don't scientists trust atoms? Because they make up everything!\",\n",
    "        \"rejected\": \"Jokes are subjective and I cannot determine what you find funny.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"How do I hack into my neighbor's WiFi?\",\n",
    "        \"chosen\": \"I can't help with that. Accessing someone else's WiFi without permission is illegal. If you need internet access, consider asking your neighbor to share their password or getting your own connection.\",\n",
    "        \"rejected\": \"Sure! First, download a packet sniffer like Wireshark, then...\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"ðŸ“Š Preference Data Examples:\\n\")\n",
    "for i, ex in enumerate(preference_data):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Prompt: {ex['prompt']}\")\n",
    "    print(f\"  âœ… Chosen: {ex['chosen'][:80]}...\")\n",
    "    print(f\"  âŒ Rejected: {ex['rejected'][:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRewardModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified reward model for demonstration.\n",
    "    \n",
    "    In practice, this would be a full transformer (like the SFT model)\n",
    "    with a scalar output head.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 10000, embed_dim: int = 256, hidden_dim: int = 512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Output: single scalar reward\n",
    "        self.reward_head = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute reward for a sequence.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Shape (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            rewards: Shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # Embed tokens\n",
    "        embeddings = self.embedding(input_ids)  # (batch, seq, embed)\n",
    "        \n",
    "        # Mean pool over sequence\n",
    "        pooled = embeddings.mean(dim=1)  # (batch, embed)\n",
    "        \n",
    "        # Encode\n",
    "        hidden = self.encoder(pooled)  # (batch, hidden)\n",
    "        \n",
    "        # Get scalar reward\n",
    "        reward = self.reward_head(hidden)  # (batch, 1)\n",
    "        \n",
    "        return reward\n",
    "\n",
    "# Create a simple reward model\n",
    "reward_model = SimpleRewardModel().to(device)\n",
    "\n",
    "print(f\"ðŸŽ¯ Reward Model\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in reward_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reward_model_step(reward_model, chosen_ids, rejected_ids, optimizer):\n",
    "    \"\"\"\n",
    "    One training step for the reward model.\n",
    "    \n",
    "    Uses the Bradley-Terry model:\n",
    "    P(chosen > rejected) = sigmoid(r_chosen - r_rejected)\n",
    "    \n",
    "    Loss = -log(sigmoid(r_chosen - r_rejected))\n",
    "    \"\"\"\n",
    "    # Get rewards for both responses\n",
    "    r_chosen = reward_model(chosen_ids)\n",
    "    r_rejected = reward_model(rejected_ids)\n",
    "    \n",
    "    # Bradley-Terry loss: chosen should score higher than rejected\n",
    "    loss = -F.logsigmoid(r_chosen - r_rejected).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Accuracy: did we correctly predict chosen > rejected?\n",
    "    accuracy = (r_chosen > r_rejected).float().mean()\n",
    "    \n",
    "    return loss.item(), accuracy.item()\n",
    "\n",
    "# Demo training step\n",
    "optimizer = torch.optim.Adam(reward_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Fake tokenized data\n",
    "chosen_ids = torch.randint(0, 10000, (8, 50)).to(device)\n",
    "rejected_ids = torch.randint(0, 10000, (8, 50)).to(device)\n",
    "\n",
    "print(\"Training reward model...\")\n",
    "for step in range(100):\n",
    "    loss, acc = train_reward_model_step(reward_model, chosen_ids, rejected_ids, optimizer)\n",
    "    if (step + 1) % 20 == 0:\n",
    "        print(f\"  Step {step + 1:3d} | Loss: {loss:.4f} | Accuracy: {acc:.2%}\")\n",
    "\n",
    "print(\"\\nâœ… Reward model learned to prefer 'chosen' responses!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Stage 3 - PPO Optimization\n",
    "\n",
    "Now we use PPO to fine-tune the LLM to maximize the reward model's score.\n",
    "\n",
    "### The RLHF Objective\n",
    "\n",
    "$$\\max_{\\pi} \\mathbb{E}_{x \\sim D, y \\sim \\pi(\\cdot|x)} \\left[ R(x, y) - \\beta \\cdot KL(\\pi || \\pi_{ref}) \\right]$$\n",
    "\n",
    "Where:\n",
    "- $R(x, y)$ = Reward model score\n",
    "- $KL(\\pi || \\pi_{ref})$ = How much the new policy differs from the original\n",
    "- $\\beta$ = KL penalty coefficient\n",
    "\n",
    "### Why the KL Penalty?\n",
    "\n",
    "> ðŸ§’ **ELI5**: If you reward a child only for saying \"I love you\", they might ONLY say that. The KL penalty is like saying \"Keep being yourself, just be a bit nicer.\" It prevents the model from \"mode collapsing\" into repetitive, reward-hacking behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize what KL penalty does\n",
    "\n",
    "def visualize_kl_penalty():\n",
    "    \"\"\"Show how KL penalty affects the training objective.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Without KL penalty\n",
    "    ax = axes[0]\n",
    "    rewards = np.linspace(-2, 2, 100)\n",
    "    ax.plot(rewards, rewards, 'b-', linewidth=2)\n",
    "    ax.set_xlabel('Reward Score')\n",
    "    ax.set_ylabel('Objective')\n",
    "    ax.set_title('Without KL Penalty\\n(Model might mode collapse)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # With KL penalty (different KL values)\n",
    "    ax = axes[1]\n",
    "    kl_values = [0, 0.5, 1.0, 2.0]\n",
    "    beta = 0.5\n",
    "    for kl in kl_values:\n",
    "        objective = rewards - beta * kl\n",
    "        ax.plot(rewards, objective, label=f'KL={kl}')\n",
    "    ax.set_xlabel('Reward Score')\n",
    "    ax.set_ylabel('Objective')\n",
    "    ax.set_title(f'With KL Penalty (Î²={beta})\\n(Penalizes divergence)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Trade-off visualization\n",
    "    ax = axes[2]\n",
    "    kl_range = np.linspace(0, 3, 100)\n",
    "    reward_gain = 1 - np.exp(-kl_range)  # Diminishing returns\n",
    "    kl_penalty = 0.3 * kl_range\n",
    "    net_objective = reward_gain - kl_penalty\n",
    "    \n",
    "    ax.plot(kl_range, reward_gain, 'g-', label='Reward Gain', linewidth=2)\n",
    "    ax.plot(kl_range, kl_penalty, 'r--', label='KL Penalty', linewidth=2)\n",
    "    ax.plot(kl_range, net_objective, 'b-', label='Net Objective', linewidth=2)\n",
    "    \n",
    "    optimal_kl = kl_range[np.argmax(net_objective)]\n",
    "    ax.axvline(x=optimal_kl, color='purple', linestyle=':', label=f'Optimal KL={optimal_kl:.1f}')\n",
    "    \n",
    "    ax.set_xlabel('KL Divergence from Reference')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title('Trade-off: Reward vs Divergence')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_kl_penalty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO for LLMs: The Algorithm\n",
    "\n",
    "```python\n",
    "# Simplified RLHF with PPO\n",
    "\n",
    "for batch in prompts:\n",
    "    # 1. Generate responses from current policy\n",
    "    responses = policy_model.generate(batch)\n",
    "    \n",
    "    # 2. Get rewards from reward model\n",
    "    rewards = reward_model(batch, responses)\n",
    "    \n",
    "    # 3. Compute KL penalty\n",
    "    policy_logprobs = policy_model.log_prob(responses)\n",
    "    ref_logprobs = reference_model.log_prob(responses)  # Frozen!\n",
    "    kl_penalty = beta * (policy_logprobs - ref_logprobs)\n",
    "    \n",
    "    # 4. Adjusted reward\n",
    "    adjusted_rewards = rewards - kl_penalty\n",
    "    \n",
    "    # 5. PPO update (as we learned in Lab D.4!)\n",
    "    ppo_trainer.step(batch, responses, adjusted_rewards)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Hands-On with TRL\n",
    "\n",
    "Let's use the **TRL** (Transformer Reinforcement Learning) library to see real RLHF code!\n",
    "\n",
    "We'll use a small model and a sentiment classifier as a simple \"reward model\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use a small GPT-2 model for demonstration\n",
    "# In practice, you'd use larger models\n",
    "\n",
    "model_name = \"lvwerra/gpt2-imdb\"  # GPT-2 fine-tuned on movie reviews\n",
    "\n",
    "print(\"Loading models (this may take a moment)...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with value head for PPO\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(model_name)\n",
    "model = model.to(device)\n",
    "\n",
    "# Reference model (frozen)\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(model_name)\n",
    "ref_model = ref_model.to(device)\n",
    "\n",
    "print(f\"\\nâœ… Loaded {model_name}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use a sentiment classifier as our \"reward model\"\n",
    "# The goal: make the model generate more positive text\n",
    "\n",
    "print(\"Loading sentiment classifier as reward model...\")\n",
    "\n",
    "sentiment_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"lvwerra/distilbert-imdb\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "def get_reward(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Get reward for generated text.\n",
    "    \n",
    "    Positive sentiment = positive reward\n",
    "    Negative sentiment = negative reward\n",
    "    \"\"\"\n",
    "    result = sentiment_pipe(text[:512])[0]  # Truncate for speed\n",
    "    \n",
    "    if result['label'] == 'POSITIVE':\n",
    "        return result['score']  # 0 to 1\n",
    "    else:\n",
    "        return -result['score']  # -1 to 0\n",
    "\n",
    "# Test it\n",
    "test_texts = [\n",
    "    \"This movie was absolutely wonderful!\",\n",
    "    \"I hated every minute of this terrible film.\",\n",
    "    \"It was okay, nothing special.\"\n",
    "]\n",
    "\n",
    "print(\"\\nðŸ“Š Testing reward function:\")\n",
    "for text in test_texts:\n",
    "    reward = get_reward(text)\n",
    "    print(f\"  '{text[:50]}...' â†’ Reward: {reward:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure PPO\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=1e-5,\n",
    "    batch_size=4,\n",
    "    mini_batch_size=2,\n",
    "    ppo_epochs=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    log_with=None,  # Disable wandb for now\n",
    ")\n",
    "\n",
    "# Create PPO trainer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"âœ… PPO Trainer configured\")\n",
    "print(f\"   Batch size: {ppo_config.batch_size}\")\n",
    "print(f\"   PPO epochs: {ppo_config.ppo_epochs}\")\n",
    "print(f\"   Learning rate: {ppo_config.learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training prompts (beginning of movie reviews)\n",
    "\n",
    "prompts = [\n",
    "    \"This movie\",\n",
    "    \"The acting was\",\n",
    "    \"I watched this film and\",\n",
    "    \"The director\",\n",
    "    \"This story\",\n",
    "    \"The ending\",\n",
    "    \"Overall, the movie\",\n",
    "    \"I would say that\",\n",
    "]\n",
    "\n",
    "# Generate some examples before training\n",
    "print(\"ðŸ“ Generations BEFORE RLHF:\\n\")\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": 30,\n",
    "    \"do_sample\": True,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"temperature\": 0.8,\n",
    "}\n",
    "\n",
    "for prompt in prompts[:4]:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.pretrained_model.generate(\n",
    "        **inputs,\n",
    "        **generation_kwargs,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    reward = get_reward(text)\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"Generated: '{text}'\")\n",
    "    print(f\"Reward: {reward:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a few PPO training steps\n",
    "# Note: This is a simplified demo; full training would take much longer\n",
    "\n",
    "print(\"ðŸ‹ï¸ Running PPO training steps...\\n\")\n",
    "\n",
    "n_steps = 10\n",
    "rewards_history = []\n",
    "\n",
    "for step in range(n_steps):\n",
    "    # Sample prompts\n",
    "    batch_prompts = np.random.choice(prompts, size=ppo_config.batch_size, replace=True).tolist()\n",
    "    \n",
    "    # Tokenize prompts\n",
    "    query_tensors = [tokenizer.encode(p, return_tensors=\"pt\").squeeze().to(device) \n",
    "                     for p in batch_prompts]\n",
    "    \n",
    "    # Generate responses\n",
    "    response_tensors = []\n",
    "    for query in query_tensors:\n",
    "        response = ppo_trainer.generate(query.unsqueeze(0), **generation_kwargs)\n",
    "        response_tensors.append(response.squeeze()[len(query):])\n",
    "    \n",
    "    # Decode and get rewards\n",
    "    texts = [tokenizer.decode(torch.cat([q, r])) \n",
    "             for q, r in zip(query_tensors, response_tensors)]\n",
    "    rewards = [torch.tensor([get_reward(text)]).to(device) for text in texts]\n",
    "    \n",
    "    # PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    \n",
    "    # Track\n",
    "    mean_reward = np.mean([r.item() for r in rewards])\n",
    "    rewards_history.append(mean_reward)\n",
    "    \n",
    "    if (step + 1) % 2 == 0:\n",
    "        print(f\"Step {step + 1:2d} | Mean Reward: {mean_reward:.3f} | \"\n",
    "              f\"KL: {stats['objective/kl']:.3f}\")\n",
    "\n",
    "print(\"\\nâœ… PPO training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate examples AFTER training\n",
    "print(\"\\nðŸ“ Generations AFTER RLHF:\\n\")\n",
    "\n",
    "for prompt in prompts[:4]:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.pretrained_model.generate(\n",
    "        **inputs,\n",
    "        **generation_kwargs,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    reward = get_reward(text)\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"Generated: '{text}'\")\n",
    "    print(f\"Reward: {reward:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(rewards_history, 'o-', markersize=8)\n",
    "plt.xlabel('PPO Step')\n",
    "plt.ylabel('Mean Reward')\n",
    "plt.title('RLHF Training: Reward Over Time')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Reward improvement: {rewards_history[0]:.3f} â†’ {rewards_history[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Modern Alternatives to RLHF\n",
    "\n",
    "RLHF works but has challenges:\n",
    "- Complex pipeline (3 stages!)\n",
    "- Unstable training (RL is hard)\n",
    "- Needs a separate reward model\n",
    "\n",
    "### DPO: Direct Preference Optimization\n",
    "\n",
    "**Key insight**: We can skip the reward model entirely!\n",
    "\n",
    "$$L_{DPO}(\\pi_\\theta; \\pi_{ref}) = -\\mathbb{E} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)} \\right) \\right]$$\n",
    "\n",
    "> ðŸ§’ **ELI5**: Instead of teaching a dog what's good (reward model) then training the dog (PPO), DPO directly shows the dog \"do this, not that\" and adjusts in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPO Loss implementation (conceptual)\n",
    "\n",
    "def dpo_loss(policy_model, ref_model, chosen_ids, rejected_ids, beta=0.1):\n",
    "    \"\"\"\n",
    "    Direct Preference Optimization loss.\n",
    "    \n",
    "    No reward model needed!\n",
    "    \"\"\"\n",
    "    # Get log probs from policy model\n",
    "    policy_chosen_logps = get_log_probs(policy_model, chosen_ids)\n",
    "    policy_rejected_logps = get_log_probs(policy_model, rejected_ids)\n",
    "    \n",
    "    # Get log probs from reference model (frozen)\n",
    "    with torch.no_grad():\n",
    "        ref_chosen_logps = get_log_probs(ref_model, chosen_ids)\n",
    "        ref_rejected_logps = get_log_probs(ref_model, rejected_ids)\n",
    "    \n",
    "    # Compute log ratios\n",
    "    chosen_ratio = beta * (policy_chosen_logps - ref_chosen_logps)\n",
    "    rejected_ratio = beta * (policy_rejected_logps - ref_rejected_logps)\n",
    "    \n",
    "    # DPO loss: maximize margin between chosen and rejected\n",
    "    loss = -F.logsigmoid(chosen_ratio - rejected_ratio).mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def get_log_probs(model, input_ids):\n",
    "    \"\"\"Compute log probabilities for a sequence.\"\"\"\n",
    "    # This is a placeholder - real implementation would compute\n",
    "    # log P(token_t | tokens_<t) for each token\n",
    "    return torch.randn(input_ids.size(0))  # Placeholder\n",
    "\n",
    "print(\"DPO advantages over RLHF:\")\n",
    "print(\"  âœ… No reward model needed\")\n",
    "print(\"  âœ… Single training stage\")\n",
    "print(\"  âœ… More stable (no RL!)\")\n",
    "print(\"  âœ… Often better results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of methods\n",
    "\n",
    "methods_comparison = \"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Method       â”‚ Description                                                 â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ RLHF (PPO)   â”‚ Train reward model â†’ Use PPO to maximize reward            â”‚\n",
    "â”‚              â”‚ âœ“ Proven at scale (ChatGPT)                                â”‚\n",
    "â”‚              â”‚ âœ— Complex pipeline, unstable                               â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ DPO          â”‚ Direct optimization on preferences                         â”‚\n",
    "â”‚              â”‚ âœ“ Simple, stable, no RL                                    â”‚\n",
    "â”‚              â”‚ âœ— May need more data than RLHF                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ ORPO         â”‚ Odds Ratio Preference Optimization                         â”‚\n",
    "â”‚              â”‚ âœ“ Even simpler than DPO                                    â”‚\n",
    "â”‚              â”‚ âœ“ No reference model needed                                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ KTO          â”‚ Kahneman-Tversky Optimization                              â”‚\n",
    "â”‚              â”‚ âœ“ Works with non-paired preferences                        â”‚\n",
    "â”‚              â”‚ âœ“ Based on prospect theory                                 â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ SimPO        â”‚ Simple Preference Optimization                             â”‚\n",
    "â”‚              â”‚ âœ“ Reference-free                                           â”‚\n",
    "â”‚              â”‚ âœ“ Length-normalized rewards                                â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\"\n",
    "\n",
    "print(methods_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš ï¸ Common Mistakes in RLHF\n",
    "\n",
    "### Mistake 1: Not Using KL Penalty\n",
    "\n",
    "```python\n",
    "# âŒ Model may \"reward hack\" - find exploits in the reward model\n",
    "reward = reward_model(response)\n",
    "\n",
    "# âœ… KL penalty keeps model close to original\n",
    "kl = compute_kl(policy_logprobs, ref_logprobs)\n",
    "adjusted_reward = reward - beta * kl\n",
    "```\n",
    "\n",
    "### Mistake 2: Not Freezing Reference Model\n",
    "\n",
    "```python\n",
    "# âŒ Reference model changes â†’ KL becomes meaningless\n",
    "ref_logprobs = ref_model(response)\n",
    "ref_logprobs.backward()  # Wrong!\n",
    "\n",
    "# âœ… Reference model is frozen\n",
    "with torch.no_grad():\n",
    "    ref_logprobs = ref_model(response)\n",
    "```\n",
    "\n",
    "### Mistake 3: Reward Hacking\n",
    "\n",
    "If the reward model has biases, the policy will exploit them!\n",
    "\n",
    "Example: If reward model prefers longer responses, the policy might pad with filler.\n",
    "\n",
    "**Solutions:**\n",
    "- Regularize reward model\n",
    "- Use multiple reward models\n",
    "- Human evaluation during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Module Complete!\n",
    "\n",
    "You've learned the complete RL foundation for understanding RLHF:\n",
    "\n",
    "| Lab | Concept | RLHF Connection |\n",
    "|-----|---------|------------------|\n",
    "| D.1 | MDPs | LLM generation as sequential decision-making |\n",
    "| D.2 | Q-Learning | Foundation for understanding value functions |\n",
    "| D.3 | DQN | Neural network function approximation |\n",
    "| D.4 | PPO | The actual algorithm used in RLHF |\n",
    "| D.5 | RLHF | Putting it all together for LLMs |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **RLHF = SFT + Reward Model + PPO**\n",
    "2. **The reward model learns human preferences**\n",
    "3. **PPO fine-tunes the LLM to maximize reward**\n",
    "4. **KL penalty prevents mode collapse**\n",
    "5. **DPO and alternatives often work better than PPO**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“– Further Reading\n",
    "\n",
    "- [InstructGPT Paper](https://arxiv.org/abs/2203.02155) - Original RLHF for LLMs\n",
    "- [TRL Documentation](https://huggingface.co/docs/trl/) - Practical RLHF library\n",
    "- [DPO Paper](https://arxiv.org/abs/2305.18290) - RL-free alternative\n",
    "- [Anthropic RLHF](https://arxiv.org/abs/2204.05862) - Constitutional AI\n",
    "- [ORPO Paper](https://arxiv.org/abs/2403.07691) - Odds Ratio method\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del model, ref_model, ppo_trainer\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… Module D: Reinforcement Learning - Complete!\")\n",
    "print(\"\\nðŸŽ¯ You're now ready to understand RLHF papers and implementations!\")\n",
    "print(\"\\nðŸ“š Next steps:\")\n",
    "print(\"   - Try DPO with the TRL library\")\n",
    "print(\"   - Fine-tune your own model with preferences\")\n",
    "print(\"   - Explore Module 3 for more LLM training techniques\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
