{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab A.2: Bias-Variance Decomposition\n",
    "\n",
    "**Module:** A - Statistical Learning Theory  \n",
    "**Time:** 1.5 hours  \n",
    "**Difficulty:** ⭐⭐⭐ (Intermediate-Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand the three sources of prediction error (bias, variance, noise)\n",
    "- [ ] Compute bias and variance empirically using bootstrap\n",
    "- [ ] Visualize the classic U-shaped curve of model complexity\n",
    "- [ ] Apply bias-variance thinking to model selection\n",
    "- [ ] Connect theory to practical underfitting/overfitting diagnosis\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Lab A.1 (VC Dimension)\n",
    "- Knowledge of: Linear regression, polynomial fitting\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "You're a data scientist at a financial firm predicting stock returns. Your model fits training data perfectly (low training error) but fails miserably on new data (high test error). Your manager asks: \"Is the model too simple or too complex?\"\n",
    "\n",
    "**Bias-variance decomposition** gives you the mathematical framework to answer this question precisely. It's the foundation for understanding:\n",
    "- Why adding more features can hurt performance\n",
    "- Why ensemble methods (Random Forests, Boosting) work\n",
    "- When to collect more data vs. simplify your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: Bias vs Variance\n",
    "\n",
    "> **Imagine you're playing darts at a dartboard...** \n",
    ">\n",
    "> **High Bias, Low Variance**: You throw 10 darts. They all land in a tight cluster... but the cluster is in the corner, far from the bullseye! You're consistent, but consistently wrong. This is like a model that's too simple - it keeps making the same mistake.\n",
    ">\n",
    "> **Low Bias, High Variance**: You throw 10 darts. On average, they center around the bullseye... but they're scattered all over the board! You're unbiased on average, but wildly inconsistent. This is like a model that's too complex - it changes drastically with small changes to training data.\n",
    ">\n",
    "> **The Goal**: Low bias AND low variance - darts tightly clustered around the bullseye!\n",
    ">\n",
    "> **The Trade-off**: Making the cluster tighter (less variance) often pushes it away from center (more bias), and vice versa. Finding the sweet spot is the art of machine learning!\n",
    ">\n",
    "> **In AI terms:**\n",
    "> - **Bias** = How far off is your model's average prediction from the truth?\n",
    "> - **Variance** = How much does your model's prediction change across different training sets?\n",
    "> - **Total Error = Bias² + Variance + Irreducible Noise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setting Up Our Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Core imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Tuple, List, Callable\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Scikit-learn for regression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score\n\n# ============================================================\n# Key sklearn imports explained:\n# ============================================================\n# make_pipeline: Chains preprocessing steps with a model\n#   Example: make_pipeline(PolynomialFeatures(2), LinearRegression())\n#   Creates: PolynomialFeatures -> LinearRegression (auto-connected)\n#\n# Ridge: Linear regression with L2 regularization (shrinks weights)\n#   alpha parameter controls regularization strength\n#   Higher alpha = more regularization = simpler model\n#\n# cross_val_score: Evaluates model using k-fold cross-validation\n#   Returns array of scores, one per fold\n# ============================================================\n\n# Set nice plotting defaults\nplt.style.use('default')\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['font.size'] = 12\n\n# Seed for reproducibility\nnp.random.seed(42)\n\nprint(\"Environment ready for Bias-Variance exploration!\")\nprint(f\"NumPy version: {np.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The Mathematical Decomposition\n",
    "\n",
    "### The Fundamental Equation\n",
    "\n",
    "For a prediction $\\hat{f}(x)$ and true function $f(x)$ with noise $\\epsilon$:\n",
    "\n",
    "$$\\mathbb{E}[(y - \\hat{f}(x))^2] = \\underbrace{(\\mathbb{E}[\\hat{f}(x)] - f(x))^2}_{\\text{Bias}^2} + \\underbrace{\\mathbb{E}[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)])^2]}_{\\text{Variance}} + \\underbrace{\\sigma^2}_{\\text{Irreducible Noise}}$$\n",
    "\n",
    "Let's break this down:\n",
    "- **Bias²**: Systematic error - how wrong is the model on average?\n",
    "- **Variance**: Random error - how much does the model change with different training data?\n",
    "- **Noise**: Inherent randomness we can never eliminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Let's create a ground truth function with known noise\n\ndef true_function(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    The \"true\" underlying function we're trying to learn.\n    In real life, we never know this!\n    \"\"\"\n    return np.sin(2 * x) + 0.5 * np.cos(4 * x)\n\n\ndef generate_data(n_samples: int, noise_std: float = 0.3, \n                  x_min: float = 0, x_max: float = 4,\n                  seed: int = None) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Generate noisy samples from the true function.\n    \n    Args:\n        n_samples: Number of samples to generate\n        noise_std: Standard deviation of Gaussian noise\n        x_min, x_max: Range of x values\n        seed: Random seed for reproducibility\n        \n    Returns:\n        X: Feature values shape (n_samples, 1)\n        y: Target values shape (n_samples,)\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    X = np.random.uniform(x_min, x_max, n_samples)\n    y = true_function(X) + np.random.normal(0, noise_std, n_samples)\n    \n    # reshape(-1, 1) converts 1D array to 2D column vector\n    # sklearn expects X to be 2D: (n_samples, n_features)\n    # -1 means \"infer this dimension\", 1 means \"1 column\"\n    return X.reshape(-1, 1), y\n\n\n# Generate and visualize data\nX_sample, y_sample = generate_data(100, noise_std=0.3, seed=42)\n\n# Plot true function and noisy data\nx_plot = np.linspace(0, 4, 200)\ny_true = true_function(x_plot)\n\nplt.figure(figsize=(12, 6))\nplt.scatter(X_sample, y_sample, alpha=0.6, s=50, label='Noisy observations')\nplt.plot(x_plot, y_true, 'r-', linewidth=3, label='True function f(x)')\n\n# plt.fill_between() fills the area between two y-values\n# Useful for showing confidence bands, error ranges, or uncertainty\n# Args: x-values, lower y-bound, upper y-bound, styling options\nplt.fill_between(x_plot, y_true - 0.3, y_true + 0.3, alpha=0.2, color='red', label='Noise band (±σ)')\n\nplt.xlabel('x', fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title('Our Learning Problem: Noisy Data from Unknown True Function', fontsize=14, fontweight='bold')\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Generated {len(X_sample)} samples with noise std = 0.3\")\nprint(f\"Irreducible error (variance of noise): {0.3**2:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Fitting Models of Different Complexity\n",
    "\n",
    "Let's fit polynomial models from degree 1 (simple line) to degree 15 (very wiggly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_polynomial(X: np.ndarray, y: np.ndarray, degree: int) -> object:\n",
    "    \"\"\"\n",
    "    Fit a polynomial regression model.\n",
    "    \n",
    "    Args:\n",
    "        X: Features shape (n_samples, 1)\n",
    "        y: Target shape (n_samples,)\n",
    "        degree: Polynomial degree\n",
    "        \n",
    "    Returns:\n",
    "        Fitted sklearn Pipeline\n",
    "    \"\"\"\n",
    "    model = make_pipeline(\n",
    "        PolynomialFeatures(degree, include_bias=False),\n",
    "        LinearRegression()\n",
    "    )\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Fit models of various degrees\n",
    "degrees = [1, 3, 5, 10, 15]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot true function in first subplot\n",
    "axes[0].scatter(X_sample, y_sample, alpha=0.4, s=30)\n",
    "axes[0].plot(x_plot, y_true, 'r-', linewidth=3)\n",
    "axes[0].set_title('True Function', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Fit and plot each model\n",
    "for ax, degree in zip(axes[1:], degrees):\n",
    "    model = fit_polynomial(X_sample, y_sample, degree)\n",
    "    y_pred = model.predict(x_plot.reshape(-1, 1))\n",
    "    \n",
    "    ax.scatter(X_sample, y_sample, alpha=0.4, s=30)\n",
    "    ax.plot(x_plot, y_true, 'r-', linewidth=2, alpha=0.5, label='True')\n",
    "    ax.plot(x_plot, y_pred, 'b-', linewidth=2, label=f'Degree {degree}')\n",
    "    \n",
    "    # Calculate training MSE\n",
    "    train_mse = np.mean((y_sample - model.predict(X_sample)) ** 2)\n",
    "    \n",
    "    ax.set_title(f'Degree {degree}\\nTrain MSE: {train_mse:.4f}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-2, 2.5)\n",
    "\n",
    "plt.suptitle('Polynomial Fits: From Underfitting to Overfitting', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "- **Degree 1 (line)**: Underfits - too simple to capture the wave pattern (HIGH BIAS)\n",
    "- **Degree 3-5**: Getting better - captures the main shape\n",
    "- **Degree 10-15**: Overfits - follows the noise too closely (HIGH VARIANCE)\n",
    "\n",
    "Notice how training MSE keeps decreasing, but that doesn't mean the model is better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Computing Bias and Variance via Bootstrap\n",
    "\n",
    "To measure bias and variance, we need to train the model on MANY different training sets. We simulate this using **bootstrap resampling**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def bootstrap_bias_variance(degree: int, \n                           n_samples: int = 100,\n                           noise_std: float = 0.3,\n                           n_bootstrap: int = 200,\n                           n_test_points: int = 50) -> dict:\n    \"\"\"\n    Compute bias and variance using bootstrap resampling.\n    \n    We generate many training sets, fit a model to each,\n    and see how predictions vary.\n    \n    Args:\n        degree: Polynomial degree\n        n_samples: Samples per training set\n        noise_std: Noise level\n        n_bootstrap: Number of bootstrap samples\n        n_test_points: Number of test points for evaluation\n        \n    Returns:\n        Dictionary with bias², variance, noise, and total error\n    \"\"\"\n    # Fixed test points\n    X_test = np.linspace(0.5, 3.5, n_test_points).reshape(-1, 1)\n    y_true_test = true_function(X_test.flatten())\n    \n    # np.zeros() creates an array filled with zeros\n    # Shape (n_bootstrap, n_test_points) stores predictions from each model\n    all_predictions = np.zeros((n_bootstrap, n_test_points))\n    \n    for i in range(n_bootstrap):\n        # Generate new training data (simulating different training sets)\n        X_train, y_train = generate_data(n_samples, noise_std, seed=i)\n        \n        # Fit model\n        model = fit_polynomial(X_train, y_train, degree)\n        \n        # Predict on test points\n        all_predictions[i] = model.predict(X_test)\n    \n    # Compute bias and variance\n    # np.mean(..., axis=0) computes mean across first axis (rows)\n    # Result: average prediction at each test point\n    mean_prediction = np.mean(all_predictions, axis=0)\n    \n    # Bias² = E[(E[f̂] - f)²] = mean over test points of (mean_pred - true)²\n    bias_squared = np.mean((mean_prediction - y_true_test) ** 2)\n    \n    # np.var(..., axis=0) computes variance across first axis\n    # Variance = E[Var(f̂)] = mean over test points of variance of predictions\n    variance = np.mean(np.var(all_predictions, axis=0))\n    \n    # Irreducible noise\n    noise = noise_std ** 2\n    \n    # Total expected error\n    total_error = bias_squared + variance + noise\n    \n    return {\n        'degree': degree,\n        'bias_squared': bias_squared,\n        'variance': variance,\n        'noise': noise,\n        'total_error': total_error,\n        'predictions': all_predictions,\n        'X_test': X_test,\n        'y_true_test': y_true_test,\n        'mean_prediction': mean_prediction\n    }\n\n\nprint(\"Bootstrap bias-variance function defined!\")\nprint(\"This may take a moment to run...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute for various polynomial degrees\n",
    "degrees_to_test = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 15]\n",
    "\n",
    "results = []\n",
    "for degree in degrees_to_test:\n",
    "    result = bootstrap_bias_variance(degree, n_bootstrap=200)\n",
    "    results.append(result)\n",
    "    print(f\"Degree {degree:2d}: Bias²={result['bias_squared']:.4f}, \"\n",
    "          f\"Var={result['variance']:.4f}, Total={result['total_error']:.4f}\")\n",
    "\n",
    "print(f\"\\nIrreducible noise (σ²): {results[0]['noise']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create the classic bias-variance tradeoff plot\ndegrees = [r['degree'] for r in results]\nbiases = [r['bias_squared'] for r in results]\nvariances = [r['variance'] for r in results]\ntotals = [r['total_error'] for r in results]\nnoise = results[0]['noise']\n\nplt.figure(figsize=(12, 7))\n\nplt.plot(degrees, biases, 'b-o', linewidth=2, markersize=8, label='Bias²')\nplt.plot(degrees, variances, 'r-o', linewidth=2, markersize=8, label='Variance')\nplt.plot(degrees, totals, 'g-o', linewidth=3, markersize=8, label='Total Error')\nplt.axhline(y=noise, color='gray', linestyle='--', linewidth=2, label=f'Irreducible Noise (σ²={noise:.2f})')\n\n# np.argmin() returns the INDEX of the minimum value in an array\n# Example: np.argmin([5, 2, 8, 1, 9]) returns 3 (index of value 1)\n# Useful for finding which model/hyperparameter gave the best result\noptimal_idx = np.argmin(totals)\noptimal_degree = degrees[optimal_idx]\noptimal_error = totals[optimal_idx]\n\nplt.scatter([optimal_degree], [optimal_error], s=300, color='gold', edgecolors='black', \n           linewidths=2, zorder=5, marker='*', label=f'Optimal (degree {optimal_degree})')\n\n# Add annotations\nplt.annotate('Underfitting\\n(High Bias)', xy=(2, 0.08), fontsize=11, \n            ha='center', color='blue', fontweight='bold')\nplt.annotate('Overfitting\\n(High Variance)', xy=(12, 0.15), fontsize=11, \n            ha='center', color='red', fontweight='bold')\n\nplt.xlabel('Model Complexity (Polynomial Degree)', fontsize=12)\nplt.ylabel('Error', fontsize=12)\nplt.title('The Bias-Variance Tradeoff\\nFinding the Sweet Spot', fontsize=14, fontweight='bold')\nplt.legend(loc='upper right', fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.xticks(degrees)\nplt.ylim(0, max(totals) * 1.1)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nOptimal model complexity: Degree {optimal_degree}\")\nprint(f\"Minimum total error: {optimal_error:.4f}\")\nprint(f\"Breakdown: Bias²={biases[optimal_idx]:.4f} + Var={variances[optimal_idx]:.4f} + Noise={noise:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The U-Shaped Curve\n",
    "\n",
    "This is one of the most important plots in all of machine learning!\n",
    "\n",
    "- **Left side (simple models)**: High bias, low variance → Underfitting\n",
    "- **Right side (complex models)**: Low bias, high variance → Overfitting  \n",
    "- **Bottom of U**: Optimal tradeoff!\n",
    "\n",
    "**The key insight:** You CANNOT eliminate both bias and variance. Reducing one typically increases the other. The art is finding the optimal balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Visualizing Prediction Spread\n",
    "\n",
    "Let's see what high bias and high variance actually look like in terms of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare low complexity (high bias) vs high complexity (high variance)\n",
    "result_low = bootstrap_bias_variance(degree=1, n_bootstrap=50)\n",
    "result_optimal = bootstrap_bias_variance(degree=optimal_degree, n_bootstrap=50)\n",
    "result_high = bootstrap_bias_variance(degree=15, n_bootstrap=50)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for ax, result, title in zip(axes, \n",
    "                             [result_low, result_optimal, result_high],\n",
    "                             ['High Bias (Degree 1)', f'Optimal (Degree {optimal_degree})', 'High Variance (Degree 15)']):\n",
    "    \n",
    "    # Plot all predictions (as thin lines)\n",
    "    for pred in result['predictions']:\n",
    "        ax.plot(result['X_test'], pred, 'b-', alpha=0.1, linewidth=1)\n",
    "    \n",
    "    # Plot mean prediction\n",
    "    ax.plot(result['X_test'], result['mean_prediction'], 'b-', linewidth=3, label='Mean prediction')\n",
    "    \n",
    "    # Plot true function\n",
    "    ax.plot(result['X_test'], result['y_true_test'], 'r-', linewidth=3, label='True function')\n",
    "    \n",
    "    ax.set_title(f'{title}\\nBias²={result[\"bias_squared\"]:.4f}, Var={result[\"variance\"]:.4f}', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-2, 3)\n",
    "\n",
    "plt.suptitle('Prediction Spread Across Different Training Sets', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Light blue lines = Individual model predictions\")\n",
    "print(\"Dark blue line = Average prediction across all models\")\n",
    "print(\"Red line = True function\")\n",
    "print(\"\\nNotice:\")\n",
    "print(\"  - Degree 1: Predictions are consistent but wrong (biased)\")\n",
    "print(f\"  - Degree {optimal_degree}: Good balance - close to true and consistent\")\n",
    "print(\"  - Degree 15: Average is close to true, but huge spread (variance)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: The Dartboard Visualization\n",
    "\n",
    "Let's make the ELI5 dartboard analogy concrete!"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Matplotlib Drawing Primitives\n\nThe dartboard visualization uses matplotlib's patch drawing capabilities:\n\n```python\n# plt.Circle() creates a circular patch object\ncircle = plt.Circle((x_center, y_center), radius, color='red')\n\n# ax.add_patch() adds the patch to the plot\nax.add_patch(circle)\n```\n\n**Other useful patches:** `plt.Rectangle()`, `plt.Polygon()`, `plt.Arrow()`\n\nThese are useful for creating custom visualizations beyond standard plots!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dartboard_plot():\n",
    "    \"\"\"\n",
    "    Create a dartboard-style visualization of bias vs variance.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "    \n",
    "    scenarios = [\n",
    "        ('High Bias, Low Variance', 2.0, 0.2),\n",
    "        ('Low Bias, High Variance', 0.3, 1.5),\n",
    "        ('High Bias, High Variance', 2.0, 1.5),\n",
    "        ('Low Bias, Low Variance (Goal!)', 0.2, 0.2),\n",
    "    ]\n",
    "    \n",
    "    for ax, (title, bias, variance) in zip(axes.flatten(), scenarios):\n",
    "        # Draw dartboard\n",
    "        for r, color in zip([3, 2, 1, 0.3], ['#eeeeee', '#cccccc', '#aaaaaa', '#ff4444']):\n",
    "            circle = plt.Circle((0, 0), r, color=color, zorder=0)\n",
    "            ax.add_patch(circle)\n",
    "        \n",
    "        # Generate \"dart throws\" (predictions)\n",
    "        n_throws = 30\n",
    "        # Bias shifts the center, variance spreads the throws\n",
    "        throws_x = np.random.normal(bias, np.sqrt(variance), n_throws)\n",
    "        throws_y = np.random.normal(0, np.sqrt(variance), n_throws)\n",
    "        \n",
    "        # Plot throws\n",
    "        ax.scatter(throws_x, throws_y, c='blue', s=100, edgecolors='black', \n",
    "                  linewidths=1, zorder=5, alpha=0.7)\n",
    "        \n",
    "        # Plot center of throws\n",
    "        ax.scatter([np.mean(throws_x)], [np.mean(throws_y)], c='yellow', s=200,\n",
    "                  edgecolors='black', linewidths=2, zorder=6, marker='*')\n",
    "        \n",
    "        ax.set_xlim(-4, 4)\n",
    "        ax.set_ylim(-4, 4)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('x error')\n",
    "        ax.set_ylabel('y error')\n",
    "        ax.axhline(y=0, color='black', linewidth=0.5, linestyle='--')\n",
    "        ax.axvline(x=0, color='black', linewidth=0.5, linestyle='--')\n",
    "    \n",
    "    plt.suptitle('The Dartboard Analogy for Bias-Variance\\n(Blue dots = predictions, Star = average, Red center = target)', \n",
    "                fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "create_dartboard_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Practical Implications\n",
    "\n",
    "### How to Diagnose Your Model\n",
    "\n",
    "| Symptom | Diagnosis | Solution |\n",
    "|---------|-----------|----------|\n",
    "| High train error, High test error | **Underfitting** (High Bias) | Increase model complexity, add features |\n",
    "| Low train error, High test error | **Overfitting** (High Variance) | Regularization, more data, simpler model |\n",
    "| Low train error, Low test error | **Just Right!** | Celebrate! |\n",
    "| High train error, Low test error | Something is wrong | Check for data leakage! |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute train and test errors to see this pattern\n",
    "\n",
    "def compute_train_test_errors(degree: int, n_train: int = 100, \n",
    "                             noise_std: float = 0.3, n_trials: int = 50) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compute average training and test errors across multiple trials.\n",
    "    \"\"\"\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    \n",
    "    # Fixed test set\n",
    "    X_test = np.linspace(0.5, 3.5, 100).reshape(-1, 1)\n",
    "    y_test = true_function(X_test.flatten()) + np.random.normal(0, noise_std, 100)\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        X_train, y_train = generate_data(n_train, noise_std, seed=trial)\n",
    "        \n",
    "        model = fit_polynomial(X_train, y_train, degree)\n",
    "        \n",
    "        train_pred = model.predict(X_train)\n",
    "        test_pred = model.predict(X_test)\n",
    "        \n",
    "        train_errors.append(np.mean((y_train - train_pred) ** 2))\n",
    "        test_errors.append(np.mean((y_test - test_pred) ** 2))\n",
    "    \n",
    "    return np.mean(train_errors), np.mean(test_errors)\n",
    "\n",
    "\n",
    "# Compute for various degrees\n",
    "degrees = list(range(1, 16))\n",
    "train_errs = []\n",
    "test_errs = []\n",
    "\n",
    "print(\"Computing train/test errors...\")\n",
    "for d in degrees:\n",
    "    tr, te = compute_train_test_errors(d)\n",
    "    train_errs.append(tr)\n",
    "    test_errs.append(te)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(degrees, train_errs, 'b-o', linewidth=2, markersize=8, label='Training Error')\n",
    "plt.plot(degrees, test_errs, 'r-o', linewidth=2, markersize=8, label='Test Error')\n",
    "\n",
    "# Mark underfitting and overfitting regions\n",
    "plt.fill_between([1, 3], 0, 0.5, alpha=0.1, color='blue', label='Underfitting zone')\n",
    "plt.fill_between([10, 15], 0, 0.5, alpha=0.1, color='red', label='Overfitting zone')\n",
    "\n",
    "optimal_idx = np.argmin(test_errs)\n",
    "plt.scatter([degrees[optimal_idx]], [test_errs[optimal_idx]], s=200, color='gold', \n",
    "           edgecolors='black', linewidths=2, zorder=5, marker='*')\n",
    "\n",
    "plt.xlabel('Model Complexity (Polynomial Degree)', fontsize=12)\n",
    "plt.ylabel('Mean Squared Error', fontsize=12)\n",
    "plt.title('Training vs Test Error\\nThe Gap Reveals Overfitting', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(degrees)\n",
    "plt.ylim(0, max(test_errs) * 1.1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOptimal degree: {degrees[optimal_idx]}\")\n",
    "print(f\"\\nPattern to remember:\")\n",
    "print(\"  - Train error always decreases with complexity\")\n",
    "print(\"  - Test error decreases, then INCREASES (overfitting!)\")\n",
    "print(\"  - The GAP between train and test reveals variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Regularization Reduces Variance\n",
    "\n",
    "One of the most practical applications: regularization (like Ridge regression) reduces variance at the cost of slightly increased bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ridge_polynomial(X: np.ndarray, y: np.ndarray, degree: int, alpha: float) -> object:\n",
    "    \"\"\"\n",
    "    Fit a regularized (Ridge) polynomial regression model.\n",
    "    \"\"\"\n",
    "    model = make_pipeline(\n",
    "        PolynomialFeatures(degree, include_bias=False),\n",
    "        Ridge(alpha=alpha)\n",
    "    )\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def bootstrap_regularized(degree: int, alpha: float, n_bootstrap: int = 100) -> dict:\n",
    "    \"\"\"\n",
    "    Compute bias-variance for regularized model.\n",
    "    \"\"\"\n",
    "    X_test = np.linspace(0.5, 3.5, 50).reshape(-1, 1)\n",
    "    y_true_test = true_function(X_test.flatten())\n",
    "    \n",
    "    all_predictions = np.zeros((n_bootstrap, 50))\n",
    "    \n",
    "    for i in range(n_bootstrap):\n",
    "        X_train, y_train = generate_data(100, noise_std=0.3, seed=i)\n",
    "        model = fit_ridge_polynomial(X_train, y_train, degree, alpha)\n",
    "        all_predictions[i] = model.predict(X_test)\n",
    "    \n",
    "    mean_prediction = np.mean(all_predictions, axis=0)\n",
    "    bias_squared = np.mean((mean_prediction - y_true_test) ** 2)\n",
    "    variance = np.mean(np.var(all_predictions, axis=0))\n",
    "    \n",
    "    return {\n",
    "        'alpha': alpha,\n",
    "        'bias_squared': bias_squared,\n",
    "        'variance': variance,\n",
    "        'total': bias_squared + variance + 0.09\n",
    "    }\n",
    "\n",
    "\n",
    "# Compare unregularized vs regularized for high-degree polynomial\n",
    "degree = 12\n",
    "alphas = [0, 0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "print(f\"Regularization effect on Degree {degree} polynomial:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "reg_results = []\n",
    "for alpha in alphas:\n",
    "    result = bootstrap_regularized(degree, alpha)\n",
    "    reg_results.append(result)\n",
    "    print(f\"α={alpha:6.3f}: Bias²={result['bias_squared']:.4f}, \"\n",
    "          f\"Var={result['variance']:.4f}, Total={result['total']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize regularization effect\nalphas_log = [r['alpha'] if r['alpha'] > 0 else 1e-4 for r in reg_results]\nbiases = [r['bias_squared'] for r in reg_results]\nvariances = [r['variance'] for r in reg_results]\ntotals = [r['total'] for r in reg_results]\n\nplt.figure(figsize=(12, 6))\n\n# plt.semilogx() plots with logarithmic x-axis, linear y-axis\n# Useful when x values span multiple orders of magnitude (like 0.001 to 10)\n# Related functions:\n#   plt.semilogy() - log y-axis, linear x-axis\n#   plt.loglog() - both axes logarithmic\nplt.semilogx(alphas_log, biases, 'b-o', linewidth=2, markersize=8, label='Bias²')\nplt.semilogx(alphas_log, variances, 'r-o', linewidth=2, markersize=8, label='Variance')\nplt.semilogx(alphas_log, totals, 'g-o', linewidth=3, markersize=8, label='Total Error')\n\nplt.xlabel('Regularization Strength (α)', fontsize=12)\nplt.ylabel('Error', fontsize=12)\nplt.title(f'Regularization Trades Variance for Bias\\n(Degree {degree} Polynomial)', fontsize=14, fontweight='bold')\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKey insight: As regularization increases...\")\nprint(\"  - Variance DECREASES (model becomes more stable)\")\nprint(\"  - Bias INCREASES (model becomes simpler)\")\nprint(\"  - There's an optimal α that minimizes total error!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself\n",
    "\n",
    "### Exercise 1: Different Noise Levels\n",
    "\n",
    "How does the optimal model complexity change with noise level? Test with noise_std = 0.1, 0.3, and 0.6.\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Higher noise makes it harder to distinguish signal from noise. You might expect simpler models to be optimal when noise is high.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Test different noise levels and find optimal degree for each\n",
    "\n",
    "noise_levels = [0.1, 0.3, 0.6]\n",
    "\n",
    "# For each noise level, compute bias-variance decomposition\n",
    "# and find the optimal polynomial degree\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: More Training Data\n",
    "\n",
    "Does having more training data allow us to use more complex models? Test n_samples = 50, 200, and 500.\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "More data reduces variance, so we can afford more complexity. This is why big data enables deep learning!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "\n",
    "# Modify bootstrap_bias_variance to accept n_samples parameter\n",
    "# and test how optimal degree changes with dataset size\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Using Training Error to Select Models\n",
    "\n",
    "```python\n",
    "# WRONG:\n",
    "# \"Degree 15 has the lowest training error, so it's the best model!\"\n",
    "\n",
    "# RIGHT:\n",
    "# Use validation/test error or cross-validation to select models.\n",
    "# Training error ALWAYS decreases with complexity!\n",
    "```\n",
    "\n",
    "### Mistake 2: Confusing Bias with Dataset Bias\n",
    "\n",
    "```python\n",
    "# WRONG:\n",
    "# \"My model has bias because my training data is biased\"\n",
    "\n",
    "# RIGHT:\n",
    "# Statistical bias (underfitting) ≠ Data bias (unfair representation)\n",
    "# Same word, completely different concepts!\n",
    "```\n",
    "\n",
    "### Mistake 3: Thinking More Data Always Helps\n",
    "\n",
    "```python\n",
    "# WRONG:\n",
    "# \"I'll just collect more data to fix my underfitting problem\"\n",
    "\n",
    "# RIGHT:\n",
    "# More data reduces VARIANCE, not BIAS.\n",
    "# If your model is too simple (high bias), more data won't help much.\n",
    "# You need a more expressive model.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- **The decomposition**: Total Error = Bias² + Variance + Irreducible Noise\n",
    "- **Bias**: Systematic error from model assumptions (underfitting)\n",
    "- **Variance**: Sensitivity to training data (overfitting)\n",
    "- **The tradeoff**: Reducing one typically increases the other\n",
    "- **Bootstrap estimation**: Empirically measuring bias and variance\n",
    "- **Regularization**: A knob to trade variance for bias\n",
    "- **Practical diagnosis**: Train-test gap reveals variance\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "### Ensemble Methods and Variance Reduction\n",
    "\n",
    "Prove that averaging predictions from multiple models (bagging) reduces variance but not bias. Implement a simple bagging scheme and show this empirically.\n",
    "\n",
    "```python\n",
    "def bagging_predictor(X_train, y_train, X_test, n_models=10, degree=10):\n",
    "    \"\"\"\n",
    "    Train multiple models on bootstrap samples and average predictions.\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Your implementation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [The Elements of Statistical Learning](https://hastie.su.domains/ElemStatLearn/) - Chapter 7 (Bias-Variance)\n",
    "- [An Introduction to Statistical Learning](https://www.statlearning.com/) - Chapter 2.2\n",
    "- [Understanding the Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html) - Excellent visual explanation\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any large variables\n",
    "import gc\n",
    "\n",
    "# Close all matplotlib figures\n",
    "plt.close('all')\n",
    "\n",
    "# Garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(\"Cleanup complete!\")\n",
    "print(\"\\nNext up: Lab A.3 - PAC Learning Bounds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}