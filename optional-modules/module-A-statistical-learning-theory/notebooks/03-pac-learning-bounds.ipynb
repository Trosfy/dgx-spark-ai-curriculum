{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab A.3: PAC Learning Bounds\n",
    "\n",
    "**Module:** A - Statistical Learning Theory  \n",
    "**Time:** 1.5 hours  \n",
    "**Difficulty:** ⭐⭐⭐⭐ (Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand the PAC (Probably Approximately Correct) learning framework\n",
    "- [ ] Calculate sample complexity bounds for learning tasks\n",
    "- [ ] Compare theoretical bounds to empirical performance\n",
    "- [ ] Apply PAC learning to estimate required dataset sizes\n",
    "- [ ] Understand why theoretical bounds are often loose\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Lab A.1 (VC Dimension)\n",
    "- Completed: Lab A.2 (Bias-Variance)\n",
    "- Knowledge of: Probability basics, hypothesis classes\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "You're leading an ML project and your manager asks: *\"How much training data do we need to deploy this model with 99% confidence that it's at least 95% accurate?\"*\n",
    "\n",
    "This isn't just a theoretical question - it has real business implications:\n",
    "- **Data collection costs**: Labeling 1 million examples might cost $500K\n",
    "- **Time to deployment**: More data = longer project timeline\n",
    "- **Risk management**: Under-trained models fail in production\n",
    "\n",
    "**PAC learning theory** provides mathematical guarantees for answering these questions. While the bounds are often conservative, they give you worst-case guarantees that regulators and risk managers love."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: What is PAC Learning?\n",
    "\n",
    "> **Imagine you're learning to be a doctor...** \n",
    ">\n",
    "> You study 100 patient cases (training data). After studying, you need to diagnose NEW patients you've never seen.\n",
    ">\n",
    "> PAC learning asks: *How many cases do you need to study to be **probably** (with 99% chance) **approximately correct** (95% accurate) on new patients?*\n",
    ">\n",
    "> - **Probably** (the P): We can't guarantee 100% success, but we can guarantee 99% chance of success\n",
    "> - **Approximately** (the A): We might make a few mistakes (up to 5%), but we'll be mostly right\n",
    "> - **Correct** (the C): On new patients we've never seen!\n",
    ">\n",
    "> **The magic:** PAC theory tells you EXACTLY how many cases you need. Study fewer, and you might fail. Study more, and you're wasting time.\n",
    ">\n",
    "> **In AI terms:**\n",
    "> - ε (epsilon) = Maximum acceptable error rate (e.g., 0.05 = 5% errors allowed)\n",
    "> - δ (delta) = Maximum acceptable failure probability (e.g., 0.01 = 1% chance we fail to learn)\n",
    "> - m = Number of training examples needed = f(ε, δ, hypothesis complexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setting Up Our Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Callable, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Set nice plotting defaults\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment ready for PAC Learning exploration!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The PAC Learning Definition\n",
    "\n",
    "### Formal Definition\n",
    "\n",
    "A hypothesis class $\\mathcal{H}$ is **PAC learnable** if there exists:\n",
    "- An algorithm $A$\n",
    "- A sample complexity function $m(\\epsilon, \\delta)$\n",
    "\n",
    "Such that: For any distribution $D$ over inputs, and any $\\epsilon, \\delta > 0$:\n",
    "\n",
    "When given $m \\geq m(\\epsilon, \\delta)$ samples drawn i.i.d. from $D$, with probability at least $1 - \\delta$, the algorithm outputs a hypothesis $h$ with:\n",
    "\n",
    "$$\\text{Error}(h) \\leq \\min_{h^* \\in \\mathcal{H}} \\text{Error}(h^*) + \\epsilon$$\n",
    "\n",
    "In other words: with high probability, we're within ε of the best possible hypothesis in our class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pac_sample_complexity_basic(epsilon: float, delta: float) -> int:\n",
    "    \"\"\"\n",
    "    Basic sample complexity bound for finite hypothesis classes.\n",
    "    \n",
    "    For a hypothesis class with |H| hypotheses:\n",
    "    m >= (1/ε) * (ln|H| + ln(1/δ))\n",
    "    \n",
    "    This is the simplest PAC bound - useful for finite hypothesis spaces.\n",
    "    \n",
    "    Args:\n",
    "        epsilon: Accuracy parameter (maximum allowed error)\n",
    "        delta: Confidence parameter (probability of failure)\n",
    "        \n",
    "    Returns:\n",
    "        Sample complexity (minimum samples needed)\n",
    "    \"\"\"\n",
    "    # For now, assume |H| = 100 (will parameterize later)\n",
    "    H_size = 100\n",
    "    m = (1 / epsilon) * (np.log(H_size) + np.log(1 / delta))\n",
    "    return int(np.ceil(m))\n",
    "\n",
    "\n",
    "# Example calculation\n",
    "print(\"Sample Complexity for Finite Hypothesis Class (|H|=100):\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "for eps in [0.1, 0.05, 0.01]:\n",
    "    for delt in [0.1, 0.05, 0.01]:\n",
    "        m = pac_sample_complexity_basic(eps, delt)\n",
    "        print(f\"ε={eps:.2f}, δ={delt:.2f}: Need {m:>6,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: VC Dimension-Based Sample Complexity\n",
    "\n",
    "For infinite hypothesis classes (like linear classifiers), we use the **VC dimension** from Lab A.1:\n",
    "\n",
    "$$m(\\epsilon, \\delta) \\geq \\frac{C}{\\epsilon}\\left(d \\cdot \\log\\frac{1}{\\epsilon} + \\log\\frac{1}{\\delta}\\right)$$\n",
    "\n",
    "where:\n",
    "- $d$ = VC dimension of the hypothesis class\n",
    "- $C$ = constant (typically 8-16 depending on the bound used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pac_sample_complexity_vc(vc_dim: int, epsilon: float, delta: float, C: float = 8.0) -> int:\n",
    "    \"\"\"\n",
    "    Sample complexity bound based on VC dimension.\n",
    "    \n",
    "    This is the fundamental theorem of PAC learning for infinite hypothesis classes.\n",
    "    \n",
    "    Args:\n",
    "        vc_dim: VC dimension of the hypothesis class\n",
    "        epsilon: Accuracy parameter (maximum allowed error)\n",
    "        delta: Confidence parameter (probability of failure)\n",
    "        C: Constant in the bound (typically 8-16)\n",
    "        \n",
    "    Returns:\n",
    "        Sample complexity (minimum samples needed)\n",
    "    \"\"\"\n",
    "    # Using the bound from \"Understanding Machine Learning\" (Shalev-Shwartz & Ben-David)\n",
    "    m = (C / epsilon) * (vc_dim * np.log(16 / epsilon) + np.log(2 / delta))\n",
    "    return int(np.ceil(m))\n",
    "\n",
    "\n",
    "def vc_dimension_linear(d: int) -> int:\n",
    "    \"\"\"VC dimension of linear classifiers in d dimensions = d + 1\"\"\"\n",
    "    return d + 1\n",
    "\n",
    "\n",
    "# Calculate sample complexity for various settings\n",
    "print(\"Sample Complexity for Linear Classifiers (VC = d + 1):\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'Dimension':>10} | {'VC Dim':>8} | {'ε=0.05':>12} | {'ε=0.01':>12}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for d in [10, 50, 100, 500, 1000]:\n",
    "    vc = vc_dimension_linear(d)\n",
    "    m_5 = pac_sample_complexity_vc(vc, epsilon=0.05, delta=0.05)\n",
    "    m_1 = pac_sample_complexity_vc(vc, epsilon=0.01, delta=0.05)\n",
    "    print(f\"{d:>10} | {vc:>8} | {m_5:>12,} | {m_1:>12,}\")\n",
    "\n",
    "print(\"\\n(δ = 0.05 for 95% confidence)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how sample complexity scales\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Sample complexity vs VC dimension\n",
    "vc_dims = np.arange(1, 101)\n",
    "m_5 = [pac_sample_complexity_vc(vc, 0.05, 0.05) for vc in vc_dims]\n",
    "m_1 = [pac_sample_complexity_vc(vc, 0.01, 0.05) for vc in vc_dims]\n",
    "\n",
    "axes[0].plot(vc_dims, m_5, 'b-', linewidth=2, label='ε = 0.05')\n",
    "axes[0].plot(vc_dims, m_1, 'r-', linewidth=2, label='ε = 0.01')\n",
    "axes[0].set_xlabel('VC Dimension', fontsize=12)\n",
    "axes[0].set_ylabel('Samples Needed', fontsize=12)\n",
    "axes[0].set_title('Sample Complexity vs VC Dimension\\n(δ = 0.05)', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Sample complexity vs epsilon\n",
    "epsilons = np.linspace(0.01, 0.2, 50)\n",
    "vc = 50  # Fixed VC dimension\n",
    "m_values = [pac_sample_complexity_vc(vc, eps, 0.05) for eps in epsilons]\n",
    "\n",
    "axes[1].plot(epsilons, m_values, 'g-', linewidth=2)\n",
    "axes[1].set_xlabel('Epsilon (ε) - Allowed Error', fontsize=12)\n",
    "axes[1].set_ylabel('Samples Needed', fontsize=12)\n",
    "axes[1].set_title(f'Sample Complexity vs ε\\n(VC = {vc}, δ = 0.05)', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: Sample complexity grows linearly with VC dimension\")\n",
    "print(\"but scales as 1/ε (inversely with allowed error).\")\n",
    "print(\"Demanding 1% error instead of 5% requires ~5x more data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Interactive Sample Complexity Calculator\n",
    "\n",
    "Let's build a practical tool for estimating data requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleComplexityCalculator:\n",
    "    \"\"\"\n",
    "    A practical tool for estimating training data requirements.\n",
    "    \n",
    "    This class provides multiple bounds and helps compare\n",
    "    theoretical requirements with practical rules of thumb.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_type: str, n_features: int):\n",
    "        \"\"\"\n",
    "        Initialize calculator.\n",
    "        \n",
    "        Args:\n",
    "            model_type: 'linear', 'polynomial', 'neural_network'\n",
    "            n_features: Number of input features\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.n_features = n_features\n",
    "        self.vc_dim = self._estimate_vc_dimension()\n",
    "    \n",
    "    def _estimate_vc_dimension(self) -> int:\n",
    "        \"\"\"Estimate VC dimension based on model type.\"\"\"\n",
    "        if self.model_type == 'linear':\n",
    "            return self.n_features + 1\n",
    "        elif self.model_type == 'polynomial':\n",
    "            # Degree 2 polynomial\n",
    "            from math import comb\n",
    "            return comb(self.n_features + 2, 2)\n",
    "        elif self.model_type == 'neural_network':\n",
    "            # Rough estimate: ~O(W*L*log(W)) where W = params\n",
    "            # Assume small network: 2 hidden layers, 100 neurons each\n",
    "            n_params = (self.n_features * 100) + (100 * 100) + (100 * 1)\n",
    "            return n_params * 2  # Very rough upper bound\n",
    "        else:\n",
    "            return self.n_features + 1\n",
    "    \n",
    "    def pac_bound(self, epsilon: float, delta: float) -> int:\n",
    "        \"\"\"PAC learning bound based on VC dimension.\"\"\"\n",
    "        return pac_sample_complexity_vc(self.vc_dim, epsilon, delta)\n",
    "    \n",
    "    def practical_rule(self) -> int:\n",
    "        \"\"\"Practical rule of thumb: 10-30x the number of parameters.\"\"\"\n",
    "        if self.model_type == 'linear':\n",
    "            return 10 * (self.n_features + 1)\n",
    "        elif self.model_type == 'neural_network':\n",
    "            n_params = (self.n_features * 100) + (100 * 100) + (100 * 1)\n",
    "            return 10 * n_params\n",
    "        else:\n",
    "            return 10 * self.vc_dim\n",
    "    \n",
    "    def summarize(self, epsilon: float = 0.05, delta: float = 0.05):\n",
    "        \"\"\"Print a summary of sample complexity estimates.\"\"\"\n",
    "        pac = self.pac_bound(epsilon, delta)\n",
    "        practical = self.practical_rule()\n",
    "        \n",
    "        print(f\"\\nSample Complexity Analysis\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Model Type: {self.model_type}\")\n",
    "        print(f\"Input Features: {self.n_features}\")\n",
    "        print(f\"Estimated VC Dimension: {self.vc_dim:,}\")\n",
    "        print(f\"\\nFor ε={epsilon} (max {epsilon*100:.1f}% error), δ={delta} ({(1-delta)*100:.0f}% confidence):\")\n",
    "        print(f\"  PAC Theoretical Bound: {pac:>15,} samples\")\n",
    "        print(f\"  Practical Rule of Thumb: {practical:>12,} samples\")\n",
    "        print(f\"\\nRecommendation: Start with {practical:,} samples,\")\n",
    "        print(f\"                scale up if validation error > {epsilon*100:.1f}%\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "print(\"Example 1: Spam classifier with 500 word features\")\n",
    "calc = SampleComplexityCalculator('linear', 500)\n",
    "calc.summarize(epsilon=0.05, delta=0.05)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"Example 2: Image classifier (CNN-like) with 784 pixels\")\n",
    "calc2 = SampleComplexityCalculator('neural_network', 784)\n",
    "calc2.summarize(epsilon=0.05, delta=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Empirical Verification\n",
    "\n",
    "Let's verify the PAC bounds empirically by training models with different amounts of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_data(n_samples: int, n_features: int = 20, \n",
    "                                 noise: float = 0.1, seed: int = None) -> Tuple:\n",
    "    \"\"\"\n",
    "    Generate synthetic classification data.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of samples\n",
    "        n_features: Number of features\n",
    "        noise: Amount of label noise\n",
    "        seed: Random seed\n",
    "        \n",
    "    Returns:\n",
    "        X, y, true_weights (for the linear decision boundary)\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    # Generate features\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    # True linear decision boundary\n",
    "    true_weights = np.random.randn(n_features)\n",
    "    true_weights = true_weights / np.linalg.norm(true_weights)\n",
    "    \n",
    "    # Labels (with noise)\n",
    "    logits = X @ true_weights\n",
    "    probs = 1 / (1 + np.exp(-3 * logits))  # Scale for sharper boundary\n",
    "    \n",
    "    # Add noise\n",
    "    y = (np.random.random(n_samples) < probs).astype(int)\n",
    "    flip_mask = np.random.random(n_samples) < noise\n",
    "    y[flip_mask] = 1 - y[flip_mask]\n",
    "    \n",
    "    return X, y, true_weights\n",
    "\n",
    "\n",
    "def run_learning_experiment(n_train: int, n_test: int = 5000, \n",
    "                           n_features: int = 20, n_trials: int = 50) -> Tuple:\n",
    "    \"\"\"\n",
    "    Run learning experiment with given training set size.\n",
    "    \n",
    "    Returns:\n",
    "        (mean_test_error, std_test_error, mean_train_error)\n",
    "    \"\"\"\n",
    "    test_errors = []\n",
    "    train_errors = []\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        # Generate data\n",
    "        X_train, y_train, _ = generate_classification_data(n_train, n_features, \n",
    "                                                           noise=0.1, seed=trial)\n",
    "        X_test, y_test, _ = generate_classification_data(n_test, n_features, \n",
    "                                                         noise=0.1, seed=1000+trial)\n",
    "        \n",
    "        # Train linear classifier\n",
    "        clf = LogisticRegression(max_iter=1000, random_state=trial)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Compute errors\n",
    "        train_error = 1 - clf.score(X_train, y_train)\n",
    "        test_error = 1 - clf.score(X_test, y_test)\n",
    "        \n",
    "        train_errors.append(train_error)\n",
    "        test_errors.append(test_error)\n",
    "    \n",
    "    return np.mean(test_errors), np.std(test_errors), np.mean(train_errors)\n",
    "\n",
    "\n",
    "print(\"Experiment functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments with various training set sizes\n",
    "n_features = 20\n",
    "vc_dim = n_features + 1  # Linear classifier\n",
    "\n",
    "# PAC bound says we need this many samples for ε=0.05, δ=0.05\n",
    "pac_bound = pac_sample_complexity_vc(vc_dim, 0.05, 0.05)\n",
    "print(f\"PAC theoretical bound for ε=0.05: {pac_bound:,} samples\")\n",
    "print(f\"Running experiments...\\n\")\n",
    "\n",
    "# Test various training set sizes\n",
    "train_sizes = [20, 50, 100, 200, 500, 1000, 2000, 5000]\n",
    "results = []\n",
    "\n",
    "for n_train in train_sizes:\n",
    "    mean_err, std_err, train_err = run_learning_experiment(n_train, n_features=n_features, n_trials=30)\n",
    "    results.append((n_train, mean_err, std_err, train_err))\n",
    "    print(f\"n_train={n_train:5d}: Test Error = {mean_err:.4f} ± {std_err:.4f}, Train Error = {train_err:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results vs PAC bound\n",
    "train_sizes = [r[0] for r in results]\n",
    "mean_errors = [r[1] for r in results]\n",
    "std_errors = [r[2] for r in results]\n",
    "train_errors = [r[3] for r in results]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot test error with error bars\n",
    "plt.errorbar(train_sizes, mean_errors, yerr=std_errors, fmt='bo-', \n",
    "            linewidth=2, markersize=8, capsize=5, label='Test Error (empirical)')\n",
    "plt.plot(train_sizes, train_errors, 'g^-', linewidth=2, markersize=8, label='Train Error')\n",
    "\n",
    "# Mark target epsilon\n",
    "plt.axhline(y=0.05, color='red', linestyle='--', linewidth=2, label='Target ε = 0.05')\n",
    "\n",
    "# Mark PAC bound\n",
    "plt.axvline(x=pac_bound, color='purple', linestyle=':', linewidth=2, label=f'PAC bound = {pac_bound}')\n",
    "\n",
    "# Mark where we empirically achieve the target\n",
    "for n, err in zip(train_sizes, mean_errors):\n",
    "    if err <= 0.05:\n",
    "        plt.axvline(x=n, color='green', linestyle='-.', linewidth=1, alpha=0.5)\n",
    "        plt.annotate(f'Achieved at {n}', xy=(n, 0.06), fontsize=10, color='green')\n",
    "        break\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Training Set Size', fontsize=12)\n",
    "plt.ylabel('Error Rate', fontsize=12)\n",
    "plt.title(f'Empirical Learning Curve vs PAC Bound\\n(d={n_features}, VC={vc_dim})', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11, loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation: We achieve ε=0.05 with far fewer samples than PAC bound!\")\n",
    "print(\"PAC bounds are worst-case guarantees - real data is often easier.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Why Are PAC Bounds Loose?\n",
    "\n",
    "PAC bounds are often pessimistic by 1-2 orders of magnitude. Here's why:\n",
    "\n",
    "1. **Worst-case over all distributions**: PAC bounds work for ANY data distribution. Real data has structure!\n",
    "2. **Uniform convergence**: The bounds require uniform convergence over all hypotheses\n",
    "3. **No distribution assumptions**: Making assumptions (e.g., margin, smoothness) gives tighter bounds\n",
    "4. **Constant factors**: The constants in theoretical bounds aren't optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_bounds_vs_empirical(n_features_list: List[int]) -> dict:\n",
    "    \"\"\"\n",
    "    Compare PAC bounds to empirical sample requirements.\n",
    "    \n",
    "    For each feature dimension, find the minimum samples\n",
    "    needed empirically to achieve 5% error.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for n_features in n_features_list:\n",
    "        print(f\"\\nTesting d = {n_features}...\")\n",
    "        vc_dim = n_features + 1\n",
    "        pac_bound = pac_sample_complexity_vc(vc_dim, 0.05, 0.05)\n",
    "        \n",
    "        # Binary search for empirical requirement\n",
    "        empirical_min = None\n",
    "        for n_train in [50, 100, 200, 500, 1000, 2000]:\n",
    "            mean_err, _, _ = run_learning_experiment(n_train, n_features=n_features, n_trials=20)\n",
    "            if mean_err <= 0.05 and empirical_min is None:\n",
    "                empirical_min = n_train\n",
    "                break\n",
    "        \n",
    "        if empirical_min is None:\n",
    "            empirical_min = float('inf')  # Didn't achieve target\n",
    "        \n",
    "        results[n_features] = {\n",
    "            'vc_dim': vc_dim,\n",
    "            'pac_bound': pac_bound,\n",
    "            'empirical': empirical_min,\n",
    "            'ratio': pac_bound / empirical_min if empirical_min < float('inf') else float('inf')\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Run comparison\n",
    "print(\"Comparing PAC bounds to empirical requirements:\")\n",
    "comparison = compare_bounds_vs_empirical([10, 20, 50])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"{'Features':>10} | {'VC Dim':>8} | {'PAC Bound':>12} | {'Empirical':>10} | {'Ratio':>8}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for d, res in comparison.items():\n",
    "    emp_str = f\"{res['empirical']:,}\" if res['empirical'] < float('inf') else \">2000\"\n",
    "    ratio_str = f\"{res['ratio']:.0f}x\" if res['ratio'] < float('inf') else \"N/A\"\n",
    "    print(f\"{d:>10} | {res['vc_dim']:>8} | {res['pac_bound']:>12,} | {emp_str:>10} | {ratio_str:>8}\")\n",
    "\n",
    "print(\"\\nPAC bounds are 50-100x more conservative than needed in practice!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Practical Guidelines\n",
    "\n",
    "Given that PAC bounds are loose, here are practical rules for data requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def practical_sample_estimate(n_parameters: int, \n",
    "                             task_difficulty: str = 'medium',\n",
    "                             data_quality: str = 'clean') -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Practical sample size estimation based on industry experience.\n",
    "    \n",
    "    Args:\n",
    "        n_parameters: Number of model parameters\n",
    "        task_difficulty: 'easy', 'medium', 'hard'\n",
    "        data_quality: 'clean', 'noisy', 'very_noisy'\n",
    "        \n",
    "    Returns:\n",
    "        (minimum_samples, recommended_samples)\n",
    "    \"\"\"\n",
    "    # Base multiplier\n",
    "    base = 10  # 10x parameters is common rule of thumb\n",
    "    \n",
    "    # Adjust for difficulty\n",
    "    difficulty_mult = {'easy': 1.0, 'medium': 2.0, 'hard': 5.0}\n",
    "    \n",
    "    # Adjust for noise\n",
    "    noise_mult = {'clean': 1.0, 'noisy': 2.0, 'very_noisy': 5.0}\n",
    "    \n",
    "    min_samples = int(n_parameters * base * difficulty_mult[task_difficulty])\n",
    "    rec_samples = int(min_samples * noise_mult[data_quality])\n",
    "    \n",
    "    return min_samples, rec_samples\n",
    "\n",
    "\n",
    "# Example calculations\n",
    "print(\"Practical Sample Size Guidelines\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "scenarios = [\n",
    "    (\"Spam filter (linear, 1000 features)\", 1000, 'easy', 'clean'),\n",
    "    (\"Sentiment analysis (BERT fine-tune, 110M params)\", 110_000_000, 'medium', 'noisy'),\n",
    "    (\"Medical diagnosis (complex, 500 features)\", 500, 'hard', 'clean'),\n",
    "    (\"Self-driving (huge, 50M params)\", 50_000_000, 'hard', 'noisy'),\n",
    "]\n",
    "\n",
    "for name, params, diff, quality in scenarios:\n",
    "    min_s, rec_s = practical_sample_estimate(params, diff, quality)\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Parameters: {params:,}\")\n",
    "    print(f\"  Minimum samples: {min_s:,}\")\n",
    "    print(f\"  Recommended: {rec_s:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: The Deep Learning Paradox\n",
    "\n",
    "Modern neural networks have billions of parameters but generalize well with \"only\" millions of examples. This seems to violate PAC bounds! What's going on?\n",
    "\n",
    "### Why Deep Learning \"Breaks\" Classical Theory\n",
    "\n",
    "1. **Implicit Regularization**: SGD finds \"flat\" minima that generalize better\n",
    "2. **Over-parameterization**: More parameters can actually help (double descent)\n",
    "3. **Data Structure**: Real images/text have rich structure not captured by worst-case bounds\n",
    "4. **Pre-training**: Transfer learning leverages massive pre-training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Double Descent Phenomenon\n",
    "print(\"The Double Descent Curve\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create illustrative plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Model complexity (number of parameters)\n",
    "complexity = np.linspace(0.1, 3, 1000)\n",
    "\n",
    "# Classical U-curve\n",
    "def classical_curve(x):\n",
    "    return 0.5 * (x - 1)**2 + 0.1\n",
    "\n",
    "# Double descent (with interpolation threshold at x=1)\n",
    "def double_descent(x):\n",
    "    # First descent\n",
    "    if x < 0.9:\n",
    "        return 0.5 * (x - 1)**2 + 0.1\n",
    "    # Peak at interpolation threshold\n",
    "    elif x < 1.1:\n",
    "        return 0.5 + 0.3 * np.sin((x - 0.9) * np.pi / 0.2)\n",
    "    # Second descent\n",
    "    else:\n",
    "        return 0.3 * np.exp(-(x - 1.1)) + 0.05\n",
    "\n",
    "y_classical = [classical_curve(x) for x in complexity]\n",
    "y_double = [double_descent(x) for x in complexity]\n",
    "\n",
    "ax.plot(complexity, y_classical, 'b--', linewidth=2, label='Classical theory (bias-variance)')\n",
    "ax.plot(complexity, y_double, 'r-', linewidth=3, label='Modern deep learning')\n",
    "\n",
    "# Mark regions\n",
    "ax.axvline(x=1.0, color='gray', linestyle=':', linewidth=1)\n",
    "ax.annotate('Interpolation\\nThreshold', xy=(1.0, 0.6), fontsize=10, ha='center')\n",
    "\n",
    "ax.fill_between([0.1, 0.7], 0, 1, alpha=0.1, color='blue', label='Underfitting')\n",
    "ax.fill_between([0.7, 1.3], 0, 1, alpha=0.1, color='orange', label='Classical overfitting')\n",
    "ax.fill_between([1.3, 3], 0, 1, alpha=0.1, color='green', label='Over-parameterized (good!)')\n",
    "\n",
    "ax.set_xlabel('Model Complexity (Parameters / Data)', fontsize=12)\n",
    "ax.set_ylabel('Test Error', fontsize=12)\n",
    "ax.set_title('Double Descent: Why Over-Parameterization Works\\n(Counter-intuitive but true!)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right', fontsize=10)\n",
    "ax.set_ylim(0, 0.8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: Modern neural networks operate in the 'over-parameterized' regime\")\n",
    "print(\"where MORE parameters can lead to LOWER test error - the opposite of classical wisdom!\")\n",
    "print(\"\\nThis is an active research area. Classical PAC bounds don't capture this phenomenon.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself\n",
    "\n",
    "### Exercise 1: Sample Complexity for Your Task\n",
    "\n",
    "You're building a model to predict customer churn with 50 features. Calculate:\n",
    "1. The PAC bound for 95% accuracy with 99% confidence\n",
    "2. A practical estimate using rule of thumb\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "For linear classifier: VC = 51. Then use pac_sample_complexity_vc with ε=0.05, δ=0.01.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "n_features = 50\n",
    "epsilon = 0.05  # 95% accuracy = 5% error\n",
    "delta = 0.01    # 99% confidence\n",
    "\n",
    "# Calculate VC dimension\n",
    "vc_dim = None  # Your answer\n",
    "\n",
    "# Calculate PAC bound\n",
    "pac_bound = None  # Your answer\n",
    "\n",
    "# Practical estimate (10x parameters)\n",
    "practical = None  # Your answer\n",
    "\n",
    "# Print results\n",
    "# print(f\"VC dimension: {vc_dim}\")\n",
    "# print(f\"PAC bound: {pac_bound:,}\")\n",
    "# print(f\"Practical estimate: {practical:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Empirical Verification on MNIST\n",
    "\n",
    "Load a subset of MNIST and verify how test accuracy improves with training set size. Compare to PAC predictions.\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "MNIST has 784 features (28x28 pixels). Use sklearn.datasets.fetch_openml to load it.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Load MNIST\n",
    "# mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "# X, y = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "# Test with various training sizes and plot learning curve\n",
    "# Compare to PAC bound for d=784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Taking PAC Bounds Literally\n",
    "\n",
    "```python\n",
    "# WRONG:\n",
    "# \"PAC says I need 100,000 samples, so I must wait to collect them all\"\n",
    "\n",
    "# RIGHT:\n",
    "# PAC bounds are worst-case. Start training with what you have,\n",
    "# use validation curves to know if you need more data.\n",
    "```\n",
    "\n",
    "### Mistake 2: Ignoring Distribution Shift\n",
    "\n",
    "```python\n",
    "# WRONG:\n",
    "# \"I achieved 99% accuracy on test set, so PAC guarantees I'll do well in production\"\n",
    "\n",
    "# RIGHT:\n",
    "# PAC assumes test data comes from same distribution as training.\n",
    "# Real production data often drifts! Monitor continuously.\n",
    "```\n",
    "\n",
    "### Mistake 3: Using the Wrong VC Dimension\n",
    "\n",
    "```python\n",
    "# WRONG:\n",
    "# For neural net with 1M parameters: VC = 1M\n",
    "\n",
    "# RIGHT:\n",
    "# Neural net VC bounds are complex and often loose.\n",
    "# Effective VC is reduced by regularization, architecture, and training dynamics.\n",
    "# Use practical guidelines instead.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- **PAC Framework**: Probably Approximately Correct learning with (ε, δ) guarantees\n",
    "- **Sample Complexity**: How many samples needed for given accuracy/confidence\n",
    "- **VC-based Bounds**: m ∝ (VC/ε) × log(1/ε) + log(1/δ)\n",
    "- **Bounds are Loose**: Typically 10-100x more conservative than practice\n",
    "- **Deep Learning Paradox**: Over-parameterization helps (double descent)\n",
    "- **Practical Guidelines**: 10-30x parameters as rule of thumb\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "### PAC-Bayes Bounds\n",
    "\n",
    "PAC-Bayes provides tighter bounds for neural networks by incorporating prior beliefs. Research and implement a simple PAC-Bayes bound calculation.\n",
    "\n",
    "The PAC-Bayes bound is:\n",
    "$$L(Q) \\leq \\hat{L}(Q) + \\sqrt{\\frac{KL(Q||P) + \\log(2n/\\delta)}{2n}}$$\n",
    "\n",
    "where Q is the posterior, P is the prior, and n is sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Implement PAC-Bayes bound here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Understanding Machine Learning: From Theory to Algorithms](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/) - Chapters 2-6\n",
    "- [Foundations of Machine Learning (Mohri et al.)](https://cs.nyu.edu/~mohri/mlbook/) - Rigorous treatment\n",
    "- [Deep Double Descent (Nakkiran et al., 2019)](https://arxiv.org/abs/1912.02292) - Modern phenomena\n",
    "- [PAC-Bayes Tutorial](https://www.cs.cmu.edu/~dpic/ml2013/pac-bayes-tut.pdf) - Tighter bounds\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any large variables\n",
    "import gc\n",
    "\n",
    "# Close all matplotlib figures\n",
    "plt.close('all')\n",
    "\n",
    "# Garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(\"Cleanup complete!\")\n",
    "print(\"\\nCongratulations! You've completed Module A: Statistical Learning Theory!\")\n",
    "print(\"\\nYou now understand:\")\n",
    "print(\"  - VC Dimension (Lab A.1)\")\n",
    "print(\"  - Bias-Variance Tradeoff (Lab A.2)\")\n",
    "print(\"  - PAC Learning Bounds (Lab A.3)\")\n",
    "print(\"\\nThese theoretical foundations will help you make better\")\n",
    "print(\"architectural decisions and debug model issues systematically.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
