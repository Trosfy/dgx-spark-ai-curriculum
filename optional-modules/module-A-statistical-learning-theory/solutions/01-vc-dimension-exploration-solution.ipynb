{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab A.1: VC Dimension Exploration - SOLUTIONS\n",
    "\n",
    "**Module:** A - Statistical Learning Theory  \n",
    "**Type:** Solution Notebook\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains complete solutions to all exercises from Lab A.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup (same as main notebook)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions from main notebook\n",
    "\n",
    "def can_linearly_separate(points: np.ndarray, labels: np.ndarray) -> bool:\n",
    "    if len(np.unique(labels)) == 1:\n",
    "        return True\n",
    "    clf = SVC(kernel='linear', C=1e10, max_iter=10000)\n",
    "    try:\n",
    "        clf.fit(points, labels)\n",
    "        predictions = clf.predict(points)\n",
    "        return np.all(predictions == labels)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def check_if_shattered(points: np.ndarray):\n",
    "    n = len(points)\n",
    "    all_labelings = list(product([0, 1], repeat=n))\n",
    "    failed_labelings = []\n",
    "    for labeling in all_labelings:\n",
    "        labels = np.array(labeling)\n",
    "        if not can_linearly_separate(points, labels):\n",
    "            failed_labelings.append(labeling)\n",
    "    is_shattered = len(failed_labelings) == 0\n",
    "    return is_shattered, failed_labelings\n",
    "\n",
    "\n",
    "def generalization_bound(vc_dim: int, n_samples: int, delta: float = 0.05) -> float:\n",
    "    if n_samples <= vc_dim:\n",
    "        return 1.0\n",
    "    gap = np.sqrt((vc_dim * np.log(2 * n_samples / vc_dim) + np.log(4 / delta)) / n_samples)\n",
    "    return min(gap, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1 Solution: Custom Point Configuration\n",
    "\n",
    "**Task:** Create 5 points in 2D and check if they can be shattered by linear classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: 5 points cannot be shattered (VC = 3 for 2D linear classifiers)\n",
    "\n",
    "# Create 5 points in various configurations\n",
    "points_5 = np.array([\n",
    "    [0, 0],\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [1, 1],\n",
    "    [0.5, 0.5]  # Point in the middle\n",
    "])\n",
    "\n",
    "# Check shattering\n",
    "is_shattered, failed = check_if_shattered(points_5)\n",
    "\n",
    "print(f\"Can 5 points be shattered by a line? {is_shattered}\")\n",
    "print(f\"Total possible labelings: {2**5} = 32\")\n",
    "print(f\"Failed labelings: {len(failed)}\")\n",
    "print(f\"Successful labelings: {32 - len(failed)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nExplanation:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Since VC dimension of linear classifiers in 2D is 3,\")\n",
    "print(\"we CANNOT shatter 5 points. In fact, we can't even\")\n",
    "print(\"shatter 4 points! The XOR problem proves this.\")\n",
    "print(\"\\nWith 5 points, many labelings will fail because they\")\n",
    "print(\"create patterns that require non-linear boundaries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few failed labelings\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Show first 8 failed labelings\n",
    "for ax, labeling in zip(axes, failed[:8]):\n",
    "    colors = ['blue' if l == 0 else 'red' for l in labeling]\n",
    "    ax.scatter(points_5[:, 0], points_5[:, 1], c=colors, s=200, \n",
    "              edgecolors='black', linewidths=2)\n",
    "    ax.set_title(f\"{labeling}\\nNOT Separable\", fontsize=10, color='red')\n",
    "    ax.set_xlim(-0.5, 1.5)\n",
    "    ax.set_ylim(-0.5, 1.5)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "plt.suptitle(\"Sample Failed Labelings for 5 Points\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2 Solution: Data Requirements Calculator\n",
    "\n",
    "**Task:** For a spam classifier with 500 features using linear classifier:\n",
    "1. What is the VC dimension?\n",
    "2. How many training emails for 10% generalization gap bound?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "d = 500  # Feature dimensionality\n",
    "\n",
    "# Q1: VC dimension for linear classifier in d dimensions = d + 1\n",
    "vc_dim = d + 1\n",
    "print(f\"Q1: VC dimension = d + 1 = {vc_dim}\")\n",
    "\n",
    "# Q2: Find n_samples where generalization_bound(vc_dim, n) < 0.10\n",
    "target_gap = 0.10\n",
    "\n",
    "# Binary search for the required sample size\n",
    "def find_required_samples(vc_dim, target_gap, delta=0.05):\n",
    "    low, high = vc_dim + 1, 10_000_000\n",
    "    \n",
    "    while low < high:\n",
    "        mid = (low + high) // 2\n",
    "        bound = generalization_bound(vc_dim, mid, delta)\n",
    "        \n",
    "        if bound <= target_gap:\n",
    "            high = mid\n",
    "        else:\n",
    "            low = mid + 1\n",
    "    \n",
    "    return low\n",
    "\n",
    "n_samples = find_required_samples(vc_dim, target_gap)\n",
    "\n",
    "print(f\"\\nQ2: Required samples for {target_gap*100:.0f}% generalization gap bound:\")\n",
    "print(f\"    n_samples = {n_samples:,}\")\n",
    "\n",
    "# Verify\n",
    "achieved_bound = generalization_bound(vc_dim, n_samples)\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"    Achieved bound: {achieved_bound:.4f} ({achieved_bound*100:.2f}%)\")\n",
    "print(f\"    Target: {target_gap:.4f} ({target_gap*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how bound changes with sample size\n",
    "n_range = np.logspace(3, 6, 100).astype(int)\n",
    "bounds = [generalization_bound(vc_dim, n) for n in n_range]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(n_range, bounds, 'b-', linewidth=2)\n",
    "plt.axhline(y=target_gap, color='red', linestyle='--', linewidth=2, label=f'Target = {target_gap}')\n",
    "plt.axvline(x=n_samples, color='green', linestyle=':', linewidth=2, label=f'Required = {n_samples:,}')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Number of Training Samples', fontsize=12)\n",
    "plt.ylabel('Generalization Gap Bound', fontsize=12)\n",
    "plt.title(f'Generalization Bound vs Sample Size\\n(VC dimension = {vc_dim})', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Solution: Empirical VC Dimension Estimation\n",
    "\n",
    "**Task:** Implement a function that empirically estimates VC dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "def estimate_vc_empirically(classifier_factory, d=2, max_points=10, n_trials=50):\n",
    "    \"\"\"\n",
    "    Empirically estimate VC dimension by testing shattering.\n",
    "    \n",
    "    For each number of points n, we:\n",
    "    1. Generate n_trials random point configurations\n",
    "    2. For each configuration, test all 2^n labelings\n",
    "    3. If ANY configuration is successfully shattered, n is \"shatterable\"\n",
    "    4. Return the largest n that was shattered\n",
    "    \n",
    "    Args:\n",
    "        classifier_factory: Function that returns a fresh classifier\n",
    "        d: Dimensionality of points\n",
    "        max_points: Maximum number of points to try\n",
    "        n_trials: Number of random configurations to test per point count\n",
    "        \n",
    "    Returns:\n",
    "        Estimated VC dimension (highest n where shattering succeeded)\n",
    "    \"\"\"\n",
    "    estimated_vc = 0\n",
    "    \n",
    "    for n_points in range(1, max_points + 1):\n",
    "        # Too many labelings to check for large n\n",
    "        if 2**n_points > 256:\n",
    "            print(f\"  n={n_points}: Skipping (2^{n_points} = {2**n_points} labelings too many)\")\n",
    "            break\n",
    "            \n",
    "        shattered_any = False\n",
    "        \n",
    "        for trial in range(n_trials):\n",
    "            # Generate random points in \"general position\"\n",
    "            points = np.random.randn(n_points, d)\n",
    "            \n",
    "            # Check if this configuration can be shattered\n",
    "            all_labelings = list(product([0, 1], repeat=n_points))\n",
    "            all_separable = True\n",
    "            \n",
    "            for labeling in all_labelings:\n",
    "                labels = np.array(labeling)\n",
    "                \n",
    "                # Skip trivial cases\n",
    "                if len(np.unique(labels)) == 1:\n",
    "                    continue\n",
    "                \n",
    "                # Try to fit classifier\n",
    "                clf = classifier_factory()\n",
    "                try:\n",
    "                    clf.fit(points, labels)\n",
    "                    pred = clf.predict(points)\n",
    "                    if not np.all(pred == labels):\n",
    "                        all_separable = False\n",
    "                        break\n",
    "                except:\n",
    "                    all_separable = False\n",
    "                    break\n",
    "            \n",
    "            if all_separable:\n",
    "                shattered_any = True\n",
    "                break\n",
    "        \n",
    "        if shattered_any:\n",
    "            estimated_vc = n_points\n",
    "            print(f\"  n={n_points}: CAN be shattered\")\n",
    "        else:\n",
    "            print(f\"  n={n_points}: Cannot be shattered (in {n_trials} trials)\")\n",
    "            break\n",
    "    \n",
    "    return estimated_vc\n",
    "\n",
    "\n",
    "# Test with linear classifier (SVM)\n",
    "print(\"Estimating VC dimension of linear classifiers in 2D:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def svm_factory():\n",
    "    return SVC(kernel='linear', C=1e10)\n",
    "\n",
    "estimated_vc = estimate_vc_empirically(svm_factory, d=2, max_points=8, n_trials=30)\n",
    "\n",
    "print(f\"\\nEstimated VC dimension: {estimated_vc}\")\n",
    "print(f\"True VC dimension: {2 + 1} = 3\")\n",
    "print(\"\\nNote: Empirical estimation should match theoretical value!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test in higher dimensions\n",
    "print(\"\\nEstimating VC dimension of linear classifiers in 5D:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "estimated_vc_5d = estimate_vc_empirically(svm_factory, d=5, max_points=10, n_trials=30)\n",
    "\n",
    "print(f\"\\nEstimated VC dimension: {estimated_vc_5d}\")\n",
    "print(f\"True VC dimension: {5 + 1} = 6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Notes\n",
    "\n",
    "1. **Exercise 1**: 5 points cannot be shattered because VC(2D linear) = 3. Many labelings fail, especially those requiring non-linear boundaries.\n",
    "\n",
    "2. **Exercise 2**: For d=500, VC=501. To get a 10% generalization gap bound, we need approximately 127,000 samples according to the theoretical bound. In practice, you'd likely need far fewer!\n",
    "\n",
    "3. **Challenge**: The empirical VC estimation correctly identifies VC=3 for 2D linear classifiers and VC=6 for 5D. The algorithm works by:\n",
    "   - Testing increasing numbers of points\n",
    "   - For each count, trying multiple random configurations\n",
    "   - Checking if any configuration can be shattered (all 2^n labelings separable)\n",
    "   - Stopping when no configuration can be shattered\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. VC dimension is the **largest number of points that can be shattered**\n",
    "2. For linear classifiers: **VC = d + 1**\n",
    "3. Higher VC dimension requires **more training data** for good generalization\n",
    "4. Theoretical bounds are often **pessimistic** but provide worst-case guarantees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
