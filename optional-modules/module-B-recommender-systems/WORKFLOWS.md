# Module B: Recommender Systems - Workflow Cheatsheets

## Workflow 1: Building a Matrix Factorization Recommender

### ğŸ“‹ When to Use
When you have explicit ratings (1-5 stars) and want a simple, interpretable baseline.

### ğŸ”„ Step-by-Step

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: Load and Prepare Data                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â–¡ Load ratings data (user_id, item_id, rating)              â”‚
â”‚ â–¡ Reindex IDs to be consecutive (0 to N-1)                  â”‚
â”‚ â–¡ Split into train/validation/test                          â”‚
â”‚                                                             â”‚
â”‚ Code:                                                       â”‚
â”‚ ```python                                                   â”‚
â”‚ from sklearn.preprocessing import LabelEncoder              â”‚
â”‚                                                             â”‚
â”‚ user_enc = LabelEncoder()                                   â”‚
â”‚ item_enc = LabelEncoder()                                   â”‚
â”‚ df['user_id'] = user_enc.fit_transform(df['user_id'])      â”‚
â”‚ df['item_id'] = item_enc.fit_transform(df['item_id'])      â”‚
â”‚                                                             â”‚
â”‚ # Temporal split (recommended)                              â”‚
â”‚ df = df.sort_values('timestamp')                           â”‚
â”‚ train = df[:int(0.8*len(df))]                              â”‚
â”‚ val = df[int(0.8*len(df)):int(0.9*len(df))]               â”‚
â”‚ test = df[int(0.9*len(df)):]                               â”‚
â”‚ ```                                                         â”‚
â”‚                                                             â”‚
â”‚ âœ“ Checkpoint: print(f"Users: {n_users}, Items: {n_items}") â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: Create Model                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â–¡ Define embedding dimensions                               â”‚
â”‚ â–¡ Create user and item embeddings                           â”‚
â”‚ â–¡ Add bias terms                                            â”‚
â”‚                                                             â”‚
â”‚ Code:                                                       â”‚
â”‚ ```python                                                   â”‚
â”‚ class MF(nn.Module):                                        â”‚
â”‚     def __init__(self, n_users, n_items, dim=64):          â”‚
â”‚         super().__init__()                                  â”‚
â”‚         self.user_emb = nn.Embedding(n_users, dim)         â”‚
â”‚         self.item_emb = nn.Embedding(n_items, dim)         â”‚
â”‚         self.user_bias = nn.Embedding(n_users, 1)          â”‚
â”‚         self.item_bias = nn.Embedding(n_items, 1)          â”‚
â”‚         nn.init.normal_(self.user_emb.weight, std=0.01)    â”‚
â”‚         nn.init.normal_(self.item_emb.weight, std=0.01)    â”‚
â”‚                                                             â”‚
â”‚     def forward(self, u, i):                               â”‚
â”‚         return (self.user_emb(u) * self.item_emb(i)).sum(1)â”‚
â”‚                + self.user_bias(u).squeeze()               â”‚
â”‚                + self.item_bias(i).squeeze()               â”‚
â”‚ ```                                                         â”‚
â”‚                                                             â”‚
â”‚ âœ“ Checkpoint: model summary shows expected parameters       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: Training Loop                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â–¡ Set optimizer with weight decay                           â”‚
â”‚ â–¡ Use MSE loss for ratings                                  â”‚
â”‚ â–¡ Track validation RMSE                                     â”‚
â”‚                                                             â”‚
â”‚ Code:                                                       â”‚
â”‚ ```python                                                   â”‚
â”‚ optimizer = torch.optim.Adam(model.parameters(),            â”‚
â”‚                              lr=1e-3, weight_decay=1e-5)    â”‚
â”‚ criterion = nn.MSELoss()                                    â”‚
â”‚                                                             â”‚
â”‚ for epoch in range(50):                                     â”‚
â”‚     model.train()                                           â”‚
â”‚     for users, items, ratings in train_loader:              â”‚
â”‚         pred = model(users.cuda(), items.cuda())           â”‚
â”‚         loss = criterion(pred, ratings.cuda())              â”‚
â”‚         optimizer.zero_grad()                               â”‚
â”‚         loss.backward()                                     â”‚
â”‚         optimizer.step()                                    â”‚
â”‚                                                             â”‚
â”‚     # Validate                                              â”‚
â”‚     val_rmse = evaluate(model, val_loader)                 â”‚
â”‚     print(f"Epoch {epoch}: Val RMSE = {val_rmse:.4f}")     â”‚
â”‚ ```                                                         â”‚
â”‚                                                             â”‚
â”‚ âœ“ Checkpoint: RMSE decreasing, < 1.0 on MovieLens          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 4: Evaluate and Visualize                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â–¡ Compute test RMSE                                         â”‚
â”‚ â–¡ Visualize embeddings with t-SNE                          â”‚
â”‚ â–¡ Find similar items                                        â”‚
â”‚                                                             â”‚
â”‚ Code:                                                       â”‚
â”‚ ```python                                                   â”‚
â”‚ # Test RMSE                                                 â”‚
â”‚ test_rmse = evaluate(model, test_loader)                   â”‚
â”‚                                                             â”‚
â”‚ # Visualize item embeddings                                 â”‚
â”‚ from sklearn.manifold import TSNE                          â”‚
â”‚ emb = model.item_emb.weight.detach().cpu().numpy()         â”‚
â”‚ tsne = TSNE(n_components=2)                                â”‚
â”‚ emb_2d = tsne.fit_transform(emb)                           â”‚
â”‚ plt.scatter(emb_2d[:,0], emb_2d[:,1], c=item_genres)       â”‚
â”‚                                                             â”‚
â”‚ # Find similar items                                        â”‚
â”‚ def similar_items(item_id, k=5):                           â”‚
â”‚     item_vec = model.item_emb.weight[item_id]              â”‚
â”‚     sims = (model.item_emb.weight @ item_vec).argsort()    â”‚
â”‚     return sims[-k-1:-1].flip(0)                           â”‚
â”‚ ```                                                         â”‚
â”‚                                                             â”‚
â”‚ âœ“ Checkpoint: Embeddings cluster by genre                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### âš ï¸ Common Pitfalls

| At Step | Watch Out For |
|---------|---------------|
| 1 | Non-consecutive IDs cause IndexError |
| 2 | Large embedding dim = more memory, risk of overfitting |
| 3 | No weight decay = embeddings may collapse |
| 4 | t-SNE perplexity too high = meaningless visualization |

### âœ… Success Criteria

- [ ] RMSE < 0.95 on MovieLens 100K
- [ ] Embeddings show genre clustering
- [ ] Similar item lookups make sense

---

## Workflow 2: Building a Two-Tower Retrieval System

### ğŸ“‹ When to Use
When you have millions of items and need real-time retrieval (< 100ms).

### ğŸ”„ Step-by-Step

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: Prepare Features                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â–¡ Extract user features (history stats, demographics)       â”‚
â”‚ â–¡ Extract item features (title embeddings, metadata)        â”‚
â”‚ â–¡ Normalize features                                        â”‚
â”‚                                                             â”‚
â”‚ Code:                                                       â”‚
â”‚ ```python                                                   â”‚
â”‚ # User features: aggregated history                         â”‚
â”‚ user_features = df.groupby('user_id').agg({                â”‚
â”‚     'rating': ['mean', 'count'],                           â”‚
â”‚     'genre_action': 'mean',                                â”‚
â”‚     'genre_comedy': 'mean',                                â”‚
â”‚     ...                                                     â”‚
â”‚ })                                                          â”‚
â”‚                                                             â”‚
â”‚ # Item features: text embeddings + metadata                 â”‚
â”‚ from sentence_transformers import SentenceTransformer      â”‚
â”‚ encoder = SentenceTransformer('all-MiniLM-L6-v2')          â”‚
â”‚ title_emb = encoder.encode(item_titles)                    â”‚
â”‚ item_features = np.hstack([title_emb, genre_onehot])       â”‚
â”‚ ```                                                         â”‚
â”‚                                                             â”‚
â”‚ âœ“ Checkpoint: Feature matrices have expected shapes         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: Build Two-Tower Model                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â–¡ Create user tower (features â†’ embedding)                  â”‚
â”‚ â–¡ Create item tower (features â†’ embedding)                  â”‚
â”‚ â–¡ Use L2 normalization for cosine similarity               â”‚
â”‚                                                             â”‚
â”‚ Code:                                                       â”‚
â”‚ ```python                                                   â”‚
â”‚ class TwoTower(nn.Module):                                  â”‚
â”‚     def __init__(self, user_dim, item_dim, emb_dim=128):   â”‚
â”‚         super().__init__()                                  â”‚
â”‚         self.user_tower = nn.Sequential(                   â”‚
â”‚             nn.Linear(user_dim, 256),                      â”‚
â”‚             nn.ReLU(),                                      â”‚
â”‚             nn.Linear(256, emb_dim),                       â”‚
â”‚             nn.LayerNorm(emb_dim)                          â”‚
â”‚         )                                                   â”‚
â”‚         self.item_tower = nn.Sequential(                   â”‚
â”‚             nn.Linear(item_dim, 256),                      â”‚
â”‚             nn.ReLU(),                                      â”‚
â”‚             nn.Linear(256, emb_dim),                       â”‚
â”‚             nn.LayerNorm(emb_dim)                          â”‚
â”‚         )                                                   â”‚
â”‚         self.temp = nn.Parameter(torch.tensor(0.07))       â”‚
â”‚                                                             â”‚
â”‚     def forward(self, user_feat, item_feat):               â”‚
â”‚         u = F.normalize(self.user_tower(user_feat), dim=-1)â”‚
â”‚         i = F.normalize(self.item_tower(item_feat), dim=-1)â”‚
â”‚         return (u @ i.T) / self.temp                       â”‚
â”‚ ```                                                         â”‚
â”‚                                                             â”‚
â”‚ âœ“ Checkpoint: Output is [batch, batch] similarity matrix   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: Train with In-Batch Negatives                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â–¡ Batch has (user, positive_item) pairs                     â”‚
â”‚ â–¡ Other items in batch are negatives                        â”‚
â”‚ â–¡ Use cross-entropy loss (softmax over similarities)        â”‚
â”‚                                                             â”‚
â”‚ Code:                                                       â”‚
â”‚ ```python                                                   â”‚
â”‚ def train_step(model, user_batch, item_batch):              â”‚
â”‚     logits = model(user_batch, item_batch)                 â”‚
â”‚     # Positives are on diagonal                             â”‚
â”‚     labels = torch.arange(len(user_batch), device=device)  â”‚
â”‚     loss = F.cross_entropy(logits, labels)                 â”‚
â”‚     return loss                                             â”‚
â”‚                                                             â”‚
â”‚ # Large batches = more negatives = better training         â”‚
â”‚ batch_size = 2048  # Use DGX Spark's memory!               â”‚
â”‚ ```                                                         â”‚
â”‚                                                             â”‚
â”‚ âœ“ Checkpoint: Loss decreasing, validation HR improving      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 4: Build FAISS Index                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â–¡ Encode all items with item tower                          â”‚
â”‚ â–¡ Build FAISS index                                         â”‚
â”‚ â–¡ Optimize for your catalog size                            â”‚
â”‚                                                             â”‚
â”‚ Code:                                                       â”‚
â”‚ ```python                                                   â”‚
â”‚ import faiss                                                â”‚
â”‚                                                             â”‚
â”‚ # Encode all items                                          â”‚
â”‚ model.eval()                                                â”‚
â”‚ with torch.no_grad():                                       â”‚
â”‚     item_emb = model.item_tower(all_item_features)         â”‚
â”‚     item_emb = F.normalize(item_emb, dim=-1)               â”‚
â”‚ item_emb = item_emb.cpu().numpy()                          â”‚
â”‚                                                             â”‚
â”‚ # Build index                                               â”‚
â”‚ dim = item_emb.shape[1]                                    â”‚
â”‚ index = faiss.IndexFlatIP(dim)  # Inner product            â”‚
â”‚ index.add(item_emb)                                         â”‚
â”‚                                                             â”‚
â”‚ # GPU acceleration                                          â”‚
â”‚ res = faiss.StandardGpuResources()                         â”‚
â”‚ index_gpu = faiss.index_cpu_to_gpu(res, 0, index)          â”‚
â”‚ ```                                                         â”‚
â”‚                                                             â”‚
â”‚ âœ“ Checkpoint: Index has correct number of items             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 5: Implement Retrieval Pipeline                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â–¡ Encode user at serving time                               â”‚
â”‚ â–¡ Search FAISS index                                        â”‚
â”‚ â–¡ Return top-K candidates                                   â”‚
â”‚                                                             â”‚
â”‚ Code:                                                       â”‚
â”‚ ```python                                                   â”‚
â”‚ def retrieve(user_features, k=100):                        â”‚
â”‚     # Encode user                                           â”‚
â”‚     with torch.no_grad():                                   â”‚
â”‚         user_emb = model.user_tower(user_features)         â”‚
â”‚         user_emb = F.normalize(user_emb, dim=-1)           â”‚
â”‚     user_emb = user_emb.cpu().numpy()                      â”‚
â”‚                                                             â”‚
â”‚     # Search                                                â”‚
â”‚     scores, indices = index_gpu.search(user_emb, k)        â”‚
â”‚     return indices[0], scores[0]                           â”‚
â”‚                                                             â”‚
â”‚ # Measure latency                                           â”‚
â”‚ import time                                                 â”‚
â”‚ start = time.time()                                         â”‚
â”‚ for _ in range(100):                                        â”‚
â”‚     retrieve(test_user_features[0:1])                      â”‚
â”‚ print(f"Avg latency: {(time.time()-start)/100*1000:.1f}ms")â”‚
â”‚ ```                                                         â”‚
â”‚                                                             â”‚
â”‚ âœ“ Checkpoint: Latency < 10ms, candidates make sense        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### âš ï¸ Common Pitfalls

| At Step | Watch Out For |
|---------|---------------|
| 1 | Missing normalization = unstable training |
| 2 | No temperature = gradients vanish |
| 3 | Small batch size = weak negatives |
| 4 | CPU index = slow, use GPU |
| 5 | Forgetting normalization at serving time |

### âœ… Success Criteria

- [ ] Retrieval latency < 10ms
- [ ] HR@100 > 0.5 (relevant item in top 100 candidates)
- [ ] Index fits in GPU memory

---

## ğŸ”€ Decision Flowchart: Which Approach?

```
                    Start
                      â”‚
                      â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Have explicit â”‚
              â”‚   ratings?    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Yes               â”‚ No (clicks, views)
            â–¼                   â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Start with   â”‚   â”‚  Start with   â”‚
    â”‚ Matrix Factor â”‚   â”‚ NeuMF/NCF     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                   â”‚
            â–¼                   â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Need < 10ms   â”‚   â”‚ Need < 10ms   â”‚
    â”‚  retrieval?   â”‚   â”‚  retrieval?   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                   â”‚
      â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
      â”‚ No        â”‚ Yes   â”‚ No        â”‚ Yes
      â–¼           â–¼       â–¼           â–¼
  [Keep MF]  [Two-Tower] [Keep NCF] [Two-Tower]
```

---

## Workflow 3: Comprehensive Evaluation

### ğŸ“‹ When to Use
After training any recommender, to properly measure its quality.

### ğŸ”„ Step-by-Step

```
Step 1: Implement Metrics
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–¡ NDCG@K - position-aware ranking quality
â–¡ HR@K - did we include the relevant item?
â–¡ MAP@K - precision at each position

Step 2: Proper Train/Test Split
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–¡ Use temporal split (not random!)
â–¡ Leave-one-out or leave-k-out
â–¡ Ensure no data leakage

Step 3: Compute Metrics
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–¡ For each test user:
  - Get model's top-K predictions
  - Compare against ground truth
  - Compute per-user metrics
â–¡ Average across users

Step 4: Compare Models
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–¡ Same test set for all models
â–¡ Statistical significance tests
â–¡ Consider diversity and coverage
```

### âœ… Success Criteria

- [ ] Using temporal split (not random)
- [ ] Reporting multiple metrics (NDCG, HR, MAP)
- [ ] Comparing against a baseline
- [ ] Checking for diversity issues
