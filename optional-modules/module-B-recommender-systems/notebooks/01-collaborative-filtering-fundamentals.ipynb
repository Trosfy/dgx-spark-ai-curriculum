{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab B.1: Collaborative Filtering Fundamentals\n",
    "\n",
    "**Module:** B - Recommender Systems  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand the user-item interaction matrix and sparsity challenges\n",
    "- [ ] Implement matrix factorization from scratch in PyTorch\n",
    "- [ ] Train a collaborative filtering model with ALS-style optimization\n",
    "- [ ] Evaluate recommendations using RMSE\n",
    "- [ ] Visualize learned embeddings with t-SNE\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Module 2.1 (PyTorch Fundamentals)\n",
    "- Knowledge of: Basic linear algebra (vectors, matrices, dot products)\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The Netflix Prize**: In 2006, Netflix offered $1 million to anyone who could improve their recommendation algorithm by 10%. The winning solution used **matrix factorization** - the exact technique you'll learn today!\n",
    "\n",
    "Matrix factorization powers recommendations at:\n",
    "- üé¨ **Netflix**: \"Because you watched...\"\n",
    "- üéµ **Spotify**: Discover Weekly playlists\n",
    "- üõí **Amazon**: \"Customers who bought this also bought...\"\n",
    "- üì± **TikTok**: The For You page\n",
    "\n",
    "---\n",
    "\n",
    "## üßí ELI5: Collaborative Filtering\n",
    "\n",
    "> **Imagine you're at a pizza party with friends...**\n",
    ">\n",
    "> You've never tried the Hawaiian pizza, but you notice that:\n",
    "> - Your friend Sarah loves pepperoni AND Hawaiian pizza\n",
    "> - You ALSO love pepperoni pizza\n",
    "> - So maybe you'd like Hawaiian pizza too!\n",
    ">\n",
    "> This is **collaborative filtering**: finding patterns in what similar people like.\n",
    ">\n",
    "> **Matrix Factorization** takes this further: instead of just finding \"similar people,\" \n",
    "> it learns *hidden factors* like \"likes spicy food\" or \"prefers comedy movies\" that \n",
    "> explain why people rate things the way they do.\n",
    ">\n",
    "> **In AI terms:** We decompose a giant ratings matrix (users √ó items) into two smaller \n",
    "> matrices: user preferences and item characteristics. The dot product of these gives us \n",
    "> predicted ratings!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Loading\n",
    "\n",
    "Let's start by loading the MovieLens dataset - the classic benchmark for recommender systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's make sure we have our utilities available\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add scripts directory to path\n",
    "module_dir = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(module_dir / 'scripts'))\n",
    "\n",
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Our utilities\n",
    "from data_utils import (\n",
    "    download_movielens, \n",
    "    print_dataset_info,\n",
    "    train_test_split_by_time,\n",
    "    RatingsDataset,\n",
    "    compute_statistics\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load MovieLens 100K dataset\n",
    "ratings_df, movies_df = download_movielens('100k')\n",
    "\n",
    "# Display dataset statistics\n",
    "print_dataset_info(ratings_df, movies_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "We loaded the MovieLens 100K dataset with:\n",
    "- **943 users** who rated **1,682 movies**\n",
    "- **100,000 ratings** on a 1-5 star scale\n",
    "- **93.7% sparsity** - most user-movie pairs have NO rating!\n",
    "\n",
    "This sparsity is the core challenge of recommender systems. How do we predict ratings for movies a user has never seen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the rating distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Rating distribution\n",
    "axes[0].hist(ratings_df['rating'], bins=5, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Rating')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Rating Distribution')\n",
    "\n",
    "# Ratings per user\n",
    "user_counts = ratings_df.groupby('user_id').size()\n",
    "axes[1].hist(user_counts, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Number of Ratings')\n",
    "axes[1].set_ylabel('Number of Users')\n",
    "axes[1].set_title('Ratings per User')\n",
    "axes[1].axvline(user_counts.median(), color='red', linestyle='--', label=f'Median: {user_counts.median():.0f}')\n",
    "axes[1].legend()\n",
    "\n",
    "# Ratings per movie\n",
    "item_counts = ratings_df.groupby('item_id').size()\n",
    "axes[2].hist(item_counts, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[2].set_xlabel('Number of Ratings')\n",
    "axes[2].set_ylabel('Number of Movies')\n",
    "axes[2].set_title('Ratings per Movie (Long Tail!)')\n",
    "axes[2].axvline(item_counts.median(), color='red', linestyle='--', label=f'Median: {item_counts.median():.0f}')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Key Observations:\")\n",
    "print(f\"   - Most ratings are 3-4 stars (positive skew)\")\n",
    "print(f\"   - Some users rate 20 movies, others rate 700+\")\n",
    "print(f\"   - Long tail: many movies have very few ratings (cold start problem!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding the User-Item Matrix\n",
    "\n",
    "Before we do matrix factorization, let's visualize what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small user-item matrix for visualization\n",
    "# (Full matrix would be 943 x 1682 = 1.58 million cells!)\n",
    "\n",
    "# Select 20 most active users and 30 most popular movies\n",
    "top_users = ratings_df.groupby('user_id').size().nlargest(20).index\n",
    "top_items = ratings_df.groupby('item_id').size().nlargest(30).index\n",
    "\n",
    "# Filter to these users/items\n",
    "subset = ratings_df[\n",
    "    ratings_df['user_id'].isin(top_users) & \n",
    "    ratings_df['item_id'].isin(top_items)\n",
    "]\n",
    "\n",
    "# Create pivot table (the user-item matrix)\n",
    "matrix = subset.pivot_table(\n",
    "    index='user_id', \n",
    "    columns='item_id', \n",
    "    values='rating',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(\n",
    "    matrix, \n",
    "    cmap='YlOrRd',\n",
    "    cbar_kws={'label': 'Rating'},\n",
    "    linewidths=0.5\n",
    ")\n",
    "plt.title('User-Item Rating Matrix (Subset)\\nWhite = No Rating (the sparsity problem!)')\n",
    "plt.xlabel('Movie ID')\n",
    "plt.ylabel('User ID')\n",
    "plt.show()\n",
    "\n",
    "# Calculate sparsity of this subset\n",
    "subset_sparsity = (matrix == 0).sum().sum() / matrix.size\n",
    "print(f\"\\nüìä Even in this active subset: {subset_sparsity:.1%} of entries are empty!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßí ELI5: Why Factorization?\n",
    "\n",
    "> **The Big Idea:**\n",
    ">\n",
    "> Instead of storing 943 √ó 1,682 = 1.58 million numbers (mostly zeros),\n",
    "> we represent each user with a small vector (say, 64 numbers)\n",
    "> and each movie with a small vector (64 numbers).\n",
    ">\n",
    "> To predict User 5's rating for Movie 100:\n",
    "> - Look up User 5's preference vector: [0.2, -0.5, 0.8, ...]\n",
    "> - Look up Movie 100's characteristic vector: [0.1, 0.3, -0.2, ...]\n",
    "> - Dot product ‚Üí predicted rating!\n",
    ">\n",
    "> **Why this works:** The 64 dimensions learn to represent things like\n",
    "> \"how much does this user like action movies?\" and \"how action-y is this movie?\"\n",
    "> We never tell the model what these dimensions mean - it discovers them from data!\n",
    "\n",
    "```\n",
    "User-Item Matrix (sparse)     =    User Matrix    √ó    Item Matrix\n",
    "    943 √ó 1,682                    943 √ó 64            64 √ó 1,682\n",
    "    1.58M parameters               60K params          108K params\n",
    "    (93% zeros)                    (dense!)            (dense!)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Implementing Matrix Factorization\n",
    "\n",
    "Now let's build our model! We'll implement the classic Matrix Factorization with biases:\n",
    "\n",
    "$$\\hat{r}_{ui} = \\mu + b_u + b_i + \\mathbf{p}_u \\cdot \\mathbf{q}_i$$\n",
    "\n",
    "Where:\n",
    "- $\\mu$ = global average rating\n",
    "- $b_u$ = user bias (\"does this user tend to rate high or low?\")\n",
    "- $b_i$ = item bias (\"is this movie generally liked or disliked?\")\n",
    "- $\\mathbf{p}_u$ = user embedding vector\n",
    "- $\\mathbf{q}_i$ = item embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization(nn.Module):\n",
    "    \"\"\"\n",
    "    Matrix Factorization for Collaborative Filtering.\n",
    "    \n",
    "    This is the model that won the Netflix Prize!\n",
    "    \n",
    "    The key insight: every user and every item can be represented\n",
    "    as a vector in a shared \"latent space\". Similar users have \n",
    "    similar vectors. Similar movies have similar vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_users, num_items, embedding_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # User and item embeddings (the \"P\" and \"Q\" matrices)\n",
    "        self.user_embeddings = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embeddings = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # Bias terms (very important for accuracy!)\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        self.global_bias = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        # Initialize with small random values\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize embeddings with small values to prevent exploding gradients.\"\"\"\n",
    "        nn.init.normal_(self.user_embeddings.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_embeddings.weight, std=0.01)\n",
    "        nn.init.zeros_(self.user_bias.weight)\n",
    "        nn.init.zeros_(self.item_bias.weight)\n",
    "        \n",
    "    def forward(self, user_ids, item_ids):\n",
    "        \"\"\"\n",
    "        Predict ratings for user-item pairs.\n",
    "        \n",
    "        Args:\n",
    "            user_ids: Tensor of user IDs (batch_size,)\n",
    "            item_ids: Tensor of item IDs (batch_size,)\n",
    "            \n",
    "        Returns:\n",
    "            Predicted ratings (batch_size,)\n",
    "        \"\"\"\n",
    "        # Look up embeddings\n",
    "        user_emb = self.user_embeddings(user_ids)  # (batch, dim)\n",
    "        item_emb = self.item_embeddings(item_ids)  # (batch, dim)\n",
    "        \n",
    "        # Dot product: sum of element-wise multiplication\n",
    "        interaction = (user_emb * item_emb).sum(dim=1)  # (batch,)\n",
    "        \n",
    "        # Add all the biases\n",
    "        prediction = (\n",
    "            self.global_bias +           # Overall average\n",
    "            self.user_bias(user_ids).squeeze() +  # User tendency\n",
    "            self.item_bias(item_ids).squeeze() +  # Item tendency\n",
    "            interaction                  # User-item affinity\n",
    "        )\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def recommend_for_user(self, user_id, top_k=10, exclude_rated=None):\n",
    "        \"\"\"\n",
    "        Get top-K recommendations for a user.\n",
    "        \n",
    "        Args:\n",
    "            user_id: The user to recommend for\n",
    "            top_k: Number of recommendations\n",
    "            exclude_rated: Set of already-rated item IDs to exclude\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (item_ids, predicted_ratings)\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Predict for all items\n",
    "            user_ids = torch.LongTensor([user_id] * self.num_items).to(\n",
    "                next(self.parameters()).device\n",
    "            )\n",
    "            item_ids = torch.arange(self.num_items).to(\n",
    "                next(self.parameters()).device\n",
    "            )\n",
    "            \n",
    "            predictions = self(user_ids, item_ids)\n",
    "            \n",
    "            # Exclude already-rated items\n",
    "            if exclude_rated is not None:\n",
    "                for item in exclude_rated:\n",
    "                    predictions[item] = float('-inf')\n",
    "            \n",
    "            # Get top K\n",
    "            top_scores, top_items = torch.topk(predictions, top_k)\n",
    "            \n",
    "        return top_items.cpu().numpy(), top_scores.cpu().numpy()\n",
    "\n",
    "# Quick test\n",
    "num_users = ratings_df['user_id'].nunique()\n",
    "num_items = ratings_df['item_id'].nunique()\n",
    "\n",
    "model = MatrixFactorization(num_users, num_items, embedding_dim=64)\n",
    "print(f\"‚úÖ Model created!\")\n",
    "print(f\"   - Users: {num_users}, Items: {num_items}\")\n",
    "print(f\"   - Embedding dimension: 64\")\n",
    "print(f\"   - Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Parameter Count Analysis\n",
    "\n",
    "Let's break down where our parameters come from:\n",
    "- User embeddings: 943 users √ó 64 dimensions = 60,352\n",
    "- Item embeddings: 1,682 items √ó 64 dimensions = 107,648\n",
    "- User biases: 943\n",
    "- Item biases: 1,682\n",
    "- Global bias: 1\n",
    "\n",
    "**Total: ~170K parameters** - much smaller than storing 1.58M ratings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Training the Model\n",
    "\n",
    "Let's train our matrix factorization model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 80% train, 20% test (by time for realism)\n",
    "train_df, test_df = train_test_split_by_time(ratings_df, test_ratio=0.2)\n",
    "\n",
    "print(f\"üìä Data Split:\")\n",
    "print(f\"   Training:   {len(train_df):,} ratings\")\n",
    "print(f\"   Testing:    {len(test_df):,} ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets and dataloaders\n",
    "train_dataset = RatingsDataset(train_df)\n",
    "test_dataset = RatingsDataset(test_df)\n",
    "\n",
    "# Batch size of 1024 works well on DGX Spark\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "print(f\"‚úÖ DataLoaders created\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for users, items, ratings in train_loader:\n",
    "        users = users.to(device)\n",
    "        items = items.to(device)\n",
    "        ratings = ratings.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(users, items)\n",
    "        loss = criterion(predictions, ratings)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * len(users)\n",
    "    \n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    \"\"\"Evaluate on test set.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for users, items, ratings in test_loader:\n",
    "            users = users.to(device)\n",
    "            items = items.to(device)\n",
    "            ratings = ratings.to(device)\n",
    "            \n",
    "            predictions = model(users, items)\n",
    "            loss = criterion(predictions, ratings)\n",
    "            \n",
    "            total_loss += loss.item() * len(users)\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(ratings.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader.dataset)\n",
    "    rmse = np.sqrt(avg_loss)\n",
    "    \n",
    "    return rmse, np.array(all_preds), np.array(all_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, optimizer, and loss\n",
    "model = MatrixFactorization(\n",
    "    num_users=num_users,\n",
    "    num_items=num_items,\n",
    "    embedding_dim=64\n",
    ").to(device)\n",
    "\n",
    "# Set global bias to mean rating (smart initialization!)\n",
    "model.global_bias.data = torch.tensor([train_df['rating'].mean()])\n",
    "\n",
    "# Optimizer with weight decay (regularization)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=0.005,           # Learning rate\n",
    "    weight_decay=1e-5   # L2 regularization to prevent overfitting\n",
    ")\n",
    "\n",
    "# Mean Squared Error loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(f\"üéØ Target: RMSE < 0.95 (Netflix Prize baseline was ~0.95)\")\n",
    "print(f\"\\nStarting training...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 30\n",
    "train_losses = []\n",
    "test_rmses = []\n",
    "best_rmse = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_rmse, _, _ = evaluate(model, test_loader, criterion, device)\n",
    "    test_rmses.append(test_rmse)\n",
    "    \n",
    "    # Track best\n",
    "    if test_rmse < best_rmse:\n",
    "        best_rmse = test_rmse\n",
    "        best_epoch = epoch + 1\n",
    "    \n",
    "    # Print progress every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:2d}/{num_epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | \"\n",
    "              f\"Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"üèÜ Best RMSE: {best_rmse:.4f} (Epoch {best_epoch})\")\n",
    "if best_rmse < 0.95:\n",
    "    print(f\"üéâ Goal achieved! RMSE < 0.95\")\n",
    "else:\n",
    "    print(f\"üìà Keep tuning! Try more epochs or adjust hyperparameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Training loss\n",
    "axes[0].plot(train_losses, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Loss (MSE)')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test RMSE\n",
    "axes[1].plot(test_rmses, 'r-', linewidth=2)\n",
    "axes[1].axhline(y=0.95, color='green', linestyle='--', label='Target: 0.95')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Test RMSE')\n",
    "axes[1].set_title('Test RMSE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "We trained a collaborative filtering model that learned:\n",
    "1. **User embeddings**: 64-dimensional vectors representing each user's preferences\n",
    "2. **Item embeddings**: 64-dimensional vectors representing each movie's characteristics\n",
    "3. **Biases**: Accounting for users who rate high/low and movies that are generally liked/disliked\n",
    "\n",
    "The model minimizes the difference between predicted and actual ratings (MSE loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Analyzing the Learned Embeddings\n",
    "\n",
    "The magic of matrix factorization is in the learned embeddings. Let's visualize them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract item embeddings\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    item_embeddings = model.item_embeddings.weight.cpu().numpy()\n",
    "    user_embeddings = model.user_embeddings.weight.cpu().numpy()\n",
    "\n",
    "print(f\"üìä Embedding shapes:\")\n",
    "print(f\"   User embeddings: {user_embeddings.shape}\")\n",
    "print(f\"   Item embeddings: {item_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Key Tools for Embedding Analysis\n\nBefore we visualize our embeddings, let's understand two important tools we'll use:\n\n**1. t-SNE (t-Distributed Stochastic Neighbor Embedding)** from `sklearn.manifold`\n- Reduces high-dimensional embeddings (64D) to 2D for visualization\n- Preserves local neighborhood structure: similar items stay close together\n- Key parameters:\n  - `n_components=2`: Output dimensions for plotting\n  - `perplexity=30`: Balance between local/global structure (15-50 typical)\n  - `random_state`: For reproducibility\n\n**2. `np.linalg.norm` (Vector Norm/Magnitude)**\n- Computes the length of vectors: for v = [x, y, z], norm = ‚àö(x¬≤ + y¬≤ + z¬≤)\n- Key parameters:\n  - `axis=1`: Compute norm for each row (useful for matrices of vectors)\n  - `keepdims=True`: Maintain shape for broadcasting during normalization\n- **Use case**: Normalizing vectors to unit length enables cosine similarity via dot product\n\n```python\n# Example: Normalize vectors to unit length\nnorms = np.linalg.norm(vectors, axis=1, keepdims=True)  # Shape: (n, 1)\nnormalized = vectors / norms  # Each row now has length 1\nsimilarity = normalized @ query  # Dot product = cosine similarity!\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use t-SNE to visualize movie embeddings in 2D\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Only use movies with enough ratings for clearer visualization\n",
    "popular_items = ratings_df.groupby('item_id').size()\n",
    "popular_items = popular_items[popular_items >= 50].index.tolist()\n",
    "\n",
    "print(f\"Visualizing {len(popular_items)} movies with 50+ ratings...\")\n",
    "\n",
    "# Get embeddings for popular items\n",
    "popular_embeddings = item_embeddings[popular_items]\n",
    "\n",
    "# Run t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "embeddings_2d = tsne.fit_transform(popular_embeddings)\n",
    "\n",
    "print(\"‚úÖ t-SNE complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get genre information for coloring\n",
    "def get_primary_genre(item_id):\n",
    "    \"\"\"Get the first (primary) genre for a movie.\"\"\"\n",
    "    genres = movies_df[movies_df['item_id'] == item_id]['genres'].values\n",
    "    if len(genres) > 0 and genres[0]:\n",
    "        return genres[0].split('|')[0]\n",
    "    return 'Unknown'\n",
    "\n",
    "# Get genres for popular items\n",
    "genres = [get_primary_genre(item_id) for item_id in popular_items]\n",
    "\n",
    "# Create color mapping\n",
    "unique_genres = list(set(genres))\n",
    "genre_to_color = {g: i for i, g in enumerate(unique_genres)}\n",
    "colors = [genre_to_color[g] for g in genres]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 10))\n",
    "scatter = plt.scatter(\n",
    "    embeddings_2d[:, 0], \n",
    "    embeddings_2d[:, 1],\n",
    "    c=colors,\n",
    "    cmap='tab20',\n",
    "    alpha=0.7,\n",
    "    s=50\n",
    ")\n",
    "\n",
    "# Add legend\n",
    "handles = []\n",
    "for genre in unique_genres[:10]:  # Show top 10 genres\n",
    "    idx = genre_to_color[genre]\n",
    "    handles.append(plt.scatter([], [], c=[plt.cm.tab20(idx/20)], label=genre, s=100))\n",
    "plt.legend(handles=handles, loc='upper right', title='Genre')\n",
    "\n",
    "plt.title('Movie Embeddings Visualization (t-SNE)\\nSimilar movies cluster together!')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Notice how movies of similar genres tend to cluster together!\")\n",
    "print(\"   This emerged naturally from the ratings - we never told the model about genres!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar movies using embedding similarity\n",
    "def find_similar_movies(movie_title, top_k=5):\n",
    "    \"\"\"Find movies similar to the given one based on learned embeddings.\"\"\"\n",
    "    # Find the movie\n",
    "    matches = movies_df[movies_df['title'].str.contains(movie_title, case=False, na=False)]\n",
    "    \n",
    "    if len(matches) == 0:\n",
    "        print(f\"Movie '{movie_title}' not found!\")\n",
    "        return\n",
    "    \n",
    "    movie_id = matches.iloc[0]['item_id']\n",
    "    movie_name = matches.iloc[0]['title']\n",
    "    \n",
    "    # Get embedding\n",
    "    movie_emb = item_embeddings[movie_id]\n",
    "    \n",
    "    # Compute similarity to all movies (dot product of normalized vectors = cosine similarity)\n",
    "    norms = np.linalg.norm(item_embeddings, axis=1, keepdims=True)\n",
    "    normalized = item_embeddings / (norms + 1e-8)\n",
    "    query_normalized = movie_emb / (np.linalg.norm(movie_emb) + 1e-8)\n",
    "    \n",
    "    similarities = normalized @ query_normalized\n",
    "    \n",
    "    # Get top K (excluding the query movie)\n",
    "    top_indices = np.argsort(similarities)[::-1][1:top_k+1]\n",
    "    \n",
    "    print(f\"\\nüé¨ Movies similar to: {movie_name}\")\n",
    "    print(\"‚îÄ\" * 60)\n",
    "    \n",
    "    for i, idx in enumerate(top_indices):\n",
    "        similar_movie = movies_df[movies_df['item_id'] == idx]\n",
    "        if len(similar_movie) > 0:\n",
    "            title = similar_movie.iloc[0]['title']\n",
    "            genre = similar_movie.iloc[0]['genres']\n",
    "            sim = similarities[idx]\n",
    "            print(f\"  {i+1}. {title}\")\n",
    "            print(f\"     Genres: {genre} | Similarity: {sim:.3f}\")\n",
    "\n",
    "# Try it out!\n",
    "find_similar_movies(\"Toy Story\")\n",
    "find_similar_movies(\"Star Wars\")\n",
    "find_similar_movies(\"Pulp Fiction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Making Recommendations\n",
    "\n",
    "Let's use our trained model to make actual recommendations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_for_user(user_id, model, ratings_df, movies_df, top_k=10):\n",
    "    \"\"\"\n",
    "    Generate recommendations for a user.\n",
    "    \n",
    "    Shows what the user has rated highly, then recommends new movies.\n",
    "    \"\"\"\n",
    "    # Get user's rated items\n",
    "    user_ratings = ratings_df[ratings_df['user_id'] == user_id].copy()\n",
    "    rated_items = set(user_ratings['item_id'].values)\n",
    "    \n",
    "    # Show user's top rated movies\n",
    "    user_ratings = user_ratings.merge(movies_df, on='item_id')\n",
    "    top_rated = user_ratings.nlargest(5, 'rating')\n",
    "    \n",
    "    print(f\"\\nüë§ User {user_id}'s Top Rated Movies:\")\n",
    "    print(\"‚îÄ\" * 60)\n",
    "    for _, row in top_rated.iterrows():\n",
    "        print(f\"  ‚≠ê {row['rating']:.0f}/5 - {row['title']}\")\n",
    "    \n",
    "    # Get recommendations\n",
    "    model.to(device)\n",
    "    rec_items, rec_scores = model.recommend_for_user(\n",
    "        user_id, \n",
    "        top_k=top_k,\n",
    "        exclude_rated=rated_items\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüé¨ Top {top_k} Recommendations:\")\n",
    "    print(\"‚îÄ\" * 60)\n",
    "    for i, (item_id, score) in enumerate(zip(rec_items, rec_scores)):\n",
    "        movie = movies_df[movies_df['item_id'] == item_id]\n",
    "        if len(movie) > 0:\n",
    "            title = movie.iloc[0]['title']\n",
    "            genre = movie.iloc[0]['genres']\n",
    "            print(f\"  {i+1:2d}. {title}\")\n",
    "            print(f\"      Predicted: {score:.2f} stars | Genres: {genre}\")\n",
    "\n",
    "# Recommend for a few users\n",
    "recommend_for_user(user_id=0, model=model, ratings_df=train_df, movies_df=movies_df)\n",
    "recommend_for_user(user_id=100, model=model, ratings_df=train_df, movies_df=movies_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself!\n",
    "\n",
    "### Exercise 1: Hyperparameter Tuning\n",
    "\n",
    "Try different embedding dimensions and see how they affect RMSE:\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "Create models with embedding_dim = 16, 32, 64, 128, 256 and compare:\n",
    "- Smaller embeddings = faster, but less expressive\n",
    "- Larger embeddings = more expressive, but risk overfitting\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Experiment with different embedding dimensions\n",
    "# embedding_dims = [16, 32, 64, 128]\n",
    "# results = {}\n",
    "# for dim in embedding_dims:\n",
    "#     model = MatrixFactorization(num_users, num_items, embedding_dim=dim)\n",
    "#     ... train and evaluate ...\n",
    "#     results[dim] = best_rmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Regularization Impact\n",
    "\n",
    "Try different weight_decay values (L2 regularization) and observe the train/test gap:\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "- weight_decay=0: No regularization, might overfit\n",
    "- weight_decay=1e-5: Light regularization (we used this)\n",
    "- weight_decay=1e-3: Strong regularization\n",
    "\n",
    "Look at the gap between training loss and test RMSE!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Experiment with regularization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting to Initialize Global Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Random global bias initialization\n",
    "# model.global_bias = nn.Parameter(torch.randn(1))  # Could be anything!\n",
    "\n",
    "# ‚úÖ Right: Initialize to mean rating\n",
    "# model.global_bias.data = torch.tensor([train_df['rating'].mean()])\n",
    "\n",
    "print(\"Why this matters:\")\n",
    "print(f\"  Mean rating in dataset: {train_df['rating'].mean():.2f}\")\n",
    "print(f\"  Random init might start at: 0.5 or -2.3 or anything!\")\n",
    "print(f\"  \\n  Smart initialization = faster convergence + better results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Not Handling Cold Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Assume all user/item IDs are valid\n",
    "# prediction = model(new_user_id, item_id)  # Crash if new_user_id >= num_users!\n",
    "\n",
    "# ‚úÖ Right: Check for cold start users/items\n",
    "def safe_predict(model, user_id, item_id, num_users, num_items):\n",
    "    if user_id >= num_users:\n",
    "        print(f\"‚ö†Ô∏è User {user_id} is new (cold start). Using global average.\")\n",
    "        return model.global_bias.item()\n",
    "    if item_id >= num_items:\n",
    "        print(f\"‚ö†Ô∏è Item {item_id} is new (cold start). Using global average.\")\n",
    "        return model.global_bias.item()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        return model(\n",
    "            torch.LongTensor([user_id]).to(device),\n",
    "            torch.LongTensor([item_id]).to(device)\n",
    "        ).item()\n",
    "\n",
    "# Test\n",
    "print(f\"Prediction for existing user/item: {safe_predict(model, 0, 0, num_users, num_items):.2f}\")\n",
    "print(f\"Prediction for new user: {safe_predict(model, 99999, 0, num_users, num_items):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Data Leakage in Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Random train/test split\n",
    "# This can leak future information into training!\n",
    "# train, test = sklearn.model_selection.train_test_split(ratings, test_size=0.2)\n",
    "\n",
    "# ‚úÖ Right: Time-based split (as we did)\n",
    "# train_df, test_df = train_test_split_by_time(ratings_df, test_ratio=0.2)\n",
    "\n",
    "print(\"Why time-based split matters:\")\n",
    "print(f\"  Train set max timestamp: {train_df['timestamp'].max()}\")\n",
    "print(f\"  Test set min timestamp:  {test_df['timestamp'].min()}\")\n",
    "print(f\"  \\n  In production, you can't use future ratings to predict!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ What the user-item matrix is and why it's sparse\n",
    "- ‚úÖ How matrix factorization decomposes ratings into embeddings\n",
    "- ‚úÖ Implementing MF with biases in PyTorch\n",
    "- ‚úÖ Training and evaluating with RMSE\n",
    "- ‚úÖ Visualizing embeddings with t-SNE\n",
    "- ‚úÖ Generating recommendations for users\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Implement SVD++ (10-15 min):**\n",
    "\n",
    "SVD++ incorporates implicit feedback: even items a user rated (regardless of score) tell us something about their preferences.\n",
    "\n",
    "$$\\hat{r}_{ui} = \\mu + b_u + b_i + \\mathbf{q}_i^T \\left( \\mathbf{p}_u + \\frac{1}{\\sqrt{|N(u)|}} \\sum_{j \\in N(u)} \\mathbf{y}_j \\right)$$\n",
    "\n",
    "Where $N(u)$ is the set of items user $u$ has rated, and $\\mathbf{y}_j$ are implicit factor vectors.\n",
    "\n",
    "This typically improves RMSE by 1-3%!\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [Matrix Factorization Techniques for Recommender Systems](https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf) - The Netflix Prize paper\n",
    "- [Factorization Machines](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf) - Extension to handle features\n",
    "- [Surprise Library](http://surpriselib.com/) - Easy recommender systems in Python\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ GPU memory cleared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚û°Ô∏è Next Steps\n",
    "\n",
    "In the next notebook, we'll build **Neural Collaborative Filtering (NeuMF)** - a deep learning approach that can learn non-linear user-item interactions!\n",
    "\n",
    "Continue to: **02-neural-collaborative-filtering.ipynb**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}