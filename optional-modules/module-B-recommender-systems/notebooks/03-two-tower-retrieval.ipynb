{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab B.3: Two-Tower Retrieval System\n",
    "\n",
    "**Module:** B - Recommender Systems  \n",
    "**Time:** 2.5 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê (Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand why two-tower architecture scales to billions of items\n",
    "- [ ] Build query (user) and item towers with PyTorch\n",
    "- [ ] Train with in-batch negative sampling\n",
    "- [ ] Build a FAISS index for fast approximate nearest neighbor search\n",
    "- [ ] Implement a real-time retrieval pipeline with sub-10ms latency\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Lab B.2 (Neural Collaborative Filtering)\n",
    "- Knowledge of: Embeddings, similarity metrics\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The Scale Problem:** YouTube has billions of videos. Netflix has millions of shows. Amazon has hundreds of millions of products. You can't score every single item for every user in real-time!\n",
    "\n",
    "**The Solution:** Two-stage recommendation:\n",
    "1. **Retrieval (Two-Tower)**: Quickly find ~1000 candidates from millions\n",
    "2. **Ranking**: Score those 1000 candidates with a more complex model\n",
    "\n",
    "Two-Tower is used at:\n",
    "- üì∫ **YouTube**: The famous \"Deep Neural Networks for YouTube Recommendations\" paper\n",
    "- üîç **Google Search**: Finding relevant documents\n",
    "- üõí **Alibaba**: E-commerce product retrieval\n",
    "- üéµ **Spotify**: Song candidate generation\n",
    "\n",
    "---\n",
    "\n",
    "## üßí ELI5: Two-Tower Architecture\n",
    "\n",
    "> **Imagine a huge library with 10 million books...**\n",
    ">\n",
    "> The librarian can't read your mind and check every book. Instead:\n",
    ">\n",
    "> 1. **Query Tower (You)**: You describe what you want ‚Üí gets converted to a \"vibe\" \n",
    ">    (a vector like [0.2, -0.5, 0.8, ...])\n",
    ">\n",
    "> 2. **Item Tower (Books)**: Every book has been pre-converted to its \"vibe\" \n",
    ">    (stored in a special index)\n",
    ">\n",
    "> 3. **Matching**: Find books with similar \"vibes\" using fast nearest neighbor search\n",
    ">\n",
    "> The magic: **Books are pre-encoded offline!** At query time, we only encode the user\n",
    "> (fast!) and do a similarity search (also fast!).\n",
    ">\n",
    "> **Why \"Two Towers\"?** Because user and item processing are completely separate -\n",
    "> like two towers that only meet at the top (similarity computation).\n",
    "\n",
    "```\n",
    "User Features ‚îÄ‚îÄ‚Üí [ Query Tower ] ‚îÄ‚îÄ‚Üí User Embedding ‚îÄ‚îê\n",
    "                                                       ‚îú‚îÄ‚Üí Similarity ‚îÄ‚îÄ‚Üí Top K Items\n",
    "Item Features ‚îÄ‚îÄ‚Üí [ Item Tower ]  ‚îÄ‚îÄ‚Üí Item Embeddings ‚îÄ‚îò\n",
    "                        ‚Üë                    ‚îÇ\n",
    "                   (offline)            (indexed in FAISS)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "module_dir = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(module_dir / 'scripts'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from data_utils import download_movielens, train_test_split_by_time\n",
    "\n",
    "# Check for FAISS\n",
    "try:\n",
    "    import faiss\n",
    "    FAISS_AVAILABLE = True\n",
    "    print(\"‚úÖ FAISS is available!\")\n",
    "except ImportError:\n",
    "    FAISS_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  FAISS not installed. Install with: pip install faiss-gpu\")\n",
    "    print(\"   We'll use a simple numpy implementation instead.\")\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nüñ•Ô∏è  Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "ratings_df, movies_df = download_movielens('100k')\n",
    "\n",
    "num_users = ratings_df['user_id'].nunique()\n",
    "num_items = ratings_df['item_id'].nunique()\n",
    "\n",
    "print(f\"üìä Dataset: {num_users} users, {num_items} items, {len(ratings_df):,} ratings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Item Features\n",
    "\n",
    "Two-tower models can use rich features. Let's create item features from genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract genre features (multi-hot encoding)\n",
    "all_genres = set()\n",
    "for genres in movies_df['genres'].dropna():\n",
    "    all_genres.update(genres.split('|'))\n",
    "all_genres = sorted([g for g in all_genres if g])  # Remove empty strings\n",
    "\n",
    "print(f\"Genres found: {all_genres}\")\n",
    "\n",
    "# Create genre feature matrix\n",
    "def create_genre_features(movies_df, all_genres):\n",
    "    \"\"\"Create multi-hot genre encoding for each movie.\"\"\"\n",
    "    genre_to_idx = {g: i for i, g in enumerate(all_genres)}\n",
    "    num_movies = movies_df['item_id'].max() + 1\n",
    "    \n",
    "    features = np.zeros((num_movies, len(all_genres)), dtype=np.float32)\n",
    "    \n",
    "    for _, row in movies_df.iterrows():\n",
    "        item_id = row['item_id']\n",
    "        genres = row['genres']\n",
    "        if pd.notna(genres):\n",
    "            for genre in genres.split('|'):\n",
    "                if genre in genre_to_idx:\n",
    "                    features[item_id, genre_to_idx[genre]] = 1.0\n",
    "    \n",
    "    return features\n",
    "\n",
    "item_genre_features = create_genre_features(movies_df, all_genres)\n",
    "print(f\"\\nItem genre features shape: {item_genre_features.shape}\")\n",
    "print(f\"Example (Toy Story): {item_genre_features[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user features from their rating history\n",
    "def create_user_features(ratings_df, item_genre_features, num_users):\n",
    "    \"\"\"\n",
    "    Create user features based on their watched genres.\n",
    "    \n",
    "    For each user, we average the genre vectors of items they've rated,\n",
    "    weighted by rating (higher rated = more weight).\n",
    "    \"\"\"\n",
    "    num_genres = item_genre_features.shape[1]\n",
    "    user_features = np.zeros((num_users, num_genres), dtype=np.float32)\n",
    "    user_counts = np.zeros(num_users)\n",
    "    \n",
    "    for _, row in ratings_df.iterrows():\n",
    "        user_id = int(row['user_id'])\n",
    "        item_id = int(row['item_id'])\n",
    "        rating = row['rating']\n",
    "        \n",
    "        # Weight by rating (normalized to 0-1)\n",
    "        weight = (rating - 1) / 4  # Maps 1-5 to 0-1\n",
    "        user_features[user_id] += item_genre_features[item_id] * weight\n",
    "        user_counts[user_id] += 1\n",
    "    \n",
    "    # Normalize\n",
    "    user_counts = np.maximum(user_counts, 1)  # Avoid division by zero\n",
    "    user_features = user_features / user_counts[:, np.newaxis]\n",
    "    \n",
    "    return user_features\n",
    "\n",
    "user_genre_features = create_user_features(ratings_df, item_genre_features, num_users)\n",
    "print(f\"User features shape: {user_genre_features.shape}\")\n",
    "print(f\"Example user 0 preferences: {user_genre_features[0][:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Two-Tower Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Key PyTorch Components for Two-Tower Models\n\nBefore building our towers, let's understand the key components we'll use:\n\n**`F.normalize` (Vector Normalization)**\n- Normalizes vectors to have unit length (L2 norm = 1)\n- Essential for cosine similarity: dot product of normalized vectors = cosine similarity\n- `F.normalize(x, p=2, dim=-1)`: L2 normalize along last dimension\n\n```python\nimport torch.nn.functional as F\n\nembeddings = model(inputs)  # Shape: (batch, 128)\nnormalized = F.normalize(embeddings, p=2, dim=-1)  # Each row has length 1\n# Now: normalized @ normalized.T gives cosine similarities\n```\n\n**`nn.BatchNorm1d` (Batch Normalization)**\n- Normalizes activations across the batch dimension\n- Speeds up training and improves stability\n- `nn.BatchNorm1d(num_features)`: normalizes features for each sample\n\n```python\nself.bn = nn.BatchNorm1d(128)  # For 128-dimensional features\noutput = self.bn(hidden)  # Normalize each feature across the batch\n```\n\n**`F.cross_entropy` (Cross-Entropy Loss)**\n- Multi-class classification loss (softmax + negative log likelihood)\n- For in-batch negatives: \"Which of these N items is the correct one?\"\n- `F.cross_entropy(logits, labels)`: logits are raw scores, labels are class indices\n\n```python\nlogits = query @ items.T  # Shape: (batch, batch) similarity matrix\nlabels = torch.arange(batch_size)  # Diagonal = positive pairs\nloss = F.cross_entropy(logits, labels)  # Softmax + NLL\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryTower(nn.Module):\n",
    "    \"\"\"\n",
    "    Query (User) Tower.\n",
    "    \n",
    "    Encodes user features and/or user ID into a dense embedding.\n",
    "    This tower is run at query time for each user request.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_users, user_feature_dim, embedding_dim=128, \n",
    "                 hidden_dims=[256, 128], use_features=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_features = use_features\n",
    "        \n",
    "        # User ID embedding\n",
    "        self.user_embedding = nn.Embedding(num_users, hidden_dims[0])\n",
    "        \n",
    "        # Feature processing\n",
    "        if use_features:\n",
    "            self.feature_layer = nn.Linear(user_feature_dim, hidden_dims[0])\n",
    "            input_dim = hidden_dims[0] * 2  # ID embedding + feature embedding\n",
    "        else:\n",
    "            input_dim = hidden_dims[0]\n",
    "        \n",
    "        # MLP layers\n",
    "        layers = []\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "            ])\n",
    "            input_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_dims[-1], embedding_dim))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.01)\n",
    "        \n",
    "    def forward(self, user_ids, user_features=None):\n",
    "        \"\"\"\n",
    "        Encode users to embedding space.\n",
    "        \n",
    "        Args:\n",
    "            user_ids: (batch_size,) tensor of user IDs\n",
    "            user_features: (batch_size, feature_dim) tensor of user features\n",
    "            \n",
    "        Returns:\n",
    "            L2-normalized user embeddings (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        user_emb = self.user_embedding(user_ids)\n",
    "        \n",
    "        if self.use_features and user_features is not None:\n",
    "            feature_emb = self.feature_layer(user_features)\n",
    "            combined = torch.cat([user_emb, feature_emb], dim=1)\n",
    "        else:\n",
    "            combined = user_emb\n",
    "        \n",
    "        output = self.mlp(combined)\n",
    "        \n",
    "        # L2 normalize for cosine similarity\n",
    "        return F.normalize(output, p=2, dim=-1)\n",
    "\n",
    "\n",
    "class ItemTower(nn.Module):\n",
    "    \"\"\"\n",
    "    Item Tower.\n",
    "    \n",
    "    Encodes item features and/or item ID into a dense embedding.\n",
    "    This tower is run OFFLINE for all items, and embeddings are stored in an index.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_items, item_feature_dim, embedding_dim=128,\n",
    "                 hidden_dims=[256, 128], use_features=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_features = use_features\n",
    "        \n",
    "        # Item ID embedding\n",
    "        self.item_embedding = nn.Embedding(num_items, hidden_dims[0])\n",
    "        \n",
    "        # Feature processing\n",
    "        if use_features:\n",
    "            self.feature_layer = nn.Linear(item_feature_dim, hidden_dims[0])\n",
    "            input_dim = hidden_dims[0] * 2\n",
    "        else:\n",
    "            input_dim = hidden_dims[0]\n",
    "        \n",
    "        # MLP layers\n",
    "        layers = []\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "            ])\n",
    "            input_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_dims[-1], embedding_dim))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        nn.init.normal_(self.item_embedding.weight, std=0.01)\n",
    "        \n",
    "    def forward(self, item_ids, item_features=None):\n",
    "        \"\"\"Encode items to embedding space.\"\"\"\n",
    "        item_emb = self.item_embedding(item_ids)\n",
    "        \n",
    "        if self.use_features and item_features is not None:\n",
    "            feature_emb = self.feature_layer(item_features)\n",
    "            combined = torch.cat([item_emb, feature_emb], dim=1)\n",
    "        else:\n",
    "            combined = item_emb\n",
    "        \n",
    "        output = self.mlp(combined)\n",
    "        return F.normalize(output, p=2, dim=-1)\n",
    "\n",
    "\n",
    "class TwoTowerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-Tower Retrieval Model.\n",
    "    \n",
    "    Combines query and item towers, computing similarity for training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_users, num_items, user_feature_dim, item_feature_dim,\n",
    "                 embedding_dim=128, hidden_dims=[256, 128], \n",
    "                 temperature=0.07, use_features=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.query_tower = QueryTower(\n",
    "            num_users, user_feature_dim, embedding_dim, hidden_dims, use_features\n",
    "        )\n",
    "        self.item_tower = ItemTower(\n",
    "            num_items, item_feature_dim, embedding_dim, hidden_dims, use_features\n",
    "        )\n",
    "        \n",
    "        # Temperature for softmax (lower = sharper distribution)\n",
    "        self.temperature = nn.Parameter(torch.tensor(temperature))\n",
    "        \n",
    "    def forward(self, user_ids, item_ids, user_features=None, item_features=None):\n",
    "        \"\"\"\n",
    "        Compute similarity matrix for in-batch negatives.\n",
    "        \n",
    "        Returns:\n",
    "            logits: (batch_size, batch_size) similarity matrix\n",
    "            Diagonal contains positive pairs, off-diagonal are negatives.\n",
    "        \"\"\"\n",
    "        query_emb = self.query_tower(user_ids, user_features)\n",
    "        item_emb = self.item_tower(item_ids, item_features)\n",
    "        \n",
    "        # Compute all pairwise similarities\n",
    "        # logits[i,j] = similarity between query i and item j\n",
    "        logits = torch.matmul(query_emb, item_emb.T) / self.temperature\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def encode_queries(self, user_ids, user_features=None):\n",
    "        \"\"\"Encode users (for inference).\"\"\"\n",
    "        return self.query_tower(user_ids, user_features)\n",
    "    \n",
    "    def encode_items(self, item_ids, item_features=None):\n",
    "        \"\"\"Encode items (for building index).\"\"\"\n",
    "        return self.item_tower(item_ids, item_features)\n",
    "\n",
    "\n",
    "# Test the model\n",
    "user_feature_dim = user_genre_features.shape[1]\n",
    "item_feature_dim = item_genre_features.shape[1]\n",
    "\n",
    "model = TwoTowerModel(\n",
    "    num_users=num_users,\n",
    "    num_items=num_items,\n",
    "    user_feature_dim=user_feature_dim,\n",
    "    item_feature_dim=item_feature_dim,\n",
    "    embedding_dim=128,\n",
    "    use_features=True\n",
    ").to(device)\n",
    "\n",
    "print(f\"‚úÖ Two-Tower Model created!\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: In-Batch Negative Sampling\n",
    "\n",
    "The key training trick: use other items in the same batch as negatives!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Two-Tower model training.\n",
    "    \n",
    "    Returns positive (user, item) pairs with their features.\n",
    "    Negatives come from other items in the batch (in-batch negatives).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, interactions_df, user_features, item_features):\n",
    "        self.users = interactions_df['user_id'].values\n",
    "        self.items = interactions_df['item_id'].values\n",
    "        self.user_features = torch.FloatTensor(user_features)\n",
    "        self.item_features = torch.FloatTensor(item_features)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user_id = self.users[idx]\n",
    "        item_id = self.items[idx]\n",
    "        \n",
    "        return {\n",
    "            'user_id': user_id,\n",
    "            'item_id': item_id,\n",
    "            'user_features': self.user_features[user_id],\n",
    "            'item_features': self.item_features[item_id],\n",
    "        }\n",
    "\n",
    "\n",
    "def in_batch_negative_loss(logits):\n",
    "    \"\"\"\n",
    "    Compute in-batch negative loss.\n",
    "    \n",
    "    The positive pairs are on the diagonal (user i, item i).\n",
    "    All other pairs in the batch are negatives.\n",
    "    \n",
    "    This is essentially a softmax classification:\n",
    "    \"Which item in the batch is the correct one for this user?\"\n",
    "    \"\"\"\n",
    "    batch_size = logits.shape[0]\n",
    "    labels = torch.arange(batch_size, device=logits.device)\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_df, test_df = train_test_split_by_time(ratings_df, test_ratio=0.2)\n",
    "\n",
    "train_dataset = TwoTowerDataset(train_df, user_genre_features, item_genre_features)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=512,  # Larger batch = more negatives per positive\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset created: {len(train_dataset):,} training interactions\")\n",
    "print(f\"   Batch size: 512 (511 negatives per positive!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßí ELI5: In-Batch Negatives\n",
    "\n",
    "> **Why is this so clever?**\n",
    ">\n",
    "> Traditional negative sampling: For each positive pair, randomly sample N negatives.\n",
    "> With batch size 512 and 4 negatives each: 512 √ó 4 = 2048 negative computations.\n",
    ">\n",
    "> In-batch negatives: Each batch has 512 positive pairs. \n",
    "> For user 1, items 2-512 are negatives. For user 2, items 1, 3-512 are negatives.\n",
    "> We get 511 negatives per positive **for free** - just from computing the batch!\n",
    ">\n",
    "> This is why two-tower training can scale to billions of items efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_two_tower(model, train_loader, optimizer, device, epoch):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
    "    for batch in pbar:\n",
    "        user_ids = batch['user_id'].to(device)\n",
    "        item_ids = batch['item_id'].to(device)\n",
    "        user_features = batch['user_features'].to(device)\n",
    "        item_features = batch['item_features'].to(device)\n",
    "        \n",
    "        # Forward: compute similarity matrix\n",
    "        logits = model(user_ids, item_ids, user_features, item_features)\n",
    "        \n",
    "        # Loss: cross-entropy with diagonal as targets\n",
    "        loss = in_batch_negative_loss(logits)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * len(user_ids)\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    return total_loss / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training\n",
    "model = TwoTowerModel(\n",
    "    num_users=num_users,\n",
    "    num_items=num_items,\n",
    "    user_feature_dim=user_feature_dim,\n",
    "    item_feature_dim=item_feature_dim,\n",
    "    embedding_dim=128,\n",
    "    hidden_dims=[256, 128],\n",
    "    temperature=0.07,\n",
    "    use_features=True\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"üéØ Training Two-Tower model...\")\n",
    "print(f\"   Embedding dim: 128\")\n",
    "print(f\"   Temperature: 0.07\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 15\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss = train_two_tower(model, train_loader, optimizer, device, epoch)\n",
    "    train_losses.append(loss)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"\\nüìä Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(range(1, len(train_losses)+1), train_losses, 'b-', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Two-Tower Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Understanding FAISS (Facebook AI Similarity Search)\n\nFAISS is a library for efficient similarity search developed by Meta AI. It's the industry standard for billion-scale vector retrieval.\n\n**Key FAISS Functions We'll Use:**\n\n1. **`faiss.IndexFlatIP(dim)`** - Exact inner product search\n   - Creates an index for exact nearest neighbor search using inner product (dot product)\n   - For normalized vectors, inner product = cosine similarity\n   - Best for small-medium datasets (< 1M vectors)\n\n2. **`faiss.StandardGpuResources()`** - GPU memory manager\n   - Allocates GPU memory for FAISS operations\n   - Required for GPU-accelerated search\n\n3. **`faiss.index_cpu_to_gpu(res, gpu_id, index)`** - Move index to GPU\n   - Transfers CPU index to GPU for faster search\n   - 10-100x speedup for large indices\n\n4. **`index.add(embeddings)`** - Add vectors to index\n   - Stores pre-computed item embeddings\n   - Call once during offline indexing\n\n5. **`index.search(query, k)`** - Find k nearest neighbors\n   - Returns (distances, indices) of top-k similar items\n   - Sub-millisecond latency for millions of items\n\n```python\nimport faiss\n\n# Create index for 128-dimensional embeddings\nindex = faiss.IndexFlatIP(128)  # Inner Product index\n\n# Add item embeddings (done offline)\nindex.add(item_embeddings.astype('float32'))\n\n# Search for top-10 similar items (done at query time)\ndistances, indices = index.search(query_embedding, k=10)\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Building the FAISS Index\n",
    "\n",
    "Now let's pre-compute all item embeddings and build an index for fast retrieval!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_item_embeddings(model, item_features, batch_size=512):\n",
    "    \"\"\"\n",
    "    Compute embeddings for all items (offline).\n",
    "    \n",
    "    This is done once and the embeddings are stored in the index.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    num_items = len(item_features)\n",
    "    \n",
    "    all_embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start_idx in tqdm(range(0, num_items, batch_size), desc='Computing item embeddings'):\n",
    "            end_idx = min(start_idx + batch_size, num_items)\n",
    "            \n",
    "            item_ids = torch.arange(start_idx, end_idx).to(device)\n",
    "            item_feats = torch.FloatTensor(item_features[start_idx:end_idx]).to(device)\n",
    "            \n",
    "            embeddings = model.encode_items(item_ids, item_feats)\n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "# Compute item embeddings\n",
    "item_embeddings = compute_all_item_embeddings(model, item_genre_features)\n",
    "print(f\"\\n‚úÖ Item embeddings shape: {item_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FAISS_AVAILABLE:\n",
    "    # Build FAISS index\n",
    "    def build_faiss_index(embeddings, use_gpu=True):\n",
    "        \"\"\"\n",
    "        Build a FAISS index for fast nearest neighbor search.\n",
    "        \n",
    "        For small datasets, we use exact search (IndexFlatIP).\n",
    "        For larger datasets, you'd use IVF or HNSW.\n",
    "        \"\"\"\n",
    "        dim = embeddings.shape[1]\n",
    "        \n",
    "        # Inner Product index (for cosine similarity with normalized vectors)\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "        \n",
    "        if use_gpu and torch.cuda.is_available():\n",
    "            try:\n",
    "                res = faiss.StandardGpuResources()\n",
    "                index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "                print(\"üöÄ Using GPU-accelerated FAISS!\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è GPU FAISS failed, using CPU: {e}\")\n",
    "        \n",
    "        # Add embeddings to index\n",
    "        index.add(embeddings.astype('float32'))\n",
    "        \n",
    "        return index\n",
    "    \n",
    "    index = build_faiss_index(item_embeddings)\n",
    "    print(f\"‚úÖ FAISS index built with {index.ntotal} items\")\n",
    "    \n",
    "else:\n",
    "    # Simple numpy fallback\n",
    "    class SimpleIndex:\n",
    "        def __init__(self, embeddings):\n",
    "            self.embeddings = embeddings\n",
    "            self.ntotal = len(embeddings)\n",
    "            \n",
    "        def search(self, queries, k):\n",
    "            # Brute force search\n",
    "            similarities = queries @ self.embeddings.T\n",
    "            top_k_indices = np.argsort(-similarities, axis=1)[:, :k]\n",
    "            top_k_scores = np.take_along_axis(similarities, top_k_indices, axis=1)\n",
    "            return top_k_scores, top_k_indices\n",
    "    \n",
    "    index = SimpleIndex(item_embeddings)\n",
    "    print(f\"‚úÖ Simple index built with {index.ntotal} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Real-Time Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerRetriever:\n",
    "    \"\"\"\n",
    "    Production-ready retrieval pipeline.\n",
    "    \n",
    "    Encodes user queries and retrieves top-K candidates from the index.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, index, user_features, movies_df, device):\n",
    "        self.model = model\n",
    "        self.index = index\n",
    "        self.user_features = user_features\n",
    "        self.movies_df = movies_df\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "        \n",
    "    def retrieve(self, user_id, k=10, return_timing=False):\n",
    "        \"\"\"\n",
    "        Retrieve top-K items for a user.\n",
    "        \n",
    "        Args:\n",
    "            user_id: User to recommend for\n",
    "            k: Number of items to retrieve\n",
    "            return_timing: If True, return timing breakdown\n",
    "            \n",
    "        Returns:\n",
    "            List of (item_id, score, title) tuples\n",
    "        \"\"\"\n",
    "        timings = {}\n",
    "        \n",
    "        # Step 1: Encode user query\n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            user_ids = torch.LongTensor([user_id]).to(self.device)\n",
    "            user_feats = torch.FloatTensor(self.user_features[user_id:user_id+1]).to(self.device)\n",
    "            query_embedding = self.model.encode_queries(user_ids, user_feats)\n",
    "            query_embedding = query_embedding.cpu().numpy()\n",
    "        \n",
    "        timings['encode_ms'] = (time.perf_counter() - start) * 1000\n",
    "        \n",
    "        # Step 2: Search index\n",
    "        start = time.perf_counter()\n",
    "        scores, indices = self.index.search(query_embedding.astype('float32'), k)\n",
    "        timings['search_ms'] = (time.perf_counter() - start) * 1000\n",
    "        \n",
    "        # Step 3: Format results\n",
    "        results = []\n",
    "        for item_id, score in zip(indices[0], scores[0]):\n",
    "            title = self.movies_df[self.movies_df['item_id'] == item_id]['title'].values\n",
    "            title = title[0] if len(title) > 0 else f\"Item {item_id}\"\n",
    "            results.append((item_id, float(score), title))\n",
    "        \n",
    "        timings['total_ms'] = timings['encode_ms'] + timings['search_ms']\n",
    "        \n",
    "        if return_timing:\n",
    "            return results, timings\n",
    "        return results\n",
    "\n",
    "\n",
    "# Create retriever\n",
    "retriever = TwoTowerRetriever(\n",
    "    model=model,\n",
    "    index=index,\n",
    "    user_features=user_genre_features,\n",
    "    movies_df=movies_df,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Retriever ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval\n",
    "user_id = 0\n",
    "results, timings = retriever.retrieve(user_id, k=10, return_timing=True)\n",
    "\n",
    "print(f\"üé¨ Top 10 Recommendations for User {user_id}:\")\n",
    "print(\"‚îÄ\" * 60)\n",
    "for item_id, score, title in results:\n",
    "    print(f\"  {score:.3f} | {title}\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Timing Breakdown:\")\n",
    "print(f\"   Query encoding: {timings['encode_ms']:.2f} ms\")\n",
    "print(f\"   Index search:   {timings['search_ms']:.2f} ms\")\n",
    "print(f\"   Total:          {timings['total_ms']:.2f} ms\")\n",
    "\n",
    "if timings['total_ms'] < 10:\n",
    "    print(f\"\\nüéâ Goal achieved! Sub-10ms latency!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark retrieval latency\n",
    "def benchmark_retrieval(retriever, num_users=100, k=100):\n",
    "    \"\"\"Benchmark retrieval latency over many users.\"\"\"\n",
    "    latencies = []\n",
    "    \n",
    "    for user_id in tqdm(range(num_users), desc='Benchmarking'):\n",
    "        _, timings = retriever.retrieve(user_id, k=k, return_timing=True)\n",
    "        latencies.append(timings['total_ms'])\n",
    "    \n",
    "    return {\n",
    "        'mean_ms': np.mean(latencies),\n",
    "        'p50_ms': np.percentile(latencies, 50),\n",
    "        'p95_ms': np.percentile(latencies, 95),\n",
    "        'p99_ms': np.percentile(latencies, 99),\n",
    "        'qps': 1000 / np.mean(latencies),\n",
    "    }\n",
    "\n",
    "benchmark = benchmark_retrieval(retriever, num_users=100, k=100)\n",
    "\n",
    "print(f\"\\nüìä Retrieval Benchmark (k=100):\")\n",
    "print(f\"   Mean latency:  {benchmark['mean_ms']:.2f} ms\")\n",
    "print(f\"   P50 latency:   {benchmark['p50_ms']:.2f} ms\")\n",
    "print(f\"   P95 latency:   {benchmark['p95_ms']:.2f} ms\")\n",
    "print(f\"   P99 latency:   {benchmark['p99_ms']:.2f} ms\")\n",
    "print(f\"   Throughput:    {benchmark['qps']:.0f} queries/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**Recap: t-SNE for Embedding Visualization**\n\nAs we learned in Lab B.1, t-SNE (from `sklearn.manifold`) reduces high-dimensional embeddings to 2D for visualization while preserving neighborhood structure. Similar items cluster together in the visualization.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize latency distribution\n",
    "latencies = []\n",
    "for user_id in range(200):\n",
    "    _, timings = retriever.retrieve(user_id, k=100, return_timing=True)\n",
    "    latencies.append(timings['total_ms'])\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(latencies, bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(x=10, color='red', linestyle='--', label='Target: 10ms')\n",
    "plt.axvline(x=np.mean(latencies), color='green', linestyle='--', label=f'Mean: {np.mean(latencies):.2f}ms')\n",
    "plt.xlabel('Latency (ms)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Retrieval Latency Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Analyzing Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Sample embeddings for visualization\n",
    "sample_size = 500\n",
    "sample_indices = np.random.choice(len(item_embeddings), sample_size, replace=False)\n",
    "sample_embeddings = item_embeddings[sample_indices]\n",
    "\n",
    "# Run t-SNE\n",
    "print(\"Running t-SNE (this takes a moment)...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "embeddings_2d = tsne.fit_transform(sample_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color by primary genre\n",
    "def get_genre(item_id):\n",
    "    genres = movies_df[movies_df['item_id'] == item_id]['genres'].values\n",
    "    if len(genres) > 0 and pd.notna(genres[0]):\n",
    "        return genres[0].split('|')[0]\n",
    "    return 'Unknown'\n",
    "\n",
    "genres = [get_genre(idx) for idx in sample_indices]\n",
    "unique_genres = list(set(genres))\n",
    "genre_to_color = {g: i for i, g in enumerate(unique_genres)}\n",
    "colors = [genre_to_color[g] for g in genres]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(\n",
    "    embeddings_2d[:, 0],\n",
    "    embeddings_2d[:, 1],\n",
    "    c=colors,\n",
    "    cmap='tab20',\n",
    "    alpha=0.6,\n",
    "    s=30\n",
    ")\n",
    "\n",
    "plt.title('Item Embeddings (t-SNE Visualization)\\nTwo-Tower Model')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "\n",
    "# Add legend for top genres\n",
    "top_genres = pd.Series(genres).value_counts().head(8).index\n",
    "handles = [plt.scatter([], [], c=[plt.cm.tab20(genre_to_color[g]/20)], label=g, s=100) \n",
    "           for g in top_genres]\n",
    "plt.legend(handles=handles, loc='upper right', title='Genre')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Notice how similar genres cluster together!\")\n",
    "print(\"   The two-tower model learned semantic structure from interactions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself!\n",
    "\n",
    "### Exercise 1: Temperature Tuning\n",
    "\n",
    "Try different temperature values (0.01, 0.07, 0.2, 0.5) and observe how it affects training.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "- Lower temperature = sharper softmax = more confident predictions\n",
    "- Higher temperature = softer softmax = more exploration\n",
    "- Too low: training instability, Too high: weak learning signal\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Batch Size Impact\n",
    "\n",
    "Try different batch sizes (64, 256, 1024) and measure the impact on training.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "Larger batch = more in-batch negatives = better gradient estimates\n",
    "But also = more GPU memory needed\n",
    "\n",
    "On DGX Spark with 128GB, you can use very large batches!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting to Normalize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Raw embeddings (magnitude varies)\n",
    "# output = self.mlp(combined)  # Magnitudes can be 0.5, 5.0, 50.0...\n",
    "# similarity = query @ item.T  # Dominated by large magnitudes!\n",
    "\n",
    "# ‚úÖ Right: L2 normalize (all vectors have magnitude 1)\n",
    "# output = F.normalize(self.mlp(combined), p=2, dim=-1)\n",
    "# similarity = query @ item.T  # Now this is cosine similarity!\n",
    "\n",
    "print(\"Why normalization matters:\")\n",
    "print(\"  Without: Dot product favors vectors with large magnitude\")\n",
    "print(\"  With:    Dot product = cosine similarity (direction only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Using the Same Batch for Queries and Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Potential Issue: All positive pairs are on the diagonal\n",
    "# If batch = [(u1,i1), (u2,i2), ...], then positive(u1) = i1, not i2\n",
    "# Make sure your loss function expects this!\n",
    "\n",
    "# ‚úÖ Right: Labels should be [0, 1, 2, ...] (diagonal indices)\n",
    "# labels = torch.arange(batch_size)\n",
    "# loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "print(\"The diagonal pattern:\")\n",
    "print(\"  logits[0,0] = similarity(user 0, item 0) ‚Üê POSITIVE\")\n",
    "print(\"  logits[0,1] = similarity(user 0, item 1) ‚Üê negative\")\n",
    "print(\"  logits[1,0] = similarity(user 1, item 0) ‚Üê negative\")\n",
    "print(\"  logits[1,1] = similarity(user 1, item 1) ‚Üê POSITIVE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Not Updating Item Index After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Use old embeddings after model update\n",
    "# model = train_more_epochs(model)\n",
    "# results = retriever.retrieve(user_id)  # Using stale index!\n",
    "\n",
    "# ‚úÖ Right: Rebuild index after training\n",
    "# model = train_more_epochs(model)\n",
    "# new_embeddings = compute_all_item_embeddings(model, item_features)\n",
    "# new_index = build_faiss_index(new_embeddings)\n",
    "# retriever = TwoTowerRetriever(model, new_index, ...)\n",
    "\n",
    "print(\"In production:\")\n",
    "print(\"  - Item embeddings are recomputed periodically (hourly/daily)\")\n",
    "print(\"  - Index is rebuilt and swapped atomically\")\n",
    "print(\"  - This is why item tower runs OFFLINE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ Why two-tower architecture scales to billions of items\n",
    "- ‚úÖ How to build query and item towers\n",
    "- ‚úÖ In-batch negative sampling for efficient training\n",
    "- ‚úÖ Building FAISS indices for fast retrieval\n",
    "- ‚úÖ Achieving sub-10ms retrieval latency\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Add Hard Negative Mining (20-30 min):**\n",
    "\n",
    "In-batch negatives are often \"easy\" - obviously wrong items. Hard negatives are items that are similar to positives but wrong.\n",
    "\n",
    "Implement:\n",
    "1. After each epoch, find top-K most similar items to each positive\n",
    "2. Add some of these as explicit hard negatives in the next epoch\n",
    "3. This typically improves retrieval quality by 5-15%!\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [Deep Neural Networks for YouTube Recommendations](https://research.google/pubs/pub45530/) - The foundational paper\n",
    "- [Sampling-Bias-Corrected Neural Modeling](https://research.google/pubs/pub48840/) - Fixing popularity bias\n",
    "- [FAISS Documentation](https://faiss.ai/) - Production-grade similarity search\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del model, index, retriever\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ GPU memory cleared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚û°Ô∏è Next Steps\n",
    "\n",
    "In the final notebook, we'll bring everything together with **comprehensive evaluation and analysis** - comparing all our models with proper ranking metrics!\n",
    "\n",
    "Continue to: **04-evaluation-and-analysis.ipynb**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}