{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab B.3 Solutions: Two-Tower Retrieval System\n",
    "\n",
    "Solutions to exercises from Lab B.3.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Temperature Tuning\n",
    "\n",
    "Temperature controls the sharpness of the softmax distribution in contrastive learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_temperature_effect(logits, temperatures):\n",
    "    \"\"\"\n",
    "    Visualize how temperature affects the softmax distribution.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(temperatures), figsize=(15, 4))\n",
    "    \n",
    "    for ax, temp in zip(axes, temperatures):\n",
    "        probs = F.softmax(logits / temp, dim=0).numpy()\n",
    "        ax.bar(range(len(probs)), probs)\n",
    "        ax.set_title(f'Temperature = {temp}')\n",
    "        ax.set_xlabel('Class')\n",
    "        ax.set_ylabel('Probability')\n",
    "        ax.set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example logits (similarity scores)\n",
    "logits = torch.tensor([2.0, 1.5, 0.5, 0.2, -0.5])\n",
    "temperatures = [0.01, 0.07, 0.2, 0.5, 1.0]\n",
    "\n",
    "visualize_temperature_effect(logits, temperatures)\n",
    "\n",
    "print(\"ðŸ“Š Observations:\")\n",
    "print(\"   - Low temp (0.01): Almost one-hot, very confident\")\n",
    "print(\"   - Medium temp (0.07): Sharp but smooth, good for learning\")\n",
    "print(\"   - High temp (0.5+): Soft distribution, weak learning signal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with different temperatures\n",
    "def train_with_temperature(temperature, batch_size=32, steps=100):\n",
    "    \"\"\"\n",
    "    Simulate training loss with different temperatures.\n",
    "    \"\"\"\n",
    "    # Simulate random embeddings\n",
    "    query_dim = 128\n",
    "    losses = []\n",
    "    \n",
    "    for step in range(steps):\n",
    "        # Random normalized embeddings\n",
    "        query = F.normalize(torch.randn(batch_size, query_dim), dim=1)\n",
    "        item = F.normalize(torch.randn(batch_size, query_dim), dim=1)\n",
    "        \n",
    "        # Add some positive signal (diagonal should be higher)\n",
    "        item = 0.8 * query + 0.2 * item\n",
    "        item = F.normalize(item, dim=1)\n",
    "        \n",
    "        # Compute similarity\n",
    "        logits = torch.matmul(query, item.T) / temperature\n",
    "        labels = torch.arange(batch_size)\n",
    "        \n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return losses\n",
    "\n",
    "\n",
    "# Compare temperatures\n",
    "temps_to_test = [0.01, 0.07, 0.2, 0.5]\n",
    "temp_results = {}\n",
    "\n",
    "for temp in temps_to_test:\n",
    "    losses = train_with_temperature(temp)\n",
    "    temp_results[temp] = losses\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "for temp, losses in temp_results.items():\n",
    "    plt.plot(losses, label=f'temp={temp}')\n",
    "\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss by Temperature')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Key insight: Lower temperature = lower loss but can be unstable\")\n",
    "print(\"   Recommended: 0.05-0.1 for most contrastive learning tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2 Solution: Batch Size Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_batch_size_negatives(batch_sizes):\n",
    "    \"\"\"\n",
    "    Analyze how batch size affects number of negatives.\n",
    "    \"\"\"\n",
    "    analysis = []\n",
    "    \n",
    "    for bs in batch_sizes:\n",
    "        num_positives = bs\n",
    "        num_negatives_per_positive = bs - 1\n",
    "        total_negative_pairs = bs * (bs - 1)\n",
    "        negative_ratio = num_negatives_per_positive / 1\n",
    "        \n",
    "        analysis.append({\n",
    "            'batch_size': bs,\n",
    "            'positives': num_positives,\n",
    "            'negatives_per_positive': num_negatives_per_positive,\n",
    "            'total_negative_pairs': total_negative_pairs,\n",
    "            'negative_ratio': f'{num_negatives_per_positive}:1'\n",
    "        })\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "\n",
    "batch_sizes = [32, 64, 128, 256, 512, 1024, 2048]\n",
    "analysis = analyze_batch_size_negatives(batch_sizes)\n",
    "\n",
    "print(\"In-Batch Negatives Analysis\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Batch Size':>12} | {'Negatives/Positive':>18} | {'Total Neg Pairs':>15}\")\n",
    "print(\"-\"*60)\n",
    "for row in analysis:\n",
    "    print(f\"{row['batch_size']:>12} | {row['negatives_per_positive']:>18} | {row['total_negative_pairs']:>15,}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Observation: Batch size 512 gives 511 negatives per positive - for free!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Negatives per positive\n",
    "negs = [a['negatives_per_positive'] for a in analysis]\n",
    "axes[0].bar([str(bs) for bs in batch_sizes], negs, color='steelblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Batch Size')\n",
    "axes[0].set_ylabel('Negatives per Positive')\n",
    "axes[0].set_title('In-Batch Negatives Scale with Batch Size')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# GPU memory estimate (rough)\n",
    "# Embedding dim = 128, FP32\n",
    "emb_dim = 128\n",
    "bytes_per_float = 4\n",
    "memory_mb = [(bs * emb_dim * bytes_per_float * 2) / (1024**2) for bs in batch_sizes]  # *2 for query+item\n",
    "\n",
    "axes[1].bar([str(bs) for bs in batch_sizes], memory_mb, color='coral', edgecolor='black')\n",
    "axes[1].set_xlabel('Batch Size')\n",
    "axes[1].set_ylabel('Embedding Memory (MB)')\n",
    "axes[1].set_title('Memory Usage by Batch Size')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ DGX Spark Advantage: With 128GB memory, you can use batch sizes of 4096+!\")\n",
    "print(\"   This gives 4095 negatives per positive - massive training signal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Hard Negative Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_negative_mining(query_embeddings, item_embeddings, positive_indices, k=5):\n",
    "    \"\"\"\n",
    "    Find hard negatives: items that are similar but not positive.\n",
    "    \n",
    "    Args:\n",
    "        query_embeddings: (num_queries, dim) tensor\n",
    "        item_embeddings: (num_items, dim) tensor\n",
    "        positive_indices: Dict mapping query_idx -> set of positive item indices\n",
    "        k: Number of hard negatives to find per query\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping query_idx -> list of hard negative indices\n",
    "    \"\"\"\n",
    "    # Compute all similarities\n",
    "    similarities = torch.matmul(query_embeddings, item_embeddings.T)\n",
    "    \n",
    "    hard_negatives = {}\n",
    "    \n",
    "    for query_idx in range(len(query_embeddings)):\n",
    "        scores = similarities[query_idx].clone()\n",
    "        \n",
    "        # Mask positives\n",
    "        for pos_idx in positive_indices.get(query_idx, set()):\n",
    "            scores[pos_idx] = -float('inf')\n",
    "        \n",
    "        # Get top-k negatives (highest scoring non-positives = hardest)\n",
    "        _, top_k = torch.topk(scores, k)\n",
    "        hard_negatives[query_idx] = top_k.tolist()\n",
    "    \n",
    "    return hard_negatives\n",
    "\n",
    "\n",
    "# Example\n",
    "num_queries = 10\n",
    "num_items = 100\n",
    "dim = 64\n",
    "\n",
    "queries = F.normalize(torch.randn(num_queries, dim), dim=1)\n",
    "items = F.normalize(torch.randn(num_items, dim), dim=1)\n",
    "\n",
    "# Assume each query has 1 positive\n",
    "positive_indices = {i: {i * 10} for i in range(num_queries)}\n",
    "\n",
    "hard_negs = hard_negative_mining(queries, items, positive_indices, k=5)\n",
    "\n",
    "print(\"Hard Negatives Found:\")\n",
    "for query_idx in range(3):\n",
    "    print(f\"  Query {query_idx}: Hard negatives = {hard_negs[query_idx]}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Use these hard negatives in the next training epoch for better learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Temperature**: 0.05-0.1 is typically optimal. Too low = unstable, too high = weak signal.\n",
    "\n",
    "2. **Batch size**: Larger is better for in-batch negatives. DGX Spark can go huge!\n",
    "\n",
    "3. **Hard negatives**: Mining similar-but-wrong items improves discrimination.\n",
    "\n",
    "4. **Memory**: Two-tower scales because item embeddings are pre-computed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
