{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab B.2 Solutions: Neural Collaborative Filtering\n",
    "\n",
    "This notebook contains complete solutions to the exercises from Lab B.2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "module_dir = Path.cwd().parent if 'solutions' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(module_dir / 'scripts'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from data_utils import download_movielens, leave_one_out_split\n",
    "from models import NeuMF\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "ratings_df, movies_df = download_movielens('100k')\n",
    "train_df, test_df = leave_one_out_split(ratings_df, by_time=True)\n",
    "\n",
    "num_users = ratings_df['user_id'].nunique()\n",
    "num_items = ratings_df['item_id'].nunique()\n",
    "\n",
    "user_positive_items = train_df.groupby('user_id')['item_id'].apply(set).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1 Solution: Different Numbers of Negative Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCFDataset(Dataset):\n",
    "    def __init__(self, interactions_df, num_items, user_positive_items, num_negatives=4):\n",
    "        self.users = interactions_df['user_id'].values\n",
    "        self.items = interactions_df['item_id'].values\n",
    "        self.num_items = num_items\n",
    "        self.user_positive_items = user_positive_items\n",
    "        self.num_negatives = num_negatives\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user = self.users[idx]\n",
    "        pos_item = self.items[idx]\n",
    "        \n",
    "        neg_items = []\n",
    "        positives = self.user_positive_items.get(user, set())\n",
    "        \n",
    "        while len(neg_items) < self.num_negatives:\n",
    "            neg_item = np.random.randint(0, self.num_items)\n",
    "            if neg_item not in positives:\n",
    "                neg_items.append(neg_item)\n",
    "        \n",
    "        users = [user] * (1 + self.num_negatives)\n",
    "        items = [pos_item] + neg_items\n",
    "        labels = [1.0] + [0.0] * self.num_negatives\n",
    "        \n",
    "        return (\n",
    "            torch.LongTensor(users),\n",
    "            torch.LongTensor(items),\n",
    "            torch.FloatTensor(labels)\n",
    "        )\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    users = torch.cat([b[0] for b in batch])\n",
    "    items = torch.cat([b[1] for b in batch])\n",
    "    labels = torch.cat([b[2] for b in batch])\n",
    "    return users, items, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_ncf(num_negatives, epochs=5):\n",
    "    \"\"\"Train NeuMF with specified number of negatives and return HR@10.\"\"\"\n",
    "    # Create dataset\n",
    "    dataset = NCFDataset(train_df, num_items, user_positive_items, num_negatives)\n",
    "    loader = DataLoader(dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    # Create model\n",
    "    model = NeuMF(num_users, num_items).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for users, items, labels in loader:\n",
    "            users, items, labels = users.to(device), items.to(device), labels.to(device)\n",
    "            predictions = model(users, items)\n",
    "            loss = F.binary_cross_entropy(predictions, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    hits = 0\n",
    "    test_users = test_df['user_id'].unique()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for user_id in test_users:\n",
    "            test_item = test_df[test_df['user_id'] == user_id]['item_id'].values[0]\n",
    "            scores = model.predict_all_items(user_id)\n",
    "            \n",
    "            train_items = list(user_positive_items.get(user_id, set()))\n",
    "            scores[train_items] = -float('inf')\n",
    "            \n",
    "            _, top_10 = torch.topk(scores, 10)\n",
    "            if test_item in top_10.cpu().numpy():\n",
    "                hits += 1\n",
    "    \n",
    "    return hits / len(test_users)\n",
    "\n",
    "\n",
    "# Test different numbers of negatives\n",
    "negative_counts = [1, 4, 8, 16]\n",
    "results = {}\n",
    "\n",
    "for num_neg in negative_counts:\n",
    "    print(f\"\\nTesting num_negatives = {num_neg}...\")\n",
    "    hr = train_and_evaluate_ncf(num_neg, epochs=5)\n",
    "    results[num_neg] = hr\n",
    "    print(f\"  HR@10: {hr:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*40)\n",
    "for num_neg, hr in sorted(results.items()):\n",
    "    print(f\"  {num_neg} negatives: HR@10 = {hr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar([str(n) for n in results.keys()], results.values(), color='steelblue', edgecolor='black')\n",
    "plt.xlabel('Number of Negative Samples')\n",
    "plt.ylabel('Hit Rate @ 10')\n",
    "plt.title('Effect of Negative Sampling on HR@10')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Observations:\")\n",
    "print(\"   - Too few negatives (1): Easy training, weaker discrimination\")\n",
    "print(\"   - Moderate (4-8): Good balance of difficulty and learning\")\n",
    "print(\"   - Many negatives (16): Harder but potentially better generalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2 Solution: Different MLP Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_architecture(mlp_layers, epochs=5):\n",
    "    \"\"\"Train NeuMF with specified MLP architecture.\"\"\"\n",
    "    dataset = NCFDataset(train_df, num_items, user_positive_items, num_negatives=4)\n",
    "    loader = DataLoader(dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    model = NeuMF(\n",
    "        num_users, num_items, \n",
    "        gmf_dim=32, mlp_dim=64, \n",
    "        mlp_layers=mlp_layers\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for users, items, labels in loader:\n",
    "            users, items, labels = users.to(device), items.to(device), labels.to(device)\n",
    "            predictions = model(users, items)\n",
    "            loss = F.binary_cross_entropy(predictions, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    hits = 0\n",
    "    test_users = test_df['user_id'].unique()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for user_id in test_users:\n",
    "            test_item = test_df[test_df['user_id'] == user_id]['item_id'].values[0]\n",
    "            scores = model.predict_all_items(user_id)\n",
    "            \n",
    "            train_items = list(user_positive_items.get(user_id, set()))\n",
    "            scores[train_items] = -float('inf')\n",
    "            \n",
    "            _, top_10 = torch.topk(scores, 10)\n",
    "            if test_item in top_10.cpu().numpy():\n",
    "                hits += 1\n",
    "    \n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    return hits / len(test_users), params\n",
    "\n",
    "\n",
    "# Test different architectures\n",
    "architectures = {\n",
    "    'Shallow [64, 32]': [64, 32],\n",
    "    'Standard [128, 64, 32]': [128, 64, 32],\n",
    "    'Deep [256, 128, 64, 32]': [256, 128, 64, 32],\n",
    "    'Wide [512, 256]': [512, 256],\n",
    "}\n",
    "\n",
    "arch_results = {}\n",
    "\n",
    "for name, layers in architectures.items():\n",
    "    print(f\"\\nTesting {name}...\")\n",
    "    hr, params = train_architecture(layers, epochs=5)\n",
    "    arch_results[name] = {'hr': hr, 'params': params}\n",
    "    print(f\"  HR@10: {hr:.4f}, Parameters: {params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize architecture comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "names = list(arch_results.keys())\n",
    "hrs = [arch_results[n]['hr'] for n in names]\n",
    "params = [arch_results[n]['params'] for n in names]\n",
    "\n",
    "# HR comparison\n",
    "axes[0].barh(names, hrs, color='steelblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Hit Rate @ 10')\n",
    "axes[0].set_title('Architecture Performance')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Parameters comparison\n",
    "axes[1].barh(names, [p/1000 for p in params], color='coral', edgecolor='black')\n",
    "axes[1].set_xlabel('Parameters (thousands)')\n",
    "axes[1].set_title('Model Size')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Observations:\")\n",
    "print(\"   - Deeper networks can learn more complex patterns\")\n",
    "print(\"   - But more parameters = more overfitting risk\")\n",
    "print(\"   - For small datasets, simpler architectures often win\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Pre-training GMF and MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original NeuMF paper suggests pre-training:\n",
    "# 1. Train GMF alone\n",
    "# 2. Train MLP alone\n",
    "# 3. Initialize NeuMF with their weights\n",
    "# 4. Fine-tune the combined model\n",
    "\n",
    "from models import GMF, MLP\n",
    "\n",
    "def pretrain_gmf(epochs=10):\n",
    "    \"\"\"Pre-train GMF model.\"\"\"\n",
    "    dataset = NCFDataset(train_df, num_items, user_positive_items, num_negatives=4)\n",
    "    loader = DataLoader(dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    model = GMF(num_users, num_items, embedding_dim=32).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for users, items, labels in loader:\n",
    "            users, items, labels = users.to(device), items.to(device), labels.to(device)\n",
    "            predictions = model(users, items)\n",
    "            loss = F.binary_cross_entropy(predictions, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def pretrain_mlp(epochs=10):\n",
    "    \"\"\"Pre-train MLP model.\"\"\"\n",
    "    dataset = NCFDataset(train_df, num_items, user_positive_items, num_negatives=4)\n",
    "    loader = DataLoader(dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    model = MLP(num_users, num_items, embedding_dim=64, hidden_layers=[128, 64, 32]).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for users, items, labels in loader:\n",
    "            users, items, labels = users.to(device), items.to(device), labels.to(device)\n",
    "            predictions = model(users, items)\n",
    "            loss = F.binary_cross_entropy(predictions, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"Pre-training GMF...\")\n",
    "pretrained_gmf = pretrain_gmf(epochs=5)\n",
    "\n",
    "print(\"Pre-training MLP...\")\n",
    "pretrained_mlp = pretrain_mlp(epochs=5)\n",
    "\n",
    "print(\"\\nâœ… Pre-training complete!\")\n",
    "print(\"\\nNext steps would be:\")\n",
    "print(\"  1. Copy GMF embeddings to NeuMF's GMF path\")\n",
    "print(\"  2. Copy MLP embeddings to NeuMF's MLP path\")\n",
    "print(\"  3. Fine-tune the combined model with smaller learning rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Negative sampling**: 4-8 negatives per positive is typically optimal\n",
    "\n",
    "2. **MLP depth**: Deeper isn't always better, especially for small datasets\n",
    "\n",
    "3. **Pre-training**: Can help but adds complexity. Usually worth it for large datasets.\n",
    "\n",
    "4. **Trade-offs**: Always balance model complexity vs dataset size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
