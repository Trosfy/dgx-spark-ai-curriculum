{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab B.4 Solutions: Evaluation and Analysis\n",
    "\n",
    "Solutions to exercises from Lab B.4.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: NDCG with Graded Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_graded(ranked_items, relevance_scores, k=10):\n",
    "    \"\"\"\n",
    "    NDCG with graded (non-binary) relevance.\n",
    "    \n",
    "    Args:\n",
    "        ranked_items: List of item IDs in ranked order\n",
    "        relevance_scores: Dict mapping item_id -> relevance score (e.g., rating)\n",
    "        k: Cutoff position\n",
    "    \n",
    "    Returns:\n",
    "        NDCG value between 0 and 1\n",
    "    \n",
    "    Example:\n",
    "        >>> ranked = [1, 2, 3, 4, 5]\n",
    "        >>> relevance = {1: 5, 2: 4, 3: 3, 5: 5}  # Item 4 has no relevance\n",
    "        >>> ndcg = ndcg_graded(ranked, relevance, k=5)\n",
    "    \"\"\"\n",
    "    # Get relevances for ranked items\n",
    "    relevances = []\n",
    "    for item in ranked_items[:k]:\n",
    "        relevances.append(relevance_scores.get(item, 0.0))\n",
    "    \n",
    "    relevances = np.array(relevances)\n",
    "    \n",
    "    # DCG: sum of relevance / log2(position + 1)\n",
    "    positions = np.arange(1, len(relevances) + 1)\n",
    "    discounts = np.log2(positions + 1)\n",
    "    dcg = np.sum(relevances / discounts)\n",
    "    \n",
    "    # Ideal DCG: best possible ranking\n",
    "    ideal_relevances = sorted(relevance_scores.values(), reverse=True)[:k]\n",
    "    if len(ideal_relevances) < k:\n",
    "        ideal_relevances = list(ideal_relevances) + [0.0] * (k - len(ideal_relevances))\n",
    "    \n",
    "    ideal_positions = np.arange(1, len(ideal_relevances) + 1)\n",
    "    ideal_discounts = np.log2(ideal_positions + 1)\n",
    "    idcg = np.sum(np.array(ideal_relevances) / ideal_discounts)\n",
    "    \n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dcg / idcg\n",
    "\n",
    "\n",
    "# Example with MovieLens ratings\n",
    "# Simulated: user rated items 10, 20, 30 with ratings 5, 3, 4\n",
    "relevance = {10: 5, 20: 3, 30: 4}\n",
    "\n",
    "# Perfect ranking (by relevance)\n",
    "perfect_ranking = [10, 30, 20, 1, 2, 3, 4, 5, 6, 7]  # 10 (5), 30 (4), 20 (3), rest 0\n",
    "ndcg_perfect = ndcg_graded(perfect_ranking, relevance, k=10)\n",
    "\n",
    "# Suboptimal ranking\n",
    "suboptimal_ranking = [20, 30, 10, 1, 2, 3, 4, 5, 6, 7]  # 20 (3), 30 (4), 10 (5) - reversed\n",
    "ndcg_suboptimal = ndcg_graded(suboptimal_ranking, relevance, k=10)\n",
    "\n",
    "# Bad ranking (relevant items at bottom)\n",
    "bad_ranking = [1, 2, 3, 4, 5, 6, 7, 10, 20, 30]\n",
    "ndcg_bad = ndcg_graded(bad_ranking, relevance, k=10)\n",
    "\n",
    "print(\"NDCG with Graded Relevance:\")\n",
    "print(f\"  Perfect ranking:    NDCG = {ndcg_perfect:.4f}\")\n",
    "print(f\"  Suboptimal ranking: NDCG = {ndcg_suboptimal:.4f}\")\n",
    "print(f\"  Bad ranking:        NDCG = {ndcg_bad:.4f}\")\n",
    "\n",
    "print(\"\\n Graded relevance penalizes putting 5-star items lower than 3-star items!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2 Solution: MMR Re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmr_rerank(item_scores, item_embeddings, lambda_param=0.5, k=10):\n",
    "    \"\"\"\n",
    "    Maximal Marginal Relevance re-ranking for diversity.\n",
    "    \n",
    "    MMR = argmax [lambda * Rel(d,q) - (1-lambda) * max_s Sim(d,s)]\n",
    "    \n",
    "    Args:\n",
    "        item_scores: Dict mapping item_id -> relevance score\n",
    "        item_embeddings: Dict mapping item_id -> embedding vector\n",
    "        lambda_param: Trade-off between relevance (1) and diversity (0)\n",
    "        k: Number of items to select\n",
    "    \n",
    "    Returns:\n",
    "        List of selected item_ids\n",
    "    \"\"\"\n",
    "    selected = []\n",
    "    candidates = list(item_scores.keys())\n",
    "    \n",
    "    for _ in range(k):\n",
    "        if not candidates:\n",
    "            break\n",
    "        \n",
    "        best_item = None\n",
    "        best_mmr = -float('inf')\n",
    "        \n",
    "        for item in candidates:\n",
    "            # Relevance term\n",
    "            relevance = item_scores[item]\n",
    "            \n",
    "            # Diversity term: max similarity to already selected\n",
    "            if selected:\n",
    "                similarities = [\n",
    "                    cosine_similarity(item_embeddings[item], item_embeddings[s])\n",
    "                    for s in selected\n",
    "                ]\n",
    "                max_sim = max(similarities)\n",
    "            else:\n",
    "                max_sim = 0\n",
    "            \n",
    "            # MMR score\n",
    "            mmr = lambda_param * relevance - (1 - lambda_param) * max_sim\n",
    "            \n",
    "            if mmr > best_mmr:\n",
    "                best_mmr = mmr\n",
    "                best_item = item\n",
    "        \n",
    "        selected.append(best_item)\n",
    "        candidates.remove(best_item)\n",
    "    \n",
    "    return selected\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-8)\n",
    "\n",
    "\n",
    "# Example\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create some items with embeddings\n",
    "# Group 1: Action movies (similar embeddings)\n",
    "# Group 2: Comedies (similar embeddings)\n",
    "# Group 3: Documentaries (similar embeddings)\n",
    "\n",
    "item_embeddings = {}\n",
    "item_scores = {}\n",
    "\n",
    "# Action movies (items 0-4)\n",
    "action_base = np.random.randn(16)\n",
    "for i in range(5):\n",
    "    item_embeddings[i] = action_base + np.random.randn(16) * 0.1\n",
    "    item_scores[i] = 0.9 - i * 0.02  # High scores\n",
    "\n",
    "# Comedies (items 5-9)\n",
    "comedy_base = np.random.randn(16)\n",
    "for i in range(5, 10):\n",
    "    item_embeddings[i] = comedy_base + np.random.randn(16) * 0.1\n",
    "    item_scores[i] = 0.85 - (i-5) * 0.02  # Medium-high scores\n",
    "\n",
    "# Documentaries (items 10-14)\n",
    "doc_base = np.random.randn(16)\n",
    "for i in range(10, 15):\n",
    "    item_embeddings[i] = doc_base + np.random.randn(16) * 0.1\n",
    "    item_scores[i] = 0.7 - (i-10) * 0.02  # Medium scores\n",
    "\n",
    "# Pure relevance ranking\n",
    "relevance_ranking = sorted(item_scores.keys(), key=lambda x: -item_scores[x])[:10]\n",
    "\n",
    "# MMR re-ranking with different lambdas\n",
    "mmr_high_relevance = mmr_rerank(item_scores, item_embeddings, lambda_param=0.9, k=10)\n",
    "mmr_balanced = mmr_rerank(item_scores, item_embeddings, lambda_param=0.5, k=10)\n",
    "mmr_high_diversity = mmr_rerank(item_scores, item_embeddings, lambda_param=0.2, k=10)\n",
    "\n",
    "print(\"Comparison of Rankings:\")\n",
    "print(f\"  Pure relevance:    {relevance_ranking}\")\n",
    "print(f\"  MMR (位=0.9):       {mmr_high_relevance}\")\n",
    "print(f\"  MMR (位=0.5):       {mmr_balanced}\")\n",
    "print(f\"  MMR (位=0.2):       {mmr_high_diversity}\")\n",
    "\n",
    "# Count genres in each\n",
    "def count_genres(ranking):\n",
    "    action = sum(1 for r in ranking if r < 5)\n",
    "    comedy = sum(1 for r in ranking if 5 <= r < 10)\n",
    "    docs = sum(1 for r in ranking if r >= 10)\n",
    "    return f\"Action:{action}, Comedy:{comedy}, Docs:{docs}\"\n",
    "\n",
    "print(\"\\nGenre Distribution:\")\n",
    "print(f\"  Pure relevance: {count_genres(relevance_ranking)}\")\n",
    "print(f\"  MMR (位=0.5):    {count_genres(mmr_balanced)}\")\n",
    "print(f\"  MMR (位=0.2):    {count_genres(mmr_high_diversity)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trade-off\n",
    "lambdas = np.arange(0, 1.05, 0.1)\n",
    "diversities = []\n",
    "avg_relevances = []\n",
    "\n",
    "for lam in lambdas:\n",
    "    ranking = mmr_rerank(item_scores, item_embeddings, lambda_param=lam, k=10)\n",
    "    \n",
    "    # Measure diversity (number of unique genres)\n",
    "    genres = set()\n",
    "    for item in ranking:\n",
    "        if item < 5:\n",
    "            genres.add('action')\n",
    "        elif item < 10:\n",
    "            genres.add('comedy')\n",
    "        else:\n",
    "            genres.add('docs')\n",
    "    diversities.append(len(genres))\n",
    "    \n",
    "    # Average relevance\n",
    "    avg_rel = np.mean([item_scores[i] for i in ranking])\n",
    "    avg_relevances.append(avg_rel)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax1.plot(lambdas, avg_relevances, 'b-o', label='Avg Relevance')\n",
    "ax1.set_xlabel('Lambda (位)')\n",
    "ax1.set_ylabel('Average Relevance', color='b')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(lambdas, diversities, 'r-s', label='Genre Diversity')\n",
    "ax2.set_ylabel('Number of Genres', color='r')\n",
    "ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "plt.title('MMR Trade-off: Relevance vs Diversity')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Observations:\")\n",
    "print(\"   - 位=1: Pure relevance, low diversity (all action movies)\")\n",
    "print(\"   - 位=0.5: Balanced - good relevance AND diversity\")\n",
    "print(\"   - 位=0: Pure diversity, lower relevance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Graded NDCG**: Uses actual ratings instead of binary relevance. Better for explicit feedback.\n",
    "\n",
    "2. **MMR**: Balances relevance and diversity with 位 parameter. 位=0.5-0.7 is typical.\n",
    "\n",
    "3. **Trade-offs**: There's always a trade-off between accuracy and diversity. Choose based on business goals.\n",
    "\n",
    "4. **Evaluation**: Use multiple metrics. No single metric captures everything!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
