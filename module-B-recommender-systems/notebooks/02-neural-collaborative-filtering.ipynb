{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab B.2: Neural Collaborative Filtering\n",
    "\n",
    "**Module:** B - Recommender Systems  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand the limitations of linear matrix factorization\n",
    "- [ ] Implement Generalized Matrix Factorization (GMF)\n",
    "- [ ] Implement Multi-Layer Perceptron (MLP) for recommendations\n",
    "- [ ] Build the full NeuMF architecture combining GMF and MLP\n",
    "- [ ] Train with implicit feedback and negative sampling\n",
    "- [ ] Evaluate with Hit Rate @ K\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Lab B.1 (Collaborative Filtering Fundamentals)\n",
    "- Completed: Module 2.1 (PyTorch Fundamentals)\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The Problem with Clicks:** Most recommendation systems don't have explicit ratings - they only know what users clicked/watched/bought (implicit feedback). You don't know what rating a user *would* give; you only know they interacted.\n",
    "\n",
    "**Neural Collaborative Filtering (NeuMF)** was published by He et al. at WWW 2017 and showed that neural networks can learn more complex user-item interactions than simple matrix factorization.\n",
    "\n",
    "NeuMF and its variants power:\n",
    "- üéµ **Spotify**: Combining collaborative signals with content features\n",
    "- üì∫ **YouTube**: Deep neural networks for candidate generation\n",
    "- üõí **Alibaba**: Neural networks for e-commerce recommendations\n",
    "\n",
    "---\n",
    "\n",
    "## üßí ELI5: Why Neural Networks?\n",
    "\n",
    "> **The Dot Product Limitation:**\n",
    ">\n",
    "> Remember matrix factorization? It predicts ratings using:\n",
    "> $$\\text{rating} = \\text{user\\_vector} \\cdot \\text{item\\_vector}$$\n",
    ">\n",
    "> This is just a weighted sum - a **linear** operation. It can't capture complex patterns like:\n",
    "> - \"User likes action movies, EXCEPT when they're also romantic\"\n",
    "> - \"User watches comedies on weekends but documentaries on weeknights\"\n",
    ">\n",
    "> **Neural Networks to the Rescue:**\n",
    ">\n",
    "> Instead of a simple dot product, we pass the user and item embeddings through \n",
    "> neural network layers that can learn **non-linear** relationships!\n",
    ">\n",
    "> Think of it like this:\n",
    "> - **Matrix Factorization**: \"Do these puzzle pieces fit?\" (yes/no)\n",
    "> - **Neural CF**: \"Let me examine these pieces under a microscope, rotate them, \n",
    ">   and figure out exactly HOW they might fit together.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "module_dir = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(module_dir / 'scripts'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from data_utils import download_movielens, leave_one_out_split\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "ratings_df, movies_df = download_movielens('100k')\n",
    "\n",
    "# For implicit feedback, convert to binary (1 = interacted)\n",
    "# We treat any rating as a positive signal\n",
    "ratings_df['label'] = 1\n",
    "\n",
    "num_users = ratings_df['user_id'].nunique()\n",
    "num_items = ratings_df['item_id'].nunique()\n",
    "\n",
    "print(f\"üìä Dataset:\")\n",
    "print(f\"   Users: {num_users}\")\n",
    "print(f\"   Items: {num_items}\")\n",
    "print(f\"   Interactions: {len(ratings_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-One-Out Evaluation Protocol\n",
    "\n",
    "For implicit feedback, we use **leave-one-out evaluation**:\n",
    "1. For each user, hold out their last interaction for testing\n",
    "2. Train on all other interactions\n",
    "3. At test time, rank ALL items and see if the held-out item appears in top K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave-one-out split\n",
    "train_df, test_df = leave_one_out_split(ratings_df, by_time=True)\n",
    "\n",
    "print(f\"üìä Data Split:\")\n",
    "print(f\"   Training interactions: {len(train_df):,}\")\n",
    "print(f\"   Test interactions: {len(test_df):,} (one per user)\")\n",
    "\n",
    "# Build user -> positive items mapping for negative sampling\n",
    "user_positive_items = train_df.groupby('user_id')['item_id'].apply(set).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Negative Sampling Dataset\n",
    "\n",
    "For implicit feedback, we only have positive examples (things users clicked). We need to generate **negative samples** - items the user probably doesn't like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCFDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Neural Collaborative Filtering with negative sampling.\n",
    "    \n",
    "    For each positive (user, item) pair, we sample negative items\n",
    "    that the user hasn't interacted with.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, interactions_df, num_items, user_positive_items, num_negatives=4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            interactions_df: DataFrame with user_id, item_id columns\n",
    "            num_items: Total number of items\n",
    "            user_positive_items: Dict mapping user_id -> set of positive item_ids\n",
    "            num_negatives: Number of negative samples per positive\n",
    "        \"\"\"\n",
    "        self.users = interactions_df['user_id'].values\n",
    "        self.items = interactions_df['item_id'].values\n",
    "        self.num_items = num_items\n",
    "        self.user_positive_items = user_positive_items\n",
    "        self.num_negatives = num_negatives\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user = self.users[idx]\n",
    "        pos_item = self.items[idx]\n",
    "        \n",
    "        # Sample negative items\n",
    "        neg_items = []\n",
    "        positives = self.user_positive_items.get(user, set())\n",
    "        \n",
    "        while len(neg_items) < self.num_negatives:\n",
    "            neg_item = np.random.randint(0, self.num_items)\n",
    "            if neg_item not in positives and neg_item not in neg_items:\n",
    "                neg_items.append(neg_item)\n",
    "        \n",
    "        # Return: users, items, labels\n",
    "        # 1 positive + num_negatives negatives\n",
    "        users = [user] * (1 + self.num_negatives)\n",
    "        items = [pos_item] + neg_items\n",
    "        labels = [1.0] + [0.0] * self.num_negatives\n",
    "        \n",
    "        return (\n",
    "            torch.LongTensor(users),\n",
    "            torch.LongTensor(items),\n",
    "            torch.FloatTensor(labels)\n",
    "        )\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Flatten batch into single tensors.\"\"\"\n",
    "    users = torch.cat([b[0] for b in batch])\n",
    "    items = torch.cat([b[1] for b in batch])\n",
    "    labels = torch.cat([b[2] for b in batch])\n",
    "    return users, items, labels\n",
    "\n",
    "\n",
    "# Create dataset and loader\n",
    "num_negatives = 4  # 4 negatives per positive (common choice)\n",
    "\n",
    "train_dataset = NCFDataset(\n",
    "    train_df, \n",
    "    num_items, \n",
    "    user_positive_items, \n",
    "    num_negatives=num_negatives\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=256, \n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Quick test\n",
    "users, items, labels = next(iter(train_loader))\n",
    "print(f\"‚úÖ Batch shape: users={users.shape}, items={items.shape}, labels={labels.shape}\")\n",
    "print(f\"   Positive ratio: {labels.mean():.2%} (expected: {1/(1+num_negatives):.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Generalized Matrix Factorization (GMF)\n",
    "\n",
    "GMF is matrix factorization with a twist: instead of a fixed dot product, we learn how to combine the element-wise product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMF(nn.Module):\n",
    "    \"\"\"\n",
    "    Generalized Matrix Factorization.\n",
    "    \n",
    "    Instead of: sum(user_emb * item_emb)  [fixed dot product]\n",
    "    We learn:   linear(user_emb * item_emb)  [learnable combination]\n",
    "    \n",
    "    This allows the model to weight different latent dimensions differently.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_users, num_items, embedding_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        self.output = nn.Linear(embedding_dim, 1)\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_embedding.weight, std=0.01)\n",
    "        nn.init.xavier_uniform_(self.output.weight)\n",
    "        \n",
    "    def forward(self, user_ids, item_ids):\n",
    "        user_emb = self.user_embedding(user_ids)\n",
    "        item_emb = self.item_embedding(item_ids)\n",
    "        \n",
    "        # Element-wise product (Hadamard product)\n",
    "        element_product = user_emb * item_emb\n",
    "        \n",
    "        # Learned linear combination + sigmoid for probability\n",
    "        output = torch.sigmoid(self.output(element_product))\n",
    "        \n",
    "        return output.squeeze()\n",
    "\n",
    "\n",
    "# Test GMF\n",
    "gmf_model = GMF(num_users, num_items, embedding_dim=32).to(device)\n",
    "test_users = torch.LongTensor([0, 1, 2]).to(device)\n",
    "test_items = torch.LongTensor([10, 20, 30]).to(device)\n",
    "test_output = gmf_model(test_users, test_items)\n",
    "\n",
    "print(f\"‚úÖ GMF output: {test_output.cpu().detach().numpy()}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in gmf_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Multi-Layer Perceptron (MLP) Tower\n",
    "\n",
    "The MLP tower concatenates user and item embeddings, then passes them through fully-connected layers to learn non-linear interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron for learning non-linear user-item interactions.\n",
    "    \n",
    "    Concatenates user and item embeddings, then passes through MLP layers.\n",
    "    The MLP can learn complex patterns that GMF cannot.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_users, num_items, embedding_dim=64, \n",
    "                 hidden_layers=[128, 64, 32], dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # Build MLP layers\n",
    "        layers = []\n",
    "        input_dim = embedding_dim * 2  # Concatenated user + item\n",
    "        \n",
    "        for hidden_dim in hidden_layers:\n",
    "            layers.extend([\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            input_dim = hidden_dim\n",
    "            \n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.output = nn.Linear(hidden_layers[-1], 1)\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_embedding.weight, std=0.01)\n",
    "        for layer in self.mlp:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "        nn.init.xavier_uniform_(self.output.weight)\n",
    "        \n",
    "    def forward(self, user_ids, item_ids):\n",
    "        user_emb = self.user_embedding(user_ids)\n",
    "        item_emb = self.item_embedding(item_ids)\n",
    "        \n",
    "        # Concatenate user and item embeddings\n",
    "        concat = torch.cat([user_emb, item_emb], dim=1)\n",
    "        \n",
    "        # Pass through MLP\n",
    "        hidden = self.mlp(concat)\n",
    "        output = torch.sigmoid(self.output(hidden))\n",
    "        \n",
    "        return output.squeeze()\n",
    "\n",
    "\n",
    "# Test MLP\n",
    "mlp_model = MLP(num_users, num_items, embedding_dim=32).to(device)\n",
    "test_output = mlp_model(test_users, test_items)\n",
    "\n",
    "print(f\"‚úÖ MLP output: {test_output.cpu().detach().numpy()}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in mlp_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: NeuMF = GMF + MLP\n",
    "\n",
    "The key insight of NeuMF: combine the linear power of GMF with the non-linear expressiveness of MLP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuMF(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Matrix Factorization (NeuMF).\n",
    "    \n",
    "    Combines GMF (linear interactions) with MLP (non-linear interactions)\n",
    "    to get the best of both worlds.\n",
    "    \n",
    "    Architecture:\n",
    "    1. GMF pathway: user_emb * item_emb -> linear\n",
    "    2. MLP pathway: concat(user_emb, item_emb) -> MLP layers\n",
    "    3. Concatenate GMF and MLP outputs -> final prediction\n",
    "    \n",
    "    Paper: \"Neural Collaborative Filtering\" (He et al., WWW 2017)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_users, num_items, \n",
    "                 gmf_dim=32, mlp_dim=64, \n",
    "                 mlp_layers=[128, 64, 32], \n",
    "                 dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        \n",
    "        # GMF pathway - separate embeddings\n",
    "        self.gmf_user_emb = nn.Embedding(num_users, gmf_dim)\n",
    "        self.gmf_item_emb = nn.Embedding(num_items, gmf_dim)\n",
    "        \n",
    "        # MLP pathway - separate embeddings\n",
    "        self.mlp_user_emb = nn.Embedding(num_users, mlp_dim)\n",
    "        self.mlp_item_emb = nn.Embedding(num_items, mlp_dim)\n",
    "        \n",
    "        # MLP layers\n",
    "        layers = []\n",
    "        input_dim = mlp_dim * 2\n",
    "        \n",
    "        for hidden_dim in mlp_layers:\n",
    "            layers.extend([\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            input_dim = hidden_dim\n",
    "            \n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        \n",
    "        # Final prediction: GMF output (gmf_dim) + MLP output (mlp_layers[-1])\n",
    "        self.output = nn.Linear(gmf_dim + mlp_layers[-1], 1)\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for emb in [self.gmf_user_emb, self.gmf_item_emb, \n",
    "                    self.mlp_user_emb, self.mlp_item_emb]:\n",
    "            nn.init.normal_(emb.weight, std=0.01)\n",
    "        for layer in self.mlp:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "        nn.init.xavier_uniform_(self.output.weight)\n",
    "        \n",
    "    def forward(self, user_ids, item_ids):\n",
    "        # GMF pathway: element-wise product\n",
    "        gmf_user = self.gmf_user_emb(user_ids)\n",
    "        gmf_item = self.gmf_item_emb(item_ids)\n",
    "        gmf_output = gmf_user * gmf_item  # Shape: (batch, gmf_dim)\n",
    "        \n",
    "        # MLP pathway: concatenate and pass through MLP\n",
    "        mlp_user = self.mlp_user_emb(user_ids)\n",
    "        mlp_item = self.mlp_item_emb(item_ids)\n",
    "        mlp_input = torch.cat([mlp_user, mlp_item], dim=1)\n",
    "        mlp_output = self.mlp(mlp_input)  # Shape: (batch, mlp_layers[-1])\n",
    "        \n",
    "        # Combine GMF and MLP\n",
    "        combined = torch.cat([gmf_output, mlp_output], dim=1)\n",
    "        prediction = torch.sigmoid(self.output(combined))\n",
    "        \n",
    "        return prediction.squeeze()\n",
    "    \n",
    "    def predict_all_items(self, user_id):\n",
    "        \"\"\"Predict scores for all items for a single user.\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            user_ids = torch.LongTensor([user_id] * self.num_items).to(\n",
    "                next(self.parameters()).device\n",
    "            )\n",
    "            item_ids = torch.arange(self.num_items).to(\n",
    "                next(self.parameters()).device\n",
    "            )\n",
    "            return self(user_ids, item_ids)\n",
    "\n",
    "\n",
    "# Test NeuMF\n",
    "neumf_model = NeuMF(num_users, num_items).to(device)\n",
    "test_output = neumf_model(test_users, test_items)\n",
    "\n",
    "print(f\"‚úÖ NeuMF output: {test_output.cpu().detach().numpy()}\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in neumf_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Architecture Visualization\n",
    "\n",
    "```\n",
    "User ID ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ GMF User Emb ‚îÄ‚îÄ‚îê\n",
    "          ‚îÇ                  ‚îú‚îÄ‚îÄ Element-wise √ó ‚îÄ‚îÄ‚îê\n",
    "Item ID ‚îÄ‚îÄ‚îº‚îÄ‚îÄ GMF Item Emb ‚îÄ‚îÄ‚îò                    ‚îÇ\n",
    "          ‚îÇ                                       ‚îú‚îÄ‚îÄ Concat ‚îÄ‚îÄ Linear ‚îÄ‚îÄ Sigmoid ‚îÄ‚îÄ Output\n",
    "          ‚îú‚îÄ‚îÄ MLP User Emb ‚îÄ‚îÄ‚îê                    ‚îÇ\n",
    "          ‚îÇ                  ‚îú‚îÄ‚îÄ Concat ‚îÄ‚îÄ MLP ‚îÄ‚îÄ‚îÄ‚îò\n",
    "          ‚îî‚îÄ‚îÄ MLP Item Emb ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ncf(model, train_loader, optimizer, device, epoch):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
    "    for users, items, labels in pbar:\n",
    "        users = users.to(device)\n",
    "        items = items.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        predictions = model(users, items)\n",
    "        loss = F.binary_cross_entropy(predictions, labels)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * len(users)\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate_hit_rate(model, test_df, user_positive_items, k=10):\n",
    "    \"\"\"\n",
    "    Evaluate Hit Rate @ K.\n",
    "    \n",
    "    For each user:\n",
    "    1. Predict scores for all items\n",
    "    2. Check if the held-out test item is in top K\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    hits = 0\n",
    "    \n",
    "    # Group test by user\n",
    "    test_users = test_df['user_id'].unique()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for user_id in tqdm(test_users, desc=f'Evaluating HR@{k}'):\n",
    "            # Get test item for this user\n",
    "            test_item = test_df[test_df['user_id'] == user_id]['item_id'].values[0]\n",
    "            \n",
    "            # Predict all items\n",
    "            scores = model.predict_all_items(user_id)\n",
    "            \n",
    "            # Mask out training items (we can't recommend items already interacted with)\n",
    "            train_items = list(user_positive_items.get(user_id, set()))\n",
    "            scores[train_items] = -float('inf')\n",
    "            \n",
    "            # Get top K\n",
    "            _, top_k_items = torch.topk(scores, k)\n",
    "            \n",
    "            # Check if test item is in top K\n",
    "            if test_item in top_k_items.cpu().numpy():\n",
    "                hits += 1\n",
    "    \n",
    "    return hits / len(test_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = NeuMF(\n",
    "    num_users=num_users,\n",
    "    num_items=num_items,\n",
    "    gmf_dim=32,\n",
    "    mlp_dim=64,\n",
    "    mlp_layers=[128, 64, 32],\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"üéØ Target: Hit Rate @ 10 > 0.65\")\n",
    "print(f\"   Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "hit_rates = []\n",
    "best_hr = 0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Train\n",
    "    train_loss = train_ncf(model, train_loader, optimizer, device, epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Evaluate every 2 epochs (evaluation is slow)\n",
    "    if epoch % 2 == 0 or epoch == num_epochs:\n",
    "        hr = evaluate_hit_rate(model, test_df, user_positive_items, k=10)\n",
    "        hit_rates.append((epoch, hr))\n",
    "        \n",
    "        if hr > best_hr:\n",
    "            best_hr = hr\n",
    "            best_epoch = epoch\n",
    "        \n",
    "        print(f\"\\nüìä Epoch {epoch}: Loss={train_loss:.4f}, HR@10={hr:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"üèÜ Best HR@10: {best_hr:.4f} (Epoch {best_epoch})\")\n",
    "if best_hr > 0.65:\n",
    "    print(f\"üéâ Goal achieved! HR@10 > 0.65\")\n",
    "else:\n",
    "    print(f\"üìà Keep tuning! Try more epochs or adjust hyperparameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Training loss\n",
    "axes[0].plot(range(1, len(train_losses)+1), train_losses, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Loss (BCE)')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Hit Rate\n",
    "epochs, hrs = zip(*hit_rates)\n",
    "axes[1].plot(epochs, hrs, 'go-', linewidth=2, markersize=8)\n",
    "axes[1].axhline(y=0.65, color='red', linestyle='--', label='Target: 0.65')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Hit Rate @ 10')\n",
    "axes[1].set_title('Hit Rate @ 10')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Comparing Models\n",
    "\n",
    "Let's compare GMF, MLP, and NeuMF to see which performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model_class, model_kwargs, train_loader, test_df, \n",
    "                             user_positive_items, device, epochs=10):\n",
    "    \"\"\"\n",
    "    Train a model and return its best hit rate.\n",
    "    \"\"\"\n",
    "    model = model_class(**model_kwargs).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    best_hr = 0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Train (silently)\n",
    "        model.train()\n",
    "        for users, items, labels in train_loader:\n",
    "            users, items, labels = users.to(device), items.to(device), labels.to(device)\n",
    "            predictions = model(users, items)\n",
    "            loss = F.binary_cross_entropy(predictions, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluate on last epoch\n",
    "        if epoch == epochs:\n",
    "            hr = evaluate_hit_rate(model, test_df, user_positive_items, k=10)\n",
    "            best_hr = hr\n",
    "    \n",
    "    return best_hr, model\n",
    "\n",
    "\n",
    "print(\"üî¨ Comparing Models (this takes a few minutes)...\\n\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "# GMF\n",
    "print(\"Training GMF...\")\n",
    "gmf_hr, _ = train_and_evaluate_model(\n",
    "    GMF, \n",
    "    {'num_users': num_users, 'num_items': num_items, 'embedding_dim': 64},\n",
    "    train_loader, test_df, user_positive_items, device, epochs=10\n",
    ")\n",
    "results['GMF'] = gmf_hr\n",
    "print(f\"  GMF HR@10: {gmf_hr:.4f}\")\n",
    "\n",
    "# MLP\n",
    "print(\"\\nTraining MLP...\")\n",
    "mlp_hr, _ = train_and_evaluate_model(\n",
    "    MLP,\n",
    "    {'num_users': num_users, 'num_items': num_items, 'embedding_dim': 64},\n",
    "    train_loader, test_df, user_positive_items, device, epochs=10\n",
    ")\n",
    "results['MLP'] = mlp_hr\n",
    "print(f\"  MLP HR@10: {mlp_hr:.4f}\")\n",
    "\n",
    "# NeuMF (already trained)\n",
    "results['NeuMF'] = best_hr\n",
    "print(f\"\\n  NeuMF HR@10: {best_hr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "models = list(results.keys())\n",
    "hrs = [results[m] for m in models]\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "\n",
    "bars = plt.bar(models, hrs, color=colors, edgecolor='black', linewidth=2)\n",
    "plt.axhline(y=0.65, color='gray', linestyle='--', label='Target: 0.65')\n",
    "\n",
    "# Add value labels\n",
    "for bar, hr in zip(bars, hrs):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{hr:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.ylabel('Hit Rate @ 10')\n",
    "plt.title('Model Comparison: Hit Rate @ 10')\n",
    "plt.legend()\n",
    "plt.ylim(0, max(hrs) + 0.1)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"   GMF:    {results['GMF']:.4f}\")\n",
    "print(f\"   MLP:    {results['MLP']:.4f}\")\n",
    "print(f\"   NeuMF:  {results['NeuMF']:.4f}\")\n",
    "print(f\"\\n   NeuMF combines the strengths of both approaches!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself!\n",
    "\n",
    "### Exercise 1: More Negative Samples\n",
    "\n",
    "Try different numbers of negative samples (1, 4, 8, 16) and see how it affects training.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "More negatives = harder training but better discrimination.\n",
    "Too few = easy training but poor generalization.\n",
    "Typical values: 4-10 negatives per positive.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Deeper MLP\n",
    "\n",
    "Try different MLP architectures:\n",
    "- Shallow: [64, 32]\n",
    "- Deep: [256, 128, 64, 32]\n",
    "- Wide: [512, 256]\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "Deeper networks can learn more complex patterns but:\n",
    "- Take longer to train\n",
    "- Risk overfitting on small datasets\n",
    "- May need more regularization\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting to Mask Training Items During Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Recommend items already seen\n",
    "# scores = model.predict_all_items(user_id)\n",
    "# top_k = torch.topk(scores, 10)  # Might include items user already rated!\n",
    "\n",
    "# ‚úÖ Right: Mask out training items\n",
    "# train_items = list(user_positive_items.get(user_id, set()))\n",
    "# scores[train_items] = -float('inf')  # These won't be in top K\n",
    "# top_k = torch.topk(scores, 10)\n",
    "\n",
    "print(\"Why masking matters:\")\n",
    "print(\"  Without masking: HR@10 might be 95%+ (trivially recommending known items)\")\n",
    "print(\"  With masking: HR@10 is the true measure of discovery\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Using the Same Embeddings for GMF and MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Shared embeddings\n",
    "# self.user_emb = nn.Embedding(num_users, dim)  # Used by both GMF and MLP\n",
    "\n",
    "# ‚úÖ Right: Separate embeddings for each pathway\n",
    "# self.gmf_user_emb = nn.Embedding(num_users, gmf_dim)\n",
    "# self.mlp_user_emb = nn.Embedding(num_users, mlp_dim)\n",
    "\n",
    "print(\"Why separate embeddings?\")\n",
    "print(\"  GMF learns embeddings optimized for element-wise product\")\n",
    "print(\"  MLP learns embeddings optimized for concatenation + non-linear transform\")\n",
    "print(\"  These are different objectives - let each pathway specialize!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Not Shuffling Negative Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Pre-generate all negatives (same negatives every epoch)\n",
    "# negatives = generate_negatives_once()  # Fixed negatives\n",
    "\n",
    "# ‚úÖ Right: Sample fresh negatives each epoch (as we do in NCFDataset)\n",
    "# Negatives are sampled in __getitem__, so each epoch sees different negatives\n",
    "\n",
    "print(\"Why fresh negatives matter:\")\n",
    "print(\"  Static negatives: Model memorizes 'item 5 is negative for user 3'\")\n",
    "print(\"  Fresh negatives: Model learns general patterns of positive vs negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ Why neural networks can outperform linear matrix factorization\n",
    "- ‚úÖ How negative sampling works for implicit feedback\n",
    "- ‚úÖ GMF: learnable element-wise product\n",
    "- ‚úÖ MLP: non-linear user-item interaction learning\n",
    "- ‚úÖ NeuMF: combining GMF and MLP for best results\n",
    "- ‚úÖ Evaluating with Hit Rate @ K\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Implement Pre-training (15-20 min):**\n",
    "\n",
    "The original NeuMF paper suggests pre-training GMF and MLP separately, then combining their weights to initialize NeuMF. This often improves final performance.\n",
    "\n",
    "Steps:\n",
    "1. Train GMF alone for N epochs\n",
    "2. Train MLP alone for N epochs\n",
    "3. Initialize NeuMF with weights from trained GMF and MLP\n",
    "4. Fine-tune the combined NeuMF\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [Neural Collaborative Filtering Paper](https://arxiv.org/abs/1708.05031) - The original NCF paper\n",
    "- [Deep Learning for Recommender Systems](https://dl.acm.org/doi/10.1145/3285029) - Survey paper\n",
    "- [RecBole Library](https://recbole.io/) - Unified framework for recommendation research\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del model, gmf_model, mlp_model, neumf_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ GPU memory cleared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚û°Ô∏è Next Steps\n",
    "\n",
    "In the next notebook, we'll build a **Two-Tower Retrieval System** - the architecture used by Google, YouTube, and other tech giants for retrieving candidates from billions of items!\n",
    "\n",
    "Continue to: **03-two-tower-retrieval.ipynb**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
