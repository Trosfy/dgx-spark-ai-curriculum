{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab B.4: Evaluation and Analysis\n",
    "\n",
    "**Module:** B - Recommender Systems  \n",
    "**Time:** 1.5 hours  \n",
    "**Difficulty:** â­â­â­ (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Implement and understand ranking metrics (NDCG, MAP, MRR)\n",
    "- [ ] Compare all recommender models on a unified benchmark\n",
    "- [ ] Analyze cold start performance\n",
    "- [ ] Evaluate recommendation diversity\n",
    "- [ ] Understand production deployment considerations\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Prerequisites\n",
    "\n",
    "- Completed: Labs B.1, B.2, B.3\n",
    "- Trained models from previous notebooks\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ Real-World Context\n",
    "\n",
    "**The Metric Matters:** Optimizing the wrong metric can destroy your product.\n",
    "\n",
    "- ğŸ“º **Netflix**: Optimized for \"thumbs up\" â†’ users gave good ratings but didn't watch\n",
    "- ğŸµ **Spotify**: Learned that \"completion rate\" beats \"skip rate\" for user satisfaction\n",
    "- ğŸ›’ **Amazon**: \"Purchase\" matters more than \"click\"\n",
    "\n",
    "This notebook teaches you to measure what matters!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§’ ELI5: Ranking Metrics\n",
    "\n",
    "> **Imagine you're a teacher grading search results...**\n",
    ">\n",
    "> **Hit Rate (HR):** \"Did ANY good answer appear in the top 10?\" (Pass/Fail)\n",
    ">\n",
    "> **NDCG:** \"Did the BEST answers appear at the TOP?\" (Position matters!)\n",
    "> - Good answer at #1: Full credit\n",
    "> - Good answer at #10: Partial credit\n",
    "> - Good answer at #100: Almost no credit\n",
    ">\n",
    "> **MRR:** \"Where's the FIRST good answer?\" (Only the first one counts)\n",
    "> - First good at #1: Score = 1.0\n",
    "> - First good at #2: Score = 0.5\n",
    "> - First good at #10: Score = 0.1\n",
    ">\n",
    "> **MAP:** \"On average, how good is the precision at each relevant position?\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "module_dir = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(module_dir / 'scripts'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from data_utils import download_movielens, leave_one_out_split, compute_statistics\n",
    "from metrics import (\n",
    "    hit_rate_at_k, ndcg_at_k, mrr, recall_at_k, \n",
    "    mean_average_precision, evaluate_ranking, format_metrics\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸ–¥ï¸  Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "ratings_df, movies_df = download_movielens('100k')\n",
    "\n",
    "# Leave-one-out split\n",
    "train_df, test_df = leave_one_out_split(ratings_df, by_time=True)\n",
    "\n",
    "num_users = ratings_df['user_id'].nunique()\n",
    "num_items = ratings_df['item_id'].nunique()\n",
    "\n",
    "# Build user -> positive items mapping\n",
    "user_positive_items = train_df.groupby('user_id')['item_id'].apply(set).to_dict()\n",
    "\n",
    "print(f\"ğŸ“Š Dataset: {num_users} users, {num_items} items\")\n",
    "print(f\"   Train: {len(train_df):,}, Test: {len(test_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train All Models\n",
    "\n",
    "Let's train our models from previous notebooks for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model classes\n",
    "from models import MatrixFactorization, NeuMF\n",
    "\n",
    "# Prepare data for implicit feedback\n",
    "class ImplicitDataset(Dataset):\n",
    "    def __init__(self, interactions_df, num_items, user_positive_items, num_negatives=4):\n",
    "        self.users = interactions_df['user_id'].values\n",
    "        self.items = interactions_df['item_id'].values\n",
    "        self.num_items = num_items\n",
    "        self.user_positive_items = user_positive_items\n",
    "        self.num_negatives = num_negatives\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user = self.users[idx]\n",
    "        pos_item = self.items[idx]\n",
    "        \n",
    "        neg_items = []\n",
    "        positives = self.user_positive_items.get(user, set())\n",
    "        \n",
    "        while len(neg_items) < self.num_negatives:\n",
    "            neg_item = np.random.randint(0, self.num_items)\n",
    "            if neg_item not in positives:\n",
    "                neg_items.append(neg_item)\n",
    "        \n",
    "        users = [user] * (1 + self.num_negatives)\n",
    "        items = [pos_item] + neg_items\n",
    "        labels = [1.0] + [0.0] * self.num_negatives\n",
    "        \n",
    "        return (\n",
    "            torch.LongTensor(users),\n",
    "            torch.LongTensor(items),\n",
    "            torch.FloatTensor(labels)\n",
    "        )\n",
    "\n",
    "def collate_fn(batch):\n",
    "    users = torch.cat([b[0] for b in batch])\n",
    "    items = torch.cat([b[1] for b in batch])\n",
    "    labels = torch.cat([b[2] for b in batch])\n",
    "    return users, items, labels\n",
    "\n",
    "\n",
    "train_dataset = ImplicitDataset(train_df, num_items, user_positive_items, num_negatives=4)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, device, epochs=10, lr=0.001):\n",
    "    \"\"\"Train a model for specified epochs.\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for users, items, labels in train_loader:\n",
    "            users, items, labels = users.to(device), items.to(device), labels.to(device)\n",
    "            \n",
    "            predictions = model(users, items)\n",
    "            \n",
    "            # Handle different model outputs\n",
    "            if predictions.dim() == 0:\n",
    "                predictions = predictions.unsqueeze(0)\n",
    "            \n",
    "            # Sigmoid for models that don't have it\n",
    "            if predictions.max() > 1 or predictions.min() < 0:\n",
    "                predictions = torch.sigmoid(predictions)\n",
    "            \n",
    "            loss = F.binary_cross_entropy(predictions, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Train Matrix Factorization\n",
    "print(\"Training Matrix Factorization...\")\n",
    "mf_model = MatrixFactorization(num_users, num_items, embedding_dim=64)\n",
    "mf_model = train_model(mf_model, train_loader, device, epochs=10)\n",
    "print(\"âœ… MF trained\")\n",
    "\n",
    "# Train NeuMF\n",
    "print(\"\\nTraining NeuMF...\")\n",
    "neumf_model = NeuMF(num_users, num_items, gmf_dim=32, mlp_dim=64)\n",
    "neumf_model = train_model(neumf_model, train_loader, device, epochs=10)\n",
    "print(\"âœ… NeuMF trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Implementing Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_df, user_positive_items, device, k_values=[5, 10, 20]):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of a recommender model.\n",
    "    \n",
    "    For each user in test set:\n",
    "    1. Predict scores for all items\n",
    "    2. Mask out training items\n",
    "    3. Compute ranking metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    all_ranked_items = []\n",
    "    all_ground_truths = []\n",
    "    \n",
    "    test_users = test_df['user_id'].unique()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for user_id in tqdm(test_users, desc='Evaluating'):\n",
    "            # Get ground truth\n",
    "            test_item = test_df[test_df['user_id'] == user_id]['item_id'].values[0]\n",
    "            \n",
    "            # Predict all items\n",
    "            user_ids = torch.LongTensor([user_id] * num_items).to(device)\n",
    "            item_ids = torch.arange(num_items).to(device)\n",
    "            \n",
    "            scores = model(user_ids, item_ids)\n",
    "            if scores.dim() == 0:\n",
    "                scores = scores.unsqueeze(0)\n",
    "            \n",
    "            # Mask training items\n",
    "            train_items = list(user_positive_items.get(user_id, set()))\n",
    "            scores[train_items] = -float('inf')\n",
    "            \n",
    "            # Get ranked items\n",
    "            _, ranked_items = torch.sort(scores, descending=True)\n",
    "            ranked_items = ranked_items.cpu().numpy()\n",
    "            \n",
    "            all_ranked_items.append(ranked_items[:max(k_values)])\n",
    "            all_ground_truths.append([test_item])\n",
    "    \n",
    "    # Compute metrics\n",
    "    results = {}\n",
    "    \n",
    "    for k in k_values:\n",
    "        # Hit Rate\n",
    "        hr_scores = [hit_rate_at_k(r, g, k) for r, g in zip(all_ranked_items, all_ground_truths)]\n",
    "        results[f'HR@{k}'] = np.mean(hr_scores)\n",
    "        \n",
    "        # NDCG\n",
    "        ndcg_scores = [ndcg_at_k(r, g, k) for r, g in zip(all_ranked_items, all_ground_truths)]\n",
    "        results[f'NDCG@{k}'] = np.mean(ndcg_scores)\n",
    "        \n",
    "        # Recall (same as HR for single test item)\n",
    "        results[f'Recall@{k}'] = results[f'HR@{k}']\n",
    "    \n",
    "    # MRR\n",
    "    mrr_scores = [mrr(r, g) for r, g in zip(all_ranked_items, all_ground_truths)]\n",
    "    results['MRR'] = np.mean(mrr_scores)\n",
    "    \n",
    "    return results, all_ranked_items\n",
    "\n",
    "\n",
    "# Evaluate models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "mf_results, mf_rankings = evaluate_model(mf_model, test_df, user_positive_items, device)\n",
    "print(\"\\nğŸ“Š Matrix Factorization Results:\")\n",
    "for metric, value in sorted(mf_results.items()):\n",
    "    print(f\"   {metric}: {value:.4f}\")\n",
    "\n",
    "neumf_results, neumf_rankings = evaluate_model(neumf_model, test_df, user_positive_items, device)\n",
    "print(\"\\nğŸ“Š NeuMF Results:\")\n",
    "for metric, value in sorted(neumf_results.items()):\n",
    "    print(f\"   {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare comparison data\n",
    "models = ['Matrix Factorization', 'NeuMF']\n",
    "all_results = [mf_results, neumf_results]\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for model_name, results in zip(models, all_results):\n",
    "    for metric, value in results.items():\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Metric': metric,\n",
    "            'Value': value\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# HR@K comparison\n",
    "hr_metrics = ['HR@5', 'HR@10', 'HR@20']\n",
    "x = np.arange(len(hr_metrics))\n",
    "width = 0.35\n",
    "\n",
    "mf_hr = [mf_results[m] for m in hr_metrics]\n",
    "neumf_hr = [neumf_results[m] for m in hr_metrics]\n",
    "\n",
    "axes[0].bar(x - width/2, mf_hr, width, label='Matrix Factorization', color='#3498db')\n",
    "axes[0].bar(x + width/2, neumf_hr, width, label='NeuMF', color='#e74c3c')\n",
    "axes[0].set_xlabel('Metric')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Hit Rate Comparison')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(hr_metrics)\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# NDCG@K comparison\n",
    "ndcg_metrics = ['NDCG@5', 'NDCG@10', 'NDCG@20']\n",
    "mf_ndcg = [mf_results[m] for m in ndcg_metrics]\n",
    "neumf_ndcg = [neumf_results[m] for m in ndcg_metrics]\n",
    "\n",
    "axes[1].bar(x - width/2, mf_ndcg, width, label='Matrix Factorization', color='#3498db')\n",
    "axes[1].bar(x + width/2, neumf_ndcg, width, label='NeuMF', color='#e74c3c')\n",
    "axes[1].set_xlabel('Metric')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('NDCG Comparison')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(ndcg_metrics)\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# MRR comparison\n",
    "mrr_values = [mf_results['MRR'], neumf_results['MRR']]\n",
    "colors = ['#3498db', '#e74c3c']\n",
    "axes[2].bar(models, mrr_values, color=colors, edgecolor='black')\n",
    "axes[2].set_ylabel('MRR Score')\n",
    "axes[2].set_title('Mean Reciprocal Rank Comparison')\n",
    "axes[2].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(mrr_values):\n",
    "    axes[2].text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Cold Start Analysis\n",
    "\n",
    "How do our models perform for users with few ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize users by activity level\n",
    "user_activity = train_df.groupby('user_id').size()\n",
    "\n",
    "# Define activity buckets\n",
    "def get_activity_bucket(num_ratings):\n",
    "    if num_ratings <= 20:\n",
    "        return 'Cold (â‰¤20)'\n",
    "    elif num_ratings <= 50:\n",
    "        return 'Warm (21-50)'\n",
    "    elif num_ratings <= 100:\n",
    "        return 'Active (51-100)'\n",
    "    else:\n",
    "        return 'Power (>100)'\n",
    "\n",
    "user_buckets = user_activity.apply(get_activity_bucket)\n",
    "\n",
    "# Show distribution\n",
    "print(\"ğŸ“Š User Activity Distribution:\")\n",
    "print(user_buckets.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_by_bucket(model, test_df, user_positive_items, user_buckets, device, k=10):\n",
    "    \"\"\"Evaluate model performance by user activity bucket.\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    bucket_results = defaultdict(list)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc='Evaluating'):\n",
    "            user_id = row['user_id']\n",
    "            test_item = row['item_id']\n",
    "            bucket = user_buckets.get(user_id, 'Unknown')\n",
    "            \n",
    "            # Predict\n",
    "            user_ids = torch.LongTensor([user_id] * num_items).to(device)\n",
    "            item_ids = torch.arange(num_items).to(device)\n",
    "            scores = model(user_ids, item_ids)\n",
    "            \n",
    "            # Mask training items\n",
    "            train_items = list(user_positive_items.get(user_id, set()))\n",
    "            scores[train_items] = -float('inf')\n",
    "            \n",
    "            # Get ranking\n",
    "            _, ranked_items = torch.sort(scores, descending=True)\n",
    "            ranked_items = ranked_items.cpu().numpy()[:k]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            hr = hit_rate_at_k(ranked_items, [test_item], k)\n",
    "            bucket_results[bucket].append(hr)\n",
    "    \n",
    "    # Aggregate\n",
    "    return {bucket: np.mean(hrs) for bucket, hrs in bucket_results.items()}\n",
    "\n",
    "\n",
    "# Evaluate both models by bucket\n",
    "print(\"\\nEvaluating Matrix Factorization by user activity...\")\n",
    "mf_bucket_results = evaluate_by_bucket(mf_model, test_df, user_positive_items, user_buckets, device)\n",
    "\n",
    "print(\"Evaluating NeuMF by user activity...\")\n",
    "neumf_bucket_results = evaluate_by_bucket(neumf_model, test_df, user_positive_items, user_buckets, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cold start analysis\n",
    "buckets = ['Cold (â‰¤20)', 'Warm (21-50)', 'Active (51-100)', 'Power (>100)']\n",
    "buckets = [b for b in buckets if b in mf_bucket_results]  # Only include existing buckets\n",
    "\n",
    "mf_hrs = [mf_bucket_results.get(b, 0) for b in buckets]\n",
    "neumf_hrs = [neumf_bucket_results.get(b, 0) for b in buckets]\n",
    "\n",
    "x = np.arange(len(buckets))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.bar(x - width/2, mf_hrs, width, label='Matrix Factorization', color='#3498db')\n",
    "bars2 = ax.bar(x + width/2, neumf_hrs, width, label='NeuMF', color='#e74c3c')\n",
    "\n",
    "ax.set_xlabel('User Activity Level')\n",
    "ax.set_ylabel('Hit Rate @ 10')\n",
    "ax.set_title('Cold Start Analysis: HR@10 by User Activity')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(buckets)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height, f'{height:.3f}',\n",
    "            ha='center', va='bottom', fontsize=9)\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height, f'{height:.3f}',\n",
    "            ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š Observations:\")\n",
    "print(\"   - Cold start users (â‰¤20 ratings) have lower hit rates\")\n",
    "print(\"   - Neural models can better leverage sparse signals\")\n",
    "print(\"   - Power users benefit most from collaborative signals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Diversity Analysis\n",
    "\n",
    "Are we recommending a diverse set of items, or just popular ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_diversity(rankings, movies_df, num_items, top_k=10):\n",
    "    \"\"\"\n",
    "    Analyze the diversity of recommendations.\n",
    "    \n",
    "    Metrics:\n",
    "    - Coverage: What fraction of items are ever recommended?\n",
    "    - Genre diversity: How many different genres in recommendations?\n",
    "    - Popularity bias: Are we only recommending popular items?\n",
    "    \"\"\"\n",
    "    # Coverage\n",
    "    all_recommended = set()\n",
    "    for ranking in rankings:\n",
    "        all_recommended.update(ranking[:top_k])\n",
    "    coverage = len(all_recommended) / num_items\n",
    "    \n",
    "    # Genre diversity per user\n",
    "    genre_diversities = []\n",
    "    for ranking in rankings:\n",
    "        genres = set()\n",
    "        for item_id in ranking[:top_k]:\n",
    "            movie_genres = movies_df[movies_df['item_id'] == item_id]['genres'].values\n",
    "            if len(movie_genres) > 0 and pd.notna(movie_genres[0]):\n",
    "                genres.update(movie_genres[0].split('|'))\n",
    "        genre_diversities.append(len(genres))\n",
    "    \n",
    "    # Recommendation frequency\n",
    "    item_counts = defaultdict(int)\n",
    "    for ranking in rankings:\n",
    "        for item_id in ranking[:top_k]:\n",
    "            item_counts[item_id] += 1\n",
    "    \n",
    "    return {\n",
    "        'coverage': coverage,\n",
    "        'avg_genre_diversity': np.mean(genre_diversities),\n",
    "        'item_counts': dict(item_counts),\n",
    "        'unique_items_recommended': len(all_recommended)\n",
    "    }\n",
    "\n",
    "\n",
    "# Analyze diversity\n",
    "mf_diversity = analyze_diversity(mf_rankings, movies_df, num_items)\n",
    "neumf_diversity = analyze_diversity(neumf_rankings, movies_df, num_items)\n",
    "\n",
    "print(\"ğŸ“Š Diversity Analysis:\")\n",
    "print(\"\\nMatrix Factorization:\")\n",
    "print(f\"   Coverage: {mf_diversity['coverage']:.2%} ({mf_diversity['unique_items_recommended']} items)\")\n",
    "print(f\"   Avg genres per recommendation: {mf_diversity['avg_genre_diversity']:.1f}\")\n",
    "\n",
    "print(\"\\nNeuMF:\")\n",
    "print(f\"   Coverage: {neumf_diversity['coverage']:.2%} ({neumf_diversity['unique_items_recommended']} items)\")\n",
    "print(f\"   Avg genres per recommendation: {neumf_diversity['avg_genre_diversity']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize popularity bias\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# MF popularity distribution\n",
    "mf_counts = sorted(mf_diversity['item_counts'].values(), reverse=True)\n",
    "axes[0].bar(range(len(mf_counts[:100])), mf_counts[:100], alpha=0.7)\n",
    "axes[0].set_xlabel('Item Rank (by recommendation frequency)')\n",
    "axes[0].set_ylabel('Times Recommended')\n",
    "axes[0].set_title('Matrix Factorization: Recommendation Frequency')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# NeuMF popularity distribution\n",
    "neumf_counts = sorted(neumf_diversity['item_counts'].values(), reverse=True)\n",
    "axes[1].bar(range(len(neumf_counts[:100])), neumf_counts[:100], alpha=0.7, color='orange')\n",
    "axes[1].set_xlabel('Item Rank (by recommendation frequency)')\n",
    "axes[1].set_ylabel('Times Recommended')\n",
    "axes[1].set_title('NeuMF: Recommendation Frequency')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š Observation: Long tail shows some items are recommended much more often\")\n",
    "print(\"   This is typical popularity bias - popular items get recommended more.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Production Recommendations\n",
    "\n",
    "Based on our analysis, here are recommendations for production deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary table\n",
    "summary_data = {\n",
    "    'Metric': ['HR@10', 'NDCG@10', 'MRR', 'Coverage', 'Avg Genres/Rec'],\n",
    "    'Matrix Factorization': [\n",
    "        f\"{mf_results['HR@10']:.4f}\",\n",
    "        f\"{mf_results['NDCG@10']:.4f}\",\n",
    "        f\"{mf_results['MRR']:.4f}\",\n",
    "        f\"{mf_diversity['coverage']:.2%}\",\n",
    "        f\"{mf_diversity['avg_genre_diversity']:.1f}\"\n",
    "    ],\n",
    "    'NeuMF': [\n",
    "        f\"{neumf_results['HR@10']:.4f}\",\n",
    "        f\"{neumf_results['NDCG@10']:.4f}\",\n",
    "        f\"{neumf_results['MRR']:.4f}\",\n",
    "        f\"{neumf_diversity['coverage']:.2%}\",\n",
    "        f\"{neumf_diversity['avg_genre_diversity']:.1f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production recommendations\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘          PRODUCTION DEPLOYMENT RECOMMENDATIONS               â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                              â•‘\n",
    "â•‘  1. MODEL SELECTION                                          â•‘\n",
    "â•‘     â”œâ”€ Start with Matrix Factorization (simpler, faster)     â•‘\n",
    "â•‘     â”œâ”€ Graduate to NeuMF for better accuracy                 â•‘\n",
    "â•‘     â””â”€ Use Two-Tower for candidate retrieval at scale        â•‘\n",
    "â•‘                                                              â•‘\n",
    "â•‘  2. COLD START HANDLING                                      â•‘\n",
    "â•‘     â”œâ”€ New users: Use content-based until enough data        â•‘\n",
    "â•‘     â”œâ”€ New items: Feature-based embeddings (Two-Tower)       â•‘\n",
    "â•‘     â””â”€ Fallback: Popular/trending items                      â•‘\n",
    "â•‘                                                              â•‘\n",
    "â•‘  3. DIVERSITY                                                â•‘\n",
    "â•‘     â”œâ”€ Add MMR (Maximal Marginal Relevance) re-ranking       â•‘\n",
    "â•‘     â”œâ”€ Inject exploration (10-20% random/trending)           â•‘\n",
    "â•‘     â””â”€ Monitor coverage metrics in production                â•‘\n",
    "â•‘                                                              â•‘\n",
    "â•‘  4. EVALUATION                                               â•‘\n",
    "â•‘     â”œâ”€ A/B test with engagement metrics (CTR, watch time)    â•‘\n",
    "â•‘     â”œâ”€ Monitor for feedback loops and filter bubbles         â•‘\n",
    "â•‘     â””â”€ Balance accuracy vs diversity vs novelty              â•‘\n",
    "â•‘                                                              â•‘\n",
    "â•‘  5. DGX SPARK ADVANTAGES                                     â•‘\n",
    "â•‘     â”œâ”€ 128GB memory: Full user/item embeddings in GPU        â•‘\n",
    "â•‘     â”œâ”€ FAISS GPU: Sub-millisecond retrieval                  â•‘\n",
    "â•‘     â””â”€ Large batches: Better in-batch negative sampling      â•‘\n",
    "â•‘                                                              â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ‹ Try It Yourself!\n",
    "\n",
    "### Exercise 1: Implement NDCG with Graded Relevance\n",
    "\n",
    "Our current NDCG uses binary relevance. Implement NDCG with graded relevance based on actual ratings.\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Hint</summary>\n",
    "\n",
    "Instead of relevance = 1 for any rated item, use:\n",
    "- relevance = (rating - 1) / 4  for 1-5 ratings â†’ 0 to 1\n",
    "- Or use the rating directly: relevance = rating\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement MMR Re-ranking\n",
    "\n",
    "Implement Maximal Marginal Relevance to balance relevance and diversity:\n",
    "\n",
    "$$MMR = \\arg\\max_{d \\in R\\setminus S} \\left[ \\lambda \\cdot Rel(d, q) - (1-\\lambda) \\cdot \\max_{d' \\in S} Sim(d, d') \\right]$$\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Hint</summary>\n",
    "\n",
    "1. Start with top item by relevance\n",
    "2. For each next item, trade off relevance vs similarity to already-selected items\n",
    "3. Î»=1 is pure relevance, Î»=0 is pure diversity\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš ï¸ Common Mistakes\n",
    "\n",
    "### Mistake 1: Evaluating on Training Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âŒ Wrong: Include training items in evaluation\n",
    "# rankings = model.get_top_k(user)  # Might include items user already rated\n",
    "# hr = compute_hr(rankings, test_item)  # Artificially high!\n",
    "\n",
    "# âœ… Right: Mask training items\n",
    "# scores[train_items] = -inf\n",
    "# rankings = get_top_k(scores)\n",
    "\n",
    "print(\"Why this matters:\")\n",
    "print(\"  Without masking: HR@10 could be 95%+ (trivially high)\")\n",
    "print(\"  With masking: Measures true discovery ability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Confusing Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common confusion: Hit Rate vs Recall\n",
    "\n",
    "print(\"Hit Rate vs Recall:\")\n",
    "print(\"\")\n",
    "print(\"HR@K:  'Was ANY relevant item in top K?'\")\n",
    "print(\"       â†’ Binary: 0 or 1\")\n",
    "print(\"       â†’ For 1 test item: HR@K = Recall@K\")\n",
    "print(\"\")\n",
    "print(\"Recall@K: 'What fraction of relevant items are in top K?'\")\n",
    "print(\"          â†’ Range: 0 to 1\")\n",
    "print(\"          â†’ If user has 5 relevant items and 3 are in top K: Recall = 0.6\")\n",
    "print(\"\")\n",
    "print(\"For leave-one-out (1 test item): HR = Recall\")\n",
    "print(\"For multiple test items: Recall is more informative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ Module Complete!\n",
    "\n",
    "Congratulations! You've completed the Recommender Systems module. You've learned:\n",
    "\n",
    "- âœ… **Lab B.1**: Matrix Factorization and collaborative filtering fundamentals\n",
    "- âœ… **Lab B.2**: Neural Collaborative Filtering (GMF + MLP = NeuMF)\n",
    "- âœ… **Lab B.3**: Two-Tower retrieval with FAISS for production scale\n",
    "- âœ… **Lab B.4**: Comprehensive evaluation with ranking metrics\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Start simple**: Matrix factorization is a strong baseline\n",
    "2. **Scale smartly**: Two-tower + FAISS for billion-scale retrieval\n",
    "3. **Measure right**: NDCG > accuracy for ranking tasks\n",
    "4. **Balance trade-offs**: Accuracy vs diversity vs novelty\n",
    "5. **DGX Spark advantage**: 128GB memory enables large-scale experimentation\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– Further Reading\n",
    "\n",
    "- [Recommender Systems Handbook](https://www.springer.com/gp/book/9780387858203) - Comprehensive reference\n",
    "- [Microsoft Recommenders](https://github.com/microsoft/recommenders) - Production-quality implementations\n",
    "- [RecBole](https://recbole.io/) - Unified recommendation framework\n",
    "- [Deep Learning for Recommender Systems](https://dl.acm.org/doi/10.1145/3285029) - Academic survey\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del mf_model, neumf_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… GPU memory cleared!\")\n",
    "print(\"\\nğŸ‰ Module B: Recommender Systems - Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
