{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.5.1: Complete RAG Demo\n",
    "\n",
    "**Module:** 4.5 - Demo Building & Prototyping  \n",
    "**Time:** 3 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Lab Objectives\n",
    "\n",
    "Build a polished, production-ready Gradio application that showcases your RAG system with:\n",
    "- [ ] Multi-tab interface using Blocks API\n",
    "- [ ] Document upload and indexing with progress indicators\n",
    "- [ ] Chat interface with conversation history\n",
    "- [ ] Source citations display\n",
    "- [ ] Settings panel for configuration\n",
    "- [ ] Custom styling for professional appearance\n",
    "- [ ] Error handling with friendly messages\n",
    "- [ ] Deployment to Hugging Face Spaces\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Scenario\n",
    "\n",
    "Your team has been developing a RAG system for 3 months. The CEO wants to see a demo next week. You need to create a polished interface that:\n",
    "1. Impresses non-technical stakeholders\n",
    "2. Actually works (no crashes!)\n",
    "3. Can be shared with investors\n",
    "\n",
    "Let's build it!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q gradio>=4.44.0 chromadb sentence-transformers pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import gradio as gr\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import time\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úÖ Dependencies loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: RAG Backend Implementation\n",
    "\n",
    "First, let's create a simple but functional RAG backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAG:\n",
    "    \"\"\"\n",
    "    A simple RAG system for the demo.\n",
    "    \n",
    "    In production, you would use your actual RAG pipeline from Module 3.5.\n",
    "    This is simplified for demo purposes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the RAG system.\"\"\"\n",
    "        # Initialize embedding model\n",
    "        print(\"Loading embedding model...\")\n",
    "        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # Initialize ChromaDB\n",
    "        self.client = chromadb.Client()\n",
    "        self.collection = self.client.create_collection(\n",
    "            name=\"documents\",\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        \n",
    "        self.documents = {}  # Track indexed documents\n",
    "        print(\"‚úÖ RAG system initialized!\")\n",
    "    \n",
    "    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "        \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = start + chunk_size\n",
    "            chunk = text[start:end]\n",
    "            chunks.append(chunk)\n",
    "            start = end - overlap\n",
    "        return chunks\n",
    "    \n",
    "    def index_document(self, filename: str, content: str) -> int:\n",
    "        \"\"\"\n",
    "        Index a document.\n",
    "        \n",
    "        Returns the number of chunks indexed.\n",
    "        \"\"\"\n",
    "        # Generate document ID\n",
    "        doc_id = hashlib.md5(filename.encode()).hexdigest()[:8]\n",
    "        \n",
    "        # Chunk the document\n",
    "        chunks = self.chunk_text(content)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = self.embedder.encode(chunks).tolist()\n",
    "        \n",
    "        # Add to ChromaDB\n",
    "        ids = [f\"{doc_id}_{i}\" for i in range(len(chunks))]\n",
    "        metadatas = [{\"source\": filename, \"chunk_id\": i} for i in range(len(chunks))]\n",
    "        \n",
    "        self.collection.add(\n",
    "            ids=ids,\n",
    "            embeddings=embeddings,\n",
    "            documents=chunks,\n",
    "            metadatas=metadatas\n",
    "        )\n",
    "        \n",
    "        # Track the document\n",
    "        self.documents[filename] = {\n",
    "            \"id\": doc_id,\n",
    "            \"chunks\": len(chunks),\n",
    "            \"indexed_at\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        return len(chunks)\n",
    "    \n",
    "    def query(self, question: str, n_results: int = 3) -> Tuple[str, List[Dict]]:\n",
    "        \"\"\"\n",
    "        Query the RAG system.\n",
    "        \n",
    "        Returns (answer, sources)\n",
    "        \"\"\"\n",
    "        if self.collection.count() == 0:\n",
    "            return \"No documents have been indexed yet. Please upload some documents first!\", []\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedder.encode([question])[0].tolist()\n",
    "        \n",
    "        # Retrieve relevant chunks\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=n_results\n",
    "        )\n",
    "        \n",
    "        # Format sources\n",
    "        sources = []\n",
    "        context_parts = []\n",
    "        \n",
    "        for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n",
    "            sources.append({\n",
    "                \"source\": metadata['source'],\n",
    "                \"chunk_id\": metadata['chunk_id'],\n",
    "                \"text\": doc[:200] + \"...\" if len(doc) > 200 else doc\n",
    "            })\n",
    "            context_parts.append(f\"[Source: {metadata['source']}]\\n{doc}\")\n",
    "        \n",
    "        # Generate answer (simplified - in production, use an LLM)\n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Simulated answer (replace with actual LLM call in production)\n",
    "        answer = self._generate_answer(question, context)\n",
    "        \n",
    "        return answer, sources\n",
    "    \n",
    "    def _generate_answer(self, question: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate an answer based on the context.\n",
    "        \n",
    "        In production, this would call your LLM (Ollama, OpenAI, etc.)\n",
    "        \"\"\"\n",
    "        # For the demo, we'll create a template response\n",
    "        # In production, replace this with actual LLM call\n",
    "        return f\"\"\"Based on the documents I found, here's what I can tell you about your question:\n",
    "\n",
    "**Question:** {question}\n",
    "\n",
    "**Answer:** The relevant information from your documents suggests the following key points:\n",
    "\n",
    "1. The documents contain information related to your query.\n",
    "2. Multiple sources were consulted to provide this answer.\n",
    "3. See the \"Sources\" section below for the specific excerpts used.\n",
    "\n",
    "*Note: In a production system, this would be a real LLM-generated response using the retrieved context.*\"\"\"\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get statistics about the indexed documents.\"\"\"\n",
    "        return {\n",
    "            \"total_documents\": len(self.documents),\n",
    "            \"total_chunks\": self.collection.count(),\n",
    "            \"documents\": list(self.documents.keys())\n",
    "        }\n",
    "\n",
    "# Create global RAG instance\n",
    "rag_system = SimpleRAG()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Document Processing Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_content(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Read content from various file types.\n",
    "    \n",
    "    Supports: .txt, .md, .pdf\n",
    "    \"\"\"\n",
    "    extension = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    if extension == '.pdf':\n",
    "        try:\n",
    "            from pypdf import PdfReader\n",
    "            reader = PdfReader(file_path)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            return f\"Error reading PDF: {str(e)}\"\n",
    "    \n",
    "    elif extension in ['.txt', '.md']:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    \n",
    "    else:\n",
    "        return f\"Unsupported file type: {extension}\"\n",
    "\n",
    "print(\"‚úÖ File processing helpers ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Custom Theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom theme for professional appearance\n",
    "custom_theme = gr.themes.Soft(\n",
    "    primary_hue=\"blue\",\n",
    "    secondary_hue=\"slate\",\n",
    "    neutral_hue=\"slate\",\n",
    "    font=gr.themes.GoogleFont(\"Inter\"),\n",
    ").set(\n",
    "    button_primary_background_fill=\"#2563eb\",\n",
    "    button_primary_background_fill_hover=\"#1d4ed8\",\n",
    "    button_primary_text_color=\"white\",\n",
    "    block_title_text_weight=\"600\",\n",
    "    block_label_text_weight=\"500\",\n",
    "    input_background_fill=\"#f8fafc\",\n",
    ")\n",
    "\n",
    "# Custom CSS for additional styling\n",
    "custom_css = \"\"\"\n",
    ".gradio-container {\n",
    "    max-width: 1200px !important;\n",
    "    margin: auto !important;\n",
    "}\n",
    "\n",
    ".source-citation {\n",
    "    background-color: #f0f9ff;\n",
    "    border-left: 4px solid #0284c7;\n",
    "    padding: 0.75rem;\n",
    "    margin: 0.5rem 0;\n",
    "    border-radius: 0 8px 8px 0;\n",
    "    font-size: 0.9em;\n",
    "}\n",
    "\n",
    ".stats-card {\n",
    "    background: linear-gradient(135deg, #f0f9ff 0%, #e0f2fe 100%);\n",
    "    border-radius: 12px;\n",
    "    padding: 1rem;\n",
    "    text-align: center;\n",
    "}\n",
    "\n",
    ".success-message {\n",
    "    background-color: #dcfce7;\n",
    "    border-left: 4px solid #16a34a;\n",
    "    padding: 0.75rem;\n",
    "    border-radius: 0 8px 8px 0;\n",
    "}\n",
    "\n",
    ".error-message {\n",
    "    background-color: #fef2f2;\n",
    "    border-left: 4px solid #dc2626;\n",
    "    padding: 0.75rem;\n",
    "    border-radius: 0 8px 8px 0;\n",
    "}\n",
    "\n",
    "footer {display: none !important;}\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚úÖ Theme and CSS configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Building the Complete Interface\n",
    "\n",
    "Now let's build the full multi-tab interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_demo():\n",
    "    \"\"\"\n",
    "    Create the complete RAG demo interface.\n",
    "    \"\"\"\n",
    "    \n",
    "    with gr.Blocks(theme=custom_theme, css=custom_css, title=\"Document Q&A\") as demo:\n",
    "        \n",
    "        # Header\n",
    "        gr.Markdown(\"\"\"\n",
    "        # üìö Document Q&A Assistant\n",
    "        \n",
    "        Upload your documents and ask questions. Powered by RAG (Retrieval-Augmented Generation).\n",
    "        \"\"\")\n",
    "        \n",
    "        # Session state\n",
    "        chat_history = gr.State(value=[])\n",
    "        settings = gr.State(value={\n",
    "            \"n_results\": 3,\n",
    "            \"temperature\": 0.7,\n",
    "            \"model\": \"local\"\n",
    "        })\n",
    "        \n",
    "        with gr.Tabs() as tabs:\n",
    "            \n",
    "            # =====================================================================\n",
    "            # TAB 1: DOCUMENTS\n",
    "            # =====================================================================\n",
    "            with gr.TabItem(\"üìÅ Documents\", id=\"documents\"):\n",
    "                gr.Markdown(\"### Upload and Index Documents\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=2):\n",
    "                        files = gr.File(\n",
    "                            label=\"Upload Documents\",\n",
    "                            file_count=\"multiple\",\n",
    "                            file_types=[\".pdf\", \".txt\", \".md\"],\n",
    "                            height=200\n",
    "                        )\n",
    "                        \n",
    "                        with gr.Row():\n",
    "                            index_btn = gr.Button(\"üì• Index Documents\", variant=\"primary\", size=\"lg\")\n",
    "                            clear_btn = gr.Button(\"üóëÔ∏è Clear All\", variant=\"secondary\")\n",
    "                        \n",
    "                        status_box = gr.HTML(\n",
    "                            value=\"<div class='stats-card'>No documents indexed yet</div>\",\n",
    "                            label=\"Status\"\n",
    "                        )\n",
    "                    \n",
    "                    with gr.Column(scale=1):\n",
    "                        gr.Markdown(\"### Indexed Documents\")\n",
    "                        doc_list = gr.Dataframe(\n",
    "                            headers=[\"Document\", \"Chunks\"],\n",
    "                            datatype=[\"str\", \"number\"],\n",
    "                            col_count=(2, \"fixed\"),\n",
    "                            interactive=False,\n",
    "                            height=200\n",
    "                        )\n",
    "            \n",
    "            # =====================================================================\n",
    "            # TAB 2: CHAT\n",
    "            # =====================================================================\n",
    "            with gr.TabItem(\"üí¨ Chat\", id=\"chat\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=3):\n",
    "                        chatbot = gr.Chatbot(\n",
    "                            height=450,\n",
    "                            show_copy_button=True,\n",
    "                            placeholder=\"Ask a question about your documents...\",\n",
    "                            bubble_full_width=False\n",
    "                        )\n",
    "                        \n",
    "                        with gr.Row():\n",
    "                            msg = gr.Textbox(\n",
    "                                label=\"Your Question\",\n",
    "                                placeholder=\"What would you like to know?\",\n",
    "                                scale=5,\n",
    "                                lines=2\n",
    "                            )\n",
    "                            send_btn = gr.Button(\"Send üì§\", variant=\"primary\", scale=1)\n",
    "                        \n",
    "                        with gr.Row():\n",
    "                            clear_chat_btn = gr.Button(\"üóëÔ∏è Clear Chat\")\n",
    "                            examples_dropdown = gr.Dropdown(\n",
    "                                choices=[\n",
    "                                    \"What are the main topics in these documents?\",\n",
    "                                    \"Summarize the key findings.\",\n",
    "                                    \"What recommendations are mentioned?\"\n",
    "                                ],\n",
    "                                label=\"Example Questions\",\n",
    "                                scale=2\n",
    "                            )\n",
    "                    \n",
    "                    with gr.Column(scale=1):\n",
    "                        gr.Markdown(\"### üìö Sources\")\n",
    "                        sources_display = gr.HTML(\n",
    "                            value=\"<p style='color: #666;'>Sources will appear here after asking a question.</p>\"\n",
    "                        )\n",
    "            \n",
    "            # =====================================================================\n",
    "            # TAB 3: SETTINGS\n",
    "            # =====================================================================\n",
    "            with gr.TabItem(\"‚öôÔ∏è Settings\", id=\"settings\"):\n",
    "                gr.Markdown(\"### Configure the Assistant\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        gr.Markdown(\"#### Retrieval Settings\")\n",
    "                        n_results = gr.Slider(\n",
    "                            minimum=1,\n",
    "                            maximum=10,\n",
    "                            value=3,\n",
    "                            step=1,\n",
    "                            label=\"Number of Sources to Retrieve\",\n",
    "                            info=\"More sources = more context, but slower\"\n",
    "                        )\n",
    "                        \n",
    "                        chunk_size = gr.Slider(\n",
    "                            minimum=200,\n",
    "                            maximum=1000,\n",
    "                            value=500,\n",
    "                            step=50,\n",
    "                            label=\"Chunk Size\",\n",
    "                            info=\"Size of text chunks for indexing\"\n",
    "                        )\n",
    "                    \n",
    "                    with gr.Column():\n",
    "                        gr.Markdown(\"#### Generation Settings\")\n",
    "                        temperature = gr.Slider(\n",
    "                            minimum=0.0,\n",
    "                            maximum=1.0,\n",
    "                            value=0.7,\n",
    "                            step=0.1,\n",
    "                            label=\"Temperature\",\n",
    "                            info=\"Higher = more creative, Lower = more focused\"\n",
    "                        )\n",
    "                        \n",
    "                        max_tokens = gr.Slider(\n",
    "                            minimum=100,\n",
    "                            maximum=2000,\n",
    "                            value=500,\n",
    "                            step=100,\n",
    "                            label=\"Max Response Length\"\n",
    "                        )\n",
    "                \n",
    "                save_settings_btn = gr.Button(\"üíæ Save Settings\", variant=\"primary\")\n",
    "                settings_status = gr.HTML()\n",
    "        \n",
    "        # Footer\n",
    "        gr.Markdown(\"---\")\n",
    "        with gr.Row():\n",
    "            gr.Markdown(\n",
    "                \"*Built with Gradio & ChromaDB | Module 4.5 Demo*\",\n",
    "                elem_classes=\"footer-text\"\n",
    "            )\n",
    "        \n",
    "        # =====================================================================\n",
    "        # EVENT HANDLERS\n",
    "        # =====================================================================\n",
    "        \n",
    "        def index_documents(files):\n",
    "            \"\"\"Index uploaded documents.\"\"\"\n",
    "            if not files:\n",
    "                return (\n",
    "                    \"<div class='error-message'>‚ö†Ô∏è Please upload at least one file.</div>\",\n",
    "                    []\n",
    "                )\n",
    "            \n",
    "            results = []\n",
    "            for file in files:\n",
    "                try:\n",
    "                    content = read_file_content(file.name)\n",
    "                    filename = os.path.basename(file.name)\n",
    "                    chunks = rag_system.index_document(filename, content)\n",
    "                    results.append((filename, chunks))\n",
    "                except Exception as e:\n",
    "                    results.append((os.path.basename(file.name), f\"Error: {str(e)}\"))\n",
    "            \n",
    "            # Update stats\n",
    "            stats = rag_system.get_stats()\n",
    "            status_html = f\"\"\"\n",
    "            <div class='success-message'>\n",
    "                ‚úÖ Successfully indexed {len(files)} document(s)!<br>\n",
    "                Total: {stats['total_documents']} documents, {stats['total_chunks']} chunks\n",
    "            </div>\n",
    "            \"\"\"\n",
    "            \n",
    "            # Format document list\n",
    "            doc_data = [[name, chunks] for name, chunks in results]\n",
    "            \n",
    "            return status_html, doc_data\n",
    "        \n",
    "        def clear_documents():\n",
    "            \"\"\"Clear all indexed documents.\"\"\"\n",
    "            # Note: In production, properly clear ChromaDB collection\n",
    "            return (\n",
    "                \"<div class='stats-card'>All documents cleared. Upload new files to get started.</div>\",\n",
    "                []\n",
    "            )\n",
    "        \n",
    "        def chat_response(message, history, settings):\n",
    "            \"\"\"Generate a response to the user's question.\"\"\"\n",
    "            if not message.strip():\n",
    "                return history, \"\", \"<p>Please enter a question.</p>\"\n",
    "            \n",
    "            try:\n",
    "                # Query the RAG system\n",
    "                answer, sources = rag_system.query(\n",
    "                    message,\n",
    "                    n_results=settings.get(\"n_results\", 3)\n",
    "                )\n",
    "                \n",
    "                # Format sources HTML\n",
    "                if sources:\n",
    "                    sources_html = \"<div>\"\n",
    "                    for i, src in enumerate(sources, 1):\n",
    "                        sources_html += f\"\"\"\n",
    "                        <div class='source-citation'>\n",
    "                            <strong>Source {i}:</strong> {src['source']}<br>\n",
    "                            <em>{src['text']}</em>\n",
    "                        </div>\n",
    "                        \"\"\"\n",
    "                    sources_html += \"</div>\"\n",
    "                else:\n",
    "                    sources_html = \"<p style='color: #666;'>No sources found.</p>\"\n",
    "                \n",
    "                # Update history\n",
    "                history = history + [[message, answer]]\n",
    "                \n",
    "                return history, \"\", sources_html\n",
    "            \n",
    "            except Exception as e:\n",
    "                error_msg = \"I apologize, but I encountered an error. Please try again or rephrase your question.\"\n",
    "                history = history + [[message, error_msg]]\n",
    "                return history, \"\", f\"<p style='color: red;'>Error: {str(e)}</p>\"\n",
    "        \n",
    "        def clear_chat():\n",
    "            \"\"\"Clear chat history.\"\"\"\n",
    "            return [], \"\", \"<p style='color: #666;'>Sources will appear here after asking a question.</p>\"\n",
    "        \n",
    "        def set_example(example):\n",
    "            \"\"\"Set an example question.\"\"\"\n",
    "            return example\n",
    "        \n",
    "        def save_settings(n_res, temp):\n",
    "            \"\"\"Save settings.\"\"\"\n",
    "            new_settings = {\n",
    "                \"n_results\": int(n_res),\n",
    "                \"temperature\": float(temp)\n",
    "            }\n",
    "            return (\n",
    "                new_settings,\n",
    "                \"<div class='success-message'>‚úÖ Settings saved!</div>\"\n",
    "            )\n",
    "        \n",
    "        # Wire up events\n",
    "        index_btn.click(\n",
    "            index_documents,\n",
    "            inputs=[files],\n",
    "            outputs=[status_box, doc_list]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            clear_documents,\n",
    "            outputs=[status_box, doc_list]\n",
    "        )\n",
    "        \n",
    "        send_btn.click(\n",
    "            chat_response,\n",
    "            inputs=[msg, chatbot, settings],\n",
    "            outputs=[chatbot, msg, sources_display]\n",
    "        )\n",
    "        \n",
    "        msg.submit(\n",
    "            chat_response,\n",
    "            inputs=[msg, chatbot, settings],\n",
    "            outputs=[chatbot, msg, sources_display]\n",
    "        )\n",
    "        \n",
    "        clear_chat_btn.click(\n",
    "            clear_chat,\n",
    "            outputs=[chatbot, msg, sources_display]\n",
    "        )\n",
    "        \n",
    "        examples_dropdown.change(\n",
    "            set_example,\n",
    "            inputs=[examples_dropdown],\n",
    "            outputs=[msg]\n",
    "        )\n",
    "        \n",
    "        save_settings_btn.click(\n",
    "            save_settings,\n",
    "            inputs=[n_results, temperature],\n",
    "            outputs=[settings, settings_status]\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "print(\"‚úÖ Demo interface created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Launch the Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and launch the demo\n",
    "demo = create_rag_demo()\n",
    "\n",
    "# Launch in notebook\n",
    "demo.launch(inline=True, share=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Export for Deployment\n",
    "\n",
    "Let's export this as a standalone app for Hugging Face Spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the demo before exporting\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deployment files\n",
    "import os\n",
    "\n",
    "deploy_dir = '/tmp/rag_demo_deploy'\n",
    "os.makedirs(deploy_dir, exist_ok=True)\n",
    "\n",
    "# Write the complete app.py file\n",
    "app_py = '''\"\"\"RAG Document Q&A Demo - Hugging Face Spaces Deployment\"\"\"\n",
    "\n",
    "import gradio as gr\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# ============================================================================\n",
    "# RAG BACKEND\n",
    "# ============================================================================\n",
    "\n",
    "class SimpleRAG:\n",
    "    def __init__(self):\n",
    "        self.embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        self.client = chromadb.Client()\n",
    "        self.collection = self.client.create_collection(\n",
    "            name=\"documents\",\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        self.documents = {}\n",
    "    \n",
    "    def chunk_text(self, text, chunk_size=500, overlap=50):\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = start + chunk_size\n",
    "            chunks.append(text[start:end])\n",
    "            start = end - overlap\n",
    "        return chunks\n",
    "    \n",
    "    def index_document(self, filename, content):\n",
    "        doc_id = hashlib.md5(filename.encode()).hexdigest()[:8]\n",
    "        chunks = self.chunk_text(content)\n",
    "        embeddings = self.embedder.encode(chunks).tolist()\n",
    "        ids = [f\"{doc_id}_{i}\" for i in range(len(chunks))]\n",
    "        metadatas = [{\"source\": filename, \"chunk_id\": i} for i in range(len(chunks))]\n",
    "        self.collection.add(ids=ids, embeddings=embeddings, documents=chunks, metadatas=metadatas)\n",
    "        self.documents[filename] = {\"chunks\": len(chunks)}\n",
    "        return len(chunks)\n",
    "    \n",
    "    def query(self, question, n_results=3):\n",
    "        if self.collection.count() == 0:\n",
    "            return \"Please upload documents first!\", []\n",
    "        query_emb = self.embedder.encode([question])[0].tolist()\n",
    "        results = self.collection.query(query_embeddings=[query_emb], n_results=n_results)\n",
    "        sources = []\n",
    "        for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "            sources.append({\"source\": meta[\"source\"], \"text\": doc[:200] + \"...\"})\n",
    "        answer = f\"Based on {len(sources)} sources, here\\'s what I found about: {question}\"\n",
    "        return answer, sources\n",
    "    \n",
    "    def get_stats(self):\n",
    "        return {\"total_documents\": len(self.documents), \"total_chunks\": self.collection.count()}\n",
    "\n",
    "rag = SimpleRAG()\n",
    "\n",
    "# ============================================================================\n",
    "# INTERFACE\n",
    "# ============================================================================\n",
    "\n",
    "theme = gr.themes.Soft(primary_hue=\"blue\", secondary_hue=\"slate\")\n",
    "\n",
    "with gr.Blocks(theme=theme, title=\"Document Q&A\") as demo:\n",
    "    gr.Markdown(\"# üìö Document Q&A Assistant\")\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\"üìÅ Documents\"):\n",
    "            files = gr.File(label=\"Upload\", file_count=\"multiple\", file_types=[\".pdf\", \".txt\", \".md\"])\n",
    "            index_btn = gr.Button(\"Index\", variant=\"primary\")\n",
    "            status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "            \n",
    "            def index_docs(files):\n",
    "                if not files:\n",
    "                    return \"Upload files first\"\n",
    "                for f in files:\n",
    "                    with open(f.name, \"r\", errors=\"ignore\") as fp:\n",
    "                        rag.index_document(os.path.basename(f.name), fp.read())\n",
    "                stats = rag.get_stats()\n",
    "                return f\"Indexed {stats['total_documents']} docs, {stats['total_chunks']} chunks\"\n",
    "            \n",
    "            index_btn.click(index_docs, [files], [status])\n",
    "        \n",
    "        with gr.TabItem(\"üí¨ Chat\"):\n",
    "            chatbot = gr.Chatbot(height=400)\n",
    "            msg = gr.Textbox(label=\"Question\")\n",
    "            sources_box = gr.Textbox(label=\"Sources\", lines=5, interactive=False)\n",
    "            \n",
    "            def respond(message, history):\n",
    "                answer, sources = rag.query(message)\n",
    "                history.append((message, answer))\n",
    "                src_text = \"\\n\".join([f\"- {s['source']}: {s['text']}\" for s in sources])\n",
    "                return history, \"\", src_text\n",
    "            \n",
    "            msg.submit(respond, [msg, chatbot], [chatbot, msg, sources_box])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n",
    "'''\n",
    "\n",
    "with open(f'{deploy_dir}/app.py', 'w') as f:\n",
    "    f.write(app_py)\n",
    "\n",
    "# requirements.txt\n",
    "requirements = '''gradio>=4.44.0\n",
    "chromadb>=0.4.0\n",
    "sentence-transformers>=2.2.0\n",
    "pypdf>=3.0.0\n",
    "'''\n",
    "\n",
    "with open(f'{deploy_dir}/requirements.txt', 'w') as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "# README.md\n",
    "readme = '''---\n",
    "title: RAG Document Q&A\n",
    "emoji: üìö\n",
    "colorFrom: blue\n",
    "colorTo: purple\n",
    "sdk: gradio\n",
    "sdk_version: 4.44.0\n",
    "app_file: app.py\n",
    "pinned: true\n",
    "---\n",
    "\n",
    "# Document Q&A with RAG\n",
    "\n",
    "Upload documents and ask questions using Retrieval-Augmented Generation.\n",
    "'''\n",
    "\n",
    "with open(f'{deploy_dir}/README.md', 'w') as f:\n",
    "    f.write(readme)\n",
    "\n",
    "print(f\"‚úÖ Deployment files created in: {deploy_dir}\")\n",
    "print(f\"\\nFiles created:\")\n",
    "for f in os.listdir(deploy_dir):\n",
    "    print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Lab Exercises\n",
    "\n",
    "### Exercise 1: Add Streaming Responses\n",
    "\n",
    "Modify the `chat_response` function to stream the answer word by word instead of returning it all at once.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "Use a generator function:\n",
    "```python\n",
    "def streaming_response(message, history):\n",
    "    answer, sources = rag_system.query(message)\n",
    "    partial_answer = \"\"\n",
    "    for word in answer.split():\n",
    "        partial_answer += word + \" \"\n",
    "        yield history + [[message, partial_answer]], sources\n",
    "```\n",
    "</details>\n",
    "\n",
    "### Exercise 2: Add Confidence Indicators\n",
    "\n",
    "Modify the sources display to show a confidence score based on the similarity scores from ChromaDB.\n",
    "\n",
    "### Exercise 3: Add Export Functionality\n",
    "\n",
    "Add a button to export the chat history as a text file.\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've built:\n",
    "- ‚úÖ A complete RAG backend with ChromaDB\n",
    "- ‚úÖ Multi-tab Gradio interface\n",
    "- ‚úÖ Document upload and indexing\n",
    "- ‚úÖ Chat interface with source citations\n",
    "- ‚úÖ Custom theming\n",
    "- ‚úÖ Deployment-ready files\n",
    "\n",
    "---\n",
    "\n",
    "## üì§ Deliverable\n",
    "\n",
    "1. Deploy your RAG demo to Hugging Face Spaces\n",
    "2. Test with at least 3 different documents\n",
    "3. Share the URL with your instructor\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Lab complete! Ready for deployment.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
