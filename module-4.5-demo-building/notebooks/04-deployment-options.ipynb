{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4.5.4: Deployment Options - Sharing Your Demos\n",
    "\n",
    "**Module:** 4.5 - Demo Building & Prototyping  \n",
    "**Time:** 1-2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Deploy Gradio apps to Hugging Face Spaces\n",
    "- [ ] Deploy Streamlit apps to Streamlit Cloud\n",
    "- [ ] Understand alternative hosting options\n",
    "- [ ] Configure secrets and environment variables\n",
    "- [ ] Set up persistent storage for demos\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Tasks 4.5.1-4.5.3\n",
    "- Accounts: GitHub, Hugging Face (free)\n",
    "- Optional: Streamlit Cloud account\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "Your demo is working locally. Now you need to share it with:\n",
    "- Remote team members who can't SSH into your machine\n",
    "- Stakeholders who want a link they can bookmark\n",
    "- Potential employers viewing your portfolio\n",
    "\n",
    "This task covers the free, easy options for hosting AI demos.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßí ELI5: Deployment\n",
    "\n",
    "> **Local development** is like cooking in your kitchen - only you (and people in your house) can taste the food.\n",
    ">\n",
    "> **Deployment** is like opening a restaurant - now anyone can come and eat. You need:\n",
    "> - A location (server/hosting)\n",
    "> - A sign outside (URL)\n",
    "> - A kitchen that works without you watching (runs 24/7)\n",
    ">\n",
    "> **Hugging Face Spaces** and **Streamlit Cloud** are like food trucks with pre-built kitchens. You bring your recipe (code), they handle the rest!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Deployment Options Comparison\n",
    "\n",
    "| Platform | Best For | Free Tier | GPU Support | Custom Domain |\n",
    "|----------|----------|-----------|-------------|---------------|\n",
    "| **Hugging Face Spaces** | Gradio/Streamlit AI demos | ‚úÖ Generous | ‚úÖ (Paid) | ‚úÖ |\n",
    "| **Streamlit Cloud** | Streamlit apps | ‚úÖ 3 apps | ‚ùå | ‚úÖ (Paid) |\n",
    "| **Gradio Share** | Quick sharing (72h) | ‚úÖ | ‚ùå | ‚ùå |\n",
    "| **Railway** | Any Python app | ‚úÖ $5/month | ‚ùå | ‚úÖ |\n",
    "| **Render** | Any web app | ‚úÖ | ‚ùå | ‚úÖ |\n",
    "| **Self-hosted** | Full control | Your costs | ‚úÖ DGX Spark | ‚úÖ |\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Hugging Face Spaces\n",
    "\n",
    "### üßí ELI5: What is Hugging Face Spaces?\n",
    "\n",
    "> Hugging Face is like GitHub for AI. Spaces is their free hosting for demos. You push your code, they run it. Magic! ‚ú®\n",
    "\n",
    "### Why Use Spaces?\n",
    "\n",
    "- **Free** (with generous limits)\n",
    "- **GPU option** (for paid tiers)\n",
    "- **Automatic HTTPS**\n",
    "- **Easy secret management**\n",
    "- **HuggingFace model integration**\n",
    "- **Community visibility**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Hugging Face Space\n",
    "\n",
    "#### Step 1: Create the Space\n",
    "\n",
    "1. Go to [huggingface.co/new-space](https://huggingface.co/new-space)\n",
    "2. Choose a name (e.g., `my-rag-demo`)\n",
    "3. Select SDK: **Gradio** or **Streamlit**\n",
    "4. Choose visibility: **Public** or **Private**\n",
    "5. Click \"Create Space\"\n",
    "\n",
    "#### Step 2: File Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a sample Hugging Face Space structure\n",
    "space_dir = '/tmp/hf_space_demo'\n",
    "os.makedirs(space_dir, exist_ok=True)\n",
    "\n",
    "# README.md - Required metadata file\n",
    "readme_content = '''---\n",
    "title: RAG Document Q&A\n",
    "emoji: üìö\n",
    "colorFrom: blue\n",
    "colorTo: purple\n",
    "sdk: gradio\n",
    "sdk_version: 4.44.0\n",
    "app_file: app.py\n",
    "pinned: true\n",
    "license: mit\n",
    "---\n",
    "\n",
    "# RAG Document Q&A Demo\n",
    "\n",
    "Ask questions about uploaded documents using RAG (Retrieval-Augmented Generation).\n",
    "\n",
    "## Features\n",
    "- Upload PDF, TXT, or MD files\n",
    "- Ask natural language questions\n",
    "- Get answers with source citations\n",
    "\n",
    "## Usage\n",
    "1. Upload your documents in the \"Documents\" tab\n",
    "2. Switch to \"Chat\" tab\n",
    "3. Ask questions!\n",
    "\n",
    "## Tech Stack\n",
    "- Gradio for the interface\n",
    "- ChromaDB for vector storage\n",
    "- Sentence Transformers for embeddings\n",
    "'''\n",
    "\n",
    "with open(f'{space_dir}/README.md', 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"Created README.md with Space metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py - Main application file\n",
    "app_content = '''import gradio as gr\n",
    "import os\n",
    "\n",
    "# Access secrets (set in Space settings)\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\")\n",
    "OLLAMA_HOST = os.environ.get(\"OLLAMA_HOST\", \"\")\n",
    "\n",
    "# Your demo code\n",
    "def process_documents(files):\n",
    "    \"\"\"Process uploaded documents.\"\"\"\n",
    "    if not files:\n",
    "        return \"Please upload at least one file.\"\n",
    "    \n",
    "    # In a real app, you would:\n",
    "    # 1. Parse the documents\n",
    "    # 2. Create embeddings\n",
    "    # 3. Store in vector database\n",
    "    \n",
    "    file_names = [f.name for f in files]\n",
    "    return f\"Successfully processed {len(files)} files: {file_names}\"\n",
    "\n",
    "def answer_question(question, history):\n",
    "    \"\"\"Answer a question using RAG.\"\"\"\n",
    "    if not question:\n",
    "        return history\n",
    "    \n",
    "    # In a real app, you would:\n",
    "    # 1. Create query embedding\n",
    "    # 2. Retrieve relevant chunks\n",
    "    # 3. Generate response with LLM\n",
    "    \n",
    "    response = f\"Based on the documents, here\\'s what I found about: {question}\"\n",
    "    history.append((question, response))\n",
    "    return history\n",
    "\n",
    "# Build the interface\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# üìö RAG Document Q&A\")\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        # Documents tab\n",
    "        with gr.TabItem(\"üìÅ Documents\"):\n",
    "            files = gr.File(\n",
    "                label=\"Upload Documents\",\n",
    "                file_count=\"multiple\",\n",
    "                file_types=[\".pdf\", \".txt\", \".md\"]\n",
    "            )\n",
    "            process_btn = gr.Button(\"Process Documents\", variant=\"primary\")\n",
    "            status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "            \n",
    "            process_btn.click(process_documents, [files], [status])\n",
    "        \n",
    "        # Chat tab\n",
    "        with gr.TabItem(\"üí¨ Chat\"):\n",
    "            chatbot = gr.Chatbot(height=400)\n",
    "            msg = gr.Textbox(\n",
    "                label=\"Ask a Question\",\n",
    "                placeholder=\"What would you like to know?\"\n",
    "            )\n",
    "            msg.submit(answer_question, [msg, chatbot], [chatbot])\n",
    "    \n",
    "    gr.Markdown(\"---\")\n",
    "    gr.Markdown(\"*Built with Gradio on Hugging Face Spaces*\")\n",
    "\n",
    "# Launch (Spaces handles this automatically)\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n",
    "'''\n",
    "\n",
    "with open(f'{space_dir}/app.py', 'w') as f:\n",
    "    f.write(app_content)\n",
    "\n",
    "print(\"Created app.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements.txt - Dependencies\n",
    "requirements = '''gradio>=4.44.0\n",
    "transformers>=4.35.0\n",
    "sentence-transformers>=2.2.0\n",
    "chromadb>=0.4.0\n",
    "pypdf>=3.0.0\n",
    "'''\n",
    "\n",
    "with open(f'{space_dir}/requirements.txt', 'w') as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "print(\"Created requirements.txt\")\n",
    "print(f\"\\nSpace files created in: {space_dir}\")\n",
    "print(\"\\nTo deploy:\")\n",
    "print(\"1. Create a new Space on huggingface.co\")\n",
    "print(\"2. Clone the Space repository\")\n",
    "print(\"3. Copy these files to the repository\")\n",
    "print(\"4. git add, commit, and push!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing Secrets on Spaces\n",
    "\n",
    "Never hardcode API keys! Use Space secrets:\n",
    "\n",
    "1. Go to your Space ‚Üí Settings ‚Üí Repository secrets\n",
    "2. Add secrets like `HF_TOKEN`, `OPENAI_API_KEY`, etc.\n",
    "3. Access them in code with `os.environ.get(\"SECRET_NAME\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Accessing secrets safely\n",
    "\n",
    "secrets_example = '''\n",
    "import os\n",
    "\n",
    "def get_config():\n",
    "    \"\"\"Get configuration from environment variables.\"\"\"\n",
    "    return {\n",
    "        # Required secrets (will fail if not set)\n",
    "        \"hf_token\": os.environ[\"HF_TOKEN\"],\n",
    "        \n",
    "        # Optional secrets (with defaults)\n",
    "        \"ollama_host\": os.environ.get(\"OLLAMA_HOST\", \"http://localhost:11434\"),\n",
    "        \"model_name\": os.environ.get(\"MODEL_NAME\", \"llama3.1:8b\"),\n",
    "        \"debug_mode\": os.environ.get(\"DEBUG\", \"false\").lower() == \"true\",\n",
    "    }\n",
    "\n",
    "# Usage\n",
    "config = get_config()\n",
    "print(f\"Using model: {config['model_name']}\")\n",
    "'''\n",
    "\n",
    "print(\"Secret management pattern:\")\n",
    "print(secrets_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU on Spaces (Paid Feature)\n",
    "\n",
    "For GPU-enabled Spaces, add this to your README.md:\n",
    "\n",
    "```yaml\n",
    "---\n",
    "hardware: t4-small   # Options: cpu, t4-small, t4-medium, a10g-small, etc.\n",
    "---\n",
    "```\n",
    "\n",
    "Available hardware options:\n",
    "- `cpu` - Free, 2 vCPU, 16GB RAM\n",
    "- `t4-small` - $0.40/hour, T4 GPU, 4 vCPU\n",
    "- `t4-medium` - $0.60/hour, T4 GPU, 8 vCPU\n",
    "- `a10g-small` - $1.05/hour, A10G GPU\n",
    "- `a10g-large` - $3.15/hour, A10G GPU, 24 vCPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Streamlit Cloud\n",
    "\n",
    "### üßí ELI5: Streamlit Cloud\n",
    "\n",
    "> If Hugging Face is the food truck of AI demos, Streamlit Cloud is the food truck specifically for Streamlit apps. One-click deployment from GitHub!\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **GitHub integration** - Deploy straight from your repo\n",
    "- **Auto-rebuild** - Push to GitHub ‚Üí App updates\n",
    "- **Secret management** - Built-in\n",
    "- **3 free apps** - Generous free tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Streamlit Cloud-ready app structure\n",
    "streamlit_cloud_dir = '/tmp/streamlit_cloud_demo'\n",
    "os.makedirs(f'{streamlit_cloud_dir}/.streamlit', exist_ok=True)\n",
    "\n",
    "# Main app\n",
    "app_content = '''import streamlit as st\n",
    "import os\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"AI Agent Playground\",\n",
    "    page_icon=\"ü§ñ\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# Access secrets (from Streamlit Cloud)\n",
    "# In local development, use .streamlit/secrets.toml\n",
    "def get_secret(key, default=\"\"):\n",
    "    try:\n",
    "        return st.secrets[key]\n",
    "    except:\n",
    "        return os.environ.get(key, default)\n",
    "\n",
    "# Initialize session state\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "st.title(\"ü§ñ AI Agent Playground\")\n",
    "\n",
    "# Sidebar\n",
    "with st.sidebar:\n",
    "    st.header(\"Settings\")\n",
    "    model = st.selectbox(\"Model\", [\"GPT-4\", \"Claude\", \"Llama\"])\n",
    "    temperature = st.slider(\"Temperature\", 0.0, 1.0, 0.7)\n",
    "    \n",
    "    if st.button(\"Clear Chat\"):\n",
    "        st.session_state.messages = []\n",
    "        st.rerun()\n",
    "\n",
    "# Chat interface\n",
    "for msg in st.session_state.messages:\n",
    "    with st.chat_message(msg[\"role\"]):\n",
    "        st.write(msg[\"content\"])\n",
    "\n",
    "if prompt := st.chat_input(\"Ask something...\"):\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    st.chat_message(\"user\").write(prompt)\n",
    "    \n",
    "    # Mock response (replace with real LLM call)\n",
    "    response = f\"[{model}] Response to: {prompt}\"\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    st.chat_message(\"assistant\").write(response)\n",
    "'''\n",
    "\n",
    "with open(f'{streamlit_cloud_dir}/app.py', 'w') as f:\n",
    "    f.write(app_content)\n",
    "\n",
    "print(\"Created app.py for Streamlit Cloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements.txt\n",
    "requirements = '''streamlit>=1.30.0\n",
    "openai>=1.0.0\n",
    "'''\n",
    "\n",
    "with open(f'{streamlit_cloud_dir}/requirements.txt', 'w') as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "# .streamlit/config.toml - Theme and settings\n",
    "config = '''[theme]\n",
    "primaryColor = \"#FF4B4B\"\n",
    "backgroundColor = \"#FFFFFF\"\n",
    "secondaryBackgroundColor = \"#F0F2F6\"\n",
    "textColor = \"#262730\"\n",
    "font = \"sans serif\"\n",
    "\n",
    "[server]\n",
    "maxUploadSize = 200\n",
    "'''\n",
    "\n",
    "with open(f'{streamlit_cloud_dir}/.streamlit/config.toml', 'w') as f:\n",
    "    f.write(config)\n",
    "\n",
    "# .streamlit/secrets.toml - Local secrets (DO NOT COMMIT!)\n",
    "# This is for local development only\n",
    "secrets_template = '''# Local development secrets\n",
    "# DO NOT commit this file! Add to .gitignore\n",
    "\n",
    "OPENAI_API_KEY = \"sk-your-key-here\"\n",
    "DATABASE_URL = \"postgresql://...\"\n",
    "\n",
    "[custom_section]\n",
    "api_key = \"your-key\"\n",
    "base_url = \"https://api.example.com\"\n",
    "'''\n",
    "\n",
    "print(\"\\nFor Streamlit Cloud secrets:\")\n",
    "print(\"1. Go to your app ‚Üí Settings ‚Üí Secrets\")\n",
    "print(\"2. Add secrets in TOML format\")\n",
    "print(\"3. Access with st.secrets['KEY']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying to Streamlit Cloud\n",
    "\n",
    "1. **Push to GitHub** - Make sure your app is in a public (or connected private) repo\n",
    "2. **Go to [share.streamlit.io](https://share.streamlit.io)**\n",
    "3. **Click \"New app\"**\n",
    "4. **Select repository and branch**\n",
    "5. **Choose the main file** (e.g., `app.py`)\n",
    "6. **Add secrets** in Advanced settings\n",
    "7. **Deploy!**\n",
    "\n",
    "Your app will be available at: `https://your-app.streamlit.app`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Quick Sharing with Gradio\n",
    "\n",
    "For quick, temporary sharing (72 hours), Gradio has a built-in option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def echo(text):\n",
    "    return f\"Echo: {text}\"\n",
    "\n",
    "demo = gr.Interface(fn=echo, inputs=\"text\", outputs=\"text\")\n",
    "\n",
    "# This creates a temporary public URL!\n",
    "# demo.launch(share=True)\n",
    "\n",
    "print(\"To create a temporary public URL:\")\n",
    "print(\"  demo.launch(share=True)\")\n",
    "print(\"\")\n",
    "print(\"This will output something like:\")\n",
    "print(\"  Running on public URL: https://abc123xyz.gradio.live\")\n",
    "print(\"\")\n",
    "print(\"‚ö†Ô∏è Note: The link expires after 72 hours!\")\n",
    "print(\"For permanent hosting, use Hugging Face Spaces.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Self-Hosting on DGX Spark\n",
    "\n",
    "For full control and GPU access, host on your own DGX Spark!\n",
    "\n",
    "### Running as a Background Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-hosting options on DGX Spark\n",
    "\n",
    "self_host_guide = '''\n",
    "# ============================================================================\n",
    "# OPTION 1: Screen/tmux (Quick & Dirty)\n",
    "# ============================================================================\n",
    "\n",
    "# Start a screen session\n",
    "screen -S demo\n",
    "\n",
    "# Run your app\n",
    "streamlit run app.py --server.port 8501 --server.address 0.0.0.0\n",
    "\n",
    "# Detach with Ctrl+A, D\n",
    "# Reattach with: screen -r demo\n",
    "\n",
    "# ============================================================================\n",
    "# OPTION 2: Systemd Service (Production)\n",
    "# ============================================================================\n",
    "\n",
    "# Create service file: /etc/systemd/system/my-demo.service\n",
    "\"\"\"\n",
    "[Unit]\n",
    "Description=My AI Demo\n",
    "After=network.target\n",
    "\n",
    "[Service]\n",
    "Type=simple\n",
    "User=ubuntu\n",
    "WorkingDirectory=/home/ubuntu/my-demo\n",
    "ExecStart=/usr/bin/python -m streamlit run app.py --server.port 8501 --server.address 0.0.0.0\n",
    "Restart=always\n",
    "RestartSec=3\n",
    "\n",
    "[Install]\n",
    "WantedBy=multi-user.target\n",
    "\"\"\"\n",
    "\n",
    "# Enable and start\n",
    "# sudo systemctl daemon-reload\n",
    "# sudo systemctl enable my-demo\n",
    "# sudo systemctl start my-demo\n",
    "\n",
    "# ============================================================================\n",
    "# OPTION 3: Docker Compose (Recommended)\n",
    "# ============================================================================\n",
    "\n",
    "# docker-compose.yml\n",
    "\"\"\"\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  demo:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8501:8501\"\n",
    "    volumes:\n",
    "      - ./data:/app/data\n",
    "    environment:\n",
    "      - OPENAI_API_KEY=${OPENAI_API_KEY}\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              count: all\n",
    "              capabilities: [gpu]\n",
    "    restart: unless-stopped\n",
    "\"\"\"\n",
    "\n",
    "# Run with: docker-compose up -d\n",
    "'''\n",
    "\n",
    "print(self_host_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up a Reverse Proxy (nginx)\n",
    "\n",
    "For HTTPS and custom domains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nginx_config = '''\n",
    "# /etc/nginx/sites-available/my-demo\n",
    "\n",
    "server {\n",
    "    listen 80;\n",
    "    server_name demo.yourdomain.com;\n",
    "    \n",
    "    # Redirect HTTP to HTTPS\n",
    "    return 301 https://$server_name$request_uri;\n",
    "}\n",
    "\n",
    "server {\n",
    "    listen 443 ssl;\n",
    "    server_name demo.yourdomain.com;\n",
    "    \n",
    "    # SSL certificates (use Let's Encrypt)\n",
    "    ssl_certificate /etc/letsencrypt/live/demo.yourdomain.com/fullchain.pem;\n",
    "    ssl_certificate_key /etc/letsencrypt/live/demo.yourdomain.com/privkey.pem;\n",
    "    \n",
    "    location / {\n",
    "        proxy_pass http://localhost:8501;\n",
    "        proxy_http_version 1.1;\n",
    "        proxy_set_header Upgrade $http_upgrade;\n",
    "        proxy_set_header Connection \"upgrade\";\n",
    "        proxy_set_header Host $host;\n",
    "        proxy_set_header X-Real-IP $remote_addr;\n",
    "        proxy_read_timeout 86400;\n",
    "    }\n",
    "    \n",
    "    # For Streamlit websockets\n",
    "    location /_stcore/stream {\n",
    "        proxy_pass http://localhost:8501/_stcore/stream;\n",
    "        proxy_http_version 1.1;\n",
    "        proxy_set_header Upgrade $http_upgrade;\n",
    "        proxy_set_header Connection \"upgrade\";\n",
    "        proxy_read_timeout 86400;\n",
    "    }\n",
    "}\n",
    "'''\n",
    "\n",
    "print(\"Nginx reverse proxy configuration:\")\n",
    "print(nginx_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Alternative Platforms\n",
    "\n",
    "### Railway\n",
    "\n",
    "Great for Python apps with easy scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Railway deployment files\n",
    "\n",
    "railway_dir = '/tmp/railway_demo'\n",
    "os.makedirs(railway_dir, exist_ok=True)\n",
    "\n",
    "# Procfile - tells Railway how to run your app\n",
    "procfile = 'web: python -m streamlit run app.py --server.port $PORT --server.address 0.0.0.0\\n'\n",
    "\n",
    "with open(f'{railway_dir}/Procfile', 'w') as f:\n",
    "    f.write(procfile)\n",
    "\n",
    "# railway.toml - Railway configuration\n",
    "railway_config = '''[build]\n",
    "builder = \"nixpacks\"\n",
    "\n",
    "[deploy]\n",
    "startCommand = \"python -m streamlit run app.py --server.port $PORT --server.address 0.0.0.0\"\n",
    "restartPolicyType = \"ON_FAILURE\"\n",
    "restartPolicyMaxRetries = 10\n",
    "'''\n",
    "\n",
    "with open(f'{railway_dir}/railway.toml', 'w') as f:\n",
    "    f.write(railway_config)\n",
    "\n",
    "print(\"Railway deployment:\")\n",
    "print(\"1. Install Railway CLI: npm i -g @railway/cli\")\n",
    "print(\"2. Login: railway login\")\n",
    "print(\"3. Initialize: railway init\")\n",
    "print(\"4. Deploy: railway up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render\n",
    "\n",
    "Similar to Railway, with a generous free tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render.yaml - Render configuration (Blueprint)\n",
    "render_config = '''services:\n",
    "  - type: web\n",
    "    name: my-ai-demo\n",
    "    runtime: python\n",
    "    buildCommand: pip install -r requirements.txt\n",
    "    startCommand: python -m streamlit run app.py --server.port $PORT --server.address 0.0.0.0\n",
    "    envVars:\n",
    "      - key: OPENAI_API_KEY\n",
    "        sync: false  # Set in Render dashboard\n",
    "    autoDeploy: true\n",
    "'''\n",
    "\n",
    "print(\"Render deployment:\")\n",
    "print(\"1. Push your code to GitHub\")\n",
    "print(\"2. Connect repo on render.com\")\n",
    "print(\"3. Render auto-detects Python and deploys!\")\n",
    "print(\"\\nrender.yaml (optional - for advanced config):\")\n",
    "print(render_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Deployment Checklist\n",
    "\n",
    "Before deploying any demo:\n",
    "\n",
    "### Security\n",
    "- [ ] No hardcoded secrets in code\n",
    "- [ ] Secrets stored in platform's secret manager\n",
    "- [ ] `.gitignore` includes secret files\n",
    "- [ ] No debug mode in production\n",
    "\n",
    "### Reliability\n",
    "- [ ] Error handling for all external calls\n",
    "- [ ] Friendly error messages for users\n",
    "- [ ] Timeouts for long operations\n",
    "- [ ] Health check endpoint (if applicable)\n",
    "\n",
    "### Performance\n",
    "- [ ] Models cached properly\n",
    "- [ ] Static files optimized\n",
    "- [ ] Resource limits configured\n",
    "\n",
    "### User Experience\n",
    "- [ ] Loading states visible\n",
    "- [ ] Mobile-responsive (test with `share=True` on phone)\n",
    "- [ ] Clear instructions/examples provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Committing Secrets\n",
    "```python\n",
    "# ‚ùå NEVER do this\n",
    "OPENAI_API_KEY = \"sk-abc123...\"\n",
    "\n",
    "# ‚úÖ Always use environment variables\n",
    "import os\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "```\n",
    "\n",
    "### Mistake 2: Wrong Port Configuration\n",
    "```python\n",
    "# ‚ùå Hardcoded port (won't work on many platforms)\n",
    "demo.launch(server_port=7860)\n",
    "\n",
    "# ‚úÖ Use environment variable\n",
    "import os\n",
    "port = int(os.environ.get(\"PORT\", 7860))\n",
    "demo.launch(server_port=port)\n",
    "```\n",
    "\n",
    "### Mistake 3: Missing Dependencies\n",
    "```bash\n",
    "# ‚ùå Forgot to include all packages\n",
    "# requirements.txt is missing chromadb\n",
    "\n",
    "# ‚úÖ Generate from environment\n",
    "pip freeze > requirements.txt\n",
    "# Or better, use pip-tools:\n",
    "pip-compile requirements.in\n",
    "```\n",
    "\n",
    "### Mistake 4: Large Files in Git\n",
    "```bash\n",
    "# ‚ùå Model files in repo (100MB+ = GitHub rejects)\n",
    "git add models/llama-7b.bin\n",
    "\n",
    "# ‚úÖ Download at runtime or use external storage\n",
    "# In app.py:\n",
    "from huggingface_hub import hf_hub_download\n",
    "model_path = hf_hub_download(repo_id=\"meta/llama\", filename=\"model.bin\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ Deploying to Hugging Face Spaces (Gradio/Streamlit)\n",
    "- ‚úÖ Deploying to Streamlit Cloud\n",
    "- ‚úÖ Quick sharing with Gradio's `share=True`\n",
    "- ‚úÖ Self-hosting on DGX Spark\n",
    "- ‚úÖ Alternative platforms (Railway, Render)\n",
    "- ‚úÖ Secret management best practices\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "Deploy your favorite demo from this module to TWO platforms:\n",
    "1. Hugging Face Spaces (for community visibility)\n",
    "2. Streamlit Cloud or self-hosted (for your portfolio)\n",
    "\n",
    "Compare:\n",
    "- Setup difficulty\n",
    "- Performance\n",
    "- URL aesthetics\n",
    "- Update workflow\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [Hugging Face Spaces Documentation](https://huggingface.co/docs/hub/spaces)\n",
    "- [Streamlit Cloud Documentation](https://docs.streamlit.io/streamlit-community-cloud)\n",
    "- [Railway Documentation](https://docs.railway.app/)\n",
    "- [Render Documentation](https://render.com/docs)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all deployment files created\n",
    "import os\n",
    "\n",
    "dirs_created = [\n",
    "    '/tmp/hf_space_demo',\n",
    "    '/tmp/streamlit_cloud_demo',\n",
    "    '/tmp/railway_demo'\n",
    "]\n",
    "\n",
    "print(\"Deployment template directories created:\")\n",
    "for d in dirs_created:\n",
    "    if os.path.exists(d):\n",
    "        files = os.listdir(d)\n",
    "        print(f\"\\n{d}/\")\n",
    "        for f in files:\n",
    "            print(f\"  - {f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Ready to deploy!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
