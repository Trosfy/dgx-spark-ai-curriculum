{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.1.3: Multimodal RAG System\n",
    "\n",
    "**Module:** 4.1 - Multimodal AI  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand how CLIP enables image-text similarity search\n",
    "- [ ] Build a vector database that indexes both images and text\n",
    "- [ ] Query the database with natural language to find relevant content\n",
    "- [ ] Combine image retrieval with VLM analysis for RAG\n",
    "- [ ] Create a complete multimodal search pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Lab 4.1.1 (Vision-Language Models)\n",
    "- Knowledge of: Vector databases, embeddings, RAG concepts\n",
    "- Running in: NGC PyTorch container\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "Multimodal RAG is revolutionizing how we search and retrieve information:\n",
    "\n",
    "- **E-commerce**: \"Show me dresses similar to this photo but in blue\"\n",
    "- **Medical**: Find X-rays similar to a patient's scan with relevant notes\n",
    "- **Legal**: Search through document archives with both text and images\n",
    "- **Creative**: Find reference images that match a mood or concept\n",
    "- **Education**: Search lecture slides by visual content or topics\n",
    "\n",
    "---\n",
    "\n",
    "## üßí ELI5: What is Multimodal RAG?\n",
    "\n",
    "> **Imagine you're organizing a photo album with notes.** You want to find photos and notes about your beach vacation, but you can't remember exact words you used.\n",
    ">\n",
    "> **Regular text search** would only look at your written notes.  \n",
    "> **Regular image search** would only look at the photos.  \n",
    "> **Multimodal RAG** understands BOTH! It can:\n",
    "> - Find beach photos even if you search \"sunny ocean day\"\n",
    "> - Find notes about sunsets even if you search with a sunset image\n",
    "> - Combine results from both to give the best answer\n",
    ">\n",
    "> **In AI terms:** Multimodal RAG uses CLIP to create embeddings that live in the same \"meaning space\" for both images and text. Similar concepts cluster together regardless of whether they're images or text!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup\n",
    "\n",
    "Let's set up our environment for multimodal RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DGX Spark Environment Check\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {device.name}\")\n",
    "    print(f\"Memory: {device.total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "# !pip install chromadb>=0.4.22 sentence-transformers>=2.3.0 transformers>=4.45.0 pillow>=10.0.0 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import gc\n",
    "import time\n",
    "import hashlib\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding CLIP Embeddings\n",
    "\n",
    "CLIP is the foundation of multimodal RAG. It creates embeddings for both images and text in the same vector space.\n",
    "\n",
    "### üßí ELI5: CLIP Embeddings\n",
    "\n",
    "> **Think of CLIP as a universal translator.** It can \"read\" an image and an English sentence and tell you how similar they are.\n",
    ">\n",
    "> It converts both into \"coordinates\" in a special 768-dimensional space where:\n",
    "> - Similar things are close together\n",
    "> - Different things are far apart\n",
    "> - Images and text that match are at the same location!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "# Load CLIP model\n",
    "print(\"Loading CLIP model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# Move to GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = clip_model.to(device)\n",
    "clip_model.eval()\n",
    "\n",
    "print(f\"‚úÖ Loaded in {time.time() - start_time:.1f}s\")\n",
    "print(f\"Embedding dimension: {clip_model.config.projection_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embedding(image: Image.Image) -> np.ndarray:\n",
    "    \"\"\"Get CLIP embedding for a single image.\"\"\"\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "        # Normalize to unit vector\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    return features.cpu().numpy()[0]\n",
    "\n",
    "def get_text_embedding(text: str) -> np.ndarray:\n",
    "    \"\"\"Get CLIP embedding for text.\"\"\"\n",
    "    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    return features.cpu().numpy()[0]\n",
    "\n",
    "def get_batch_embeddings(images: List[Image.Image] = None, texts: List[str] = None) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Get embeddings for batches of images and/or texts.\"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    if images:\n",
    "        inputs = clip_processor(images=images, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = clip_model.get_image_features(**inputs)\n",
    "            features = features / features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        result['image_embeddings'] = features.cpu().numpy()\n",
    "    \n",
    "    if texts:\n",
    "        inputs = clip_processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = clip_model.get_text_features(**inputs)\n",
    "            features = features / features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        result['text_embeddings'] = features.cpu().numpy()\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"‚úÖ Embedding functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate embedding similarity\n",
    "# Let's see how CLIP embeddings capture semantic similarity\n",
    "\n",
    "texts = [\n",
    "    \"a golden retriever playing in the park\",\n",
    "    \"a dog running in the grass\",\n",
    "    \"a cute puppy\",\n",
    "    \"a cat sleeping on a couch\",\n",
    "    \"a red sports car\",\n",
    "]\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = [get_text_embedding(t) for t in texts]\n",
    "\n",
    "# Compute pairwise similarity\n",
    "print(\"üìä Text Similarity Matrix:\")\n",
    "print(\"(Higher values = more similar)\\n\")\n",
    "\n",
    "# Print header\n",
    "print(\" \" * 35, end=\"\")\n",
    "for i in range(len(texts)):\n",
    "    print(f\"{i+1:6}\", end=\"\")\n",
    "print()\n",
    "\n",
    "for i, (text_i, emb_i) in enumerate(zip(texts, embeddings)):\n",
    "    print(f\"{i+1}. {text_i[:30]:32}\", end=\"\")\n",
    "    for emb_j in embeddings:\n",
    "        similarity = np.dot(emb_i, emb_j)\n",
    "        print(f\"{similarity:6.2f}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "Notice how:\n",
    "- Texts about dogs (1-3) are highly similar to each other (~0.7-0.8)\n",
    "- The cat text (4) is moderately similar to dog texts (~0.5) since they're both pets\n",
    "- The car text (5) is very different from animal texts (~0.2)\n",
    "\n",
    "This is the power of CLIP - it understands **semantic meaning**, not just keywords!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Building the Vector Database\n",
    "\n",
    "We'll use ChromaDB to store and search our multimodal embeddings.\n",
    "\n",
    "### üßí ELI5: Vector Database\n",
    "\n",
    "> **A vector database is like a magical library** where instead of organizing books by title, you organize them by what they're about.\n",
    ">\n",
    "> Want to find books about \"adventure\"? The librarian doesn't search titles - they go to the \"adventure\" section and grab the closest books!\n",
    ">\n",
    "> In our case, the \"location\" of each item is its CLIP embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Create in-memory ChromaDB client\n",
    "# (Use PersistentClient for production)\n",
    "chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "\n",
    "# Create a collection for our multimodal content\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"multimodal_demo\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}  # Use cosine similarity\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ChromaDB collection created!\")\n",
    "print(f\"Collection name: {collection.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a sample dataset with images and text\n",
    "# For demo, we'll use some sample image URLs\n",
    "\n",
    "sample_data = [\n",
    "    {\n",
    "        \"type\": \"image\",\n",
    "        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg\",\n",
    "        \"description\": \"Orange tabby cat\"\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"image\",\n",
    "        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/YellowLabradorLooking_new.jpg/1200px-YellowLabradorLooking_new.jpg\",\n",
    "        \"description\": \"Yellow labrador retriever\"\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"content\": \"Dogs are loyal companions that love to play fetch and go for walks. They are known as man's best friend.\",\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"content\": \"Cats are independent pets that enjoy lounging in sunny spots and playing with toys. They purr when happy.\",\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"content\": \"Mountains are majestic natural formations that attract hikers and climbers from around the world.\",\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"content\": \"The ocean is vast and mysterious, home to countless marine species from tiny plankton to massive whales.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Sample dataset: {len(sample_data)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_url(url: str) -> Image.Image:\n",
    "    \"\"\"Load image from URL.\"\"\"\n",
    "    response = requests.get(url, timeout=10)\n",
    "    response.raise_for_status()\n",
    "    return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "# Index our sample data\n",
    "print(\"Indexing sample data...\")\n",
    "\n",
    "for i, item in enumerate(sample_data):\n",
    "    # Generate unique ID\n",
    "    item_id = f\"item_{i}\"\n",
    "    \n",
    "    if item[\"type\"] == \"image\":\n",
    "        # Load and embed image\n",
    "        print(f\"  Loading image: {item['description']}...\")\n",
    "        image = load_image_from_url(item[\"url\"])\n",
    "        embedding = get_image_embedding(image)\n",
    "        \n",
    "        metadata = {\n",
    "            \"content_type\": \"image\",\n",
    "            \"description\": item[\"description\"],\n",
    "            \"url\": item[\"url\"],\n",
    "        }\n",
    "        document = item[\"description\"]\n",
    "        \n",
    "    else:\n",
    "        # Embed text\n",
    "        print(f\"  Embedding text: {item['content'][:40]}...\")\n",
    "        embedding = get_text_embedding(item[\"content\"])\n",
    "        \n",
    "        metadata = {\n",
    "            \"content_type\": \"text\",\n",
    "        }\n",
    "        document = item[\"content\"]\n",
    "    \n",
    "    # Add to collection\n",
    "    collection.add(\n",
    "        ids=[item_id],\n",
    "        embeddings=[embedding.tolist()],\n",
    "        metadatas=[metadata],\n",
    "        documents=[document],\n",
    "    )\n",
    "\n",
    "print(f\"\\n‚úÖ Indexed {collection.count()} items!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Querying the Multimodal Index\n",
    "\n",
    "Now we can search our index using natural language or images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str, top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search the multimodal index with a text query.\n",
    "    \n",
    "    Args:\n",
    "        query: Natural language search query\n",
    "        top_k: Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of search results with content and similarity scores\n",
    "    \"\"\"\n",
    "    # Get query embedding\n",
    "    query_embedding = get_text_embedding(query)\n",
    "    \n",
    "    # Search ChromaDB\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding.tolist()],\n",
    "        n_results=top_k,\n",
    "        include=[\"metadatas\", \"documents\", \"distances\"],\n",
    "    )\n",
    "    \n",
    "    # Format results\n",
    "    formatted = []\n",
    "    for i in range(len(results[\"ids\"][0])):\n",
    "        # Convert distance to similarity (ChromaDB returns cosine distance)\n",
    "        similarity = 1 - results[\"distances\"][0][i]\n",
    "        \n",
    "        formatted.append({\n",
    "            \"id\": results[\"ids\"][0][i],\n",
    "            \"similarity\": similarity,\n",
    "            \"metadata\": results[\"metadatas\"][0][i],\n",
    "            \"content\": results[\"documents\"][0][i],\n",
    "        })\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "print(\"‚úÖ Search function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test search with different queries\n",
    "queries = [\n",
    "    \"cute furry pet\",\n",
    "    \"loyal companion animal\",\n",
    "    \"nature and outdoors\",\n",
    "    \"swimming and water\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nüîç Query: '{query}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    results = search(query, top_k=3)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        content_type = result[\"metadata\"][\"content_type\"]\n",
    "        similarity = result[\"similarity\"]\n",
    "        \n",
    "        if content_type == \"image\":\n",
    "            content = f\"[Image: {result['metadata']['description']}]\"\n",
    "        else:\n",
    "            content = result[\"content\"][:60] + \"...\"\n",
    "        \n",
    "        emoji = \"üñºÔ∏è\" if content_type == \"image\" else \"üìù\"\n",
    "        print(f\"  {i}. {emoji} {similarity:.3f} - {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "Notice how:\n",
    "- \"cute furry pet\" finds both cat and dog images AND the text about cats/dogs\n",
    "- \"loyal companion animal\" prioritizes the dog content\n",
    "- \"nature and outdoors\" finds the mountain text\n",
    "- \"swimming and water\" finds the ocean text\n",
    "\n",
    "**The magic**: Images and text are searched together, and the most semantically relevant content rises to the top regardless of format!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Building a Complete Multimodal RAG Pipeline\n",
    "\n",
    "Now let's combine everything into a production-ready RAG system!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SearchResult:\n",
    "    \"\"\"A single search result.\"\"\"\n",
    "    content_type: str  # \"image\" or \"text\"\n",
    "    content: str       # Text content or image description\n",
    "    score: float       # Similarity score (0-1)\n",
    "    metadata: Dict[str, Any]\n",
    "    image: Optional[Image.Image] = None  # Loaded image if applicable\n",
    "\n",
    "\n",
    "class MultimodalRAG:\n",
    "    \"\"\"\n",
    "    A complete multimodal RAG system using CLIP and ChromaDB.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"multimodal_rag\"):\n",
    "        \"\"\"Initialize the RAG system.\"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        # Initialize ChromaDB\n",
    "        self.client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        \n",
    "        # CLIP model reference (use existing loaded model)\n",
    "        self.clip_model = clip_model\n",
    "        self.clip_processor = clip_processor\n",
    "        self.device = device\n",
    "    \n",
    "    def _get_id(self, content: str, content_type: str) -> str:\n",
    "        \"\"\"Generate unique ID for content.\"\"\"\n",
    "        return hashlib.md5(f\"{content_type}:{content}\".encode()).hexdigest()\n",
    "    \n",
    "    def _embed_image(self, image: Image.Image) -> np.ndarray:\n",
    "        \"\"\"Get CLIP embedding for image.\"\"\"\n",
    "        inputs = self.clip_processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = self.clip_model.get_image_features(**inputs)\n",
    "            features = features / features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        return features.cpu().numpy()[0]\n",
    "    \n",
    "    def _embed_text(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Get CLIP embedding for text.\"\"\"\n",
    "        inputs = self.clip_processor(text=[text], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = self.clip_model.get_text_features(**inputs)\n",
    "            features = features / features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        return features.cpu().numpy()[0]\n",
    "    \n",
    "    def add_image(self, image: Image.Image, description: str, metadata: Dict = None) -> str:\n",
    "        \"\"\"\n",
    "        Add an image to the index.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image\n",
    "            description: Text description for display\n",
    "            metadata: Additional metadata\n",
    "            \n",
    "        Returns:\n",
    "            ID of added item\n",
    "        \"\"\"\n",
    "        item_id = self._get_id(description, \"image\")\n",
    "        embedding = self._embed_image(image)\n",
    "        \n",
    "        meta = {\n",
    "            \"content_type\": \"image\",\n",
    "            \"description\": description,\n",
    "        }\n",
    "        if metadata:\n",
    "            meta.update(metadata)\n",
    "        \n",
    "        self.collection.add(\n",
    "            ids=[item_id],\n",
    "            embeddings=[embedding.tolist()],\n",
    "            metadatas=[meta],\n",
    "            documents=[description],\n",
    "        )\n",
    "        \n",
    "        return item_id\n",
    "    \n",
    "    def add_text(self, text: str, metadata: Dict = None) -> str:\n",
    "        \"\"\"\n",
    "        Add text to the index.\n",
    "        \n",
    "        Args:\n",
    "            text: Text content\n",
    "            metadata: Additional metadata\n",
    "            \n",
    "        Returns:\n",
    "            ID of added item\n",
    "        \"\"\"\n",
    "        item_id = self._get_id(text, \"text\")\n",
    "        embedding = self._embed_text(text)\n",
    "        \n",
    "        meta = {\n",
    "            \"content_type\": \"text\",\n",
    "        }\n",
    "        if metadata:\n",
    "            meta.update(metadata)\n",
    "        \n",
    "        self.collection.add(\n",
    "            ids=[item_id],\n",
    "            embeddings=[embedding.tolist()],\n",
    "            metadatas=[meta],\n",
    "            documents=[text],\n",
    "        )\n",
    "        \n",
    "        return item_id\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 5, content_type: str = None) -> List[SearchResult]:\n",
    "        \"\"\"\n",
    "        Search the index with a text query.\n",
    "        \n",
    "        Args:\n",
    "            query: Natural language search query\n",
    "            top_k: Number of results\n",
    "            content_type: Filter by \"image\" or \"text\" (None for both)\n",
    "            \n",
    "        Returns:\n",
    "            List of SearchResult objects\n",
    "        \"\"\"\n",
    "        query_embedding = self._embed_text(query)\n",
    "        \n",
    "        # Build where clause\n",
    "        where = None\n",
    "        if content_type:\n",
    "            where = {\"content_type\": content_type}\n",
    "        \n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding.tolist()],\n",
    "            n_results=top_k,\n",
    "            where=where,\n",
    "            include=[\"metadatas\", \"documents\", \"distances\"],\n",
    "        )\n",
    "        \n",
    "        search_results = []\n",
    "        for i in range(len(results[\"ids\"][0])):\n",
    "            similarity = 1 - results[\"distances\"][0][i]\n",
    "            meta = results[\"metadatas\"][0][i]\n",
    "            \n",
    "            search_results.append(SearchResult(\n",
    "                content_type=meta[\"content_type\"],\n",
    "                content=results[\"documents\"][0][i],\n",
    "                score=similarity,\n",
    "                metadata=meta,\n",
    "            ))\n",
    "        \n",
    "        return search_results\n",
    "    \n",
    "    def search_by_image(self, image: Image.Image, top_k: int = 5) -> List[SearchResult]:\n",
    "        \"\"\"\n",
    "        Search the index using an image query.\n",
    "        \n",
    "        Args:\n",
    "            image: Query image\n",
    "            top_k: Number of results\n",
    "            \n",
    "        Returns:\n",
    "            List of SearchResult objects\n",
    "        \"\"\"\n",
    "        query_embedding = self._embed_image(image)\n",
    "        \n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding.tolist()],\n",
    "            n_results=top_k,\n",
    "            include=[\"metadatas\", \"documents\", \"distances\"],\n",
    "        )\n",
    "        \n",
    "        search_results = []\n",
    "        for i in range(len(results[\"ids\"][0])):\n",
    "            similarity = 1 - results[\"distances\"][0][i]\n",
    "            meta = results[\"metadatas\"][0][i]\n",
    "            \n",
    "            search_results.append(SearchResult(\n",
    "                content_type=meta[\"content_type\"],\n",
    "                content=results[\"documents\"][0][i],\n",
    "                score=similarity,\n",
    "                metadata=meta,\n",
    "            ))\n",
    "        \n",
    "        return search_results\n",
    "    \n",
    "    def count(self) -> int:\n",
    "        \"\"\"Get number of items in index.\"\"\"\n",
    "        return self.collection.count()\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear all items from index.\"\"\"\n",
    "        self.client.delete_collection(self.collection_name)\n",
    "        self.collection = self.client.create_collection(\n",
    "            name=self.collection_name,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "\n",
    "print(\"‚úÖ MultimodalRAG class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and populate a new RAG instance\n",
    "rag = MultimodalRAG(\"production_rag\")\n",
    "\n",
    "# Add more diverse content\n",
    "print(\"Building knowledge base...\")\n",
    "\n",
    "# Add images\n",
    "image_data = [\n",
    "    (\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg\", \"Orange tabby cat with green eyes\"),\n",
    "    (\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/YellowLabradorLooking_new.jpg/1200px-YellowLabradorLooking_new.jpg\", \"Yellow labrador retriever dog\"),\n",
    "]\n",
    "\n",
    "for url, desc in image_data:\n",
    "    print(f\"  Adding image: {desc}\")\n",
    "    img = load_image_from_url(url)\n",
    "    rag.add_image(img, desc)\n",
    "\n",
    "# Add text documents\n",
    "text_data = [\n",
    "    \"Dogs make wonderful family pets due to their loyal and playful nature. They need regular exercise and enjoy activities like walking, running, and playing fetch.\",\n",
    "    \"Cats are independent and low-maintenance pets. They are excellent hunters and spend much of their time grooming and sleeping. Cats can live indoors or outdoors.\",\n",
    "    \"Golden retrievers are one of the most popular dog breeds. They are known for their friendly temperament and are often used as therapy and service dogs.\",\n",
    "    \"The African savanna is home to lions, elephants, giraffes, and many other amazing animals. It features vast grasslands with scattered trees.\",\n",
    "    \"Deep sea creatures have adapted to extreme pressure and darkness. Bioluminescent fish create their own light to attract prey in the ocean depths.\",\n",
    "    \"The Northern Lights, or Aurora Borealis, are natural light displays in the sky. They occur when solar particles interact with Earth's magnetic field.\",\n",
    "    \"Coffee is made from roasted coffee beans, the seeds of berries from the Coffea plant. It is one of the most consumed beverages in the world.\",\n",
    "    \"Machine learning is a subset of artificial intelligence that enables computers to learn from data. Neural networks are a popular machine learning architecture.\",\n",
    "]\n",
    "\n",
    "for text in text_data:\n",
    "    print(f\"  Adding text: {text[:40]}...\")\n",
    "    rag.add_text(text)\n",
    "\n",
    "print(f\"\\n‚úÖ Knowledge base built with {rag.count()} items!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the complete RAG system\n",
    "test_queries = [\n",
    "    \"friendly pet that plays fetch\",\n",
    "    \"wildlife in Africa\",\n",
    "    \"lights in the night sky\",\n",
    "    \"artificial intelligence and learning\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üîç Query: '{query}'\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = rag.search(query, top_k=3)\n",
    "    \n",
    "    for i, r in enumerate(results, 1):\n",
    "        emoji = \"üñºÔ∏è\" if r.content_type == \"image\" else \"üìù\"\n",
    "        content_preview = r.content[:70] + \"...\" if len(r.content) > 70 else r.content\n",
    "        print(f\"\\n  {i}. {emoji} Score: {r.score:.3f}\")\n",
    "        print(f\"     {content_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Image-Based Search\n",
    "\n",
    "We can also search using an image as the query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a query image\n",
    "query_image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Cat_November_2010-1a.jpg/1200px-Cat_November_2010-1a.jpg\"\n",
    "query_image = load_image_from_url(query_image_url)\n",
    "\n",
    "# Display the query image\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(query_image)\n",
    "plt.axis('off')\n",
    "plt.title(\"Query Image\")\n",
    "plt.show()\n",
    "\n",
    "# Search using the image\n",
    "print(\"\\nüîç Searching with image query...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = rag.search_by_image(query_image, top_k=5)\n",
    "\n",
    "for i, r in enumerate(results, 1):\n",
    "    emoji = \"üñºÔ∏è\" if r.content_type == \"image\" else \"üìù\"\n",
    "    content_preview = r.content[:70] + \"...\" if len(r.content) > 70 else r.content\n",
    "    print(f\"\\n  {i}. {emoji} Score: {r.score:.3f}\")\n",
    "    print(f\"     {content_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "When we searched with a cat image, the system found:\n",
    "1. The cat image already in our database (highest similarity)\n",
    "2. Text about cats\n",
    "3. Other pet-related content\n",
    "\n",
    "This is **reverse image search** enhanced with cross-modal understanding!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Normalizing Embeddings\n",
    "```python\n",
    "# ‚ùå Wrong: Raw embeddings without normalization\n",
    "embedding = clip_model.get_image_features(**inputs)\n",
    "# Different scale embeddings break similarity calculations!\n",
    "\n",
    "# ‚úÖ Right: Always normalize to unit vectors\n",
    "embedding = clip_model.get_image_features(**inputs)\n",
    "embedding = embedding / embedding.norm(dim=-1, keepdim=True)\n",
    "```\n",
    "**Why:** CLIP similarity is cosine similarity, which requires normalized vectors.\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 2: Using Wrong Distance Metric\n",
    "```python\n",
    "# ‚ùå Wrong: Using L2 distance with cosine embeddings\n",
    "collection = client.create_collection(\n",
    "    name=\"my_collection\",\n",
    "    metadata={\"hnsw:space\": \"l2\"}  # Wrong for CLIP!\n",
    ")\n",
    "\n",
    "# ‚úÖ Right: Use cosine distance\n",
    "collection = client.create_collection(\n",
    "    name=\"my_collection\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "```\n",
    "**Why:** CLIP embeddings are designed for cosine similarity.\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 3: Text Too Long for CLIP\n",
    "```python\n",
    "# ‚ùå Wrong: Long text gets truncated silently\n",
    "text = \"Very long document...\" * 100  # 77 tokens max!\n",
    "embedding = get_text_embedding(text)\n",
    "\n",
    "# ‚úÖ Right: Chunk long text or summarize\n",
    "chunks = split_into_chunks(text, max_tokens=70)\n",
    "for chunk in chunks:\n",
    "    rag.add_text(chunk)\n",
    "```\n",
    "**Why:** CLIP truncates text to 77 tokens. Long documents should be chunked.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How CLIP creates unified embeddings for images and text\n",
    "- ‚úÖ Building a vector database with ChromaDB\n",
    "- ‚úÖ Indexing both images and text in the same space\n",
    "- ‚úÖ Querying with natural language to find multimodal content\n",
    "- ‚úÖ Reverse image search across images and text\n",
    "- ‚úÖ Building a production-ready MultimodalRAG class\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "Build a **Visual Question Answering RAG** that:\n",
    "1. Takes a question about an image\n",
    "2. Retrieves relevant context from your knowledge base\n",
    "3. Uses a VLM to answer the question with the retrieved context\n",
    "\n",
    "Example: \"What type of animal is this and what do they eat?\" + image of a cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Your code here!\n",
    "\n",
    "def visual_qa_with_rag(\n",
    "    image: Image.Image,\n",
    "    question: str,\n",
    "    rag_system: MultimodalRAG,\n",
    "    vlm_model = None,\n",
    "    vlm_processor = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Answer questions about images using RAG context.\n",
    "    \n",
    "    Args:\n",
    "        image: Query image\n",
    "        question: Question about the image\n",
    "        rag_system: Multimodal RAG instance\n",
    "        vlm_model: Vision-language model\n",
    "        vlm_processor: VLM processor\n",
    "        \n",
    "    Returns:\n",
    "        Answer with RAG context\n",
    "    \"\"\"\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [CLIP Paper](https://arxiv.org/abs/2103.00020)\n",
    "- [ChromaDB Documentation](https://docs.trychroma.com/)\n",
    "- [LlamaIndex Multimodal RAG](https://docs.llamaindex.ai/en/stable/examples/multi_modal/multi_modal_retrieval/)\n",
    "- [OpenAI CLIP Repository](https://github.com/openai/CLIP)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "if 'clip_model' in dir():\n",
    "    del clip_model\n",
    "if 'clip_processor' in dir():\n",
    "    del clip_processor\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next lab, we'll build a **Document AI Pipeline** for processing PDFs with OCR and layout analysis!\n",
    "\n",
    "‚û°Ô∏è Continue to [Lab 04: Document AI Pipeline](./04-document-ai-pipeline.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
