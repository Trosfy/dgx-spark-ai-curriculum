{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.1.1: Vision-Language Models\n",
    "\n",
    "**Module:** 4.1 - Multimodal AI  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand how vision-language models combine visual and text understanding\n",
    "- [ ] Use LLaVA to analyze and describe images\n",
    "- [ ] Use CLIP for image-text similarity matching\n",
    "- [ ] Build a practical image analysis pipeline\n",
    "- [ ] Optimize VLM inference for DGX Spark's 128GB memory\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Module 3.6 (AI Agents)\n",
    "- Knowledge of: Transformers, PyTorch basics\n",
    "- Running in: NGC PyTorch container\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "Vision-language models are transforming how we interact with visual content:\n",
    "\n",
    "- **Accessibility**: Screen readers that describe images for visually impaired users\n",
    "- **E-commerce**: Automated product tagging and description generation\n",
    "- **Healthcare**: Analyzing medical images alongside patient records\n",
    "- **Security**: Understanding surveillance footage with natural language queries\n",
    "- **Creative**: AI assistants that can \"see\" and discuss your work\n",
    "\n",
    "---\n",
    "\n",
    "## üßí ELI5: What are Vision-Language Models?\n",
    "\n",
    "> **Imagine you have a friend who speaks two languages fluently** - let's call them \"Image\" and \"English.\" When you show them a photo, they can describe it in words. When you ask a question about the photo, they understand both the picture AND your words.\n",
    ">\n",
    "> Vision-language models work the same way! They have:\n",
    "> 1. **An \"eye\"** (vision encoder) - like CLIP's vision transformer - that converts images into a language the model understands\n",
    "> 2. **A \"brain\"** (language model) - like LLaMA - that can read, write, and reason\n",
    "> 3. **A \"translator\"** (projection layer) - that connects the eye to the brain\n",
    ">\n",
    "> **In AI terms:** VLMs encode images into the same representation space as text, allowing a language model to \"understand\" visual content as if it were reading about it.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup\n",
    "\n",
    "First, let's verify our DGX Spark environment and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability and memory\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DGX Spark Environment Check\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {device.name}\")\n",
    "    print(f\"Total Memory: {device.total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"Compute Capability: {device.major}.{device.minor}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected! VLMs require GPU acceleration.\")\n",
    "\n",
    "# Memory status\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    print(f\"\\nMemory Status:\")\n",
    "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"  Reserved: {reserved:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install transformers>=4.45.0 accelerate>=0.27.0 bitsandbytes>=0.42.0 pillow>=10.0.0 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import gc\n",
    "import time\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, List\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set default dtype for Blackwell optimization\n",
    "torch.set_default_dtype(torch.bfloat16)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "We've set up our environment with:\n",
    "- **torch.bfloat16**: The optimal data type for Blackwell GPUs (DGX Spark's GB10 chip)\n",
    "- **Memory monitoring**: Essential when working with large models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding CLIP - The Foundation\n",
    "\n",
    "Before diving into full VLMs, let's understand CLIP - the model that made vision-language AI practical.\n",
    "\n",
    "### üßí ELI5: How CLIP Works\n",
    "\n",
    "> **Imagine a game of matching cards.** On one side, you have image cards. On the other, you have text cards describing images.\n",
    ">\n",
    "> CLIP learned to play this game by looking at 400 million image-text pairs from the internet. It learned to put matching image and text cards close together in a \"magic space\" where similar things cluster.\n",
    ">\n",
    "> Now, when you give it a new image, it can find the closest text descriptions. When you give it text, it can find matching images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "# Load CLIP - it's lightweight (~2GB)\n",
    "print(\"Loading CLIP model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# Move to GPU\n",
    "clip_model = clip_model.to(\"cuda\")\n",
    "clip_model.eval()\n",
    "\n",
    "print(f\"Loaded in {time.time() - start_time:.1f}s\")\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load images\n",
    "def load_image(source: Union[str, Path]) -> Image.Image:\n",
    "    \"\"\"Load image from URL or local path.\"\"\"\n",
    "    source_str = str(source)\n",
    "    \n",
    "    if source_str.startswith((\"http://\", \"https://\")):\n",
    "        response = requests.get(source_str, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "    else:\n",
    "        image = Image.open(source_str)\n",
    "    \n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "# Load a sample image\n",
    "sample_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg\"\n",
    "sample_image = load_image(sample_url)\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(sample_image)\n",
    "plt.axis('off')\n",
    "plt.title(\"Sample Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP: Image-Text Similarity\n",
    "# Let's see how CLIP matches images with text descriptions\n",
    "\n",
    "text_options = [\n",
    "    \"a photo of a cat\",\n",
    "    \"a photo of a dog\",\n",
    "    \"a photo of a bird\",\n",
    "    \"a photo of a car\",\n",
    "    \"a photo of a house\",\n",
    "]\n",
    "\n",
    "# Process inputs\n",
    "inputs = clip_processor(\n",
    "    text=text_options,\n",
    "    images=sample_image,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "# Move to GPU\n",
    "inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "# Get similarity scores\n",
    "with torch.no_grad():\n",
    "    outputs = clip_model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # Image-to-text similarity\n",
    "    probs = logits_per_image.softmax(dim=1)  # Convert to probabilities\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüìä CLIP Image-Text Matching Results:\")\n",
    "print(\"=\" * 40)\n",
    "for text, prob in zip(text_options, probs[0]):\n",
    "    bar = \"‚ñà\" * int(prob * 40)\n",
    "    print(f\"{text:25} {prob:.1%} {bar}\")\n",
    "\n",
    "best_match = text_options[probs.argmax()]\n",
    "print(f\"\\nüéØ Best match: '{best_match}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "CLIP computed embeddings for both the image and all text options, then calculated the cosine similarity between them. The text description with the highest similarity score is the best match!\n",
    "\n",
    "**Key Insight**: CLIP can do \"zero-shot\" classification - it can recognize objects it's never been explicitly trained to classify, just by matching to text descriptions.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úã Try It Yourself\n",
    "\n",
    "Modify the code above to:\n",
    "1. Try a different image URL\n",
    "2. Add more text options (be creative!)\n",
    "3. Try more specific descriptions like \"an orange tabby cat sleeping\"\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "More specific text descriptions often work better! Try:\n",
    "- \"a close-up photo of a cat's face\"\n",
    "- \"a ginger/orange cat\"\n",
    "- \"a domestic short-haired cat\"\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: LLaVA - Visual Language Assistant\n",
    "\n",
    "Now let's move to a full vision-language model that can have conversations about images!\n",
    "\n",
    "### üßí ELI5: How LLaVA Works\n",
    "\n",
    "> **LLaVA is like a very smart friend who can look at a photo and answer ANY question about it.**\n",
    ">\n",
    "> Here's how it works:\n",
    "> 1. **The Eye (CLIP Vision Encoder)**: Looks at the image and creates a \"summary\" of what it sees\n",
    "> 2. **The Translator (Projection Layer)**: Converts that visual summary into words the brain can understand\n",
    "> 3. **The Brain (LLaMA)**: A powerful language model that can reason about what it \"sees\"\n",
    ">\n",
    "> Unlike CLIP which just matches images to text, LLaVA can generate NEW text describing what it sees!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up CLIP to free memory for LLaVA\n",
    "del clip_model, clip_processor\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Freed memory. Current usage: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "# Load LLaVA-1.5-7B\n",
    "# This model uses ~16GB of our 128GB - plenty of headroom!\n",
    "print(\"Loading LLaVA-1.5-7B...\")\n",
    "print(\"(This may take a minute on first run as it downloads the model)\")\n",
    "start_time = time.time()\n",
    "\n",
    "model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,  # Optimal for Blackwell\n",
    "    device_map=\"auto\",           # Automatically use GPU\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded in {time.time() - start_time:.1f}s\")\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB / 128 GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image(image: Image.Image, question: str, max_new_tokens: int = 256) -> str:\n",
    "    \"\"\"\n",
    "    Analyze an image and answer a question about it.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image to analyze\n",
    "        question: Question to answer about the image\n",
    "        max_new_tokens: Maximum length of response\n",
    "        \n",
    "    Returns:\n",
    "        Model's response\n",
    "    \"\"\"\n",
    "    # Create the conversation prompt\n",
    "    prompt = f\"USER: <image>\\n{question}\\nASSISTANT:\"\n",
    "    \n",
    "    # Process inputs\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # Deterministic for reproducibility\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    response = processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the assistant's response\n",
    "    if \"ASSISTANT:\" in response:\n",
    "        response = response.split(\"ASSISTANT:\")[-1].strip()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test it with our cat image!\n",
    "question = \"Describe this image in detail. What do you see?\"\n",
    "\n",
    "print(\"üñºÔ∏è  Image Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Question: {question}\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "response = analyze_image(sample_image, question)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"\\n‚è±Ô∏è  Generated in {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try different types of questions!\n",
    "\n",
    "questions = [\n",
    "    \"What color is this cat?\",\n",
    "    \"What is the cat's expression? Does it look happy, curious, or sleepy?\",\n",
    "    \"Is this cat indoors or outdoors?\",\n",
    "    \"What breed might this cat be?\",\n",
    "]\n",
    "\n",
    "print(\"üîç Multi-Question Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\n‚ùì {q}\")\n",
    "    response = analyze_image(sample_image, q, max_new_tokens=100)\n",
    "    print(f\"üí¨ {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "LLaVA processed our questions in two stages:\n",
    "1. **Visual encoding**: The image was converted to visual tokens (like \"words\" describing the image)\n",
    "2. **Language generation**: The LLM reasoned about the visual tokens and our question to generate a response\n",
    "\n",
    "**Notice**: The model can answer different types of questions - descriptive, emotional, spatial, and even make educated guesses about the breed!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Practical Application - Image Analysis Pipeline\n",
    "\n",
    "Let's build a more sophisticated pipeline that can analyze multiple aspects of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_image_analysis(image: Image.Image) -> dict:\n",
    "    \"\"\"\n",
    "    Perform comprehensive analysis of an image.\n",
    "    \n",
    "    Returns a structured analysis with multiple aspects.\n",
    "    \"\"\"\n",
    "    analysis = {}\n",
    "    \n",
    "    # 1. General Description\n",
    "    analysis[\"description\"] = analyze_image(\n",
    "        image,\n",
    "        \"Describe this image in 2-3 sentences. Focus on the main subject and setting.\",\n",
    "        max_new_tokens=150\n",
    "    )\n",
    "    \n",
    "    # 2. Objects Detection\n",
    "    analysis[\"objects\"] = analyze_image(\n",
    "        image,\n",
    "        \"List all the objects you can see in this image, separated by commas.\",\n",
    "        max_new_tokens=100\n",
    "    )\n",
    "    \n",
    "    # 3. Colors & Style\n",
    "    analysis[\"colors_style\"] = analyze_image(\n",
    "        image,\n",
    "        \"Describe the main colors and visual style of this image.\",\n",
    "        max_new_tokens=80\n",
    "    )\n",
    "    \n",
    "    # 4. Mood/Atmosphere\n",
    "    analysis[\"mood\"] = analyze_image(\n",
    "        image,\n",
    "        \"What is the mood or atmosphere of this image? Use 2-3 adjectives.\",\n",
    "        max_new_tokens=50\n",
    "    )\n",
    "    \n",
    "    # 5. Suggested Caption\n",
    "    analysis[\"caption\"] = analyze_image(\n",
    "        image,\n",
    "        \"Suggest a creative caption for this image suitable for social media.\",\n",
    "        max_new_tokens=50\n",
    "    )\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Run the analysis\n",
    "print(\"üî¨ Comprehensive Image Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "results = comprehensive_image_analysis(sample_image)\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "for key, value in results.items():\n",
    "    print(f\"\\nüìå {key.upper().replace('_', ' ')}:\")\n",
    "    print(f\"   {value}\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Total analysis time: {total_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself: Analyze Your Own Image\n",
    "\n",
    "Try analyzing a different image! You can:\n",
    "1. Use a URL to any image on the web\n",
    "2. Upload a local image to your workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Try your own image!\n",
    "# Uncomment and modify the URL below:\n",
    "\n",
    "# your_image_url = \"YOUR_IMAGE_URL_HERE\"\n",
    "# your_image = load_image(your_image_url)\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.imshow(your_image)\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "# Ask your own question:\n",
    "# your_question = \"What is happening in this image?\"\n",
    "# response = analyze_image(your_image, your_question)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: DGX Spark Optimization - Loading Larger Models\n",
    "\n",
    "With 128GB of unified memory, we can run much larger models! Let's explore our options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Size Comparison for DGX Spark\n",
    "print(\"üìä VLM Model Sizes on DGX Spark (128GB)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "models = [\n",
    "    (\"LLaVA-1.5-7B\", 16, \"Full precision, fastest\"),\n",
    "    (\"LLaVA-1.5-13B\", 28, \"Better quality, still fast\"),\n",
    "    (\"Qwen2-VL-7B\", 18, \"Excellent for documents\"),\n",
    "    (\"Qwen2-VL-72B (4-bit)\", 45, \"State-of-the-art with quantization\"),\n",
    "    (\"LLaVA-NeXT-34B\", 70, \"Best open-source VLM\"),\n",
    "]\n",
    "\n",
    "total_memory = 128\n",
    "\n",
    "for name, vram, notes in models:\n",
    "    usage_pct = (vram / total_memory) * 100\n",
    "    bar = \"‚ñà\" * int(usage_pct / 2) + \"‚ñë\" * (50 - int(usage_pct / 2))\n",
    "    fits = \"‚úÖ\" if vram < total_memory else \"‚ùå\"\n",
    "    print(f\"{fits} {name:25} {vram:3}GB [{bar}] {usage_pct:.0f}%\")\n",
    "    print(f\"   ‚îî‚îÄ {notes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Loading LLaVA-13B with 4-bit quantization\n",
    "# This is optional - only run if you want to try a larger model\n",
    "\n",
    "LOAD_LARGER_MODEL = False  # Change to True to try 13B model\n",
    "\n",
    "if LOAD_LARGER_MODEL:\n",
    "    # Clean up current model\n",
    "    del model, processor\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    from transformers import BitsAndBytesConfig\n",
    "    \n",
    "    # Configure 4-bit quantization\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "    \n",
    "    print(\"Loading LLaVA-1.5-13B with 4-bit quantization...\")\n",
    "    \n",
    "    model_name = \"llava-hf/llava-1.5-13b-hf\"\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(model_name)\n",
    "    model = LlavaForConditionalGeneration.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=quantization_config,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Loaded 13B model!\")\n",
    "    print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"Using existing 7B model. Set LOAD_LARGER_MODEL=True to try 13B.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Wrong Image Format\n",
    "```python\n",
    "# ‚ùå Wrong: Using RGBA image directly\n",
    "image = Image.open(\"screenshot.png\")  # May have alpha channel!\n",
    "inputs = processor(images=image, ...)  # Can cause errors\n",
    "\n",
    "# ‚úÖ Right: Always convert to RGB\n",
    "image = Image.open(\"screenshot.png\").convert(\"RGB\")\n",
    "inputs = processor(images=image, ...)\n",
    "```\n",
    "**Why:** VLMs expect RGB images. RGBA (with transparency) or grayscale can cause silent errors.\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 2: Forgetting to Move Inputs to GPU\n",
    "```python\n",
    "# ‚ùå Wrong: Inputs on CPU, model on GPU\n",
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)  # Error!\n",
    "\n",
    "# ‚úÖ Right: Move inputs to same device as model\n",
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "outputs = model.generate(**inputs)\n",
    "```\n",
    "**Why:** PyTorch requires all tensors to be on the same device.\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 3: Running Out of Memory\n",
    "```python\n",
    "# ‚ùå Wrong: Loading multiple large models\n",
    "clip_model = CLIPModel.from_pretrained(...)  # 2GB\n",
    "llava_model = LlavaForConditionalGeneration.from_pretrained(...)  # 16GB\n",
    "# Now trying to load another...\n",
    "\n",
    "# ‚úÖ Right: Clean up before loading new models\n",
    "del clip_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# Now safe to load new model\n",
    "```\n",
    "**Why:** Even with 128GB, loading models without cleanup can fragment memory.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How CLIP creates a shared embedding space for images and text\n",
    "- ‚úÖ How LLaVA combines a vision encoder with a language model\n",
    "- ‚úÖ How to analyze images and ask questions about them\n",
    "- ‚úÖ How to build practical image analysis pipelines\n",
    "- ‚úÖ How to optimize VLM loading for DGX Spark\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "Build an **Image Comparison Assistant** that can:\n",
    "1. Take two images as input\n",
    "2. Describe similarities between them\n",
    "3. Describe differences between them\n",
    "4. Determine which image is \"better\" for a given purpose\n",
    "\n",
    "Hint: You'll need to process both images and construct a clever prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Your code here!\n",
    "\n",
    "def compare_images(image1: Image.Image, image2: Image.Image, purpose: str = \"general\") -> dict:\n",
    "    \"\"\"\n",
    "    Compare two images and return analysis.\n",
    "    \n",
    "    Args:\n",
    "        image1: First image\n",
    "        image2: Second image\n",
    "        purpose: What the images are being compared for\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with similarities, differences, and recommendation\n",
    "    \"\"\"\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [LLaVA Paper](https://arxiv.org/abs/2304.08485) - Visual Instruction Tuning\n",
    "- [CLIP Paper](https://arxiv.org/abs/2103.00020) - Learning Transferable Visual Models\n",
    "- [Qwen-VL](https://arxiv.org/abs/2308.12966) - A Versatile Vision-Language Model\n",
    "- [HuggingFace VLM Hub](https://huggingface.co/models?other=vision-language-model)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "if 'model' in dir():\n",
    "    del model\n",
    "if 'processor' in dir():\n",
    "    del processor\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next lab, we'll explore **Image Generation** with Stable Diffusion and SDXL - learning to create images from text descriptions!\n",
    "\n",
    "‚û°Ô∏è Continue to [Lab 02: Image Generation](./02-image-generation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
