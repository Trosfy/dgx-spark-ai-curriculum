{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.1.2: Image Generation with Diffusion Models\n",
    "\n",
    "**Module:** 4.1 - Multimodal AI  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand how diffusion models generate images from noise\n",
    "- [ ] Use SDXL to generate high-quality images from text prompts\n",
    "- [ ] Apply ControlNet for guided image generation\n",
    "- [ ] Use image-to-image translation to transform existing images\n",
    "- [ ] Optimize generation for DGX Spark's capabilities\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Lab 4.1.1 (Vision-Language Models)\n",
    "- Knowledge of: Basic neural networks, PyTorch\n",
    "- Running in: NGC PyTorch container\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "Image generation has transformed creative industries:\n",
    "\n",
    "- **Design**: Rapid prototyping of product concepts\n",
    "- **Marketing**: Generating campaign imagery at scale\n",
    "- **Gaming**: Creating game assets and concept art\n",
    "- **Architecture**: Visualizing building designs\n",
    "- **Fashion**: Designing and previewing clothing\n",
    "\n",
    "---\n",
    "\n",
    "## üßí ELI5: How Do Diffusion Models Work?\n",
    "\n",
    "> **Imagine you have a beautiful painting, and you slowly add TV static noise to it** - like when an old TV loses signal. After adding lots of noise, the painting just looks like pure static.\n",
    ">\n",
    "> Diffusion models learn to **reverse this process**! They start with pure noise (static) and gradually remove it, step by step, until a beautiful image appears.\n",
    ">\n",
    "> The magic part: By telling the model what image you want (\"a cat wearing sunglasses\"), it removes the noise in a way that reveals that specific image!\n",
    ">\n",
    "> **In AI terms:** The model is trained on millions of images that have been progressively noised. It learns to predict and remove noise at each step, conditioned on a text description that guides what image should emerge.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup\n",
    "\n",
    "Let's set up our environment for image generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU and environment\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DGX Spark Environment Check\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {device.name}\")\n",
    "    print(f\"Total Memory: {device.total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"Compute Capability: {device.major}.{device.minor}\")\n",
    "    \n",
    "    # Check for Blackwell (compute capability 10.x)\n",
    "    if device.major >= 10:\n",
    "        print(\"‚úÖ Blackwell GPU detected - optimal for bfloat16!\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU detected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "# !pip install diffusers>=0.27.0 transformers>=4.45.0 accelerate>=0.27.0 safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Helper function to display images\n",
    "def show_image(image: Image.Image, title: str = \"\"):\n",
    "    \"\"\"Display a single image.\"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    if title:\n",
    "        plt.title(title, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_images(images: List[Image.Image], titles: List[str] = None, cols: int = 3):\n",
    "    \"\"\"Display multiple images in a grid.\"\"\"\n",
    "    n = len(images)\n",
    "    rows = (n + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 5*rows))\n",
    "    axes = axes.flatten() if n > 1 else [axes]\n",
    "    \n",
    "    for i, (ax, img) in enumerate(zip(axes, images)):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        if titles and i < len(titles):\n",
    "            ax.set_title(titles[i], fontsize=10)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for ax in axes[n:]:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Basic Image Generation with SDXL\n",
    "\n",
    "Stable Diffusion XL (SDXL) is a powerful open-source model that generates high-quality 1024x1024 images.\n",
    "\n",
    "### üßí ELI5: What is SDXL?\n",
    "\n",
    "> **SDXL is like a super talented artist** who has studied millions of images and can create almost anything you describe.\n",
    ">\n",
    "> When you give it a \"prompt\" (text description), it:\n",
    "> 1. **Understands** what you want using a text encoder (CLIP)\n",
    "> 2. **Starts** with random noise\n",
    "> 3. **Refines** the noise over many steps, guided by your description\n",
    "> 4. **Produces** a detailed 1024x1024 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline\n",
    "\n",
    "# Load SDXL - uses about 8GB VRAM\n",
    "print(\"Loading SDXL...\")\n",
    "print(\"(First run downloads ~6GB model)\")\n",
    "start_time = time.time()\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.bfloat16,  # Optimal for Blackwell\n",
    "    use_safetensors=True,\n",
    ")\n",
    "\n",
    "# Move to GPU\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "# Enable memory-efficient attention\n",
    "pipe.enable_vae_slicing()\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded in {time.time() - start_time:.1f}s\")\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your first image generation!\n",
    "prompt = \"A majestic mountain landscape at golden hour, with a crystal-clear lake reflecting snow-capped peaks, photorealistic, 8k\"\n",
    "\n",
    "# Negative prompt helps avoid unwanted elements\n",
    "negative_prompt = \"blurry, low quality, distorted, ugly, watermark, text\"\n",
    "\n",
    "print(f\"üé® Generating: '{prompt[:50]}...'\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate with fixed seed for reproducibility\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=30,  # More steps = better quality\n",
    "    guidance_scale=7.5,      # How closely to follow the prompt\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n‚è±Ô∏è Generated in {elapsed:.1f}s\")\n",
    "\n",
    "show_image(image, prompt[:80] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Understanding the Parameters\n",
    "\n",
    "| Parameter | What it does | Typical values |\n",
    "|-----------|--------------|----------------|\n",
    "| `num_inference_steps` | More steps = finer details, slower | 20-50 |\n",
    "| `guidance_scale` | Higher = follows prompt more strictly | 5-15 |\n",
    "| `generator` | Random seed for reproducibility | Any integer |\n",
    "| `negative_prompt` | What to avoid in the image | \"blurry, ugly...\" |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore different prompts!\n",
    "\n",
    "prompts = [\n",
    "    \"A cyberpunk city at night, neon lights, rain-slicked streets, cinematic\",\n",
    "    \"A cozy cottage in an enchanted forest, fairy lights, magical atmosphere\",\n",
    "    \"An astronaut riding a horse on Mars, digital art, highly detailed\",\n",
    "]\n",
    "\n",
    "images = []\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"Generating {i+1}/{len(prompts)}: {prompt[:40]}...\")\n",
    "    \n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(42 + i)\n",
    "    \n",
    "    image = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_inference_steps=25,\n",
    "        guidance_scale=7.5,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    \n",
    "    images.append(image)\n",
    "\n",
    "# Display all images\n",
    "show_images(images, [p[:40] + \"...\" for p in prompts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself\n",
    "\n",
    "Generate an image with your own prompt! Try:\n",
    "- Different artistic styles (\"oil painting\", \"watercolor\", \"anime\")\n",
    "- Different subjects (animals, people, landscapes, abstract)\n",
    "- Different moods (dark, cheerful, mysterious, serene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn! Modify the prompt below\n",
    "your_prompt = \"YOUR CREATIVE PROMPT HERE\"\n",
    "\n",
    "# Uncomment to generate:\n",
    "# generator = torch.Generator(device=\"cuda\").manual_seed(123)\n",
    "# your_image = pipe(\n",
    "#     prompt=your_prompt,\n",
    "#     negative_prompt=negative_prompt,\n",
    "#     num_inference_steps=30,\n",
    "#     guidance_scale=7.5,\n",
    "#     generator=generator,\n",
    "# ).images[0]\n",
    "# show_image(your_image, your_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Understanding Guidance Scale and Steps\n",
    "\n",
    "Let's visualize how different parameters affect the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different guidance scales\n",
    "prompt = \"A red apple on a wooden table, studio lighting\"\n",
    "\n",
    "guidance_scales = [3.0, 7.5, 12.0, 20.0]\n",
    "images = []\n",
    "\n",
    "print(\"Comparing guidance scales...\")\n",
    "for gs in guidance_scales:\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "    \n",
    "    image = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_inference_steps=25,\n",
    "        guidance_scale=gs,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    \n",
    "    images.append(image)\n",
    "    print(f\"  Guidance {gs}: Done\")\n",
    "\n",
    "show_images(images, [f\"Guidance = {gs}\" for gs in guidance_scales], cols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "- **Low guidance (3.0)**: More creative/random, may drift from prompt\n",
    "- **Medium guidance (7.5)**: Good balance - recommended default\n",
    "- **High guidance (12-20)**: Very literal interpretation, may look artificial\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different step counts\n",
    "step_counts = [10, 20, 30, 50]\n",
    "images = []\n",
    "times = []\n",
    "\n",
    "print(\"Comparing inference steps...\")\n",
    "for steps in step_counts:\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "    \n",
    "    start = time.time()\n",
    "    image = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_inference_steps=steps,\n",
    "        guidance_scale=7.5,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    images.append(image)\n",
    "    times.append(elapsed)\n",
    "    print(f\"  Steps {steps}: {elapsed:.1f}s\")\n",
    "\n",
    "show_images(images, [f\"Steps = {s} ({t:.1f}s)\" for s, t in zip(step_counts, times)], cols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Steps vs Quality Trade-off\n",
    "\n",
    "- **10 steps**: Fast but rough - good for previews\n",
    "- **20-30 steps**: Sweet spot for most uses\n",
    "- **50+ steps**: Diminishing returns, only for final output\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Image-to-Image Translation\n",
    "\n",
    "Transform existing images while keeping their structure!\n",
    "\n",
    "### üßí ELI5: Image-to-Image\n",
    "\n",
    "> **Imagine tracing over a photo with colored pencils in a new style.** The basic shapes stay the same, but the style completely changes.\n",
    ">\n",
    "> That's img2img! You give it a starting image and a prompt, and it transforms the image to match the new description while keeping the original composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLImg2ImgPipeline\n",
    "\n",
    "# Load img2img pipeline (shares weights with base)\n",
    "print(\"Loading img2img pipeline...\")\n",
    "\n",
    "img2img_pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "img2img_pipe = img2img_pipe.to(\"cuda\")\n",
    "img2img_pipe.enable_vae_slicing()\n",
    "\n",
    "print(\"‚úÖ Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create a base image\n",
    "base_prompt = \"A simple sketch of a house with a tree\"\n",
    "\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "base_image = pipe(\n",
    "    prompt=base_prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=25,\n",
    "    guidance_scale=7.5,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "\n",
    "show_image(base_image, \"Base Image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now transform it with different styles!\n",
    "style_prompts = [\n",
    "    \"A house and tree in the style of Van Gogh's Starry Night, oil painting\",\n",
    "    \"A house and tree as a Japanese anime scene, Studio Ghibli style\",\n",
    "    \"A house and tree in cyberpunk style, neon lights, futuristic\",\n",
    "]\n",
    "\n",
    "transformed_images = [base_image]  # Include original\n",
    "titles = [\"Original\"]\n",
    "\n",
    "for prompt in style_prompts:\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "    \n",
    "    transformed = img2img_pipe(\n",
    "        prompt=prompt,\n",
    "        image=base_image,\n",
    "        strength=0.75,  # How much to change (0-1)\n",
    "        num_inference_steps=30,\n",
    "        guidance_scale=7.5,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    \n",
    "    transformed_images.append(transformed)\n",
    "    titles.append(prompt.split(\",\")[0][:30])\n",
    "\n",
    "show_images(transformed_images, titles, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Understanding Strength Parameter\n",
    "\n",
    "The `strength` parameter controls how much the output can differ from the input:\n",
    "- **0.0**: No change (identical to input)\n",
    "- **0.5**: Moderate changes, keeps composition\n",
    "- **0.75**: Significant transformation\n",
    "- **1.0**: Complete reimagining (ignores input structure)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: ControlNet - Guided Generation\n",
    "\n",
    "ControlNet lets you guide image generation using edges, poses, depth maps, and more!\n",
    "\n",
    "### üßí ELI5: What is ControlNet?\n",
    "\n",
    "> **Imagine you're giving an artist specific instructions:** \"Paint a dog, but it should be in exactly THIS pose\" (shows a stick figure).\n",
    ">\n",
    "> ControlNet lets you give the AI similar \"guides\":\n",
    "> - **Edge detection**: The outlines of objects\n",
    "> - **Pose estimation**: Where people/animals should be positioned\n",
    "> - **Depth maps**: What should be in front vs background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up previous pipelines to free memory\n",
    "del img2img_pipe\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Memory after cleanup: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline\n",
    "import cv2\n",
    "\n",
    "# Load ControlNet for edge detection (Canny)\n",
    "print(\"Loading ControlNet...\")\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"diffusers/controlnet-canny-sdxl-1.0\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "controlnet_pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "controlnet_pipe = controlnet_pipe.to(\"cuda\")\n",
    "controlnet_pipe.enable_vae_slicing()\n",
    "\n",
    "print(f\"‚úÖ Loaded! Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_canny_edges(image: Image.Image, low_threshold: int = 100, high_threshold: int = 200) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Extract edge map from an image using Canny edge detection.\n",
    "    \"\"\"\n",
    "    # Convert to numpy array\n",
    "    img_array = np.array(image)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Apply Canny edge detection\n",
    "    edges = cv2.Canny(gray, low_threshold, high_threshold)\n",
    "    \n",
    "    # Convert back to RGB PIL Image\n",
    "    edges_rgb = np.stack([edges] * 3, axis=-1)\n",
    "    return Image.fromarray(edges_rgb)\n",
    "\n",
    "# Use our previously generated image\n",
    "control_image = get_canny_edges(base_image)\n",
    "\n",
    "# Display edge map\n",
    "show_images([base_image, control_image], [\"Original\", \"Edge Map (Canny)\"], cols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new images guided by the edge map\n",
    "controlnet_prompts = [\n",
    "    \"A Victorian mansion with a cherry blossom tree, sunset lighting\",\n",
    "    \"A futuristic building with a holographic tree, sci-fi\",\n",
    "    \"A gingerbread house with a candy cane tree, fantasy\",\n",
    "]\n",
    "\n",
    "controlled_images = [control_image]\n",
    "titles = [\"Edge Map\"]\n",
    "\n",
    "for prompt in controlnet_prompts:\n",
    "    print(f\"Generating: {prompt[:40]}...\")\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "    \n",
    "    result = controlnet_pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        image=control_image,\n",
    "        controlnet_conditioning_scale=0.5,  # How strongly to follow edges\n",
    "        num_inference_steps=30,\n",
    "        guidance_scale=7.5,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    \n",
    "    controlled_images.append(result)\n",
    "    titles.append(prompt.split(\",\")[0][:25])\n",
    "\n",
    "show_images(controlled_images, titles, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "Notice how all generated images follow the same basic structure (house + tree) but with completely different styles! The edge map acted as a \"skeleton\" for the AI to paint over.\n",
    "\n",
    "**Use cases for ControlNet:**\n",
    "- Convert sketches to realistic images\n",
    "- Maintain pose/composition when changing styles\n",
    "- Generate variations that keep the same layout\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: DGX Spark Optimization Tips\n",
    "\n",
    "Let's explore how to get the best performance from your DGX Spark!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory and speed optimization techniques\n",
    "print(\"üìä DGX Spark Image Generation Optimization Guide\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Current memory usage\n",
    "allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "\n",
    "print(f\"\\nCurrent Memory: {allocated:.1f}GB / {total:.1f}GB ({allocated/total*100:.1f}%)\")\n",
    "\n",
    "print(\"\"\"\n",
    "‚úÖ OPTIMIZATION TECHNIQUES:\n",
    "\n",
    "1. Use bfloat16 (native Blackwell support):\n",
    "   pipe.to(torch.bfloat16)  # Already enabled!\n",
    "\n",
    "2. VAE slicing for lower memory:\n",
    "   pipe.enable_vae_slicing()  # Already enabled!\n",
    "\n",
    "3. VAE tiling for very large images (2048x2048+):\n",
    "   pipe.enable_vae_tiling()\n",
    "\n",
    "4. Model CPU offload (if needed):\n",
    "   pipe.enable_model_cpu_offload()  # Slower but saves VRAM\n",
    "\n",
    "5. Compile with torch.compile() for 10-30% speedup:\n",
    "   pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\")\n",
    "\n",
    "6. Use smaller batch sizes:\n",
    "   num_images_per_prompt=1  # Default\n",
    "\n",
    "üìà WHAT FITS ON DGX SPARK (128GB):\n",
    "\n",
    "| Model               | VRAM   | Fits? |\n",
    "|---------------------|--------|-------|\n",
    "| SDXL Base           | ~8GB   | ‚úÖ Easily |\n",
    "| SDXL + Refiner      | ~16GB  | ‚úÖ Yes |\n",
    "| Flux.1-dev          | ~24GB  | ‚úÖ Yes |\n",
    "| SD 3.5 Large        | ~20GB  | ‚úÖ Yes |\n",
    "| Multiple models     | ~50GB  | ‚úÖ Yes |\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting Negative Prompt\n",
    "```python\n",
    "# ‚ùå Wrong: No negative prompt\n",
    "image = pipe(prompt=\"A cat\").images[0]  # May include artifacts\n",
    "\n",
    "# ‚úÖ Right: Always include negative prompt\n",
    "image = pipe(\n",
    "    prompt=\"A cat\",\n",
    "    negative_prompt=\"blurry, low quality, distorted, ugly\"\n",
    ").images[0]\n",
    "```\n",
    "**Why:** Negative prompts significantly improve quality by guiding the model away from common issues.\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 2: Using Wrong Image Size\n",
    "```python\n",
    "# ‚ùå Wrong: Non-standard size\n",
    "image = pipe(prompt=\"...\", width=1000, height=1000).images[0]  # Bad quality!\n",
    "\n",
    "# ‚úÖ Right: Use sizes the model was trained on\n",
    "# SDXL optimal sizes: 1024x1024, 1152x896, 896x1152, 1216x832, etc.\n",
    "image = pipe(prompt=\"...\", width=1024, height=1024).images[0]\n",
    "```\n",
    "**Why:** Models are trained on specific aspect ratios. Non-standard sizes cause artifacts.\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 3: Too Many Steps Without Benefit\n",
    "```python\n",
    "# ‚ùå Wrong: 100 steps takes forever with minimal quality gain\n",
    "image = pipe(prompt=\"...\", num_inference_steps=100).images[0]\n",
    "\n",
    "# ‚úÖ Right: Sweet spot is 25-35 steps\n",
    "image = pipe(prompt=\"...\", num_inference_steps=30).images[0]\n",
    "```\n",
    "**Why:** Quality gains diminish rapidly after ~30 steps. Use fewer steps for previews, more for final output.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How diffusion models generate images from noise\n",
    "- ‚úÖ Using SDXL for high-quality text-to-image generation\n",
    "- ‚úÖ Understanding guidance scale and inference steps\n",
    "- ‚úÖ Image-to-image translation for style transfer\n",
    "- ‚úÖ ControlNet for guided generation with edge maps\n",
    "- ‚úÖ Optimization techniques for DGX Spark\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "Build a **Style Transfer Pipeline** that:\n",
    "1. Takes a reference image\n",
    "2. Extracts its edge map using Canny\n",
    "3. Generates 5 different style variations\n",
    "4. Creates a comparison grid\n",
    "5. Saves all images with metadata (prompt, seed, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Your code here!\n",
    "\n",
    "def style_transfer_pipeline(\n",
    "    reference_image: Image.Image,\n",
    "    styles: List[str],\n",
    "    output_dir: str = \"outputs\"\n",
    ") -> List[Image.Image]:\n",
    "    \"\"\"\n",
    "    Apply multiple styles to a reference image using ControlNet.\n",
    "    \n",
    "    Args:\n",
    "        reference_image: Input image to transform\n",
    "        styles: List of style prompts\n",
    "        output_dir: Directory to save results\n",
    "        \n",
    "    Returns:\n",
    "        List of generated images\n",
    "    \"\"\"\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [Stable Diffusion Paper](https://arxiv.org/abs/2112.10752)\n",
    "- [SDXL Technical Report](https://arxiv.org/abs/2307.01952)\n",
    "- [ControlNet Paper](https://arxiv.org/abs/2302.05543)\n",
    "- [Diffusers Documentation](https://huggingface.co/docs/diffusers)\n",
    "- [Prompt Engineering Guide](https://stable-diffusion-art.com/prompt-guide/)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "if 'pipe' in dir():\n",
    "    del pipe\n",
    "if 'controlnet_pipe' in dir():\n",
    "    del controlnet_pipe\n",
    "if 'controlnet' in dir():\n",
    "    del controlnet\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next lab, we'll build a **Multimodal RAG System** that can search across both images and text using CLIP embeddings!\n",
    "\n",
    "‚û°Ô∏è Continue to [Lab 03: Multimodal RAG](./03-multimodal-rag.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
