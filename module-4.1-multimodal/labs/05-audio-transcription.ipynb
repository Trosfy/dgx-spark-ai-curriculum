{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.1.5: Audio Transcription with Whisper\n",
    "\n",
    "**Module:** 4.1 - Multimodal AI  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand how Whisper converts speech to text\n",
    "- [ ] Transcribe audio files with accurate timestamps\n",
    "- [ ] Detect languages automatically\n",
    "- [ ] Translate audio from any language to English\n",
    "- [ ] Build an audio Q&A pipeline with LLMs\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Module 3 (LLM Systems)\n",
    "- Knowledge of: Basic audio concepts, Python\n",
    "- Running in: NGC PyTorch container\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "Audio transcription is everywhere:\n",
    "\n",
    "- **Meetings**: Automatic meeting notes and summaries\n",
    "- **Podcasts**: Generate searchable transcripts\n",
    "- **Accessibility**: Subtitles for videos\n",
    "- **Customer Service**: Transcribe and analyze support calls\n",
    "- **Healthcare**: Dictation for medical records\n",
    "\n",
    "---\n",
    "\n",
    "## üßí ELI5: How Does Whisper Work?\n",
    "\n",
    "> **Imagine you're learning to understand a foreign language by watching thousands of movies with subtitles.** You start to recognize that certain sounds match certain words.\n",
    ">\n",
    "> Whisper learned the same way! It was trained on 680,000 hours of audio with transcripts from the internet. Now it can:\n",
    "> 1. **Listen** to any audio in almost any language\n",
    "> 2. **Recognize** the words being spoken\n",
    "> 3. **Write out** exactly what was said, with punctuation!\n",
    ">\n",
    "> **In AI terms:** Whisper is a transformer model that converts mel spectrograms (visual representations of audio) into text tokens, one at a time, similar to how GPT generates text.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup\n",
    "\n",
    "Let's set up Whisper for audio transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DGX Spark Environment Check\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {device.name}\")\n",
    "    print(f\"Memory: {device.total_memory / 1024**3:.1f} GB\")\n",
    "    print(\"\\nüí° Whisper large-v3 uses ~10GB - easily fits!\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected! Whisper will run slowly on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "# !pip install openai-whisper soundfile librosa scipy numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, List, Dict, Any\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for audio dependencies\n",
    "try:\n",
    "    import whisper\n",
    "    print(\"‚úÖ OpenAI Whisper installed\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå OpenAI Whisper not installed. Run: pip install openai-whisper\")\n",
    "\n",
    "try:\n",
    "    import librosa\n",
    "    print(\"‚úÖ librosa installed\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå librosa not installed. Run: pip install librosa\")\n",
    "\n",
    "try:\n",
    "    import soundfile as sf\n",
    "    print(\"‚úÖ soundfile installed\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå soundfile not installed. Run: pip install soundfile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding Whisper Models\n",
    "\n",
    "Whisper comes in different sizes - larger models are more accurate but slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whisper model comparison\n",
    "print(\"üìä Whisper Model Comparison\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Model':<15} {'Parameters':<12} {'VRAM':<10} {'Speed':<12} {'Best For'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "models = [\n",
    "    (\"tiny\", \"39M\", \"~1GB\", \"~32x\", \"Testing, quick previews\"),\n",
    "    (\"base\", \"74M\", \"~1GB\", \"~16x\", \"Simple recordings\"),\n",
    "    (\"small\", \"244M\", \"~2GB\", \"~6x\", \"Good accuracy/speed balance\"),\n",
    "    (\"medium\", \"769M\", \"~5GB\", \"~2x\", \"Better accuracy\"),\n",
    "    (\"large-v3\", \"1.55B\", \"~10GB\", \"~1x\", \"Best accuracy (recommended)\"),\n",
    "]\n",
    "\n",
    "for name, params, vram, speed, use_case in models:\n",
    "    fits = \"‚úÖ\" if \"GB\" in vram and float(vram.replace(\"~\", \"\").replace(\"GB\", \"\")) < 20 else \"‚úÖ\"\n",
    "    print(f\"{fits} {name:<13} {params:<12} {vram:<10} {speed:<12} {use_case}\")\n",
    "\n",
    "print(\"\\nüí° With 128GB on DGX Spark, use large-v3 for best quality!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "# Load Whisper model\n",
    "# Use large-v3 for best quality, or smaller models for speed\n",
    "MODEL_SIZE = \"base\"  # Start with base for quick testing, change to \"large-v3\" for production\n",
    "\n",
    "print(f\"Loading Whisper {MODEL_SIZE}...\")\n",
    "start_time = time.time()\n",
    "\n",
    "model = whisper.load_model(MODEL_SIZE)\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded in {time.time() - start_time:.1f}s\")\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Creating Sample Audio\n",
    "\n",
    "Let's create a sample audio file for testing. We'll generate a simple sine wave with \"speech-like\" patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "from scipy.io import wavfile\n",
    "\n",
    "def create_sample_audio(output_path: str = \"sample_audio.wav\", duration: float = 5.0):\n",
    "    \"\"\"\n",
    "    Create a simple sample audio file.\n",
    "    \n",
    "    Note: For real testing, use actual speech recordings!\n",
    "    \"\"\"\n",
    "    # Sample rate for speech\n",
    "    sample_rate = 16000\n",
    "    \n",
    "    # Generate duration's worth of samples\n",
    "    t = np.linspace(0, duration, int(sample_rate * duration), dtype=np.float32)\n",
    "    \n",
    "    # Create a more complex waveform (not real speech, just for testing)\n",
    "    # Real speech would come from recordings\n",
    "    audio = 0.3 * np.sin(2 * np.pi * 440 * t)  # A4 note\n",
    "    audio += 0.2 * np.sin(2 * np.pi * 880 * t)  # A5 note\n",
    "    audio += 0.1 * np.sin(2 * np.pi * 220 * t)  # A3 note\n",
    "    \n",
    "    # Add some variation\n",
    "    envelope = np.exp(-t / 2) * (1 + 0.5 * np.sin(2 * np.pi * 2 * t))\n",
    "    audio = audio * envelope\n",
    "    \n",
    "    # Normalize\n",
    "    audio = audio / np.max(np.abs(audio)) * 0.8\n",
    "    \n",
    "    # Save as WAV\n",
    "    sf.write(output_path, audio, sample_rate)\n",
    "    \n",
    "    return output_path, sample_rate, len(audio) / sample_rate\n",
    "\n",
    "# Create sample (note: this won't produce actual speech!)\n",
    "sample_path, sr, duration = create_sample_audio()\n",
    "print(f\"‚úÖ Created sample audio: {sample_path}\")\n",
    "print(f\"   Sample rate: {sr} Hz\")\n",
    "print(f\"   Duration: {duration:.1f}s\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Note: This is a synthetic tone, not real speech.\")\n",
    "print(\"   For proper testing, use an actual audio recording!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the audio\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "def visualize_audio(audio_path: str):\n",
    "    \"\"\"\n",
    "    Visualize an audio file's waveform and spectrogram.\n",
    "    \"\"\"\n",
    "    # Load audio\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 6))\n",
    "    \n",
    "    # Waveform\n",
    "    axes[0].set_title(\"Waveform\")\n",
    "    librosa.display.waveshow(audio, sr=sr, ax=axes[0])\n",
    "    axes[0].set_xlabel(\"Time (s)\")\n",
    "    \n",
    "    # Mel spectrogram (what Whisper \"sees\")\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=80)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    \n",
    "    axes[1].set_title(\"Mel Spectrogram (what Whisper sees)\")\n",
    "    img = librosa.display.specshow(mel_spec_db, x_axis='time', y_axis='mel', sr=sr, ax=axes[1])\n",
    "    fig.colorbar(img, ax=axes[1], format='%+2.0f dB')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return audio, sr\n",
    "\n",
    "audio, sr = visualize_audio(sample_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Understanding the Mel Spectrogram\n",
    "\n",
    "The mel spectrogram is a visual representation of audio:\n",
    "- **X-axis**: Time\n",
    "- **Y-axis**: Frequency (mel scale, which matches human perception)\n",
    "- **Color**: Intensity (louder = brighter)\n",
    "\n",
    "Whisper converts this \"image\" of sound into text, similar to how a VLM converts images to descriptions!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Basic Transcription\n",
    "\n",
    "Let's transcribe our audio file. For real-world testing, use an actual speech recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TranscriptionSegment:\n",
    "    \"\"\"A segment of transcribed audio.\"\"\"\n",
    "    id: int\n",
    "    start: float  # Start time in seconds\n",
    "    end: float    # End time in seconds\n",
    "    text: str\n",
    "    \n",
    "    @property\n",
    "    def duration(self) -> float:\n",
    "        return self.end - self.start\n",
    "    \n",
    "    def format_timestamp(self, t: float) -> str:\n",
    "        \"\"\"Format time as HH:MM:SS.mmm\"\"\"\n",
    "        hours = int(t // 3600)\n",
    "        minutes = int((t % 3600) // 60)\n",
    "        seconds = t % 60\n",
    "        return f\"{hours:02d}:{minutes:02d}:{seconds:06.3f}\"\n",
    "    \n",
    "    def to_srt(self) -> str:\n",
    "        \"\"\"Convert to SRT subtitle format.\"\"\"\n",
    "        start_ts = self.format_timestamp(self.start).replace(\".\", \",\")\n",
    "        end_ts = self.format_timestamp(self.end).replace(\".\", \",\")\n",
    "        return f\"{self.id}\\n{start_ts} --> {end_ts}\\n{self.text.strip()}\\n\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TranscriptionResult:\n",
    "    \"\"\"Complete transcription result.\"\"\"\n",
    "    text: str\n",
    "    segments: List[TranscriptionSegment]\n",
    "    language: str\n",
    "    duration: float\n",
    "    processing_time: float\n",
    "    \n",
    "    def to_srt(self) -> str:\n",
    "        \"\"\"Export as SRT subtitle format.\"\"\"\n",
    "        return \"\\n\".join(seg.to_srt() for seg in self.segments)\n",
    "    \n",
    "    def to_vtt(self) -> str:\n",
    "        \"\"\"Export as WebVTT format.\"\"\"\n",
    "        lines = [\"WEBVTT\\n\"]\n",
    "        for seg in self.segments:\n",
    "            start_ts = seg.format_timestamp(seg.start)\n",
    "            end_ts = seg.format_timestamp(seg.end)\n",
    "            lines.append(f\"{start_ts} --> {end_ts}\")\n",
    "            lines.append(seg.text.strip())\n",
    "            lines.append(\"\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "print(\"‚úÖ Data classes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe(\n",
    "    audio_path: str,\n",
    "    language: Optional[str] = None,\n",
    "    task: str = \"transcribe\",  # or \"translate\" (to English)\n",
    "    verbose: bool = True,\n",
    ") -> TranscriptionResult:\n",
    "    \"\"\"\n",
    "    Transcribe an audio file using Whisper.\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Path to audio file (mp3, wav, m4a, etc.)\n",
    "        language: Source language (auto-detected if None)\n",
    "        task: \"transcribe\" or \"translate\" (to English)\n",
    "        verbose: Print progress\n",
    "        \n",
    "    Returns:\n",
    "        TranscriptionResult with text and segments\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"üé§ Transcribing: {audio_path}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load audio\n",
    "    audio = whisper.load_audio(audio_path)\n",
    "    audio_duration = len(audio) / whisper.audio.SAMPLE_RATE\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   Duration: {audio_duration:.1f}s\")\n",
    "    \n",
    "    # Transcribe\n",
    "    result = model.transcribe(\n",
    "        audio,\n",
    "        language=language,\n",
    "        task=task,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    # Convert segments\n",
    "    segments = [\n",
    "        TranscriptionSegment(\n",
    "            id=i + 1,\n",
    "            start=seg[\"start\"],\n",
    "            end=seg[\"end\"],\n",
    "            text=seg[\"text\"],\n",
    "        )\n",
    "        for i, seg in enumerate(result[\"segments\"])\n",
    "    ]\n",
    "    \n",
    "    transcription = TranscriptionResult(\n",
    "        text=result[\"text\"],\n",
    "        segments=segments,\n",
    "        language=result.get(\"language\", \"unknown\"),\n",
    "        duration=audio_duration,\n",
    "        processing_time=processing_time,\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        ratio = audio_duration / processing_time\n",
    "        print(f\"\\n‚úÖ Completed in {processing_time:.1f}s ({ratio:.1f}x realtime)\")\n",
    "        print(f\"   Detected language: {transcription.language}\")\n",
    "    \n",
    "    return transcription\n",
    "\n",
    "print(\"‚úÖ Transcription function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test transcription with our sample\n",
    "# Note: Our synthetic audio won't produce meaningful text!\n",
    "\n",
    "print(\"\\nüìù Transcription Test\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = transcribe(sample_path)\n",
    "\n",
    "print(f\"\\nüìÑ Transcription:\")\n",
    "print(f\"   '{result.text}'\")\n",
    "\n",
    "print(f\"\\nüìä Statistics:\")\n",
    "print(f\"   Segments: {len(result.segments)}\")\n",
    "print(f\"   Audio duration: {result.duration:.1f}s\")\n",
    "print(f\"   Processing time: {result.processing_time:.1f}s\")\n",
    "\n",
    "if result.segments:\n",
    "    print(f\"\\nüîñ Segments:\")\n",
    "    for seg in result.segments[:5]:  # Show first 5\n",
    "        print(f\"   [{seg.format_timestamp(seg.start)} --> {seg.format_timestamp(seg.end)}] {seg.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Language Detection and Translation\n",
    "\n",
    "Whisper can automatically detect languages and translate to English!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(audio_path: str, top_k: int = 5) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Detect the language of an audio file.\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Path to audio file\n",
    "        top_k: Number of top languages to return\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping language codes to probabilities\n",
    "    \"\"\"\n",
    "    # Load and pad/trim audio\n",
    "    audio = whisper.load_audio(audio_path)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "    \n",
    "    # Create mel spectrogram\n",
    "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "    \n",
    "    # Detect language\n",
    "    _, probs = model.detect_language(mel)\n",
    "    \n",
    "    # Sort and get top-k\n",
    "    sorted_probs = sorted(probs.items(), key=lambda x: x[1], reverse=True)\n",
    "    return dict(sorted_probs[:top_k])\n",
    "\n",
    "print(\"Testing language detection...\")\n",
    "languages = detect_language(sample_path)\n",
    "\n",
    "print(\"\\nüåç Detected Languages:\")\n",
    "for lang, prob in languages.items():\n",
    "    bar = \"‚ñà\" * int(prob * 40)\n",
    "    print(f\"   {lang}: {prob:.1%} {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whisper language support\n",
    "print(\"\\nüåê Whisper Language Support\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get available languages\n",
    "from whisper.tokenizer import LANGUAGES\n",
    "\n",
    "print(f\"Supports {len(LANGUAGES)} languages including:\")\n",
    "\n",
    "# Show some common languages\n",
    "common = [\"en\", \"es\", \"fr\", \"de\", \"it\", \"pt\", \"ru\", \"ja\", \"ko\", \"zh\", \"ar\", \"hi\"]\n",
    "for code in common:\n",
    "    if code in LANGUAGES:\n",
    "        print(f\"   {code}: {LANGUAGES[code]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Exporting Transcriptions\n",
    "\n",
    "Let's export transcriptions in different formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_transcription(\n",
    "    result: TranscriptionResult,\n",
    "    output_path: str,\n",
    "    format: str = \"txt\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Export transcription to file.\n",
    "    \n",
    "    Args:\n",
    "        result: TranscriptionResult\n",
    "        output_path: Output file path\n",
    "        format: \"txt\", \"srt\", \"vtt\", or \"json\"\n",
    "        \n",
    "    Returns:\n",
    "        Path to saved file\n",
    "    \"\"\"\n",
    "    output_path = Path(output_path)\n",
    "    \n",
    "    if format == \"txt\":\n",
    "        content = result.text\n",
    "    elif format == \"srt\":\n",
    "        content = result.to_srt()\n",
    "    elif format == \"vtt\":\n",
    "        content = result.to_vtt()\n",
    "    elif format == \"json\":\n",
    "        content = json.dumps({\n",
    "            \"text\": result.text,\n",
    "            \"language\": result.language,\n",
    "            \"duration\": result.duration,\n",
    "            \"segments\": [\n",
    "                {\n",
    "                    \"id\": s.id,\n",
    "                    \"start\": s.start,\n",
    "                    \"end\": s.end,\n",
    "                    \"text\": s.text,\n",
    "                }\n",
    "                for s in result.segments\n",
    "            ],\n",
    "        }, indent=2)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown format: {format}\")\n",
    "    \n",
    "    output_path.write_text(content)\n",
    "    print(f\"‚úÖ Saved {format.upper()}: {output_path}\")\n",
    "    \n",
    "    return str(output_path)\n",
    "\n",
    "# Export in different formats\n",
    "if result.text.strip():\n",
    "    export_transcription(result, \"transcript.txt\", \"txt\")\n",
    "    export_transcription(result, \"transcript.srt\", \"srt\")\n",
    "    export_transcription(result, \"transcript.json\", \"json\")\n",
    "else:\n",
    "    print(\"(Skipping export - no transcription text)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Audio Q&A Pipeline\n",
    "\n",
    "Let's combine Whisper with an LLM for audio question-answering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_audio_qa_prompt(\n",
    "    transcription: TranscriptionResult,\n",
    "    question: str,\n",
    "    include_timestamps: bool = True,\n",
    "    max_context_length: int = 3000,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a prompt for audio Q&A based on transcription.\n",
    "    \n",
    "    Args:\n",
    "        transcription: Transcription result\n",
    "        question: User's question\n",
    "        include_timestamps: Include timestamps in context\n",
    "        max_context_length: Maximum context length\n",
    "        \n",
    "    Returns:\n",
    "        Formatted prompt for an LLM\n",
    "    \"\"\"\n",
    "    if include_timestamps and transcription.segments:\n",
    "        # Build timestamped transcript\n",
    "        lines = []\n",
    "        for seg in transcription.segments:\n",
    "            timestamp = f\"[{seg.format_timestamp(seg.start)}]\"\n",
    "            lines.append(f\"{timestamp} {seg.text.strip()}\")\n",
    "        transcript = \"\\n\".join(lines)\n",
    "    else:\n",
    "        transcript = transcription.text\n",
    "    \n",
    "    # Truncate if needed\n",
    "    if len(transcript) > max_context_length:\n",
    "        transcript = transcript[:max_context_length] + \"...\\n[Truncated]\"\n",
    "    \n",
    "    prompt = f\"\"\"Audio Transcription (Duration: {transcription.duration:.1f}s, Language: {transcription.language}):\n",
    "\n",
    "{transcript}\n",
    "\n",
    "Based on the audio transcription above, please answer the following question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Example prompt creation\n",
    "if result.text.strip():\n",
    "    example_prompt = create_audio_qa_prompt(result, \"What is the main topic discussed?\")\n",
    "    print(\"\\nüìù Example Q&A Prompt:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(example_prompt)\n",
    "else:\n",
    "    print(\"(No transcription available for Q&A example)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioQAPipeline:\n",
    "    \"\"\"\n",
    "    Complete audio Q&A pipeline using Whisper and an LLM.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, whisper_model=None, llm_model=None, llm_processor=None):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline.\n",
    "        \n",
    "        Args:\n",
    "            whisper_model: Loaded Whisper model (uses global if None)\n",
    "            llm_model: LLM for Q&A (optional)\n",
    "            llm_processor: LLM processor\n",
    "        \"\"\"\n",
    "        self.whisper_model = whisper_model or model\n",
    "        self.llm_model = llm_model\n",
    "        self.llm_processor = llm_processor\n",
    "        \n",
    "        # Cache for transcriptions\n",
    "        self._transcription_cache = {}\n",
    "    \n",
    "    def transcribe(self, audio_path: str, **kwargs) -> TranscriptionResult:\n",
    "        \"\"\"\n",
    "        Transcribe audio with caching.\n",
    "        \"\"\"\n",
    "        if audio_path in self._transcription_cache:\n",
    "            print(\"üìé Using cached transcription\")\n",
    "            return self._transcription_cache[audio_path]\n",
    "        \n",
    "        result = transcribe(audio_path, **kwargs)\n",
    "        self._transcription_cache[audio_path] = result\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def ask(\n",
    "        self,\n",
    "        audio_path: str,\n",
    "        question: str,\n",
    "        include_timestamps: bool = False,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Ask a question about an audio file.\n",
    "        \n",
    "        Args:\n",
    "            audio_path: Path to audio file\n",
    "            question: Question about the audio\n",
    "            include_timestamps: Include timestamps in context\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with answer and metadata\n",
    "        \"\"\"\n",
    "        # Transcribe\n",
    "        transcription = self.transcribe(audio_path)\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = create_audio_qa_prompt(\n",
    "            transcription,\n",
    "            question,\n",
    "            include_timestamps=include_timestamps,\n",
    "        )\n",
    "        \n",
    "        # If no LLM, return the prompt for manual use\n",
    "        if self.llm_model is None:\n",
    "            return {\n",
    "                \"transcription\": transcription.text,\n",
    "                \"question\": question,\n",
    "                \"prompt\": prompt,\n",
    "                \"answer\": \"(LLM not configured - use prompt with your preferred model)\",\n",
    "                \"language\": transcription.language,\n",
    "                \"duration\": transcription.duration,\n",
    "            }\n",
    "        \n",
    "        # Generate answer with LLM\n",
    "        # (Implementation depends on LLM type)\n",
    "        answer = self._generate_answer(prompt)\n",
    "        \n",
    "        return {\n",
    "            \"transcription\": transcription.text,\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"language\": transcription.language,\n",
    "            \"duration\": transcription.duration,\n",
    "        }\n",
    "    \n",
    "    def _generate_answer(self, prompt: str) -> str:\n",
    "        \"\"\"Generate answer using LLM.\"\"\"\n",
    "        # Placeholder - implement based on your LLM\n",
    "        return \"(LLM response would go here)\"\n",
    "    \n",
    "    def summarize(self, audio_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate a summary of the audio content.\n",
    "        \"\"\"\n",
    "        return self.ask(\n",
    "            audio_path,\n",
    "            \"Provide a brief summary of the main points discussed in this audio.\"\n",
    "        )\n",
    "\n",
    "print(\"‚úÖ AudioQAPipeline class ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the pipeline\n",
    "pipeline = AudioQAPipeline()\n",
    "\n",
    "print(\"\\nüéØ Audio Q&A Pipeline Test\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = pipeline.ask(\n",
    "    sample_path,\n",
    "    \"What is being discussed in this audio?\"\n",
    ")\n",
    "\n",
    "print(f\"\\n‚ùì Question: {result['question']}\")\n",
    "print(f\"üìù Transcription: {result['transcription'][:200]}...\" if len(result['transcription']) > 200 else f\"üìù Transcription: {result['transcription']}\")\n",
    "print(f\"üåç Language: {result['language']}\")\n",
    "print(f\"‚è±Ô∏è Duration: {result['duration']:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Wrong Audio Format\n",
    "```python\n",
    "# ‚ùå Wrong: Whisper expects specific sample rate\n",
    "audio, sr = librosa.load(\"audio.wav\", sr=44100)  # CD quality\n",
    "result = model.transcribe(audio)  # May have issues!\n",
    "\n",
    "# ‚úÖ Right: Use Whisper's audio loader (16kHz)\n",
    "audio = whisper.load_audio(\"audio.wav\")  # Auto-resamples to 16kHz\n",
    "result = model.transcribe(audio)\n",
    "```\n",
    "**Why:** Whisper was trained on 16kHz audio. Always use `whisper.load_audio()`.\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 2: Using Tiny Model for Production\n",
    "```python\n",
    "# ‚ùå Wrong: Tiny model has high error rate\n",
    "model = whisper.load_model(\"tiny\")\n",
    "result = model.transcribe(important_meeting)  # Many errors!\n",
    "\n",
    "# ‚úÖ Right: Use large-v3 for important transcriptions\n",
    "model = whisper.load_model(\"large-v3\")\n",
    "result = model.transcribe(important_meeting)  # Much more accurate\n",
    "```\n",
    "**Why:** Larger models are significantly more accurate. With 128GB on DGX Spark, always use large-v3!\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 3: Not Setting Language for Known Audio\n",
    "```python\n",
    "# ‚ùå Wrong: Let Whisper guess (may be wrong)\n",
    "result = model.transcribe(french_audio)  # Might detect wrong language\n",
    "\n",
    "# ‚úÖ Right: Specify language when known\n",
    "result = model.transcribe(french_audio, language=\"fr\")\n",
    "```\n",
    "**Why:** Specifying language improves accuracy and speed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How Whisper converts audio to text using mel spectrograms\n",
    "- ‚úÖ Choosing the right Whisper model size for your needs\n",
    "- ‚úÖ Transcribing audio with accurate timestamps\n",
    "- ‚úÖ Detecting languages automatically\n",
    "- ‚úÖ Exporting transcriptions in SRT/VTT/JSON formats\n",
    "- ‚úÖ Building an audio Q&A pipeline\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "Build a **Meeting Notes Generator** that:\n",
    "1. Transcribes a meeting recording\n",
    "2. Identifies speakers (speaker diarization)\n",
    "3. Extracts action items and decisions\n",
    "4. Generates a summary with key points\n",
    "5. Exports as formatted meeting notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Your code here!\n",
    "\n",
    "def generate_meeting_notes(audio_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate structured meeting notes from audio.\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Path to meeting recording\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "        - transcript: Full transcript\n",
    "        - summary: Brief summary\n",
    "        - action_items: List of action items\n",
    "        - decisions: List of decisions made\n",
    "        - participants: Identified speakers\n",
    "    \"\"\"\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [Whisper Paper](https://arxiv.org/abs/2212.04356)\n",
    "- [OpenAI Whisper GitHub](https://github.com/openai/whisper)\n",
    "- [Whisper.cpp](https://github.com/ggerganov/whisper.cpp) - Efficient C++ implementation\n",
    "- [Faster Whisper](https://github.com/SYSTRAN/faster-whisper) - CTranslate2 acceleration\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "import os\n",
    "\n",
    "# Remove sample files\n",
    "for f in [\"sample_audio.wav\", \"transcript.txt\", \"transcript.srt\", \"transcript.json\"]:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "\n",
    "# Free memory\n",
    "if 'model' in dir():\n",
    "    del model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Module Complete! üéâ\n",
    "\n",
    "Congratulations! You've completed Module 4.1: Multimodal AI!\n",
    "\n",
    "You've learned:\n",
    "1. **Vision-Language Models** - Analyzing and understanding images with LLaVA and CLIP\n",
    "2. **Image Generation** - Creating images with SDXL and ControlNet\n",
    "3. **Multimodal RAG** - Searching across images and text\n",
    "4. **Document AI** - Processing PDFs with OCR and VLMs\n",
    "5. **Audio Transcription** - Converting speech to text with Whisper\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to **Module 4.2: AI Safety & Alignment** to learn about building safe and aligned AI systems!\n",
    "\n",
    "‚û°Ô∏è [Module 4.2: AI Safety & Alignment](../../module-4.2-ai-safety/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
