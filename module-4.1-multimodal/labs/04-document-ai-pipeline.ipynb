{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.1.4: Document AI Pipeline\n",
    "\n",
    "**Module:** 4.1 - Multimodal AI  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Extract text from PDFs using PyMuPDF\n",
    "- [ ] Apply OCR to scanned documents using Tesseract\n",
    "- [ ] Detect and extract tables from documents\n",
    "- [ ] Use VLMs to understand complex document layouts\n",
    "- [ ] Build a complete document Q&A pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Lab 4.1.1 (Vision-Language Models)\n",
    "- Knowledge of: PDF structure, basic NLP concepts\n",
    "- Running in: NGC PyTorch container\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "Document AI is transforming how organizations handle paperwork:\n",
    "\n",
    "- **Legal**: Extract key clauses from contracts automatically\n",
    "- **Finance**: Process invoices and receipts at scale\n",
    "- **Healthcare**: Digitize patient records with high accuracy\n",
    "- **Insurance**: Extract information from claim forms\n",
    "- **Research**: Parse scientific papers for key findings\n",
    "\n",
    "---\n",
    "\n",
    "## üßí ELI5: What is Document AI?\n",
    "\n",
    "> **Imagine you're a librarian who needs to organize thousands of old books and papers.** Some are typed, some are handwritten, some have pictures and tables.\n",
    ">\n",
    "> Document AI is like having a super-powered assistant who can:\n",
    "> 1. **Read** any document, even messy handwriting\n",
    "> 2. **Understand** the layout - which part is the title, which is a table\n",
    "> 3. **Extract** specific information you need\n",
    "> 4. **Answer questions** about what's in the documents\n",
    ">\n",
    "> **In AI terms:** Document AI combines OCR (converting images to text), layout analysis (understanding structure), and NLP/VLMs (understanding meaning) to process any document intelligently.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup\n",
    "\n",
    "Let's set up the tools we need for document processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DGX Spark Environment Check\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {device.name}\")\n",
    "    print(f\"Memory: {device.total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "# !pip install pymupdf>=1.23.0 pytesseract pdf2image pillow>=10.0.0\n",
    "# !apt-get update && apt-get install -y tesseract-ocr poppler-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import gc\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, List, Dict, Any, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from io import BytesIO\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Document processing\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for OCR availability\n",
    "import shutil\n",
    "\n",
    "print(\"\\nüîç Checking Document AI dependencies:\")\n",
    "\n",
    "# Check Tesseract\n",
    "tesseract_path = shutil.which(\"tesseract\")\n",
    "if tesseract_path:\n",
    "    print(f\"  ‚úÖ Tesseract OCR: {tesseract_path}\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è  Tesseract OCR: Not found (install with: apt-get install tesseract-ocr)\")\n",
    "\n",
    "# Check pdftoppm (for pdf2image)\n",
    "pdftoppm_path = shutil.which(\"pdftoppm\")\n",
    "if pdftoppm_path:\n",
    "    print(f\"  ‚úÖ pdftoppm: {pdftoppm_path}\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è  pdftoppm: Not found (install with: apt-get install poppler-utils)\")\n",
    "\n",
    "# Check PyMuPDF\n",
    "print(f\"  ‚úÖ PyMuPDF: v{fitz.version[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Creating a Sample PDF\n",
    "\n",
    "Let's create a sample PDF document to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_pdf(output_path: str = \"sample_document.pdf\") -> str:\n",
    "    \"\"\"\n",
    "    Create a sample PDF with various content types for testing.\n",
    "    \n",
    "    Returns:\n",
    "        Path to created PDF\n",
    "    \"\"\"\n",
    "    doc = fitz.open()\n",
    "    \n",
    "    # Page 1: Title and introduction\n",
    "    page = doc.new_page(width=612, height=792)  # Letter size\n",
    "    \n",
    "    # Title\n",
    "    title_rect = fitz.Rect(72, 72, 540, 120)\n",
    "    page.insert_textbox(\n",
    "        title_rect,\n",
    "        \"Annual Performance Report 2024\",\n",
    "        fontsize=24,\n",
    "        fontname=\"helv\",\n",
    "        align=fitz.TEXT_ALIGN_CENTER\n",
    "    )\n",
    "    \n",
    "    # Subtitle\n",
    "    subtitle_rect = fitz.Rect(72, 130, 540, 160)\n",
    "    page.insert_textbox(\n",
    "        subtitle_rect,\n",
    "        \"DGX Spark AI Division\",\n",
    "        fontsize=14,\n",
    "        fontname=\"helv\",\n",
    "        align=fitz.TEXT_ALIGN_CENTER\n",
    "    )\n",
    "    \n",
    "    # Introduction\n",
    "    intro_rect = fitz.Rect(72, 200, 540, 400)\n",
    "    intro_text = \"\"\"Executive Summary\n",
    "\n",
    "This report presents the annual performance metrics for the DGX Spark AI Division. \n",
    "Key highlights include:\n",
    "\n",
    "‚Ä¢ Revenue growth of 45% year-over-year\n",
    "‚Ä¢ Successful launch of 3 new AI products\n",
    "‚Ä¢ Customer satisfaction score of 4.8/5.0\n",
    "‚Ä¢ Team expansion from 50 to 85 employees\n",
    "\n",
    "The following sections provide detailed analysis of each department's performance \n",
    "and projections for the upcoming fiscal year.\"\"\"\n",
    "    \n",
    "    page.insert_textbox(\n",
    "        intro_rect,\n",
    "        intro_text,\n",
    "        fontsize=11,\n",
    "        fontname=\"helv\"\n",
    "    )\n",
    "    \n",
    "    # Page 2: Financial data with table\n",
    "    page2 = doc.new_page(width=612, height=792)\n",
    "    \n",
    "    # Section header\n",
    "    header_rect = fitz.Rect(72, 72, 540, 100)\n",
    "    page2.insert_textbox(\n",
    "        header_rect,\n",
    "        \"1. Financial Performance\",\n",
    "        fontsize=16,\n",
    "        fontname=\"helv\"\n",
    "    )\n",
    "    \n",
    "    # Table data\n",
    "    table_text = \"\"\"Quarterly Revenue (in millions USD)\n",
    "\n",
    "Quarter    | 2023      | 2024      | Growth\n",
    "-----------|-----------|-----------|--------\n",
    "Q1         | $12.5     | $18.2     | +45.6%\n",
    "Q2         | $14.3     | $21.5     | +50.3%\n",
    "Q3         | $15.8     | $23.1     | +46.2%\n",
    "Q4         | $18.2     | $26.8     | +47.3%\n",
    "-----------|-----------|-----------|--------\n",
    "Total      | $60.8     | $89.6     | +47.4%\n",
    "\n",
    "Key Financial Metrics:\n",
    "\n",
    "‚Ä¢ Operating Margin: 28.5% (up from 23.2%)\n",
    "‚Ä¢ EBITDA: $25.5M (up 62% YoY)\n",
    "‚Ä¢ Cash Position: $45.2M\n",
    "‚Ä¢ R&D Investment: $15.8M (17.6% of revenue)\"\"\"\n",
    "    \n",
    "    table_rect = fitz.Rect(72, 120, 540, 450)\n",
    "    page2.insert_textbox(\n",
    "        table_rect,\n",
    "        table_text,\n",
    "        fontsize=10,\n",
    "        fontname=\"cour\"  # Monospace for table\n",
    "    )\n",
    "    \n",
    "    # Page 3: Product highlights\n",
    "    page3 = doc.new_page(width=612, height=792)\n",
    "    \n",
    "    header_rect = fitz.Rect(72, 72, 540, 100)\n",
    "    page3.insert_textbox(\n",
    "        header_rect,\n",
    "        \"2. Product Highlights\",\n",
    "        fontsize=16,\n",
    "        fontname=\"helv\"\n",
    "    )\n",
    "    \n",
    "    products_text = \"\"\"New Product Launches:\n",
    "\n",
    "1. SPARK Vision Pro\n",
    "   Released: March 2024\n",
    "   Description: Advanced computer vision system for manufacturing quality control.\n",
    "   Revenue Impact: $8.2M in first 9 months\n",
    "   Customer Adoption: 45 enterprise clients\n",
    "\n",
    "2. SPARK NLP Suite\n",
    "   Released: June 2024  \n",
    "   Description: Natural language processing toolkit for document automation.\n",
    "   Revenue Impact: $5.4M in first 6 months\n",
    "   Customer Adoption: 78 enterprise clients\n",
    "\n",
    "3. SPARK Edge Deployment\n",
    "   Released: September 2024\n",
    "   Description: Edge computing framework for real-time AI inference.\n",
    "   Revenue Impact: $3.1M in first 3 months\n",
    "   Customer Adoption: 32 enterprise clients\n",
    "\n",
    "Product Roadmap 2025:\n",
    "‚Ä¢ Q1: SPARK Multimodal (vision + language integration)\n",
    "‚Ä¢ Q2: SPARK AutoML Platform\n",
    "‚Ä¢ Q3: SPARK Real-time Analytics\n",
    "‚Ä¢ Q4: SPARK Enterprise Security Suite\"\"\"\n",
    "    \n",
    "    products_rect = fitz.Rect(72, 120, 540, 650)\n",
    "    page3.insert_textbox(\n",
    "        products_rect,\n",
    "        products_text,\n",
    "        fontsize=10,\n",
    "        fontname=\"helv\"\n",
    "    )\n",
    "    \n",
    "    # Save the PDF\n",
    "    doc.save(output_path)\n",
    "    doc.close()\n",
    "    \n",
    "    print(f\"‚úÖ Created sample PDF: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "# Create the sample PDF\n",
    "sample_pdf_path = create_sample_pdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Basic PDF Text Extraction\n",
    "\n",
    "Let's extract text from our PDF using PyMuPDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> Dict[int, str]:\n",
    "    \"\"\"\n",
    "    Extract text from each page of a PDF.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping page numbers to text content\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    \n",
    "    pages = {}\n",
    "    for page_num, page in enumerate(doc, 1):\n",
    "        text = page.get_text()\n",
    "        pages[page_num] = text\n",
    "    \n",
    "    doc.close()\n",
    "    return pages\n",
    "\n",
    "# Extract text\n",
    "print(\"üìÑ Extracting text from PDF...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pages = extract_text_from_pdf(sample_pdf_path)\n",
    "\n",
    "for page_num, text in pages.items():\n",
    "    print(f\"\\nüìÉ Page {page_num}:\")\n",
    "    print(\"-\" * 40)\n",
    "    # Show first 500 characters\n",
    "    preview = text[:500] + \"...\" if len(text) > 500 else text\n",
    "    print(preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get structured text blocks with positions\n",
    "def extract_structured_blocks(pdf_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Extract text blocks with position information.\n",
    "    \n",
    "    Returns:\n",
    "        List of blocks with text, position, and metadata\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    \n",
    "    all_blocks = []\n",
    "    \n",
    "    for page_num, page in enumerate(doc, 1):\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        \n",
    "        for block in blocks:\n",
    "            if \"lines\" in block:  # Text block\n",
    "                # Combine text from all lines\n",
    "                text = \" \".join(\n",
    "                    \" \".join(span[\"text\"] for span in line[\"spans\"])\n",
    "                    for line in block[\"lines\"]\n",
    "                ).strip()\n",
    "                \n",
    "                if text:\n",
    "                    all_blocks.append({\n",
    "                        \"page\": page_num,\n",
    "                        \"bbox\": block[\"bbox\"],  # (x0, y0, x1, y1)\n",
    "                        \"text\": text,\n",
    "                        \"type\": \"text\",\n",
    "                    })\n",
    "            \n",
    "            elif \"image\" in block:  # Image block\n",
    "                all_blocks.append({\n",
    "                    \"page\": page_num,\n",
    "                    \"bbox\": block[\"bbox\"],\n",
    "                    \"type\": \"image\",\n",
    "                    \"size\": (block.get(\"width\"), block.get(\"height\")),\n",
    "                })\n",
    "    \n",
    "    doc.close()\n",
    "    return all_blocks\n",
    "\n",
    "# Get structured blocks\n",
    "blocks = extract_structured_blocks(sample_pdf_path)\n",
    "\n",
    "print(f\"\\nüìä Found {len(blocks)} content blocks:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, block in enumerate(blocks[:10]):  # Show first 10\n",
    "    block_type = block[\"type\"]\n",
    "    page = block[\"page\"]\n",
    "    \n",
    "    if block_type == \"text\":\n",
    "        preview = block[\"text\"][:60] + \"...\" if len(block[\"text\"]) > 60 else block[\"text\"]\n",
    "        print(f\"  {i+1}. Page {page}: [TEXT] {preview}\")\n",
    "    else:\n",
    "        print(f\"  {i+1}. Page {page}: [IMAGE] Size: {block['size']}\")\n",
    "\n",
    "if len(blocks) > 10:\n",
    "    print(f\"  ... and {len(blocks) - 10} more blocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Table Extraction\n",
    "\n",
    "Let's extract tables from our PDF document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TableData:\n",
    "    \"\"\"Extracted table data.\"\"\"\n",
    "    rows: List[List[str]]\n",
    "    headers: Optional[List[str]] = None\n",
    "    page_number: int = 0\n",
    "    \n",
    "    def to_markdown(self) -> str:\n",
    "        \"\"\"Convert table to markdown format.\"\"\"\n",
    "        if not self.rows:\n",
    "            return \"\"\n",
    "        \n",
    "        lines = []\n",
    "        \n",
    "        if self.headers:\n",
    "            lines.append(\"| \" + \" | \".join(self.headers) + \" |\")\n",
    "            lines.append(\"| \" + \" | \".join([\"---\"] * len(self.headers)) + \" |\")\n",
    "        \n",
    "        for row in self.rows:\n",
    "            lines.append(\"| \" + \" | \".join(str(cell) for cell in row) + \" |\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    def to_dict(self) -> List[Dict]:\n",
    "        \"\"\"Convert table to list of dictionaries.\"\"\"\n",
    "        if not self.headers or not self.rows:\n",
    "            return []\n",
    "        \n",
    "        return [\n",
    "            dict(zip(self.headers, row))\n",
    "            for row in self.rows\n",
    "        ]\n",
    "\n",
    "\n",
    "def extract_tables(pdf_path: str) -> List[TableData]:\n",
    "    \"\"\"\n",
    "    Extract tables from a PDF.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        \n",
    "    Returns:\n",
    "        List of TableData objects\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    tables = []\n",
    "    \n",
    "    for page_num, page in enumerate(doc, 1):\n",
    "        # Try to find tables\n",
    "        try:\n",
    "            page_tables = page.find_tables()\n",
    "            \n",
    "            for table in page_tables:\n",
    "                if table.row_count > 1:\n",
    "                    rows = []\n",
    "                    for row in table.extract():\n",
    "                        cleaned = [str(cell).strip() if cell else \"\" for cell in row]\n",
    "                        rows.append(cleaned)\n",
    "                    \n",
    "                    if rows:\n",
    "                        tables.append(TableData(\n",
    "                            rows=rows[1:] if len(rows) > 1 else [],\n",
    "                            headers=rows[0] if rows else None,\n",
    "                            page_number=page_num,\n",
    "                        ))\n",
    "        except Exception as e:\n",
    "            print(f\"  Note: Table extraction not available for page {page_num}\")\n",
    "    \n",
    "    doc.close()\n",
    "    return tables\n",
    "\n",
    "# Extract tables\n",
    "print(\"üìä Extracting tables from PDF...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tables = extract_tables(sample_pdf_path)\n",
    "print(f\"\\nFound {len(tables)} table(s)\")\n",
    "\n",
    "for i, table in enumerate(tables):\n",
    "    print(f\"\\nüìã Table {i+1} (Page {table.page_number}):\")\n",
    "    print(table.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: OCR for Scanned Documents\n",
    "\n",
    "For scanned PDFs (images instead of selectable text), we need OCR.\n",
    "\n",
    "### üßí ELI5: What is OCR?\n",
    "\n",
    "> **OCR is like teaching a computer to read.** When you take a photo of a page, the computer just sees colored dots (pixels). OCR looks at the shapes of those dots and figures out what letters they represent.\n",
    ">\n",
    "> It's like how you learned to read - first you learned that certain shapes mean certain letters, then you could read any text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_page_to_image(pdf_path: str, page_num: int = 0, dpi: int = 150) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Convert a PDF page to an image.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        page_num: Page number (0-indexed)\n",
    "        dpi: Resolution for rendering\n",
    "        \n",
    "    Returns:\n",
    "        PIL Image of the page\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc[page_num]\n",
    "    \n",
    "    # Render at specified DPI\n",
    "    mat = fitz.Matrix(dpi / 72, dpi / 72)\n",
    "    pix = page.get_pixmap(matrix=mat)\n",
    "    \n",
    "    # Convert to PIL Image\n",
    "    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    \n",
    "    doc.close()\n",
    "    return img\n",
    "\n",
    "# Convert first page to image\n",
    "page_image = pdf_page_to_image(sample_pdf_path, 0)\n",
    "\n",
    "plt.figure(figsize=(10, 12))\n",
    "plt.imshow(page_image)\n",
    "plt.axis('off')\n",
    "plt.title(\"PDF Page 1 as Image\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image size: {page_image.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCR using Tesseract (if available)\n",
    "try:\n",
    "    import pytesseract\n",
    "    \n",
    "    def ocr_image(image: Image.Image, lang: str = \"eng\") -> str:\n",
    "        \"\"\"\n",
    "        Perform OCR on an image.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image to OCR\n",
    "            lang: Language code for Tesseract\n",
    "            \n",
    "        Returns:\n",
    "            Extracted text\n",
    "        \"\"\"\n",
    "        return pytesseract.image_to_string(image, lang=lang)\n",
    "    \n",
    "    def ocr_with_boxes(image: Image.Image) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Perform OCR and get word bounding boxes.\n",
    "        \n",
    "        Returns:\n",
    "            List of words with their positions\n",
    "        \"\"\"\n",
    "        data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)\n",
    "        \n",
    "        words = []\n",
    "        for i in range(len(data[\"text\"])):\n",
    "            if int(data[\"conf\"][i]) > 30 and data[\"text\"][i].strip():\n",
    "                words.append({\n",
    "                    \"text\": data[\"text\"][i],\n",
    "                    \"x\": data[\"left\"][i],\n",
    "                    \"y\": data[\"top\"][i],\n",
    "                    \"width\": data[\"width\"][i],\n",
    "                    \"height\": data[\"height\"][i],\n",
    "                    \"confidence\": data[\"conf\"][i],\n",
    "                })\n",
    "        \n",
    "        return words\n",
    "    \n",
    "    # Test OCR on our page\n",
    "    print(\"üîç Performing OCR on page image...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    ocr_text = ocr_image(page_image)\n",
    "    print(\"\\nOCR Result (first 500 chars):\")\n",
    "    print(ocr_text[:500])\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è pytesseract not installed. Run: pip install pytesseract\")\n",
    "    print(\"   Also ensure tesseract-ocr is installed on the system.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Using VLMs for Document Understanding\n",
    "\n",
    "Vision-Language Models can understand complex document layouts better than traditional OCR!\n",
    "\n",
    "### üßí ELI5: VLMs for Documents\n",
    "\n",
    "> **Regular OCR just reads the letters.** A VLM is like a person who can look at the whole page and understand:\n",
    "> - \"This is a title because it's big and at the top\"\n",
    "> - \"These numbers are a table because they're in rows and columns\"\n",
    "> - \"This part is the footer because it's at the bottom of every page\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLaVA for document understanding\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "print(\"Loading LLaVA for document understanding...\")\n",
    "start_time = time.time()\n",
    "\n",
    "model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded in {time.time() - start_time:.1f}s\")\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_document_image(image: Image.Image, question: str, max_new_tokens: int = 512) -> str:\n",
    "    \"\"\"\n",
    "    Analyze a document image using VLM.\n",
    "    \n",
    "    Args:\n",
    "        image: Document page image\n",
    "        question: Question about the document\n",
    "        max_new_tokens: Maximum response length\n",
    "        \n",
    "    Returns:\n",
    "        VLM's response\n",
    "    \"\"\"\n",
    "    prompt = f\"USER: <image>\\n{question}\\nASSISTANT:\"\n",
    "    \n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "        )\n",
    "    \n",
    "    response = processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    if \"ASSISTANT:\" in response:\n",
    "        response = response.split(\"ASSISTANT:\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"‚úÖ Document analysis function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test VLM document understanding\n",
    "page2_image = pdf_page_to_image(sample_pdf_path, 1)  # Financial page\n",
    "\n",
    "# Display the page\n",
    "plt.figure(figsize=(10, 12))\n",
    "plt.imshow(page2_image)\n",
    "plt.axis('off')\n",
    "plt.title(\"Document Page for Analysis\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Ask questions about the document\n",
    "questions = [\n",
    "    \"What type of document is this?\",\n",
    "    \"Summarize the key financial metrics shown in this document.\",\n",
    "    \"What was the total revenue growth percentage?\",\n",
    "]\n",
    "\n",
    "print(\"\\nüìä VLM Document Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\n‚ùì {q}\")\n",
    "    response = analyze_document_image(page2_image, q)\n",
    "    print(f\"üí¨ {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Building a Complete Document Q&A Pipeline\n",
    "\n",
    "Now let's combine everything into a production-ready pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProcessedDocument:\n",
    "    \"\"\"A fully processed document.\"\"\"\n",
    "    source_path: str\n",
    "    num_pages: int\n",
    "    text_content: Dict[int, str]  # page -> text\n",
    "    tables: List[TableData]\n",
    "    metadata: Dict[str, Any]\n",
    "    \n",
    "    @property\n",
    "    def full_text(self) -> str:\n",
    "        \"\"\"Get all text concatenated.\"\"\"\n",
    "        return \"\\n\\n\".join(\n",
    "            f\"--- Page {page} ---\\n{text}\"\n",
    "            for page, text in sorted(self.text_content.items())\n",
    "        )\n",
    "\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"\n",
    "    Complete document processing pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vlm_model=None, vlm_processor=None):\n",
    "        \"\"\"Initialize with optional VLM for advanced understanding.\"\"\"\n",
    "        self.vlm_model = vlm_model\n",
    "        self.vlm_processor = vlm_processor\n",
    "    \n",
    "    def process(self, pdf_path: str, use_vlm: bool = True) -> ProcessedDocument:\n",
    "        \"\"\"\n",
    "        Process a PDF document.\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to PDF file\n",
    "            use_vlm: Whether to use VLM for enhanced understanding\n",
    "            \n",
    "        Returns:\n",
    "            ProcessedDocument with all extracted content\n",
    "        \"\"\"\n",
    "        print(f\"üìÑ Processing: {pdf_path}\")\n",
    "        \n",
    "        doc = fitz.open(pdf_path)\n",
    "        \n",
    "        # Extract metadata\n",
    "        metadata = {\n",
    "            \"title\": doc.metadata.get(\"title\", \"\"),\n",
    "            \"author\": doc.metadata.get(\"author\", \"\"),\n",
    "            \"pages\": doc.page_count,\n",
    "        }\n",
    "        \n",
    "        # Extract text from each page\n",
    "        text_content = {}\n",
    "        for page_num, page in enumerate(doc, 1):\n",
    "            text = page.get_text()\n",
    "            text_content[page_num] = text\n",
    "            print(f\"  Page {page_num}: {len(text)} characters extracted\")\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "        # Extract tables\n",
    "        tables = extract_tables(pdf_path)\n",
    "        print(f\"  Found {len(tables)} table(s)\")\n",
    "        \n",
    "        return ProcessedDocument(\n",
    "            source_path=pdf_path,\n",
    "            num_pages=metadata[\"pages\"],\n",
    "            text_content=text_content,\n",
    "            tables=tables,\n",
    "            metadata=metadata,\n",
    "        )\n",
    "    \n",
    "    def ask_question(\n",
    "        self,\n",
    "        document: ProcessedDocument,\n",
    "        question: str,\n",
    "        use_vlm_for_page: Optional[int] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Answer a question about the document.\n",
    "        \n",
    "        Args:\n",
    "            document: Processed document\n",
    "            question: Question to answer\n",
    "            use_vlm_for_page: If specified, use VLM on this page\n",
    "            \n",
    "        Returns:\n",
    "            Answer based on document content\n",
    "        \"\"\"\n",
    "        if use_vlm_for_page and self.vlm_model:\n",
    "            # Use VLM for visual understanding\n",
    "            page_image = pdf_page_to_image(document.source_path, use_vlm_for_page - 1)\n",
    "            return analyze_document_image(page_image, question)\n",
    "        \n",
    "        # Build context from document\n",
    "        context = document.full_text\n",
    "        \n",
    "        # Add tables if present\n",
    "        if document.tables:\n",
    "            context += \"\\n\\n--- TABLES ---\\n\"\n",
    "            for i, table in enumerate(document.tables):\n",
    "                context += f\"\\nTable {i+1}:\\n{table.to_markdown()}\\n\"\n",
    "        \n",
    "        # Truncate if too long\n",
    "        max_context = 3000\n",
    "        if len(context) > max_context:\n",
    "            context = context[:max_context] + \"...\\n[Truncated]\"\n",
    "        \n",
    "        # Simple keyword-based answering (in production, use LLM)\n",
    "        # For demo, we'll use the VLM on page 1\n",
    "        if self.vlm_model:\n",
    "            page_image = pdf_page_to_image(document.source_path, 0)\n",
    "            full_question = f\"Based on this document, {question}\"\n",
    "            return analyze_document_image(page_image, full_question)\n",
    "        \n",
    "        return f\"Document contains {document.num_pages} pages. Use VLM for detailed Q&A.\"\n",
    "\n",
    "print(\"‚úÖ DocumentProcessor class ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and use the document processor\n",
    "processor = DocumentProcessor(vlm_model=model, vlm_processor=processor)\n",
    "\n",
    "# Process our sample document\n",
    "print(\"\\nüîÑ Processing document...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "doc = processor.process(sample_pdf_path)\n",
    "\n",
    "print(f\"\\nüìä Document Summary:\")\n",
    "print(f\"  Pages: {doc.num_pages}\")\n",
    "print(f\"  Tables: {len(doc.tables)}\")\n",
    "print(f\"  Total characters: {sum(len(t) for t in doc.text_content.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask questions about the document\n",
    "questions = [\n",
    "    \"What is the title of this report?\",\n",
    "    \"What were the key highlights mentioned?\",\n",
    "    \"What is the revenue growth percentage?\",\n",
    "]\n",
    "\n",
    "print(\"\\nüìù Document Q&A\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\n‚ùì {q}\")\n",
    "    # Use VLM on specific pages for visual content\n",
    "    answer = processor.ask_question(doc, q, use_vlm_for_page=1)\n",
    "    print(f\"üí¨ {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Handling Scanned PDFs\n",
    "```python\n",
    "# ‚ùå Wrong: Assumes all PDFs have selectable text\n",
    "text = page.get_text()\n",
    "if not text:  # Empty! Document is scanned\n",
    "    # Now what?\n",
    "\n",
    "# ‚úÖ Right: Fallback to OCR for scanned pages\n",
    "text = page.get_text()\n",
    "if not text.strip():\n",
    "    # Page is scanned - use OCR\n",
    "    image = pdf_page_to_image(pdf_path, page_num)\n",
    "    text = ocr_image(image)\n",
    "```\n",
    "**Why:** Many documents (especially older ones) are scanned images.\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 2: Ignoring Document Structure\n",
    "```python\n",
    "# ‚ùå Wrong: Just dump all text together\n",
    "all_text = \" \".join(page.get_text() for page in doc)\n",
    "\n",
    "# ‚úÖ Right: Preserve structure\n",
    "content = {\n",
    "    \"title\": extract_title(doc),\n",
    "    \"sections\": extract_sections(doc),\n",
    "    \"tables\": extract_tables(doc),\n",
    "    \"figures\": extract_figures(doc),\n",
    "}\n",
    "```\n",
    "**Why:** Document structure carries important information (headings, tables, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 3: Low Resolution for OCR\n",
    "```python\n",
    "# ‚ùå Wrong: Low DPI loses detail\n",
    "image = pdf_page_to_image(pdf_path, page, dpi=72)\n",
    "text = ocr_image(image)  # Poor results!\n",
    "\n",
    "# ‚úÖ Right: Use sufficient resolution\n",
    "image = pdf_page_to_image(pdf_path, page, dpi=150)  # Or 300 for fine print\n",
    "text = ocr_image(image)\n",
    "```\n",
    "**Why:** OCR accuracy depends on image resolution. 150-300 DPI is recommended.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ Extracting text from PDFs using PyMuPDF\n",
    "- ‚úÖ Getting structured content blocks with positions\n",
    "- ‚úÖ Extracting tables from documents\n",
    "- ‚úÖ Using OCR for scanned documents\n",
    "- ‚úÖ Leveraging VLMs for visual document understanding\n",
    "- ‚úÖ Building a complete document Q&A pipeline\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "Build an **Invoice Processing System** that:\n",
    "1. Takes invoice images/PDFs as input\n",
    "2. Extracts key fields: invoice number, date, vendor, line items, total\n",
    "3. Validates extracted data (e.g., line items sum to total)\n",
    "4. Outputs structured JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Your code here!\n",
    "\n",
    "def process_invoice(invoice_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process an invoice and extract structured data.\n",
    "    \n",
    "    Args:\n",
    "        invoice_path: Path to invoice PDF or image\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with extracted invoice data\n",
    "    \"\"\"\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [PyMuPDF Documentation](https://pymupdf.readthedocs.io/)\n",
    "- [Tesseract OCR](https://github.com/tesseract-ocr/tesseract)\n",
    "- [Document AI with Vision Transformers](https://arxiv.org/abs/2111.15664)\n",
    "- [LayoutLM Paper](https://arxiv.org/abs/1912.13318)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "if 'model' in dir():\n",
    "    del model\n",
    "if 'processor' in dir():\n",
    "    del processor\n",
    "\n",
    "# Remove sample PDF\n",
    "import os\n",
    "if os.path.exists(sample_pdf_path):\n",
    "    os.remove(sample_pdf_path)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next lab, we'll explore **Audio Transcription** using Whisper for speech-to-text conversion!\n",
    "\n",
    "‚û°Ô∏è Continue to [Lab 05: Audio Transcription](./05-audio-transcription.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
