{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.1.1: Vision-Language Models - SOLUTIONS\n",
    "\n",
    "This notebook contains complete solutions for all exercises and challenges from Lab 01.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup (same as main notebook)\n",
    "import gc\n",
    "import time\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from typing import Optional, Union, List, Dict\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "# Load model\n",
    "model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "def load_image(source: Union[str, str]) -> Image.Image:\n",
    "    source_str = str(source)\n",
    "    if source_str.startswith((\"http://\", \"https://\")):\n",
    "        response = requests.get(source_str, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "    else:\n",
    "        image = Image.open(source_str)\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "def analyze_image(image: Image.Image, question: str, max_new_tokens: int = 256) -> str:\n",
    "    prompt = f\"USER: <image>\\n{question}\\nASSISTANT:\"\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "    \n",
    "    response = processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "    if \"ASSISTANT:\" in response:\n",
    "        response = response.split(\"ASSISTANT:\")[-1].strip()\n",
    "    return response\n",
    "\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Solution: Image Comparison Assistant\n",
    "\n",
    "Build an assistant that compares two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_images(\n",
    "    image1: Image.Image,\n",
    "    image2: Image.Image,\n",
    "    purpose: str = \"general\"\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Compare two images and return analysis.\n",
    "    \n",
    "    Args:\n",
    "        image1: First image\n",
    "        image2: Second image\n",
    "        purpose: What the images are being compared for\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with similarities, differences, and recommendation\n",
    "    \"\"\"\n",
    "    # Analyze each image individually first\n",
    "    desc1 = analyze_image(image1, \"Describe this image in detail in 2-3 sentences.\")\n",
    "    desc2 = analyze_image(image2, \"Describe this image in detail in 2-3 sentences.\")\n",
    "    \n",
    "    # Create a combined analysis prompt\n",
    "    # Note: LLaVA can only see one image at a time, so we use descriptions\n",
    "    comparison_prompt = f\"\"\"I have two images:\n",
    "\n",
    "Image 1: {desc1}\n",
    "\n",
    "Image 2: {desc2}\n",
    "\n",
    "Compare these two images. What are the similarities and differences?\"\"\"\n",
    "    \n",
    "    # Get similarities\n",
    "    similarities = analyze_image(\n",
    "        image1,  # Use first image as visual context\n",
    "        f\"Based on this image and the description '{desc2}' of another image, what might be similar between them?\",\n",
    "        max_new_tokens=150\n",
    "    )\n",
    "    \n",
    "    # Get differences\n",
    "    differences = analyze_image(\n",
    "        image1,\n",
    "        f\"Based on this image and the description '{desc2}' of another image, what are the key differences?\",\n",
    "        max_new_tokens=150\n",
    "    )\n",
    "    \n",
    "    # Get recommendation based on purpose\n",
    "    recommendation_prompt = f\"\"\"Comparing two images:\n",
    "Image 1: {desc1}\n",
    "Image 2: {desc2}\n",
    "\n",
    "For the purpose of '{purpose}', which image would be better and why? Give a brief recommendation.\"\"\"\n",
    "    \n",
    "    recommendation = analyze_image(\n",
    "        image1,\n",
    "        recommendation_prompt,\n",
    "        max_new_tokens=150\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"image1_description\": desc1,\n",
    "        \"image2_description\": desc2,\n",
    "        \"similarities\": similarities,\n",
    "        \"differences\": differences,\n",
    "        \"recommendation\": recommendation,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the comparison\n",
    "img1_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg\"\n",
    "img2_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/YellowLabradorLooking_new.jpg/1200px-YellowLabradorLooking_new.jpg\"\n",
    "\n",
    "img1 = load_image(img1_url)\n",
    "img2 = load_image(img2_url)\n",
    "\n",
    "# Display images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].imshow(img1)\n",
    "axes[0].set_title(\"Image 1\")\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(img2)\n",
    "axes[1].set_title(\"Image 2\")\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare\n",
    "print(\"\\nðŸ” Comparing images...\")\n",
    "result = compare_images(img1, img2, purpose=\"choosing a family pet\")\n",
    "\n",
    "print(\"\\nðŸ“Š Comparison Results:\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in result.items():\n",
    "    print(f\"\\n{key.upper().replace('_', ' ')}:\")\n",
    "    print(f\"  {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Alternative Approach: Side-by-Side Grid\n",
    "\n",
    "If you want to show both images to the VLM at once, you can create a combined image grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_image(\n",
    "    image1: Image.Image,\n",
    "    image2: Image.Image,\n",
    "    labels: tuple = (\"Image 1\", \"Image 2\"),\n",
    "    padding: int = 20,\n",
    ") -> Image.Image:\n",
    "    \"\"\"\n",
    "    Create a side-by-side comparison image.\n",
    "    \"\"\"\n",
    "    from PIL import ImageDraw, ImageFont\n",
    "    \n",
    "    # Resize images to same height\n",
    "    max_height = max(image1.height, image2.height)\n",
    "    \n",
    "    # Calculate new dimensions maintaining aspect ratio\n",
    "    ratio1 = max_height / image1.height\n",
    "    ratio2 = max_height / image2.height\n",
    "    \n",
    "    new_size1 = (int(image1.width * ratio1), max_height)\n",
    "    new_size2 = (int(image2.width * ratio2), max_height)\n",
    "    \n",
    "    img1_resized = image1.resize(new_size1, Image.Resampling.LANCZOS)\n",
    "    img2_resized = image2.resize(new_size2, Image.Resampling.LANCZOS)\n",
    "    \n",
    "    # Create combined canvas\n",
    "    label_height = 40\n",
    "    total_width = new_size1[0] + new_size2[0] + padding * 3\n",
    "    total_height = max_height + label_height + padding * 2\n",
    "    \n",
    "    combined = Image.new('RGB', (total_width, total_height), (255, 255, 255))\n",
    "    \n",
    "    # Paste images\n",
    "    combined.paste(img1_resized, (padding, label_height + padding))\n",
    "    combined.paste(img2_resized, (padding * 2 + new_size1[0], label_height + padding))\n",
    "    \n",
    "    # Add labels\n",
    "    draw = ImageDraw.Draw(combined)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 24)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    draw.text((padding + new_size1[0]//2, 10), labels[0], fill=(0, 0, 0), font=font, anchor=\"mt\")\n",
    "    draw.text((padding * 2 + new_size1[0] + new_size2[0]//2, 10), labels[1], fill=(0, 0, 0), font=font, anchor=\"mt\")\n",
    "    \n",
    "    return combined\n",
    "\n",
    "# Create and analyze comparison image\n",
    "comparison_img = create_comparison_image(img1, img2, (\"Cat\", \"Dog\"))\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.imshow(comparison_img)\n",
    "plt.axis('off')\n",
    "plt.title(\"Combined Comparison Image\")\n",
    "plt.show()\n",
    "\n",
    "# Now the VLM can see both!\n",
    "response = analyze_image(\n",
    "    comparison_img,\n",
    "    \"This image shows two pictures side by side. Compare the left image and the right image. What are the similarities and differences?\",\n",
    "    max_new_tokens=300\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š VLM Comparison (seeing both images):\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, processor\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"âœ… Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
