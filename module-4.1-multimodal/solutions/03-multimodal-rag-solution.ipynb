{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.1.3: Multimodal RAG - SOLUTIONS\n",
    "\n",
    "This notebook contains the complete solution for the Visual Question Answering RAG challenge.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import gc\n",
    "import time\n",
    "import hashlib\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from typing import Optional, List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from transformers import CLIPModel, CLIPProcessor, AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load CLIP\n",
    "print(\"Loading CLIP...\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "clip_model.eval()\n",
    "\n",
    "# Load LLaVA\n",
    "print(\"Loading LLaVA...\")\n",
    "vlm_processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "vlm_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    \"llava-hf/llava-1.5-7b-hf\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Models loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Solution: Visual Question Answering RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualQARAG:\n",
    "    \"\"\"\n",
    "    A Visual Question Answering system that uses RAG to enhance answers.\n",
    "    \n",
    "    This system:\n",
    "    1. Takes a question about an image\n",
    "    2. Retrieves relevant context from a knowledge base\n",
    "    3. Uses a VLM to answer the question with the retrieved context\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize ChromaDB\n",
    "        self.client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=\"visual_qa_kb\",\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "    \n",
    "    def _embed_image(self, image: Image.Image) -> np.ndarray:\n",
    "        \"\"\"Get CLIP embedding for image.\"\"\"\n",
    "        inputs = clip_processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = clip_model.get_image_features(**inputs)\n",
    "            features = features / features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        return features.cpu().numpy()[0]\n",
    "    \n",
    "    def _embed_text(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Get CLIP embedding for text.\"\"\"\n",
    "        inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = clip_model.get_text_features(**inputs)\n",
    "            features = features / features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        return features.cpu().numpy()[0]\n",
    "    \n",
    "    def _analyze_image(self, image: Image.Image, question: str) -> str:\n",
    "        \"\"\"Get VLM analysis of image.\"\"\"\n",
    "        prompt = f\"USER: <image>\\n{question}\\nASSISTANT:\"\n",
    "        \n",
    "        inputs = vlm_processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(vlm_model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output_ids = vlm_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                do_sample=False,\n",
    "            )\n",
    "        \n",
    "        response = vlm_processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "        if \"ASSISTANT:\" in response:\n",
    "            response = response.split(\"ASSISTANT:\")[-1].strip()\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def add_knowledge(self, text: str, metadata: Dict = None) -> str:\n",
    "        \"\"\"\n",
    "        Add knowledge to the RAG database.\n",
    "        \n",
    "        Args:\n",
    "            text: Knowledge text to add\n",
    "            metadata: Optional metadata\n",
    "            \n",
    "        Returns:\n",
    "            ID of added item\n",
    "        \"\"\"\n",
    "        item_id = hashlib.md5(text.encode()).hexdigest()\n",
    "        embedding = self._embed_text(text)\n",
    "        \n",
    "        meta = {\"type\": \"knowledge\"}\n",
    "        if metadata:\n",
    "            meta.update(metadata)\n",
    "        \n",
    "        self.collection.add(\n",
    "            ids=[item_id],\n",
    "            embeddings=[embedding.tolist()],\n",
    "            metadatas=[meta],\n",
    "            documents=[text],\n",
    "        )\n",
    "        \n",
    "        return item_id\n",
    "    \n",
    "    def retrieve_context(\n",
    "        self,\n",
    "        query: str,\n",
    "        image: Optional[Image.Image] = None,\n",
    "        top_k: int = 3,\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant context for a query.\n",
    "        \n",
    "        Args:\n",
    "            query: Text query\n",
    "            image: Optional image to include in search\n",
    "            top_k: Number of results\n",
    "            \n",
    "        Returns:\n",
    "            List of relevant context strings\n",
    "        \"\"\"\n",
    "        # Get text embedding\n",
    "        text_emb = self._embed_text(query)\n",
    "        \n",
    "        # If image provided, combine with image embedding\n",
    "        if image is not None:\n",
    "            image_emb = self._embed_image(image)\n",
    "            # Average the embeddings\n",
    "            combined_emb = (text_emb + image_emb) / 2\n",
    "            combined_emb = combined_emb / np.linalg.norm(combined_emb)\n",
    "            query_emb = combined_emb\n",
    "        else:\n",
    "            query_emb = text_emb\n",
    "        \n",
    "        # Search\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_emb.tolist()],\n",
    "            n_results=top_k,\n",
    "            include=[\"documents\"],\n",
    "        )\n",
    "        \n",
    "        if results[\"documents\"]:\n",
    "            return results[\"documents\"][0]\n",
    "        return []\n",
    "    \n",
    "    def answer_question(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        question: str,\n",
    "        use_rag: bool = True,\n",
    "        top_k: int = 3,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Answer a question about an image using RAG.\n",
    "        \n",
    "        Args:\n",
    "            image: Query image\n",
    "            question: Question about the image\n",
    "            use_rag: Whether to use RAG context\n",
    "            top_k: Number of context items to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with answer, context, and metadata\n",
    "        \"\"\"\n",
    "        context_items = []\n",
    "        \n",
    "        if use_rag and self.collection.count() > 0:\n",
    "            # Retrieve relevant context\n",
    "            context_items = self.retrieve_context(question, image, top_k)\n",
    "        \n",
    "        # Build enhanced question with context\n",
    "        if context_items:\n",
    "            context_str = \"\\n\".join(f\"- {item}\" for item in context_items)\n",
    "            enhanced_question = f\"\"\"Based on the image and the following relevant information:\n",
    "\n",
    "{context_str}\n",
    "\n",
    "Please answer: {question}\"\"\"\n",
    "        else:\n",
    "            enhanced_question = question\n",
    "        \n",
    "        # Get VLM answer\n",
    "        answer = self._analyze_image(image, enhanced_question)\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"context_used\": context_items,\n",
    "            \"rag_enabled\": use_rag,\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ VisualQARAG class ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and populate the RAG system\n",
    "rag = VisualQARAG()\n",
    "\n",
    "# Add knowledge about animals\n",
    "knowledge = [\n",
    "    \"Cats are obligate carnivores and primarily eat meat. They need taurine in their diet which is found in animal tissue.\",\n",
    "    \"Domestic cats (Felis catus) typically live 12-18 years. Indoor cats often live longer than outdoor cats.\",\n",
    "    \"Cats sleep an average of 12-16 hours per day. They are crepuscular, meaning most active at dawn and dusk.\",\n",
    "    \"Dogs are omnivores and can eat a variety of foods including meat, vegetables, and grains.\",\n",
    "    \"Dogs have been domesticated for over 15,000 years and are known for their loyalty and social nature.\",\n",
    "    \"Golden Retrievers are known for being friendly, reliable, and excellent family pets. They were originally bred for hunting.\",\n",
    "    \"Orange tabby cats are not a specific breed but a color pattern. About 80% of orange tabby cats are male.\",\n",
    "    \"Cats have over 20 vocalizations, including the meow, purr, hiss, and chirp. Each has different meanings.\",\n",
    "]\n",
    "\n",
    "print(\"Adding knowledge to RAG...\")\n",
    "for item in knowledge:\n",
    "    rag.add_knowledge(item)\n",
    "\n",
    "print(f\"‚úÖ Added {len(knowledge)} knowledge items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a cat image\n",
    "def load_image_from_url(url: str) -> Image.Image:\n",
    "    response = requests.get(url, timeout=10)\n",
    "    return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "cat_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg\"\n",
    "cat_image = load_image_from_url(cat_url)\n",
    "\n",
    "# Display image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cat_image)\n",
    "plt.axis('off')\n",
    "plt.title(\"Query Image\")\n",
    "plt.show()\n",
    "\n",
    "# Test questions\n",
    "questions = [\n",
    "    \"What type of animal is this and what do they eat?\",\n",
    "    \"How long do these animals typically live?\",\n",
    "    \"What interesting facts can you tell me about this animal's behavior?\",\n",
    "]\n",
    "\n",
    "print(\"\\nüìù Visual QA with RAG\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\n‚ùì Question: {q}\")\n",
    "    \n",
    "    result = rag.answer_question(cat_image, q, use_rag=True)\n",
    "    \n",
    "    print(f\"\\nüìö Context Retrieved:\")\n",
    "    for ctx in result['context_used']:\n",
    "        print(f\"   ‚Ä¢ {ctx[:80]}...\")\n",
    "    \n",
    "    print(f\"\\nüí¨ Answer: {result['answer']}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with and without RAG\n",
    "print(\"\\nüìä Comparison: With vs Without RAG\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "question = \"What type of animal is this and what special facts do you know about orange cats?\"\n",
    "\n",
    "# Without RAG\n",
    "result_no_rag = rag.answer_question(cat_image, question, use_rag=False)\n",
    "print(f\"\\n‚ùå Without RAG:\")\n",
    "print(f\"   {result_no_rag['answer']}\")\n",
    "\n",
    "# With RAG\n",
    "result_with_rag = rag.answer_question(cat_image, question, use_rag=True)\n",
    "print(f\"\\n‚úÖ With RAG:\")\n",
    "print(f\"   Context: {result_with_rag['context_used']}\")\n",
    "print(f\"   Answer: {result_with_rag['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del clip_model, clip_processor, vlm_model, vlm_processor\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"‚úÖ Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
