{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions: Task 8.1 - Attention from Scratch\n",
    "\n",
    "This notebook contains solutions to the exercises from notebook 01.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Implement Attention with Dropout\n",
    "\n",
    "**Task:** Add dropout to the attention weights (commonly used to prevent overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention_with_dropout(Q, K, V, dropout_p=0.1, mask=None, training=True):\n",
    "    \"\"\"\n",
    "    Scaled dot-product attention with dropout.\n",
    "    \n",
    "    Args:\n",
    "        Q, K, V: Query, Key, Value tensors\n",
    "        dropout_p: Dropout probability\n",
    "        mask: Optional attention mask\n",
    "        training: Whether in training mode (dropout active)\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output\n",
    "        attention_weights: Attention weights (after dropout)\n",
    "    \"\"\"\n",
    "    d_k = K.size(-1)\n",
    "    \n",
    "    # Step 1: Compute attention scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "    \n",
    "    # Step 2: Scale by sqrt(d_k)\n",
    "    scores = scores / math.sqrt(d_k)\n",
    "    \n",
    "    # Step 3: Apply mask if provided\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    # Step 4: Apply softmax\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Step 5: Apply dropout to attention weights\n",
    "    if training and dropout_p > 0:\n",
    "        attention_weights = F.dropout(attention_weights, p=dropout_p, training=training)\n",
    "    \n",
    "    # Step 6: Multiply with values\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test\n",
    "Q = torch.randn(1, 4, 8)\n",
    "K = torch.randn(1, 4, 8)\n",
    "V = torch.randn(1, 4, 8)\n",
    "\n",
    "output, attention = scaled_dot_product_attention_with_dropout(Q, K, V, dropout_p=0.1)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention shape: {attention.shape}\")\n",
    "print(\"\\nSolution verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Attention Complexity Analysis\n",
    "\n",
    "**Task:** What is the time and space complexity of self-attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_attention_complexity():\n",
    "    \"\"\"\n",
    "    Measure memory usage for different sequence lengths.\n",
    "    \"\"\"\n",
    "    sequence_lengths = [128, 256, 512, 1024, 2048]\n",
    "    d_model = 64\n",
    "    \n",
    "    memory_usage = []\n",
    "    \n",
    "    for seq_len in sequence_lengths:\n",
    "        # Create attention matrix\n",
    "        attention_matrix = torch.randn(1, seq_len, seq_len)\n",
    "        \n",
    "        # Calculate memory (in MB)\n",
    "        # Each float32 is 4 bytes\n",
    "        memory_mb = attention_matrix.numel() * 4 / (1024 * 1024)\n",
    "        memory_usage.append(memory_mb)\n",
    "        \n",
    "        print(f\"Seq len {seq_len}: Attention matrix size = {seq_len}x{seq_len} = {seq_len**2:,} elements\")\n",
    "        print(f\"  Memory: {memory_mb:.2f} MB\")\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(sequence_lengths, memory_usage, 'bo-')\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Memory (MB)')\n",
    "    plt.title('Attention Memory: O(n²)')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(sequence_lengths, [s**2 for s in sequence_lengths], 'ro-')\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('n²')\n",
    "    plt.title('Quadratic Growth')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n=== Complexity Analysis ===\")\n",
    "    print(\"Time Complexity: O(n² × d) where n = sequence length, d = dimension\")\n",
    "    print(\"Space Complexity: O(n²) for the attention matrix\")\n",
    "    print(\"\\nThis is why:\")\n",
    "    print(\"  - GPT-4 has a 128K context limit\")\n",
    "    print(\"  - Efficient attention variants exist (Flash Attention, Linear Attention)\")\n",
    "    print(\"  - Long sequences are expensive!\")\n",
    "\n",
    "analyze_attention_complexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Relative Position Attention (Challenge)\n",
    "\n",
    "**Task:** Implement relative position attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePositionAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-attention with relative position bias.\n",
    "    \n",
    "    score(i, j) = q_i · k_j + q_i · r_(i-j)\n",
    "    \n",
    "    Where r_(i-j) is a learned embedding for relative position.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_relative_position=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.max_relative_position = max_relative_position\n",
    "        \n",
    "        # Q, K, V projections\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Relative position embeddings\n",
    "        # Range: -max_relative_position to +max_relative_position\n",
    "        num_positions = 2 * max_relative_position + 1\n",
    "        self.relative_embeddings = nn.Embedding(num_positions, d_model)\n",
    "        \n",
    "    def _get_relative_positions(self, seq_len):\n",
    "        \"\"\"Compute relative position matrix.\"\"\"\n",
    "        positions = torch.arange(seq_len)\n",
    "        relative = positions.unsqueeze(0) - positions.unsqueeze(1)\n",
    "        \n",
    "        # Clip to valid range\n",
    "        relative = torch.clamp(\n",
    "            relative, \n",
    "            -self.max_relative_position, \n",
    "            self.max_relative_position\n",
    "        )\n",
    "        \n",
    "        # Shift to positive indices\n",
    "        relative = relative + self.max_relative_position\n",
    "        \n",
    "        return relative\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        Q = self.W_q(x)  # (batch, seq, d_model)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Content-based attention scores: Q @ K^T\n",
    "        content_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_model)\n",
    "        \n",
    "        # Position-based attention scores\n",
    "        relative_positions = self._get_relative_positions(seq_len).to(x.device)\n",
    "        relative_emb = self.relative_embeddings(relative_positions)  # (seq, seq, d_model)\n",
    "        \n",
    "        # Q @ relative_emb^T for each query position\n",
    "        # Q: (batch, seq, d_model)\n",
    "        # relative_emb: (seq, seq, d_model)\n",
    "        position_scores = torch.einsum('bqd,qkd->bqk', Q, relative_emb) / math.sqrt(self.d_model)\n",
    "        \n",
    "        # Combine scores\n",
    "        scores = content_scores + position_scores\n",
    "        \n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax and apply to values\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention, V)\n",
    "        \n",
    "        return output, attention\n",
    "\n",
    "# Test\n",
    "rel_attn = RelativePositionAttention(d_model=64, max_relative_position=16)\n",
    "x = torch.randn(2, 20, 64)\n",
    "output, attention = rel_attn(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention shape: {attention.shape}\")\n",
    "print(\"\\nRelative position attention implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Dropout in attention** is applied AFTER softmax but BEFORE multiplying with V\n",
    "2. **Attention complexity** is O(n²) which limits context length\n",
    "3. **Relative position** encodes distance between tokens, not absolute position\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
