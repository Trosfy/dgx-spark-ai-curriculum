{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions: Task 8.4 - Tokenization Lab\n",
    "\n",
    "This notebook contains solutions to the exercises from notebook 04.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    HAS_TRANSFORMERS = True\n",
    "except ImportError:\n",
    "    HAS_TRANSFORMERS = False\n",
    "    print(\"transformers not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Multilingual Tokenization\n",
    "\n",
    "**Task:** Compare how different tokenizers handle non-English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TRANSFORMERS:\n",
    "    # Load tokenizers\n",
    "    tokenizers = {\n",
    "        \"GPT-2\": AutoTokenizer.from_pretrained(\"gpt2\"),\n",
    "        \"BERT\": AutoTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
    "    }\n",
    "    \n",
    "    # Try to load multilingual tokenizers\n",
    "    try:\n",
    "        tokenizers[\"mBERT\"] = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Test multilingual tokenization\n",
    "    multilingual_texts = [\n",
    "        (\"English\", \"Hello, world!\"),\n",
    "        (\"French\", \"Bonjour le monde!\"),\n",
    "        (\"Spanish\", \"Hola, mundo!\"),\n",
    "        (\"German\", \"Hallo, Welt!\"),\n",
    "        (\"Russian\", \"Привет мир!\"),\n",
    "        (\"Chinese\", \"你好世界!\"),\n",
    "        (\"Japanese\", \"こんにちは世界!\"),\n",
    "    ]\n",
    "    \n",
    "    print(\"Multilingual Tokenization Comparison\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    results = []\n",
    "    for lang, text in multilingual_texts:\n",
    "        print(f\"\\n{lang}: '{text}'\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        row = {\"Language\": lang, \"Text\": text}\n",
    "        \n",
    "        for name, tokenizer in tokenizers.items():\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "            row[name] = len(tokens)\n",
    "            print(f\"  {name}: {len(tokens)} tokens\")\n",
    "            print(f\"    {tokens[:10]}{'...' if len(tokens) > 10 else ''}\")\n",
    "        \n",
    "        results.append(row)\n",
    "    \n",
    "    # Visualize\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"\\nAnalysis:\")\n",
    "    print(\"- GPT-2 is trained mostly on English, so non-Latin scripts get many tokens\")\n",
    "    print(\"- mBERT handles multiple languages more efficiently\")\n",
    "    print(\"- Chinese/Japanese often tokenized character-by-character with English tokenizers\")\n",
    "else:\n",
    "    print(\"Please install transformers to run this exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Code Tokenization\n",
    "\n",
    "**Task:** Analyze how different tokenizers handle programming code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TRANSFORMERS:\n",
    "    code_samples = [\n",
    "        ('print', 'print(\"Hello, World!\")'),\n",
    "        ('function', 'def factorial(n): return 1 if n <= 1 else n * factorial(n-1)'),\n",
    "        ('class', 'class MyClass:\\n    def __init__(self, value):\\n        self.value = value'),\n",
    "        ('list_comp', '[x**2 for x in range(10) if x % 2 == 0]'),\n",
    "        ('lambda', 'sorted(items, key=lambda x: x.name.lower())'),\n",
    "    ]\n",
    "    \n",
    "    print(\"Code Tokenization Analysis\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    gpt2_tok = tokenizers[\"GPT-2\"]\n",
    "    \n",
    "    for name, code in code_samples:\n",
    "        tokens = gpt2_tok.tokenize(code)\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Code: {code[:60]}{'...' if len(code) > 60 else ''}\")\n",
    "        print(f\"  Tokens ({len(tokens)}): {tokens}\")\n",
    "        \n",
    "        # Analyze token types\n",
    "        keywords = sum(1 for t in tokens if t.strip() in ['def', 'class', 'return', 'if', 'else', 'for', 'in', 'lambda'])\n",
    "        operators = sum(1 for t in tokens if t.strip() in ['(', ')', '[', ']', ':', '=', '+', '-', '*', '**', '%'])\n",
    "        \n",
    "        print(f\"  Keywords: {keywords}, Operators/punctuation: {operators}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"\\nObservations:\")\n",
    "    print(\"- GPT-2 often splits camelCase/snake_case identifiers\")\n",
    "    print(\"- Common keywords like 'def', 'class' are usually single tokens\")\n",
    "    print(\"- Indentation (spaces/tabs) becomes tokens too\")\n",
    "    print(\"- Special characters like '**' may be single or multiple tokens\")\n",
    "else:\n",
    "    print(\"Please install transformers to run this exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Implement WordPiece Tokenization\n",
    "\n",
    "**Task:** Implement WordPiece (used by BERT), which differs from BPE in how it scores merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleWordPiece:\n",
    "    \"\"\"\n",
    "    Simple WordPiece implementation for educational purposes.\n",
    "    \n",
    "    Unlike BPE which merges based on frequency, WordPiece\n",
    "    merges based on likelihood improvement:\n",
    "    \n",
    "    score(a, b) = freq(ab) / (freq(a) * freq(b))\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "        self.merges = {}\n",
    "        \n",
    "    def _get_word_freqs(self, text):\n",
    "        \"\"\"Split text into words and count frequencies.\"\"\"\n",
    "        words = re.findall(r\"\\w+\", text.lower())\n",
    "        return Counter(words)\n",
    "    \n",
    "    def _word_to_chars(self, word):\n",
    "        \"\"\"Convert word to characters with ## prefix for non-initial.\"\"\"\n",
    "        if not word:\n",
    "            return []\n",
    "        chars = [word[0]]\n",
    "        for c in word[1:]:\n",
    "            chars.append(f\"##{c}\")\n",
    "        return chars\n",
    "    \n",
    "    def _get_pair_scores(self, word_freqs, word_tokens):\n",
    "        \"\"\"Compute WordPiece scores for pairs.\"\"\"\n",
    "        # Count individual token frequencies\n",
    "        token_freqs = Counter()\n",
    "        pair_freqs = Counter()\n",
    "        \n",
    "        for word, freq in word_freqs.items():\n",
    "            tokens = word_tokens[word]\n",
    "            for t in tokens:\n",
    "                token_freqs[t] += freq\n",
    "            for i in range(len(tokens) - 1):\n",
    "                pair = (tokens[i], tokens[i+1])\n",
    "                pair_freqs[pair] += freq\n",
    "        \n",
    "        # Compute scores: freq(ab) / (freq(a) * freq(b))\n",
    "        scores = {}\n",
    "        for pair, freq in pair_freqs.items():\n",
    "            a, b = pair\n",
    "            if token_freqs[a] > 0 and token_freqs[b] > 0:\n",
    "                scores[pair] = freq / (token_freqs[a] * token_freqs[b])\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def _merge_pair(self, word_tokens, pair):\n",
    "        \"\"\"Merge a pair in all words.\"\"\"\n",
    "        new_word_tokens = {}\n",
    "        new_token = pair[0] + pair[1].replace(\"##\", \"\")\n",
    "        \n",
    "        for word, tokens in word_tokens.items():\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == pair:\n",
    "                    new_tokens.append(new_token)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "            new_word_tokens[word] = new_tokens\n",
    "        \n",
    "        return new_word_tokens, new_token\n",
    "    \n",
    "    def train(self, text, num_merges=100, verbose=True):\n",
    "        \"\"\"Train WordPiece tokenizer.\"\"\"\n",
    "        word_freqs = self._get_word_freqs(text)\n",
    "        word_tokens = {word: self._word_to_chars(word) for word in word_freqs}\n",
    "        \n",
    "        # Initial vocabulary\n",
    "        vocab = set()\n",
    "        for tokens in word_tokens.values():\n",
    "            vocab.update(tokens)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Initial vocabulary size: {len(vocab)}\")\n",
    "        \n",
    "        for i in range(num_merges):\n",
    "            scores = self._get_pair_scores(word_freqs, word_tokens)\n",
    "            if not scores:\n",
    "                break\n",
    "            \n",
    "            # Find best pair (highest score)\n",
    "            best_pair = max(scores, key=scores.get)\n",
    "            best_score = scores[best_pair]\n",
    "            \n",
    "            word_tokens, new_token = self._merge_pair(word_tokens, best_pair)\n",
    "            self.merges[best_pair] = new_token\n",
    "            vocab.add(new_token)\n",
    "            \n",
    "            if verbose and (i + 1) % 20 == 0:\n",
    "                print(f\"Merge {i+1}: {best_pair} -> '{new_token}' (score: {best_score:.4f})\")\n",
    "        \n",
    "        self.vocab = {token: i for i, token in enumerate(sorted(vocab))}\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Final vocabulary size: {len(self.vocab)}\")\n",
    "    \n",
    "    def tokenize(self, word):\n",
    "        \"\"\"Tokenize a word using learned vocabulary.\"\"\"\n",
    "        tokens = self._word_to_chars(word.lower())\n",
    "        \n",
    "        changed = True\n",
    "        while changed:\n",
    "            changed = False\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if i < len(tokens) - 1:\n",
    "                    pair = (tokens[i], tokens[i+1])\n",
    "                    if pair in self.merges:\n",
    "                        new_tokens.append(self.merges[pair])\n",
    "                        i += 2\n",
    "                        changed = True\n",
    "                        continue\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "            tokens = new_tokens\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "# Test WordPiece\n",
    "training_text = \"\"\"\n",
    "Machine learning is a subset of artificial intelligence.\n",
    "Deep learning uses neural networks with many layers.\n",
    "Transformers revolutionized natural language processing.\n",
    "\"\"\" * 50\n",
    "\n",
    "wp = SimpleWordPiece()\n",
    "wp.train(training_text, num_merges=50, verbose=True)\n",
    "\n",
    "print(\"\\nTest tokenization:\")\n",
    "test_words = [\"learning\", \"machine\", \"transformers\", \"neural\"]\n",
    "for word in test_words:\n",
    "    tokens = wp.tokenize(word)\n",
    "    print(f\"  '{word}' -> {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Multilingual handling** varies greatly between tokenizers - use multilingual models for non-English\n",
    "2. **Code tokenization** splits identifiers and operators differently than natural language\n",
    "3. **WordPiece** differs from BPE by using likelihood ratios instead of raw frequencies\n",
    "4. **Token efficiency** matters for cost and context length in production\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
