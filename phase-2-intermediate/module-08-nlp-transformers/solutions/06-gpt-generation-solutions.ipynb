{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions: Task 8.6 - GPT Text Generation\n",
    "\n",
    "This notebook contains solutions to the exercises from notebook 06.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "    HAS_TRANSFORMERS = True\n",
    "except ImportError:\n",
    "    HAS_TRANSFORMERS = False\n",
    "    print(\"Please install transformers: pip install transformers\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Implement Nucleus Sampling (Top-p)\n",
    "\n",
    "**Task:** Implement top-p (nucleus) sampling from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_sampling(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompt, \n",
    "    max_length=50,\n",
    "    p=0.9,\n",
    "    temperature=1.0,\n",
    "    device='cpu'\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate text using nucleus (top-p) sampling.\n",
    "    \n",
    "    Top-p sampling includes the smallest set of tokens whose\n",
    "    cumulative probability exceeds p.\n",
    "    \n",
    "    Args:\n",
    "        model: Language model\n",
    "        tokenizer: Tokenizer\n",
    "        prompt: Starting text\n",
    "        max_length: Maximum tokens to generate\n",
    "        p: Cumulative probability threshold (0.9 = top 90%)\n",
    "        temperature: Sampling temperature\n",
    "        device: Device to use\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    generated = input_ids[0].tolist()\n",
    "    \n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(torch.tensor([generated]).to(device))\n",
    "            logits = outputs.logits[0, -1, :]  # Last token's logits\n",
    "        \n",
    "        # Apply temperature\n",
    "        scaled_logits = logits / temperature\n",
    "        probs = F.softmax(scaled_logits, dim=-1)\n",
    "        \n",
    "        # Sort by probability (descending)\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "        \n",
    "        # Compute cumulative probability\n",
    "        cumsum = torch.cumsum(sorted_probs, dim=-1)\n",
    "        \n",
    "        # Find cutoff index (first index where cumsum >= p)\n",
    "        cutoff_mask = cumsum >= p\n",
    "        if cutoff_mask.any():\n",
    "            cutoff_idx = cutoff_mask.nonzero()[0].item() + 1\n",
    "        else:\n",
    "            cutoff_idx = len(sorted_probs)\n",
    "        \n",
    "        # Keep only tokens in the nucleus\n",
    "        nucleus_probs = sorted_probs[:cutoff_idx]\n",
    "        nucleus_indices = sorted_indices[:cutoff_idx]\n",
    "        \n",
    "        # Re-normalize\n",
    "        nucleus_probs = nucleus_probs / nucleus_probs.sum()\n",
    "        \n",
    "        # Sample from nucleus\n",
    "        selected_idx = torch.multinomial(nucleus_probs, num_samples=1).item()\n",
    "        next_token = nucleus_indices[selected_idx].item()\n",
    "        \n",
    "        # Check for EOS\n",
    "        if next_token == eos_token_id:\n",
    "            break\n",
    "        \n",
    "        generated.append(next_token)\n",
    "    \n",
    "    return tokenizer.decode(generated)\n",
    "\n",
    "# Test\n",
    "if HAS_TRANSFORMERS:\n",
    "    print(\"Loading GPT-2...\")\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    prompt = \"The future of artificial intelligence\"\n",
    "    \n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"\\nGenerated (top-p=0.9):\")\n",
    "    text = top_p_sampling(model, tokenizer, prompt, max_length=50, p=0.9, device=device)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Implement Beam Search\n",
    "\n",
    "**Task:** Implement beam search decoding from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_length=50,\n",
    "    beam_width=5,\n",
    "    length_penalty=1.0,\n",
    "    device='cpu'\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate text using beam search.\n",
    "    \n",
    "    Maintains multiple hypotheses (beams) and returns the best one.\n",
    "    \n",
    "    Args:\n",
    "        model: Language model\n",
    "        tokenizer: Tokenizer\n",
    "        prompt: Starting text\n",
    "        max_length: Maximum tokens to generate\n",
    "        beam_width: Number of beams to maintain\n",
    "        length_penalty: Penalty for longer sequences (>1 encourages longer)\n",
    "        device: Device to use\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    # Each beam: (sequence as list, cumulative log probability)\n",
    "    beams = [(input_ids[0].tolist(), 0.0)]\n",
    "    completed = []\n",
    "    \n",
    "    for step in range(max_length):\n",
    "        all_candidates = []\n",
    "        \n",
    "        for seq, score in beams:\n",
    "            # Get predictions\n",
    "            with torch.no_grad():\n",
    "                outputs = model(torch.tensor([seq]).to(device))\n",
    "                logits = outputs.logits[0, -1, :]\n",
    "            \n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            \n",
    "            # Get top beam_width tokens\n",
    "            top_log_probs, top_indices = log_probs.topk(beam_width)\n",
    "            \n",
    "            for log_prob, token_id in zip(top_log_probs, top_indices):\n",
    "                new_seq = seq + [token_id.item()]\n",
    "                new_score = score + log_prob.item()\n",
    "                \n",
    "                if token_id.item() == eos_token_id:\n",
    "                    # Apply length penalty for completed sequences\n",
    "                    normalized_score = new_score / (len(new_seq) ** length_penalty)\n",
    "                    completed.append((new_seq, normalized_score))\n",
    "                else:\n",
    "                    all_candidates.append((new_seq, new_score))\n",
    "        \n",
    "        # Keep top beam_width candidates\n",
    "        all_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        beams = all_candidates[:beam_width]\n",
    "        \n",
    "        # Early stopping if all beams completed\n",
    "        if not beams:\n",
    "            break\n",
    "    \n",
    "    # Add remaining beams to completed\n",
    "    for seq, score in beams:\n",
    "        normalized_score = score / (len(seq) ** length_penalty)\n",
    "        completed.append((seq, normalized_score))\n",
    "    \n",
    "    # Return best sequence\n",
    "    if completed:\n",
    "        best_seq, best_score = max(completed, key=lambda x: x[1])\n",
    "        return tokenizer.decode(best_seq), best_score\n",
    "    else:\n",
    "        return tokenizer.decode(beams[0][0]), beams[0][1]\n",
    "\n",
    "# Test\n",
    "if HAS_TRANSFORMERS:\n",
    "    prompt = \"The best way to learn programming is\"\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"\\nBeam search (width=5):\")\n",
    "    text, score = beam_search(model, tokenizer, prompt, max_length=30, beam_width=5, device=device)\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Compare Decoding Strategies\n",
    "\n",
    "**Task:** Compare different decoding strategies on the same prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_decoding_strategies(model, tokenizer, prompt, device='cpu'):\n",
    "    \"\"\"\n",
    "    Compare different text generation strategies.\n",
    "    \"\"\"\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    strategies = [\n",
    "        (\"Greedy\", lambda: model.generate(\n",
    "            tokenizer.encode(prompt, return_tensors='pt').to(device),\n",
    "            max_new_tokens=40,\n",
    "            do_sample=False\n",
    "        )),\n",
    "        (\"Top-k (k=50)\", lambda: model.generate(\n",
    "            tokenizer.encode(prompt, return_tensors='pt').to(device),\n",
    "            max_new_tokens=40,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            temperature=1.0\n",
    "        )),\n",
    "        (\"Top-p (p=0.9)\", lambda: model.generate(\n",
    "            tokenizer.encode(prompt, return_tensors='pt').to(device),\n",
    "            max_new_tokens=40,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=1.0\n",
    "        )),\n",
    "        (\"Beam (width=5)\", lambda: model.generate(\n",
    "            tokenizer.encode(prompt, return_tensors='pt').to(device),\n",
    "            max_new_tokens=40,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )),\n",
    "        (\"Temperature=0.7\", lambda: model.generate(\n",
    "            tokenizer.encode(prompt, return_tensors='pt').to(device),\n",
    "            max_new_tokens=40,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "        )),\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, generate_fn in strategies:\n",
    "        torch.manual_seed(42)  # For reproducibility\n",
    "        start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output_ids = generate_fn()\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        results.append({\n",
    "            'name': name,\n",
    "            'text': text,\n",
    "            'time': elapsed,\n",
    "            'tokens': len(output_ids[0])\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{name} ({elapsed:.2f}s):\")\n",
    "        print(f\"  {text}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"{'Strategy':<20} {'Time':<10} {'Tokens':<10}\")\n",
    "    print(\"-\" * 40)\n",
    "    for r in results:\n",
    "        print(f\"{r['name']:<20} {r['time']:.3f}s    {r['tokens']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "if HAS_TRANSFORMERS:\n",
    "    results = compare_decoding_strategies(\n",
    "        model, tokenizer,\n",
    "        \"In the year 2050, humans will\",\n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Implement Contrastive Search\n",
    "\n",
    "**Task:** Implement contrastive search which balances likelihood with diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_search(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_length=50,\n",
    "    k=5,\n",
    "    alpha=0.6,\n",
    "    device='cpu'\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate text using contrastive search.\n",
    "    \n",
    "    At each step, selects the token that maximizes:\n",
    "    (1 - alpha) * log_prob(token) - alpha * max_sim(token, prev_tokens)\n",
    "    \n",
    "    This balances likelihood with diversity.\n",
    "    \n",
    "    Args:\n",
    "        model: Language model (with hidden states)\n",
    "        tokenizer: Tokenizer\n",
    "        prompt: Starting text\n",
    "        max_length: Maximum tokens to generate\n",
    "        k: Number of top candidates to consider\n",
    "        alpha: Balance between likelihood (0) and diversity (1)\n",
    "        device: Device to use\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    generated = input_ids[0].tolist()\n",
    "    \n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    # Store hidden states for similarity computation\n",
    "    prev_hidden_states = []\n",
    "    \n",
    "    for step in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                torch.tensor([generated]).to(device),\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            logits = outputs.logits[0, -1, :]\n",
    "            hidden = outputs.hidden_states[-1][0, -1, :]  # Last layer, last token\n",
    "        \n",
    "        # Get top-k candidates\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        top_log_probs, top_indices = log_probs.topk(k)\n",
    "        \n",
    "        if len(prev_hidden_states) == 0:\n",
    "            # First token: just use highest probability\n",
    "            best_idx = 0\n",
    "        else:\n",
    "            # Compute scores for each candidate\n",
    "            scores = []\n",
    "            prev_hiddens = torch.stack(prev_hidden_states)  # (num_prev, hidden_dim)\n",
    "            \n",
    "            for i, token_id in enumerate(top_indices):\n",
    "                # Likelihood component\n",
    "                likelihood = (1 - alpha) * top_log_probs[i].item()\n",
    "                \n",
    "                # Diversity component: negative max similarity to previous tokens\n",
    "                # (Simplified: using embedding similarity instead of full hidden states)\n",
    "                token_hidden = hidden  # Would ideally get hidden for this token\n",
    "                similarities = F.cosine_similarity(\n",
    "                    token_hidden.unsqueeze(0),\n",
    "                    prev_hiddens,\n",
    "                    dim=-1\n",
    "                )\n",
    "                max_sim = similarities.max().item()\n",
    "                diversity = -alpha * max_sim\n",
    "                \n",
    "                scores.append(likelihood + diversity)\n",
    "            \n",
    "            best_idx = max(range(len(scores)), key=lambda i: scores[i])\n",
    "        \n",
    "        next_token = top_indices[best_idx].item()\n",
    "        \n",
    "        if next_token == eos_token_id:\n",
    "            break\n",
    "        \n",
    "        generated.append(next_token)\n",
    "        prev_hidden_states.append(hidden.clone())\n",
    "        \n",
    "        # Limit memory\n",
    "        if len(prev_hidden_states) > 50:\n",
    "            prev_hidden_states = prev_hidden_states[-50:]\n",
    "    \n",
    "    return tokenizer.decode(generated)\n",
    "\n",
    "# Test\n",
    "if HAS_TRANSFORMERS:\n",
    "    prompt = \"The scientist discovered that\"\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"\\nContrastive search (alpha=0.6):\")\n",
    "    text = contrastive_search(model, tokenizer, prompt, max_length=40, alpha=0.6, device=device)\n",
    "    print(text)\n",
    "    \n",
    "    print(\"\\nContrastive search (alpha=0.3, more focused):\")\n",
    "    text = contrastive_search(model, tokenizer, prompt, max_length=40, alpha=0.3, device=device)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Top-p sampling** dynamically adjusts the candidate set based on probability mass\n",
    "2. **Beam search** finds high-probability sequences but can be repetitive\n",
    "3. **Temperature** controls randomness - lower is more deterministic\n",
    "4. **Contrastive search** explicitly encourages diversity to reduce repetition\n",
    "5. Best strategy depends on use case: creative writing vs factual generation\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
