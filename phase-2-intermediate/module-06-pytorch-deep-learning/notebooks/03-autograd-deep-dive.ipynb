{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6.3: Autograd Deep Dive - Custom Automatic Differentiation\n",
    "\n",
    "**Module:** 6 - Deep Learning with PyTorch  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê (Intermediate-Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand how PyTorch's autograd system works\n",
    "- [ ] Implement custom autograd functions with `torch.autograd.Function`\n",
    "- [ ] Create novel activation functions (Swish, Mish)\n",
    "- [ ] Verify gradients using `torch.autograd.gradcheck`\n",
    "- [ ] Use hooks for model introspection\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Tasks 6.1, 6.2\n",
    "- Knowledge of: Calculus (derivatives), backpropagation\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "Sometimes PyTorch's built-in operations aren't enough. You might need custom autograd functions for:\n",
    "\n",
    "- **Novel activation functions**: Research new architectures\n",
    "- **Memory-efficient backprop**: Gradient checkpointing\n",
    "- **Custom loss functions**: Domain-specific objectives\n",
    "- **Numerical stability**: Special handling for edge cases\n",
    "- **Hardware optimization**: Custom CUDA kernels\n",
    "\n",
    "Companies like OpenAI, DeepMind, and Meta regularly implement custom autograd functions for their research.\n",
    "\n",
    "---\n",
    "\n",
    "## ELI5: What is Automatic Differentiation?\n",
    "\n",
    "> **Imagine you're tracking a recipe...** üìù\n",
    ">\n",
    "> You make a cake by:\n",
    "> 1. Mix flour and eggs ‚Üí batter\n",
    "> 2. Add sugar ‚Üí sweet batter\n",
    "> 3. Bake ‚Üí cake\n",
    ">\n",
    "> Now the cake came out wrong. You want to know: \"How much did each ingredient affect the final result?\"\n",
    ">\n",
    "> Autograd is like keeping a detailed journal of every step. When you taste the cake, you can trace back:\n",
    "> - \"The cake is too sweet ‚Üí sweet batter contributed too much ‚Üí I used too much sugar!\"\n",
    ">\n",
    "> **In AI terms:** Each operation records how it transforms inputs. During backpropagation, we reverse the journal to compute gradients (how much each parameter affected the loss).\n",
    ">\n",
    "> The \"computational graph\" is your recipe journal!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding Autograd Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function, gradcheck\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic autograd example\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "y = x ** 2 + 2 * x + 1  # Polynomial: x¬≤ + 2x + 1\n",
    "z = y.sum()  # Need scalar for backward\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"y = x¬≤ + 2x + 1 = {y}\")\n",
    "print(f\"z = sum(y) = {z}\")\n",
    "\n",
    "# Compute gradients\n",
    "z.backward()\n",
    "\n",
    "# Gradient should be: dy/dx = 2x + 2\n",
    "# At x = [2, 3]: gradients = [6, 8]\n",
    "print(f\"\\nGradient (dy/dx = 2x + 2): {x.grad}\")\n",
    "print(f\"Expected: {2 * x.detach() + 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Computational Graph\n",
    "\n",
    "Every tensor with `requires_grad=True` records operations in a DAG (Directed Acyclic Graph). Let's visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the computational graph\n",
    "a = torch.tensor([1.0], requires_grad=True)\n",
    "b = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "c = a * b  # Multiplication\n",
    "d = c + a  # Addition\n",
    "e = d.relu()  # ReLU activation\n",
    "\n",
    "print(\"=== Computational Graph ===\")\n",
    "print(f\"a: {a}, requires_grad={a.requires_grad}\")\n",
    "print(f\"b: {b}, requires_grad={b.requires_grad}\")\n",
    "print(f\"c = a * b = {c}, grad_fn={c.grad_fn}\")\n",
    "print(f\"d = c + a = {d}, grad_fn={d.grad_fn}\")\n",
    "print(f\"e = relu(d) = {e}, grad_fn={e.grad_fn}\")\n",
    "\n",
    "# Backpropagate\n",
    "e.backward()\n",
    "\n",
    "print(f\"\\nGradients:\")\n",
    "print(f\"da/de = {a.grad}\")\n",
    "print(f\"db/de = {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "The `grad_fn` attribute shows which operation created each tensor:\n",
    "- `MulBackward0` - multiplication\n",
    "- `AddBackward0` - addition\n",
    "- `ReluBackward0` - ReLU\n",
    "\n",
    "When we call `backward()`, PyTorch traverses this graph in reverse, applying the chain rule!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Custom Autograd Functions\n",
    "\n",
    "To create a custom operation with gradients, we extend `torch.autograd.Function`:\n",
    "\n",
    "```python\n",
    "class MyFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Compute output\n",
    "        # Save tensors for backward using ctx.save_for_backward()\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Retrieve saved tensors\n",
    "        # Compute gradient with respect to input\n",
    "        return grad_input\n",
    "```\n",
    "\n",
    "Let's implement some custom activation functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Activation 1: Swish\n",
    "\n",
    "**Swish** (also called SiLU) was discovered by Google Brain through neural architecture search.\n",
    "\n",
    "$$\\text{Swish}(x) = x \\cdot \\sigma(x) = \\frac{x}{1 + e^{-x}}$$\n",
    "\n",
    "Derivative:\n",
    "$$\\frac{d}{dx}\\text{Swish}(x) = \\sigma(x) + x \\cdot \\sigma(x) \\cdot (1 - \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwishFunction(Function):\n",
    "    \"\"\"\n",
    "    Custom autograd function for Swish activation.\n",
    "    \n",
    "    Swish(x) = x * sigmoid(x)\n",
    "    \n",
    "    This is also known as SiLU (Sigmoid Linear Unit) and is used\n",
    "    in modern architectures like EfficientNet and Transformers.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: compute Swish(x) = x * sigmoid(x)\n",
    "        \n",
    "        Args:\n",
    "            ctx: Context object to save tensors for backward\n",
    "            x: Input tensor\n",
    "        \n",
    "        Returns:\n",
    "            Swish activation applied to x\n",
    "        \"\"\"\n",
    "        sigmoid_x = torch.sigmoid(x)\n",
    "        result = x * sigmoid_x\n",
    "        \n",
    "        # Save sigmoid for backward (saves recomputing it)\n",
    "        ctx.save_for_backward(x, sigmoid_x)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradient of Swish.\n",
    "        \n",
    "        d/dx Swish(x) = sigmoid(x) + x * sigmoid(x) * (1 - sigmoid(x))\n",
    "                      = sigmoid(x) * (1 + x * (1 - sigmoid(x)))\n",
    "        \n",
    "        Args:\n",
    "            ctx: Context with saved tensors\n",
    "            grad_output: Gradient from downstream\n",
    "        \n",
    "        Returns:\n",
    "            Gradient with respect to input\n",
    "        \"\"\"\n",
    "        x, sigmoid_x = ctx.saved_tensors\n",
    "        \n",
    "        # Compute gradient\n",
    "        # d/dx [x * sigmoid(x)] = sigmoid(x) + x * sigmoid(x) * (1 - sigmoid(x))\n",
    "        grad_input = sigmoid_x * (1 + x * (1 - sigmoid_x))\n",
    "        \n",
    "        # Chain rule: multiply by upstream gradient\n",
    "        return grad_output * grad_input\n",
    "\n",
    "\n",
    "# Create a convenient wrapper\n",
    "def swish_custom(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Apply custom Swish activation.\"\"\"\n",
    "    return SwishFunction.apply(x)\n",
    "\n",
    "\n",
    "# Test our implementation\n",
    "x = torch.randn(5, requires_grad=True)\n",
    "y = swish_custom(x)\n",
    "y.sum().backward()\n",
    "\n",
    "print(\"=== Custom Swish Test ===\")\n",
    "print(f\"Input: {x.data}\")\n",
    "print(f\"Output: {y.data}\")\n",
    "print(f\"Gradient: {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Gradients with gradcheck\n",
    "\n",
    "`gradcheck` computes numerical gradients (finite differences) and compares them to our analytical gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify gradients using numerical differentiation\n",
    "x_test = torch.randn(3, 4, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "# gradcheck uses float64 for numerical precision\n",
    "test_passed = gradcheck(SwishFunction.apply, (x_test,), eps=1e-6, atol=1e-4, rtol=1e-3)\n",
    "\n",
    "print(f\"Gradient check passed: {test_passed}\")\n",
    "\n",
    "if test_passed:\n",
    "    print(\"Our custom gradient is correct!\")\n",
    "else:\n",
    "    print(\"WARNING: Gradient mismatch detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Activation 2: Mish\n",
    "\n",
    "**Mish** is a self-regularized non-monotonic activation function.\n",
    "\n",
    "$$\\text{Mish}(x) = x \\cdot \\tanh(\\text{softplus}(x)) = x \\cdot \\tanh(\\ln(1 + e^x))$$\n",
    "\n",
    "The derivative is more complex:\n",
    "$$\\frac{d}{dx}\\text{Mish}(x) = \\frac{e^x \\cdot \\omega}{\\delta^2}$$\n",
    "\n",
    "where:\n",
    "- $\\omega = 4(x+1) + 4e^{2x} + e^{3x} + e^x(4x+6)$\n",
    "- $\\delta = 2e^x + e^{2x} + 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MishFunction(Function):\n",
    "    \"\"\"\n",
    "    Custom autograd function for Mish activation.\n",
    "    \n",
    "    Mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^x))\n",
    "    \n",
    "    Paper: \"Mish: A Self Regularized Non-Monotonic Activation Function\"\n",
    "    https://arxiv.org/abs/1908.08681\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass: compute Mish(x).\"\"\"\n",
    "        # softplus(x) = ln(1 + e^x)\n",
    "        softplus_x = F.softplus(x)\n",
    "        tanh_softplus = torch.tanh(softplus_x)\n",
    "        result = x * tanh_softplus\n",
    "        \n",
    "        # Save for backward\n",
    "        ctx.save_for_backward(x, tanh_softplus)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradient of Mish.\n",
    "        \n",
    "        Using the formula:\n",
    "        d/dx Mish(x) = tanh(sp) + x * sech¬≤(sp) * œÉ(x)\n",
    "        where sp = softplus(x), œÉ = sigmoid\n",
    "        \"\"\"\n",
    "        x, tanh_sp = ctx.saved_tensors\n",
    "        \n",
    "        # Recompute softplus and sigmoid\n",
    "        sp = F.softplus(x)\n",
    "        sigmoid_x = torch.sigmoid(x)\n",
    "        \n",
    "        # sech¬≤(x) = 1 - tanh¬≤(x)\n",
    "        sech2_sp = 1 - tanh_sp ** 2\n",
    "        \n",
    "        # Gradient: tanh(softplus(x)) + x * sech¬≤(softplus(x)) * sigmoid(x)\n",
    "        grad_input = tanh_sp + x * sech2_sp * sigmoid_x\n",
    "        \n",
    "        return grad_output * grad_input\n",
    "\n",
    "\n",
    "def mish_custom(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Apply custom Mish activation.\"\"\"\n",
    "    return MishFunction.apply(x)\n",
    "\n",
    "\n",
    "# Verify Mish gradients\n",
    "x_test = torch.randn(3, 4, dtype=torch.float64, requires_grad=True)\n",
    "test_passed = gradcheck(MishFunction.apply, (x_test,), eps=1e-6, atol=1e-4, rtol=1e-3)\n",
    "print(f\"Mish gradient check passed: {test_passed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare activation functions\n",
    "x = torch.linspace(-5, 5, 200)\n",
    "\n",
    "activations = {\n",
    "    'ReLU': F.relu(x),\n",
    "    'Swish (Custom)': swish_custom(x.clone().requires_grad_(True)).detach(),\n",
    "    'Mish (Custom)': mish_custom(x.clone().requires_grad_(True)).detach(),\n",
    "    'GELU': F.gelu(x),\n",
    "    'Tanh': torch.tanh(x),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot activations\n",
    "ax1 = axes[0]\n",
    "for name, y in activations.items():\n",
    "    ax1.plot(x.numpy(), y.numpy(), label=name, linewidth=2)\n",
    "ax1.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax1.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('f(x)')\n",
    "ax1.set_title('Activation Functions')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot derivatives\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Compute derivatives numerically for visualization\n",
    "for name, _ in activations.items():\n",
    "    x_grad = x.clone().requires_grad_(True)\n",
    "    if name == 'ReLU':\n",
    "        y = F.relu(x_grad)\n",
    "    elif 'Swish' in name:\n",
    "        y = swish_custom(x_grad)\n",
    "    elif 'Mish' in name:\n",
    "        y = mish_custom(x_grad)\n",
    "    elif name == 'GELU':\n",
    "        y = F.gelu(x_grad)\n",
    "    else:  # Tanh\n",
    "        y = torch.tanh(x_grad)\n",
    "    \n",
    "    y.sum().backward()\n",
    "    ax2.plot(x.numpy(), x_grad.grad.numpy(), label=name, linewidth=2)\n",
    "\n",
    "ax2.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax2.axhline(y=1, color='gray', linestyle='--', linewidth=0.5)\n",
    "ax2.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel(\"f'(x)\")\n",
    "ax2.set_title('Activation Derivatives')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations:\n",
    "\n",
    "1. **ReLU**: Derivative is 0 for x < 0 (\"dead neurons\" problem)\n",
    "2. **Swish/Mish**: Non-monotonic, smooth, with small negative values\n",
    "3. **Tanh**: Derivatives saturate to 0 for large |x| (vanishing gradients)\n",
    "\n",
    "Swish and Mish avoid dead neurons while maintaining smooth gradients!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Using Hooks for Introspection\n",
    "\n",
    "Hooks let you inspect or modify:\n",
    "- **Forward hooks**: Activations during forward pass\n",
    "- **Backward hooks**: Gradients during backward pass\n",
    "\n",
    "This is incredibly useful for debugging and understanding model behavior!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    \"\"\"Simple network for hook demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for activations and gradients\n",
    "activations = {}\n",
    "gradients = {}\n",
    "\n",
    "def get_activation_hook(name):\n",
    "    \"\"\"Create a forward hook that saves activations.\"\"\"\n",
    "    def hook(module, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "def get_gradient_hook(name):\n",
    "    \"\"\"Create a backward hook that saves gradients.\"\"\"\n",
    "    def hook(module, grad_input, grad_output):\n",
    "        gradients[name] = grad_output[0].detach()\n",
    "    return hook\n",
    "\n",
    "# Register hooks\n",
    "handles = []\n",
    "for name, layer in model.named_modules():\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        handles.append(layer.register_forward_hook(get_activation_hook(name)))\n",
    "        handles.append(layer.register_full_backward_hook(get_gradient_hook(name)))\n",
    "\n",
    "# Run forward and backward\n",
    "x = torch.randn(1, 10)\n",
    "y = model(x)\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(\"=== Activations ===\")\n",
    "for name, act in activations.items():\n",
    "    print(f\"{name}: shape={act.shape}, mean={act.mean():.4f}, std={act.std():.4f}\")\n",
    "\n",
    "print(\"\\n=== Gradients ===\")\n",
    "for name, grad in gradients.items():\n",
    "    print(f\"{name}: shape={grad.shape}, mean={grad.mean():.4f}, std={grad.std():.4f}\")\n",
    "\n",
    "# Clean up hooks\n",
    "for h in handles:\n",
    "    h.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Application: Gradient Clipping Hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_clip_hook(max_norm: float):\n",
    "    \"\"\"\n",
    "    Create a hook that clips gradients during backward pass.\n",
    "    \n",
    "    This can help prevent exploding gradients!\n",
    "    \"\"\"\n",
    "    def hook(module, grad_input, grad_output):\n",
    "        # Clip each gradient tensor\n",
    "        clipped_grads = []\n",
    "        for grad in grad_input:\n",
    "            if grad is not None:\n",
    "                norm = grad.norm()\n",
    "                if norm > max_norm:\n",
    "                    grad = grad * max_norm / norm\n",
    "            clipped_grads.append(grad)\n",
    "        return tuple(clipped_grads)\n",
    "    return hook\n",
    "\n",
    "# Apply to a layer\n",
    "model2 = SimpleNet()\n",
    "handle = model2.fc1.register_full_backward_hook(gradient_clip_hook(1.0))\n",
    "\n",
    "# Test\n",
    "x = torch.randn(1, 10)\n",
    "y = model2(x)\n",
    "(y * 1000).sum().backward()  # Large gradient\n",
    "\n",
    "print(f\"fc1 input grad norm: {model2.fc1.weight.grad.norm():.4f}\")\n",
    "\n",
    "handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Benchmarking Custom vs Built-in\n",
    "\n",
    "Let's compare our custom implementations against PyTorch's built-in versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_activation(activation_fn, name, input_size=(1000, 1000), num_runs=100):\n",
    "    \"\"\"\n",
    "    Benchmark an activation function.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (forward_time_ms, backward_time_ms)\n",
    "    \"\"\"\n",
    "    x = torch.randn(*input_size, device=device, requires_grad=True)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        y = activation_fn(x)\n",
    "        y.sum().backward()\n",
    "        x.grad = None\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark forward\n",
    "    start = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        y = activation_fn(x)\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    forward_time = (time.time() - start) / num_runs * 1000\n",
    "    \n",
    "    # Benchmark backward\n",
    "    start = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        y = activation_fn(x)\n",
    "        y.sum().backward()\n",
    "        x.grad = None\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    backward_time = (time.time() - start) / num_runs * 1000 - forward_time\n",
    "    \n",
    "    return forward_time, backward_time\n",
    "\n",
    "\n",
    "# Benchmark different activations\n",
    "activations_to_benchmark = [\n",
    "    ('ReLU (built-in)', F.relu),\n",
    "    ('Swish (custom)', swish_custom),\n",
    "    ('Swish (built-in F.silu)', F.silu),\n",
    "    ('Mish (custom)', mish_custom),\n",
    "    ('Mish (built-in)', F.mish),\n",
    "    ('GELU (built-in)', F.gelu),\n",
    "]\n",
    "\n",
    "print(f\"Benchmarking on {device}...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, fn in activations_to_benchmark:\n",
    "    try:\n",
    "        fwd, bwd = benchmark_activation(fn, name)\n",
    "        print(f\"{name:25s} | Forward: {fwd:.3f}ms | Backward: {bwd:.3f}ms\")\n",
    "    except Exception as e:\n",
    "        print(f\"{name:25s} | Error: {e}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "- Built-in functions are typically faster due to optimized C++/CUDA kernels\n",
    "- Our custom implementations are mathematically correct but slightly slower\n",
    "- For production, prefer built-in functions when available\n",
    "- Custom functions are great for prototyping new ideas!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úã Try It Yourself: Exercise\n",
    "\n",
    "Implement a custom autograd function for **Hard Swish**:\n",
    "\n",
    "$$\\text{HardSwish}(x) = x \\cdot \\frac{\\text{ReLU6}(x + 3)}{6}$$\n",
    "\n",
    "Where ReLU6(x) = min(max(0, x), 6)\n",
    "\n",
    "This is a computationally cheaper approximation of Swish!\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "The derivative is piecewise:\n",
    "- For x < -3: 0\n",
    "- For -3 <= x <= 3: (2x + 3) / 6\n",
    "- For x > 3: 1\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Implement HardSwishFunction\n",
    "class HardSwishFunction(Function):\n",
    "    \"\"\"\n",
    "    Custom autograd function for Hard Swish activation.\n",
    "    \n",
    "    HardSwish(x) = x * ReLU6(x + 3) / 6\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: Implement forward pass\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: Implement backward pass\n",
    "        pass\n",
    "\n",
    "# Test your implementation\n",
    "# x_test = torch.randn(3, 4, dtype=torch.float64, requires_grad=True)\n",
    "# test_passed = gradcheck(HardSwishFunction.apply, (x_test,), eps=1e-6)\n",
    "# print(f\"HardSwish gradient check passed: {test_passed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Not using `ctx.save_for_backward`\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - storing tensors as attributes\n",
    "class BadFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.x = x  # This can cause memory issues!\n",
    "        return x * 2\n",
    "\n",
    "# ‚úÖ Right - use save_for_backward\n",
    "class GoodFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)  # Proper memory management\n",
    "        return x * 2\n",
    "```\n",
    "\n",
    "### Mistake 2: Modifying tensors in-place\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - in-place modification breaks autograd\n",
    "@staticmethod\n",
    "def forward(ctx, x):\n",
    "    x.mul_(2)  # In-place!\n",
    "    return x\n",
    "\n",
    "# ‚úÖ Right - create new tensor\n",
    "@staticmethod\n",
    "def forward(ctx, x):\n",
    "    return x * 2  # New tensor\n",
    "```\n",
    "\n",
    "### Mistake 3: Forgetting to handle multiple inputs\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - not returning gradient for each input\n",
    "@staticmethod\n",
    "def backward(ctx, grad_output):\n",
    "    return grad_x  # Only one gradient, but forward had 2 inputs!\n",
    "\n",
    "# ‚úÖ Right - return gradient for each input\n",
    "@staticmethod\n",
    "def backward(ctx, grad_output):\n",
    "    return grad_x, grad_y  # One per input\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How PyTorch's autograd builds computational graphs\n",
    "- ‚úÖ Creating custom `torch.autograd.Function` classes\n",
    "- ‚úÖ Implementing Swish and Mish activations from scratch\n",
    "- ‚úÖ Verifying gradients with `gradcheck`\n",
    "- ‚úÖ Using hooks for model introspection\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "Implement **Gradient Checkpointing** as a custom autograd function!\n",
    "\n",
    "Gradient checkpointing trades compute for memory by not saving intermediate activations. During backward, it recomputes the forward pass to get the activations.\n",
    "\n",
    "This is how large models like GPT-3 fit in GPU memory!\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [PyTorch Autograd Mechanics](https://pytorch.org/docs/stable/notes/autograd.html)\n",
    "- [Extending PyTorch](https://pytorch.org/docs/stable/notes/extending.html)\n",
    "- [Swish Paper](https://arxiv.org/abs/1710.05941)\n",
    "- [Mish Paper](https://arxiv.org/abs/1908.08681)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB allocated\")\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
