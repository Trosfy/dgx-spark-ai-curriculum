{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6.5: Profiling Workshop - Find and Fix Bottlenecks\n",
    "\n",
    "**Module:** 6 - Deep Learning with PyTorch  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** â­â­â­ (Intermediate-Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Use PyTorch Profiler to analyze training performance\n",
    "- [ ] Generate and interpret Chrome trace files\n",
    "- [ ] Identify CPU, GPU, and data loading bottlenecks\n",
    "- [ ] Apply optimizations based on profiling results\n",
    "- [ ] Monitor memory usage during training\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Tasks 6.1-6.4\n",
    "- Knowledge of: Training loops, GPU operations\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "Training neural networks is expensive! Profiling helps you:\n",
    "- **Find bottlenecks**: Is it CPU, GPU, or I/O bound?\n",
    "- **Optimize resource usage**: Use all your hardware effectively\n",
    "- **Reduce training cost**: Faster training = less compute cost\n",
    "- **Debug performance issues**: Why did training suddenly slow down?\n",
    "\n",
    "At scale, even 10% speedup can save thousands of dollars!\n",
    "\n",
    "---\n",
    "\n",
    "## ELI5: What is Profiling?\n",
    "\n",
    "> **Imagine you're watching a relay race...** ðŸƒ\n",
    ">\n",
    "> Each runner (operation) takes time. The race (training) is only as fast as the slowest runner.\n",
    ">\n",
    "> **Profiling is like using a stopwatch on each runner:**\n",
    "> - \"The handoff is taking too long!\" (data loading)\n",
    "> - \"Runner 3 is really slow!\" (GPU bottleneck)\n",
    "> - \"Everyone's waiting for Runner 2!\" (CPU-GPU sync)\n",
    ">\n",
    "> Once you know who's slow, you can coach them to be faster!\n",
    ">\n",
    "> **In AI terms:** Profiling measures exactly how long each operation takes, showing you where to focus optimization efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.profiler import profile, ProfilerActivity, tensorboard_trace_handler, schedule\n",
    "\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model with various layer types for profiling\n",
    "class ProfilingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A model designed to demonstrate different operation types in profiling.\n",
    "    \n",
    "    Includes convolutions, batch norms, linear layers, and attention-like operations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int = 10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Convolutional layers (GPU-heavy)\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        \n",
    "        # Linear layers\n",
    "        self.fc_block = nn.Sequential(\n",
    "            nn.Linear(256 * 8 * 8, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_block(x)\n",
    "        return x\n",
    "\n",
    "model = ProfilingModel(10).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Create data loaders with different configurations\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Suboptimal loader (for demonstration)\n",
    "slow_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "# Optimized loader\n",
    "fast_loader = DataLoader(\n",
    "    trainset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "    num_workers=4, pin_memory=True, persistent_workers=True\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(trainset):,}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Basic Profiling\n",
    "\n",
    "Let's start with simple profiling to understand the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple timing profile\n",
    "def profile_training_loop(model, dataloader, num_batches=50):\n",
    "    \"\"\"\n",
    "    Profile a training loop and return timing breakdown.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    \n",
    "    timings = {\n",
    "        'data_loading': [],\n",
    "        'forward': [],\n",
    "        'backward': [],\n",
    "        'optimizer': [],\n",
    "    }\n",
    "    \n",
    "    data_iter = iter(dataloader)\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        # Data loading\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            inputs, labels = next(data_iter)\n",
    "        except StopIteration:\n",
    "            data_iter = iter(dataloader)\n",
    "            inputs, labels = next(data_iter)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        timings['data_loading'].append(time.time() - t0)\n",
    "        \n",
    "        # Forward\n",
    "        t0 = time.time()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        timings['forward'].append(time.time() - t0)\n",
    "        \n",
    "        # Backward\n",
    "        t0 = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        timings['backward'].append(time.time() - t0)\n",
    "        \n",
    "        # Optimizer step\n",
    "        t0 = time.time()\n",
    "        optimizer.step()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        timings['optimizer'].append(time.time() - t0)\n",
    "    \n",
    "    # Calculate averages (skip warmup)\n",
    "    warmup = 5\n",
    "    return {k: sum(v[warmup:]) / len(v[warmup:]) * 1000 for k, v in timings.items()}\n",
    "\n",
    "# Profile with slow loader\n",
    "print(\"=== Profiling with num_workers=0 ===\")\n",
    "slow_timings = profile_training_loop(model, slow_loader)\n",
    "for name, time_ms in slow_timings.items():\n",
    "    print(f\"  {name:15s}: {time_ms:.2f} ms\")\n",
    "print(f\"  {'TOTAL':15s}: {sum(slow_timings.values()):.2f} ms\")\n",
    "\n",
    "# Profile with fast loader\n",
    "print(\"\\n=== Profiling with num_workers=4 ===\")\n",
    "fast_timings = profile_training_loop(model, fast_loader)\n",
    "for name, time_ms in fast_timings.items():\n",
    "    print(f\"  {name:15s}: {time_ms:.2f} ms\")\n",
    "print(f\"  {'TOTAL':15s}: {sum(fast_timings.values()):.2f} ms\")\n",
    "\n",
    "# Speedup\n",
    "speedup = sum(slow_timings.values()) / sum(fast_timings.values())\n",
    "print(f\"\\nSpeedup from optimized data loading: {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: PyTorch Profiler Deep Dive\n",
    "\n",
    "The PyTorch Profiler provides detailed information about:\n",
    "- CPU and GPU operations\n",
    "- Memory allocation\n",
    "- Kernel launches\n",
    "- Data transfer between CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, inputs, labels, criterion, optimizer):\n",
    "    \"\"\"Single training step for profiling.\"\"\"\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Setup\n",
    "model = ProfilingModel(10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Profile with PyTorch Profiler\n",
    "print(\"Running PyTorch Profiler...\")\n",
    "\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True,\n",
    ") as prof:\n",
    "    # Run a few training steps\n",
    "    data_iter = iter(fast_loader)\n",
    "    for i in range(20):\n",
    "        inputs, labels = next(data_iter)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        training_step(model, inputs, labels, criterion, optimizer)\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\n=== Top Operations by CUDA Time ===\")\n",
    "print(prof.key_averages().table(\n",
    "    sort_by=\"cuda_time_total\", \n",
    "    row_limit=15\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary by operation type\n",
    "print(\"\\n=== Top Operations by CPU Time ===\")\n",
    "print(prof.key_averages().table(\n",
    "    sort_by=\"cpu_time_total\", \n",
    "    row_limit=15\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory profiling\n",
    "print(\"\\n=== Operations with Memory Allocation ===\")\n",
    "print(prof.key_averages().table(\n",
    "    sort_by=\"self_cuda_memory_usage\", \n",
    "    row_limit=10\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Generating Chrome Trace\n",
    "\n",
    "Chrome traces provide a visual timeline of operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for traces\n",
    "trace_dir = Path('./profiler_traces')\n",
    "trace_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Profile with trace export\n",
    "print(\"Generating Chrome trace...\")\n",
    "\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True,\n",
    ") as prof:\n",
    "    data_iter = iter(fast_loader)\n",
    "    for i in range(20):\n",
    "        inputs, labels = next(data_iter)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        training_step(model, inputs, labels, criterion, optimizer)\n",
    "\n",
    "# Export Chrome trace\n",
    "trace_path = trace_dir / 'training_trace.json'\n",
    "prof.export_chrome_trace(str(trace_path))\n",
    "print(f\"Chrome trace saved to: {trace_path}\")\n",
    "print(\"\\nTo view the trace:\")\n",
    "print(\"1. Open Chrome browser\")\n",
    "print(\"2. Go to chrome://tracing\")\n",
    "print(\"3. Click 'Load' and select the JSON file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Using Profiler Schedule\n",
    "\n",
    "For long training runs, use a schedule to profile specific steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profiler with schedule\n",
    "def trace_handler(p):\n",
    "    \"\"\"Custom trace handler to save traces.\"\"\"\n",
    "    output = p.key_averages().table(sort_by=\"cuda_time_total\", row_limit=5)\n",
    "    print(f\"\\n=== Step {p.step_num} ===\")\n",
    "    print(output)\n",
    "\n",
    "# Use schedule: wait 2 steps, warmup 2 steps, active 3 steps, repeat\n",
    "prof_schedule = schedule(\n",
    "    wait=2,      # Don't profile first 2 steps\n",
    "    warmup=2,    # Profile but don't record next 2 steps\n",
    "    active=3,    # Actually record next 3 steps\n",
    "    repeat=2,    # Repeat this cycle 2 times\n",
    ")\n",
    "\n",
    "print(\"Running scheduled profiling...\")\n",
    "\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    schedule=prof_schedule,\n",
    "    on_trace_ready=trace_handler,\n",
    "    record_shapes=True,\n",
    "    with_stack=True,\n",
    ") as prof:\n",
    "    data_iter = iter(fast_loader)\n",
    "    for i in range(20):\n",
    "        inputs, labels = next(data_iter)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        training_step(model, inputs, labels, criterion, optimizer)\n",
    "        prof.step()  # Important: signal end of step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Memory Profiling\n",
    "\n",
    "Understanding memory usage is crucial, especially on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_memory_stats():\n",
    "    \"\"\"Print current GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        max_allocated = torch.cuda.max_memory_allocated() / 1e9\n",
    "        print(f\"Allocated: {allocated:.2f} GB | Reserved: {reserved:.2f} GB | Peak: {max_allocated:.2f} GB\")\n",
    "\n",
    "# Reset memory stats\n",
    "torch.cuda.reset_peak_memory_stats() if torch.cuda.is_available() else None\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "print(\"=== Memory Usage During Training ===\")\n",
    "\n",
    "# Create fresh model\n",
    "model = ProfilingModel(10).to(device)\n",
    "print(\"After model creation:\")\n",
    "print_memory_stats()\n",
    "\n",
    "# Forward pass\n",
    "inputs = torch.randn(BATCH_SIZE, 3, 32, 32, device=device)\n",
    "labels = torch.randint(0, 10, (BATCH_SIZE,), device=device)\n",
    "outputs = model(inputs)\n",
    "print(\"\\nAfter forward pass:\")\n",
    "print_memory_stats()\n",
    "\n",
    "# Loss computation\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(outputs, labels)\n",
    "print(\"\\nAfter loss computation:\")\n",
    "print_memory_stats()\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "print(\"\\nAfter backward pass:\")\n",
    "print_memory_stats()\n",
    "\n",
    "# Cleanup\n",
    "del outputs, loss\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nAfter cleanup:\")\n",
    "print_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile memory allocation per layer\n",
    "def profile_model_memory(model, input_shape):\n",
    "    \"\"\"\n",
    "    Profile memory usage for each part of the model.\n",
    "    \"\"\"\n",
    "    memory_by_layer = {}\n",
    "    \n",
    "    def hook_fn(name):\n",
    "        def hook(module, input, output):\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "                memory_by_layer[name] = {\n",
    "                    'output_size': output.element_size() * output.nelement() / 1e6,  # MB\n",
    "                    'output_shape': tuple(output.shape),\n",
    "                }\n",
    "        return hook\n",
    "    \n",
    "    # Register hooks\n",
    "    hooks = []\n",
    "    for name, module in model.named_modules():\n",
    "        if len(list(module.children())) == 0:  # Leaf modules only\n",
    "            hooks.append(module.register_forward_hook(hook_fn(name)))\n",
    "    \n",
    "    # Run forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = torch.randn(*input_shape, device=device)\n",
    "        _ = model(x)\n",
    "    \n",
    "    # Remove hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "    \n",
    "    return memory_by_layer\n",
    "\n",
    "# Profile our model\n",
    "memory_profile = profile_model_memory(model, (1, 3, 32, 32))\n",
    "\n",
    "print(\"\\n=== Memory per Layer (Single Sample) ===\")\n",
    "total_mb = 0\n",
    "for name, info in sorted(memory_profile.items()):\n",
    "    print(f\"{name:40s} | Shape: {str(info['output_shape']):20s} | Size: {info['output_size']:.2f} MB\")\n",
    "    total_mb += info['output_size']\n",
    "print(f\"{'TOTAL':40s} | {'':20s} | Size: {total_mb:.2f} MB\")\n",
    "print(f\"\\nFor batch_size={BATCH_SIZE}: ~{total_mb * BATCH_SIZE:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Common Bottlenecks and Solutions\n",
    "\n",
    "Let's identify and fix common performance issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Bottleneck 1: Data loading on CPU thread\nprint(\"=== Bottleneck 1: Data Loading ===\")\n\ndef benchmark_data_loading(num_workers_list=[0, 2, 4, 8]):\n    results = {}\n    \n    for num_workers in num_workers_list:\n        # Clean up memory between benchmarks\n        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n        import gc; gc.collect()\n        \n        loader = DataLoader(\n            trainset, batch_size=BATCH_SIZE, shuffle=True,\n            num_workers=num_workers, pin_memory=True if num_workers > 0 else False\n        )\n        \n        # Time data loading\n        start = time.time()\n        for i, (inputs, labels) in enumerate(loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            if i >= 50:\n                break\n        \n        # Sync before measuring time\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        elapsed = time.time() - start\n        \n        results[num_workers] = elapsed\n        print(f\"num_workers={num_workers}: {elapsed:.2f}s for 50 batches\")\n    \n    return results\n\ndata_loading_results = benchmark_data_loading([0, 2, 4])\nbest_workers = min(data_loading_results, key=data_loading_results.get)\nprint(f\"\\nOptimal num_workers: {best_workers}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Bottleneck 2: Unnecessary CPU-GPU synchronization\nprint(\"\\n=== Bottleneck 2: CPU-GPU Synchronization ===\")\n\n# Bad: Printing loss every iteration (forces sync)\ndef train_with_frequent_sync(model, loader, num_batches=50):\n    model.train()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.01)\n    \n    start = time.time()\n    data_iter = iter(loader)\n    for i in range(num_batches):\n        inputs, labels = next(data_iter)\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # BAD: This forces CPU-GPU sync every iteration!\n        _ = loss.item()\n    \n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    return time.time() - start\n\n# Good: Accumulate loss, sync less often\ndef train_with_reduced_sync(model, loader, num_batches=50, log_interval=10):\n    model.train()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.01)\n    \n    start = time.time()\n    running_loss = 0.0\n    data_iter = iter(loader)\n    \n    for i in range(num_batches):\n        inputs, labels = next(data_iter)\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss  # Keep as tensor (no sync)\n        \n        # Only sync occasionally\n        if (i + 1) % log_interval == 0:\n            avg_loss = running_loss.item() / log_interval\n            running_loss = 0.0\n    \n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    return time.time() - start\n\n# Clean up before benchmarks\nimport gc\ntorch.cuda.empty_cache() if torch.cuda.is_available() else None\ngc.collect()\n\n# Benchmark\nmodel_copy = ProfilingModel(10).to(device)\ntime_frequent = train_with_frequent_sync(model_copy, fast_loader)\n\n# Clean up between benchmarks\ndel model_copy\ntorch.cuda.empty_cache() if torch.cuda.is_available() else None\ngc.collect()\n\nmodel_copy = ProfilingModel(10).to(device)  # Fresh model\ntime_reduced = train_with_reduced_sync(model_copy, fast_loader)\n\n# Clean up\ndel model_copy\ntorch.cuda.empty_cache() if torch.cuda.is_available() else None\ngc.collect()\n\nprint(f\"Frequent sync: {time_frequent:.2f}s\")\nprint(f\"Reduced sync: {time_reduced:.2f}s\")\nprint(f\"Speedup: {time_frequent/time_reduced:.2f}x\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ‹ Try It Yourself: Exercise\n",
    "\n",
    "Profile the following inefficient training loop and identify all bottlenecks:\n",
    "\n",
    "1. Profile with PyTorch Profiler\n",
    "2. Identify at least 3 performance issues\n",
    "3. Fix each issue and measure the improvement\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ’¡ Hint</summary>\n",
    "\n",
    "Look for:\n",
    "- Data loading issues (num_workers)\n",
    "- Unnecessary .item() calls\n",
    "- Missing pin_memory\n",
    "- Inefficient optimizer.zero_grad()\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inefficient training loop - Find and fix the bottlenecks!\n",
    "def inefficient_training(model, trainset, epochs=1):\n",
    "    \"\"\"This function has multiple performance issues. Can you find them?\"\"\"\n",
    "    \n",
    "    # Issue 1: ?\n",
    "    loader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i, (inputs, labels) in enumerate(loader):\n",
    "            # Issue 2: ?\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Issue 3: ?\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Issue 4: ?\n",
    "            if i % 1 == 0:\n",
    "                print(f\"Loss: {loss.item():.4f}\", end='\\r')\n",
    "            \n",
    "            if i >= 100:\n",
    "                break\n",
    "\n",
    "# YOUR CODE HERE: Profile and fix the issues\n",
    "# Then create efficient_training() with all fixes applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- âœ… Using PyTorch Profiler for detailed analysis\n",
    "- âœ… Generating and reading Chrome traces\n",
    "- âœ… Identifying CPU, GPU, and data loading bottlenecks\n",
    "- âœ… Memory profiling techniques\n",
    "- âœ… Common optimizations (num_workers, pin_memory, reduced sync)\n",
    "\n",
    "---\n",
    "\n",
    "## DGX Spark Tips\n",
    "\n",
    "With 128GB unified memory:\n",
    "- Increase `num_workers` (try 4-8)\n",
    "- Use larger `batch_size` \n",
    "- Enable `pin_memory=True`\n",
    "- Use `persistent_workers=True` for faster epoch starts\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [PyTorch Profiler Tutorial](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)\n",
    "- [CUDA Best Practices](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)\n",
    "- [TensorBoard Profiler Plugin](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cleanup\nimport gc\nimport shutil\n\n# Set to False if you want to keep trace files for inspection\nDELETE_TRACES = True\n\n# Remove trace files (optional - set DELETE_TRACES=False to keep them)\nif DELETE_TRACES and trace_dir.exists():\n    shutil.rmtree(trace_dir)\n    print(f\"Removed trace directory: {trace_dir}\")\nelse:\n    print(f\"Trace files preserved at: {trace_dir}\")\n    print(\"To view traces: Open Chrome, go to chrome://tracing, and load the JSON file\")\n\ntorch.cuda.empty_cache()\ngc.collect()\n\nprint(\"Cleanup complete!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}