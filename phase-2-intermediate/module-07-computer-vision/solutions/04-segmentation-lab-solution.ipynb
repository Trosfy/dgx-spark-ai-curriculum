{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 7.4 Solution: Segmentation Lab\n",
        "\n",
        "**Module:** 7 - Computer Vision  \n",
        "**Type:** Solution Notebook\n",
        "\n",
        "---\n",
        "\n",
        "This notebook contains solutions for semantic segmentation exercises using U-Net architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple, List, Optional\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise Solution: Larger U-Net Architecture\n",
        "\n",
        "This solution implements a larger U-Net with doubled channel dimensions for improved segmentation performance.\n",
        "\n",
        "**Changes from standard U-Net:**\n",
        "- Original: 64 → 128 → 256 → 512 → 1024\n",
        "- This version: 128 → 256 → 512 → 1024 → 2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Double convolution block: (Conv2d -> BN -> ReLU) x 2\n",
        "    \n",
        "    This is the basic building block of U-Net.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels: int, out_channels: int, mid_channels: Optional[int] = None):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        \n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "print(\"DoubleConv block defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with MaxPool then double conv.\"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        super(Down, self).__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv.\"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels: int, out_channels: int, bilinear: bool = True):\n",
        "        super(Up, self).__init__()\n",
        "        \n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "    \n",
        "    def forward(self, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:\n",
        "        x1 = self.up(x1)\n",
        "        \n",
        "        # Handle size mismatch due to odd dimensions\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "        \n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                       diffY // 2, diffY - diffY // 2])\n",
        "        \n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "print(\"Down and Up blocks defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LargerUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Larger U-Net with more channels.\n",
        "    \n",
        "    Original U-Net: 64 -> 128 -> 256 -> 512 -> 1024\n",
        "    This version:   128 -> 256 -> 512 -> 1024 -> 2048\n",
        "    \n",
        "    Benefits:\n",
        "    - More capacity for complex segmentation tasks\n",
        "    - Better feature representation\n",
        "    - Suitable for high-resolution images\n",
        "    \n",
        "    Trade-offs:\n",
        "    - More parameters (~4x standard U-Net)\n",
        "    - Higher memory usage\n",
        "    - Longer training time\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_channels: int = 3, n_classes: int = 21, bilinear: bool = False):\n",
        "        super(LargerUNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "        \n",
        "        # Encoder path (doubled channels)\n",
        "        self.inc = DoubleConv(n_channels, 128)\n",
        "        self.down1 = Down(128, 256)\n",
        "        self.down2 = Down(256, 512)\n",
        "        self.down3 = Down(512, 1024)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(1024, 2048 // factor)\n",
        "        \n",
        "        # Decoder path\n",
        "        self.up1 = Up(2048, 1024 // factor, bilinear)\n",
        "        self.up2 = Up(1024, 512 // factor, bilinear)\n",
        "        self.up3 = Up(512, 256 // factor, bilinear)\n",
        "        self.up4 = Up(256, 128, bilinear)\n",
        "        \n",
        "        # Output\n",
        "        self.outc = nn.Conv2d(128, n_classes, kernel_size=1)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Encoder\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        \n",
        "        # Decoder with skip connections\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        \n",
        "        logits = self.outc(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Test the model\n",
        "large_unet = LargerUNet(n_channels=3, n_classes=21)\n",
        "x = torch.randn(1, 3, 256, 256)\n",
        "output = large_unet(x)\n",
        "\n",
        "print(f\"Larger U-Net Architecture\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Parameters: {sum(p.numel() for p in large_unet.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise Solution: Attention U-Net\n",
        "\n",
        "U-Net with attention gates that focus on relevant features during upsampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttentionGate(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention Gate for skip connections.\n",
        "    \n",
        "    Learns to focus on relevant spatial features from the encoder\n",
        "    based on the decoder's context.\n",
        "    \n",
        "    Paper: \"Attention U-Net: Learning Where to Look\" (Oktay et al., 2018)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, gate_channels: int, skip_channels: int, inter_channels: int):\n",
        "        super(AttentionGate, self).__init__()\n",
        "        \n",
        "        # Transform gate signal (from decoder)\n",
        "        self.W_g = nn.Sequential(\n",
        "            nn.Conv2d(gate_channels, inter_channels, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(inter_channels)\n",
        "        )\n",
        "        \n",
        "        # Transform skip connection (from encoder)\n",
        "        self.W_x = nn.Sequential(\n",
        "            nn.Conv2d(skip_channels, inter_channels, kernel_size=1, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(inter_channels)\n",
        "        )\n",
        "        \n",
        "        # Compute attention coefficients\n",
        "        self.psi = nn.Sequential(\n",
        "            nn.Conv2d(inter_channels, 1, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "    \n",
        "    def forward(self, g: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            g: Gate signal from decoder (coarse, low-res)\n",
        "            x: Skip connection from encoder (fine, high-res)\n",
        "        \n",
        "        Returns:\n",
        "            Attended skip connection features\n",
        "        \"\"\"\n",
        "        # Resize gate signal to match skip connection\n",
        "        g1 = F.interpolate(g, size=x.shape[2:], mode='bilinear', align_corners=True)\n",
        "        g1 = self.W_g(g1)\n",
        "        \n",
        "        x1 = self.W_x(x)\n",
        "        \n",
        "        # Combine and compute attention\n",
        "        psi = self.relu(g1 + x1)\n",
        "        psi = self.psi(psi)\n",
        "        \n",
        "        return x * psi  # Apply attention to skip features\n",
        "\n",
        "\n",
        "print(\"AttentionGate defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttentionUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    U-Net with Attention Gates.\n",
        "    \n",
        "    Attention gates help the model focus on relevant features\n",
        "    during the decoding phase.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_channels: int = 3, n_classes: int = 21):\n",
        "        super(AttentionUNet, self).__init__()\n",
        "        \n",
        "        # Encoder\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 1024)\n",
        "        \n",
        "        # Attention gates\n",
        "        self.att4 = AttentionGate(gate_channels=1024, skip_channels=512, inter_channels=256)\n",
        "        self.att3 = AttentionGate(gate_channels=512, skip_channels=256, inter_channels=128)\n",
        "        self.att2 = AttentionGate(gate_channels=256, skip_channels=128, inter_channels=64)\n",
        "        self.att1 = AttentionGate(gate_channels=128, skip_channels=64, inter_channels=32)\n",
        "        \n",
        "        # Decoder\n",
        "        self.up4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.conv_up4 = DoubleConv(1024, 512)\n",
        "        \n",
        "        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.conv_up3 = DoubleConv(512, 256)\n",
        "        \n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.conv_up2 = DoubleConv(256, 128)\n",
        "        \n",
        "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.conv_up1 = DoubleConv(128, 64)\n",
        "        \n",
        "        # Output\n",
        "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Encoder\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        \n",
        "        # Decoder with attention\n",
        "        d4 = self.up4(x5)\n",
        "        x4 = self.att4(d4, x4)  # Attend to encoder features\n",
        "        d4 = torch.cat([x4, d4], dim=1)\n",
        "        d4 = self.conv_up4(d4)\n",
        "        \n",
        "        d3 = self.up3(d4)\n",
        "        x3 = self.att3(d3, x3)\n",
        "        d3 = torch.cat([x3, d3], dim=1)\n",
        "        d3 = self.conv_up3(d3)\n",
        "        \n",
        "        d2 = self.up2(d3)\n",
        "        x2 = self.att2(d2, x2)\n",
        "        d2 = torch.cat([x2, d2], dim=1)\n",
        "        d2 = self.conv_up2(d2)\n",
        "        \n",
        "        d1 = self.up1(d2)\n",
        "        x1 = self.att1(d1, x1)\n",
        "        d1 = torch.cat([x1, d1], dim=1)\n",
        "        d1 = self.conv_up1(d1)\n",
        "        \n",
        "        return self.outc(d1)\n",
        "\n",
        "\n",
        "# Test Attention U-Net\n",
        "att_unet = AttentionUNet(n_channels=3, n_classes=21)\n",
        "x = torch.randn(1, 3, 256, 256)\n",
        "output = att_unet(x)\n",
        "\n",
        "print(f\"Attention U-Net Architecture\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Parameters: {sum(p.numel() for p in att_unet.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise Solution: Segmentation Metrics\n",
        "\n",
        "Implementation of common segmentation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dice_coefficient(pred: torch.Tensor, target: torch.Tensor, smooth: float = 1e-6) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Calculate Dice coefficient.\n",
        "    \n",
        "    Dice = 2 * |A ∩ B| / (|A| + |B|)\n",
        "    \n",
        "    Args:\n",
        "        pred: Predicted mask (binary or probabilities)\n",
        "        target: Ground truth mask (binary)\n",
        "        smooth: Smoothing factor to avoid division by zero\n",
        "    \n",
        "    Returns:\n",
        "        Dice coefficient (0 to 1, higher is better)\n",
        "    \"\"\"\n",
        "    pred_flat = pred.view(-1)\n",
        "    target_flat = target.view(-1)\n",
        "    \n",
        "    intersection = (pred_flat * target_flat).sum()\n",
        "    \n",
        "    dice = (2. * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth)\n",
        "    return dice\n",
        "\n",
        "\n",
        "def iou_score(pred: torch.Tensor, target: torch.Tensor, smooth: float = 1e-6) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Calculate Intersection over Union (IoU / Jaccard Index).\n",
        "    \n",
        "    IoU = |A ∩ B| / |A ∪ B|\n",
        "    \n",
        "    Args:\n",
        "        pred: Predicted mask\n",
        "        target: Ground truth mask\n",
        "        smooth: Smoothing factor\n",
        "    \n",
        "    Returns:\n",
        "        IoU score (0 to 1, higher is better)\n",
        "    \"\"\"\n",
        "    pred_flat = pred.view(-1)\n",
        "    target_flat = target.view(-1)\n",
        "    \n",
        "    intersection = (pred_flat * target_flat).sum()\n",
        "    union = pred_flat.sum() + target_flat.sum() - intersection\n",
        "    \n",
        "    iou = (intersection + smooth) / (union + smooth)\n",
        "    return iou\n",
        "\n",
        "\n",
        "def pixel_accuracy(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Calculate pixel-wise accuracy.\n",
        "    \n",
        "    Args:\n",
        "        pred: Predicted class indices [B, H, W]\n",
        "        target: Ground truth class indices [B, H, W]\n",
        "    \n",
        "    Returns:\n",
        "        Pixel accuracy (0 to 1)\n",
        "    \"\"\"\n",
        "    correct = (pred == target).float().sum()\n",
        "    total = target.numel()\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "# Demonstration\n",
        "print(\"Segmentation Metrics Demo\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create sample predictions and targets\n",
        "pred = torch.zeros(1, 256, 256)\n",
        "pred[:, 50:200, 50:200] = 1  # Predicted region\n",
        "\n",
        "target = torch.zeros(1, 256, 256)\n",
        "target[:, 60:190, 60:190] = 1  # Ground truth region\n",
        "\n",
        "dice = dice_coefficient(pred, target)\n",
        "iou = iou_score(pred, target)\n",
        "acc = pixel_accuracy(pred.long(), target.long())\n",
        "\n",
        "print(f\"Dice Coefficient: {dice:.4f}\")\n",
        "print(f\"IoU Score: {iou:.4f}\")\n",
        "print(f\"Pixel Accuracy: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise Solution: Dice Loss\n",
        "\n",
        "Dice loss is often used for segmentation tasks, especially with imbalanced classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Dice Loss for segmentation.\n",
        "    \n",
        "    Dice Loss = 1 - Dice Coefficient\n",
        "    \n",
        "    Benefits:\n",
        "    - Works well with imbalanced classes\n",
        "    - Directly optimizes the evaluation metric\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, smooth: float = 1e-6):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "    \n",
        "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pred: Predictions [B, C, H, W] (logits or probabilities)\n",
        "            target: Ground truth [B, H, W] (class indices)\n",
        "        \"\"\"\n",
        "        num_classes = pred.shape[1]\n",
        "        \n",
        "        # Convert to probabilities\n",
        "        pred_soft = F.softmax(pred, dim=1)\n",
        "        \n",
        "        # One-hot encode target\n",
        "        target_one_hot = F.one_hot(target.long(), num_classes)  # [B, H, W, C]\n",
        "        target_one_hot = target_one_hot.permute(0, 3, 1, 2).float()  # [B, C, H, W]\n",
        "        \n",
        "        # Calculate dice for each class\n",
        "        dice_total = 0\n",
        "        for c in range(num_classes):\n",
        "            pred_c = pred_soft[:, c]\n",
        "            target_c = target_one_hot[:, c]\n",
        "            \n",
        "            intersection = (pred_c * target_c).sum()\n",
        "            dice_c = (2. * intersection + self.smooth) / (pred_c.sum() + target_c.sum() + self.smooth)\n",
        "            dice_total += dice_c\n",
        "        \n",
        "        dice_avg = dice_total / num_classes\n",
        "        return 1 - dice_avg\n",
        "\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Combined Cross-Entropy + Dice Loss.\n",
        "    \n",
        "    Balances pixel-wise accuracy (CE) with region overlap (Dice).\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, ce_weight: float = 0.5, dice_weight: float = 0.5):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.ce = nn.CrossEntropyLoss()\n",
        "        self.dice = DiceLoss()\n",
        "        self.ce_weight = ce_weight\n",
        "        self.dice_weight = dice_weight\n",
        "    \n",
        "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        ce_loss = self.ce(pred, target.long())\n",
        "        dice_loss = self.dice(pred, target)\n",
        "        return self.ce_weight * ce_loss + self.dice_weight * dice_loss\n",
        "\n",
        "\n",
        "# Test losses\n",
        "pred = torch.randn(2, 21, 64, 64)  # [B, C, H, W]\n",
        "target = torch.randint(0, 21, (2, 64, 64))  # [B, H, W]\n",
        "\n",
        "dice_loss = DiceLoss()\n",
        "combined_loss = CombinedLoss()\n",
        "\n",
        "print(f\"Dice Loss: {dice_loss(pred, target):.4f}\")\n",
        "print(f\"Combined Loss: {combined_loss(pred, target):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Key concepts covered:\n",
        "\n",
        "1. **Larger U-Net**: Doubled channel dimensions for more capacity\n",
        "2. **Attention U-Net**: Attention gates for better feature selection\n",
        "3. **Segmentation Metrics**: Dice coefficient, IoU, pixel accuracy\n",
        "4. **Dice Loss**: Region-based loss for imbalanced segmentation\n",
        "\n",
        "Best practices:\n",
        "- Use combined CE + Dice loss for stable training\n",
        "- Monitor both pixel accuracy and IoU metrics\n",
        "- Consider attention mechanisms for complex scenes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup\n",
        "import gc\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "print(\"Cleanup complete!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
