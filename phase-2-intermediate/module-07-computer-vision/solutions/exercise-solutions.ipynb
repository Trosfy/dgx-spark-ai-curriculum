{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 7: Computer Vision - Exercise Solutions\n",
    "\n",
    "This notebook contains solutions for all \"Try It Yourself\" exercises and challenges from Module 7.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Task 7.1 Solutions - CNN Architectures](#task-71)\n",
    "2. [Task 7.2 Solutions - Transfer Learning](#task-72)\n",
    "3. [Task 7.3 Solutions - Object Detection](#task-73)\n",
    "4. [Task 7.4 Solutions - Segmentation](#task-74)\n",
    "5. [Task 7.5 Solutions - Vision Transformer](#task-75)\n",
    "6. [Task 7.6 Solutions - SAM](#task-76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='task-71'></a>\n",
    "## Task 7.1 Solutions: CNN Architecture Study\n",
    "\n",
    "### Exercise 1: LeNet with MaxPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5_MaxPool(nn.Module):\n",
    "    \"\"\"\n",
    "    LeNet-5 with MaxPooling instead of AvgPooling.\n",
    "    \n",
    "    MaxPooling typically works better for object recognition because\n",
    "    it keeps the strongest activations (\"Was there an edge here?\")\n",
    "    rather than averaging (\"How much edge on average?\").\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int = 10):\n",
    "        super(LeNet5_MaxPool, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Changed from AvgPool\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Test\n",
    "model = LeNet5_MaxPool()\n",
    "x = torch.randn(1, 3, 32, 32)\n",
    "print(f\"LeNet5_MaxPool output shape: {model(x).shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Compare Gradient Flow in ResNet vs Plain Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainBlock(nn.Module):\n",
    "    \"\"\"Plain convolutional block without skip connection.\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(PlainBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        return F.relu(out)  # No skip connection!\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with skip connection.\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        return F.relu(out + x)  # Skip connection!\n",
    "\n",
    "\n",
    "def compare_gradient_flow(num_blocks: int = 10):\n",
    "    \"\"\"\n",
    "    Compare gradient magnitudes through stacked blocks.\n",
    "    \n",
    "    As depth increases, gradients in plain networks tend to vanish,\n",
    "    while residual networks maintain gradient flow.\n",
    "    \"\"\"\n",
    "    channels = 64\n",
    "    \n",
    "    # Stack blocks\n",
    "    plain_blocks = nn.Sequential(*[PlainBlock(channels) for _ in range(num_blocks)])\n",
    "    res_blocks = nn.Sequential(*[ResidualBlock(channels) for _ in range(num_blocks)])\n",
    "    \n",
    "    # Create input with gradient tracking\n",
    "    x_plain = torch.randn(1, channels, 32, 32, requires_grad=True)\n",
    "    x_res = torch.randn(1, channels, 32, 32, requires_grad=True)\n",
    "    \n",
    "    # Forward pass\n",
    "    out_plain = plain_blocks(x_plain)\n",
    "    out_res = res_blocks(x_res)\n",
    "    \n",
    "    # Backward pass\n",
    "    out_plain.sum().backward()\n",
    "    out_res.sum().backward()\n",
    "    \n",
    "    # Compare gradient magnitudes\n",
    "    grad_plain = x_plain.grad.abs().mean().item()\n",
    "    grad_res = x_res.grad.abs().mean().item()\n",
    "    \n",
    "    return grad_plain, grad_res\n",
    "\n",
    "# Compare at different depths\n",
    "print(\"Gradient Magnitude Comparison (Higher is Better)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Depth':<10} {'Plain Network':<20} {'ResNet':<20}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for depth in [5, 10, 20, 30]:\n",
    "    grad_plain, grad_res = compare_gradient_flow(depth)\n",
    "    print(f\"{depth:<10} {grad_plain:<20.6f} {grad_res:<20.6f}\")\n",
    "\n",
    "print(\"\\nConclusion: ResNet maintains gradient flow even at great depth!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: Squeeze-and-Excitation ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Squeeze-and-Excitation block.\n",
    "    \n",
    "    Learns to weight channels by their importance.\n",
    "    Paper: \"Squeeze-and-Excitation Networks\" (Hu et al., 2018)\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int, reduction: int = 16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excite = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        b, c, _, _ = x.size()\n",
    "        # Squeeze: global average pooling\n",
    "        y = self.squeeze(x).view(b, c)\n",
    "        # Excite: learn channel weights\n",
    "        y = self.excite(y).view(b, c, 1, 1)\n",
    "        # Scale: multiply input by channel weights\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class SEBasicBlock(nn.Module):\n",
    "    \"\"\"ResNet BasicBlock with SE attention.\"\"\"\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1, reduction: int = 16):\n",
    "        super(SEBasicBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # SE Block\n",
    "        self.se = SEBlock(out_channels, reduction)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.se(out)  # Apply SE attention\n",
    "        out += self.shortcut(x)\n",
    "        return F.relu(out)\n",
    "\n",
    "\n",
    "class SEResNet18(nn.Module):\n",
    "    \"\"\"ResNet-18 with Squeeze-and-Excitation blocks.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int = 10):\n",
    "        super(SEResNet18, self).__init__()\n",
    "        \n",
    "        self.in_channels = 64\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.layer1 = self._make_layer(64, 2, 1)\n",
    "        self.layer2 = self._make_layer(128, 2, 2)\n",
    "        self.layer3 = self._make_layer(256, 2, 2)\n",
    "        self.layer4 = self._make_layer(512, 2, 2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def _make_layer(self, out_channels: int, num_blocks: int, stride: int) -> nn.Sequential:\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(SEBasicBlock(self.in_channels, out_channels, s))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Test SE-ResNet\n",
    "se_resnet = SEResNet18(num_classes=10)\n",
    "x = torch.randn(1, 3, 32, 32)\n",
    "print(f\"SE-ResNet18 output: {se_resnet(x).shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in se_resnet.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='task-72'></a>\n",
    "## Task 7.2 Solutions: Transfer Learning\n",
    "\n",
    "### Achieving >90% Accuracy on CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution approach for >90% on CIFAR-100:\n",
    "# 1. Use a larger model (EfficientNet-B3 or ConvNeXt)\n",
    "# 2. Full dataset (not subset)\n",
    "# 3. Longer training (50+ epochs)\n",
    "# 4. Advanced techniques: Mixup, Label Smoothing\n",
    "\n",
    "# Here's the configuration that should achieve >90%:\n",
    "\n",
    "training_config = {\n",
    "    'model': 'efficientnet_b3',  # Larger model\n",
    "    'dataset_subset': None,  # Full dataset (50,000 images)\n",
    "    'epochs': 50,\n",
    "    'batch_size': 32,\n",
    "    'base_lr': 1e-4,\n",
    "    'weight_decay': 0.1,\n",
    "    'warmup_epochs': 5,\n",
    "    'label_smoothing': 0.1,\n",
    "    'mixup_alpha': 0.2,\n",
    "    'strategy': 'gradual_unfreeze',\n",
    "}\n",
    "\n",
    "print(\"Configuration for >90% accuracy on CIFAR-100:\")\n",
    "for key, value in training_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Mixup augmentation implementation\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"Apply Mixup augmentation.\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Compute loss for Mixup.\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='task-73'></a>\n",
    "## Task 7.3 Solutions: Object Detection\n",
    "\n",
    "### Detecting Specific Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Custom image detection\n",
    "# Note: Run this after installing ultralytics\n",
    "\n",
    "def detect_custom_image(image_url: str, classes_to_detect: List[str] = None):\n",
    "    \"\"\"\n",
    "    Download and detect objects in a custom image.\n",
    "    \n",
    "    Args:\n",
    "        image_url: URL of image to process\n",
    "        classes_to_detect: Optional list of class names to filter\n",
    "    \"\"\"\n",
    "    import urllib.request\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Download image\n",
    "    image_path = Path('custom_image.jpg')\n",
    "    urllib.request.urlretrieve(image_url, image_path)\n",
    "    \n",
    "    # Load YOLO\n",
    "    try:\n",
    "        from ultralytics import YOLO\n",
    "        model = YOLO('yolov8s.pt')\n",
    "        \n",
    "        # Get class indices if filtering\n",
    "        if classes_to_detect:\n",
    "            class_to_idx = {v: k for k, v in model.names.items()}\n",
    "            class_indices = [class_to_idx[c] for c in classes_to_detect if c in class_to_idx]\n",
    "            results = model(str(image_path), classes=class_indices)\n",
    "        else:\n",
    "            results = model(str(image_path))\n",
    "        \n",
    "        # Display\n",
    "        annotated = results[0].plot()\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(annotated[:, :, ::-1])\n",
    "        plt.title(f'Detections: {len(results[0].boxes)} objects found')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        # Cleanup\n",
    "        image_path.unlink()\n",
    "        \n",
    "        return results[0]\n",
    "    except ImportError:\n",
    "        print(\"Please install ultralytics: pip install ultralytics\")\n",
    "        return None\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# results = detect_custom_image(\n",
    "#     'https://images.unsplash.com/photo-1517849845537-4d257902454a',\n",
    "#     classes_to_detect=['dog', 'person']\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='task-74'></a>\n",
    "## Task 7.4 Solutions: Segmentation\n",
    "\n",
    "### Larger U-Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargerUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Larger U-Net with more channels.\n",
    "    \n",
    "    Original: 64 -> 128 -> 256 -> 512 -> 1024\n",
    "    This:     128 -> 256 -> 512 -> 1024 -> 2048\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_channels: int = 3, n_classes: int = 21):\n",
    "        super(LargerUNet, self).__init__()\n",
    "        \n",
    "        def double_conv(in_ch, out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        \n",
    "        # Encoder (double the channels)\n",
    "        self.inc = double_conv(n_channels, 128)\n",
    "        self.down1 = nn.Sequential(nn.MaxPool2d(2), double_conv(128, 256))\n",
    "        self.down2 = nn.Sequential(nn.MaxPool2d(2), double_conv(256, 512))\n",
    "        self.down3 = nn.Sequential(nn.MaxPool2d(2), double_conv(512, 1024))\n",
    "        self.down4 = nn.Sequential(nn.MaxPool2d(2), double_conv(1024, 1024))\n",
    "        \n",
    "        # Decoder\n",
    "        self.up1 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
    "        self.conv_up1 = double_conv(1024 + 512, 512)\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.conv_up2 = double_conv(512 + 256, 256)\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.conv_up3 = double_conv(256 + 128, 128)\n",
    "        \n",
    "        self.up4 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.conv_up4 = double_conv(128 + 64, 64)\n",
    "        \n",
    "        self.outc = nn.Conv2d(64, n_classes, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.up1(x5)\n",
    "        x = torch.cat([x, x4], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "        \n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x, x3], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "        \n",
    "        x = self.up3(x)\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    "        \n",
    "        x = self.up4(x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.conv_up4(x)\n",
    "        \n",
    "        return self.outc(x)\n",
    "\n",
    "# Test\n",
    "large_unet = LargerUNet(n_classes=21)\n",
    "x = torch.randn(1, 3, 256, 256)\n",
    "print(f\"Larger U-Net output: {large_unet(x).shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in large_unet.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='task-75'></a>\n",
    "## Task 7.5 Solutions: Vision Transformer\n",
    "\n",
    "### Comparing Different Patch Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Patch embedding for ViT.\"\"\"\n",
    "    def __init__(self, img_size, patch_size, in_channels, embed_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.projection = nn.Conv2d(in_channels, embed_dim, patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "def compare_patch_sizes(img_size: int = 32, embed_dim: int = 256):\n",
    "    \"\"\"\n",
    "    Compare different patch sizes for ViT.\n",
    "    \n",
    "    Smaller patches = more tokens = more compute but potentially better accuracy.\n",
    "    \"\"\"\n",
    "    patch_sizes = [2, 4, 8, 16]\n",
    "    \n",
    "    print(f\"Patch Size Comparison for {img_size}x{img_size} images\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Patch Size':<12} {'Num Patches':<15} {'Seq Length':<15} {'Params':<15}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    for patch_size in patch_sizes:\n",
    "        if img_size % patch_size != 0:\n",
    "            continue\n",
    "        \n",
    "        patch_embed = PatchEmbedding(img_size, patch_size, 3, embed_dim)\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        seq_length = num_patches + 1  # +1 for CLS token\n",
    "        params = sum(p.numel() for p in patch_embed.parameters())\n",
    "        \n",
    "        print(f\"{patch_size:<12} {num_patches:<15} {seq_length:<15} {params:,}\")\n",
    "\n",
    "compare_patch_sizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeiT with Distillation Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeiT(nn.Module):\n",
    "    \"\"\"\n",
    "    Data-efficient Image Transformer (DeiT).\n",
    "    \n",
    "    Adds a distillation token that learns from a CNN teacher.\n",
    "    Paper: \"Training data-efficient image transformers\" (Touvron et al., 2021)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int = 32,\n",
    "        patch_size: int = 4,\n",
    "        num_classes: int = 10,\n",
    "        embed_dim: int = 256,\n",
    "        depth: int = 6,\n",
    "        num_heads: int = 8\n",
    "    ):\n",
    "        super(DeiT, self).__init__()\n",
    "        \n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = nn.Conv2d(3, embed_dim, patch_size, stride=patch_size)\n",
    "        \n",
    "        # CLS token and DISTILLATION token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim))  # New!\n",
    "        \n",
    "        # Position embedding for patches + cls + dist tokens\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 2, embed_dim))\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * 4,\n",
    "            dropout=0.1,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Two classification heads\n",
    "        self.head = nn.Linear(embed_dim, num_classes)  # For CLS token\n",
    "        self.dist_head = nn.Linear(embed_dim, num_classes)  # For distillation token\n",
    "        \n",
    "        # Initialize\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.dist_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x).flatten(2).transpose(1, 2)\n",
    "        \n",
    "        # Prepend CLS and DIST tokens\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        dist_tokens = self.dist_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, dist_tokens, x], dim=1)  # [B, N+2, D]\n",
    "        \n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Transformer\n",
    "        x = self.encoder(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Two outputs\n",
    "        cls_output = self.head(x[:, 0])  # From CLS token\n",
    "        dist_output = self.dist_head(x[:, 1])  # From DIST token\n",
    "        \n",
    "        # During inference, average both predictions\n",
    "        if not self.training:\n",
    "            return (cls_output + dist_output) / 2\n",
    "        \n",
    "        return cls_output, dist_output\n",
    "\n",
    "# Test DeiT\n",
    "deit = DeiT(img_size=32, patch_size=4, num_classes=10)\n",
    "x = torch.randn(2, 3, 32, 32)\n",
    "\n",
    "# Training mode (returns two outputs)\n",
    "deit.train()\n",
    "cls_out, dist_out = deit(x)\n",
    "print(f\"DeiT training outputs: cls={cls_out.shape}, dist={dist_out.shape}\")\n",
    "\n",
    "# Eval mode (returns averaged output)\n",
    "deit.eval()\n",
    "out = deit(x)\n",
    "print(f\"DeiT inference output: {out.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in deit.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='task-76'></a>\n",
    "## Task 7.6 Solutions: SAM\n",
    "\n",
    "### Magic Wand Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MagicWand:\n",
    "    \"\"\"\n",
    "    Interactive \"Magic Wand\" tool using SAM.\n",
    "    \n",
    "    Allows iterative refinement with positive and negative clicks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sam_predictor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sam_predictor: SamPredictor instance\n",
    "        \"\"\"\n",
    "        self.predictor = sam_predictor\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Clear all clicks and masks.\"\"\"\n",
    "        self.positive_points = []\n",
    "        self.negative_points = []\n",
    "        self.current_mask = None\n",
    "        self.all_masks = []  # History of masks\n",
    "    \n",
    "    def set_image(self, image):\n",
    "        \"\"\"Set the image to segment.\"\"\"\n",
    "        self.predictor.set_image(image)\n",
    "        self.image = image\n",
    "        self.reset()\n",
    "    \n",
    "    def click(self, point, is_positive=True):\n",
    "        \"\"\"\n",
    "        Add a click point and update the mask.\n",
    "        \n",
    "        Args:\n",
    "            point: (x, y) coordinates\n",
    "            is_positive: True for \"include\", False for \"exclude\"\n",
    "        \n",
    "        Returns:\n",
    "            Updated mask\n",
    "        \"\"\"\n",
    "        if is_positive:\n",
    "            self.positive_points.append(point)\n",
    "        else:\n",
    "            self.negative_points.append(point)\n",
    "        \n",
    "        self._update_mask()\n",
    "        return self.current_mask\n",
    "    \n",
    "    def undo(self):\n",
    "        \"\"\"Undo last click.\"\"\"\n",
    "        if self.positive_points:\n",
    "            self.positive_points.pop()\n",
    "        elif self.negative_points:\n",
    "            self.negative_points.pop()\n",
    "        \n",
    "        if self.positive_points or self.negative_points:\n",
    "            self._update_mask()\n",
    "        else:\n",
    "            self.current_mask = None\n",
    "        \n",
    "        return self.current_mask\n",
    "    \n",
    "    def grow(self):\n",
    "        \"\"\"Get a larger mask (if available from multi-mask output).\"\"\"\n",
    "        if len(self.all_masks) > 1:\n",
    "            # Return larger mask\n",
    "            areas = [m.sum() for m in self.all_masks]\n",
    "            largest_idx = areas.index(max(areas))\n",
    "            self.current_mask = self.all_masks[largest_idx]\n",
    "        return self.current_mask\n",
    "    \n",
    "    def shrink(self):\n",
    "        \"\"\"Get a smaller mask (if available from multi-mask output).\"\"\"\n",
    "        if len(self.all_masks) > 1:\n",
    "            # Return smaller mask\n",
    "            areas = [m.sum() for m in self.all_masks]\n",
    "            smallest_idx = areas.index(min(areas))\n",
    "            self.current_mask = self.all_masks[smallest_idx]\n",
    "        return self.current_mask\n",
    "    \n",
    "    def _update_mask(self):\n",
    "        \"\"\"Update mask based on all accumulated points.\"\"\"\n",
    "        if not self.positive_points and not self.negative_points:\n",
    "            self.current_mask = None\n",
    "            return\n",
    "        \n",
    "        all_points = self.positive_points + self.negative_points\n",
    "        labels = [1] * len(self.positive_points) + [0] * len(self.negative_points)\n",
    "        \n",
    "        masks, scores, _ = self.predictor.predict(\n",
    "            point_coords=np.array(all_points),\n",
    "            point_labels=np.array(labels),\n",
    "            multimask_output=True\n",
    "        )\n",
    "        \n",
    "        # Store all masks for grow/shrink\n",
    "        self.all_masks = [masks[i] for i in range(len(masks))]\n",
    "        \n",
    "        # Use highest scoring mask as default\n",
    "        self.current_mask = masks[scores.argmax()]\n",
    "    \n",
    "    def visualize(self):\n",
    "        \"\"\"Visualize current state.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # Image with points\n",
    "        axes[0].imshow(self.image)\n",
    "        for p in self.positive_points:\n",
    "            axes[0].scatter(p[0], p[1], c='green', s=200, marker='*', edgecolors='white', linewidths=2)\n",
    "        for p in self.negative_points:\n",
    "            axes[0].scatter(p[0], p[1], c='red', s=200, marker='x', linewidths=3)\n",
    "        axes[0].set_title(f'Clicks: {len(self.positive_points)} positive, {len(self.negative_points)} negative')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Mask overlay\n",
    "        axes[1].imshow(self.image)\n",
    "        if self.current_mask is not None:\n",
    "            colored_mask = np.zeros((*self.current_mask.shape, 4))\n",
    "            colored_mask[self.current_mask] = [0.3, 0.7, 0.3, 0.6]\n",
    "            axes[1].imshow(colored_mask)\n",
    "        axes[1].set_title('Current Mask')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"MagicWand class defined successfully!\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  wand = MagicWand(sam_predictor)\")\n",
    "print(\"  wand.set_image(image)\")\n",
    "print(\"  wand.click((x, y), is_positive=True)\")\n",
    "print(\"  wand.click((x, y), is_positive=False)  # Exclude region\")\n",
    "print(\"  wand.grow()  # Get larger mask\")\n",
    "print(\"  wand.shrink()  # Get smaller mask\")\n",
    "print(\"  wand.undo()  # Remove last click\")\n",
    "print(\"  wand.visualize()  # Show current state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook contains solutions for all exercises in Module 7:\n",
    "\n",
    "1. **Task 7.1**: LeNet with MaxPool, gradient flow comparison, SE-ResNet\n",
    "2. **Task 7.2**: Configuration for >90% on CIFAR-100, Mixup implementation\n",
    "3. **Task 7.3**: Custom image detection with YOLO\n",
    "4. **Task 7.4**: Larger U-Net architecture\n",
    "5. **Task 7.5**: Patch size comparison, DeiT with distillation\n",
    "6. **Task 7.6**: Interactive MagicWand tool with SAM\n",
    "\n",
    "Remember: The best way to learn is to try implementing these yourself first!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
