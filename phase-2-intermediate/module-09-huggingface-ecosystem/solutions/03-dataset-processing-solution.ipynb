{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 9.3 Solutions: Dataset Processing\n",
    "\n",
    "This notebook contains solutions to the exercises in the Dataset Processing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Solution: Process the AG News Dataset\n",
    "\n",
    "Build a complete processing pipeline for the AG News dataset:\n",
    "1. Load `ag_news` dataset\n",
    "2. Create train/validation split (90/10)\n",
    "3. Tokenize with `distilbert-base-uncased`\n",
    "4. Add a column for text length\n",
    "5. Filter out very short texts (< 10 words)\n",
    "6. Save the processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load AG News dataset\n",
    "print(\"Step 1: Loading AG News dataset...\")\n",
    "ag_news = load_dataset(\"ag_news\")\n",
    "\n",
    "print(f\"Dataset structure: {ag_news}\")\n",
    "print(f\"\\nFeatures: {ag_news['train'].features}\")\n",
    "print(f\"\\nLabel mapping:\")\n",
    "print(\"  0 = World\")\n",
    "print(\"  1 = Sports\")\n",
    "print(\"  2 = Business\")\n",
    "print(\"  3 = Sci/Tech\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nSample: {ag_news['train'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create train/validation split (90/10)\n",
    "print(\"Step 2: Creating train/validation split...\")\n",
    "\n",
    "# Split the training data\n",
    "train_val_split = ag_news['train'].train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=42,\n",
    "    stratify_by_column='label'  # Maintain class balance\n",
    ")\n",
    "\n",
    "# Create final dataset dict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_val_split['train'],\n",
    "    'validation': train_val_split['test'],\n",
    "    'test': ag_news['test']\n",
    "})\n",
    "\n",
    "print(f\"Train: {len(dataset['train']):,}\")\n",
    "print(f\"Validation: {len(dataset['validation']):,}\")\n",
    "print(f\"Test: {len(dataset['test']):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Tokenize with distilbert-base-uncased\n",
    "print(\"Step 3: Tokenizing...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize text using DistilBERT tokenizer.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128  # AG News texts are relatively short\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "print(f\"Columns after tokenization: {tokenized_dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Add a column for text length\n",
    "print(\"Step 4: Adding text length column...\")\n",
    "\n",
    "def add_word_count(examples):\n",
    "    \"\"\"Add word count to each example.\"\"\"\n",
    "    examples['word_count'] = [len(text.split()) for text in examples['text']]\n",
    "    return examples\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(\n",
    "    add_word_count,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    desc=\"Adding word counts\"\n",
    ")\n",
    "\n",
    "# Show word count distribution\n",
    "word_counts = tokenized_dataset['train']['word_count']\n",
    "print(f\"Word count stats:\")\n",
    "print(f\"  Min: {min(word_counts)}\")\n",
    "print(f\"  Max: {max(word_counts)}\")\n",
    "print(f\"  Mean: {np.mean(word_counts):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Filter out very short texts (< 10 words)\n",
    "print(\"Step 5: Filtering short texts...\")\n",
    "\n",
    "print(f\"Before filtering: {len(tokenized_dataset['train']):,}\")\n",
    "\n",
    "def is_long_enough(example):\n",
    "    \"\"\"Keep only texts with 10+ words.\"\"\"\n",
    "    return example['word_count'] >= 10\n",
    "\n",
    "filtered_dataset = tokenized_dataset.filter(\n",
    "    is_long_enough,\n",
    "    desc=\"Filtering short texts\"\n",
    ")\n",
    "\n",
    "print(f\"After filtering: {len(filtered_dataset['train']):,}\")\n",
    "print(f\"Removed: {len(tokenized_dataset['train']) - len(filtered_dataset['train']):,} short texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Prepare for training and save\n",
    "print(\"Step 6: Finalizing and saving...\")\n",
    "\n",
    "# Rename label to labels (expected by Trainer)\n",
    "final_dataset = filtered_dataset.rename_column('label', 'labels')\n",
    "\n",
    "# Remove original text to save space (we have tokens now)\n",
    "final_dataset = final_dataset.remove_columns(['text', 'word_count'])\n",
    "\n",
    "print(f\"Final columns: {final_dataset['train'].column_names}\")\n",
    "\n",
    "# Set format for PyTorch\n",
    "final_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Save to disk\n",
    "save_path = \"./processed_ag_news\"\n",
    "final_dataset.save_to_disk(save_path)\n",
    "print(f\"Saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the saved dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "loaded = load_from_disk(save_path)\n",
    "print(\"Loaded dataset:\")\n",
    "print(loaded)\n",
    "\n",
    "# Check a sample\n",
    "sample = loaded['train'][0]\n",
    "print(f\"\\nSample:\")\n",
    "print(f\"  input_ids shape: {sample['input_ids'].shape}\")\n",
    "print(f\"  labels: {sample['labels']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Solution: Multi-Dataset Pipeline\n",
    "\n",
    "Create a processing pipeline that:\n",
    "1. Loads IMDB, Yelp, and Amazon reviews\n",
    "2. Standardizes them to same format\n",
    "3. Combines into one dataset\n",
    "4. Creates balanced splits\n",
    "5. Tokenizes with consistent settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Dataset Pipeline\n",
    "print(\"MULTI-DATASET PIPELINE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Step 1: Load datasets (using small subsets for demo)\n",
    "print(\"\\n1. Loading datasets...\")\n",
    "\n",
    "# IMDB - binary sentiment (0=neg, 1=pos)\n",
    "imdb = load_dataset(\"imdb\", split=\"train[:5000]\")\n",
    "print(f\"   IMDB: {len(imdb)} samples\")\n",
    "\n",
    "# Yelp - 5-star ratings, convert to binary\n",
    "yelp = load_dataset(\"yelp_review_full\", split=\"train[:5000]\")\n",
    "print(f\"   Yelp: {len(yelp)} samples\")\n",
    "\n",
    "# Amazon - polarity (1=neg, 2=pos -> convert to 0, 1)\n",
    "amazon = load_dataset(\"amazon_polarity\", split=\"train[:5000]\")\n",
    "print(f\"   Amazon: {len(amazon)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Standardize format - all to {'text': str, 'label': int (0 or 1)}\n",
    "print(\"\\n2. Standardizing formats...\")\n",
    "\n",
    "def standardize_imdb(examples):\n",
    "    \"\"\"IMDB is already in correct format.\"\"\"\n",
    "    return {\n",
    "        'text': examples['text'],\n",
    "        'label': examples['label'],\n",
    "        'source': ['imdb'] * len(examples['text'])\n",
    "    }\n",
    "\n",
    "def standardize_yelp(examples):\n",
    "    \"\"\"Convert Yelp 5-star to binary (1-2 = neg, 4-5 = pos, 3 = neutral -> neg).\"\"\"\n",
    "    binary_labels = [0 if label < 3 else 1 for label in examples['label']]\n",
    "    return {\n",
    "        'text': examples['text'],\n",
    "        'label': binary_labels,\n",
    "        'source': ['yelp'] * len(examples['text'])\n",
    "    }\n",
    "\n",
    "def standardize_amazon(examples):\n",
    "    \"\"\"Convert Amazon polarity (1/2) to (0/1).\"\"\"\n",
    "    binary_labels = [label - 1 for label in examples['label']]  # 1->0, 2->1\n",
    "    # Amazon has 'content' not 'text'\n",
    "    return {\n",
    "        'text': examples['content'],\n",
    "        'label': binary_labels,\n",
    "        'source': ['amazon'] * len(examples['content'])\n",
    "    }\n",
    "\n",
    "# Apply standardization\n",
    "imdb_std = imdb.map(standardize_imdb, batched=True, remove_columns=imdb.column_names)\n",
    "yelp_std = yelp.map(standardize_yelp, batched=True, remove_columns=yelp.column_names)\n",
    "amazon_std = amazon.map(standardize_amazon, batched=True, remove_columns=amazon.column_names)\n",
    "\n",
    "print(f\"   IMDB columns: {imdb_std.column_names}\")\n",
    "print(f\"   Yelp columns: {yelp_std.column_names}\")\n",
    "print(f\"   Amazon columns: {amazon_std.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Combine into one dataset\n",
    "print(\"\\n3. Combining datasets...\")\n",
    "\n",
    "combined = concatenate_datasets([imdb_std, yelp_std, amazon_std])\n",
    "print(f\"   Combined size: {len(combined)}\")\n",
    "\n",
    "# Show source distribution\n",
    "import collections\n",
    "source_dist = collections.Counter(combined['source'])\n",
    "print(f\"   Source distribution: {dict(source_dist)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create balanced splits\n",
    "print(\"\\n4. Creating balanced splits...\")\n",
    "\n",
    "# Shuffle and split\n",
    "combined = combined.shuffle(seed=42)\n",
    "splits = combined.train_test_split(\n",
    "    test_size=0.2,\n",
    "    seed=42,\n",
    "    stratify_by_column='label'  # Maintain label balance\n",
    ")\n",
    "\n",
    "# Further split test into validation and test\n",
    "val_test = splits['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "multi_dataset = DatasetDict({\n",
    "    'train': splits['train'],\n",
    "    'validation': val_test['train'],\n",
    "    'test': val_test['test']\n",
    "})\n",
    "\n",
    "print(f\"   Train: {len(multi_dataset['train'])}\")\n",
    "print(f\"   Validation: {len(multi_dataset['validation'])}\")\n",
    "print(f\"   Test: {len(multi_dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Tokenize with consistent settings\n",
    "print(\"\\n5. Tokenizing...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_multi(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "tokenized_multi = multi_dataset.map(\n",
    "    tokenize_multi,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    remove_columns=['text'],\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "# Rename for Trainer\n",
    "tokenized_multi = tokenized_multi.rename_column('label', 'labels')\n",
    "\n",
    "print(f\"   Final columns: {tokenized_multi['train'].column_names}\")\n",
    "print(\"\\nMulti-dataset pipeline complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(\"./processed_ag_news\"):\n",
    "    shutil.rmtree(\"./processed_ag_news\")\n",
    "    print(\"Cleaned up processed_ag_news directory\")\n",
    "\n",
    "print(\"Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this solution notebook, we demonstrated:\n",
    "\n",
    "1. **AG News Processing Pipeline**:\n",
    "   - Loaded and explored dataset\n",
    "   - Created stratified train/val/test splits\n",
    "   - Tokenized with DistilBERT\n",
    "   - Added custom columns (word count)\n",
    "   - Filtered by criteria\n",
    "   - Saved processed dataset\n",
    "\n",
    "2. **Multi-Dataset Pipeline**:\n",
    "   - Combined IMDB, Yelp, and Amazon reviews\n",
    "   - Standardized different formats\n",
    "   - Created balanced splits\n",
    "   - Applied consistent tokenization\n",
    "\n",
    "Key learnings:\n",
    "- Use `stratify_by_column` for balanced splits\n",
    "- Standardize formats before combining datasets\n",
    "- Use `concatenate_datasets` to merge datasets\n",
    "- Remove unnecessary columns to save memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
