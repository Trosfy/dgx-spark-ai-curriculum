{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 9.5: LoRA - Parameter-Efficient Fine-Tuning\n",
    "\n",
    "**Module:** 9 - Hugging Face Ecosystem  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐⭐⭐ (Intermediate-Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand the theory behind LoRA (Low-Rank Adaptation)\n",
    "- [ ] Configure LoRA with the PEFT library\n",
    "- [ ] Compare memory usage: LoRA vs full fine-tuning\n",
    "- [ ] Fine-tune a model using LoRA\n",
    "- [ ] Merge LoRA adapters back into the base model\n",
    "- [ ] Appreciate when and why to use PEFT methods\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Task 9.4 (Trainer Fine-tuning)\n",
    "- Knowledge of: Basic linear algebra (matrices), fine-tuning concepts\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "Imagine you want to customize a 70 billion parameter model for your specific use case. Full fine-tuning would require:\n",
    "- ~140GB just for model weights (in fp16)\n",
    "- ~560GB for optimizer states (Adam needs 4x model size)\n",
    "- Multiple high-end GPUs and weeks of training\n",
    "\n",
    "**LoRA changes everything:**\n",
    "- Train only 0.1-1% of parameters\n",
    "- Fit training on a single GPU\n",
    "- Complete in hours, not weeks\n",
    "- Achieve 90-99% of full fine-tuning quality!\n",
    "\n",
    "**Real-world LoRA users:**\n",
    "- Stability AI: Fine-tuned Stable Diffusion for specific styles\n",
    "- Microsoft: Personalized Copilot assistants\n",
    "- Countless developers: Custom chatbots and assistants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ELI5: What is LoRA?\n\n> **Imagine you're customizing a car.** You could:\n> - Option A: Rebuild the entire engine from scratch (expensive, slow)\n> - Option B: Add a turbocharger attachment (cheap, fast, removable!)\n>\n> **LoRA is Option B for AI models.**\n>\n> Instead of modifying the entire model (billions of numbers), LoRA:\n> 1. **Freezes** the original model completely\n> 2. **Adds** tiny \"adapter\" layers alongside key components\n> 3. **Trains** only the adapters (millions vs billions of parameters)\n\n### Visual: How LoRA Works\n\n```\nSTANDARD LAYER:                    LORA LAYER:\n                                   \n  Input (x)                          Input (x)\n     │                                  │\n     ▼                                  ├──────────────┐\n  ┌─────┐                               ▼              ▼\n  │  W  │  ← Full weight matrix     ┌─────┐        ┌─────┐\n  │     │    (10000×10000 params)   │  W  │ FROZEN │  A  │ (10000×8)\n  │     │                           │     │        └──┬──┘\n  └──┬──┘                           └──┬──┘           ▼\n     │                                 │          ┌─────┐\n     ▼                                 │          │  B  │ (8×10000)\n  Output                               │          └──┬──┘\n                                       │             │\n                                       ▼             ▼\n                                    Original    +   LoRA\n                                       └─────┬───────┘\n                                             ▼\n                                          Output\n\n  100,000,000 params             160,000 trainable params\n  (all trainable)                  (625x fewer!)\n```\n\n> **The clever math trick:**\n> - Original layer: multiply by huge matrix W (10000 × 10000 = 100M parameters)\n> - LoRA says: \"Changes to W can be approximated by two small matrices\"\n> - Small matrices A (10000 × 8) and B (8 × 10000) = only 160K parameters!\n> - That's 625x fewer parameters to train!\n>\n> **In AI terms:** LoRA approximates weight updates using low-rank matrix decomposition, dramatically reducing trainable parameters while maintaining model quality."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup and Understanding the Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install PEFT library\n# Note: These packages are pre-installed in the NGC PyTorch container.\n# Running pip install ensures you have compatible versions.\n# If NOT using NGC container, ensure you have ARM64-compatible packages for DGX Spark.\n\n!pip install -q \"peft>=0.6.0\" \"transformers>=4.35.0\" \"datasets>=2.14.0\" \"evaluate>=0.4.0\" \"accelerate>=0.24.0\"\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport evaluate\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom peft import LoraConfig, get_peft_model, TaskType\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Math Behind LoRA\n",
    "\n",
    "In a standard neural network layer:\n",
    "$$y = Wx$$\n",
    "\n",
    "During fine-tuning, we update W:\n",
    "$$y = (W + \\Delta W)x$$\n",
    "\n",
    "LoRA's key insight: $\\Delta W$ can be decomposed into two smaller matrices:\n",
    "$$\\Delta W = BA$$\n",
    "\n",
    "Where:\n",
    "- $W$ is the original weight matrix (frozen)\n",
    "- $B$ is a small matrix of shape (d, r)\n",
    "- $A$ is a small matrix of shape (r, k)\n",
    "- $r$ is the \"rank\" (typically 8-64, much smaller than d or k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the math\n",
    "def visualize_lora_math():\n",
    "    # Typical transformer dimensions\n",
    "    d_model = 768  # Hidden size\n",
    "    \n",
    "    # Original weight matrix\n",
    "    W_params = d_model * d_model\n",
    "    print(f\"Original W matrix: {d_model} x {d_model} = {W_params:,} parameters\")\n",
    "    \n",
    "    # LoRA decomposition with different ranks\n",
    "    print(\"\\nLoRA decomposition:\")\n",
    "    print(f\"{'Rank (r)':<10} {'A params':<12} {'B params':<12} {'Total':<12} {'Reduction':<12}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for r in [4, 8, 16, 32, 64]:\n",
    "        A_params = d_model * r\n",
    "        B_params = r * d_model\n",
    "        total = A_params + B_params\n",
    "        reduction = W_params / total\n",
    "        print(f\"{r:<10} {A_params:<12,} {B_params:<12,} {total:<12,} {reduction:.0f}x\")\n",
    "\n",
    "visualize_lora_math()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate LoRA computation\n",
    "class SimpleLoRALayer(nn.Module):\n",
    "    \"\"\"A simple demonstration of how LoRA works.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, rank=8, alpha=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Original frozen weight\n",
    "        self.W = nn.Linear(in_features, out_features, bias=False)\n",
    "        self.W.weight.requires_grad = False  # Frozen!\n",
    "        \n",
    "        # LoRA adapters (these are trainable)\n",
    "        self.lora_A = nn.Linear(in_features, rank, bias=False)\n",
    "        self.lora_B = nn.Linear(rank, out_features, bias=False)\n",
    "        \n",
    "        # Scaling factor\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # Initialize A with random, B with zeros (so LoRA starts as identity)\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight)\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Original path (frozen)\n",
    "        original = self.W(x)\n",
    "        \n",
    "        # LoRA path (trainable)\n",
    "        lora = self.lora_B(self.lora_A(x)) * self.scaling\n",
    "        \n",
    "        return original + lora\n",
    "\n",
    "# Create and test\n",
    "layer = SimpleLoRALayer(768, 768, rank=8)\n",
    "\n",
    "# Count parameters\n",
    "frozen_params = sum(p.numel() for p in layer.W.parameters())\n",
    "trainable_params = sum(p.numel() for p in [layer.lora_A.weight, layer.lora_B.weight])\n",
    "\n",
    "print(f\"Frozen parameters: {frozen_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Trainable %: {100 * trainable_params / (frozen_params + trainable_params):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Using PEFT Library for LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to track memory\n",
    "def get_memory_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        return allocated, reserved\n",
    "    return 0, 0\n",
    "\n",
    "def print_memory(label):\n",
    "    alloc, res = get_memory_usage()\n",
    "    print(f\"[{label}] Allocated: {alloc:.2f} GB, Reserved: {res:.2f} GB\")\n",
    "\n",
    "# Clear any existing memory\n",
    "torch.cuda.empty_cache()\n",
    "print_memory(\"Start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model for comparison\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print_memory(\"After loading base model\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in base_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in base_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nBase model:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure LoRA\n# NOTE: target_modules names vary by model architecture!\n# See \"Common patterns by model architecture\" section (cell 36) for model-specific names.\n# PEFT uses partial string matching, so \"query\" matches \"bert.encoder.layer.X.attention.self.query\"\n\nlora_config = LoraConfig(\n    # Core LoRA parameters\n    r=8,                        # Rank (lower = fewer params, higher = more capacity)\n    lora_alpha=16,              # Scaling factor (usually 2*r)\n    lora_dropout=0.1,           # Dropout for regularization\n    \n    # Which layers to apply LoRA to\n    # For BERT/RoBERTa: use \"query\", \"key\", \"value\", \"dense\"\n    # For DistilBERT: use \"q_lin\", \"k_lin\", \"v_lin\"\n    # For LLaMA/Mistral: use \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"\n    target_modules=[\"query\", \"value\"],  # Attention layers (works for BERT variants)\n    \n    # Task type\n    task_type=TaskType.SEQ_CLS,  # Sequence classification\n    \n    # Additional options\n    bias=\"none\",                # Don't train biases\n    modules_to_save=[\"classifier\"],  # Train classifier head normally\n)\n\nprint(\"LoRA Configuration:\")\nprint(f\"  Rank: {lora_config.r}\")\nprint(f\"  Alpha: {lora_config.lora_alpha}\")\nprint(f\"  Target modules: {lora_config.target_modules}\")\nprint(f\"  Dropout: {lora_config.lora_dropout}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to the model\n",
    "peft_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "print_memory(\"After applying LoRA\")\n",
    "\n",
    "# Print trainable parameters\n",
    "print(\"\\nParameter summary:\")\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the model structure\n",
    "print(\"\\nLoRA layers added:\")\n",
    "for name, param in peft_model.named_parameters():\n",
    "    if 'lora' in name.lower():\n",
    "        print(f\"  {name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Memory Comparison - LoRA vs Full Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "\n",
    "def measure_training_memory(model, tokenizer, dataset, use_lora=True, description=\"\"):\n",
    "    \"\"\"Measure memory and time for training.\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    # Small subset for quick test\n",
    "    small_train = dataset['train'].select(range(500))\n",
    "    small_eval = dataset['train'].select(range(500, 600))\n",
    "    \n",
    "    # Tokenize\n",
    "    def tokenize(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=128\n",
    "        )\n",
    "    \n",
    "    tokenized_train = small_train.map(tokenize, batched=True, remove_columns=['text'])\n",
    "    tokenized_train = tokenized_train.rename_column('label', 'labels')\n",
    "    tokenized_train.set_format('torch')\n",
    "    \n",
    "    tokenized_eval = small_eval.map(tokenize, batched=True, remove_columns=['text'])\n",
    "    tokenized_eval = tokenized_eval.rename_column('label', 'labels')\n",
    "    tokenized_eval.set_format('torch')\n",
    "    \n",
    "    # Training args\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"./temp_training\",\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        learning_rate=2e-5,\n",
    "        bf16=True,\n",
    "        logging_strategy=\"no\",\n",
    "        save_strategy=\"no\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_eval,\n",
    "    )\n",
    "    \n",
    "    # Train and measure\n",
    "    start_time = time.time()\n",
    "    trainer.train()\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "    \n",
    "    return {\n",
    "        'description': description,\n",
    "        'peak_memory_gb': peak_memory,\n",
    "        'train_time_seconds': train_time,\n",
    "        'trainable_params': sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    }\n",
    "\n",
    "print(\"Memory measurement function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Full fine-tuning\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST 1: FULL FINE-TUNING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Fresh model for full fine-tuning\n",
    "full_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=2, torch_dtype=torch.bfloat16\n",
    ").cuda()\n",
    "\n",
    "full_results = measure_training_memory(full_model, tokenizer, imdb, use_lora=False, description=\"Full Fine-tuning\")\n",
    "print(f\"Peak memory: {full_results['peak_memory_gb']:.2f} GB\")\n",
    "print(f\"Train time: {full_results['train_time_seconds']:.1f}s\")\n",
    "print(f\"Trainable params: {full_results['trainable_params']:,}\")\n",
    "\n",
    "# Cleanup\n",
    "del full_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: LoRA fine-tuning (r=8)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST 2: LoRA (r=8)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "lora_model_r8 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=2, torch_dtype=torch.bfloat16\n",
    ")\n",
    "lora_config_r8 = LoraConfig(\n",
    "    r=8, lora_alpha=16, lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    modules_to_save=[\"classifier\"]\n",
    ")\n",
    "lora_model_r8 = get_peft_model(lora_model_r8, lora_config_r8).cuda()\n",
    "\n",
    "lora_r8_results = measure_training_memory(lora_model_r8, tokenizer, imdb, use_lora=True, description=\"LoRA r=8\")\n",
    "print(f\"Peak memory: {lora_r8_results['peak_memory_gb']:.2f} GB\")\n",
    "print(f\"Train time: {lora_r8_results['train_time_seconds']:.1f}s\")\n",
    "print(f\"Trainable params: {lora_r8_results['trainable_params']:,}\")\n",
    "\n",
    "del lora_model_r8\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: LoRA fine-tuning (r=16)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST 3: LoRA (r=16)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "lora_model_r16 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=2, torch_dtype=torch.bfloat16\n",
    ")\n",
    "lora_config_r16 = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    modules_to_save=[\"classifier\"]\n",
    ")\n",
    "lora_model_r16 = get_peft_model(lora_model_r16, lora_config_r16).cuda()\n",
    "\n",
    "lora_r16_results = measure_training_memory(lora_model_r16, tokenizer, imdb, use_lora=True, description=\"LoRA r=16\")\n",
    "print(f\"Peak memory: {lora_r16_results['peak_memory_gb']:.2f} GB\")\n",
    "print(f\"Train time: {lora_r16_results['train_time_seconds']:.1f}s\")\n",
    "print(f\"Trainable params: {lora_r16_results['trainable_params']:,}\")\n",
    "\n",
    "del lora_model_r16\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MEMORY COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = [full_results, lora_r8_results, lora_r16_results]\n",
    "\n",
    "print(f\"{'Method':<20} {'Memory (GB)':<15} {'Time (s)':<12} {'Params':<15} {'Reduction'}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "baseline_memory = full_results['peak_memory_gb']\n",
    "baseline_params = full_results['trainable_params']\n",
    "\n",
    "for r in results:\n",
    "    mem_reduction = baseline_memory / r['peak_memory_gb']\n",
    "    param_reduction = baseline_params / r['trainable_params']\n",
    "    print(f\"{r['description']:<20} {r['peak_memory_gb']:<15.2f} {r['train_time_seconds']:<12.1f} {r['trainable_params']:<15,} {param_reduction:.0f}x params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Full Training with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training pipeline with LoRA\n",
    "from datasets import DatasetDict\n",
    "import evaluate\n",
    "\n",
    "print(\"Setting up full LoRA training pipeline...\")\n",
    "\n",
    "# Prepare data\n",
    "train_val = imdb['train'].train_test_split(test_size=0.1, seed=42)\n",
    "dataset = DatasetDict({\n",
    "    'train': train_val['train'],\n",
    "    'validation': train_val['test'],\n",
    "    'test': imdb['test']\n",
    "})\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "tokenized = dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "tokenized = tokenized.rename_column('label', 'labels')\n",
    "\n",
    "print(f\"Train: {len(tokenized['train']):,}\")\n",
    "print(f\"Validation: {len(tokenized['validation']):,}\")\n",
    "print(f\"Test: {len(tokenized['test']):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with LoRA\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# LoRA config with more target modules\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"key\", \"value\", \"dense\"],  # All attention + FFN\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    modules_to_save=[\"classifier\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy.compute(predictions=predictions, references=labels)['accuracy'],\n",
    "        'f1': f1.compute(predictions=predictions, references=labels)['f1']\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=1e-4,  # LoRA can use higher LR than full fine-tuning!\n",
    "    warmup_ratio=0.1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    bf16=True,\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized['train'],\n",
    "    eval_dataset=tokenized['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Trainer ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"\\nStarting LoRA training...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Time: {train_result.metrics['train_runtime']:.1f}s\")\n",
    "print(f\"Samples/sec: {train_result.metrics['train_samples_per_second']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_results = trainer.evaluate(tokenized['test'])\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "for key, value in test_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Saving and Merging LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save just the LoRA adapters (very small!)\nadapter_path = \"./lora_adapters\"\nmodel.save_pretrained(adapter_path)\ntokenizer.save_pretrained(adapter_path)  # Always save tokenizer alongside adapter!\n\nimport os\nprint(\"Saved adapter files:\")\nfor f in os.listdir(adapter_path):\n    size_mb = os.path.getsize(os.path.join(adapter_path, f)) / 1e6\n    print(f\"  {f}: {size_mb:.2f} MB\")\n\n# Compare to full model size\nadapter_size = sum(os.path.getsize(os.path.join(adapter_path, f)) for f in os.listdir(adapter_path)) / 1e6\nprint(f\"\\nTotal adapter size: {adapter_size:.2f} MB\")\nprint(f\"Full BERT-base model: ~440 MB\")\nprint(f\"Savings: {440/adapter_size:.0f}x smaller!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load adapters onto a fresh model\n",
    "from peft import PeftModel\n",
    "\n",
    "print(\"Loading adapters onto fresh base model...\")\n",
    "\n",
    "# Fresh base model\n",
    "fresh_base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=2, torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load adapters\n",
    "loaded_model = PeftModel.from_pretrained(fresh_base, adapter_path)\n",
    "\n",
    "print(\"Adapters loaded!\")\n",
    "loaded_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge adapters into base model (for faster inference)\n",
    "print(\"Merging adapters into base model...\")\n",
    "\n",
    "merged_model = loaded_model.merge_and_unload()\n",
    "\n",
    "print(f\"\\nMerged model parameters: {sum(p.numel() for p in merged_model.parameters()):,}\")\n",
    "print(\"Adapters are now part of the model weights!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged model (full size, but includes LoRA changes)\n",
    "merged_path = \"./merged_model\"\n",
    "merged_model.save_pretrained(merged_path)\n",
    "tokenizer.save_pretrained(merged_path)\n",
    "\n",
    "print(f\"\\nMerged model saved to {merged_path}\")\n",
    "print(\"This model can be loaded without PEFT library!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Multiple LoRA Adapters (Advanced)\n",
    "\n",
    "One powerful feature: you can train multiple adapters and switch between them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate multiple adapter concept\n",
    "print(\"Multiple LoRA Adapters Concept:\")\n",
    "print(\"\\n1. Train one adapter for sentiment analysis\")\n",
    "print(\"2. Train another adapter for spam detection\")\n",
    "print(\"3. Load same base model\")\n",
    "print(\"4. Switch adapters based on task!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ADAPTER SWITCHING EXAMPLE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# This would work like:\n",
    "# model.load_adapter(\"sentiment_adapter\")\n",
    "# model.set_adapter(\"sentiment_adapter\")\n",
    "# output1 = model(sentiment_input)\n",
    "#\n",
    "# model.set_adapter(\"spam_adapter\")\n",
    "# output2 = model(spam_input)\n",
    "\n",
    "print(\"\"\"\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model once\n",
    "base = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Add multiple adapters\n",
    "model = PeftModel.from_pretrained(base, \"sentiment_adapter\")\n",
    "model.load_adapter(\"spam_adapter\", adapter_name=\"spam\")\n",
    "\n",
    "# Switch between them\n",
    "model.set_adapter(\"default\")  # sentiment\n",
    "sentiment_output = model(**sentiment_inputs)\n",
    "\n",
    "model.set_adapter(\"spam\")  # spam\n",
    "spam_output = model(**spam_inputs)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Experiment with LoRA Configurations\n",
    "\n",
    "Try different LoRA configurations and compare results:\n",
    "1. Different ranks (4, 8, 16, 32, 64)\n",
    "2. Different target modules\n",
    "3. Different alpha values\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "```python\n",
    "# Try these configurations:\n",
    "configs = [\n",
    "    LoraConfig(r=4, lora_alpha=8, target_modules=[\"query\", \"value\"]),\n",
    "    LoraConfig(r=16, lora_alpha=32, target_modules=[\"query\", \"value\"]),\n",
    "    LoraConfig(r=8, lora_alpha=16, target_modules=[\"query\", \"key\", \"value\", \"dense\"]),\n",
    "]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Experiment with different LoRA configurations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Wrong Target Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# WRONG: Guessing module names\n# LoraConfig(target_modules=[\"attention\", \"mlp\"])  # May not match!\n\n# CORRECT: Check actual module names\nprint(\"Finding target modules:\")\nprint(\"\\nMethod 1: Print named modules\")\nfor name, module in base_model.named_modules():\n    if isinstance(module, nn.Linear):\n        if 'attention' in name.lower() or 'query' in name.lower():\n            print(f\"  {name}\")\n\nprint(\"\\nCommon patterns by model architecture:\")\nprint(\"  BERT/RoBERTa:    'query', 'key', 'value', 'dense'\")\nprint(\"  DistilBERT:      'q_lin', 'k_lin', 'v_lin'\")\nprint(\"  GPT-2:           'c_attn', 'c_proj', 'c_fc'\")\nprint(\"  LLaMA/Mistral:   'q_proj', 'k_proj', 'v_proj', 'o_proj'\")\nprint(\"  DeBERTa:         'query_proj', 'key_proj', 'value_proj'\")\nprint(\"  T5:              'q', 'k', 'v', 'o'\")\nprint(\"\\nTip: Always inspect your model's named_modules() to find correct names!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Not Saving Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Only saving adapter\n",
    "# model.save_pretrained(path)  # Missing tokenizer!\n",
    "\n",
    "# CORRECT: Save both\n",
    "# model.save_pretrained(path)\n",
    "# tokenizer.save_pretrained(path)\n",
    "\n",
    "print(\"Always save the tokenizer with your adapter!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Using Too High LR with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA allows higher LR than full fine-tuning, but not too high!\n",
    "print(\"Learning rate guidelines:\")\n",
    "print(\"\\nFull fine-tuning:\")\n",
    "print(\"  1e-5 to 5e-5 (conservative)\")\n",
    "print(\"\\nLoRA fine-tuning:\")\n",
    "print(\"  1e-4 to 3e-4 (can be higher!)\")\n",
    "print(\"\\nBut still not:\")\n",
    "print(\"  1e-2+ (too high, unstable training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ✅ The theory behind LoRA (low-rank decomposition)\n",
    "- ✅ How to configure LoRA with PEFT\n",
    "- ✅ Memory savings comparison (LoRA vs full fine-tuning)\n",
    "- ✅ How to train with LoRA\n",
    "- ✅ How to save and merge adapters\n",
    "- ✅ When and why to use PEFT methods\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge: LoRA for a Large Model\n",
    "\n",
    "Apply LoRA to fine-tune a larger model (e.g., `microsoft/deberta-v3-base` or `roberta-large`) and compare the memory savings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n",
    "- [PEFT Documentation](https://huggingface.co/docs/peft)\n",
    "- [QLoRA Paper](https://arxiv.org/abs/2305.14314) (LoRA + Quantization)\n",
    "- [PEFT Methods Comparison](https://huggingface.co/blog/peft)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import gc\n",
    "\n",
    "# Clean up saved files\n",
    "for path in [\"./lora_results\", \"./lora_adapters\", \"./merged_model\", \"./temp_training\"]:\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Removed {path}\")\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nCleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, **06-model-upload.ipynb**, we'll learn how to share your fine-tuned models on the Hugging Face Hub, including creating proper model cards!\n",
    "\n",
    "Great job completing Task 9.5! You now understand how to efficiently fine-tune large models with LoRA!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}