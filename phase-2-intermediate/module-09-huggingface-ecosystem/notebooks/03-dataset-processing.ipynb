{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 9.3: Dataset Processing with Hugging Face Datasets\n",
    "\n",
    "**Module:** 9 - Hugging Face Ecosystem  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐⭐ (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Load datasets from the Hugging Face Hub\n",
    "- [ ] Apply map(), filter(), and select() operations\n",
    "- [ ] Process large datasets efficiently with batching and multiprocessing\n",
    "- [ ] Create train/validation/test splits\n",
    "- [ ] Understand streaming for massive datasets\n",
    "- [ ] Save and share processed datasets\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Tasks 9.1 and 9.2 (Hub and Pipelines)\n",
    "- Knowledge of: Pandas basics, tokenization concepts\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "Imagine you're a chef preparing ingredients for a restaurant:\n",
    "- You receive raw ingredients (raw data)\n",
    "- You wash, chop, measure, and portion everything (preprocessing)\n",
    "- You organize into prep containers (batching)\n",
    "- You store properly for service (caching)\n",
    "\n",
    "Data preprocessing is 80% of ML work! The `datasets` library makes this efficient and reproducible.\n",
    "\n",
    "**Real examples:**\n",
    "- Training a chatbot: Process millions of conversation pairs\n",
    "- Sentiment analysis: Tokenize and encode text with labels\n",
    "- Translation: Align source and target language pairs\n",
    "- Fine-tuning LLMs: Format instruction-response pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: What is the Datasets Library?\n",
    "\n",
    "> **Imagine you have a huge recipe book** with millions of recipes. Reading the whole book into your head at once would be impossible!\n",
    ">\n",
    "> The `datasets` library is like a magical bookmark that:\n",
    "> - Lets you flip to any page instantly (memory-mapping)\n",
    "> - Can modify recipes without rewriting the whole book (lazy evaluation)\n",
    "> - Has helpers that can work on many recipes at once (multiprocessing)\n",
    "> - Remembers your changes so you don't redo work (caching)\n",
    ">\n",
    "> **Special trick:** Instead of loading all recipes, it just remembers WHERE each recipe is. When you need one, it grabs just that page!\n",
    ">\n",
    "> **In AI terms:** The library uses Apache Arrow format for efficient memory-mapped I/O, enabling zero-copy reads and fast random access without loading entire datasets into RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Install required packages\n# Note: These packages are pre-installed in the NGC PyTorch container.\n# Running pip install ensures you have compatible versions.\n\n!pip install -q \"transformers>=4.35.0\" \"huggingface_hub>=0.19.0\" \"datasets>=2.14.0\"\n\nprint(\"Packages ready!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import the datasets library\nfrom datasets import load_dataset, Dataset, DatasetDict\nfrom datasets import load_dataset_builder\nimport torch\nimport numpy as np\nimport os\n\n# Set cache directory (useful for DGX Spark with mounted volumes)\n# os.environ['HF_DATASETS_CACHE'] = '/workspace/data/hf_cache'\n\nprint(\"Datasets library ready!\")\nprint(f\"Cache dir: {os.environ.get('HF_DATASETS_CACHE', '~/.cache/huggingface/datasets')}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading from the Hub\n",
    "\n",
    "The simplest way to load a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB dataset (sentiment analysis)\n",
    "print(\"Loading IMDB dataset...\")\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(imdb)\n",
    "\n",
    "print(f\"\\nTrain split size: {len(imdb['train']):,} examples\")\n",
    "print(f\"Test split size: {len(imdb['test']):,} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the structure\n",
    "print(\"Dataset features (columns):\")\n",
    "print(imdb['train'].features)\n",
    "\n",
    "print(\"\\nFirst example:\")\n",
    "example = imdb['train'][0]\n",
    "print(f\"  Text: {example['text'][:200]}...\")\n",
    "print(f\"  Label: {example['label']} (0=negative, 1=positive)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Specific Splits and Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load only the train split\n",
    "train_only = load_dataset(\"imdb\", split=\"train\")\n",
    "print(f\"Train only: {len(train_only):,} examples\")\n",
    "\n",
    "# Load a subset (first 1000 examples)\n",
    "small_train = load_dataset(\"imdb\", split=\"train[:1000]\")\n",
    "print(f\"First 1000: {len(small_train):,} examples\")\n",
    "\n",
    "# Load a percentage\n",
    "tiny_train = load_dataset(\"imdb\", split=\"train[:5%]\")\n",
    "print(f\"First 5%: {len(tiny_train):,} examples\")\n",
    "\n",
    "# Load a range\n",
    "middle = load_dataset(\"imdb\", split=\"train[1000:2000]\")\n",
    "print(f\"Range 1000-2000: {len(middle):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "When you load a dataset:\n",
    "1. **Download**: Files are downloaded to cache (~/.cache/huggingface/datasets)\n",
    "2. **Convert**: Data is converted to Arrow format (efficient binary)\n",
    "3. **Memory-map**: Files are memory-mapped (not loaded into RAM!)\n",
    "4. **Cache**: Processing is cached for future use\n",
    "\n",
    "On your DGX Spark with 128GB memory, you can load MUCH larger datasets than typical systems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Exploring Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info without downloading\n",
    "builder = load_dataset_builder(\"squad\")\n",
    "\n",
    "print(\"SQuAD Dataset Info:\")\n",
    "print(f\"  Description: {builder.info.description[:200]}...\")\n",
    "print(f\"  Homepage: {builder.info.homepage}\")\n",
    "print(f\"  Features: {builder.info.features}\")\n",
    "print(f\"  Download size: {builder.info.download_size / 1e6:.1f} MB\")\n",
    "print(f\"  Dataset size: {builder.info.dataset_size / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore available datasets\n",
    "from huggingface_hub import list_datasets\n",
    "\n",
    "# Find popular datasets for specific tasks\n",
    "text_class_datasets = list(list_datasets(\n",
    "    filter=\"task_categories:text-classification\",\n",
    "    sort=\"downloads\",\n",
    "    direction=-1,\n",
    "    limit=10\n",
    "))\n",
    "\n",
    "print(\"Top 10 Text Classification Datasets:\\n\")\n",
    "for i, ds in enumerate(text_class_datasets, 1):\n",
    "    print(f\"{i}. {ds.id} ({ds.downloads:,} downloads)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: The Map Operation (Your Most Important Tool)\n",
    "\n",
    "`.map()` applies a function to every example. It's how you:\n",
    "- Tokenize text\n",
    "- Add new columns\n",
    "- Transform existing columns\n",
    "- Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple map: add text length\n",
    "def add_length(example):\n",
    "    \"\"\"Add word count to each example.\"\"\"\n",
    "    example['word_count'] = len(example['text'].split())\n",
    "    return example\n",
    "\n",
    "# Apply to small subset\n",
    "small_imdb = load_dataset(\"imdb\", split=\"train[:100]\")\n",
    "\n",
    "print(\"Before map:\")\n",
    "print(f\"  Columns: {small_imdb.column_names}\")\n",
    "\n",
    "# Apply the function\n",
    "small_imdb = small_imdb.map(add_length)\n",
    "\n",
    "print(\"\\nAfter map:\")\n",
    "print(f\"  Columns: {small_imdb.column_names}\")\n",
    "print(f\"  Sample word counts: {small_imdb['word_count'][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batched Map (Much Faster!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Comparison: batched vs non-batched\n",
    "test_data = load_dataset(\"imdb\", split=\"train[:5000]\")\n",
    "\n",
    "# Non-batched (slower)\n",
    "def process_single(example):\n",
    "    example['upper_text'] = example['text'].upper()[:100]\n",
    "    return example\n",
    "\n",
    "start = time.time()\n",
    "_ = test_data.map(process_single)\n",
    "non_batched_time = time.time() - start\n",
    "\n",
    "# Batched (faster!)\n",
    "def process_batch(examples):\n",
    "    examples['upper_text'] = [t.upper()[:100] for t in examples['text']]\n",
    "    return examples\n",
    "\n",
    "start = time.time()\n",
    "_ = test_data.map(process_batch, batched=True, batch_size=1000)\n",
    "batched_time = time.time() - start\n",
    "\n",
    "print(f\"Non-batched: {non_batched_time:.2f}s\")\n",
    "print(f\"Batched: {batched_time:.2f}s\")\n",
    "print(f\"Speedup: {non_batched_time/batched_time:.1f}x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with Map (The Most Common Use Case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize text using BERT tokenizer.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "# Tokenize with batching and multiprocessing\n",
    "tokenized = test_data.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,  # Use 4 CPU cores\n",
    "    remove_columns=['text']  # Remove original text to save memory\n",
    ")\n",
    "\n",
    "print(\"Tokenized dataset:\")\n",
    "print(f\"  Columns: {tokenized.column_names}\")\n",
    "print(f\"  Example input_ids shape: {len(tokenized[0]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "The `map()` call:\n",
    "1. Split data into batches of 1000\n",
    "2. Distributed batches across 4 CPU cores\n",
    "3. Applied tokenization in parallel\n",
    "4. Removed original text column to save memory\n",
    "5. Cached results for future use\n",
    "\n",
    "**DGX Spark Tip:** With 12 CPU cores available, you can use `num_proc=8` or higher for even faster processing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Filter and Select Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload with text for filtering examples\n",
    "imdb_sample = load_dataset(\"imdb\", split=\"train[:10000]\")\n",
    "\n",
    "# Add word count\n",
    "def add_stats(examples):\n",
    "    examples['word_count'] = [len(t.split()) for t in examples['text']]\n",
    "    return examples\n",
    "\n",
    "imdb_sample = imdb_sample.map(add_stats, batched=True)\n",
    "\n",
    "print(f\"Original size: {len(imdb_sample):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter: Keep only reviews with 100-500 words\n",
    "def is_medium_length(example):\n",
    "    return 100 <= example['word_count'] <= 500\n",
    "\n",
    "filtered = imdb_sample.filter(is_medium_length)\n",
    "print(f\"After filtering (100-500 words): {len(filtered):,}\")\n",
    "\n",
    "# Filter: Keep only positive reviews\n",
    "positive = imdb_sample.filter(lambda x: x['label'] == 1)\n",
    "print(f\"Positive reviews only: {len(positive):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batched filtering is also available\n",
    "def filter_batch(examples):\n",
    "    return [100 <= wc <= 500 for wc in examples['word_count']]\n",
    "\n",
    "filtered_batch = imdb_sample.filter(filter_batch, batched=True, batch_size=1000)\n",
    "print(f\"Batched filter result: {len(filtered_batch):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific indices\n",
    "indices = [0, 100, 200, 300, 400]\n",
    "selected = imdb_sample.select(indices)\n",
    "\n",
    "print(f\"Selected 5 specific examples:\")\n",
    "for i, example in enumerate(selected):\n",
    "    print(f\"  {indices[i]}: {example['word_count']} words, label={example['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Creating Train/Validation/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset without pre-defined splits\n",
    "# (IMDB has train/test, let's create our own validation)\n",
    "\n",
    "# Method 1: train_test_split\n",
    "train_data = load_dataset(\"imdb\", split=\"train\")\n",
    "\n",
    "# Split train into train and validation (90/10)\n",
    "split_data = train_data.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(\"After splitting:\")\n",
    "print(f\"  New train: {len(split_data['train']):,}\")\n",
    "print(f\"  Validation: {len(split_data['test']):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Stratified split (maintains class balance)\n",
    "split_stratified = train_data.train_test_split(\n",
    "    test_size=0.1, \n",
    "    seed=42,\n",
    "    stratify_by_column='label'  # Maintain label distribution!\n",
    ")\n",
    "\n",
    "# Check label distribution\n",
    "import collections\n",
    "\n",
    "original_dist = collections.Counter(train_data['label'])\n",
    "train_dist = collections.Counter(split_stratified['train']['label'])\n",
    "val_dist = collections.Counter(split_stratified['test']['label'])\n",
    "\n",
    "print(\"Label distributions:\")\n",
    "print(f\"  Original: {dict(original_dist)}\")\n",
    "print(f\"  New train: {dict(train_dist)}\")\n",
    "print(f\"  Validation: {dict(val_dist)}\")\n",
    "print(f\"\\n  Original ratio: {original_dist[1]/sum(original_dist.values()):.2%} positive\")\n",
    "print(f\"  Train ratio: {train_dist[1]/sum(train_dist.values()):.2%} positive\")\n",
    "print(f\"  Val ratio: {val_dist[1]/sum(val_dist.values()):.2%} positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Method 3: Create custom splits using select\n# (numpy was imported at the top of the notebook)\n\n# Shuffle indices\nnp.random.seed(42)\nindices = np.random.permutation(len(train_data))\n\n# Split 80/10/10\ntrain_size = int(0.8 * len(indices))\nval_size = int(0.1 * len(indices))\n\ntrain_idx = indices[:train_size]\nval_idx = indices[train_size:train_size + val_size]\ntest_idx = indices[train_size + val_size:]\n\n# Create splits\ncustom_train = train_data.select(train_idx)\ncustom_val = train_data.select(val_idx)\ncustom_test = train_data.select(test_idx)\n\n# Combine into DatasetDict\nfull_dataset = DatasetDict({\n    'train': custom_train,\n    'validation': custom_val,\n    'test': custom_test\n})\n\nprint(\"Custom 80/10/10 split:\")\nprint(full_dataset)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Working with Large Datasets (Streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming: Process data without downloading everything\n",
    "# Perfect for datasets too large to fit on disk!\n",
    "\n",
    "# Stream a HUGE dataset\n",
    "streaming_ds = load_dataset(\n",
    "    \"HuggingFaceFW/fineweb\",  # A 15+ TB dataset!\n",
    "    name=\"sample-10BT\",\n",
    "    split=\"train\",\n",
    "    streaming=True  # Magic flag!\n",
    ")\n",
    "\n",
    "print(\"Streaming dataset created (no download yet!)\")\n",
    "print(f\"Type: {type(streaming_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through streaming dataset\n",
    "# Data is downloaded on-demand!\n",
    "\n",
    "print(\"First 3 examples from streaming dataset:\\n\")\n",
    "for i, example in enumerate(streaming_ds):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(f\"  Text: {example['text'][:100]}...\")\n",
    "    print()\n",
    "    if i >= 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operations on streaming datasets\n",
    "# They're applied lazily!\n",
    "\n",
    "# Filter and map work on streaming datasets\n",
    "processed_stream = (\n",
    "    streaming_ds\n",
    "    .filter(lambda x: len(x['text']) > 100)  # Only long texts\n",
    "    .map(lambda x: {'word_count': len(x['text'].split()), **x})\n",
    "    .take(5)  # Take first 5 that pass filter\n",
    ")\n",
    "\n",
    "print(\"Processed streaming results:\\n\")\n",
    "for example in processed_stream:\n",
    "    print(f\"  Word count: {example['word_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Streaming\n",
    "\n",
    "| Situation | Use Streaming? |\n",
    "|-----------|----------------|\n",
    "| Dataset fits in memory | No - regular loading is faster |\n",
    "| Dataset > disk space | Yes! |\n",
    "| Only need a sample | Yes |\n",
    "| Need random access | No |\n",
    "| Training with shuffling | Depends (use shuffle buffer) |\n",
    "\n",
    "**DGX Spark Note:** With 128GB memory, you can load datasets that would crash most systems. Streaming is still useful for truly massive datasets like The Pile, FineWeb, or RedPajama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Complete Processing Pipeline\n",
    "\n",
    "Let's build a complete pipeline for preparing the IMDB dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPLETE DATASET PROCESSING PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Step 1: Load dataset\n",
    "print(\"\\n[1/5] Loading dataset...\")\n",
    "raw_dataset = load_dataset(\"imdb\")\n",
    "print(f\"      Loaded {len(raw_dataset['train']):,} train, {len(raw_dataset['test']):,} test\")\n",
    "\n",
    "# Step 2: Create validation split\n",
    "print(\"\\n[2/5] Creating validation split...\")\n",
    "train_val = raw_dataset['train'].train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=42,\n",
    "    stratify_by_column='label'\n",
    ")\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_val['train'],\n",
    "    'validation': train_val['test'],\n",
    "    'test': raw_dataset['test']\n",
    "})\n",
    "print(f\"      Train: {len(dataset['train']):,}\")\n",
    "print(f\"      Validation: {len(dataset['validation']):,}\")\n",
    "print(f\"      Test: {len(dataset['test']):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Tokenize\n",
    "print(\"\\n[3/5] Tokenizing...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_batch(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=256,\n",
    "        return_tensors=None  # Return lists, not tensors\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_batch,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    "    remove_columns=['text'],\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "print(f\"      Columns: {tokenized_dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Format for PyTorch\n",
    "print(\"\\n[4/5] Formatting for PyTorch...\")\n",
    "\n",
    "# Rename label column to 'labels' (expected by HF Trainer)\n",
    "tokenized_dataset = tokenized_dataset.rename_column('label', 'labels')\n",
    "\n",
    "# Set format to PyTorch tensors\n",
    "tokenized_dataset.set_format(\n",
    "    type='torch',\n",
    "    columns=['input_ids', 'attention_mask', 'labels']\n",
    ")\n",
    "\n",
    "print(f\"      Format: PyTorch tensors\")\n",
    "print(f\"      Sample input_ids type: {type(tokenized_dataset['train'][0]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Verify and summarize\n",
    "print(\"\\n[5/5] Verification...\")\n",
    "\n",
    "# Check a sample\n",
    "sample = tokenized_dataset['train'][0]\n",
    "print(f\"      Sample shapes:\")\n",
    "print(f\"        input_ids: {sample['input_ids'].shape}\")\n",
    "print(f\"        attention_mask: {sample['attention_mask'].shape}\")\n",
    "print(f\"        labels: {sample['labels']}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"PROCESSING COMPLETE in {total_time:.1f}s\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nFinal dataset:\")\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Saving and Loading Processed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk\n",
    "save_path = \"./processed_imdb\"\n",
    "\n",
    "print(f\"Saving to {save_path}...\")\n",
    "tokenized_dataset.save_to_disk(save_path)\n",
    "print(\"Saved!\")\n",
    "\n",
    "# Check what was created\n",
    "import os\n",
    "for root, dirs, files in os.walk(save_path):\n",
    "    level = root.replace(save_path, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files[:5]:  # Show first 5 files\n",
    "        print(f'{subindent}{file}')\n",
    "    if len(files) > 5:\n",
    "        print(f'{subindent}... and {len(files)-5} more files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from disk (much faster than reprocessing!)\n",
    "from datasets import load_from_disk\n",
    "\n",
    "print(\"Loading from disk...\")\n",
    "start = time.time()\n",
    "loaded_dataset = load_from_disk(save_path)\n",
    "load_time = time.time() - start\n",
    "\n",
    "print(f\"Loaded in {load_time:.2f}s!\")\n",
    "print(loaded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to Hub (requires HF login)\n",
    "# tokenized_dataset.push_to_hub(\"username/imdb-bert-tokenized\")\n",
    "\n",
    "print(\"To push to Hub:\")\n",
    "print(\"1. Run: huggingface-cli login\")\n",
    "print(\"2. Then: dataset.push_to_hub('your-username/dataset-name')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Process the AG News Dataset\n",
    "\n",
    "Build a complete processing pipeline for the AG News dataset:\n",
    "1. Load `ag_news` dataset\n",
    "2. Create train/validation split (90/10)\n",
    "3. Tokenize with `distilbert-base-uncased`\n",
    "4. Add a column for text length\n",
    "5. Filter out very short texts (< 10 words)\n",
    "6. Save the processed dataset\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "```python\n",
    "# AG News has 'text' and 'label' columns\n",
    "ag_news = load_dataset(\"ag_news\")\n",
    "\n",
    "# AG News labels: 0=World, 1=Sports, 2=Business, 3=Sci/Tech\n",
    "print(ag_news['train'].features)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Process the AG News dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Using Batched Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Processing one at a time (slow!)\n",
    "# def slow_tokenize(example):\n",
    "#     return tokenizer(example['text'], truncation=True)\n",
    "# dataset.map(slow_tokenize)  # Very slow!\n",
    "\n",
    "# CORRECT: Batched processing\n",
    "# def fast_tokenize(examples):\n",
    "#     return tokenizer(examples['text'], truncation=True)\n",
    "# dataset.map(fast_tokenize, batched=True)  # Much faster!\n",
    "\n",
    "print(\"Always use batched=True for tokenization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Forgetting to Remove Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Keeping original text after tokenization wastes memory\n",
    "# tokenized = dataset.map(tokenize_fn, batched=True)\n",
    "# Result: columns = ['text', 'label', 'input_ids', 'attention_mask']\n",
    "\n",
    "# CORRECT: Remove original text\n",
    "# tokenized = dataset.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "# Result: columns = ['label', 'input_ids', 'attention_mask']\n",
    "\n",
    "print(\"Use remove_columns to save memory after processing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Not Setting the Format for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Trying to use dataset directly with PyTorch\n",
    "# for batch in DataLoader(dataset):  # May fail or be slow!\n",
    "\n",
    "# CORRECT: Set format first\n",
    "# dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "# for batch in DataLoader(dataset):  # Works great!\n",
    "\n",
    "print(\"Call set_format('torch', ...) before creating DataLoader!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ✅ How to load datasets from the Hub\n",
    "- ✅ How to use map(), filter(), and select() operations\n",
    "- ✅ How to process data efficiently with batching and multiprocessing\n",
    "- ✅ How to create train/validation/test splits (including stratified)\n",
    "- ✅ How to use streaming for massive datasets\n",
    "- ✅ How to save and load processed datasets\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge: Multi-Dataset Pipeline\n",
    "\n",
    "Create a processing pipeline that:\n",
    "1. Loads IMDB, Yelp, and Amazon reviews\n",
    "2. Standardizes them to same format\n",
    "3. Combines into one dataset\n",
    "4. Creates balanced splits\n",
    "5. Tokenizes with consistent settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE\n",
    "# Hint: Use concatenate_datasets from datasets\n",
    "# from datasets import concatenate_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Datasets Documentation](https://huggingface.co/docs/datasets)\n",
    "- [Processing Data Tutorial](https://huggingface.co/docs/datasets/process)\n",
    "- [Streaming Datasets](https://huggingface.co/docs/datasets/stream)\n",
    "- [Dataset Performance Tips](https://huggingface.co/docs/datasets/about_cache)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up saved files\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(\"./processed_imdb\"):\n",
    "    shutil.rmtree(\"./processed_imdb\")\n",
    "    print(\"Cleaned up processed_imdb directory\")\n",
    "\n",
    "# Clear memory\n",
    "import gc\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, **04-trainer-finetuning.ipynb**, we'll use our processed datasets to actually train models using the Hugging Face Trainer API!\n",
    "\n",
    "Great job completing Task 9.3! You now know how to efficiently process datasets for any ML task!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}