{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 10.6: LLaMA Factory Exploration\n",
    "\n",
    "**Module:** 10 - Large Language Model Fine-Tuning  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** â­â­â˜†â˜†â˜†\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Install and configure LLaMA Factory\n",
    "- [ ] Use the web UI to configure fine-tuning\n",
    "- [ ] Monitor training progress through the interface\n",
    "- [ ] Compare GUI vs script-based workflows\n",
    "- [ ] Understand when to use each approach\n",
    "\n",
    "---\n",
    "\n",
    "## What is LLaMA Factory?\n",
    "\n",
    "**LLaMA Factory** is an open-source project that provides a unified, easy-to-use interface for fine-tuning LLMs. It supports:\n",
    "\n",
    "- Multiple training methods (SFT, DPO, RLHF, PPO)\n",
    "- Many model families (Llama, Mistral, Qwen, etc.)\n",
    "- LoRA, QLoRA, and full fine-tuning\n",
    "- Web UI and command-line interfaces\n",
    "- Built-in evaluation and export\n",
    "\n",
    "**Why use it?**\n",
    "- Visual configuration with no coding\n",
    "- Preset configurations for common tasks\n",
    "- Real-time training monitoring\n",
    "- Great for beginners and quick experiments\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELI5: Why Use a GUI for Fine-Tuning?\n",
    "\n",
    "> **Imagine learning to drive:**\n",
    ">\n",
    "> **Script-based training** is like learning to drive a manual transmission. You have full control over every gear shift, but you need to understand clutch timing, engine RPM, and coordination. Powerful, but steep learning curve.\n",
    ">\n",
    "> **LLaMA Factory** is like driving an automatic. The car handles the complex mechanics while you focus on where you want to go. You might sacrifice some control, but you get there faster and with less stress.\n",
    ">\n",
    "> **When to use which:**\n",
    "> - **GUI (LLaMA Factory)**: Quick experiments, learning, prototyping\n",
    "> - **Scripts**: Production pipelines, custom requirements, maximum control\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Installation\n",
    "\n",
    "Let's install LLaMA Factory on your DGX Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation commands\n",
    "# Run these in your terminal, not in the notebook\n",
    "\n",
    "installation_commands = \"\"\"\n",
    "# Step 1: Clone the repository\n",
    "git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
    "cd LLaMA-Factory\n",
    "\n",
    "# Step 2: Create conda environment (recommended)\n",
    "conda create -n llama_factory python=3.10\n",
    "conda activate llama_factory\n",
    "\n",
    "# Step 3: Install dependencies\n",
    "pip install -e \".[torch,metrics]\"\n",
    "\n",
    "# Step 4: Verify installation\n",
    "python -c \"import llamafactory; print('LLaMA Factory installed successfully!')\"\n",
    "\n",
    "# Step 5: Launch the web UI\n",
    "llamafactory-cli webui\n",
    "\"\"\"\n",
    "\n",
    "print(\"LLaMA Factory Installation Commands:\")\n",
    "print(\"=\"*50)\n",
    "print(installation_commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Alternative: Docker installation (recommended for DGX Spark)\ndocker_commands = \"\"\"\n# Option A: Using Docker (cleaner, isolated)\n\n# Pull the image\ndocker pull hiyouga/llama-factory:latest\n\n# Run with GPU support\ndocker run --gpus all -it --rm \\\\\n    -v $HOME/workspace:/workspace \\\\\n    -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\\\n    -p 7860:7860 \\\\\n    --ipc=host \\\\\n    hiyouga/llama-factory:latest \\\\\n    llamafactory-cli webui --host 0.0.0.0\n\n# Notes:\n# --gpus all      : Required for GPU access\n# --ipc=host      : Required for DataLoader multiprocessing (prevents shared memory errors)\n# -p 7860:7860    : Web UI port mapping\n\n# Access the web UI at: http://localhost:7860\n\n# ============================================================\n# DGX SPARK ARM64 COMPATIBILITY NOTE\n# ============================================================\n# DGX Spark uses ARM64 (aarch64) architecture. If you encounter \n# architecture-related issues with the LLaMA Factory Docker image:\n#\n# Option 1: Use the NGC PyTorch container + pip install\n#   docker run --gpus all -it --rm \\\\\n#       -v $HOME/workspace:/workspace \\\\\n#       -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\\\n#       -p 7860:7860 --ipc=host \\\\\n#       nvcr.io/nvidia/pytorch:25.03-py3 bash\n#   \n#   # Inside container:\n#   pip install llamafactory[torch,metrics]\n#   llamafactory-cli webui --host 0.0.0.0\n#\n# Option 2: Build from source with ARM64 base image\n#   git clone https://github.com/hiyouga/LLaMA-Factory.git\n#   cd LLaMA-Factory\n#   # Modify Dockerfile to use ARM64-compatible base image\n#   docker build -t llama-factory-arm64 .\n#\n# The NGC container approach (Option 1) is recommended for DGX Spark\n# as it includes ARM64-optimized PyTorch and CUDA libraries.\n\"\"\"\n\nprint(\"Docker Installation (Recommended for DGX Spark):\")\nprint(\"=\"*50)\nprint(docker_commands)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Web UI Overview\n",
    "\n",
    "The LLaMA Factory web UI has several main sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web UI Components\n",
    "ui_components = \"\"\"\n",
    "LLAMA FACTORY WEB UI - MAIN COMPONENTS\n",
    "======================================\n",
    "\n",
    "1. MODEL SELECTION\n",
    "   â”œâ”€â”€ Model Name: Choose from supported models (Llama, Mistral, Qwen, etc.)\n",
    "   â”œâ”€â”€ Model Path: Load from HuggingFace Hub or local path\n",
    "   â””â”€â”€ Quantization: None, 4-bit, 8-bit options\n",
    "\n",
    "2. TRAINING METHOD\n",
    "   â”œâ”€â”€ SFT (Supervised Fine-Tuning): Standard instruction tuning\n",
    "   â”œâ”€â”€ LoRA/QLoRA: Efficient adapter training\n",
    "   â”œâ”€â”€ DPO: Preference optimization\n",
    "   â”œâ”€â”€ RLHF/PPO: Reinforcement learning from human feedback\n",
    "   â””â”€â”€ Full: Full parameter fine-tuning\n",
    "\n",
    "3. DATASET\n",
    "   â”œâ”€â”€ Built-in datasets: Alpaca, ShareGPT, etc.\n",
    "   â”œâ”€â”€ Custom datasets: Upload your own\n",
    "   â””â”€â”€ Dataset preview: See examples before training\n",
    "\n",
    "4. TRAINING ARGUMENTS\n",
    "   â”œâ”€â”€ Epochs, Batch Size, Learning Rate\n",
    "   â”œâ”€â”€ LoRA Config (rank, alpha, target modules)\n",
    "   â”œâ”€â”€ Optimizer settings\n",
    "   â””â”€â”€ Memory optimization options\n",
    "\n",
    "5. TRAINING TAB\n",
    "   â”œâ”€â”€ Start/Stop training\n",
    "   â”œâ”€â”€ Real-time loss curves\n",
    "   â”œâ”€â”€ Memory usage monitoring\n",
    "   â””â”€â”€ Checkpoint management\n",
    "\n",
    "6. CHAT TAB\n",
    "   â”œâ”€â”€ Test trained models interactively\n",
    "   â”œâ”€â”€ Compare base vs fine-tuned\n",
    "   â””â”€â”€ Adjust generation parameters\n",
    "\n",
    "7. EXPORT TAB\n",
    "   â”œâ”€â”€ Merge LoRA weights\n",
    "   â”œâ”€â”€ Export to GGUF (for Ollama)\n",
    "   â””â”€â”€ Push to HuggingFace Hub\n",
    "\"\"\"\n",
    "\n",
    "print(ui_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Screenshot Walkthrough\n",
    "\n",
    "The LLaMA Factory UI looks like this:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  LLaMA Factory                        [Train] [Chat] [Export]   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Model Settings                                                 â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚\n",
    "â”‚  â”‚ Model: â–¼        â”‚  â”‚ Finetuning: â–¼   â”‚                      â”‚\n",
    "â”‚  â”‚ Llama-3.1-8B    â”‚  â”‚ LoRA            â”‚                      â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Dataset: [alpaca_en â–¼]          Quantization: [4-bit â–¼]       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Training Arguments                                             â”‚\n",
    "â”‚  Learning Rate: [2e-4]     Epochs: [3]      Batch Size: [2]    â”‚\n",
    "â”‚  LoRA Rank: [16]           Alpha: [32]      Dropout: [0.05]    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  [â–¶ Start Training]                         GPU Mem: 45.2 GB   â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Training Progress â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  85%        â”‚\n",
    "â”‚  Loss: 0.423 | Step: 850/1000 | ETA: 5:30                      â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  ğŸ“ˆ Loss Curve                                                  â”‚\n",
    "â”‚  1.5 â”‚\\                                                        â”‚\n",
    "â”‚  1.0 â”‚ \\___                                                    â”‚\n",
    "â”‚  0.5 â”‚     \\____                                               â”‚\n",
    "â”‚  0.0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚\n",
    "â”‚      0   250  500  750  1000                                   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Configuration Walkthrough\n",
    "\n",
    "Let's go through a complete fine-tuning workflow step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-step configuration for fine-tuning Llama 3.1 8B with LoRA\n",
    "config_walkthrough = \"\"\"\n",
    "FINE-TUNING WALKTHROUGH: Llama 3.1 8B with LoRA\n",
    "================================================\n",
    "\n",
    "STEP 1: MODEL CONFIGURATION\n",
    "---------------------------\n",
    "1. Open the web UI (http://localhost:7860)\n",
    "2. In \"Model Name\", select: Llama-3.1-8B-Instruct\n",
    "3. Set \"Finetuning Method\": LoRA\n",
    "4. Set \"Quantization\": 4-bit (for memory efficiency)\n",
    "5. Leave \"Adapter Path\" empty (we're creating a new adapter)\n",
    "\n",
    "STEP 2: DATASET CONFIGURATION  \n",
    "-----------------------------\n",
    "1. Click \"Dataset\" dropdown\n",
    "2. Select a built-in dataset OR:\n",
    "   - Click \"Upload\" to add your own\n",
    "   - Format: JSON or JSONL with \"instruction\", \"input\", \"output\" fields\n",
    "3. Set \"Dataset Split\": train (or specify train/val split)\n",
    "4. Preview data to verify formatting\n",
    "\n",
    "STEP 3: TRAINING ARGUMENTS\n",
    "--------------------------\n",
    "Recommended settings for DGX Spark:\n",
    "\n",
    "| Setting              | Value     | Notes                          |\n",
    "|---------------------|-----------|--------------------------------|\n",
    "| Learning Rate       | 2e-4      | Standard for LoRA              |\n",
    "| Epochs              | 3         | Adjust based on dataset size   |\n",
    "| Batch Size          | 4         | Can increase on DGX Spark      |\n",
    "| Gradient Accum      | 4         | Effective batch = 16           |\n",
    "| Max Sequence Length | 2048      | Llama 3.1 supports up to 131K  |\n",
    "| LoRA Rank (r)       | 16        | Balance of efficiency/quality  |\n",
    "| LoRA Alpha          | 32        | Usually 2x rank                |\n",
    "| LoRA Dropout        | 0.05      | Light regularization           |\n",
    "| Target Modules      | All attn  | q,k,v,o projections           |\n",
    "\n",
    "STEP 4: ADVANCED OPTIONS\n",
    "------------------------\n",
    "- Gradient Checkpointing: ON (saves memory)\n",
    "- BF16: ON (native Blackwell support)\n",
    "- Flash Attention: ON (faster training)\n",
    "- DeepSpeed: OFF (single GPU on DGX Spark)\n",
    "\n",
    "STEP 5: START TRAINING\n",
    "----------------------\n",
    "1. Review all settings\n",
    "2. Click \"Start Training\"\n",
    "3. Monitor loss curve and GPU memory\n",
    "4. Wait for training to complete\n",
    "\n",
    "STEP 6: TESTING (CHAT TAB)\n",
    "--------------------------\n",
    "1. Switch to \"Chat\" tab\n",
    "2. Load your trained adapter\n",
    "3. Test with sample prompts\n",
    "4. Compare with base model\n",
    "\n",
    "STEP 7: EXPORT (EXPORT TAB)\n",
    "---------------------------\n",
    "1. Switch to \"Export\" tab\n",
    "2. Choose export format:\n",
    "   - Merge LoRA: For standard deployment\n",
    "   - GGUF: For Ollama/llama.cpp\n",
    "   - HuggingFace Hub: For sharing\n",
    "3. Configure export settings\n",
    "4. Export and verify\n",
    "\"\"\"\n",
    "\n",
    "print(config_walkthrough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Custom Dataset Upload\n",
    "\n",
    "Let's prepare a custom dataset for LLaMA Factory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a sample dataset in LLaMA Factory format\n",
    "custom_dataset = [\n",
    "    {\n",
    "        \"instruction\": \"Explain the concept of machine learning in simple terms.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Machine learning is like teaching a computer to learn from examples. Instead of programming every rule, you show the computer many examples and it figures out the patterns. For instance, to recognize cats in photos, you show it thousands of cat pictures until it learns what makes a cat look like a cat.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is the difference between AI and machine learning?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"AI (Artificial Intelligence) is the broad goal of making computers think and act intelligently. Machine learning is one way to achieve AI - specifically, by having computers learn from data rather than being explicitly programmed. Think of AI as the destination and machine learning as one popular route to get there.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Write a Python function based on the following description.\",\n",
    "        \"input\": \"Create a function that takes a list of numbers and returns only the even ones.\",\n",
    "        \"output\": \"\"\"def get_even_numbers(numbers):\n",
    "    \\\"\\\"\\\"Return only the even numbers from a list.\\\"\\\"\\\"\n",
    "    return [num for num in numbers if num % 2 == 0]\n",
    "\n",
    "# Example usage:\n",
    "# get_even_numbers([1, 2, 3, 4, 5, 6])  # Returns [2, 4, 6]\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Summarize the following text.\",\n",
    "        \"input\": \"The transformer architecture was introduced in the paper 'Attention is All You Need' in 2017. It uses self-attention mechanisms to process sequences in parallel, which is more efficient than the recurrent approach used by LSTMs and GRUs. Transformers have become the foundation for modern language models like BERT, GPT, and their successors.\",\n",
    "        \"output\": \"Transformers, introduced in 2017, use self-attention for efficient parallel sequence processing, replacing recurrent networks and becoming the foundation for models like BERT and GPT.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What are the main components of a neural network?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"The main components of a neural network are: 1) Input layer - receives the data, 2) Hidden layers - process the data through weighted connections and activation functions, 3) Output layer - produces the final prediction, 4) Weights and biases - learnable parameters, and 5) Activation functions - introduce non-linearity (like ReLU, sigmoid). These components work together to learn patterns from data.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# LLaMA Factory also needs a dataset_info.json file\n",
    "dataset_info = {\n",
    "    \"custom_data\": {\n",
    "        \"file_name\": \"custom_data.json\",\n",
    "        \"columns\": {\n",
    "            \"prompt\": \"instruction\",\n",
    "            \"query\": \"input\",\n",
    "            \"response\": \"output\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save files (in actual use, save to LLaMA Factory's data directory)\n",
    "print(\"Sample dataset for LLaMA Factory:\")\n",
    "print(json.dumps(custom_dataset[0], indent=2))\n",
    "print(\"\\ndataset_info.json entry:\")\n",
    "print(json.dumps(dataset_info, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions for adding custom dataset to LLaMA Factory\n",
    "custom_dataset_instructions = \"\"\"\n",
    "ADDING CUSTOM DATASET TO LLAMA FACTORY\n",
    "======================================\n",
    "\n",
    "METHOD 1: Through Web UI\n",
    "------------------------\n",
    "1. Click \"Dataset\" dropdown\n",
    "2. Click \"Upload Dataset\"\n",
    "3. Upload your JSON/JSONL file\n",
    "4. Select the dataset in the dropdown\n",
    "\n",
    "METHOD 2: Manual Addition\n",
    "-------------------------\n",
    "1. Place your data file in: LLaMA-Factory/data/\n",
    "2. Edit data/dataset_info.json to add your dataset:\n",
    "\n",
    "   {\n",
    "     \"my_dataset\": {\n",
    "       \"file_name\": \"my_data.json\",\n",
    "       \"columns\": {\n",
    "         \"prompt\": \"instruction\",\n",
    "         \"query\": \"input\",\n",
    "         \"response\": \"output\"\n",
    "       }\n",
    "     }\n",
    "   }\n",
    "\n",
    "3. Restart the web UI\n",
    "4. Your dataset appears in the dropdown\n",
    "\n",
    "SUPPORTED FORMATS\n",
    "-----------------\n",
    "1. Alpaca: {\"instruction\": ..., \"input\": ..., \"output\": ...}\n",
    "2. ShareGPT: {\"conversations\": [{\"from\": \"human\", \"value\": ...}, ...]}\n",
    "3. Custom: Specify column mapping in dataset_info.json\n",
    "\n",
    "TIPS\n",
    "----\n",
    "- Keep examples consistent in length and quality\n",
    "- Aim for 100+ examples minimum\n",
    "- Include diverse instructions\n",
    "- Preview data before training\n",
    "\"\"\"\n",
    "\n",
    "print(custom_dataset_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Command Line Interface\n",
    "\n",
    "LLaMA Factory also supports command-line training for scripting and automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLI training example\n",
    "cli_training = \"\"\"\n",
    "LLAMA FACTORY CLI TRAINING\n",
    "==========================\n",
    "\n",
    "Basic training command:\n",
    "------------------------\n",
    "llamafactory-cli train \\\\\n",
    "    --stage sft \\\\\n",
    "    --model_name_or_path meta-llama/Llama-3.1-8B-Instruct \\\\\n",
    "    --dataset alpaca_en \\\\\n",
    "    --finetuning_type lora \\\\\n",
    "    --quantization_bit 4 \\\\\n",
    "    --lora_rank 16 \\\\\n",
    "    --lora_alpha 32 \\\\\n",
    "    --lora_target q_proj,v_proj \\\\\n",
    "    --output_dir ./output/llama3-8b-lora \\\\\n",
    "    --per_device_train_batch_size 4 \\\\\n",
    "    --gradient_accumulation_steps 4 \\\\\n",
    "    --lr_scheduler_type cosine \\\\\n",
    "    --learning_rate 2e-4 \\\\\n",
    "    --num_train_epochs 3 \\\\\n",
    "    --bf16 \\\\\n",
    "    --logging_steps 10\n",
    "\n",
    "DPO training:\n",
    "-------------\n",
    "llamafactory-cli train \\\\\n",
    "    --stage dpo \\\\\n",
    "    --model_name_or_path meta-llama/Llama-3.1-8B-Instruct \\\\\n",
    "    --adapter_name_or_path ./output/sft-adapter \\\\\n",
    "    --dataset dpo_preference_data \\\\\n",
    "    --finetuning_type lora \\\\\n",
    "    --output_dir ./output/dpo-adapter \\\\\n",
    "    --dpo_beta 0.1\n",
    "\n",
    "Export to GGUF:\n",
    "---------------\n",
    "llamafactory-cli export \\\\\n",
    "    --model_name_or_path meta-llama/Llama-3.1-8B-Instruct \\\\\n",
    "    --adapter_name_or_path ./output/llama3-8b-lora \\\\\n",
    "    --export_dir ./output/merged \\\\\n",
    "    --export_size 2 \\\\\n",
    "    --export_quantization_bit 4 \\\\\n",
    "    --export_legacy_format false\n",
    "\n",
    "Chat with model:\n",
    "----------------\n",
    "llamafactory-cli chat \\\\\n",
    "    --model_name_or_path meta-llama/Llama-3.1-8B-Instruct \\\\\n",
    "    --adapter_name_or_path ./output/llama3-8b-lora \\\\\n",
    "    --finetuning_type lora \\\\\n",
    "    --quantization_bit 4\n",
    "\"\"\"\n",
    "\n",
    "print(cli_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAML configuration for reproducible training\n",
    "yaml_config = \"\"\"\n",
    "# Save this as: train_config.yaml\n",
    "# Run with: llamafactory-cli train train_config.yaml\n",
    "\n",
    "### Model Configuration ###\n",
    "model_name_or_path: meta-llama/Llama-3.1-8B-Instruct\n",
    "trust_remote_code: true\n",
    "\n",
    "### Training Method ###\n",
    "stage: sft\n",
    "finetuning_type: lora\n",
    "\n",
    "### Quantization ###\n",
    "quantization_bit: 4\n",
    "quantization_method: bitsandbytes\n",
    "\n",
    "### Dataset ###\n",
    "dataset: alpaca_en\n",
    "template: llama3\n",
    "cutoff_len: 2048\n",
    "\n",
    "### LoRA Configuration ###\n",
    "lora_rank: 16\n",
    "lora_alpha: 32\n",
    "lora_dropout: 0.05\n",
    "lora_target: q_proj,k_proj,v_proj,o_proj\n",
    "\n",
    "### Training Arguments ###\n",
    "output_dir: ./output/llama3-8b-sft\n",
    "per_device_train_batch_size: 4\n",
    "gradient_accumulation_steps: 4\n",
    "learning_rate: 2e-4\n",
    "num_train_epochs: 3\n",
    "lr_scheduler_type: cosine\n",
    "warmup_ratio: 0.1\n",
    "\n",
    "### Memory Optimization ###\n",
    "bf16: true\n",
    "gradient_checkpointing: true\n",
    "\n",
    "### Logging ###\n",
    "logging_steps: 10\n",
    "save_steps: 100\n",
    "save_total_limit: 3\n",
    "\"\"\"\n",
    "\n",
    "print(\"YAML Configuration Example:\")\n",
    "print(\"=\"*50)\n",
    "print(yaml_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Comparison - GUI vs Scripts\n",
    "\n",
    "Let's compare the two approaches systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = \"\"\"\n",
    "GUI (LLAMA FACTORY) vs SCRIPT-BASED TRAINING\n",
    "=============================================\n",
    "\n",
    "LLAMA FACTORY WEB UI\n",
    "--------------------\n",
    "Pros:\n",
    "  âœ… No coding required\n",
    "  âœ… Visual configuration with validation\n",
    "  âœ… Real-time training monitoring\n",
    "  âœ… Built-in chat testing\n",
    "  âœ… One-click export to multiple formats\n",
    "  âœ… Presets for common configurations\n",
    "  âœ… Great for learning and prototyping\n",
    "\n",
    "Cons:\n",
    "  âŒ Less flexibility for custom training loops\n",
    "  âŒ Harder to integrate into CI/CD pipelines\n",
    "  âŒ Limited access to advanced features\n",
    "  âŒ Requires running web server\n",
    "  âŒ Debugging can be harder\n",
    "\n",
    "SCRIPT-BASED (CUSTOM CODE)\n",
    "--------------------------\n",
    "Pros:\n",
    "  âœ… Maximum control and flexibility\n",
    "  âœ… Easy to version control\n",
    "  âœ… Integrates with CI/CD and MLOps\n",
    "  âœ… Custom training loops possible\n",
    "  âœ… Advanced debugging and profiling\n",
    "  âœ… Reproducible experiments\n",
    "\n",
    "Cons:\n",
    "  âŒ Requires Python/ML expertise\n",
    "  âŒ More boilerplate code\n",
    "  âŒ No visual monitoring (need external tools)\n",
    "  âŒ Manual configuration management\n",
    "\n",
    "WHEN TO USE EACH\n",
    "----------------\n",
    "\n",
    "Use LLaMA Factory when:\n",
    "  â€¢ Learning fine-tuning for the first time\n",
    "  â€¢ Quick experiments and prototyping\n",
    "  â€¢ You want visual feedback during training\n",
    "  â€¢ Standard fine-tuning (SFT, LoRA, DPO) is sufficient\n",
    "  â€¢ Non-programmers need to run training\n",
    "\n",
    "Use Scripts when:\n",
    "  â€¢ Production pipelines and automation\n",
    "  â€¢ Custom training logic required\n",
    "  â€¢ Integration with existing MLOps tools\n",
    "  â€¢ Need maximum control over every aspect\n",
    "  â€¢ Running on headless servers (no GUI)\n",
    "  â€¢ Reproducibility is critical\n",
    "\n",
    "HYBRID APPROACH (RECOMMENDED)\n",
    "-----------------------------\n",
    "1. Use LLaMA Factory UI for initial experiments\n",
    "2. Export the configuration once satisfied\n",
    "3. Use CLI/scripts for production training\n",
    "4. Keep LLaMA Factory for quick testing\n",
    "\"\"\"\n",
    "\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Troubleshooting Common Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "troubleshooting = \"\"\"\n",
    "LLAMA FACTORY TROUBLESHOOTING\n",
    "=============================\n",
    "\n",
    "ISSUE: Out of Memory (OOM)\n",
    "--------------------------\n",
    "Solutions:\n",
    "1. Reduce batch size to 1\n",
    "2. Enable gradient checkpointing\n",
    "3. Use 4-bit quantization\n",
    "4. Reduce max sequence length\n",
    "5. Use smaller LoRA rank (8 instead of 16)\n",
    "6. Clear buffer cache before starting\n",
    "\n",
    "ISSUE: Web UI Won't Start\n",
    "-------------------------\n",
    "Solutions:\n",
    "1. Check port 7860 is available\n",
    "2. Try: llamafactory-cli webui --host 0.0.0.0 --port 8080\n",
    "3. Verify all dependencies installed\n",
    "4. Check CUDA is accessible\n",
    "\n",
    "ISSUE: Model Not Showing in Dropdown\n",
    "------------------------------------\n",
    "Solutions:\n",
    "1. Check you have HuggingFace access (for gated models)\n",
    "2. Set HF_TOKEN environment variable\n",
    "3. Verify model name is correct\n",
    "4. Try using local model path\n",
    "\n",
    "ISSUE: Training Loss Not Decreasing\n",
    "-----------------------------------\n",
    "Solutions:\n",
    "1. Increase learning rate (try 5e-4)\n",
    "2. Check dataset quality\n",
    "3. Increase LoRA rank\n",
    "4. Verify template matches model\n",
    "5. Train for more epochs\n",
    "\n",
    "ISSUE: Export/GGUF Conversion Fails\n",
    "-----------------------------------\n",
    "Solutions:\n",
    "1. Install llama-cpp-python\n",
    "2. Merge LoRA first, then convert\n",
    "3. Check disk space (70B needs ~140GB)\n",
    "4. Use correct quantization format\n",
    "\n",
    "ISSUE: Chat Responses Are Strange\n",
    "---------------------------------\n",
    "Solutions:\n",
    "1. Verify correct chat template\n",
    "2. Check adapter is properly loaded\n",
    "3. Try different generation parameters\n",
    "4. Ensure training completed successfully\n",
    "\"\"\"\n",
    "\n",
    "print(troubleshooting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Exercises\n",
    "\n",
    "### Exercise 1: Setup and Explore\n",
    "\n",
    "1. Install LLaMA Factory\n",
    "2. Launch the web UI\n",
    "3. Explore all tabs and options\n",
    "4. Document 5 features you find interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Document your findings\n",
    "llama_factory_features = [\n",
    "    # 1. ...\n",
    "    # 2. ...\n",
    "    # 3. ...\n",
    "    # 4. ...\n",
    "    # 5. ...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Train with GUI\n",
    "\n",
    "1. Create a custom dataset (at least 20 examples)\n",
    "2. Upload it to LLaMA Factory\n",
    "3. Configure LoRA training\n",
    "4. Monitor training\n",
    "5. Test the result in the Chat tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- âœ… How to install and launch LLaMA Factory\n",
    "- âœ… Web UI components and configuration\n",
    "- âœ… How to add custom datasets\n",
    "- âœ… CLI and YAML-based training\n",
    "- âœ… Comparison of GUI vs script approaches\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [LLaMA Factory GitHub](https://github.com/hiyouga/LLaMA-Factory)\n",
    "- [LLaMA Factory Documentation](https://github.com/hiyouga/LLaMA-Factory/wiki)\n",
    "- [Supported Models](https://github.com/hiyouga/LLaMA-Factory/blob/main/README.md#supported-models)\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "**[Task 10.7: Ollama Integration](07-ollama-integration.ipynb)**\n",
    "\n",
    "Learn to deploy your fine-tuned model to Ollama for easy local inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Cleanup",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cleanup (if you ran training in this session)\nimport gc\ngc.collect()\n\n# If you have GPU resources to clear:\ntry:\n    import torch\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        print(\"GPU cache cleared\")\nexcept ImportError:\n    pass\n\nprint(\"Cleanup complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}