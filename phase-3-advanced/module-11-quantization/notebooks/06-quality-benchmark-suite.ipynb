{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Task 11.6: Quality Benchmark Suite\n\n**Module:** 11 - Model Quantization & Optimization  \n**Time:** 2 hours  \n**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ\n\n---\n\n## üéØ Learning Objectives\n\nBy the end of this notebook, you will:\n- [ ] Build a comprehensive benchmark suite for quantized models\n- [ ] Measure perplexity across multiple datasets\n- [ ] Evaluate task-specific accuracy (MMLU, HellaSwag)\n- [ ] Compare all quantization methods systematically\n- [ ] Create publication-quality comparison tables and visualizations\n\n---\n\n## üìö Prerequisites\n\n- Completed: Tasks 11.1-11.5 (all quantization notebooks)\n- Models: Quantized models from previous notebooks (or we'll create them)\n- Hardware: DGX Spark with 128GB unified memory\n\n### ‚ö†Ô∏è Optional Dependencies (for full comparison)\n\nFor complete GPTQ and AWQ benchmarking, you should have completed:\n- **Task 11.2** (`02-gptq-quantization.ipynb`) ‚Üí Creates `./quantized_models/opt-350m-gptq-4bit-g128`\n- **Task 11.3** (`03-awq-quantization.ipynb`) ‚Üí Creates `./quantized_models/opt-350m-awq-4bit-g128`\n\n**Note:** This notebook will still work without these models! It will benchmark FP16, INT8, and INT4 (bitsandbytes), and gracefully skip GPTQ/AWQ if not available.\n\n---\n\n## üåç Real-World Context\n\n**The Problem:** You've created multiple quantized versions of your model. Which one should you deploy?\n\n**The Answer:** It depends on your priorities!\n- **Maximum quality** ‚Üí FP16 or Q8\n- **Best balance** ‚Üí Q4_K_M (GGUF) or AWQ\n- **Maximum compression** ‚Üí NVFP4 (Blackwell) or Q2_K\n- **Task-specific** ‚Üí Benchmark on YOUR task!\n\nThis notebook gives you the tools to make data-driven decisions.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßí ELI5: Why Benchmarking Matters\n",
    "\n",
    "> **Imagine you're buying a car...**\n",
    ">\n",
    "> You could just look at the price tag. But smart buyers check:\n",
    "> - **Fuel efficiency** (like model size)\n",
    "> - **Horsepower** (like inference speed)\n",
    "> - **Safety rating** (like model quality)\n",
    "> - **How it handles YOUR roads** (like task-specific accuracy)\n",
    ">\n",
    "> A sports car might be \"best\" on a track but terrible for city driving.\n",
    "> Similarly, the \"best\" quantization depends on YOUR use case!\n",
    ">\n",
    "> **In AI terms:** A comprehensive benchmark tests multiple dimensions so you can pick the right model for YOUR deployment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting Up the Benchmark Suite\n",
    "\n",
    "We'll create a modular benchmark system that can evaluate any model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Optional, Callable\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DGX Spark Benchmark Suite\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install lm-eval for comprehensive benchmarking\ntry:\n    import lm_eval\n    print(f\"‚úÖ lm-eval version: {lm_eval.__version__}\")\n    \n    # Check minimum version (API changed significantly in 0.4.0)\n    from packaging import version\n    if version.parse(lm_eval.__version__) < version.parse(\"0.4.0\"):\n        print(f\"‚ö†Ô∏è  Warning: lm-eval version {lm_eval.__version__} is older than 0.4.0\")\n        print(\"   Some APIs may differ. Consider upgrading: pip install --upgrade lm-eval\")\nexcept ImportError:\n    print(\"Installing lm-eval...\")\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"lm-eval\", \"--quiet\"], check=True)\n    import lm_eval\n    print(f\"‚úÖ lm-eval installed: {lm_eval.__version__}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass BenchmarkResult:\n    \"\"\"Store benchmark results for a single model.\"\"\"\n    model_name: str\n    quantization_type: str\n    model_size_mb: float\n    perplexity: Optional[float] = None\n    tokens_per_second: Optional[float] = None\n    memory_used_gb: Optional[float] = None\n    task_scores: Dict[str, float] = field(default_factory=dict)\n    metadata: Dict[str, any] = field(default_factory=dict)\n    \n    def compression_ratio(self, baseline_size_mb: float) -> float:\n        \"\"\"Calculate compression ratio vs baseline.\"\"\"\n        return baseline_size_mb / self.model_size_mb\n    \n    def to_dict(self) -> dict:\n        \"\"\"Convert to dictionary for DataFrame.\"\"\"\n        result = {\n            'Model': self.model_name,\n            'Quantization': self.quantization_type,\n            'Size (MB)': self.model_size_mb,\n            'Perplexity': self.perplexity,\n            'Tokens/s': self.tokens_per_second,\n            'Memory (GB)': self.memory_used_gb,\n        }\n        result.update(self.task_scores)\n        return result\n\n\nclass BenchmarkSuite:\n    \"\"\"Comprehensive benchmark suite for quantized models.\"\"\"\n    \n    def __init__(self, baseline_model_id: str):\n        self.baseline_model_id = baseline_model_id\n        self.results: List[BenchmarkResult] = []\n        self.tokenizer = None\n        \n    def _load_tokenizer(self):\n        \"\"\"Load tokenizer if not already loaded.\"\"\"\n        if self.tokenizer is None:\n            from transformers import AutoTokenizer\n            self.tokenizer = AutoTokenizer.from_pretrained(self.baseline_model_id)\n            if self.tokenizer.pad_token is None:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n        return self.tokenizer\n    \n    def calculate_perplexity(\n        self, \n        model, \n        texts: List[str], \n        max_length: int = 512\n    ) -> float:\n        \"\"\"Calculate perplexity on a list of texts.\"\"\"\n        tokenizer = self._load_tokenizer()\n        model.eval()\n        \n        total_loss = 0\n        total_tokens = 0\n        \n        with torch.no_grad():\n            for text in tqdm(texts, desc=\"Perplexity\", leave=False):\n                encodings = tokenizer(\n                    text,\n                    return_tensors='pt',\n                    truncation=True,\n                    max_length=max_length\n                )\n                input_ids = encodings.input_ids.to(model.device)\n                \n                if input_ids.size(1) < 2:\n                    continue\n                \n                outputs = model(input_ids, labels=input_ids)\n                loss = outputs.loss.item()\n                num_tokens = input_ids.size(1) - 1\n                \n                total_loss += loss * num_tokens\n                total_tokens += num_tokens\n        \n        return math.exp(total_loss / total_tokens) if total_tokens > 0 else float('inf')\n    \n    def benchmark_speed(\n        self, \n        model, \n        prompt: str = \"The future of artificial intelligence is\",\n        num_tokens: int = 50,\n        num_runs: int = 5\n    ) -> float:\n        \"\"\"Benchmark generation speed.\"\"\"\n        tokenizer = self._load_tokenizer()\n        model.eval()\n        \n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n        \n        # Warmup\n        with torch.no_grad():\n            _ = model.generate(**inputs, max_new_tokens=10, do_sample=False)\n        \n        torch.cuda.synchronize()\n        \n        times = []\n        for _ in range(num_runs):\n            torch.cuda.synchronize()\n            start = time.perf_counter()\n            \n            with torch.no_grad():\n                _ = model.generate(\n                    **inputs,\n                    max_new_tokens=num_tokens,\n                    do_sample=False,\n                    pad_token_id=tokenizer.pad_token_id\n                )\n            \n            torch.cuda.synchronize()\n            times.append(time.perf_counter() - start)\n        \n        avg_time = sum(times) / len(times)\n        return num_tokens / avg_time\n    \n    def add_result(self, result: BenchmarkResult):\n        \"\"\"Add a benchmark result.\"\"\"\n        self.results.append(result)\n    \n    def to_dataframe(self) -> pd.DataFrame:\n        \"\"\"Convert results to pandas DataFrame.\"\"\"\n        return pd.DataFrame([r.to_dict() for r in self.results])\n    \n    def plot_comparison(self, save_path: str = None):\n        \"\"\"Create comparison visualization.\"\"\"\n        df = self.to_dataframe()\n        \n        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n        \n        colors = plt.cm.tab10(np.linspace(0, 1, len(df)))\n        \n        # Size comparison\n        ax = axes[0, 0]\n        ax.barh(df['Quantization'], df['Size (MB)'], color=colors)\n        ax.set_xlabel('Size (MB)')\n        ax.set_title('Model Size')\n        ax.invert_yaxis()\n        \n        # Perplexity comparison\n        ax = axes[0, 1]\n        if 'Perplexity' in df.columns and df['Perplexity'].notna().any():\n            ax.barh(df['Quantization'], df['Perplexity'], color=colors)\n            ax.set_xlabel('Perplexity (lower is better)')\n            ax.set_title('Model Quality')\n            ax.invert_yaxis()\n        else:\n            ax.text(0.5, 0.5, 'No perplexity data', ha='center', va='center')\n        \n        # Speed comparison\n        ax = axes[1, 0]\n        if 'Tokens/s' in df.columns and df['Tokens/s'].notna().any():\n            ax.barh(df['Quantization'], df['Tokens/s'], color=colors)\n            ax.set_xlabel('Tokens/second')\n            ax.set_title('Inference Speed')\n            ax.invert_yaxis()\n        else:\n            ax.text(0.5, 0.5, 'No speed data', ha='center', va='center')\n        \n        # Efficiency (quality/size)\n        ax = axes[1, 1]\n        if 'Perplexity' in df.columns and df['Perplexity'].notna().any():\n            baseline_size = df['Size (MB)'].max()\n            efficiency = (baseline_size / df['Size (MB)']) / df['Perplexity']\n            ax.barh(df['Quantization'], efficiency, color=colors)\n            ax.set_xlabel('Efficiency Score (higher is better)')\n            ax.set_title('Quality/Size Efficiency')\n            ax.invert_yaxis()\n        else:\n            ax.text(0.5, 0.5, 'No efficiency data', ha='center', va='center')\n        \n        plt.tight_layout()\n        \n        if save_path:\n            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n        \n        plt.show()\n        plt.close(fig)  # Free memory from figure\n\n    def print_summary(self):\n        \"\"\"Print summary table.\"\"\"\n        df = self.to_dataframe()\n        print(\"\\n\" + \"=\"*80)\n        print(\"BENCHMARK SUMMARY\")\n        print(\"=\"*80)\n        print(df.to_string(index=False))\n        print(\"=\"*80)\n\n\nprint(\"Benchmark suite classes defined!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Evaluation Datasets\n",
    "\n",
    "We'll use multiple evaluation datasets for comprehensive benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diverse evaluation texts for perplexity\n",
    "PERPLEXITY_EVAL_TEXTS = [\n",
    "    # General knowledge\n",
    "    \"The history of human civilization spans thousands of years, from ancient Mesopotamia to modern times.\",\n",
    "    \"The solar system contains eight planets orbiting around the Sun, each with unique characteristics.\",\n",
    "    \"Climate change is caused by the accumulation of greenhouse gases in the Earth's atmosphere.\",\n",
    "    \n",
    "    # Technical/Scientific\n",
    "    \"Machine learning algorithms can be categorized into supervised, unsupervised, and reinforcement learning.\",\n",
    "    \"DNA molecules carry genetic information through sequences of nucleotide bases.\",\n",
    "    \"Quantum mechanics describes the behavior of particles at the atomic and subatomic level.\",\n",
    "    \n",
    "    # Creative/Literary\n",
    "    \"The old lighthouse stood on the cliff, its beam cutting through the fog like a sword of light.\",\n",
    "    \"She walked through the autumn forest, leaves crunching beneath her feet like whispered secrets.\",\n",
    "    \"The city never sleeps; its streets pulse with the rhythm of a million heartbeats.\",\n",
    "    \n",
    "    # Factual/News-like\n",
    "    \"The stock market experienced significant volatility as investors reacted to economic indicators.\",\n",
    "    \"New research published today suggests a breakthrough in renewable energy technology.\",\n",
    "    \"The international conference brought together leaders from over fifty countries.\",\n",
    "    \n",
    "    # Conversational\n",
    "    \"How are you doing today? I hope everything is going well with your projects.\",\n",
    "    \"Could you please explain how to solve this problem step by step?\",\n",
    "    \"That's a great question! Let me think about the best way to answer it.\",\n",
    "    \n",
    "    # Code-like (for code models)\n",
    "    \"The function takes two parameters and returns their sum after validation.\",\n",
    "    \"Import the necessary libraries and initialize the model with default parameters.\",\n",
    "    \"Error handling is crucial for robust software development and user experience.\",\n",
    "    \n",
    "    # More diverse topics\n",
    "    \"The recipe calls for flour, sugar, eggs, and butter mixed in specific proportions.\",\n",
    "    \"The game ended with a dramatic last-minute goal that shocked everyone in the stadium.\",\n",
    "    \"Music has the power to evoke emotions and connect people across cultures.\",\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(PERPLEXITY_EVAL_TEXTS)} evaluation texts for perplexity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-specific evaluation using lm-eval\n",
    "BENCHMARK_TASKS = [\n",
    "    \"hellaswag\",      # Common sense reasoning\n",
    "    \"arc_easy\",       # Science questions (easy)\n",
    "    \"winogrande\",     # Commonsense reasoning\n",
    "    # \"mmlu\",         # Massive Multitask Language Understanding (slow!)\n",
    "]\n",
    "\n",
    "print(f\"Will evaluate on tasks: {BENCHMARK_TASKS}\")\n",
    "print(\"\\nNote: MMLU is commented out by default as it takes a long time.\")\n",
    "print(\"Uncomment it for comprehensive evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Running the Benchmark\n",
    "\n",
    "Let's benchmark multiple quantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Model to benchmark\n",
    "MODEL_ID = \"facebook/opt-350m\"  # Small for demo; use larger for real benchmarks\n",
    "\n",
    "# Initialize benchmark suite\n",
    "suite = BenchmarkSuite(MODEL_ID)\n",
    "\n",
    "print(f\"Benchmarking model: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def get_model_size(model) -> float:\n    \"\"\"\n    Calculate model size in MB based on parameter count and data types.\n\n    Args:\n        model: PyTorch model to measure\n\n    Returns:\n        Size in megabytes (MB)\n    \"\"\"\n    param_bytes = sum(\n        p.numel() * p.element_size() for p in model.parameters()\n    )\n    return param_bytes / 1e6\n\n\ndef get_memory_usage() -> float:\n    \"\"\"\n    Get current GPU memory usage in GB.\n\n    Returns:\n        Memory allocated on GPU in gigabytes (GB), or 0 if no GPU\n    \"\"\"\n    if torch.cuda.is_available():\n        return torch.cuda.memory_allocated() / 1e9\n    return 0\n\n\ndef clear_memory():\n    \"\"\"Clear GPU memory cache and run garbage collection.\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n\nprint(\"Utility functions defined!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark 1: FP16 Baseline\n",
    "print(\"=\"*60)\n",
    "print(\"Benchmarking FP16 Baseline\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "clear_memory()\n",
    "\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "\n",
    "# Measure size and memory\n",
    "fp16_size = get_model_size(model_fp16)\n",
    "fp16_memory = get_memory_usage()\n",
    "\n",
    "# Calculate perplexity\n",
    "print(\"Calculating perplexity...\")\n",
    "fp16_ppl = suite.calculate_perplexity(model_fp16, PERPLEXITY_EVAL_TEXTS)\n",
    "\n",
    "# Benchmark speed\n",
    "print(\"Benchmarking speed...\")\n",
    "fp16_speed = suite.benchmark_speed(model_fp16)\n",
    "\n",
    "# Store results\n",
    "suite.add_result(BenchmarkResult(\n",
    "    model_name=MODEL_ID,\n",
    "    quantization_type=\"FP16\",\n",
    "    model_size_mb=fp16_size,\n",
    "    perplexity=fp16_ppl,\n",
    "    tokens_per_second=fp16_speed,\n",
    "    memory_used_gb=fp16_memory\n",
    "))\n",
    "\n",
    "print(f\"\\nFP16 Results:\")\n",
    "print(f\"  Size: {fp16_size:.1f} MB\")\n",
    "print(f\"  Perplexity: {fp16_ppl:.2f}\")\n",
    "print(f\"  Speed: {fp16_speed:.1f} tok/s\")\n",
    "print(f\"  Memory: {fp16_memory:.2f} GB\")\n",
    "\n",
    "del model_fp16\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark 2: INT8 (bitsandbytes)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Benchmarking INT8 (bitsandbytes)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    \n",
    "    clear_memory()\n",
    "    \n",
    "    int8_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    \n",
    "    model_int8 = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        quantization_config=int8_config,\n",
    "        device_map=\"cuda\"\n",
    "    )\n",
    "    \n",
    "    int8_size = fp16_size / 2  # Approximate\n",
    "    int8_memory = get_memory_usage()\n",
    "    \n",
    "    print(\"Calculating perplexity...\")\n",
    "    int8_ppl = suite.calculate_perplexity(model_int8, PERPLEXITY_EVAL_TEXTS)\n",
    "    \n",
    "    print(\"Benchmarking speed...\")\n",
    "    int8_speed = suite.benchmark_speed(model_int8)\n",
    "    \n",
    "    suite.add_result(BenchmarkResult(\n",
    "        model_name=MODEL_ID,\n",
    "        quantization_type=\"INT8\",\n",
    "        model_size_mb=int8_size,\n",
    "        perplexity=int8_ppl,\n",
    "        tokens_per_second=int8_speed,\n",
    "        memory_used_gb=int8_memory\n",
    "    ))\n",
    "    \n",
    "    print(f\"\\nINT8 Results:\")\n",
    "    print(f\"  Size: {int8_size:.1f} MB\")\n",
    "    print(f\"  Perplexity: {int8_ppl:.2f} (+{int8_ppl - fp16_ppl:.2f})\")\n",
    "    print(f\"  Speed: {int8_speed:.1f} tok/s\")\n",
    "    print(f\"  Memory: {int8_memory:.2f} GB\")\n",
    "    \n",
    "    del model_int8\n",
    "    clear_memory()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"INT8 benchmark skipped: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark 3: INT4 (bitsandbytes NF4)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Benchmarking INT4/NF4 (bitsandbytes)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    clear_memory()\n",
    "    \n",
    "    int4_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    \n",
    "    model_int4 = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        quantization_config=int4_config,\n",
    "        device_map=\"cuda\"\n",
    "    )\n",
    "    \n",
    "    int4_size = fp16_size / 4  # Approximate\n",
    "    int4_memory = get_memory_usage()\n",
    "    \n",
    "    print(\"Calculating perplexity...\")\n",
    "    int4_ppl = suite.calculate_perplexity(model_int4, PERPLEXITY_EVAL_TEXTS)\n",
    "    \n",
    "    print(\"Benchmarking speed...\")\n",
    "    int4_speed = suite.benchmark_speed(model_int4)\n",
    "    \n",
    "    suite.add_result(BenchmarkResult(\n",
    "        model_name=MODEL_ID,\n",
    "        quantization_type=\"INT4/NF4\",\n",
    "        model_size_mb=int4_size,\n",
    "        perplexity=int4_ppl,\n",
    "        tokens_per_second=int4_speed,\n",
    "        memory_used_gb=int4_memory\n",
    "    ))\n",
    "    \n",
    "    print(f\"\\nINT4/NF4 Results:\")\n",
    "    print(f\"  Size: {int4_size:.1f} MB\")\n",
    "    print(f\"  Perplexity: {int4_ppl:.2f} (+{int4_ppl - fp16_ppl:.2f})\")\n",
    "    print(f\"  Speed: {int4_speed:.1f} tok/s\")\n",
    "    print(f\"  Memory: {int4_memory:.2f} GB\")\n",
    "    \n",
    "    del model_int4\n",
    "    clear_memory()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"INT4 benchmark skipped: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark 4: GPTQ (if available from previous notebook)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Benchmarking GPTQ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import os\n",
    "gptq_path = \"./quantized_models/opt-350m-gptq-4bit-g128\"\n",
    "\n",
    "if os.path.exists(gptq_path):\n",
    "    try:\n",
    "        from auto_gptq import AutoGPTQForCausalLM\n",
    "        \n",
    "        clear_memory()\n",
    "        \n",
    "        model_gptq = AutoGPTQForCausalLM.from_quantized(\n",
    "            gptq_path,\n",
    "            device=\"cuda:0\",\n",
    "            use_safetensors=True\n",
    "        )\n",
    "        \n",
    "        # Get actual model size from files\n",
    "        gptq_size = sum(\n",
    "            os.path.getsize(os.path.join(gptq_path, f))\n",
    "            for f in os.listdir(gptq_path)\n",
    "            if f.endswith('.safetensors') or f.endswith('.bin')\n",
    "        ) / 1e6\n",
    "        gptq_memory = get_memory_usage()\n",
    "        \n",
    "        print(\"Calculating perplexity...\")\n",
    "        gptq_ppl = suite.calculate_perplexity(model_gptq, PERPLEXITY_EVAL_TEXTS)\n",
    "        \n",
    "        print(\"Benchmarking speed...\")\n",
    "        gptq_speed = suite.benchmark_speed(model_gptq)\n",
    "        \n",
    "        suite.add_result(BenchmarkResult(\n",
    "            model_name=MODEL_ID,\n",
    "            quantization_type=\"GPTQ-4bit\",\n",
    "            model_size_mb=gptq_size,\n",
    "            perplexity=gptq_ppl,\n",
    "            tokens_per_second=gptq_speed,\n",
    "            memory_used_gb=gptq_memory\n",
    "        ))\n",
    "        \n",
    "        print(f\"\\nGPTQ Results:\")\n",
    "        print(f\"  Size: {gptq_size:.1f} MB\")\n",
    "        print(f\"  Perplexity: {gptq_ppl:.2f} (+{gptq_ppl - fp16_ppl:.2f})\")\n",
    "        print(f\"  Speed: {gptq_speed:.1f} tok/s\")\n",
    "        print(f\"  Memory: {gptq_memory:.2f} GB\")\n",
    "        \n",
    "        del model_gptq\n",
    "        clear_memory()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"GPTQ benchmark failed: {e}\")\n",
    "else:\n",
    "    print(f\"GPTQ model not found at {gptq_path}\")\n",
    "    print(\"Run notebook 02 first to create GPTQ models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark 5: AWQ (if available from previous notebook)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Benchmarking AWQ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "awq_path = \"./quantized_models/opt-350m-awq-4bit-g128\"\n",
    "\n",
    "if os.path.exists(awq_path):\n",
    "    try:\n",
    "        from awq import AutoAWQForCausalLM\n",
    "        \n",
    "        clear_memory()\n",
    "        \n",
    "        model_awq = AutoAWQForCausalLM.from_quantized(\n",
    "            awq_path,\n",
    "            fuse_layers=True\n",
    "        )\n",
    "        \n",
    "        awq_size = sum(\n",
    "            os.path.getsize(os.path.join(awq_path, f))\n",
    "            for f in os.listdir(awq_path)\n",
    "            if f.endswith('.safetensors') or f.endswith('.bin')\n",
    "        ) / 1e6\n",
    "        awq_memory = get_memory_usage()\n",
    "        \n",
    "        print(\"Calculating perplexity...\")\n",
    "        awq_ppl = suite.calculate_perplexity(model_awq, PERPLEXITY_EVAL_TEXTS)\n",
    "        \n",
    "        print(\"Benchmarking speed...\")\n",
    "        awq_speed = suite.benchmark_speed(model_awq)\n",
    "        \n",
    "        suite.add_result(BenchmarkResult(\n",
    "            model_name=MODEL_ID,\n",
    "            quantization_type=\"AWQ-4bit\",\n",
    "            model_size_mb=awq_size,\n",
    "            perplexity=awq_ppl,\n",
    "            tokens_per_second=awq_speed,\n",
    "            memory_used_gb=awq_memory\n",
    "        ))\n",
    "        \n",
    "        print(f\"\\nAWQ Results:\")\n",
    "        print(f\"  Size: {awq_size:.1f} MB\")\n",
    "        print(f\"  Perplexity: {awq_ppl:.2f} (+{awq_ppl - fp16_ppl:.2f})\")\n",
    "        print(f\"  Speed: {awq_speed:.1f} tok/s\")\n",
    "        print(f\"  Memory: {awq_memory:.2f} GB\")\n",
    "        \n",
    "        del model_awq\n",
    "        clear_memory()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"AWQ benchmark failed: {e}\")\n",
    "else:\n",
    "    print(f\"AWQ model not found at {awq_path}\")\n",
    "    print(\"Run notebook 03 first to create AWQ models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Results Analysis\n",
    "\n",
    "Let's analyze and visualize our benchmark results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary table\n",
    "suite.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create visualization\nsuite.plot_comparison('benchmark_comparison.png')\nplt.close('all')  # Free memory from figures"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis\n",
    "df = suite.to_dataframe()\n",
    "\n",
    "if len(df) > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DETAILED ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    baseline = df[df['Quantization'] == 'FP16'].iloc[0]\n",
    "    \n",
    "    print(f\"\\nBaseline (FP16):\")\n",
    "    print(f\"  Size: {baseline['Size (MB)']:.1f} MB\")\n",
    "    print(f\"  Perplexity: {baseline['Perplexity']:.2f}\")\n",
    "    print(f\"  Speed: {baseline['Tokens/s']:.1f} tok/s\")\n",
    "    \n",
    "    print(f\"\\nCompression Analysis:\")\n",
    "    for _, row in df.iterrows():\n",
    "        if row['Quantization'] != 'FP16':\n",
    "            compression = baseline['Size (MB)'] / row['Size (MB)']\n",
    "            ppl_delta = row['Perplexity'] - baseline['Perplexity']\n",
    "            speed_ratio = row['Tokens/s'] / baseline['Tokens/s']\n",
    "            \n",
    "            print(f\"\\n{row['Quantization']}:\")\n",
    "            print(f\"  Compression: {compression:.1f}x\")\n",
    "            print(f\"  Perplexity increase: +{ppl_delta:.2f} ({ppl_delta/baseline['Perplexity']*100:.1f}%)\")\n",
    "            print(f\"  Speed ratio: {speed_ratio:.2f}x\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RECOMMENDATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if len(df) > 1:\n",
    "        # Best quality (lowest perplexity after FP16)\n",
    "        quant_only = df[df['Quantization'] != 'FP16']\n",
    "        if len(quant_only) > 0:\n",
    "            best_quality = quant_only.loc[quant_only['Perplexity'].idxmin()]\n",
    "            print(f\"\\nüèÜ Best Quality: {best_quality['Quantization']}\")\n",
    "            print(f\"   Perplexity: {best_quality['Perplexity']:.2f}\")\n",
    "            \n",
    "            # Best compression\n",
    "            best_compression = quant_only.loc[quant_only['Size (MB)'].idxmin()]\n",
    "            print(f\"\\nüíæ Best Compression: {best_compression['Quantization']}\")\n",
    "            print(f\"   Size: {best_compression['Size (MB)']:.1f} MB ({baseline['Size (MB)']/best_compression['Size (MB)']:.1f}x)\")\n",
    "            \n",
    "            # Best speed\n",
    "            best_speed = quant_only.loc[quant_only['Tokens/s'].idxmax()]\n",
    "            print(f\"\\n‚ö° Best Speed: {best_speed['Quantization']}\")\n",
    "            print(f\"   Speed: {best_speed['Tokens/s']:.1f} tok/s\")\n",
    "            \n",
    "            # Best balance (efficiency)\n",
    "            quant_only['Efficiency'] = (baseline['Size (MB)'] / quant_only['Size (MB)']) / quant_only['Perplexity'] * 100\n",
    "            best_balance = quant_only.loc[quant_only['Efficiency'].idxmax()]\n",
    "            print(f\"\\n‚öñÔ∏è  Best Balance: {best_balance['Quantization']}\")\n",
    "            print(f\"   Efficiency score: {best_balance['Efficiency']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create comprehensive comparison chart\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\ndf = suite.to_dataframe()\n\nif len(df) > 0:\n    baseline_ppl = df[df['Quantization'] == 'FP16']['Perplexity'].values[0]\n    baseline_size = df[df['Quantization'] == 'FP16']['Size (MB)'].values[0]\n    \n    # Scatter: Size vs Perplexity\n    ax = axes[0]\n    scatter = ax.scatter(\n        df['Size (MB)'], \n        df['Perplexity'], \n        s=200, \n        c=range(len(df)), \n        cmap='viridis',\n        alpha=0.7\n    )\n    \n    for i, row in df.iterrows():\n        ax.annotate(\n            row['Quantization'],\n            (row['Size (MB)'], row['Perplexity']),\n            textcoords=\"offset points\",\n            xytext=(0, 10),\n            ha='center'\n        )\n    \n    ax.set_xlabel('Model Size (MB)')\n    ax.set_ylabel('Perplexity (lower is better)')\n    ax.set_title('Size vs Quality Trade-off')\n    ax.grid(True, alpha=0.3)\n    \n    # Pareto frontier line\n    sorted_df = df.sort_values('Size (MB)')\n    pareto = []\n    min_ppl = float('inf')\n    for _, row in sorted_df.iterrows():\n        if row['Perplexity'] < min_ppl:\n            pareto.append(row)\n            min_ppl = row['Perplexity']\n    if len(pareto) > 1:\n        pareto_df = pd.DataFrame(pareto)\n        ax.plot(pareto_df['Size (MB)'], pareto_df['Perplexity'], 'r--', alpha=0.5, label='Pareto frontier')\n        ax.legend()\n    \n    # Bar chart: Relative comparison\n    ax = axes[1]\n    \n    x = np.arange(len(df))\n    width = 0.35\n    \n    # Normalize to percentages\n    size_pct = df['Size (MB)'] / baseline_size * 100\n    ppl_pct = df['Perplexity'] / baseline_ppl * 100\n    \n    bars1 = ax.bar(x - width/2, size_pct, width, label='Size (% of FP16)', color='steelblue')\n    bars2 = ax.bar(x + width/2, ppl_pct, width, label='Perplexity (% of FP16)', color='coral')\n    \n    ax.axhline(y=100, color='gray', linestyle='--', alpha=0.5)\n    ax.set_ylabel('Percentage of FP16 Baseline')\n    ax.set_title('Relative Performance')\n    ax.set_xticks(x)\n    ax.set_xticklabels(df['Quantization'], rotation=45, ha='right')\n    ax.legend()\n    \n    plt.tight_layout()\n    plt.savefig('benchmark_detailed.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    plt.close(fig)  # Free memory from figure"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Benchmark a Larger Model\n",
    "\n",
    "Run the full benchmark suite on Llama-2-7B or Mistral-7B.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "```python\n",
    "MODEL_ID = \"meta-llama/Llama-2-7b-hf\"\n",
    "suite = BenchmarkSuite(MODEL_ID)\n",
    "# Run all benchmarks...\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Benchmark a larger model\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Task-Specific Evaluation\n",
    "\n",
    "Use lm-eval to evaluate on specific tasks (HellaSwag, ARC, etc.).\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "```python\n",
    "from lm_eval import evaluator\n",
    "\n",
    "results = evaluator.simple_evaluate(\n",
    "    model=\"hf\",\n",
    "    model_args=f\"pretrained={model_path}\",\n",
    "    tasks=[\"hellaswag\"],\n",
    "    batch_size=8\n",
    ")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run task-specific evaluation\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Only Using Perplexity\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Only perplexity\n",
    "if ppl < 10:\n",
    "    deploy(model)\n",
    "\n",
    "# ‚úÖ Right: Multiple metrics\n",
    "if ppl < 10 and accuracy > 0.8 and speed > 20:\n",
    "    deploy(model)\n",
    "```\n",
    "\n",
    "**Why:** Perplexity doesn't capture task-specific performance.\n",
    "\n",
    "### Mistake 2: Not Testing on Target Task\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: General benchmarks only\n",
    "results = evaluate_on_hellaswag(model)  # For a coding assistant?\n",
    "\n",
    "# ‚úÖ Right: Evaluate on your use case\n",
    "results = evaluate_on_code_completion(model)\n",
    "```\n",
    "\n",
    "**Why:** A model that's great at common sense may be terrible at code.\n",
    "\n",
    "### Mistake 3: Ignoring Memory During Benchmark\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Not clearing between models\n",
    "model1 = load(\"model1\")  # Uses 8GB\n",
    "model2 = load(\"model2\")  # Now using 16GB total!\n",
    "\n",
    "# ‚úÖ Right: Clear memory between benchmarks\n",
    "model1 = load(\"model1\")\n",
    "benchmark(model1)\n",
    "del model1\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "model2 = load(\"model2\")\n",
    "```\n",
    "\n",
    "**Why:** Memory fragmentation affects benchmark accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "\n",
    "- ‚úÖ **Comprehensive benchmarking**: Size, speed, quality, efficiency\n",
    "- ‚úÖ **Multiple metrics**: Perplexity isn't everything\n",
    "- ‚úÖ **Visualization**: Clear comparison charts\n",
    "- ‚úÖ **Data-driven decisions**: Choose based on YOUR priorities\n",
    "- ‚úÖ **Production readiness**: Benchmark like you deploy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Build an Automated Model Selection Pipeline**\n",
    "\n",
    "Create a function that:\n",
    "1. Takes constraints (max size, min quality, min speed)\n",
    "2. Benchmarks all available quantization methods\n",
    "3. Returns the best model for your constraints\n",
    "\n",
    "```python\n",
    "def select_best_model(\n",
    "    model_id: str,\n",
    "    max_size_mb: float = 1000,\n",
    "    max_ppl_increase: float = 0.5,\n",
    "    min_speed_tok_s: float = 20\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Automatically select the best quantization method.\n",
    "    \n",
    "    Returns: Path to the best model\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Further Reading\n",
    "\n",
    "- [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)\n",
    "- [HELM Benchmark](https://crfm.stanford.edu/helm/)\n",
    "- [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n",
    "- [Quantization Benchmarks Collection](https://huggingface.co/collections/quantization-benchmarks)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup\n",
    "clear_memory()\n",
    "\n",
    "print(\"Cleanup complete!\")\n",
    "print(f\"Final GPU memory: {get_memory_usage():.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Module Complete!\n",
    "\n",
    "Congratulations! You've completed **Module 11: Model Quantization & Optimization**.\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **Quantization Fundamentals** - Data types, precision, memory tradeoffs\n",
    "2. **GPTQ Quantization** - Hessian-based post-training quantization\n",
    "3. **AWQ Quantization** - Activation-aware weight quantization\n",
    "4. **GGUF Conversion** - llama.cpp compatibility\n",
    "5. **FP4 Deep Dive** - Blackwell exclusive quantization\n",
    "6. **Quality Benchmarking** - Comprehensive evaluation suite\n",
    "\n",
    "### Your DGX Spark Superpowers:\n",
    "\n",
    "- üöÄ Run 70B models with FP4 quantization\n",
    "- ‚ö° 3√ó prefill speedup with native FP4 tensor cores\n",
    "- üíæ 3.5√ó memory reduction with <1% quality loss\n",
    "- üéØ Data-driven quantization method selection\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "Continue to **Module 12: Inference Optimization** to learn about:\n",
    "- TensorRT deployment\n",
    "- Continuous batching\n",
    "- KV cache optimization\n",
    "- Production inference pipelines\n",
    "\n",
    "---\n",
    "\n",
    "*Happy quantizing! You're now a DGX Spark quantization expert!* üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}