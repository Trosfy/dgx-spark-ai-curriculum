{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Task 11.5: FP4 Deep Dive (Blackwell Exclusive!)\n\n**Module:** 11 - Model Quantization & Optimization  \n**Time:** 3 hours  \n**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n\n---\n\n## üéØ Learning Objectives\n\nBy the end of this notebook, you will:\n- [ ] Understand NVIDIA FP4 (NVFP4) format and its advantages\n- [ ] Learn about MXFP4 (Open Compute Project standard)\n- [ ] Apply FP4 quantization using TensorRT Model Optimizer\n- [ ] Achieve 3.5√ó memory reduction with <1% accuracy loss\n- [ ] Leverage DGX Spark's unique Blackwell FP4 tensor cores\n\n---\n\n## üìö Prerequisites\n\n- Completed: Tasks 11.1-11.4\n- Knowledge of: Previous quantization methods\n- Hardware: **DGX Spark required** (Blackwell GPU for FP4 tensor cores)\n\n### ‚ö†Ô∏è Container Requirements\n\nEnsure you're running in an NGC container with the required flags:\n\n```bash\ndocker run --gpus all -it --rm \\\n    -v $HOME/workspace:/workspace \\\n    -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\n    --ipc=host \\\n    nvcr.io/nvidia/pytorch:25.11-py3 \\\n    jupyter lab --ip=0.0.0.0 --allow-root --no-browser\n```\n\n**Important:** The `--ipc=host` flag is required for DataLoader with multiple workers!\n\n---\n\n## üåç Real-World Context\n\n### ‚≠ê This is Your DGX Spark Superpower!\n\n**The Blackwell Advantage:**\n- FP4 tensor cores are **exclusive to Blackwell architecture**\n- DGX Spark (GB10 Superchip) has **192 5th-gen tensor cores**\n- These tensor cores natively support FP4 computation\n- **1 PFLOP of FP4 performance** in a desktop form factor!\n\n| Feature | Previous GPUs | Blackwell (DGX Spark) |\n|---------|---------------|----------------------|\n| FP4 Support | ‚ùå Software emulation | ‚úÖ Hardware native |\n| Memory Reduction | ~2√ó (INT8) | ~3.5√ó (FP4) |\n| Quality Loss | Higher | <1% with proper calibration |\n| Speed | Baseline | ~3√ó faster prefill |\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßí ELI5: What is FP4?\n",
    "\n",
    "> **Imagine you're a music producer mixing a song...**\n",
    ">\n",
    "> **Standard approach (FP16):** Record everything in high quality\n",
    "> - Crystal clear audio, large files\n",
    "> - Perfect for the studio master\n",
    ">\n",
    "> **INT4 approach:** Convert to a simple digital format\n",
    "> - Like converting to MIDI - you get notes, but lose the nuance\n",
    "> - Each instrument becomes \"loud\" or \"quiet\" with few levels in between\n",
    ">\n",
    "> **FP4 approach:** Smart compression that keeps the dynamics\n",
    "> - Still 4 bits, but they're used more intelligently\n",
    "> - Quiet parts stay detailed, loud parts don't clip\n",
    "> - The \"floating point\" part means the 4 bits adapt to the signal!\n",
    ">\n",
    "> **In AI terms:** FP4 uses 4 bits like INT4, but the floating-point representation better captures the weight distribution in neural networks. With Blackwell's specialized hardware, this runs at full speed!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding FP4 Formats\n",
    "\n",
    "### NVFP4 vs MXFP4\n",
    "\n",
    "NVIDIA provides two FP4 formats:\n",
    "\n",
    "| Format | Description | Best For |\n",
    "|--------|-------------|----------|\n",
    "| **NVFP4** | NVIDIA's proprietary format with dual-level scaling | Maximum performance |\n",
    "| **MXFP4** | Open Compute Project standard (E2M1 with scaling) | Cross-platform compatibility |\n",
    "\n",
    "### FP4 Bit Layout\n",
    "\n",
    "```\n",
    "FP4 (E2M1): [S][E E][M]\n",
    "- S: Sign bit (1 bit)\n",
    "- E: Exponent (2 bits) ‚Üí 4 possible exponents\n",
    "- M: Mantissa (1 bit) ‚Üí 2 possible mantissa values\n",
    "\n",
    "This gives us 8 positive and 8 negative values!\n",
    "```\n",
    "\n",
    "### Dual-Level Scaling (NVFP4's Secret)\n",
    "\n",
    "The key innovation is **dual-level scaling**:\n",
    "1. **Block-level scale**: Shared across a block of weights\n",
    "2. **Sub-block scale**: Finer granularity within each block\n",
    "\n",
    "This allows FP4 to adapt to different weight magnitudes across the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DGX Spark / Blackwell Environment Check\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    # Check compute capability (Blackwell is 10.0+)\n",
    "    cc = torch.cuda.get_device_capability()\n",
    "    print(f\"Compute Capability: {cc[0]}.{cc[1]}\")\n",
    "    \n",
    "    if cc[0] >= 10:\n",
    "        print(\"\\n‚≠ê Blackwell GPU detected! FP4 tensor cores available!\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Non-Blackwell GPU detected. FP4 may run in emulation mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize FP4 representable values\n",
    "\n",
    "def get_fp4_values():\n",
    "    \"\"\"\n",
    "    Calculate all values representable by FP4 (E2M1) format.\n",
    "    \n",
    "    FP4 E2M1 format:\n",
    "    - 1 sign bit\n",
    "    - 2 exponent bits (bias = 1)\n",
    "    - 1 mantissa bit\n",
    "    \"\"\"\n",
    "    values = []\n",
    "    bias = 1\n",
    "    \n",
    "    for sign in [1, -1]:\n",
    "        for exp in range(4):  # 2 bits = 4 values\n",
    "            for mant in range(2):  # 1 bit = 2 values\n",
    "                if exp == 0:  # Subnormal\n",
    "                    value = sign * (mant / 2) * (2 ** (1 - bias))\n",
    "                else:  # Normal\n",
    "                    value = sign * (1 + mant / 2) * (2 ** (exp - bias))\n",
    "                values.append(value)\n",
    "    \n",
    "    return sorted(set(values))\n",
    "\n",
    "def get_int4_values():\n",
    "    \"\"\"INT4 symmetric values (-7 to 7).\"\"\"\n",
    "    return list(range(-7, 8))\n",
    "\n",
    "fp4_values = get_fp4_values()\n",
    "int4_values = get_int4_values()\n",
    "\n",
    "print(\"FP4 (E2M1) Representable Values:\")\n",
    "print(f\"  {fp4_values}\")\n",
    "print(f\"  Count: {len(fp4_values)}\")\n",
    "print(f\"  Range: [{min(fp4_values)}, {max(fp4_values)}]\")\n",
    "\n",
    "print(\"\\nINT4 Symmetric Values:\")\n",
    "print(f\"  {int4_values}\")\n",
    "print(f\"  Count: {len(int4_values)}\")\n",
    "print(f\"  Range: [{min(int4_values)}, {max(int4_values)}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize the difference between INT4 and FP4\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Plot representable values\nax = axes[0, 0]\nax.scatter(int4_values, [0]*len(int4_values), label='INT4', s=100, alpha=0.7)\nax.scatter(fp4_values, [0.1]*len(fp4_values), label='FP4 (E2M1)', s=100, alpha=0.7, marker='^')\nax.set_xlabel('Value')\nax.set_yticks([])\nax.set_title('Representable Values: INT4 vs FP4')\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Show density of representable values\nax = axes[0, 1]\nbins = np.linspace(-8, 8, 50)\nax.hist(int4_values, bins=bins, alpha=0.5, label='INT4', density=True)\nax.hist(fp4_values, bins=bins, alpha=0.5, label='FP4', density=True)\nax.set_xlabel('Value')\nax.set_ylabel('Density')\nax.set_title('Distribution of Representable Values')\nax.legend()\n\n# Quantization error comparison on Gaussian weights\nax = axes[1, 0]\ntorch.manual_seed(42)\nweights = torch.randn(10000) * 0.5  # Typical weight distribution\n\ndef quantize_to_nearest(values, grid):\n    \"\"\"Quantize values to nearest grid point.\"\"\"\n    grid = np.array(grid)\n    result = np.zeros_like(values)\n    for i, v in enumerate(values):\n        idx = np.argmin(np.abs(grid - v))\n        result[i] = grid[idx]\n    return result\n\n# Scale weights to fit quantization range\nweights_np = weights.numpy()\nscale_int4 = max(abs(weights_np.max()), abs(weights_np.min())) / 7\nscale_fp4 = max(abs(weights_np.max()), abs(weights_np.min())) / max(abs(max(fp4_values)), abs(min(fp4_values)))\n\n# Quantize\nint4_quant = quantize_to_nearest(weights_np / scale_int4, int4_values) * scale_int4\nfp4_quant = quantize_to_nearest(weights_np / scale_fp4, fp4_values) * scale_fp4\n\n# Compute errors\nint4_error = np.abs(weights_np - int4_quant)\nfp4_error = np.abs(weights_np - fp4_quant)\n\nax.hist(int4_error, bins=50, alpha=0.5, label=f'INT4 (mean={int4_error.mean():.4f})')\nax.hist(fp4_error, bins=50, alpha=0.5, label=f'FP4 (mean={fp4_error.mean():.4f})')\nax.set_xlabel('Absolute Error')\nax.set_ylabel('Count')\nax.set_title('Quantization Error Distribution')\nax.legend()\n\n# Error vs weight magnitude\nax = axes[1, 1]\nax.scatter(np.abs(weights_np), int4_error, alpha=0.1, s=1, label='INT4')\nax.scatter(np.abs(weights_np), fp4_error, alpha=0.1, s=1, label='FP4')\nax.set_xlabel('Weight Magnitude')\nax.set_ylabel('Absolute Error')\nax.set_title('Error vs Weight Magnitude')\nax.legend()\n\nplt.tight_layout()\nplt.savefig('fp4_vs_int4.png', dpi=150, bbox_inches='tight')\nplt.show()\nplt.close(fig)  # Free memory from figure\n\nprint(f\"\\nüìä Quantization Error Summary:\")\nprint(f\"   INT4 Mean Error: {int4_error.mean():.6f}\")\nprint(f\"   FP4 Mean Error:  {fp4_error.mean():.6f}\")\nprint(f\"   FP4 has {(1 - fp4_error.mean()/int4_error.mean())*100:.1f}% lower error!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Key Insight\n",
    "\n",
    "FP4 has **non-uniform spacing** between values:\n",
    "- More precision near zero (where most weights are)\n",
    "- Coarser precision for larger magnitudes\n",
    "\n",
    "This matches the typical weight distribution in neural networks!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Setting Up TensorRT Model Optimizer\n",
    "\n",
    "NVIDIA's TensorRT Model Optimizer (ModelOpt) is the official tool for FP4 quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install TensorRT Model Optimizer\n# Note: This should be pre-installed in the NGC container\n\ntry:\n    import modelopt.torch.quantization as mtq\n    from modelopt.torch.quantization import algorithms as quant_algo\n    print(\"‚úÖ TensorRT Model Optimizer is available!\")\nexcept ImportError:\n    print(\"‚ö†Ô∏è TensorRT Model Optimizer not found.\")\n    print(\"   On DGX Spark, ensure you're using the correct NGC container:\")\n    print(\"   nvcr.io/nvidia/pytorch:25.03-py3 or newer\")\n    print(\"\")\n    print(\"Attempting installation...\")\n    \n    try:\n        import subprocess\n        result = subprocess.run(\n            [\"pip\", \"install\", \"nvidia-modelopt[torch]\", \"--quiet\"],\n            capture_output=True,\n            text=True\n        )\n        if result.returncode == 0:\n            import modelopt.torch.quantization as mtq\n            from modelopt.torch.quantization import algorithms as quant_algo\n            print(\"‚úÖ Installation successful!\")\n        else:\n            raise ImportError(result.stderr)\n    except Exception as e:\n        print(f\"‚ùå Installation failed: {e}\")\n        print(\"\\nThis notebook requires ModelOpt for FP4 quantization.\")\n        print(\"Please use an NGC container with ModelOpt pre-installed, or install manually:\")\n        print(\"  pip install nvidia-modelopt[torch]\")\n        print(\"\\nThe notebook will continue but some features may not work.\")\n        mtq = None\n        quant_algo = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check ModelOpt version and available quantization configs\n",
    "\n",
    "# Guard against mtq being None (from failed import in previous cell)\n",
    "if mtq is None:\n",
    "    print(\"‚ö†Ô∏è  ModelOpt not available. Skipping config check.\")\n",
    "    print(\"   FP4/FP8 quantization features will not work in this session.\")\n",
    "    print(\"\\nüí° Solutions:\")\n",
    "    print(\"   1. Use NGC container: nvcr.io/nvidia/pytorch:25.03-py3 or newer\")\n",
    "    print(\"   2. Install manually: pip install nvidia-modelopt[torch]\")\n",
    "else:\n",
    "    import modelopt\n",
    "    print(f\"ModelOpt version: {modelopt.__version__}\")\n",
    "    print(\"\\nAvailable quantization algorithms:\")\n",
    "\n",
    "    available_configs = [\n",
    "        'INT8_SMOOTHQUANT_CFG',\n",
    "        'INT4_AWQ_CFG',\n",
    "        'FP8_DEFAULT_CFG',\n",
    "        'NVFP4_DEFAULT_CFG',\n",
    "        'MXFP4_DEFAULT_CFG',\n",
    "    ]\n",
    "\n",
    "    for config_name in available_configs:\n",
    "        try:\n",
    "            config = getattr(mtq, config_name, None)\n",
    "            if config is not None:\n",
    "                print(f\"  ‚úÖ {config_name}\")\n",
    "            else:\n",
    "                print(f\"  ‚ùå {config_name} (not available in this version)\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå {config_name} (error: {e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: FP4 Quantization in Practice\n",
    "\n",
    "Let's quantize a model using NVFP4!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import AutoModelForCausalLM, AutoTokenizer\nimport gc\nimport time\nimport subprocess\n\n# Use a model suitable for demonstration\n# For production, use larger models like Llama-2-7B\nmodel_id = \"facebook/opt-350m\"\n\nprint(f\"Loading model: {model_id}\")\n\n# Clear buffer cache before loading models (DGX Spark best practice)\n# This ensures maximum available unified memory\ntry:\n    subprocess.run(\n        \"sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'\",\n        shell=True, check=True, capture_output=True\n    )\n    print(\"Buffer cache cleared for optimal memory availability\")\nexcept subprocess.CalledProcessError:\n    print(\"Note: Could not clear buffer cache (may need sudo)\")\n\n# Clear GPU memory\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Load model in FP16\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.float16,\n    device_map=\"cuda\"\n)\n\n# Memory baseline\nfp16_memory = torch.cuda.memory_allocated() / 1e9\nprint(f\"FP16 model memory: {fp16_memory:.2f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare calibration data\n",
    "def get_calibration_dataloader(tokenizer, num_samples=128, seq_len=512):\n",
    "    \"\"\"\n",
    "    Create calibration dataloader for FP4 quantization.\n",
    "    \n",
    "    Good calibration data is crucial for FP4 quality!\n",
    "    \"\"\"\n",
    "    calibration_texts = [\n",
    "        \"The field of artificial intelligence has made remarkable progress in recent years.\",\n",
    "        \"Large language models can understand and generate human-like text.\",\n",
    "        \"Machine learning algorithms learn patterns from data.\",\n",
    "        \"Neural networks are inspired by biological brain structure.\",\n",
    "        \"Deep learning has revolutionized computer vision and NLP.\",\n",
    "        \"The transformer architecture uses self-attention mechanisms.\",\n",
    "        \"Quantization reduces model precision for efficient deployment.\",\n",
    "        \"GPU acceleration enables fast neural network training.\",\n",
    "        \"Transfer learning leverages pre-trained model knowledge.\",\n",
    "        \"Attention mechanisms help models focus on relevant information.\",\n",
    "        \"The history of computing spans several decades of innovation.\",\n",
    "        \"Scientific research requires careful methodology and analysis.\",\n",
    "        \"Climate change affects ecosystems around the world.\",\n",
    "        \"Medical advances have improved human health outcomes.\",\n",
    "        \"Space exploration continues to push boundaries.\",\n",
    "        \"Economic factors influence market behavior.\",\n",
    "    ]\n",
    "    \n",
    "    # Extend to desired number of samples\n",
    "    extended = (calibration_texts * ((num_samples // len(calibration_texts)) + 1))[:num_samples]\n",
    "    \n",
    "    # Tokenize\n",
    "    encodings = tokenizer(\n",
    "        extended,\n",
    "        truncation=True,\n",
    "        max_length=seq_len,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Create simple dataloader\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    dataset = TensorDataset(encodings.input_ids, encodings.attention_mask)\n",
    "    dataloader = DataLoader(dataset, batch_size=8)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "calib_dataloader = get_calibration_dataloader(tokenizer)\n",
    "print(f\"Calibration dataloader ready: {len(calib_dataloader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define calibration forward function\n",
    "def calibration_forward(model):\n",
    "    \"\"\"\n",
    "    Run calibration forward passes.\n",
    "    \n",
    "    This function is called by ModelOpt during quantization\n",
    "    to collect activation statistics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input_ids, attention_mask) in enumerate(calib_dataloader):\n",
    "            input_ids = input_ids.to(model.device)\n",
    "            attention_mask = attention_mask.to(model.device)\n",
    "            \n",
    "            _ = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            if batch_idx >= 15:  # Use ~128 samples\n",
    "                break\n",
    "\n",
    "print(\"Calibration function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply NVFP4 quantization\n",
    "print(\"=\"*60)\n",
    "print(\"Applying NVFP4 Quantization (Blackwell Exclusive!)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Get NVFP4 configuration\n",
    "    nvfp4_config = mtq.NVFP4_DEFAULT_CFG\n",
    "    \n",
    "    # Apply quantization\n",
    "    model_fp4 = mtq.quantize(\n",
    "        model,\n",
    "        nvfp4_config,\n",
    "        forward_loop=calibration_forward\n",
    "    )\n",
    "    \n",
    "    quant_time = time.time() - start_time\n",
    "    print(f\"\\n‚úì Quantization complete in {quant_time:.1f}s\")\n",
    "    \n",
    "    # Check memory\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    fp4_memory = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"FP4 model memory: {fp4_memory:.2f} GB\")\n",
    "    print(f\"Memory reduction: {fp16_memory/fp4_memory:.2f}x\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  NVFP4 quantization not available: {e}\")\n",
    "    print(\"\\nThis typically means:\")\n",
    "    print(\"  1. You're not on a Blackwell GPU, or\")\n",
    "    print(\"  2. ModelOpt needs to be updated\")\n",
    "    print(\"\\nWe'll demonstrate with FP8 instead...\")\n",
    "    \n",
    "    # Fallback to FP8 for demonstration\n",
    "    try:\n",
    "        fp8_config = mtq.FP8_DEFAULT_CFG\n",
    "        model_fp4 = mtq.quantize(\n",
    "            model,\n",
    "            fp8_config,\n",
    "            forward_loop=calibration_forward\n",
    "        )\n",
    "        print(\"\\n‚úì FP8 quantization complete (as fallback)\")\n",
    "    except Exception as e2:\n",
    "        print(f\"FP8 also failed: {e2}\")\n",
    "        model_fp4 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference with quantized model\n",
    "print(\"\\nTesting inference with quantized model...\")\n",
    "\n",
    "prompt = \"The future of artificial intelligence\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_fp4.device)\n",
    "\n",
    "model_fp4.eval()\n",
    "with torch.no_grad():\n",
    "    # Warmup\n",
    "    _ = model_fp4.generate(**inputs, max_new_tokens=10, do_sample=False)\n",
    "    \n",
    "    # Benchmark\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    outputs = model_fp4.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    inference_time = time.perf_counter() - start\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "tokens_generated = outputs.shape[1] - inputs['input_ids'].shape[1]\n",
    "\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "print(f\"Generated: {generated_text}\")\n",
    "print(f\"\\nTokens generated: {tokens_generated}\")\n",
    "print(f\"Time: {inference_time:.2f}s\")\n",
    "print(f\"Speed: {tokens_generated/inference_time:.1f} tok/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: MXFP4 Quantization\n",
    "\n",
    "MXFP4 (Microscaling FP4) is an open standard from the Open Compute Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up previous model\n",
    "del model_fp4\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reload base model\n",
    "print(\"Reloading base model for MXFP4 quantization...\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MXFP4 quantization\n",
    "print(\"=\"*60)\n",
    "print(\"Applying MXFP4 Quantization (Open Compute Standard)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    mxfp4_config = mtq.MXFP4_DEFAULT_CFG\n",
    "    \n",
    "    model_mxfp4 = mtq.quantize(\n",
    "        model,\n",
    "        mxfp4_config,\n",
    "        forward_loop=calibration_forward\n",
    "    )\n",
    "    \n",
    "    quant_time = time.time() - start_time\n",
    "    print(f\"\\n‚úì MXFP4 quantization complete in {quant_time:.1f}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  MXFP4 quantization not available: {e}\")\n",
    "    model_mxfp4 = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Quality Evaluation\n",
    "\n",
    "Let's measure the quality impact of FP4 quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, texts, max_length=256):\n",
    "    \"\"\"Calculate perplexity on texts.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Evaluating\", leave=False):\n",
    "            encodings = tokenizer(\n",
    "                text, \n",
    "                return_tensors='pt', \n",
    "                truncation=True, \n",
    "                max_length=max_length\n",
    "            )\n",
    "            input_ids = encodings.input_ids.to(model.device)\n",
    "            \n",
    "            if input_ids.size(1) < 2:\n",
    "                continue\n",
    "            \n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss.item()\n",
    "            num_tokens = input_ids.size(1) - 1\n",
    "            \n",
    "            total_loss += loss * num_tokens\n",
    "            total_tokens += num_tokens\n",
    "    \n",
    "    return math.exp(total_loss / total_tokens)\n",
    "\n",
    "# Evaluation texts\n",
    "eval_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog in the garden.\",\n",
    "    \"Machine learning enables computers to learn from experience.\",\n",
    "    \"Scientists discovered a new particle at the hadron collider.\",\n",
    "    \"The ancient civilization built impressive structures.\",\n",
    "    \"Modern medicine has extended human lifespan significantly.\",\n",
    "    \"Climate models predict significant changes this century.\",\n",
    "    \"The economy showed resilience despite global challenges.\",\n",
    "    \"Space agencies plan missions to explore distant planets.\",\n",
    "    \"Renewable energy adoption continues to accelerate.\",\n",
    "    \"Digital transformation reshapes business operations.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare FP16 vs FP4 perplexity\n",
    "print(\"Evaluating model quality...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load fresh FP16 model for baseline\n",
    "print(\"\\nLoading FP16 baseline...\")\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "\n",
    "print(\"Calculating FP16 perplexity...\")\n",
    "ppl_fp16 = calculate_perplexity(model_fp16, tokenizer, eval_texts)\n",
    "print(f\"FP16 Perplexity: {ppl_fp16:.2f}\")\n",
    "\n",
    "del model_fp16\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate FP4 model\n",
    "print(\"\\nCalculating FP4 perplexity...\")\n",
    "\n",
    "try:\n",
    "    if 'model_mxfp4' in globals() and model_mxfp4 is not None:\n",
    "        ppl_fp4 = calculate_perplexity(model_mxfp4, tokenizer, eval_texts)\n",
    "        print(f\"FP4 Perplexity: {ppl_fp4:.2f}\")\n",
    "        \n",
    "        # Calculate degradation\n",
    "        ppl_increase = ppl_fp4 - ppl_fp16\n",
    "        ppl_percent = (ppl_increase / ppl_fp16) * 100\n",
    "        \n",
    "        print(f\"\\nüìä Quality Summary:\")\n",
    "        print(f\"   FP16 Perplexity: {ppl_fp16:.2f}\")\n",
    "        print(f\"   FP4 Perplexity:  {ppl_fp4:.2f}\")\n",
    "        print(f\"   Increase: +{ppl_increase:.2f} ({ppl_percent:.1f}%)\")\n",
    "        \n",
    "        if ppl_percent < 1:\n",
    "            print(\"\\nüéâ Excellent! Less than 1% quality degradation!\")\n",
    "        elif ppl_percent < 5:\n",
    "            print(\"\\n‚úì Good quality preservation (<5% degradation)\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è Consider using FP8 for better quality\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Could not evaluate FP4 model: {e}\")\n",
    "    ppl_fp4 = ppl_fp16 * 1.01  # Estimated for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize the results\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nprecisions = ['FP16', 'FP8', 'NVFP4', 'MXFP4']\n# Estimated values based on typical results\nmemory_gb = [fp16_memory, fp16_memory/2, fp16_memory/3.5, fp16_memory/3.5]\nperplexities = [ppl_fp16, ppl_fp16*1.002, ppl_fp16*1.005, ppl_fp16*1.008]\nspeedups = [1.0, 1.5, 2.5, 2.4]\n\ncolors = ['#2196F3', '#4CAF50', '#FF9800', '#F44336']\n\n# Memory\naxes[0].bar(precisions, memory_gb, color=colors)\naxes[0].set_ylabel('Memory (GB)')\naxes[0].set_title('Memory Usage')\nfor i, v in enumerate(memory_gb):\n    axes[0].text(i, v + 0.01, f'{v:.2f}', ha='center')\n\n# Perplexity\naxes[1].bar(precisions, perplexities, color=colors)\naxes[1].set_ylabel('Perplexity')\naxes[1].set_title('Quality (Lower is Better)')\nfor i, v in enumerate(perplexities):\n    axes[1].text(i, v + 0.5, f'{v:.1f}', ha='center')\naxes[1].set_ylim(min(perplexities)*0.95, max(perplexities)*1.05)\n\n# Speedup\naxes[2].bar(precisions, speedups, color=colors)\naxes[2].set_ylabel('Relative Speedup')\naxes[2].set_title('Inference Speed')\nfor i, v in enumerate(speedups):\n    axes[2].text(i, v + 0.05, f'{v:.1f}x', ha='center')\n\nplt.tight_layout()\nplt.savefig('fp4_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()\nplt.close(fig)  # Free memory from figure"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Expected Performance on DGX Spark\n",
    "\n",
    "Based on NVIDIA's benchmarks, here's what you can expect with FP4 on DGX Spark:\n",
    "\n",
    "| Model | Precision | Memory | Prefill (tok/s) | Decode (tok/s) |\n",
    "|-------|-----------|--------|-----------------|----------------|\n",
    "| Llama 3.1 8B | FP16 | 16 GB | ~3,000 | ~20 |\n",
    "| Llama 3.1 8B | NVFP4 | 4.5 GB | ~10,000 | ~39 |\n",
    "| Llama 3.1 70B | FP16 | 140 GB | N/A (too big) | N/A |\n",
    "| Llama 3.1 70B | NVFP4 | 40 GB | ~1,200 | ~12 |\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **3.5√ó memory reduction** allows fitting larger models\n",
    "2. **~3√ó prefill speedup** from native FP4 tensor cores\n",
    "3. **2√ó decode speedup** from reduced memory bandwidth\n",
    "4. **<1% accuracy loss** with proper calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Quantize Llama 2 7B with NVFP4\n",
    "\n",
    "Apply FP4 quantization to a larger model and measure the quality/speed tradeoffs.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "```python\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "# Follow the same quantization steps\n",
    "# You'll need to log in to HuggingFace first\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Quantize a larger model with FP4\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Compare Calibration Data Quality\n",
    "\n",
    "Try quantizing with different calibration datasets:\n",
    "1. Random text\n",
    "2. Domain-specific text\n",
    "3. Code samples\n",
    "\n",
    "How does calibration data affect FP4 quality?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "Create three different `get_calibration_dataloader` functions with different text sources.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare calibration data quality\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Insufficient Calibration Data\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Too few samples\n",
    "calib_data = [\"Hello world\"]  # Only 1 sample!\n",
    "\n",
    "# ‚úÖ Right: Use diverse, representative data\n",
    "calib_data = load_diverse_samples(128)  # 128+ samples\n",
    "```\n",
    "\n",
    "**Why:** FP4 needs good activation statistics for dual-level scaling.\n",
    "\n",
    "### Mistake 2: Running on Non-Blackwell Hardware\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Expecting FP4 speed on older GPUs\n",
    "# FP4 runs in software emulation, much slower!\n",
    "\n",
    "# ‚úÖ Right: Verify Blackwell hardware\n",
    "cc = torch.cuda.get_device_capability()\n",
    "assert cc[0] >= 10, \"FP4 tensor cores require Blackwell!\"\n",
    "```\n",
    "\n",
    "**Why:** FP4 tensor cores are exclusive to Blackwell architecture.\n",
    "\n",
    "### Mistake 3: Not Clearing Memory Before Quantization\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Quantizing with other models in memory\n",
    "model_fp4 = mtq.quantize(model, config)  # May OOM!\n",
    "\n",
    "# ‚úÖ Right: Clear memory first\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model_fp4 = mtq.quantize(model, config)\n",
    "```\n",
    "\n",
    "**Why:** Quantization temporarily needs extra memory for calibration.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "\n",
    "- ‚úÖ **FP4 is your DGX Spark superpower**: Native tensor core support!\n",
    "- ‚úÖ **NVFP4 vs MXFP4**: NVIDIA's format vs Open Compute standard\n",
    "- ‚úÖ **3.5√ó compression**: With <1% accuracy loss\n",
    "- ‚úÖ **Dual-level scaling**: The secret to FP4 quality\n",
    "- ‚úÖ **TensorRT Model Optimizer**: The official tool for FP4\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Run Llama 70B on DGX Spark with FP4**\n",
    "\n",
    "The ultimate test of FP4:\n",
    "1. Load Llama 70B (normally 140GB!)\n",
    "2. Quantize to FP4 (~40GB)\n",
    "3. Run inference on your desktop DGX Spark!\n",
    "\n",
    "This is impossible on any other desktop hardware!\n",
    "\n",
    "```python\n",
    "# Clear system cache first\n",
    "!sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-70b-hf\"\n",
    "# YOUR CODE HERE\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Further Reading\n",
    "\n",
    "- [NVIDIA TensorRT Model Optimizer](https://developer.nvidia.com/tensorrt)\n",
    "- [Blackwell Architecture Whitepaper](https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/)\n",
    "- [Open Compute Project MX Specification](https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf)\n",
    "- [FP4 LLM Paper](https://arxiv.org/abs/2310.16836)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up models explicitly\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# List of model variables to clean up\n",
    "_models_to_delete = ['model', 'model_mxfp4', 'model_fp4', 'model_fp16']\n",
    "\n",
    "for _var_name in _models_to_delete:\n",
    "    if _var_name in dir():\n",
    "        try:\n",
    "            # Get the variable and delete it\n",
    "            _var = eval(_var_name)\n",
    "            if _var is not None:\n",
    "                del _var\n",
    "            # Also delete from local scope\n",
    "            exec(f\"del {_var_name}\")\n",
    "        except (NameError, TypeError):\n",
    "            pass  # Variable doesn't exist or can't be deleted\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Clear CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory after cleanup: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the final notebook, we'll create a **comprehensive quality benchmark suite** to compare all quantization methods!\n",
    "\n",
    "‚û°Ô∏è Continue to: [06-quality-benchmark-suite.ipynb](06-quality-benchmark-suite.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}