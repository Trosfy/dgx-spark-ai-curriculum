{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 11.1 Solutions: Quantization Overview\n",
    "\n",
    "This notebook contains solutions to the exercises from Task 11.1.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Asymmetric Quantization\n",
    "\n",
    "Implement asymmetric quantization with zero-point for tensors with non-symmetric distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asymmetric_quantize(tensor: torch.Tensor, bits: int = 8) -> tuple:\n",
    "    \"\"\"\n",
    "    Asymmetric quantization with zero-point.\n",
    "    \n",
    "    This is useful for tensors with non-symmetric distributions,\n",
    "    like activations after ReLU.\n",
    "    \n",
    "    Args:\n",
    "        tensor: Input tensor to quantize\n",
    "        bits: Number of bits for quantization\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (quantized tensor, scale, zero_point)\n",
    "    \"\"\"\n",
    "    qmax = 2 ** bits - 1  # 255 for 8-bit\n",
    "    qmin = 0\n",
    "    \n",
    "    # Find min and max\n",
    "    min_val = tensor.min()\n",
    "    max_val = tensor.max()\n",
    "    \n",
    "    # Compute scale\n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "    scale = max(scale, 1e-10)  # Avoid division by zero\n",
    "    \n",
    "    # Compute zero point\n",
    "    zero_point = round(-min_val / scale)\n",
    "    zero_point = max(qmin, min(qmax, zero_point))  # Clamp\n",
    "    \n",
    "    # Quantize\n",
    "    quantized = torch.round(tensor / scale + zero_point).clamp(qmin, qmax)\n",
    "    \n",
    "    return quantized.to(torch.uint8), scale, zero_point\n",
    "\n",
    "\n",
    "def asymmetric_dequantize(quantized: torch.Tensor, scale: float, zero_point: int) -> torch.Tensor:\n",
    "    \"\"\"Dequantize asymmetric quantized tensor.\"\"\"\n",
    "    return (quantized.float() - zero_point) * scale\n",
    "\n",
    "\n",
    "# Test with non-symmetric data (e.g., ReLU output)\n",
    "test_tensor = torch.tensor([0.0, 0.5, 1.0, 1.5, 2.0])\n",
    "q, s, zp = asymmetric_quantize(test_tensor)\n",
    "\n",
    "print(\"Original:\", test_tensor.numpy())\n",
    "print(f\"Quantized: {q.numpy()}, Scale: {s:.4f}, Zero point: {zp}\")\n",
    "\n",
    "# Verify reconstruction\n",
    "reconstructed = asymmetric_dequantize(q, s, zp)\n",
    "error = (test_tensor - reconstructed).abs().mean()\n",
    "print(f\"Reconstructed: {reconstructed.numpy()}\")\n",
    "print(f\"Mean error: {error:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare symmetric vs asymmetric on biased data\n",
    "biased_tensor = torch.rand(1000) * 2  # All positive [0, 2]\n",
    "\n",
    "# Symmetric quantization\n",
    "def symmetric_quantize(tensor: torch.Tensor, bits: int = 8) -> tuple:\n",
    "    \"\"\"Symmetric quantization around zero.\"\"\"\n",
    "    qmax = 2 ** (bits - 1) - 1\n",
    "    scale = tensor.abs().max() / qmax\n",
    "    quantized = torch.round(tensor / scale).clamp(-qmax-1, qmax)\n",
    "    return quantized.to(torch.int8), scale\n",
    "\n",
    "sym_q, sym_s = symmetric_quantize(biased_tensor)\n",
    "sym_recon = sym_q.float() * sym_s\n",
    "sym_error = (biased_tensor - sym_recon).abs().mean()\n",
    "\n",
    "# Asymmetric quantization\n",
    "asym_q, asym_s, asym_zp = asymmetric_quantize(biased_tensor)\n",
    "asym_recon = asymmetric_dequantize(asym_q, asym_s, asym_zp)\n",
    "asym_error = (biased_tensor - asym_recon).abs().mean()\n",
    "\n",
    "print(f\"Symmetric error:  {sym_error:.6f}\")\n",
    "print(f\"Asymmetric error: {asym_error:.6f}\")\n",
    "print(f\"\\nAsymmetric is {sym_error/asym_error:.1f}x better for biased data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Larger Model Comparison\n",
    "\n",
    "Compare FP16, INT8, and INT4 on a larger model (Llama-2-7B or OPT-1.3B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for larger model (Llama-2-7B or OPT-1.3B)\n",
    "# Note: Llama-2 requires HuggingFace login and model access\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Choose your model:\n",
    "# model_id = \"meta-llama/Llama-2-7b-hf\"  # Requires HF login\n",
    "model_id = \"facebook/opt-1.3b\"  # No login required\n",
    "\n",
    "def compare_precisions(model_id: str):\n",
    "    \"\"\"Compare memory usage across different precisions.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # FP16\n",
    "    print(\"Loading FP16...\")\n",
    "    clear_memory()\n",
    "    model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, torch_dtype=torch.float16, device_map=\"cuda\"\n",
    "    )\n",
    "    results['FP16'] = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"  FP16: {results['FP16']:.2f} GB\")\n",
    "    del model_fp16\n",
    "    clear_memory()\n",
    "    \n",
    "    # INT8\n",
    "    print(\"Loading INT8...\")\n",
    "    model_int8 = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, \n",
    "        quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    "        device_map=\"cuda\"\n",
    "    )\n",
    "    results['INT8'] = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"  INT8: {results['INT8']:.2f} GB\")\n",
    "    del model_int8\n",
    "    clear_memory()\n",
    "    \n",
    "    # INT4\n",
    "    print(\"Loading INT4...\")\n",
    "    model_int4 = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        ),\n",
    "        device_map=\"cuda\"\n",
    "    )\n",
    "    results['INT4'] = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"  INT4: {results['INT4']:.2f} GB\")\n",
    "    del model_int4\n",
    "    clear_memory()\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Memory Comparison Summary\")\n",
    "    print(\"=\"*50)\n",
    "    for name, mem in results.items():\n",
    "        compression = results['FP16'] / mem\n",
    "        print(f\"{name}: {mem:.2f} GB ({compression:.1f}x compression)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Uncomment to run:\n",
    "# results = compare_precisions(model_id)\n",
    "print(\"Run compare_precisions() to test with your chosen model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Asymmetric quantization** is better for non-symmetric distributions (like ReLU outputs)\n",
    "2. **Symmetric quantization** is simpler and faster, good for weights\n",
    "3. **Memory savings** scale roughly linearly with bit reduction\n",
    "4. **bitsandbytes** makes INT8/INT4 quantization easy on HuggingFace models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
