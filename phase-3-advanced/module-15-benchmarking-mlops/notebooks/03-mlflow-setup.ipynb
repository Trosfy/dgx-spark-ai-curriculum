{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 15.3: MLflow Experiment Tracking\n",
    "\n",
    "**Module:** 15 - Benchmarking, Evaluation & MLOps  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand why experiment tracking matters\n",
    "- [ ] Set up MLflow tracking server on DGX Spark\n",
    "- [ ] Log parameters, metrics, and artifacts\n",
    "- [ ] Create reproducible experiments\n",
    "- [ ] Use the MLflow UI to compare runs\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Tasks 15.1-15.2\n",
    "- Knowledge of: Python, basic ML training loops\n",
    "- Hardware: Any system (MLflow is lightweight)\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**Have you ever trained a model, gotten great results... and then forgotten what hyperparameters you used?**\n",
    "\n",
    "This happens to everyone! That's why companies use experiment tracking:\n",
    "\n",
    "- **Netflix:** Tracks thousands of recommendation model experiments\n",
    "- **Airbnb:** Logs all search ranking experiments\n",
    "- **Meta:** Records every model training run\n",
    "\n",
    "**MLflow** is the industry-standard open-source tool for:\n",
    "1. **Tracking** - Log everything about your experiments\n",
    "2. **Projects** - Package code for reproducibility\n",
    "3. **Models** - Manage and deploy models\n",
    "4. **Registry** - Version and stage models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßí ELI5: What is Experiment Tracking?\n",
    "\n",
    "> **Imagine you're a chef creating a new recipe.** Each time you cook, you'd want to record:\n",
    "> - **Ingredients** (parameters): 2 cups flour, 1 tsp salt\n",
    "> - **How it turned out** (metrics): Taste score 8/10, texture 7/10\n",
    "> - **The actual dish** (artifacts): A photo of the result\n",
    ">\n",
    "> After many attempts, you can look back and see:\n",
    "> - \"Aha! More butter = better taste!\"\n",
    "> - \"The recipe from Tuesday was the best!\"\n",
    ">\n",
    "> **Experiment tracking is the same for ML!** You record:\n",
    "> - **Parameters:** learning rate, batch size, model architecture\n",
    "> - **Metrics:** accuracy, loss, training time\n",
    "> - **Artifacts:** model weights, plots, predictions\n",
    ">\n",
    "> Then you can compare and find what works best!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting Up MLflow\n",
    "\n",
    "Let's install and configure MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install MLflow\n# MLflow is pure Python and works well on ARM64 (DGX Spark)\n\nimport subprocess\nimport sys\n\ntry:\n    import mlflow\n    print(f\"MLflow already installed: {mlflow.__version__}\")\nexcept ImportError:\n    print(\"Installing MLflow...\")\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"mlflow\", \"-q\"])\n    import mlflow\n    print(f\"MLflow installed: {mlflow.__version__}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create directory for MLflow data\n# Use Path for robust cross-platform path handling\nfrom pathlib import Path\n\n# Get module directory relative to notebooks folder\n# This works regardless of the current working directory\nNOTEBOOK_DIR = Path.cwd()\nMODULE_DIR = (NOTEBOOK_DIR / \"..\").resolve()  # Go up from notebooks/\nMLFLOW_DIR = str(MODULE_DIR / \"mlflow\")\n\nos.makedirs(MLFLOW_DIR, exist_ok=True)\n\n# Set tracking URI to local directory\n# In production, you'd use a server: mlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_tracking_uri(f\"file://{MLFLOW_DIR}\")\n\nprint(f\"MLflow tracking directory: {MLFLOW_DIR}\")\nprint(f\"Tracking URI: {mlflow.get_tracking_uri()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç MLflow Storage Options\n",
    "\n",
    "| Storage Type | URI Format | Use Case |\n",
    "|-------------|------------|----------|\n",
    "| Local Files | `file:///path/to/mlflow` | Development |\n",
    "| Local Server | `http://localhost:5000` | Team sharing |\n",
    "| Remote Server | `http://mlflow.company.com` | Production |\n",
    "| Databricks | `databricks` | Databricks platform |\n",
    "\n",
    "For DGX Spark, local files work great for personal use. For teams, run a server!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Creating Your First Experiment\n",
    "\n",
    "Experiments in MLflow are like folders for related runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or get an experiment\n",
    "experiment_name = \"LLM-Finetuning-Demo\"\n",
    "\n",
    "# Check if experiment exists\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "if experiment is None:\n",
    "    experiment_id = mlflow.create_experiment(\n",
    "        experiment_name,\n",
    "        tags={\n",
    "            \"project\": \"dgx-spark-curriculum\",\n",
    "            \"module\": \"15\",\n",
    "            \"task\": \"experiment-tracking\"\n",
    "        }\n",
    "    )\n",
    "    print(f\"Created experiment '{experiment_name}' with ID: {experiment_id}\")\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id\n",
    "    print(f\"Using existing experiment '{experiment_name}' with ID: {experiment_id}\")\n",
    "\n",
    "# Set as active experiment\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßí ELI5: Experiments vs Runs\n",
    "\n",
    "> **Experiment** = A folder (\"Chocolate Chip Cookie Recipes\")\n",
    ">\n",
    "> **Run** = One attempt (\"Recipe #5: with brown butter\")\n",
    ">\n",
    "> You might have:\n",
    "> - Experiment: \"LLM Finetuning\"\n",
    ">   - Run 1: learning_rate=1e-4, epochs=3\n",
    ">   - Run 2: learning_rate=1e-5, epochs=5\n",
    ">   - Run 3: learning_rate=1e-4, epochs=3, with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a run and log some data\n",
    "with mlflow.start_run(run_name=\"demo-run-1\") as run:\n",
    "    \n",
    "    # Log parameters (inputs to your experiment)\n",
    "    mlflow.log_param(\"model_name\", \"microsoft/phi-2\")\n",
    "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
    "    mlflow.log_param(\"batch_size\", 8)\n",
    "    mlflow.log_param(\"epochs\", 3)\n",
    "    mlflow.log_param(\"lora_rank\", 16)\n",
    "    \n",
    "    # Log metrics (outputs/results)\n",
    "    mlflow.log_metric(\"train_loss\", 0.45)\n",
    "    mlflow.log_metric(\"eval_loss\", 0.52)\n",
    "    mlflow.log_metric(\"accuracy\", 0.87)\n",
    "    mlflow.log_metric(\"training_time_seconds\", 3600)\n",
    "    \n",
    "    # Log metrics over time (for charts)\n",
    "    for epoch in range(3):\n",
    "        mlflow.log_metric(\"epoch_loss\", 0.5 - epoch * 0.1, step=epoch)\n",
    "        mlflow.log_metric(\"epoch_accuracy\", 0.7 + epoch * 0.05, step=epoch)\n",
    "    \n",
    "    # Log tags (metadata)\n",
    "    mlflow.set_tag(\"gpu\", \"DGX Spark\")\n",
    "    mlflow.set_tag(\"framework\", \"pytorch\")\n",
    "    mlflow.set_tag(\"status\", \"completed\")\n",
    "    \n",
    "    print(f\"Run ID: {run.info.run_id}\")\n",
    "    print(f\"Artifact URI: {run.info.artifact_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View what we logged\n",
    "run_data = mlflow.get_run(run.info.run_id)\n",
    "\n",
    "print(\"\\nüìä Logged Data:\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nParameters:\")\n",
    "for key, value in run_data.data.params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nMetrics:\")\n",
    "for key, value in run_data.data.metrics.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nTags:\")\n",
    "for key, value in run_data.data.tags.items():\n",
    "    if not key.startswith(\"mlflow.\"):  # Skip internal tags\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Logging Artifacts\n",
    "\n",
    "Artifacts are files associated with a run: models, plots, configs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample plot to log\n",
    "def create_training_plot():\n",
    "    \"\"\"Create a training progress plot.\"\"\"\n",
    "    epochs = np.arange(1, 11)\n",
    "    train_loss = np.exp(-epochs * 0.3) + 0.1\n",
    "    val_loss = np.exp(-epochs * 0.25) + 0.15\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(epochs, train_loss, 'b-', label='Training Loss', linewidth=2)\n",
    "    ax.plot(epochs, val_loss, 'r--', label='Validation Loss', linewidth=2)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training Progress')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create plot\n",
    "fig = create_training_plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log artifacts in a new run\n",
    "with mlflow.start_run(run_name=\"demo-run-with-artifacts\") as run:\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"model_name\": \"microsoft/phi-2\",\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"batch_size\": 16,\n",
    "        \"epochs\": 10\n",
    "    })\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"final_train_loss\": 0.15,\n",
    "        \"final_val_loss\": 0.22,\n",
    "        \"accuracy\": 0.91\n",
    "    })\n",
    "    \n",
    "    # Save and log the plot\n",
    "    fig = create_training_plot()\n",
    "    plot_path = \"/tmp/training_progress.png\"\n",
    "    fig.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    mlflow.log_artifact(plot_path, artifact_path=\"plots\")\n",
    "    print(f\"Logged plot: {plot_path}\")\n",
    "    \n",
    "    # Log a config file\n",
    "    config = {\n",
    "        \"model\": {\n",
    "            \"name\": \"microsoft/phi-2\",\n",
    "            \"dtype\": \"bfloat16\"\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"learning_rate\": 2e-4,\n",
    "            \"batch_size\": 16,\n",
    "            \"epochs\": 10,\n",
    "            \"warmup_steps\": 100\n",
    "        },\n",
    "        \"lora\": {\n",
    "            \"rank\": 16,\n",
    "            \"alpha\": 32,\n",
    "            \"dropout\": 0.1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config_path = \"/tmp/config.json\"\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    mlflow.log_artifact(config_path, artifact_path=\"configs\")\n",
    "    print(f\"Logged config: {config_path}\")\n",
    "    \n",
    "    # Log a text file with notes\n",
    "    notes = \"\"\"Training Notes\n",
    "================\n",
    "- Used LoRA for efficient finetuning\n",
    "- Training on DGX Spark with 128GB unified memory\n",
    "- Dataset: Custom instruction dataset (10k samples)\n",
    "- Notable: Lower learning rate helped stability\n",
    "\"\"\"\n",
    "    notes_path = \"/tmp/notes.txt\"\n",
    "    with open(notes_path, 'w') as f:\n",
    "        f.write(notes)\n",
    "    \n",
    "    mlflow.log_artifact(notes_path)\n",
    "    print(f\"Logged notes: {notes_path}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List artifacts for the run\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "print(\"\\nüìÅ Artifacts:\")\n",
    "for artifact in client.list_artifacts(run.info.run_id):\n",
    "    print(f\"  {artifact.path} ({artifact.file_size if artifact.file_size else 'directory'})\")\n",
    "    if artifact.is_dir:\n",
    "        for sub_artifact in client.list_artifacts(run.info.run_id, artifact.path):\n",
    "            print(f\"    ‚îî‚îÄ‚îÄ {sub_artifact.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Logging PyTorch Models\n",
    "\n",
    "MLflow has built-in support for logging PyTorch models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a simple model for demonstration\n",
    "class SimpleClassifier(nn.Module):\n",
    "    \"\"\"Simple neural network classifier.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int = 768, hidden_size: int = 256, num_classes: int = 10):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Create model instance\n",
    "model = SimpleClassifier()\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate training and log the model\n",
    "with mlflow.start_run(run_name=\"model-logging-demo\") as run:\n",
    "    \n",
    "    # Log model architecture info\n",
    "    mlflow.log_params({\n",
    "        \"input_size\": 768,\n",
    "        \"hidden_size\": 256,\n",
    "        \"num_classes\": 10,\n",
    "        \"total_params\": sum(p.numel() for p in model.parameters())\n",
    "    })\n",
    "    \n",
    "    # Simulate training metrics\n",
    "    for epoch in range(5):\n",
    "        train_loss = 1.0 * (0.7 ** epoch)\n",
    "        val_loss = 1.2 * (0.75 ** epoch)\n",
    "        accuracy = 0.5 + epoch * 0.1\n",
    "        \n",
    "        mlflow.log_metrics({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"accuracy\": accuracy\n",
    "        }, step=epoch)\n",
    "    \n",
    "    # Log the PyTorch model\n",
    "    # This saves:\n",
    "    # - Model weights\n",
    "    # - Model signature (input/output schema)\n",
    "    # - Requirements (pip dependencies)\n",
    "    \n",
    "    # Create a sample input for signature inference\n",
    "    sample_input = torch.randn(1, 768)\n",
    "    \n",
    "    mlflow.pytorch.log_model(\n",
    "        model,\n",
    "        artifact_path=\"model\",\n",
    "        registered_model_name=None,  # Don't register yet\n",
    "        input_example=sample_input.numpy()\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Model logged to run: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model back from MLflow\n",
    "model_uri = f\"runs:/{run.info.run_id}/model\"\n",
    "loaded_model = mlflow.pytorch.load_model(model_uri)\n",
    "\n",
    "print(f\"Loaded model from: {model_uri}\")\n",
    "print(f\"Model type: {type(loaded_model)}\")\n",
    "\n",
    "# Test inference\n",
    "test_input = torch.randn(1, 768)\n",
    "with torch.no_grad():\n",
    "    output = loaded_model(test_input)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Comparing Experiments\n",
    "\n",
    "The real power of MLflow is comparing multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple experiments with different hyperparameters\n",
    "import random\n",
    "\n",
    "# Hyperparameter grid\n",
    "learning_rates = [1e-5, 1e-4, 1e-3]\n",
    "batch_sizes = [8, 16, 32]\n",
    "\n",
    "print(\"üî¨ Running hyperparameter sweep...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        with mlflow.start_run(run_name=f\"lr={lr}_bs={bs}\"):\n",
    "            \n",
    "            # Log parameters\n",
    "            mlflow.log_params({\n",
    "                \"learning_rate\": lr,\n",
    "                \"batch_size\": bs,\n",
    "                \"epochs\": 5,\n",
    "                \"model\": \"phi-2\"\n",
    "            })\n",
    "            \n",
    "            # Simulate training with some \"realistic\" patterns\n",
    "            # Higher LR = faster convergence but more variance\n",
    "            # Larger batch = more stable but slower convergence\n",
    "            \n",
    "            base_loss = 1.0\n",
    "            lr_factor = 1.0 + (lr - 1e-4) * 1000  # LR affects convergence\n",
    "            bs_factor = 1.0 - (bs - 16) * 0.01   # BS affects stability\n",
    "            \n",
    "            for epoch in range(5):\n",
    "                noise = random.gauss(0, 0.05)\n",
    "                loss = base_loss * (0.7 ** (epoch * lr_factor)) * bs_factor + noise\n",
    "                loss = max(0.1, loss)  # Floor\n",
    "                \n",
    "                accuracy = min(0.95, 0.5 + epoch * 0.1 * lr_factor + noise)\n",
    "                \n",
    "                mlflow.log_metrics({\n",
    "                    \"loss\": loss,\n",
    "                    \"accuracy\": accuracy\n",
    "                }, step=epoch)\n",
    "            \n",
    "            # Log final metrics\n",
    "            mlflow.log_metrics({\n",
    "                \"final_loss\": loss,\n",
    "                \"final_accuracy\": accuracy\n",
    "            })\n",
    "            \n",
    "            print(f\"  lr={lr}, bs={bs} -> loss={loss:.4f}, acc={accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Hyperparameter sweep complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query runs and find the best one\n",
    "import pandas as pd\n",
    "\n",
    "# Search for all runs in this experiment\n",
    "runs_df = mlflow.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    filter_string=\"\",\n",
    "    order_by=[\"metrics.final_accuracy DESC\"]\n",
    ")\n",
    "\n",
    "# Display relevant columns\n",
    "display_cols = [\n",
    "    \"run_id\",\n",
    "    \"params.learning_rate\",\n",
    "    \"params.batch_size\",\n",
    "    \"metrics.final_loss\",\n",
    "    \"metrics.final_accuracy\"\n",
    "]\n",
    "\n",
    "available_cols = [c for c in display_cols if c in runs_df.columns]\n",
    "\n",
    "print(\"\\nüìä All Runs (sorted by accuracy):\")\n",
    "print(runs_df[available_cols].head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best run\n",
    "best_run = runs_df.iloc[0]\n",
    "\n",
    "print(\"\\nüèÜ Best Run:\")\n",
    "print(f\"  Run ID: {best_run['run_id'][:8]}...\")\n",
    "print(f\"  Learning Rate: {best_run.get('params.learning_rate', 'N/A')}\")\n",
    "print(f\"  Batch Size: {best_run.get('params.batch_size', 'N/A')}\")\n",
    "print(f\"  Final Accuracy: {best_run.get('metrics.final_accuracy', 'N/A'):.4f}\")\n",
    "print(f\"  Final Loss: {best_run.get('metrics.final_loss', 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the hyperparameter sweep\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Filter for runs with hyperparameter data\n",
    "sweep_runs = runs_df[runs_df['params.learning_rate'].notna()].copy()\n",
    "\n",
    "if len(sweep_runs) > 0:\n",
    "    # Convert to numeric for plotting\n",
    "    sweep_runs['lr'] = sweep_runs['params.learning_rate'].astype(float)\n",
    "    sweep_runs['bs'] = sweep_runs['params.batch_size'].astype(float)\n",
    "    \n",
    "    # Plot 1: Learning Rate vs Accuracy\n",
    "    for bs in sweep_runs['bs'].unique():\n",
    "        subset = sweep_runs[sweep_runs['bs'] == bs]\n",
    "        axes[0].scatter(\n",
    "            subset['lr'], \n",
    "            subset['metrics.final_accuracy'],\n",
    "            label=f'batch_size={int(bs)}',\n",
    "            s=100\n",
    "        )\n",
    "    \n",
    "    axes[0].set_xscale('log')\n",
    "    axes[0].set_xlabel('Learning Rate')\n",
    "    axes[0].set_ylabel('Final Accuracy')\n",
    "    axes[0].set_title('Learning Rate vs Accuracy')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Batch Size vs Loss\n",
    "    for lr in sweep_runs['lr'].unique():\n",
    "        subset = sweep_runs[sweep_runs['lr'] == lr]\n",
    "        axes[1].scatter(\n",
    "            subset['bs'],\n",
    "            subset['metrics.final_loss'],\n",
    "            label=f'lr={lr}',\n",
    "            s=100\n",
    "        )\n",
    "    \n",
    "    axes[1].set_xlabel('Batch Size')\n",
    "    axes[1].set_ylabel('Final Loss')\n",
    "    axes[1].set_title('Batch Size vs Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/tmp/hyperparam_sweep.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Saved hyperparameter sweep visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Starting the MLflow UI\n",
    "\n",
    "MLflow includes a beautiful web UI for exploring experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions for starting the MLflow UI\n",
    "print(\"\"\"\n",
    "üñ•Ô∏è  Starting the MLflow UI\n",
    "{'='*50}\n",
    "\n",
    "To view your experiments in a web browser, run this in a terminal:\n",
    "\n",
    "    mlflow ui --backend-store-uri {mlflow_dir} --host 0.0.0.0 --port 5000\n",
    "\n",
    "Then open: http://localhost:5000\n",
    "\n",
    "Features of the UI:\n",
    "- üìä Compare runs side-by-side\n",
    "- üìà View metric charts over time\n",
    "- üìÅ Download artifacts\n",
    "- üîç Filter and search runs\n",
    "- üìã Export to CSV\n",
    "\n",
    "For DGX Spark with Docker, expose port 5000:\n",
    "\n",
    "    docker run -p 5000:5000 ... \n",
    "\n",
    "\"\"\".format(mlflow_dir=MLFLOW_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally start MLflow server in background (for demo purposes)\n",
    "# Note: This will run in the background; you'll need to kill it manually\n",
    "\n",
    "# Uncomment to run:\n",
    "# import subprocess\n",
    "# server_process = subprocess.Popen(\n",
    "#     [\"mlflow\", \"ui\", \"--backend-store-uri\", MLFLOW_DIR, \"--host\", \"0.0.0.0\", \"--port\", \"5000\"],\n",
    "#     stdout=subprocess.DEVNULL,\n",
    "#     stderr=subprocess.DEVNULL\n",
    "# )\n",
    "# print(f\"MLflow UI started on http://localhost:5000 (PID: {server_process.pid})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Autologging\n",
    "\n",
    "MLflow can automatically log metrics from popular frameworks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autologging for PyTorch\n",
    "mlflow.pytorch.autolog(\n",
    "    log_models=True,           # Log model artifacts\n",
    "    log_every_n_epoch=1,       # Log metrics every epoch\n",
    "    log_every_n_step=None,     # Don't log every step (too much data)\n",
    "    registered_model_name=None # Don't auto-register\n",
    ")\n",
    "\n",
    "print(\"‚úÖ PyTorch autologging enabled!\")\n",
    "print(\"\"\"\n",
    "With autologging, MLflow will automatically capture:\n",
    "- Training loss and metrics\n",
    "- Model architecture\n",
    "- Optimizer parameters\n",
    "- Model artifacts\n",
    "\n",
    "Just run your training code normally!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using autolog with a training loop\n",
    "# This would automatically log everything!\n",
    "\n",
    "demo_code = '''\n",
    "import mlflow\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Enable autologging\n",
    "mlflow.pytorch.autolog()\n",
    "\n",
    "# Your normal training code - MLflow captures everything automatically!\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# This will automatically log to MLflow!\n",
    "trainer.train()\n",
    "'''\n",
    "\n",
    "print(\"Example autologging code:\")\n",
    "print(demo_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself: Exercise\n",
    "\n",
    "**Task:** Create a complete experiment tracking workflow.\n",
    "\n",
    "1. Create a new experiment called \"my-first-experiment\"\n",
    "2. Run at least 5 training simulations with different hyperparameters\n",
    "3. Log: parameters, metrics over time, and at least one artifact\n",
    "4. Query to find the best run\n",
    "5. Create a visualization comparing runs\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "Use the hyperparameter sweep pattern from Part 5, but:\n",
    "- Add more hyperparameters (dropout, warmup steps, etc.)\n",
    "- Log a confusion matrix as an artifact\n",
    "- Add tags to categorize runs\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Step 1: Create experiment\n",
    "\n",
    "# Step 2: Run training simulations\n",
    "\n",
    "# Step 3: Query for best run\n",
    "\n",
    "# Step 4: Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Ending Runs Properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Run never ends if code crashes\n",
    "# mlflow.start_run()\n",
    "# ... training code that might crash ...\n",
    "# mlflow.end_run()  # Never reached!\n",
    "\n",
    "# ‚úÖ Right: Use context manager\n",
    "# with mlflow.start_run():\n",
    "#     ... training code ...\n",
    "#     # Run automatically ends, even if code crashes\n",
    "\n",
    "print(\"Always use 'with mlflow.start_run():' to ensure proper cleanup!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Logging Too Frequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Logging every step creates huge databases\n",
    "# for step in range(1000000):\n",
    "#     mlflow.log_metric(\"loss\", loss, step=step)  # 1M log entries!\n",
    "\n",
    "# ‚úÖ Right: Log at reasonable intervals\n",
    "# for step in range(1000000):\n",
    "#     if step % 1000 == 0:  # Every 1000 steps\n",
    "#         mlflow.log_metric(\"loss\", loss, step=step)  # Only 1000 entries\n",
    "\n",
    "print(\"Log metrics at reasonable intervals (every N steps/epochs).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Not Setting Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: All runs go to \"Default\" experiment\n",
    "# with mlflow.start_run():\n",
    "#     ...  # Which project is this for??\n",
    "\n",
    "# ‚úÖ Right: Always set experiment first\n",
    "# mlflow.set_experiment(\"my-project-name\")\n",
    "# with mlflow.start_run():\n",
    "#     ...  # Clearly organized!\n",
    "\n",
    "print(\"Always call mlflow.set_experiment() before starting runs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ Setting up MLflow tracking\n",
    "- ‚úÖ Logging parameters, metrics, and artifacts\n",
    "- ‚úÖ Comparing multiple experiment runs\n",
    "- ‚úÖ Using the MLflow UI\n",
    "- ‚úÖ Autologging with PyTorch\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Set up a production-ready MLflow deployment:**\n",
    "\n",
    "1. Run MLflow server with PostgreSQL backend\n",
    "2. Use S3/MinIO for artifact storage\n",
    "3. Set up authentication\n",
    "4. Create a CI/CD pipeline that logs to MLflow\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [MLflow Documentation](https://mlflow.org/docs/latest/)\n",
    "- [MLflow Tracking Guide](https://mlflow.org/docs/latest/tracking.html)\n",
    "- [MLflow with PyTorch](https://mlflow.org/docs/latest/python_api/mlflow.pytorch.html)\n",
    "- [Weights & Biases](https://wandb.ai/) (alternative to MLflow)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"MLflow data saved to: {MLFLOW_DIR}\")\n",
    "print(\"To view results, run: mlflow ui --backend-store-uri \" + MLFLOW_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. **Set up** MLflow for local experiment tracking\n",
    "2. **Created** experiments and logged runs\n",
    "3. **Logged** parameters, metrics, and artifacts\n",
    "4. **Ran** a hyperparameter sweep\n",
    "5. **Compared** runs to find the best configuration\n",
    "6. **Learned** about autologging and the MLflow UI\n",
    "\n",
    "**Next up:** In notebook 04, we'll learn about model versioning and registry!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}