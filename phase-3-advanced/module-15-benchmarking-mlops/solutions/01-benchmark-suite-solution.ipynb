{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 15.1 Solution: LLM Benchmark Suite\n",
    "\n",
    "This notebook provides solutions to the exercises in the benchmark suite notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Compare Two Models\n",
    "\n",
    "**Task:** Run a full benchmark comparison on two models of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "import subprocess\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup\n",
    "NOTEBOOK_DIR = Path(os.getcwd())\n",
    "RESULTS_DIR = str((NOTEBOOK_DIR / \"../data/benchmark_results\").resolve())\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Results directory: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory() -> None:\n",
    "    \"\"\"Clear GPU memory.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def run_benchmark(\n",
    "    model_name: str,\n",
    "    tasks: list,\n",
    "    output_name: str,\n",
    "    batch_size: int = 8,\n",
    "    limit: int = None,\n",
    "    dtype: str = \"bfloat16\"\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Run lm-eval benchmark on a model.\n",
    "    \n",
    "    Args:\n",
    "        model_name: HuggingFace model path\n",
    "        tasks: List of benchmark tasks\n",
    "        output_name: Name for output directory\n",
    "        batch_size: Batch size for evaluation\n",
    "        limit: Optional limit on number of samples\n",
    "        dtype: Data type for model\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of results or None if failed\n",
    "    \"\"\"\n",
    "    clear_memory()\n",
    "    \n",
    "    output_path = f\"{RESULTS_DIR}/{output_name}\"\n",
    "    tasks_str = \",\".join(tasks)\n",
    "    \n",
    "    cmd = [\n",
    "        \"lm_eval\",\n",
    "        \"--model\", \"hf\",\n",
    "        \"--model_args\", f\"pretrained={model_name},dtype={dtype}\",\n",
    "        \"--tasks\", tasks_str,\n",
    "        \"--batch_size\", str(batch_size),\n",
    "        \"--output_path\", output_path\n",
    "    ]\n",
    "    \n",
    "    if limit:\n",
    "        cmd.extend([\"--limit\", str(limit)])\n",
    "    \n",
    "    print(f\"\\nüöÄ Starting evaluation of {model_name}\")\n",
    "    print(f\"   Tasks: {tasks_str}\")\n",
    "    print(f\"   Limit: {limit if limit else 'Full evaluation'}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è  Completed in {elapsed/60:.1f} minutes\")\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(f\"\\n‚ùå Error: {result.stderr}\")\n",
    "        return None\n",
    "    \n",
    "    # Load results\n",
    "    result_files = glob.glob(f\"{output_path}/*/results.json\")\n",
    "    if result_files:\n",
    "        with open(result_files[0], 'r') as f:\n",
    "            return json.load(f)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define models to compare\n",
    "# Comparing same-family models of different sizes\n",
    "my_models = [\n",
    "    {\n",
    "        \"name\": \"microsoft/phi-2\",\n",
    "        \"size\": \"2.7B\",\n",
    "        \"description\": \"Microsoft Phi-2 - compact but powerful\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "        \"size\": \"1.1B\",\n",
    "        \"description\": \"TinyLlama - efficient small model\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Step 2: Define benchmarks (at least 3)\n",
    "my_benchmarks = [\"hellaswag\", \"arc_easy\", \"winogrande\"]\n",
    "\n",
    "print(\"üìã Models to compare:\")\n",
    "for m in my_models:\n",
    "    print(f\"  ‚Ä¢ {m['name']} ({m['size']}) - {m['description']}\")\n",
    "\n",
    "print(f\"\\nüìä Benchmarks: {my_benchmarks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Run evaluations\n",
    "all_results = {}\n",
    "\n",
    "for model_info in my_models:\n",
    "    model_name = model_info['name']\n",
    "    safe_name = model_name.replace('/', '_').replace('-', '_')\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating: {model_name} ({model_info['size']})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    results = run_benchmark(\n",
    "        model_name=model_name,\n",
    "        tasks=my_benchmarks,\n",
    "        output_name=f\"{safe_name}_comparison\",\n",
    "        batch_size=8,\n",
    "        limit=100  # Remove for full evaluation\n",
    "    )\n",
    "    \n",
    "    if results:\n",
    "        all_results[model_name] = results\n",
    "    \n",
    "    clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create comparison visualization\n",
    "def create_comparison_table(results_dict: dict, benchmarks: list) -> pd.DataFrame:\n",
    "    \"\"\"Create a comparison DataFrame.\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for model_name, results in results_dict.items():\n",
    "        row = {'Model': model_name.split('/')[-1]}\n",
    "        task_results = results.get('results', {})\n",
    "        \n",
    "        for task_name, metrics in task_results.items():\n",
    "            score = metrics.get('acc_norm', metrics.get('acc', 0))\n",
    "            if isinstance(score, (int, float)):\n",
    "                row[task_name] = score * 100\n",
    "        \n",
    "        scores = [v for k, v in row.items() if k != 'Model']\n",
    "        row['Average'] = sum(scores) / len(scores) if scores else 0\n",
    "        data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(data).set_index('Model')\n",
    "    return df.round(2)\n",
    "\n",
    "if all_results:\n",
    "    comparison_df = create_comparison_table(all_results, my_benchmarks)\n",
    "    print(\"\\nüìä Model Comparison Table:\")\n",
    "    print(comparison_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Visualize results\n",
    "if all_results and len(all_results) > 1:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar chart comparison\n",
    "    x = np.arange(len(comparison_df.columns) - 1)\n",
    "    width = 0.35\n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, len(comparison_df)))\n",
    "    \n",
    "    for i, (model, row) in enumerate(comparison_df.iterrows()):\n",
    "        values = [row[col] for col in comparison_df.columns if col != 'Average']\n",
    "        offset = width * (i - len(comparison_df)/2 + 0.5)\n",
    "        axes[0].bar(x + offset, values, width, label=model, color=colors[i])\n",
    "    \n",
    "    axes[0].set_ylabel('Score (%)')\n",
    "    axes[0].set_title('Benchmark Comparison')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels([col for col in comparison_df.columns if col != 'Average'],\n",
    "                           rotation=45, ha='right')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_ylim(0, 100)\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Average scores\n",
    "    models = comparison_df.index.tolist()\n",
    "    averages = comparison_df['Average'].values\n",
    "    axes[1].barh(models, averages, color=colors[:len(models)])\n",
    "    axes[1].set_xlabel('Average Score (%)')\n",
    "    axes[1].set_title('Overall Average')\n",
    "    axes[1].set_xlim(0, 100)\n",
    "    axes[1].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    for i, v in enumerate(averages):\n",
    "        axes[1].text(v + 1, i, f'{v:.1f}%', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/model_comparison.png\", dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìÅ Chart saved to {RESULTS_DIR}/model_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Analysis\n",
    "print(\"\\nüìù Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if comparison_df is not None and len(comparison_df) >= 2:\n",
    "    best_model = comparison_df['Average'].idxmax()\n",
    "    best_score = comparison_df['Average'].max()\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Model: {best_model}\")\n",
    "    print(f\"   Average Score: {best_score:.2f}%\")\n",
    "    \n",
    "    # Per-benchmark winner\n",
    "    print(\"\\nüìä Per-Benchmark Winners:\")\n",
    "    for col in comparison_df.columns:\n",
    "        if col != 'Average':\n",
    "            winner = comparison_df[col].idxmax()\n",
    "            score = comparison_df[col].max()\n",
    "            print(f\"   {col}: {winner} ({score:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nüìã Conclusion:\")\n",
    "    print(f\"   {best_model} performs best overall, likely due to its larger\")\n",
    "    print(f\"   parameter count and training data quality.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Fair Comparison**: Both models evaluated with identical settings (0-shot, same benchmarks)\n",
    "2. **Multiple Metrics**: Using 3+ benchmarks gives a more complete picture\n",
    "3. **Visualization**: Charts make differences immediately clear\n",
    "4. **Trade-offs**: Larger models often perform better but require more memory/time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "clear_memory()\n",
    "print(\"‚úÖ Solution complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
