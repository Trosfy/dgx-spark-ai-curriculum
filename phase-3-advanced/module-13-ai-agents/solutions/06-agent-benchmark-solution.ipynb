{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 13.6: Agent Benchmark - Solutions\n",
    "\n",
    "This notebook contains complete solutions for the Agent Benchmarking exercises.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Continuous Evaluation Pipeline\n",
    "\n",
    "**Solution:** A pipeline that tracks performance over time and alerts on degradation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Callable, Optional\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import statistics\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'scripts'))\n",
    "\n",
    "from benchmark_utils import (\n",
    "    TestCase, BenchmarkResults, AgentEvaluator, TestCategory, Difficulty,\n",
    "    generate_report, save_results_json\n",
    ")\n",
    "\n",
    "print(\"Imports ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PerformanceSnapshot:\n",
    "    \"\"\"A snapshot of performance at a point in time.\"\"\"\n",
    "    timestamp: datetime\n",
    "    overall_score: float\n",
    "    category_scores: Dict[str, float]\n",
    "    mean_latency_ms: float\n",
    "    tests_passed: int\n",
    "    tests_total: int\n",
    "    version: str = \"unknown\"\n",
    "\n",
    "@dataclass\n",
    "class Alert:\n",
    "    \"\"\"Alert for performance issues.\"\"\"\n",
    "    severity: str  # info, warning, critical\n",
    "    message: str\n",
    "    metric: str\n",
    "    current_value: float\n",
    "    threshold: float\n",
    "    timestamp: datetime = field(default_factory=datetime.now)\n",
    "\n",
    "print(\"Data classes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousEvaluationPipeline:\n",
    "    \"\"\"Continuous evaluation pipeline with alerting.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        agent_func: Callable,\n",
    "        test_cases: List[TestCase],\n",
    "        agent_name: str = \"Agent\"\n",
    "    ):\n",
    "        self.agent_func = agent_func\n",
    "        self.test_cases = test_cases\n",
    "        self.agent_name = agent_name\n",
    "        self.history: List[PerformanceSnapshot] = []\n",
    "        self.alerts: List[Alert] = []\n",
    "        \n",
    "        # Thresholds\n",
    "        self.thresholds = {\n",
    "            \"min_score\": 0.7,\n",
    "            \"max_latency_ms\": 5000,\n",
    "            \"min_pass_rate\": 0.8,\n",
    "            \"score_degradation\": 0.1,  # Alert if score drops by 10%\n",
    "        }\n",
    "    \n",
    "    def run_evaluation(self, version: str = \"current\") -> PerformanceSnapshot:\n",
    "        \"\"\"Run a single evaluation and record results.\"\"\"\n",
    "        print(f\"\\nüîç Running evaluation for {self.agent_name} (v{version})...\")\n",
    "        \n",
    "        evaluator = AgentEvaluator(self.agent_func, verbose=False)\n",
    "        results = evaluator.run_benchmark(self.test_cases, self.agent_name)\n",
    "        \n",
    "        # Create snapshot\n",
    "        snapshot = PerformanceSnapshot(\n",
    "            timestamp=datetime.now(),\n",
    "            overall_score=results.overall_score,\n",
    "            category_scores=results.category_scores,\n",
    "            mean_latency_ms=results.timing_stats[\"mean_latency_ms\"],\n",
    "            tests_passed=sum(1 for r in results.results if r.passed),\n",
    "            tests_total=len(results.results),\n",
    "            version=version\n",
    "        )\n",
    "        \n",
    "        self.history.append(snapshot)\n",
    "        \n",
    "        # Check for alerts\n",
    "        self._check_alerts(snapshot)\n",
    "        \n",
    "        return snapshot\n",
    "    \n",
    "    def _check_alerts(self, snapshot: PerformanceSnapshot):\n",
    "        \"\"\"Check thresholds and create alerts.\"\"\"\n",
    "        \n",
    "        # Check minimum score\n",
    "        if snapshot.overall_score < self.thresholds[\"min_score\"]:\n",
    "            self.alerts.append(Alert(\n",
    "                severity=\"warning\",\n",
    "                message=f\"Score below minimum threshold\",\n",
    "                metric=\"overall_score\",\n",
    "                current_value=snapshot.overall_score,\n",
    "                threshold=self.thresholds[\"min_score\"]\n",
    "            ))\n",
    "        \n",
    "        # Check latency\n",
    "        if snapshot.mean_latency_ms > self.thresholds[\"max_latency_ms\"]:\n",
    "            self.alerts.append(Alert(\n",
    "                severity=\"warning\",\n",
    "                message=f\"Latency above threshold\",\n",
    "                metric=\"mean_latency_ms\",\n",
    "                current_value=snapshot.mean_latency_ms,\n",
    "                threshold=self.thresholds[\"max_latency_ms\"]\n",
    "            ))\n",
    "        \n",
    "        # Check pass rate\n",
    "        pass_rate = snapshot.tests_passed / snapshot.tests_total\n",
    "        if pass_rate < self.thresholds[\"min_pass_rate\"]:\n",
    "            self.alerts.append(Alert(\n",
    "                severity=\"critical\",\n",
    "                message=f\"Pass rate below minimum\",\n",
    "                metric=\"pass_rate\",\n",
    "                current_value=pass_rate,\n",
    "                threshold=self.thresholds[\"min_pass_rate\"]\n",
    "            ))\n",
    "        \n",
    "        # Check for degradation from previous run\n",
    "        if len(self.history) >= 2:\n",
    "            prev = self.history[-2]\n",
    "            score_drop = prev.overall_score - snapshot.overall_score\n",
    "            if score_drop > self.thresholds[\"score_degradation\"]:\n",
    "                self.alerts.append(Alert(\n",
    "                    severity=\"critical\",\n",
    "                    message=f\"Significant score degradation from previous run\",\n",
    "                    metric=\"score_degradation\",\n",
    "                    current_value=score_drop,\n",
    "                    threshold=self.thresholds[\"score_degradation\"]\n",
    "                ))\n",
    "    \n",
    "    def get_trend_report(self) -> str:\n",
    "        \"\"\"Generate a trend report from history.\"\"\"\n",
    "        if not self.history:\n",
    "            return \"No evaluation history available.\"\n",
    "        \n",
    "        lines = [\n",
    "            \"=\"*60,\n",
    "            f\"PERFORMANCE TREND REPORT: {self.agent_name}\",\n",
    "            \"=\"*60,\n",
    "            f\"Total evaluations: {len(self.history)}\",\n",
    "            \"\"\n",
    "        ]\n",
    "        \n",
    "        # Score trend\n",
    "        scores = [s.overall_score for s in self.history]\n",
    "        lines.append(\"Score Trend:\")\n",
    "        lines.append(f\"  Latest: {scores[-1]:.1%}\")\n",
    "        lines.append(f\"  Average: {statistics.mean(scores):.1%}\")\n",
    "        lines.append(f\"  Min: {min(scores):.1%}\")\n",
    "        lines.append(f\"  Max: {max(scores):.1%}\")\n",
    "        \n",
    "        # Latency trend\n",
    "        latencies = [s.mean_latency_ms for s in self.history]\n",
    "        lines.append(\"\\nLatency Trend:\")\n",
    "        lines.append(f\"  Latest: {latencies[-1]:.0f}ms\")\n",
    "        lines.append(f\"  Average: {statistics.mean(latencies):.0f}ms\")\n",
    "        \n",
    "        # Recent alerts\n",
    "        recent_alerts = [a for a in self.alerts if (datetime.now() - a.timestamp).seconds < 3600]\n",
    "        if recent_alerts:\n",
    "            lines.append(\"\\n‚ö†Ô∏è Recent Alerts:\")\n",
    "            for alert in recent_alerts[-5:]:\n",
    "                lines.append(f\"  [{alert.severity.upper()}] {alert.message}\")\n",
    "        else:\n",
    "            lines.append(\"\\n‚úÖ No recent alerts\")\n",
    "        \n",
    "        lines.append(\"\\n\" + \"=\"*60)\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    def save_history(self, path: str):\n",
    "        \"\"\"Save evaluation history to JSON.\"\"\"\n",
    "        data = {\n",
    "            \"agent_name\": self.agent_name,\n",
    "            \"history\": [\n",
    "                {\n",
    "                    \"timestamp\": s.timestamp.isoformat(),\n",
    "                    \"overall_score\": s.overall_score,\n",
    "                    \"category_scores\": s.category_scores,\n",
    "                    \"mean_latency_ms\": s.mean_latency_ms,\n",
    "                    \"tests_passed\": s.tests_passed,\n",
    "                    \"tests_total\": s.tests_total,\n",
    "                    \"version\": s.version\n",
    "                }\n",
    "                for s in self.history\n",
    "            ],\n",
    "            \"alerts\": [\n",
    "                {\n",
    "                    \"severity\": a.severity,\n",
    "                    \"message\": a.message,\n",
    "                    \"metric\": a.metric,\n",
    "                    \"current_value\": a.current_value,\n",
    "                    \"threshold\": a.threshold,\n",
    "                    \"timestamp\": a.timestamp.isoformat()\n",
    "                }\n",
    "                for a in self.alerts\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        \n",
    "        print(f\"History saved to: {path}\")\n",
    "\n",
    "print(\"ContinuousEvaluationPipeline defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test cases\n",
    "test_cases = [\n",
    "    TestCase(\n",
    "        id=\"test_001\",\n",
    "        query=\"What is DGX Spark's memory?\",\n",
    "        expected_answer=\"128GB unified memory\",\n",
    "        category=TestCategory.FACTUAL_RETRIEVAL,\n",
    "        difficulty=Difficulty.EASY,\n",
    "        keywords=[\"128GB\", \"unified\"]\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"test_002\",\n",
    "        query=\"How many CUDA cores?\",\n",
    "        expected_answer=\"6,144 CUDA cores\",\n",
    "        category=TestCategory.FACTUAL_RETRIEVAL,\n",
    "        difficulty=Difficulty.EASY,\n",
    "        keywords=[\"6144\", \"CUDA\"]\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create a simple test agent\n",
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama3.1:8b\", temperature=0.1)\n",
    "\n",
    "def test_agent(query: str) -> str:\n",
    "    knowledge = \"DGX Spark has 128GB unified memory and 6,144 CUDA cores.\"\n",
    "    return llm.invoke(f\"Based on: {knowledge}\\n\\nAnswer: {query}\")\n",
    "\n",
    "print(\"Test setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and use the pipeline\n",
    "pipeline = ContinuousEvaluationPipeline(\n",
    "    agent_func=test_agent,\n",
    "    test_cases=test_cases,\n",
    "    agent_name=\"Test RAG Agent\"\n",
    ")\n",
    "\n",
    "# Simulate multiple evaluation runs\n",
    "for version in [\"1.0.0\", \"1.0.1\", \"1.1.0\"]:\n",
    "    snapshot = pipeline.run_evaluation(version=version)\n",
    "    print(f\"  Score: {snapshot.overall_score:.1%}, Latency: {snapshot.mean_latency_ms:.0f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the trend report\n",
    "print(pipeline.get_trend_report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save history\n",
    "output_dir = Path.cwd().parent / \"data\" / \"benchmark_results\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "pipeline.save_history(str(output_dir / \"continuous_eval_history.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Continuous evaluation** catches regressions early\n",
    "2. **Alerting thresholds** provide automatic monitoring\n",
    "3. **Version tracking** helps identify which changes caused issues\n",
    "4. **Trend analysis** shows long-term performance patterns\n",
    "5. **Persistent history** enables post-mortem analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
