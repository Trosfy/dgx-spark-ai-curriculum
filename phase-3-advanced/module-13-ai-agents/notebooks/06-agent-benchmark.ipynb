{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 13.6: Agent Benchmarking & Evaluation Framework\n",
    "\n",
    "**Module:** 13 - AI Agents & Agentic Systems  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand how to evaluate AI agent performance\n",
    "- [ ] Build a comprehensive benchmarking framework\n",
    "- [ ] Implement metrics for retrieval, generation, and tool use\n",
    "- [ ] Create test suites for your agents\n",
    "- [ ] Generate actionable performance reports\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Tasks 13.1-13.5\n",
    "- Knowledge of: Testing, metrics, evaluation concepts\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**Why benchmark agents?**\n",
    "\n",
    "Building an agent is just the beginning. You need to know:\n",
    "- üéØ **Accuracy**: Does it give correct answers?\n",
    "- ‚ö° **Speed**: Is it fast enough for your use case?\n",
    "- üîß **Reliability**: Does it consistently work?\n",
    "- üìä **Improvement**: How do changes affect performance?\n",
    "\n",
    "**Real scenarios:**\n",
    "- üè¢ **Enterprise**: \"Can we deploy this agent to 1000 users?\"\n",
    "- üî¨ **Research**: \"Did our improvements actually help?\"\n",
    "- üí∞ **Business**: \"What's the ROI on this AI investment?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: Agent Evaluation\n",
    "\n",
    "> **Imagine you're grading a student's test...** üìù\n",
    ">\n",
    "> You don't just mark answers right or wrong. You look at:\n",
    "> - Did they understand the question?\n",
    "> - Was their reasoning correct?\n",
    "> - Did they show their work?\n",
    "> - How fast did they finish?\n",
    ">\n",
    "> **Agent evaluation is the same!**\n",
    "> - **Retrieval**: Did it find the right information?\n",
    "> - **Reasoning**: Did it use the right tools?\n",
    "> - **Generation**: Was the answer helpful?\n",
    "> - **Latency**: How long did it take?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Standard imports\nimport os\nimport sys\nfrom pathlib import Path\nfrom typing import List, Dict, Any, Optional, Callable\nfrom dataclasses import dataclass, field\nimport json\nimport time\nimport statistics\nfrom datetime import datetime\n\n# Add scripts directory with robust path resolution\n# This handles cases where the notebook is run from different working directories\nNOTEBOOK_DIR = Path(os.getcwd())\n\n# Try multiple approaches to find the scripts directory\nscripts_paths = [\n    NOTEBOOK_DIR.parent / 'scripts',  # Standard case: running from notebooks/\n    NOTEBOOK_DIR / 'scripts',          # Running from module root\n    Path(__file__).parent.parent / 'scripts' if '__file__' in dir() else None,\n]\n\nscripts_found = False\nfor scripts_path in scripts_paths:\n    if scripts_path and scripts_path.exists():\n        sys.path.insert(0, str(scripts_path))\n        scripts_found = True\n        print(f\"‚úÖ Scripts directory found: {scripts_path}\")\n        break\n\nif not scripts_found:\n    print(\"‚ö†Ô∏è Scripts directory not found. Trying import anyway...\")\n\n# Import our benchmark utilities\ntry:\n    from benchmark_utils import (\n        TestCase, TestResult, BenchmarkResults, TestCategory, Difficulty,\n        AgentEvaluator, keyword_match_score, f1_score, rouge_l_score,\n        generate_report, save_results_json, load_test_cases_from_json\n    )\n    print(\"‚úÖ Imports successful!\")\nexcept ImportError as e:\n    print(f\"‚ùå Import failed: {e}\")\n    print(\"   Make sure you're running from the notebooks/ directory\")\n    print(\"   Or run: cd phase-3-advanced/module-13-ai-agents/notebooks\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM for testing\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0.1,  # Low temperature for more consistent results\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "print(\"LLM initialized for evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Creating Test Cases\n",
    "\n",
    "Good test cases are specific, measurable, and cover various scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test cases for our RAG system\n",
    "rag_test_cases = [\n",
    "    # Factual retrieval - Easy\n",
    "    TestCase(\n",
    "        id=\"rag_001\",\n",
    "        query=\"What is the memory capacity of DGX Spark?\",\n",
    "        expected_answer=\"128GB unified LPDDR5X memory\",\n",
    "        category=TestCategory.FACTUAL_RETRIEVAL,\n",
    "        difficulty=Difficulty.EASY,\n",
    "        keywords=[\"128GB\", \"unified\", \"memory\", \"LPDDR5X\"]\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"rag_002\",\n",
    "        query=\"How many CUDA cores does the Blackwell GB10 have?\",\n",
    "        expected_answer=\"6,144 CUDA cores\",\n",
    "        category=TestCategory.FACTUAL_RETRIEVAL,\n",
    "        difficulty=Difficulty.EASY,\n",
    "        keywords=[\"6144\", \"CUDA\", \"cores\"]\n",
    "    ),\n",
    "    \n",
    "    # Multi-hop reasoning - Medium\n",
    "    TestCase(\n",
    "        id=\"rag_003\",\n",
    "        query=\"Can a 70B parameter model fit in DGX Spark's memory in FP16?\",\n",
    "        expected_answer=\"Yes, a 70B model in FP16 requires about 140GB, and DGX Spark has 128GB. With some optimization it can fit.\",\n",
    "        category=TestCategory.MULTI_HOP_REASONING,\n",
    "        difficulty=Difficulty.MEDIUM,\n",
    "        keywords=[\"70B\", \"FP16\", \"memory\", \"fit\"]\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"rag_004\",\n",
    "        query=\"What precision formats does DGX Spark's Blackwell architecture support natively?\",\n",
    "        expected_answer=\"FP4, FP8, bfloat16\",\n",
    "        category=TestCategory.FACTUAL_RETRIEVAL,\n",
    "        difficulty=Difficulty.MEDIUM,\n",
    "        keywords=[\"FP4\", \"FP8\", \"bfloat16\", \"Blackwell\"]\n",
    "    ),\n",
    "    \n",
    "    # Complex reasoning - Hard\n",
    "    TestCase(\n",
    "        id=\"rag_005\",\n",
    "        query=\"Compare the advantages of LoRA vs full fine-tuning for a 70B model on DGX Spark.\",\n",
    "        expected_answer=\"LoRA allows fine-tuning with minimal additional parameters (0.1% of total), enabling 70B model fine-tuning on DGX Spark's 128GB memory. Full fine-tuning would require storing optimizer states for all parameters.\",\n",
    "        category=TestCategory.MULTI_HOP_REASONING,\n",
    "        difficulty=Difficulty.HARD,\n",
    "        keywords=[\"LoRA\", \"fine-tuning\", \"memory\", \"parameters\"]\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Created {len(rag_test_cases)} RAG test cases\")\n",
    "for tc in rag_test_cases:\n",
    "    print(f\"  - {tc.id}: {tc.category.value} ({tc.difficulty.value})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test cases for tool use\n",
    "tool_test_cases = [\n",
    "    TestCase(\n",
    "        id=\"tool_001\",\n",
    "        query=\"Calculate 15% tip on a $127.50 restaurant bill.\",\n",
    "        expected_answer=\"$19.13\",\n",
    "        category=TestCategory.CALCULATION,\n",
    "        difficulty=Difficulty.EASY,\n",
    "        keywords=[\"19.13\", \"19.125\"],\n",
    "        requires_tool=\"calculator\"\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"tool_002\",\n",
    "        query=\"What is the square root of 256 multiplied by 4?\",\n",
    "        expected_answer=\"64\",\n",
    "        category=TestCategory.CALCULATION,\n",
    "        difficulty=Difficulty.EASY,\n",
    "        keywords=[\"64\"],\n",
    "        requires_tool=\"calculator\"\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"tool_003\",\n",
    "        query=\"Calculate how many 7B parameter models (at 2 bytes per parameter) can fit in 128GB.\",\n",
    "        expected_answer=\"About 9 models (128GB / 14GB per model)\",\n",
    "        category=TestCategory.CALCULATION,\n",
    "        difficulty=Difficulty.MEDIUM,\n",
    "        keywords=[\"9\", \"models\"],\n",
    "        requires_tool=\"calculator\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"\\nCreated {len(tool_test_cases)} tool use test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Understanding Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's understand each metric\n",
    "\n",
    "# Sample response and expected answer\n",
    "expected = \"The DGX Spark has 128GB of unified LPDDR5X memory.\"\n",
    "response1 = \"DGX Spark features 128GB unified memory using LPDDR5X technology.\"  # Good\n",
    "response2 = \"The system has a lot of memory.\"  # Vague\n",
    "response3 = \"DGX Spark has 64GB of DDR5 memory.\"  # Wrong\n",
    "\n",
    "print(\"METRIC COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Expected: {expected}\")\n",
    "print(\"\\nResponses:\")\n",
    "print(f\"  1 (Good): {response1}\")\n",
    "print(f\"  2 (Vague): {response2}\")\n",
    "print(f\"  3 (Wrong): {response3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric 1: Keyword Match\n",
    "keywords = [\"128GB\", \"unified\", \"LPDDR5X\", \"memory\"]\n",
    "\n",
    "print(\"\\n1. KEYWORD MATCH SCORE\")\n",
    "print(\"   Measures: What % of expected keywords appear in response\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "for name, response in [(\"Good\", response1), (\"Vague\", response2), (\"Wrong\", response3)]:\n",
    "    score = keyword_match_score(response, keywords)\n",
    "    print(f\"   {name}: {score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric 2: F1 Score (Token Overlap)\n",
    "print(\"\\n2. F1 SCORE (Token Overlap)\")\n",
    "print(\"   Measures: Balance of precision and recall on words\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "expected_tokens = expected.lower().split()\n",
    "\n",
    "for name, response in [(\"Good\", response1), (\"Vague\", response2), (\"Wrong\", response3)]:\n",
    "    response_tokens = response.lower().split()\n",
    "    score = f1_score(response_tokens, expected_tokens)\n",
    "    print(f\"   {name}: {score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric 3: ROUGE-L (Longest Common Subsequence)\n",
    "print(\"\\n3. ROUGE-L SCORE\")\n",
    "print(\"   Measures: Longest common subsequence similarity\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "for name, response in [(\"Good\", response1), (\"Vague\", response2), (\"Wrong\", response3)]:\n",
    "    score = rouge_l_score(response, expected)\n",
    "    print(f\"   {name}: {score:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Building the Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple agent for testing\n",
    "def create_simple_rag_agent(llm):\n",
    "    \"\"\"Create a simple RAG agent for testing.\"\"\"\n",
    "    # Simulated knowledge base\n",
    "    knowledge = \"\"\"\n",
    "    DGX Spark is NVIDIA's desktop AI supercomputer featuring:\n",
    "    - 128GB unified LPDDR5X memory\n",
    "    - Blackwell GB10 Superchip with 6,144 CUDA cores\n",
    "    - 192 5th-generation Tensor Cores\n",
    "    - Native support for FP4, FP8, and bfloat16 precision\n",
    "    - Up to 1 PFLOP FP4 compute\n",
    "    \n",
    "    LoRA (Low-Rank Adaptation) enables efficient fine-tuning by:\n",
    "    - Training only 0.1% of total parameters\n",
    "    - Keeping base model weights frozen\n",
    "    - Using rank decomposition matrices\n",
    "    \"\"\"\n",
    "    \n",
    "    def agent_func(query: str) -> str:\n",
    "        prompt = f\"\"\"Based on this knowledge:\n",
    "{knowledge}\n",
    "\n",
    "Answer the question concisely:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        return llm.invoke(prompt)\n",
    "    \n",
    "    return agent_func\n",
    "\n",
    "# Create the agent\n",
    "test_agent = create_simple_rag_agent(llm)\n",
    "print(\"Test agent created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent\n",
    "test_query = \"What is the memory capacity of DGX Spark?\"\n",
    "response = test_agent(test_query)\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the evaluator\n",
    "evaluator = AgentEvaluator(\n",
    "    agent_func=test_agent,\n",
    "    embedding_model=None,  # Skip semantic similarity for speed\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Evaluator created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Running the Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmark on RAG test cases\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RUNNING RAG BENCHMARK\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "rag_results = evaluator.run_benchmark(\n",
    "    test_cases=rag_test_cases,\n",
    "    name=\"RAG Pipeline Evaluation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display the report\n",
    "report = generate_report(rag_results)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed look at individual results\n",
    "print(\"\\nDETAILED RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for result in rag_results.results:\n",
    "    print(f\"\\nTest: {result.test_case.id}\")\n",
    "    print(f\"Query: {result.test_case.query}\")\n",
    "    print(f\"Expected: {result.test_case.expected_answer[:100]}...\")\n",
    "    print(f\"Got: {result.agent_response[:100]}...\")\n",
    "    print(f\"Score: {result.score:.2%} | Latency: {result.latency_ms:.0f}ms\")\n",
    "    print(f\"Metrics: {result.metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Comparing Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a second agent with different settings\n",
    "def create_detailed_agent(llm):\n",
    "    \"\"\"Agent that gives more detailed responses.\"\"\"\n",
    "    knowledge = \"\"\"\n",
    "    DGX Spark is NVIDIA's desktop AI supercomputer featuring:\n",
    "    - 128GB unified LPDDR5X memory\n",
    "    - Blackwell GB10 Superchip with 6,144 CUDA cores\n",
    "    - 192 5th-generation Tensor Cores\n",
    "    \"\"\"\n",
    "    \n",
    "    def agent_func(query: str) -> str:\n",
    "        prompt = f\"\"\"Based on this knowledge:\n",
    "{knowledge}\n",
    "\n",
    "Provide a detailed, comprehensive answer to:\n",
    "{query}\n",
    "\n",
    "Include specific numbers and technical details.\n",
    "\n",
    "Answer:\"\"\"\n",
    "        return llm.invoke(prompt)\n",
    "    \n",
    "    return agent_func\n",
    "\n",
    "detailed_agent = create_detailed_agent(llm)\n",
    "print(\"Detailed agent created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both agents\n",
    "from benchmark_utils import compare_agents\n",
    "\n",
    "# Evaluate simple agent\n",
    "simple_evaluator = AgentEvaluator(test_agent, verbose=False)\n",
    "simple_results = simple_evaluator.run_benchmark(rag_test_cases, \"Simple Agent\")\n",
    "\n",
    "# Evaluate detailed agent\n",
    "detailed_evaluator = AgentEvaluator(detailed_agent, verbose=False)\n",
    "detailed_results = detailed_evaluator.run_benchmark(rag_test_cases, \"Detailed Agent\")\n",
    "\n",
    "# Compare\n",
    "comparison = compare_agents(\n",
    "    results_list=[simple_results, detailed_results],\n",
    "    agent_names=[\"Simple Agent\", \"Detailed Agent\"]\n",
    ")\n",
    "\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Saving and Loading Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "output_dir = Path.cwd().parent / \"data\" / \"benchmark_results\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "results_file = output_dir / f\"rag_benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "save_results_json(rag_results, str(results_file))\n",
    "\n",
    "print(f\"Results saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and view saved results\n",
    "with open(results_file, 'r') as f:\n",
    "    loaded_data = json.load(f)\n",
    "\n",
    "print(\"Loaded benchmark results:\")\n",
    "print(f\"  Name: {loaded_data['name']}\")\n",
    "print(f\"  Overall Score: {loaded_data['overall_score']:.2%}\")\n",
    "print(f\"  Test Count: {len(loaded_data['results'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Building a Test Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentTestSuite:\n",
    "    \"\"\"A comprehensive test suite for AI agents.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent_func: Callable, name: str):\n",
    "        self.agent_func = agent_func\n",
    "        self.name = name\n",
    "        self.test_cases: Dict[str, List[TestCase]] = {\n",
    "            'retrieval': [],\n",
    "            'reasoning': [],\n",
    "            'tool_use': [],\n",
    "        }\n",
    "        self.results: Dict[str, BenchmarkResults] = {}\n",
    "    \n",
    "    def add_test_cases(self, category: str, cases: List[TestCase]):\n",
    "        \"\"\"Add test cases to a category.\"\"\"\n",
    "        if category not in self.test_cases:\n",
    "            self.test_cases[category] = []\n",
    "        self.test_cases[category].extend(cases)\n",
    "    \n",
    "    def run_all(self, verbose: bool = True) -> Dict[str, BenchmarkResults]:\n",
    "        \"\"\"Run all test categories.\"\"\"\n",
    "        evaluator = AgentEvaluator(self.agent_func, verbose=verbose)\n",
    "        \n",
    "        for category, cases in self.test_cases.items():\n",
    "            if cases:\n",
    "                if verbose:\n",
    "                    print(f\"\\n{'='*60}\")\n",
    "                    print(f\"Running {category.upper()} tests...\")\n",
    "                    print('='*60)\n",
    "                \n",
    "                self.results[category] = evaluator.run_benchmark(\n",
    "                    cases, f\"{self.name} - {category}\"\n",
    "                )\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def get_summary(self) -> str:\n",
    "        \"\"\"Get a summary of all test results.\"\"\"\n",
    "        lines = [\n",
    "            \"=\"*60,\n",
    "            f\"TEST SUITE SUMMARY: {self.name}\",\n",
    "            \"=\"*60,\n",
    "            \"\"\n",
    "        ]\n",
    "        \n",
    "        total_tests = 0\n",
    "        total_passed = 0\n",
    "        \n",
    "        for category, result in self.results.items():\n",
    "            passed = sum(1 for r in result.results if r.passed)\n",
    "            total = len(result.results)\n",
    "            total_tests += total\n",
    "            total_passed += passed\n",
    "            \n",
    "            lines.append(f\"{category.capitalize()}:\")\n",
    "            lines.append(f\"  Score: {result.overall_score:.1%}\")\n",
    "            lines.append(f\"  Passed: {passed}/{total}\")\n",
    "            lines.append(f\"  Avg Latency: {result.timing_stats['mean_latency_ms']:.0f}ms\")\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        lines.append(\"-\"*40)\n",
    "        lines.append(f\"OVERALL: {total_passed}/{total_tests} tests passed\")\n",
    "        lines.append(\"=\"*60)\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "print(\"AgentTestSuite class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run a test suite\n",
    "suite = AgentTestSuite(test_agent, \"RAG Agent v1.0\")\n",
    "\n",
    "# Add test cases\n",
    "suite.add_test_cases('retrieval', [\n",
    "    tc for tc in rag_test_cases if tc.category == TestCategory.FACTUAL_RETRIEVAL\n",
    "])\n",
    "suite.add_test_cases('reasoning', [\n",
    "    tc for tc in rag_test_cases if tc.category == TestCategory.MULTI_HOP_REASONING\n",
    "])\n",
    "\n",
    "# Run all tests\n",
    "all_results = suite.run_all(verbose=True)\n",
    "\n",
    "# Get summary\n",
    "print(\"\\n\" + suite.get_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Only Testing Happy Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Only test cases where the answer is in the knowledge\n",
    "# good_case = TestCase(query=\"What is DGX Spark?\", ...)\n",
    "\n",
    "# ‚úÖ Right: Also test edge cases\n",
    "edge_cases = [\n",
    "    # Out of domain\n",
    "    TestCase(\n",
    "        id=\"edge_001\",\n",
    "        query=\"What is the capital of France?\",\n",
    "        expected_answer=\"I don't have information about that\",\n",
    "        category=TestCategory.FACTUAL_RETRIEVAL,\n",
    "        difficulty=Difficulty.EASY,\n",
    "        keywords=[\"don't\", \"not\", \"outside\"]\n",
    "    ),\n",
    "    # Ambiguous\n",
    "    TestCase(\n",
    "        id=\"edge_002\",\n",
    "        query=\"Is it good?\",\n",
    "        expected_answer=\"I need more context to answer that question\",\n",
    "        category=TestCategory.FACTUAL_RETRIEVAL,\n",
    "        difficulty=Difficulty.MEDIUM,\n",
    "        keywords=[\"context\", \"clarify\", \"specific\"]\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"Always test edge cases and failure modes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Ignoring Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Only measuring accuracy\n",
    "# score = accuracy_only(response, expected)\n",
    "\n",
    "# ‚úÖ Right: Track latency as a key metric\n",
    "def evaluate_with_latency(agent_func, query):\n",
    "    start = time.time()\n",
    "    response = agent_func(query)\n",
    "    latency = (time.time() - start) * 1000  # ms\n",
    "    \n",
    "    return {\n",
    "        'response': response,\n",
    "        'latency_ms': latency,\n",
    "        'acceptable': latency < 5000  # 5 second threshold\n",
    "    }\n",
    "\n",
    "print(\"Latency matters for user experience!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ Why agent evaluation is crucial\n",
    "- ‚úÖ Different metrics: keyword match, F1, ROUGE-L\n",
    "- ‚úÖ Building test cases for various scenarios\n",
    "- ‚úÖ Creating a benchmarking framework\n",
    "- ‚úÖ Comparing different agent configurations\n",
    "- ‚úÖ Saving and analyzing results\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "Build a continuous evaluation pipeline that:\n",
    "1. Runs automatically on code changes\n",
    "2. Tracks metrics over time\n",
    "3. Alerts when performance degrades\n",
    "4. Generates trend reports\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [RAGAS - RAG Assessment](https://docs.ragas.io/)\n",
    "- [LangSmith Evaluation](https://docs.smith.langchain.com/)\n",
    "- [Eleuther AI LM Eval Harness](https://github.com/EleutherAI/lm-evaluation-harness)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Comprehensive cleanup for DGX Spark\nimport gc\n\n# Clear GPU memory if available\ntry:\n    import torch\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n        torch.cuda.empty_cache()\n        allocated = torch.cuda.memory_allocated() / 1e9\n        print(f\"‚úÖ GPU memory cleared ({allocated:.2f} GB still allocated)\")\nexcept ImportError:\n    pass\n\n# Python garbage collection\ngc.collect()\nprint(\"‚úÖ Cleanup complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Summary\n",
    "\n",
    "In this notebook, you built a comprehensive agent evaluation framework:\n",
    "\n",
    "1. **Test Cases**: Structured tests with expected answers\n",
    "2. **Metrics**: Keyword match, F1, ROUGE-L, latency\n",
    "3. **Evaluator**: Automated testing and scoring\n",
    "4. **Test Suite**: Organized test categories\n",
    "5. **Reports**: Actionable performance summaries\n",
    "\n",
    "**Key takeaways:**\n",
    "- Always measure before and after changes\n",
    "- Test edge cases, not just happy paths\n",
    "- Track latency alongside accuracy\n",
    "- Save results for historical comparison\n",
    "\n",
    "**Congratulations! You've completed Module 13: AI Agents & Agentic Systems!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}