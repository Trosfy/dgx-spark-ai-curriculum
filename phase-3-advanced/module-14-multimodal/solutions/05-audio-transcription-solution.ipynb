{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 14.5 Solutions: Audio Transcription\n",
    "\n",
    "This notebook contains solutions to the exercises in the Audio Transcription notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Challenge Solution: Streaming Transcription System\n\nThe challenge was to create a real-time streaming transcription system that processes audio in chunks.\n\n### Approach\nWe implement a `StreamingTranscriber` class that:\n1. **Chunks Audio**: Splits audio into fixed-size chunks (e.g., 5 seconds)\n2. **Maintains Context**: Keeps overlap between chunks to avoid cutting words\n3. **Processes Incrementally**: Transcribes each chunk and yields results immediately\n4. **Tracks Statistics**: Monitors real-time factor to ensure we can keep up\n\n### Key Design Decisions\n- **Chunk Size**: 5 seconds balances latency vs context (longer = more accurate but higher latency)\n- **Overlap**: 0.5 second overlap prevents word truncation at chunk boundaries\n- **Buffering**: Stores incomplete audio for the next chunk\n\n### Real-Time Factor (RTF)\nRTF = processing_time / audio_duration\n- RTF < 1.0: Faster than real-time (can handle live streaming)\n- RTF > 1.0: Slower than real-time (will fall behind)\n\nOn DGX Spark with Whisper large-v3, we typically achieve RTF of 0.3-0.5x (3x faster than real-time)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "from typing import List, Generator\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Whisper\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "print(\"Loading Whisper...\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v3\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    \"openai/whisper-large-v3\",\n",
    "    torch_dtype=torch.bfloat16  # Optimized for Blackwell\n",
    ").to(\"cuda\")\n",
    "print(\"Loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingTranscriber:\n",
    "    \"\"\"\n",
    "    Real-time streaming transcription system.\n",
    "    \n",
    "    Processes audio in chunks and provides incremental transcription,\n",
    "    simulating a real-time streaming scenario.\n",
    "    \n",
    "    Example:\n",
    "        >>> transcriber = StreamingTranscriber(chunk_size_seconds=5.0)\n",
    "        >>> for chunk in audio_chunks:\n",
    "        ...     text = transcriber.process_chunk(chunk)\n",
    "        ...     print(text)\n",
    "        >>> final = transcriber.get_full_transcript()\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        chunk_size_seconds: float = 5.0,\n",
    "        sample_rate: int = 16000,\n",
    "        overlap_seconds: float = 0.5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the streaming transcriber.\n",
    "        \n",
    "        Args:\n",
    "            chunk_size_seconds: Size of each audio chunk to process\n",
    "            sample_rate: Audio sample rate (must be 16000 for Whisper)\n",
    "            overlap_seconds: Overlap between chunks to maintain context\n",
    "        \"\"\"\n",
    "        self.chunk_size = int(chunk_size_seconds * sample_rate)\n",
    "        self.overlap_size = int(overlap_seconds * sample_rate)\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "        # Buffer for incomplete audio\n",
    "        self.audio_buffer = np.array([], dtype=np.float32)\n",
    "        \n",
    "        # Store all transcriptions\n",
    "        self.transcriptions: List[str] = []\n",
    "        \n",
    "        # Previous chunk's last bit for context\n",
    "        self.previous_overlap = np.array([], dtype=np.float32)\n",
    "        \n",
    "        self.total_audio_duration = 0.0\n",
    "        self.total_processing_time = 0.0\n",
    "    \n",
    "    def _transcribe_chunk(self, audio_chunk: np.ndarray) -> str:\n",
    "        \"\"\"\n",
    "        Transcribe a single audio chunk.\n",
    "        \n",
    "        Args:\n",
    "            audio_chunk: Audio data for this chunk\n",
    "            \n",
    "        Returns:\n",
    "            Transcription text for this chunk\n",
    "        \"\"\"\n",
    "        # Ensure correct format\n",
    "        audio_chunk = audio_chunk.astype(np.float32)\n",
    "        if audio_chunk.max() > 1.0:\n",
    "            audio_chunk = audio_chunk / np.abs(audio_chunk).max()\n",
    "        \n",
    "        # Process with Whisper\n",
    "        input_features = processor(\n",
    "            audio_chunk,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features.to(model.device, dtype=torch.float16)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            predicted_ids = model.generate(\n",
    "                input_features,\n",
    "                max_new_tokens=128  # Shorter for real-time\n",
    "            )\n",
    "        \n",
    "        transcription = processor.batch_decode(\n",
    "            predicted_ids,\n",
    "            skip_special_tokens=True\n",
    "        )[0]\n",
    "        \n",
    "        return transcription.strip()\n",
    "    \n",
    "    def process_chunk(self, audio_chunk: np.ndarray) -> str:\n",
    "        \"\"\"\n",
    "        Process a single audio chunk and return its transcription.\n",
    "        \n",
    "        Args:\n",
    "            audio_chunk: Audio data for this chunk\n",
    "            \n",
    "        Returns:\n",
    "            Transcription for this chunk\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Add overlap from previous chunk for context\n",
    "        if len(self.previous_overlap) > 0:\n",
    "            audio_with_context = np.concatenate([self.previous_overlap, audio_chunk])\n",
    "        else:\n",
    "            audio_with_context = audio_chunk\n",
    "        \n",
    "        # Transcribe\n",
    "        transcription = self._transcribe_chunk(audio_with_context)\n",
    "        \n",
    "        # Store transcription\n",
    "        if transcription:\n",
    "            self.transcriptions.append(transcription)\n",
    "        \n",
    "        # Save overlap for next chunk\n",
    "        if len(audio_chunk) > self.overlap_size:\n",
    "            self.previous_overlap = audio_chunk[-self.overlap_size:]\n",
    "        else:\n",
    "            self.previous_overlap = audio_chunk.copy()\n",
    "        \n",
    "        # Update stats\n",
    "        chunk_duration = len(audio_chunk) / self.sample_rate\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        self.total_audio_duration += chunk_duration\n",
    "        self.total_processing_time += processing_time\n",
    "        \n",
    "        return transcription\n",
    "    \n",
    "    def stream_audio(self, audio: np.ndarray) -> Generator[str, None, None]:\n",
    "        \"\"\"\n",
    "        Stream through audio and yield transcriptions.\n",
    "        \n",
    "        Args:\n",
    "            audio: Full audio to process\n",
    "            \n",
    "        Yields:\n",
    "            Transcription for each chunk\n",
    "        \"\"\"\n",
    "        # Split into chunks\n",
    "        for i in range(0, len(audio), self.chunk_size):\n",
    "            chunk = audio[i:i + self.chunk_size]\n",
    "            \n",
    "            # Skip very short final chunks\n",
    "            if len(chunk) < self.sample_rate * 0.5:  # Less than 0.5 seconds\n",
    "                continue\n",
    "            \n",
    "            transcription = self.process_chunk(chunk)\n",
    "            yield transcription\n",
    "    \n",
    "    def get_full_transcript(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the combined transcript from all chunks.\n",
    "        \n",
    "        Returns:\n",
    "            Full transcript as a single string\n",
    "        \"\"\"\n",
    "        # Simple joining - in production, you'd use more sophisticated merging\n",
    "        return \" \".join(self.transcriptions)\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"\n",
    "        Get processing statistics.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with processing stats\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'total_audio_duration': self.total_audio_duration,\n",
    "            'total_processing_time': self.total_processing_time,\n",
    "            'real_time_factor': self.total_processing_time / max(self.total_audio_duration, 0.001),\n",
    "            'chunks_processed': len(self.transcriptions),\n",
    "            'is_real_time': self.total_processing_time < self.total_audio_duration\n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the transcriber state.\"\"\"\n",
    "        self.audio_buffer = np.array([], dtype=np.float32)\n",
    "        self.previous_overlap = np.array([], dtype=np.float32)\n",
    "        self.transcriptions = []\n",
    "        self.total_audio_duration = 0.0\n",
    "        self.total_processing_time = 0.0\n",
    "\n",
    "print(\"StreamingTranscriber class ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with synthetic audio (tones - will produce minimal transcription)\n",
    "\n",
    "def generate_test_audio(duration: float, sample_rate: int = 16000) -> np.ndarray:\n",
    "    \"\"\"Generate test audio (tones).\"\"\"\n",
    "    t = np.linspace(0, duration, int(sample_rate * duration), endpoint=False)\n",
    "    # Mix of frequencies\n",
    "    audio = 0.3 * np.sin(2 * np.pi * 440 * t)  # A4\n",
    "    audio += 0.3 * np.sin(2 * np.pi * 880 * t)  # A5\n",
    "    return audio.astype(np.float32)\n",
    "\n",
    "# Generate 30 seconds of test audio\n",
    "test_audio = generate_test_audio(30.0)\n",
    "print(f\"Generated {len(test_audio) / 16000:.1f} seconds of test audio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test streaming transcription\n",
    "transcriber = StreamingTranscriber(chunk_size_seconds=5.0, overlap_seconds=0.5)\n",
    "\n",
    "print(\"Streaming transcription simulation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "chunk_num = 0\n",
    "for transcription in transcriber.stream_audio(test_audio):\n",
    "    chunk_num += 1\n",
    "    stats = transcriber.get_stats()\n",
    "    print(f\"\\nChunk {chunk_num}:\")\n",
    "    print(f\"  Transcription: '{transcription}'\")\n",
    "    print(f\"  Audio: {stats['total_audio_duration']:.1f}s | Processing: {stats['total_processing_time']:.1f}s\")\n",
    "    print(f\"  Real-time factor: {stats['real_time_factor']:.2f}x\")\n",
    "\n",
    "# Final stats\n",
    "final_stats = transcriber.get_stats()\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FINAL STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total audio duration: {final_stats['total_audio_duration']:.1f}s\")\n",
    "print(f\"Total processing time: {final_stats['total_processing_time']:.1f}s\")\n",
    "print(f\"Real-time factor: {final_stats['real_time_factor']:.2f}x\")\n",
    "print(f\"Is real-time capable: {final_stats['is_real_time']}\")\n",
    "print(f\"\\nFull transcript: '{transcriber.get_full_transcript()}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meeting Minutes Generator (using pre-made transcript)\n",
    "\n",
    "sample_meeting_transcript = \"\"\"\n",
    "Good morning everyone. Let's start the weekly status meeting. \n",
    "\n",
    "First, the development team update. We've completed the user authentication module \n",
    "and it's now in testing. John, can you share the timeline for the next sprint?\n",
    "\n",
    "Sure. We're planning to start the dashboard redesign next Monday. The estimated \n",
    "completion is two weeks. We'll need design assets from the UX team by Friday.\n",
    "\n",
    "Great. Sarah, what about the marketing side?\n",
    "\n",
    "We're preparing for the product launch on January 15th. The press release is ready, \n",
    "and we've scheduled social media posts. Budget approval is pending from finance.\n",
    "\n",
    "Okay, let's note that as an action item - follow up with finance about the budget.\n",
    "\n",
    "Any blockers? No? Alright, meeting adjourned. Next meeting is Wednesday at 10 AM.\n",
    "\"\"\"\n",
    "\n",
    "class MeetingMinutesGenerator:\n",
    "    \"\"\"Generate meeting minutes from transcripts using an LLM.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = None\n",
    "        self.tokenizer = None\n",
    "    \n",
    "    def load_llm(self):\n",
    "        if self.llm is not None:\n",
    "            return\n",
    "        \n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "        \n",
    "        print(\"Loading LLM...\")\n",
    "        model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        print(\"LLM loaded!\")\n",
    "    \n",
    "    def generate_minutes(self, transcript: str) -> dict:\n",
    "        self.load_llm()\n",
    "        \n",
    "        prompts = {\n",
    "            'summary': f\"Summarize this meeting in 2-3 sentences:\\n\\n{transcript}\",\n",
    "            'action_items': f\"Extract all action items from this meeting as a numbered list:\\n\\n{transcript}\",\n",
    "            'key_decisions': f\"What key decisions were made in this meeting?\\n\\n{transcript}\",\n",
    "            'next_steps': f\"What are the next steps mentioned in this meeting?\\n\\n{transcript}\"\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for key, prompt in prompts.items():\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that analyzes meeting transcripts.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "            \n",
    "            text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.llm.device)\n",
    "            \n",
    "            with torch.inference_mode():\n",
    "                outputs = self.llm.generate(**inputs, max_new_tokens=200, temperature=0.7)\n",
    "            \n",
    "            response = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "            results[key] = response\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Generate meeting minutes\n",
    "generator = MeetingMinutesGenerator()\n",
    "minutes = generator.generate_minutes(sample_meeting_transcript)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MEETING MINUTES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(minutes['summary'])\n",
    "\n",
    "print(\"\\n--- Action Items ---\")\n",
    "print(minutes['action_items'])\n",
    "\n",
    "print(\"\\n--- Key Decisions ---\")\n",
    "print(minutes['key_decisions'])\n",
    "\n",
    "print(\"\\n--- Next Steps ---\")\n",
    "print(minutes['next_steps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del model, processor\n",
    "if generator.llm is not None:\n",
    "    del generator.llm, generator.tokenizer\n",
    "clear_gpu_memory()\n",
    "print(\"Solutions notebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}