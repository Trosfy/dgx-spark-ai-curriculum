{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 14.1 Solutions: Vision-Language Models Demo\n",
    "\n",
    "This notebook contains solutions to the exercises in the Vision-Language Demo notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Challenge Solution: Multi-Image Comparison Tool\n\nThe challenge was to build a function that:\n1. Takes two images as input\n2. Describes each image separately\n3. Compares and contrasts the two images\n4. Suggests which image is better for a given purpose\n\n### Approach\nWe use a two-step strategy:\n1. **Individual Analysis**: First, analyze each image separately to get detailed descriptions\n2. **Combined Comparison**: Create a side-by-side composite image and ask the VLM to compare \"left\" vs \"right\"\n\nThis approach works better than just using text descriptions because VLMs can notice visual differences that are hard to describe in text (like color vibrancy, composition balance, etc.).\n\n### Alternative Approach (Also Shown)\nWe also demonstrate using CLIP embeddings for objective comparison against specific criteria. This gives numerical similarity scores that can be compared programmatically."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from typing import Dict\n",
    "import gc\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VLM\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_about_image(image: Image.Image, question: str) -> str:\n",
    "    \"\"\"Ask a question about a single image.\"\"\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": question}\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(text=[text], images=[image], return_tensors=\"pt\", padding=True)\n",
    "    inputs = inputs.to(model.device)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=200, temperature=0.7)\n",
    "    \n",
    "    generated_ids = output_ids[:, inputs.input_ids.shape[1]:]\n",
    "    return processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "\n",
    "def compare_images(\n",
    "    image1: Image.Image, \n",
    "    image2: Image.Image, \n",
    "    purpose: str = \"social media post\"\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Compare two images and recommend which is better for a given purpose.\n",
    "    \n",
    "    Args:\n",
    "        image1: First image\n",
    "        image2: Second image  \n",
    "        purpose: What the images will be used for\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with comparison results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Get descriptions of each image\n",
    "    print(\"Analyzing Image 1...\")\n",
    "    results['image1_description'] = ask_about_image(\n",
    "        image1, \n",
    "        \"Describe this image in 2-3 sentences, noting its composition, colors, and mood.\"\n",
    "    )\n",
    "    \n",
    "    print(\"Analyzing Image 2...\")\n",
    "    results['image2_description'] = ask_about_image(\n",
    "        image2,\n",
    "        \"Describe this image in 2-3 sentences, noting its composition, colors, and mood.\"\n",
    "    )\n",
    "    \n",
    "    # Create a side-by-side comparison image\n",
    "    # Resize images to same height\n",
    "    target_height = 400\n",
    "    \n",
    "    img1_resized = image1.copy()\n",
    "    img1_resized.thumbnail((img1_resized.width * target_height // img1_resized.height, target_height))\n",
    "    \n",
    "    img2_resized = image2.copy()\n",
    "    img2_resized.thumbnail((img2_resized.width * target_height // img2_resized.height, target_height))\n",
    "    \n",
    "    # Create combined image\n",
    "    gap = 20\n",
    "    combined_width = img1_resized.width + img2_resized.width + gap\n",
    "    combined = Image.new('RGB', (combined_width, target_height), 'white')\n",
    "    combined.paste(img1_resized, (0, 0))\n",
    "    combined.paste(img2_resized, (img1_resized.width + gap, 0))\n",
    "    \n",
    "    # Ask for comparison\n",
    "    print(\"Comparing images...\")\n",
    "    comparison_prompt = f\"\"\"These are two images side by side - the left image and the right image.\n",
    "    \n",
    "Compare them and tell me:\n",
    "1. What are the main differences between them?\n",
    "2. Which image would be better for {purpose} and why?\n",
    "3. What improvements could make each image better?\n",
    "\n",
    "Be specific and concise.\"\"\"\n",
    "    \n",
    "    results['comparison'] = ask_about_image(combined, comparison_prompt)\n",
    "    results['purpose'] = purpose\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"compare_images() function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the comparison function with sample images\n",
    "\n",
    "# Create two different test images\n",
    "from PIL import ImageDraw\n",
    "\n",
    "# Image 1: Bright, simple composition\n",
    "img1 = Image.new('RGB', (400, 400), 'lightblue')\n",
    "draw1 = ImageDraw.Draw(img1)\n",
    "draw1.ellipse([100, 100, 300, 300], fill='yellow')  # Sun\n",
    "\n",
    "# Image 2: Darker, complex composition  \n",
    "img2 = Image.new('RGB', (400, 400), 'darkblue')\n",
    "draw2 = ImageDraw.Draw(img2)\n",
    "draw2.ellipse([250, 50, 350, 150], fill='white')  # Moon\n",
    "for i in range(20):  # Stars\n",
    "    x, y = (i * 20) % 400, (i * 17) % 400\n",
    "    draw2.ellipse([x, y, x+5, y+5], fill='white')\n",
    "\n",
    "# Compare\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"IMAGE COMPARISON RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results = compare_images(img1, img2, purpose=\"a children's book cover\")\n",
    "\n",
    "print(f\"\\nImage 1 Description:\")\n",
    "print(f\"  {results['image1_description']}\")\n",
    "\n",
    "print(f\"\\nImage 2 Description:\")\n",
    "print(f\"  {results['image2_description']}\")\n",
    "\n",
    "print(f\"\\nComparison for '{results['purpose']}':\")\n",
    "print(f\"  {results['comparison']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach: Using CLIP for similarity-based comparison\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "\n",
    "def compare_with_clip(\n",
    "    image1: Image.Image,\n",
    "    image2: Image.Image,\n",
    "    criteria: list\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Compare images using CLIP embeddings against various criteria.\n",
    "    \n",
    "    Args:\n",
    "        image1: First image\n",
    "        image2: Second image\n",
    "        criteria: List of text descriptions to compare against\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with similarity scores\n",
    "    \"\"\"\n",
    "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    clip_model = CLIPModel.from_pretrained(\n",
    "        \"openai/clip-vit-large-patch14\",\n",
    "        torch_dtype=torch.float16\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    results = {'image1_scores': {}, 'image2_scores': {}, 'winner': {}}\n",
    "    \n",
    "    for criterion in criteria:\n",
    "        # Get embeddings\n",
    "        inputs1 = clip_processor(images=image1, text=[criterion], return_tensors=\"pt\")\n",
    "        inputs1 = {k: v.to(clip_model.device) for k, v in inputs1.items()}\n",
    "        \n",
    "        inputs2 = clip_processor(images=image2, text=[criterion], return_tensors=\"pt\")\n",
    "        inputs2 = {k: v.to(clip_model.device) for k, v in inputs2.items()}\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            outputs1 = clip_model(**inputs1)\n",
    "            outputs2 = clip_model(**inputs2)\n",
    "            \n",
    "            # Cosine similarity between image and text\n",
    "            score1 = outputs1.logits_per_image.item()\n",
    "            score2 = outputs2.logits_per_image.item()\n",
    "        \n",
    "        results['image1_scores'][criterion] = score1\n",
    "        results['image2_scores'][criterion] = score2\n",
    "        results['winner'][criterion] = 'Image 1' if score1 > score2 else 'Image 2'\n",
    "    \n",
    "    # Cleanup\n",
    "    del clip_model, clip_processor\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test CLIP comparison\n",
    "criteria = [\n",
    "    \"a bright and cheerful image\",\n",
    "    \"a calm and peaceful scene\",\n",
    "    \"suitable for children\",\n",
    "    \"professional and polished\"\n",
    "]\n",
    "\n",
    "clip_results = compare_with_clip(img1, img2, criteria)\n",
    "\n",
    "print(\"\\nCLIP-based comparison:\")\n",
    "print(\"-\" * 50)\n",
    "for criterion in criteria:\n",
    "    print(f\"\\n{criterion}:\")\n",
    "    print(f\"  Image 1: {clip_results['image1_scores'][criterion]:.2f}\")\n",
    "    print(f\"  Image 2: {clip_results['image2_scores'][criterion]:.2f}\")\n",
    "    print(f\"  Winner: {clip_results['winner'][criterion]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del model, processor\n",
    "clear_gpu_memory()\n",
    "print(\"Solutions notebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}