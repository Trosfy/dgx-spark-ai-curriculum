{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 14.3: Multimodal RAG\n",
    "\n",
    "**Module:** 14 - Multimodal AI  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ⭐⭐⭐⭐\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand how CLIP creates joint image-text embeddings\n",
    "- [ ] Build a multimodal index with both images and text\n",
    "- [ ] Search images using natural language queries\n",
    "- [ ] Create a complete multimodal RAG pipeline\n",
    "- [ ] Combine image search with VLM-powered Q&A\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Task 14.1 (Vision-Language Demo)\n",
    "- Knowledge of: Embeddings, vector databases (from Module 13)\n",
    "- Running in: NGC PyTorch container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "Traditional RAG works with text only. But what if your knowledge base includes images, diagrams, and photos?\n",
    "\n",
    "**Industry Applications:**\n",
    "- **E-commerce**: \"Show me red dresses similar to this photo\"\n",
    "- **Medical**: \"Find X-rays showing similar conditions\"\n",
    "- **Manufacturing**: \"Locate defect images matching this description\"\n",
    "- **Real Estate**: \"Find properties with modern kitchens like this\"\n",
    "- **Fashion**: \"Search for outfits matching this style\"\n",
    "\n",
    "**Why DGX Spark?**\n",
    "- CLIP: ~2GB - extremely lightweight\n",
    "- Can index thousands of images while running a VLM for Q&A\n",
    "- Fast embedding generation with Blackwell's tensor cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ELI5: What is CLIP and Multimodal Embeddings?\n",
    "\n",
    "> **Imagine you're organizing a photo album, but you want to find pictures using words, not scrolling!**\n",
    ">\n",
    "> CLIP learned by looking at 400 million images with captions. For each pair, it asked:\n",
    "> - \"What numbers describe this image?\" (image embedding)\n",
    "> - \"What numbers describe this caption?\" (text embedding)\n",
    ">\n",
    "> It learned to make the numbers for matching pairs SIMILAR, and non-matching pairs DIFFERENT.\n",
    ">\n",
    "> Now, you can:\n",
    "> 1. Give CLIP all your photos → Get number codes for each\n",
    "> 2. Type \"sunset at the beach\" → Get a number code\n",
    "> 3. Find photos with similar numbers → Beach sunset photos appear!\n",
    ">\n",
    "> **In AI terms:**\n",
    "> - **Contrastive Learning**: Train embeddings so matching pairs are close in vector space\n",
    "> - **Shared Embedding Space**: Both images and text live in the same 768-dimensional space\n",
    "> - **Zero-shot**: Works on any images without retraining!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install transformers sentence-transformers chromadb pillow -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from typing import List, Dict, Optional, Union, Tuple\n",
    "import warnings\n",
    "from IPython.display import display\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU\n",
    "print(\"=\" * 50)\n",
    "print(\"GPU Configuration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Total Memory: {total_memory:.1f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available\")\n",
    "\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    print(\"GPU memory cleared!\")\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        return f\"Allocated: {allocated:.2f}GB\"\n",
    "    return \"No GPU\"\n",
    "\n",
    "def load_image_from_url(url: str) -> Image.Image:\n",
    "    \"\"\"Load image from URL.\"\"\"\n",
    "    response = requests.get(url, timeout=10)\n",
    "    return Image.open(BytesIO(response.content)).convert('RGB')\n",
    "\n",
    "print(\"Utility functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding CLIP Architecture\n",
    "\n",
    "### The Two Towers\n",
    "\n",
    "```\n",
    "        IMAGE                           TEXT\n",
    "          │                               │\n",
    "          ▼                               ▼\n",
    "  ┌───────────────┐               ┌───────────────┐\n",
    "  │ Vision Trans. │               │ Text Trans.   │\n",
    "  │  (ViT-L/14)   │               │  (Transformer)│\n",
    "  └───────────────┘               └───────────────┘\n",
    "          │                               │\n",
    "          ▼                               ▼\n",
    "  ┌───────────────┐               ┌───────────────┐\n",
    "  │ Image Embed   │               │ Text Embed    │\n",
    "  │  [768 dims]   │               │  [768 dims]   │\n",
    "  └───────────────┘               └───────────────┘\n",
    "          │                               │\n",
    "          └──────────┬────────────────────┘\n",
    "                     ▼\n",
    "              Cosine Similarity\n",
    "```\n",
    "\n",
    "Both images and text get mapped to the SAME 768-dimensional space, allowing direct comparison!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Loading CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load CLIP model\n",
    "print(\"Loading CLIP model...\")\n",
    "\n",
    "clip_model_id = \"openai/clip-vit-large-patch14\"\n",
    "\n",
    "clip_processor = CLIPProcessor.from_pretrained(clip_model_id)\n",
    "clip_model = CLIPModel.from_pretrained(\n",
    "    clip_model_id,\n",
    "    torch_dtype=torch.bfloat16  # Use bfloat16 for Blackwell optimization\n",
    ").to(\"cuda\")\n",
    "\n",
    "clip_model.eval()\n",
    "print(f\"CLIP loaded! Model: {clip_model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embedding(image: Image.Image) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get CLIP embedding for an image.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image\n",
    "        \n",
    "    Returns:\n",
    "        Normalized embedding as numpy array\n",
    "    \"\"\"\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(clip_model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        image_features = clip_model.get_image_features(**inputs)\n",
    "    \n",
    "    # Normalize to unit vector\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    return image_features.cpu().numpy()[0]\n",
    "\n",
    "def get_text_embedding(text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get CLIP embedding for text.\n",
    "    \n",
    "    Args:\n",
    "        text: Text string\n",
    "        \n",
    "    Returns:\n",
    "        Normalized embedding as numpy array\n",
    "    \"\"\"\n",
    "    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(clip_model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        text_features = clip_model.get_text_features(**inputs)\n",
    "    \n",
    "    # Normalize to unit vector\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    return text_features.cpu().numpy()[0]\n",
    "\n",
    "def get_batch_image_embeddings(images: List[Image.Image], batch_size: int = 8) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get CLIP embeddings for a batch of images.\n",
    "    \n",
    "    Args:\n",
    "        images: List of PIL Images\n",
    "        batch_size: Processing batch size\n",
    "        \n",
    "    Returns:\n",
    "        Array of embeddings [N, 768]\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch = images[i:i+batch_size]\n",
    "        inputs = clip_processor(images=batch, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(clip_model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            features = clip_model.get_image_features(**inputs)\n",
    "            features = features / features.norm(dim=-1, keepdim=True)\n",
    "            all_embeddings.append(features.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "print(\"Embedding functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the embedding functions\n",
    "\n",
    "# Create a simple test image\n",
    "test_image = Image.new('RGB', (224, 224), color='red')\n",
    "\n",
    "# Get embeddings\n",
    "img_emb = get_image_embedding(test_image)\n",
    "text_emb = get_text_embedding(\"a solid red image\")\n",
    "\n",
    "print(f\"Image embedding shape: {img_emb.shape}\")\n",
    "print(f\"Text embedding shape: {text_emb.shape}\")\n",
    "\n",
    "# Compute similarity\n",
    "similarity = np.dot(img_emb, text_emb)\n",
    "print(f\"\\nSimilarity between red image and 'a solid red image': {similarity:.4f}\")\n",
    "\n",
    "# Try a non-matching text\n",
    "blue_emb = get_text_embedding(\"a solid blue image\")\n",
    "similarity2 = np.dot(img_emb, blue_emb)\n",
    "print(f\"Similarity between red image and 'a solid blue image': {similarity2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "CLIP correctly identified that:\n",
    "- The red image is more similar to \"a solid red image\" (higher score)\n",
    "- The red image is less similar to \"a solid blue image\" (lower score)\n",
    "\n",
    "This is the foundation of multimodal search!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Creating a Sample Image Dataset\n",
    "\n",
    "Let's create a small dataset of images to search through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic dataset with different colored shapes\n",
    "from PIL import ImageDraw\n",
    "\n",
    "def create_sample_image(shape: str, color: str, bg_color: str = \"white\") -> Image.Image:\n",
    "    \"\"\"\n",
    "    Create a simple image with a colored shape.\n",
    "    \n",
    "    Args:\n",
    "        shape: 'circle', 'square', or 'triangle'\n",
    "        color: Shape color\n",
    "        bg_color: Background color\n",
    "        \n",
    "    Returns:\n",
    "        PIL Image\n",
    "    \"\"\"\n",
    "    img = Image.new('RGB', (224, 224), color=bg_color)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    if shape == 'circle':\n",
    "        draw.ellipse([50, 50, 174, 174], fill=color)\n",
    "    elif shape == 'square':\n",
    "        draw.rectangle([50, 50, 174, 174], fill=color)\n",
    "    elif shape == 'triangle':\n",
    "        draw.polygon([(112, 50), (50, 174), (174, 174)], fill=color)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Create our dataset\n",
    "shapes = ['circle', 'square', 'triangle']\n",
    "colors = ['red', 'blue', 'green', 'yellow', 'purple', 'orange']\n",
    "\n",
    "dataset = []\n",
    "for shape in shapes:\n",
    "    for color in colors:\n",
    "        img = create_sample_image(shape, color)\n",
    "        metadata = {\n",
    "            'shape': shape,\n",
    "            'color': color,\n",
    "            'description': f\"A {color} {shape} on a white background\"\n",
    "        }\n",
    "        dataset.append({'image': img, 'metadata': metadata})\n",
    "\n",
    "print(f\"Created dataset with {len(dataset)} images\")\n",
    "\n",
    "# Show a few samples\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(9, 6))\n",
    "for idx, ax in enumerate(axes.flatten()):\n",
    "    ax.imshow(dataset[idx]['image'])\n",
    "    ax.set_title(dataset[idx]['metadata']['description'][:20])\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Building the Multimodal Index\n",
    "\n",
    "We'll use ChromaDB to store our embeddings and enable fast similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "def image_to_base64(image: Image.Image) -> str:\n",
    "    \"\"\"Convert PIL Image to base64 string for storage.\"\"\"\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format=\"PNG\")\n",
    "    return base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "def base64_to_image(b64_string: str) -> Image.Image:\n",
    "    \"\"\"Convert base64 string back to PIL Image.\"\"\"\n",
    "    return Image.open(BytesIO(base64.b64decode(b64_string)))\n",
    "\n",
    "# Create ChromaDB client (in-memory for demo)\n",
    "chroma_client = chromadb.Client(Settings(\n",
    "    anonymized_telemetry=False\n",
    "))\n",
    "\n",
    "# Create a collection for our images\n",
    "# Delete if exists (for re-running)\n",
    "try:\n",
    "    chroma_client.delete_collection(\"multimodal_demo\")\n",
    "except ValueError:\n",
    "    pass  # Collection doesn't exist\n",
    "\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"multimodal_demo\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}  # Use cosine similarity\n",
    ")\n",
    "\n",
    "print(\"ChromaDB collection created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index all images\n",
    "print(f\"Indexing {len(dataset)} images...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Get all image embeddings\n",
    "images = [item['image'] for item in dataset]\n",
    "embeddings = get_batch_image_embeddings(images)\n",
    "\n",
    "# Add to ChromaDB\n",
    "for idx, item in enumerate(dataset):\n",
    "    collection.add(\n",
    "        ids=[f\"img_{idx}\"],\n",
    "        embeddings=[embeddings[idx].tolist()],\n",
    "        metadatas=[{\n",
    "            'shape': item['metadata']['shape'],\n",
    "            'color': item['metadata']['color'],\n",
    "            'description': item['metadata']['description'],\n",
    "            'image_b64': image_to_base64(item['image'])\n",
    "        }]\n",
    "    )\n",
    "\n",
    "index_time = time.time() - start_time\n",
    "print(f\"Indexed in {index_time:.1f} seconds\")\n",
    "print(f\"Collection contains {collection.count()} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Searching with Natural Language\n",
    "\n",
    "Now the magic happens - we can search our image database using plain English!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_images(query: str, n_results: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search images using a natural language query.\n",
    "    \n",
    "    Args:\n",
    "        query: Natural language search query\n",
    "        n_results: Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of search results with images and metadata\n",
    "    \"\"\"\n",
    "    # Get text embedding for query\n",
    "    query_embedding = get_text_embedding(query)\n",
    "    \n",
    "    # Search in ChromaDB\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding.tolist()],\n",
    "        n_results=n_results,\n",
    "        include=['metadatas', 'distances']\n",
    "    )\n",
    "    \n",
    "    # Format results\n",
    "    formatted_results = []\n",
    "    for idx in range(len(results['ids'][0])):\n",
    "        metadata = results['metadatas'][0][idx]\n",
    "        distance = results['distances'][0][idx]\n",
    "        \n",
    "        formatted_results.append({\n",
    "            'id': results['ids'][0][idx],\n",
    "            'image': base64_to_image(metadata['image_b64']),\n",
    "            'shape': metadata['shape'],\n",
    "            'color': metadata['color'],\n",
    "            'description': metadata['description'],\n",
    "            'similarity': 1 - distance  # Convert distance to similarity\n",
    "        })\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "def display_search_results(query: str, results: List[Dict]):\n",
    "    \"\"\"Display search results nicely.\"\"\"\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    n_cols = min(len(results), 5)\n",
    "    fig, axes = plt.subplots(1, n_cols, figsize=(3*n_cols, 3))\n",
    "    if n_cols == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (ax, result) in enumerate(zip(axes, results)):\n",
    "        ax.imshow(result['image'])\n",
    "        ax.set_title(f\"Sim: {result['similarity']:.3f}\\n{result['color']} {result['shape']}\", fontsize=10)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Search functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test some searches!\n",
    "\n",
    "# Search 1: Find red things\n",
    "results = search_images(\"something red\", n_results=5)\n",
    "display_search_results(\"something red\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search 2: Find circles\n",
    "results = search_images(\"circular shape\", n_results=5)\n",
    "display_search_results(\"circular shape\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search 3: More abstract query\n",
    "results = search_images(\"warm colors\", n_results=5)\n",
    "display_search_results(\"warm colors\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search 4: Cool colors\n",
    "results = search_images(\"cool colors like the ocean or sky\", n_results=5)\n",
    "display_search_results(\"cool colors like the ocean or sky\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "1. **Query Embedding**: We converted the text query to a 768-dimensional vector\n",
    "2. **Similarity Search**: ChromaDB found images whose embeddings are closest to the query\n",
    "3. **Semantic Understanding**: CLIP understands concepts like \"warm colors\" even though we never explicitly labeled them!\n",
    "\n",
    "This is the power of CLIP - it has learned general visual-linguistic associations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Image-to-Image Search\n",
    "\n",
    "We can also search using an image as the query - find similar images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_image(query_image: Image.Image, n_results: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search images using another image as query.\n",
    "    \n",
    "    Args:\n",
    "        query_image: PIL Image to search with\n",
    "        n_results: Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of similar images with metadata\n",
    "    \"\"\"\n",
    "    # Get image embedding for query\n",
    "    query_embedding = get_image_embedding(query_image)\n",
    "    \n",
    "    # Search in ChromaDB\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding.tolist()],\n",
    "        n_results=n_results,\n",
    "        include=['metadatas', 'distances']\n",
    "    )\n",
    "    \n",
    "    # Format results\n",
    "    formatted_results = []\n",
    "    for idx in range(len(results['ids'][0])):\n",
    "        metadata = results['metadatas'][0][idx]\n",
    "        distance = results['distances'][0][idx]\n",
    "        \n",
    "        formatted_results.append({\n",
    "            'id': results['ids'][0][idx],\n",
    "            'image': base64_to_image(metadata['image_b64']),\n",
    "            'shape': metadata['shape'],\n",
    "            'color': metadata['color'],\n",
    "            'description': metadata['description'],\n",
    "            'similarity': 1 - distance\n",
    "        })\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "print(\"Image-to-image search ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a query image (not in our dataset)\n",
    "query_image = create_sample_image('circle', 'pink', 'gray')\n",
    "\n",
    "print(\"Query image:\")\n",
    "display(query_image)\n",
    "\n",
    "# Find similar images\n",
    "results = search_by_image(query_image, n_results=5)\n",
    "display_search_results(\"Similar to pink circle (image query)\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Complete Multimodal RAG Pipeline\n",
    "\n",
    "Now let's build a complete RAG pipeline that:\n",
    "1. Accepts a natural language question\n",
    "2. Retrieves relevant images\n",
    "3. Uses a VLM to answer questions about the retrieved images\n",
    "\n",
    "### ELI5: Multimodal RAG\n",
    "\n",
    "> **Imagine you're a librarian with a photo library:**\n",
    ">\n",
    "> 1. Someone asks: \"What blue things do you have that are round?\"\n",
    "> 2. You search your catalog and find matching photos\n",
    "> 3. You look at the photos and describe them in detail\n",
    ">\n",
    "> That's multimodal RAG - retrieve images, then analyze them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalRAG:\n",
    "    \"\"\"\n",
    "    Complete Multimodal Retrieval-Augmented Generation system.\n",
    "    \n",
    "    Combines CLIP for retrieval with a VLM for question answering.\n",
    "    \"\"\"\n",
    "    \n",
    "    # === Initialization ===\n",
    "    def __init__(self, clip_model, clip_processor, collection):\n",
    "        \"\"\"\n",
    "        Initialize the Multimodal RAG system.\n",
    "        \n",
    "        Args:\n",
    "            clip_model: Loaded CLIP model\n",
    "            clip_processor: CLIP processor\n",
    "            collection: ChromaDB collection\n",
    "        \"\"\"\n",
    "        self.clip_model = clip_model\n",
    "        self.clip_processor = clip_processor\n",
    "        self.collection = collection\n",
    "        self.vlm_model = None\n",
    "        self.vlm_processor = None\n",
    "        \n",
    "    # === Embedding Methods ===\n",
    "    def get_text_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Get CLIP embedding for text.\"\"\"\n",
    "        inputs = self.clip_processor(text=[text], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(self.clip_model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            features = self.clip_model.get_text_features(**inputs)\n",
    "            features = features / features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        return features.cpu().numpy()[0]\n",
    "    \n",
    "    def retrieve(self, query: str, n_results: int = 3) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant images for a query.\n",
    "        \n",
    "        Args:\n",
    "            query: Natural language query\n",
    "            n_results: Number of images to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            List of retrieved images with metadata\n",
    "        \"\"\"\n",
    "        query_embedding = self.get_text_embedding(query)\n",
    "        \n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding.tolist()],\n",
    "            n_results=n_results,\n",
    "            include=['metadatas', 'distances']\n",
    "        )\n",
    "        \n",
    "        retrieved = []\n",
    "        for idx in range(len(results['ids'][0])):\n",
    "            metadata = results['metadatas'][0][idx]\n",
    "            distance = results['distances'][0][idx]\n",
    "            \n",
    "            retrieved.append({\n",
    "                'id': results['ids'][0][idx],\n",
    "                'image': base64_to_image(metadata['image_b64']),\n",
    "                'metadata': {\n",
    "                    'shape': metadata['shape'],\n",
    "                    'color': metadata['color'],\n",
    "                    'description': metadata['description']\n",
    "                },\n",
    "                'similarity': 1 - distance\n",
    "            })\n",
    "        \n",
    "        return retrieved\n",
    "    \n",
    "    # === VLM Integration ===\n",
    "    def load_vlm(self):\n",
    "        \"\"\"Load the VLM for question answering.\"\"\"\n",
    "        if self.vlm_model is not None:\n",
    "            return\n",
    "            \n",
    "        from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "        \n",
    "        print(\"Loading Qwen2-VL for Q&A...\")\n",
    "        \n",
    "        self.vlm_processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "        self.vlm_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "            \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        print(\"VLM loaded!\")\n",
    "    \n",
    "    def answer_with_images(self, question: str, images: List[Image.Image]) -> str:\n",
    "        \"\"\"\n",
    "        Answer a question using retrieved images.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            images: Retrieved images for context\n",
    "            \n",
    "        Returns:\n",
    "            Answer from the VLM\n",
    "        \"\"\"\n",
    "        if self.vlm_model is None:\n",
    "            self.load_vlm()\n",
    "        \n",
    "        # Create a combined image (side by side)\n",
    "        total_width = sum(img.width for img in images)\n",
    "        max_height = max(img.height for img in images)\n",
    "        combined = Image.new('RGB', (total_width, max_height), 'white')\n",
    "        \n",
    "        x_offset = 0\n",
    "        for img in images:\n",
    "            combined.paste(img, (x_offset, 0))\n",
    "            x_offset += img.width\n",
    "        \n",
    "        # Prepare prompt\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": combined},\n",
    "                {\"type\": \"text\", \"text\": f\"These are search results from an image database. {question}\"}\n",
    "            ]\n",
    "        }]\n",
    "        \n",
    "        text = self.vlm_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = self.vlm_processor(text=[text], images=[combined], return_tensors=\"pt\", padding=True)\n",
    "        inputs = inputs.to(self.vlm_model.device)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            output_ids = self.vlm_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "        \n",
    "        generated_ids = output_ids[:, inputs.input_ids.shape[1]:]\n",
    "        response = self.vlm_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    # === Main Query Interface ===\n",
    "    def query(self, question: str, n_retrieve: int = 3) -> Dict:\n",
    "        \"\"\"\n",
    "        Complete RAG query: retrieve images and answer question.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            n_retrieve: Number of images to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with retrieved images and answer\n",
    "        \"\"\"\n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Retrieve relevant images\n",
    "        print(f\"Retrieving {n_retrieve} relevant images...\")\n",
    "        retrieved = self.retrieve(question, n_retrieve)\n",
    "        \n",
    "        # Display retrieved images\n",
    "        print(f\"Retrieved images:\")\n",
    "        for r in retrieved:\n",
    "            print(f\"  - {r['metadata']['description']} (sim: {r['similarity']:.3f})\")\n",
    "        \n",
    "        # Get answer from VLM\n",
    "        images = [r['image'] for r in retrieved]\n",
    "        answer = self.answer_with_images(question, images)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'retrieved': retrieved,\n",
    "            'answer': answer\n",
    "        }\n",
    "\n",
    "print(\"MultimodalRAG class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Multimodal RAG system\n",
    "rag = MultimodalRAG(clip_model, clip_processor, collection)\n",
    "\n",
    "# Let's ask a question!\n",
    "result = rag.query(\"What colors are the circles in the database?\")\n",
    "\n",
    "print(f\"\\nAnswer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another query\n",
    "result = rag.query(\"Which shapes have warm colors like red, orange, or yellow?\")\n",
    "\n",
    "print(f\"\\nAnswer: {result['answer']}\")\n",
    "\n",
    "# Display the retrieved images\n",
    "fig, axes = plt.subplots(1, len(result['retrieved']), figsize=(9, 3))\n",
    "for ax, r in zip(axes, result['retrieved']):\n",
    "    ax.imshow(r['image'])\n",
    "    ax.set_title(f\"{r['metadata']['color']} {r['metadata']['shape']}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Build Your Own Image Database\n",
    "\n",
    "Create a more interesting image database and test the search!\n",
    "\n",
    "Ideas:\n",
    "1. Download images from the web and index them\n",
    "2. Create images with text labels\n",
    "3. Mix different types of images (photos, icons, diagrams)\n",
    "\n",
    "<details>\n",
    "<summary>Hint: Loading Images from URLs</summary>\n",
    "\n",
    "```python\n",
    "# Unsplash provides royalty-free images\n",
    "urls = [\n",
    "    \"https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba?w=400\",  # cat\n",
    "    \"https://images.unsplash.com/photo-1587300003388-59208cc962cb?w=400\",  # dog\n",
    "    \"https://images.unsplash.com/photo-1501854140801-50d01698950b?w=400\",  # nature\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    image = load_image_from_url(url)\n",
    "    embedding = get_image_embedding(image)\n",
    "    # Add to collection...\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Build your own image database and test the search!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Normalizing Embeddings\n",
    "\n",
    "```python\n",
    "# Wrong - unnormalized embeddings give wrong similarity\n",
    "embedding = clip_model.get_image_features(**inputs)\n",
    "\n",
    "# Right - always normalize for cosine similarity\n",
    "embedding = clip_model.get_image_features(**inputs)\n",
    "embedding = embedding / embedding.norm(dim=-1, keepdim=True)\n",
    "```\n",
    "\n",
    "### Mistake 2: Mixing Embedding Types\n",
    "\n",
    "```python\n",
    "# Wrong - comparing CLIP embeddings with sentence-transformer embeddings\n",
    "clip_emb = get_clip_embedding(image)\n",
    "text_emb = sentence_transformer.encode(text)  # Different space!\n",
    "similarity = cosine_similarity(clip_emb, text_emb)  # Meaningless!\n",
    "\n",
    "# Right - use CLIP for both\n",
    "clip_img_emb = clip_model.get_image_features(...)\n",
    "clip_text_emb = clip_model.get_text_features(...)\n",
    "similarity = cosine_similarity(clip_img_emb, clip_text_emb)  # Correct!\n",
    "```\n",
    "\n",
    "### Mistake 3: Forgetting Image Preprocessing\n",
    "\n",
    "```python\n",
    "# Wrong - passing raw image without processor\n",
    "features = clip_model.get_image_features(image_tensor)  # May fail or give bad results\n",
    "\n",
    "# Right - always use the processor\n",
    "inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "features = clip_model.get_image_features(**inputs)  # Correct!\n",
    "```\n",
    "\n",
    "### Mistake 4: Storing Images Incorrectly\n",
    "\n",
    "```python\n",
    "# Wrong - storing PIL Image directly (loses data)\n",
    "collection.add(metadatas=[{'image': image}])  # Won't work!\n",
    "\n",
    "# Right - convert to base64 or store path\n",
    "collection.add(metadatas=[{'image_b64': image_to_base64(image)}])  # Works!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- How CLIP creates joint image-text embeddings\n",
    "- How to build a multimodal index with ChromaDB\n",
    "- How to search images using natural language\n",
    "- How to search for similar images\n",
    "- How to build a complete multimodal RAG pipeline\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **CLIP enables cross-modal search**: Same embedding space for images and text\n",
    "2. **Semantic understanding**: CLIP understands concepts like \"warm colors\"\n",
    "3. **RAG extends to images**: Retrieve relevant images, then reason with VLM\n",
    "4. **Embeddings must be normalized**: For cosine similarity to work correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "### Build a Hybrid Search System\n",
    "\n",
    "Create a system that can:\n",
    "1. Accept queries with both text AND an example image\n",
    "2. Weight the text and image embeddings (e.g., 70% text, 30% image)\n",
    "3. Find results that match both criteria\n",
    "\n",
    "Example: \"Find red things that look similar to this circle\" + [image of circle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE\n",
    "\n",
    "def hybrid_search(\n",
    "    text_query: str,\n",
    "    image_query: Image.Image,\n",
    "    text_weight: float = 0.7,\n",
    "    n_results: int = 5\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search using both text and image queries.\n",
    "    \n",
    "    Args:\n",
    "        text_query: Natural language query\n",
    "        image_query: Image to match\n",
    "        text_weight: Weight for text (image_weight = 1 - text_weight)\n",
    "        n_results: Number of results\n",
    "        \n",
    "    Returns:\n",
    "        List of matching images\n",
    "    \"\"\"\n",
    "    # TODO: Implement hybrid search\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [CLIP Paper](https://arxiv.org/abs/2103.00020)\n",
    "- [ChromaDB Documentation](https://docs.trychroma.com/)\n",
    "- [OpenAI CLIP Blog Post](https://openai.com/blog/clip/)\n",
    "- [Multimodal RAG Patterns](https://www.llamaindex.ai/blog/multimodal-rag)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "if 'rag' in dir() and rag.vlm_model is not None:\n",
    "    del rag.vlm_model\n",
    "    del rag.vlm_processor\n",
    "\n",
    "del clip_model\n",
    "del clip_processor\n",
    "\n",
    "clear_gpu_memory()\n",
    "print(f\"Final memory state: {get_memory_usage()}\")\n",
    "print(\"\\nNotebook complete! Ready for the next task.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}