{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 12.5: Production API with FastAPI\n",
    "\n",
    "**Module:** 12 - Model Deployment & Inference Engines  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** \u2b50\u2b50\u2b50\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Build an OpenAI-compatible API with FastAPI\n",
    "- [ ] Implement streaming responses (SSE)\n",
    "- [ ] Add production features: rate limiting, monitoring, error handling\n",
    "- [ ] Deploy and test under load\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcda Prerequisites\n",
    "\n",
    "- Completed: Tasks 12.1-12.4\n",
    "- Knowledge of: REST APIs, async Python\n",
    "- Running: At least one inference engine (Ollama recommended)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udf0d Real-World Context\n",
    "\n",
    "**Why build a custom API layer?**\n",
    "\n",
    "While inference engines like Ollama and vLLM provide APIs, production deployments often need:\n",
    "- **Rate limiting**: Prevent abuse and ensure fair usage\n",
    "- **Authentication**: Secure access with API keys\n",
    "- **Monitoring**: Track latency, errors, and usage\n",
    "- **Load balancing**: Distribute requests across multiple engines\n",
    "- **Request transformation**: Adapt to specific frontend needs\n",
    "\n",
    "A FastAPI wrapper gives you full control over these features while maintaining OpenAI API compatibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83e\uddd2 ELI5: What is an API?\n",
    "\n",
    "> **Imagine a restaurant...**\n",
    ">\n",
    "> You (the client) don't walk into the kitchen to make your food.\n",
    "> Instead, you talk to the waiter (the API) who:\n",
    "> 1. Takes your order (receives your request)\n",
    "> 2. Brings it to the kitchen (forwards to the model)\n",
    "> 3. Checks if it's ready (handles streaming)\n",
    "> 4. Brings you your food (returns the response)\n",
    ">\n",
    "> **Streaming is like a sushi conveyor belt:**\n",
    "> Instead of waiting for all dishes at once, pieces arrive as they're ready.\n",
    "> You can start eating immediately!\n",
    ">\n",
    "> **In AI terms:**\n",
    "> - Client sends a message via HTTP request\n",
    "> - API validates, rate-limits, logs the request\n",
    "> - Forwards to inference engine\n",
    "> - Streams tokens back as they're generated\n",
    "> - Logs completion and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Basic FastAPI Setup\n",
    "\n",
    "Let's start with a simple API that wraps an inference engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# !pip install fastapi uvicorn aiohttp pydantic\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from typing import List, Optional, AsyncIterator\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# Check imports\n",
    "try:\n",
    "    from fastapi import FastAPI, HTTPException, Request\n",
    "    from fastapi.responses import StreamingResponse, JSONResponse\n",
    "    from pydantic import BaseModel, Field\n",
    "    import aiohttp\n",
    "    print(\"\u2705 All dependencies installed!\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u274c Missing dependency: {e}\")\n",
    "    print(\"   Install with: pip install fastapi uvicorn aiohttp pydantic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define OpenAI-compatible request/response models\n",
    "\n",
    "class ChatMessage(BaseModel):\n",
    "    \"\"\"A single chat message.\"\"\"\n",
    "    role: str = Field(..., description=\"Role: system, user, or assistant\")\n",
    "    content: str = Field(..., description=\"Message content\")\n",
    "\n",
    "class ChatCompletionRequest(BaseModel):\n",
    "    \"\"\"OpenAI-compatible chat completion request.\"\"\"\n",
    "    model: str = Field(default=\"default\")\n",
    "    messages: List[ChatMessage]\n",
    "    max_tokens: Optional[int] = Field(default=512)\n",
    "    temperature: Optional[float] = Field(default=0.7, ge=0.0, le=2.0)\n",
    "    top_p: Optional[float] = Field(default=0.9, ge=0.0, le=1.0)\n",
    "    stream: Optional[bool] = Field(default=False)\n",
    "    stop: Optional[List[str]] = None\n",
    "    user: Optional[str] = None\n",
    "\n",
    "class ChatCompletionChoice(BaseModel):\n",
    "    \"\"\"A single completion choice.\"\"\"\n",
    "    index: int = 0\n",
    "    message: ChatMessage\n",
    "    finish_reason: str = \"stop\"\n",
    "\n",
    "class UsageInfo(BaseModel):\n",
    "    \"\"\"Token usage information.\"\"\"\n",
    "    prompt_tokens: int\n",
    "    completion_tokens: int\n",
    "    total_tokens: int\n",
    "\n",
    "class ChatCompletionResponse(BaseModel):\n",
    "    \"\"\"OpenAI-compatible chat completion response.\"\"\"\n",
    "    id: str = Field(default_factory=lambda: f\"chatcmpl-{uuid.uuid4().hex[:8]}\")\n",
    "    object: str = \"chat.completion\"\n",
    "    created: int = Field(default_factory=lambda: int(time.time()))\n",
    "    model: str\n",
    "    choices: List[ChatCompletionChoice]\n",
    "    usage: UsageInfo\n",
    "\n",
    "print(\"\u2705 Pydantic models defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple inference client that wraps Ollama\n",
    "import requests\n",
    "\n",
    "class OllamaBackend:\n",
    "    \"\"\"\n",
    "    Backend client for Ollama inference.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str = \"http://localhost:11434\", model: str = \"llama3.1:8b\"):\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "    \n",
    "    def is_healthy(self) -> bool:\n",
    "        \"\"\"Check if Ollama is running.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.base_url}/api/tags\", timeout=5)\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def chat(self, messages: List[dict], max_tokens: int = 512, temperature: float = 0.7) -> str:\n",
    "        \"\"\"Generate a non-streaming response.\"\"\"\n",
    "        response = requests.post(\n",
    "            f\"{self.base_url}/api/chat\",\n",
    "            json={\n",
    "                \"model\": self.model,\n",
    "                \"messages\": messages,\n",
    "                \"options\": {\n",
    "                    \"num_predict\": max_tokens,\n",
    "                    \"temperature\": temperature\n",
    "                },\n",
    "                \"stream\": False\n",
    "            },\n",
    "            timeout=120\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json().get(\"message\", {}).get(\"content\", \"\")\n",
    "    \n",
    "    async def stream_chat(\n",
    "        self, \n",
    "        messages: List[dict], \n",
    "        max_tokens: int = 512, \n",
    "        temperature: float = 0.7\n",
    "    ) -> AsyncIterator[str]:\n",
    "        \"\"\"Generate a streaming response.\"\"\"\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.post(\n",
    "                f\"{self.base_url}/api/chat\",\n",
    "                json={\n",
    "                    \"model\": self.model,\n",
    "                    \"messages\": messages,\n",
    "                    \"options\": {\n",
    "                        \"num_predict\": max_tokens,\n",
    "                        \"temperature\": temperature\n",
    "                    },\n",
    "                    \"stream\": True\n",
    "                },\n",
    "                timeout=aiohttp.ClientTimeout(total=120)\n",
    "            ) as response:\n",
    "                async for line in response.content:\n",
    "                    if line:\n",
    "                        try:\n",
    "                            chunk = json.loads(line)\n",
    "                            content = chunk.get(\"message\", {}).get(\"content\", \"\")\n",
    "                            if content:\n",
    "                                yield content\n",
    "                        except json.JSONDecodeError:\n",
    "                            pass\n",
    "\n",
    "# Test connection\n",
    "backend = OllamaBackend()\n",
    "if backend.is_healthy():\n",
    "    print(\"\u2705 Connected to Ollama!\")\n",
    "else:\n",
    "    print(\"\u274c Ollama not running. Start with: ollama serve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Building the FastAPI Server\n",
    "\n",
    "Now let's create the full API server with streaming support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Complete FastAPI server code\n# We'll write this as an EXAMPLE file (the module includes a more complete api_server.py)\n\napi_server_code = '''\n\"\"\"\nProduction LLM API Server with FastAPI (Learning Example)\n\nThis is a simplified learning example. For production use, see:\n    ../api/api_server.py (full-featured version with inference_client integration)\n\nFeatures:\n- OpenAI-compatible API\n- Streaming responses (SSE)\n- Rate limiting\n- Request logging\n- Health checks\n- Error handling\n\nUsage:\n    uvicorn simple_api_example:app --host 0.0.0.0 --port 8080 --reload\n\"\"\"\n\nimport asyncio\nimport json\nimport logging\nimport time\nimport uuid\nfrom collections import deque\nfrom datetime import datetime\nfrom typing import List, Optional, AsyncIterator, Dict, Any\n\nimport aiohttp\nfrom fastapi import FastAPI, HTTPException, Request, status\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import StreamingResponse, JSONResponse\nfrom pydantic import BaseModel, Field\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(\"api\")\n\n# ============================================================\n# Configuration\n# ============================================================\n\nOLLAMA_URL = \"http://localhost:11434\"\nDEFAULT_MODEL = \"llama3.1:8b\"\nRATE_LIMIT_RPM = 60  # Requests per minute\n\n# ============================================================\n# Pydantic Models\n# ============================================================\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\nclass ChatCompletionRequest(BaseModel):\n    model: str = DEFAULT_MODEL\n    messages: List[ChatMessage]\n    max_tokens: Optional[int] = 512\n    temperature: Optional[float] = 0.7\n    stream: Optional[bool] = False\n    user: Optional[str] = None\n\nclass ChatCompletionResponse(BaseModel):\n    id: str\n    object: str = \"chat.completion\"\n    created: int\n    model: str\n    choices: List[dict]\n    usage: dict\n\n# ============================================================\n# Rate Limiter\n# ============================================================\n\nclass RateLimiter:\n    \"\"\"Token bucket rate limiter per IP.\"\"\"\n    \n    def __init__(self, requests_per_minute: int = 60):\n        self.rpm = requests_per_minute\n        self.requests: Dict[str, deque] = {}\n    \n    def is_allowed(self, client_ip: str) -> bool:\n        now = time.time()\n        if client_ip not in self.requests:\n            self.requests[client_ip] = deque()\n        \n        # Remove old requests\n        while self.requests[client_ip] and now - self.requests[client_ip][0] > 60:\n            self.requests[client_ip].popleft()\n        \n        if len(self.requests[client_ip]) >= self.rpm:\n            return False\n        \n        self.requests[client_ip].append(now)\n        return True\n\n# ============================================================\n# Metrics Tracker\n# ============================================================\n\nclass Metrics:\n    \"\"\"Simple metrics tracker.\"\"\"\n    \n    def __init__(self):\n        self.start_time = time.time()\n        self.total_requests = 0\n        self.successful_requests = 0\n        self.failed_requests = 0\n        self.latencies: deque = deque(maxlen=1000)\n    \n    def record_request(self, latency_ms: float, success: bool):\n        self.total_requests += 1\n        self.latencies.append(latency_ms)\n        if success:\n            self.successful_requests += 1\n        else:\n            self.failed_requests += 1\n    \n    def get_stats(self) -> dict:\n        latencies = list(self.latencies)\n        return {\n            \"uptime_seconds\": time.time() - self.start_time,\n            \"total_requests\": self.total_requests,\n            \"successful_requests\": self.successful_requests,\n            \"failed_requests\": self.failed_requests,\n            \"avg_latency_ms\": sum(latencies) / len(latencies) if latencies else 0,\n            \"p90_latency_ms\": sorted(latencies)[int(len(latencies) * 0.9)] if latencies else 0\n        }\n\n# ============================================================\n# FastAPI App\n# ============================================================\n\napp = FastAPI(\n    title=\"LLM Inference API\",\n    description=\"OpenAI-compatible API for local LLM inference\",\n    version=\"1.0.0\"\n)\n\n# CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"]\n)\n\n# Global state\nrate_limiter = RateLimiter(RATE_LIMIT_RPM)\nmetrics = Metrics()\n\n# ============================================================\n# Endpoints\n# ============================================================\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint.\"\"\"\n    try:\n        async with aiohttp.ClientSession() as session:\n            async with session.get(f\"{OLLAMA_URL}/api/tags\", timeout=5) as resp:\n                backend_healthy = resp.status == 200\n    except:\n        backend_healthy = False\n    \n    return {\n        \"status\": \"healthy\" if backend_healthy else \"degraded\",\n        \"backend\": \"ollama\",\n        \"backend_url\": OLLAMA_URL,\n        \"backend_healthy\": backend_healthy,\n        **metrics.get_stats()\n    }\n\n@app.get(\"/v1/models\")\nasync def list_models():\n    \"\"\"List available models.\"\"\"\n    return {\n        \"object\": \"list\",\n        \"data\": [{\"id\": DEFAULT_MODEL, \"object\": \"model\", \"owned_by\": \"local\"}]\n    }\n\n@app.post(\"/v1/chat/completions\")\nasync def chat_completions(request: ChatCompletionRequest, req: Request):\n    \"\"\"OpenAI-compatible chat completion endpoint.\"\"\"\n    start_time = time.time()\n    client_ip = req.client.host if req.client else \"unknown\"\n    \n    # Rate limiting\n    if not rate_limiter.is_allowed(client_ip):\n        raise HTTPException(\n            status_code=status.HTTP_429_TOO_MANY_REQUESTS,\n            detail=\"Rate limit exceeded\"\n        )\n    \n    logger.info(f\"Request from {client_ip}: {len(request.messages)} messages, stream={request.stream}\")\n    \n    # Convert messages\n    messages = [{\"role\": m.role, \"content\": m.content} for m in request.messages]\n    \n    if request.stream:\n        return await stream_response(messages, request, start_time)\n    else:\n        return await non_stream_response(messages, request, start_time)\n\nasync def non_stream_response(messages: List[dict], request: ChatCompletionRequest, start_time: float):\n    \"\"\"Handle non-streaming response.\"\"\"\n    try:\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                f\"{OLLAMA_URL}/api/chat\",\n                json={\n                    \"model\": request.model,\n                    \"messages\": messages,\n                    \"options\": {\n                        \"num_predict\": request.max_tokens,\n                        \"temperature\": request.temperature\n                    },\n                    \"stream\": False\n                },\n                timeout=aiohttp.ClientTimeout(total=120)\n            ) as resp:\n                data = await resp.json()\n        \n        content = data.get(\"message\", {}).get(\"content\", \"\")\n        latency_ms = (time.time() - start_time) * 1000\n        metrics.record_request(latency_ms, True)\n        \n        return ChatCompletionResponse(\n            id=f\"chatcmpl-{uuid.uuid4().hex[:8]}\",\n            created=int(time.time()),\n            model=request.model,\n            choices=[{\n                \"index\": 0,\n                \"message\": {\"role\": \"assistant\", \"content\": content},\n                \"finish_reason\": \"stop\"\n            }],\n            usage={\n                \"prompt_tokens\": data.get(\"prompt_eval_count\", 0),\n                \"completion_tokens\": data.get(\"eval_count\", 0),\n                \"total_tokens\": data.get(\"prompt_eval_count\", 0) + data.get(\"eval_count\", 0)\n            }\n        )\n    except Exception as e:\n        latency_ms = (time.time() - start_time) * 1000\n        metrics.record_request(latency_ms, False)\n        logger.error(f\"Error: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\nasync def stream_response(messages: List[dict], request: ChatCompletionRequest, start_time: float):\n    \"\"\"Handle streaming response.\"\"\"\n    completion_id = f\"chatcmpl-{uuid.uuid4().hex[:8]}\"\n    \n    async def generate():\n        try:\n            # Initial chunk with role\n            initial = {\n                \"id\": completion_id,\n                \"object\": \"chat.completion.chunk\",\n                \"created\": int(time.time()),\n                \"model\": request.model,\n                \"choices\": [{\"index\": 0, \"delta\": {\"role\": \"assistant\"}, \"finish_reason\": None}]\n            }\n            yield f\"data: {json.dumps(initial)}\\\\n\\\\n\"\n            \n            async with aiohttp.ClientSession() as session:\n                async with session.post(\n                    f\"{OLLAMA_URL}/api/chat\",\n                    json={\n                        \"model\": request.model,\n                        \"messages\": messages,\n                        \"options\": {\n                            \"num_predict\": request.max_tokens,\n                            \"temperature\": request.temperature\n                        },\n                        \"stream\": True\n                    },\n                    timeout=aiohttp.ClientTimeout(total=120)\n                ) as resp:\n                    async for line in resp.content:\n                        if line:\n                            try:\n                                chunk = json.loads(line)\n                                content = chunk.get(\"message\", {}).get(\"content\", \"\")\n                                if content:\n                                    data = {\n                                        \"id\": completion_id,\n                                        \"object\": \"chat.completion.chunk\",\n                                        \"created\": int(time.time()),\n                                        \"model\": request.model,\n                                        \"choices\": [{\"index\": 0, \"delta\": {\"content\": content}, \"finish_reason\": None}]\n                                    }\n                                    yield f\"data: {json.dumps(data)}\\\\n\\\\n\"\n                            except json.JSONDecodeError:\n                                pass\n            \n            # Final chunk\n            final = {\n                \"id\": completion_id,\n                \"object\": \"chat.completion.chunk\",\n                \"created\": int(time.time()),\n                \"model\": request.model,\n                \"choices\": [{\"index\": 0, \"delta\": {}, \"finish_reason\": \"stop\"}]\n            }\n            yield f\"data: {json.dumps(final)}\\\\n\\\\n\"\n            yield \"data: [DONE]\\\\n\\\\n\"\n            \n            latency_ms = (time.time() - start_time) * 1000\n            metrics.record_request(latency_ms, True)\n            \n        except Exception as e:\n            latency_ms = (time.time() - start_time) * 1000\n            metrics.record_request(latency_ms, False)\n            logger.error(f\"Streaming error: {e}\")\n            yield f\"data: {{\\\\\"error\\\\\": \\\\\"{str(e)}\\\\\"}}\\\\n\\\\n\"\n    \n    return StreamingResponse(\n        generate(),\n        media_type=\"text/event-stream\",\n        headers={\"Cache-Control\": \"no-cache\", \"Connection\": \"keep-alive\"}\n    )\n\n@app.get(\"/metrics\")\nasync def get_metrics():\n    \"\"\"Get server metrics.\"\"\"\n    return metrics.get_stats()\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8080)\n'''\n\n# Write to file - use a unique name to avoid overwriting the full-featured api_server.py\nfrom pathlib import Path\nimport os\n\n# Get the notebook's directory reliably\n# When running in Jupyter, the cwd is typically the notebook's directory\nnotebook_dir = Path.cwd()\n\n# Navigate to expected api directory location\napi_dir = (notebook_dir / \"../api\").resolve()\n\n# Verify we're in the right directory structure\nscripts_dir = (notebook_dir / \"../scripts\").resolve()\nif not scripts_dir.exists():\n    print(\"\u26a0\ufe0f  Warning: Cannot verify correct directory structure\")\n    print(f\"   Expected 'scripts/' folder at: {scripts_dir}\")\n    print(f\"   Current working directory: {notebook_dir}\")\n    print(\"\\n   If you're running from a different directory, the file may be\")\n    print(\"   written to an unexpected location.\")\n    user_confirm = input(\"   Continue anyway? (y/n): \").strip().lower()\n    if user_confirm != 'y':\n        print(\"   Aborted. Please run this notebook from the notebooks/ directory.\")\n        raise SystemExit(0)\n\n# Create the api directory if it doesn't exist\napi_dir.mkdir(exist_ok=True)\n\n# Write as an example/learning file\noutput_file = api_dir / \"simple_api_example.py\"\nwith open(output_file, \"w\") as f:\n    f.write(api_server_code)\n\nprint(f\"\u2705 Example API server code written to {output_file}\")\nprint(\"\\n\ud83d\udca1 Note: The module also includes a full-featured server at ../api/api_server.py\")\nprint(\"   that integrates with the inference_client for multi-engine support.\")\nprint(\"\\n\ud83d\ude80 To start the example server, run:\")\nprint(\"   cd ../api && uvicorn simple_api_example:app --host 0.0.0.0 --port 8080 --reload\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Testing the API\n",
    "\n",
    "Let's create test functions to verify our API works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "API_URL = \"http://localhost:8080\"\n",
    "\n",
    "def test_health():\n",
    "    \"\"\"Test health endpoint.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{API_URL}/health\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(f\"\u2705 Health check passed: {data['status']}\")\n",
    "            print(f\"   Backend: {data.get('backend', 'unknown')}\")\n",
    "            print(f\"   Uptime: {data.get('uptime_seconds', 0):.1f}s\")\n",
    "            return True\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"\u274c Cannot connect to API server\")\n",
    "        print(\"   Start it with: uvicorn production_api:app --port 8080\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Health check failed: {e}\")\n",
    "    return False\n",
    "\n",
    "def test_non_streaming():\n",
    "    \"\"\"Test non-streaming completion.\"\"\"\n",
    "    try:\n",
    "        start = time.time()\n",
    "        response = requests.post(\n",
    "            f\"{API_URL}/v1/chat/completions\",\n",
    "            json={\n",
    "                \"model\": \"llama3.1:8b\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": \"Say hello!\"}],\n",
    "                \"max_tokens\": 50,\n",
    "                \"stream\": False\n",
    "            },\n",
    "            timeout=60\n",
    "        )\n",
    "        latency = (time.time() - start) * 1000\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            content = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "            print(f\"\u2705 Non-streaming works! ({latency:.0f}ms)\")\n",
    "            print(f\"   Response: {content[:100]}...\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"\u274c Error: {response.status_code} - {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Non-streaming test failed: {e}\")\n",
    "    return False\n",
    "\n",
    "def test_streaming():\n",
    "    \"\"\"Test streaming completion.\"\"\"\n",
    "    try:\n",
    "        start = time.time()\n",
    "        first_token = None\n",
    "        chunks = []\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{API_URL}/v1/chat/completions\",\n",
    "            json={\n",
    "                \"model\": \"llama3.1:8b\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": \"Count from 1 to 5.\"}],\n",
    "                \"max_tokens\": 50,\n",
    "                \"stream\": True\n",
    "            },\n",
    "            stream=True,\n",
    "            timeout=60\n",
    "        )\n",
    "        \n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                line_str = line.decode()\n",
    "                if line_str.startswith(\"data: \"):\n",
    "                    data_str = line_str[6:]\n",
    "                    if data_str == \"[DONE]\":\n",
    "                        break\n",
    "                    try:\n",
    "                        chunk = json.loads(data_str)\n",
    "                        content = chunk.get(\"choices\", [{}])[0].get(\"delta\", {}).get(\"content\", \"\")\n",
    "                        if content:\n",
    "                            if first_token is None:\n",
    "                                first_token = time.time()\n",
    "                            chunks.append(content)\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        total_time = (time.time() - start) * 1000\n",
    "        ttft = ((first_token - start) * 1000) if first_token else 0\n",
    "        \n",
    "        print(f\"\u2705 Streaming works! (TTFT: {ttft:.0f}ms, Total: {total_time:.0f}ms)\")\n",
    "        print(f\"   Response: {''.join(chunks)}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Streaming test failed: {e}\")\n",
    "    return False\n",
    "\n",
    "# Run tests\n",
    "print(\"\ud83e\uddea Running API Tests\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if test_health():\n",
    "    print(\"\")\n",
    "    test_non_streaming()\n",
    "    print(\"\")\n",
    "    test_streaming()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Load Testing\n",
    "\n",
    "Let's test how the API handles concurrent requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "\n",
    "async def send_request(session: aiohttp.ClientSession, request_id: int) -> Tuple[int, float, bool]:\n",
    "    \"\"\"Send a single request and return (id, latency, success).\"\"\"\n",
    "    start = time.time()\n",
    "    try:\n",
    "        async with session.post(\n",
    "            f\"{API_URL}/v1/chat/completions\",\n",
    "            json={\n",
    "                \"model\": \"llama3.1:8b\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": f\"Request {request_id}: Say 'hello'\"}],\n",
    "                \"max_tokens\": 20,\n",
    "                \"stream\": False\n",
    "            },\n",
    "            timeout=aiohttp.ClientTimeout(total=60)\n",
    "        ) as response:\n",
    "            await response.json()\n",
    "            latency = (time.time() - start) * 1000\n",
    "            return (request_id, latency, response.status == 200)\n",
    "    except Exception as e:\n",
    "        latency = (time.time() - start) * 1000\n",
    "        return (request_id, latency, False)\n",
    "\n",
    "async def load_test(num_requests: int, concurrency: int) -> dict:\n",
    "    \"\"\"Run load test with specified concurrency.\"\"\"\n",
    "    semaphore = asyncio.Semaphore(concurrency)\n",
    "    \n",
    "    async def limited_request(session, req_id):\n",
    "        async with semaphore:\n",
    "            return await send_request(session, req_id)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [limited_request(session, i) for i in range(num_requests)]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Analyze results\n",
    "    latencies = [r[1] for r in results if r[2]]\n",
    "    successful = sum(1 for r in results if r[2])\n",
    "    \n",
    "    return {\n",
    "        \"total_requests\": num_requests,\n",
    "        \"concurrency\": concurrency,\n",
    "        \"successful\": successful,\n",
    "        \"failed\": num_requests - successful,\n",
    "        \"total_time_s\": total_time,\n",
    "        \"throughput_rps\": successful / total_time if total_time > 0 else 0,\n",
    "        \"avg_latency_ms\": sum(latencies) / len(latencies) if latencies else 0,\n",
    "        \"p90_latency_ms\": sorted(latencies)[int(len(latencies) * 0.9)] if latencies else 0,\n",
    "    }\n",
    "\n",
    "# Check if API is running\n",
    "try:\n",
    "    requests.get(f\"{API_URL}/health\", timeout=2)\n",
    "    api_running = True\n",
    "except:\n",
    "    api_running = False\n",
    "\n",
    "if api_running:\n",
    "    print(\"\ud83d\udd25 Running Load Test\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test at different concurrency levels\n",
    "    for concurrency in [1, 2, 4]:\n",
    "        print(f\"\\nConcurrency: {concurrency}\")\n",
    "        result = asyncio.run(load_test(num_requests=10, concurrency=concurrency))\n",
    "        print(f\"   Throughput: {result['throughput_rps']:.2f} req/s\")\n",
    "        print(f\"   Avg Latency: {result['avg_latency_ms']:.0f}ms\")\n",
    "        print(f\"   P90 Latency: {result['p90_latency_ms']:.0f}ms\")\n",
    "        print(f\"   Success Rate: {result['successful']}/{result['total_requests']}\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f API server not running. Start it first to run load tests.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \u26a0\ufe0f Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Handling Streaming Correctly\n",
    "\n",
    "```python\n",
    "# \u274c Wrong - Response buffered, defeats purpose of streaming\n",
    "return JSONResponse(content=full_response)  # After collecting all chunks\n",
    "\n",
    "# \u2705 Right - True streaming\n",
    "return StreamingResponse(\n",
    "    generate(),\n",
    "    media_type=\"text/event-stream\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Mistake 2: Blocking the Event Loop\n",
    "\n",
    "```python\n",
    "# \u274c Wrong - Blocks async event loop\n",
    "response = requests.post(url, json=data)  # Synchronous!\n",
    "\n",
    "# \u2705 Right - Non-blocking async\n",
    "async with aiohttp.ClientSession() as session:\n",
    "    async with session.post(url, json=data) as response:\n",
    "        data = await response.json()\n",
    "```\n",
    "\n",
    "### Mistake 3: Missing Error Handling in Streaming\n",
    "\n",
    "```python\n",
    "# \u274c Wrong - Errors crash silently\n",
    "async def generate():\n",
    "    async for chunk in backend.stream():\n",
    "        yield f\"data: {chunk}\\n\\n\"\n",
    "\n",
    "# \u2705 Right - Handle errors gracefully\n",
    "async def generate():\n",
    "    try:\n",
    "        async for chunk in backend.stream():\n",
    "            yield f\"data: {chunk}\\n\\n\"\n",
    "    except Exception as e:\n",
    "        yield f\"data: {{\\\"error\\\": \\\"{str(e)}\\\"}}\\n\\n\"\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u270b Try It Yourself\n",
    "\n",
    "### Exercise 1: Add API Key Authentication\n",
    "\n",
    "Add simple API key authentication to protect your endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Implement API key auth\n",
    "# TODO: Add a middleware or dependency that:\n",
    "#   1. Checks for Authorization header: \"Bearer sk-xxx\"\n",
    "#   2. Validates against a list of valid keys\n",
    "#   3. Returns 401 if invalid\n",
    "\n",
    "# Hint: Use FastAPI's Depends and HTTPBearer\n",
    "# from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Add Request Logging to File\n",
    "\n",
    "Log all requests to a file for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Add file logging\n",
    "# TODO: Create a middleware that logs:\n",
    "#   - Timestamp\n",
    "#   - Client IP\n",
    "#   - Request path\n",
    "#   - Latency\n",
    "#   - Status code\n",
    "# to a JSON Lines file (one JSON object per line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83c\udf89 Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- \u2705 How to build an OpenAI-compatible API with FastAPI\n",
    "- \u2705 Implementing streaming responses with SSE\n",
    "- \u2705 Adding rate limiting and metrics\n",
    "- \u2705 Load testing your API\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\ude80 Challenge (Optional)\n",
    "\n",
    "**Build a Multi-Backend Load Balancer**\n",
    "\n",
    "Create an API that:\n",
    "1. Routes requests to multiple inference backends\n",
    "2. Implements health checking and failover\n",
    "3. Load balances based on current queue depth\n",
    "4. Provides a unified API across different engines\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcd6 Further Reading\n",
    "\n",
    "- [FastAPI Documentation](https://fastapi.tiangolo.com/)\n",
    "- [Server-Sent Events (SSE) Spec](https://html.spec.whatwg.org/multipage/server-sent-events.html)\n",
    "- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)\n",
    "- [Uvicorn Production Deployment](https://www.uvicorn.org/deployment/)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83e\uddf9 Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"\u2705 Cleanup complete!\")\n",
    "print(\"\\n\ud83d\udca1 To stop the API server: Ctrl+C in the terminal\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}