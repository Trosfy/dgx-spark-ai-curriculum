{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.2.2: Dataset Preprocessing Pipeline\n",
    "\n",
    "**Module:** 1.2 - Python for AI/ML  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Load and explore datasets using Pandas\n",
    "- [ ] Handle missing values with multiple strategies\n",
    "- [ ] Encode categorical variables properly\n",
    "- [ ] Implement feature scaling transformations\n",
    "- [ ] Build a reusable `Preprocessor` class\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Lab 1.2.1 (NumPy Broadcasting)\n",
    "- Knowledge of: Basic Python classes, NumPy basics\n",
    "\n",
    "### Required Packages\n",
    "- Python 3.9+\n",
    "- NumPy >= 1.21\n",
    "- Pandas >= 1.3\n",
    "- scikit-learn >= 1.0 (for train_test_split)\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**\"Garbage in, garbage out\"** - the oldest saying in data science.\n",
    "\n",
    "In the real world:\n",
    "- 80% of a data scientist's time is spent on data preparation\n",
    "- Raw data has missing values, inconsistent formats, outliers\n",
    "- A model is only as good as the data it's trained on\n",
    "\n",
    "**Examples:**\n",
    "- Medical records with missing patient data\n",
    "- E-commerce data with mixed currencies and formats\n",
    "- Sensor data with failed readings\n",
    "\n",
    "A solid preprocessing pipeline is the foundation of any ML project!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßí ELI5: Data Preprocessing\n",
    "\n",
    "> **Imagine you're making a recipe, but the ingredients are a mess...** üç≥\n",
    ">\n",
    "> - Some tomatoes are still in the fridge, some are rotten (missing data)\n",
    "> - The recipe uses cups but you only have a scale (different formats)\n",
    "> - Some ingredients are in grams, others in pounds (different scales)\n",
    ">\n",
    "> Before you can cook, you need to:\n",
    "> 1. Find and replace the bad tomatoes\n",
    "> 2. Convert everything to the same units\n",
    "> 3. Measure out equal portions\n",
    ">\n",
    "> **In AI terms:** ML models need clean, consistent, properly-scaled data.\n",
    "> Preprocessing transforms messy real-world data into something models can digest.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Environment Setup and Dependency Checks\n",
    "# ============================================================\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Determine the notebook's directory for reliable path resolution\n",
    "# Works in Jupyter, VS Code, and command-line execution\n",
    "try:\n",
    "    # VS Code Jupyter\n",
    "    notebook_dir = Path(__vsc_ipynb_file__).parent\n",
    "except NameError:\n",
    "    try:\n",
    "        # Standard Jupyter with ipykernel\n",
    "        import IPython\n",
    "        notebook_dir = Path(IPython.get_ipython().kernel.session.config.get('IPKernelApp', {}).get('connection_file', '')).parent\n",
    "        if not (notebook_dir / '../scripts').exists():\n",
    "            notebook_dir = Path.cwd()\n",
    "    except:\n",
    "        notebook_dir = Path.cwd()\n",
    "\n",
    "# Add scripts directory to path (robust method)\n",
    "scripts_dir = (notebook_dir / '../scripts').resolve()\n",
    "if scripts_dir.exists() and str(scripts_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(scripts_dir))\n",
    "elif not scripts_dir.exists():\n",
    "    # Fallback: try relative to cwd\n",
    "    scripts_dir = Path('../scripts').resolve()\n",
    "    if scripts_dir.exists() and str(scripts_dir) not in sys.path:\n",
    "        sys.path.insert(0, str(scripts_dir))\n",
    "\n",
    "# Check required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "\n",
    "# Check sklearn availability (needed later)\n",
    "try:\n",
    "    import sklearn\n",
    "    print(f\"scikit-learn version: {sklearn.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è scikit-learn not installed!\")\n",
    "    print(\"   Install with: pip install scikit-learn\")\n",
    "    print(\"   Some cells will fail without it.\")\n",
    "\n",
    "# Check if data files exist\n",
    "data_dir = (notebook_dir / '../data').resolve()\n",
    "if not data_dir.exists():\n",
    "    data_dir = Path('../data').resolve()\n",
    "\n",
    "required_files = ['sample_customers.csv', 'sample_training_history.json', \n",
    "                  'sample_embeddings.npy', 'sample_confusion_data.json']\n",
    "missing_files = [f for f in required_files if not (data_dir / f).exists()]\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\n‚ö†Ô∏è Data files not found: {missing_files}\")\n",
    "    generator_script = data_dir / 'generate_sample_data.py'\n",
    "    if generator_script.exists():\n",
    "        print(\"   Generating sample data...\")\n",
    "        result = subprocess.run([sys.executable, str(generator_script)], \n",
    "                               capture_output=True, text=True, cwd=str(data_dir))\n",
    "        if result.returncode == 0:\n",
    "            print(\"   ‚úÖ Sample data generated successfully!\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Error generating data: {result.stderr}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Generator script not found at: {generator_script}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All data files present in {data_dir}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Welcome to the Preprocessing Pipeline Lab! üîß\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Loading and Exploring Data\n",
    "\n",
    "Let's create a realistic dataset with common data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic dataset with realistic issues\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate synthetic customer data\n",
    "data = {\n",
    "    'age': np.random.randint(18, 80, n_samples).astype(float),\n",
    "    'income': np.random.lognormal(10.5, 0.5, n_samples),\n",
    "    'credit_score': np.random.randint(300, 850, n_samples).astype(float),\n",
    "    'years_employed': np.random.exponential(5, n_samples),\n",
    "    'education': np.random.choice(\n",
    "        ['High School', 'Bachelor', 'Master', 'PhD', None], \n",
    "        n_samples, \n",
    "        p=[0.3, 0.35, 0.2, 0.1, 0.05]\n",
    "    ),\n",
    "    'employment_type': np.random.choice(\n",
    "        ['Full-time', 'Part-time', 'Self-employed', 'Unemployed'],\n",
    "        n_samples,\n",
    "        p=[0.6, 0.15, 0.15, 0.1]\n",
    "    ),\n",
    "    'default': np.random.choice([0, 1], n_samples, p=[0.85, 0.15])\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introduce realistic data quality issues\n",
    "# Missing values\n",
    "missing_age_idx = np.random.choice(n_samples, 50, replace=False)\n",
    "missing_income_idx = np.random.choice(n_samples, 80, replace=False)\n",
    "missing_credit_idx = np.random.choice(n_samples, 30, replace=False)\n",
    "\n",
    "df.loc[missing_age_idx, 'age'] = np.nan\n",
    "df.loc[missing_income_idx, 'income'] = np.nan\n",
    "df.loc[missing_credit_idx, 'credit_score'] = np.nan\n",
    "\n",
    "# Some outliers\n",
    "df.loc[np.random.choice(n_samples, 5), 'income'] = 1e7  # Millionaires\n",
    "df.loc[np.random.choice(n_samples, 3), 'years_employed'] = 50  # Very long tenure\n",
    "\n",
    "print(\"Dataset created! Let's explore it...\")\n",
    "print(f\"\\nShape: {df.shape[0]} rows, {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look at the data\n",
    "print(\"First 10 rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and basic info\n",
    "print(\"Data Types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (df.isnull().sum() / len(df) * 100).round(1)\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Missing %': missing_pct\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Numerical Features Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features\n",
    "print(\"Categorical Features:\")\n",
    "for col in ['education', 'employment_type']:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df[col].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Did We Find?\n",
    "\n",
    "Our dataset has several issues to address:\n",
    "1. **Missing values** in age (5%), income (8%), credit_score (3%), education (5%)\n",
    "2. **Outliers** in income (some very high values)\n",
    "3. **Categorical variables** that need encoding\n",
    "4. **Different scales** (age ~18-80, income ~thousands, credit_score ~300-850)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Handling Missing Values\n",
    "\n",
    "### üßí ELI5: Missing Data Strategies\n",
    "\n",
    "> **Imagine you're taking attendance, but some kids forgot to sign in...** üìã\n",
    ">\n",
    "> You have options:\n",
    "> - **Just remove them:** Don't count missing kids (deletion)\n",
    "> - **Use the average:** \"Most kids are 10 years old, so mark this one as 10\" (mean imputation)\n",
    "> - **Use the middle value:** \"Half the class is above/below 10\" (median imputation)\n",
    "> - **Use the most common:** \"Most kids chose pizza for lunch\" (mode imputation)\n",
    "> - **Ask a friend:** \"Johnny usually sits near missing kid, what would he guess?\" (model-based)\n",
    ">\n",
    "> Each method has trade-offs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Mean/Median imputation\n",
    "# Good for normally distributed data (mean) or skewed data (median)\n",
    "\n",
    "df_imputed = df.copy()\n",
    "\n",
    "# For age: Use median (less affected by outliers)\n",
    "age_median = df_imputed['age'].median()\n",
    "df_imputed['age'].fillna(age_median, inplace=True)\n",
    "print(f\"Age: Imputed {df['age'].isna().sum()} missing values with median = {age_median}\")\n",
    "\n",
    "# For income: Use median (it's right-skewed with outliers)\n",
    "income_median = df_imputed['income'].median()\n",
    "df_imputed['income'].fillna(income_median, inplace=True)\n",
    "print(f\"Income: Imputed {df['income'].isna().sum()} missing values with median = ${income_median:,.0f}\")\n",
    "\n",
    "# For credit_score: Use mean (approximately normal distribution)\n",
    "credit_mean = df_imputed['credit_score'].mean()\n",
    "df_imputed['credit_score'].fillna(credit_mean, inplace=True)\n",
    "print(f\"Credit Score: Imputed {df['credit_score'].isna().sum()} missing values with mean = {credit_mean:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Mode imputation for categorical data\n",
    "# Fill with the most frequent value\n",
    "\n",
    "education_mode = df_imputed['education'].mode()[0]\n",
    "df_imputed['education'].fillna(education_mode, inplace=True)\n",
    "print(f\"Education: Imputed missing values with mode = '{education_mode}'\")\n",
    "\n",
    "# Verify no more missing values\n",
    "print(f\"\\nRemaining missing values: {df_imputed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 3: Create a \"missing\" indicator (sometimes useful!)\n",
    "# The fact that data is missing can itself be informative\n",
    "\n",
    "df_with_indicators = df.copy()\n",
    "\n",
    "# Add indicators for missing values\n",
    "df_with_indicators['income_missing'] = df['income'].isna().astype(int)\n",
    "df_with_indicators['credit_missing'] = df['credit_score'].isna().astype(int)\n",
    "\n",
    "# Then impute\n",
    "df_with_indicators['income'].fillna(df['income'].median(), inplace=True)\n",
    "df_with_indicators['credit_score'].fillna(df['credit_score'].mean(), inplace=True)\n",
    "\n",
    "print(\"Added missing value indicators:\")\n",
    "print(df_with_indicators[['income', 'income_missing', 'credit_score', 'credit_missing']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself: Exercise 1\n",
    "\n",
    "**Task:** Implement group-based imputation.\n",
    "\n",
    "Instead of using the global median for `income`, impute using the median income *for each education level*.\n",
    "\n",
    "This makes sense because income likely varies by education!\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "Use `df.groupby('education')['income'].transform()` with a function that returns the median where values are not null.\n",
    "\n",
    "```python\n",
    "df['income'] = df.groupby('education')['income'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE - Exercise 1\n",
    "df_group_imputed = df.copy()\n",
    "\n",
    "# First, fill education missing values (we need groups)\n",
    "df_group_imputed['education'].fillna(df['education'].mode()[0], inplace=True)\n",
    "\n",
    "# TODO: Impute income using median per education group\n",
    "# Hint: Use groupby and transform\n",
    "\n",
    "# Uncomment to verify:\n",
    "# print(\"Median income by education (before imputation):\")\n",
    "# print(df.groupby('education')['income'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Encoding Categorical Variables\n",
    "\n",
    "ML models work with numbers, not text. We need to convert categorical variables!\n",
    "\n",
    "### üßí ELI5: Why Encode Categories?\n",
    "\n",
    "> **Imagine teaching a robot to understand colors...** ü§ñ\n",
    ">\n",
    "> The robot only understands numbers. How do you explain \"red\", \"blue\", \"green\"?\n",
    ">\n",
    "> **Option 1 - Label Encoding:** Red=1, Blue=2, Green=3\n",
    "> - Problem: Robot thinks Green(3) > Blue(2) > Red(1). But colors aren't ordered!\n",
    ">\n",
    "> **Option 2 - One-Hot Encoding:** \n",
    "> - Red   = [1, 0, 0]\n",
    "> - Blue  = [0, 1, 0]  \n",
    "> - Green = [0, 0, 1]\n",
    "> - Now each color is equally different from the others!\n",
    ">\n",
    "> **When to use which:**\n",
    "> - Label: For ordinal data (Low < Medium < High)\n",
    "> - One-Hot: For nominal data (Red, Blue, Green - no order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start fresh with imputed data\n",
    "df_encoded = df_imputed.copy()\n",
    "\n",
    "print(\"Before encoding:\")\n",
    "print(df_encoded[['education', 'employment_type']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Label Encoding (for ordinal data)\n",
    "# Education has a natural order: High School < Bachelor < Master < PhD\n",
    "\n",
    "education_order = {\n",
    "    'High School': 0,\n",
    "    'Bachelor': 1,\n",
    "    'Master': 2,\n",
    "    'PhD': 3\n",
    "}\n",
    "\n",
    "df_encoded['education_encoded'] = df_encoded['education'].map(education_order)\n",
    "\n",
    "print(\"Label Encoded Education:\")\n",
    "print(df_encoded[['education', 'education_encoded']].drop_duplicates().sort_values('education_encoded'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: One-Hot Encoding (for nominal data)\n",
    "# Employment type has no natural order\n",
    "\n",
    "employment_dummies = pd.get_dummies(\n",
    "    df_encoded['employment_type'], \n",
    "    prefix='emp',\n",
    "    dtype=int\n",
    ")\n",
    "\n",
    "print(\"One-Hot Encoded Employment Type:\")\n",
    "print(employment_dummies.head())\n",
    "print(f\"\\nNew columns created: {list(employment_dummies.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine with original dataframe\n",
    "df_encoded = pd.concat([df_encoded, employment_dummies], axis=1)\n",
    "\n",
    "# Drop original categorical columns\n",
    "df_encoded = df_encoded.drop(['education', 'employment_type'], axis=1)\n",
    "\n",
    "print(\"Final encoded dataframe:\")\n",
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Feature Scaling\n",
    "\n",
    "Different features have different scales. Most ML algorithms work better when features are on similar scales.\n",
    "\n",
    "### üßí ELI5: Why Scale Features?\n",
    "\n",
    "> **Imagine comparing people by height and age...** üìè\n",
    ">\n",
    "> - Person A: Height = 180cm, Age = 25\n",
    "> - Person B: Height = 160cm, Age = 45\n",
    ">\n",
    "> If you calculate \"distance\" between them:\n",
    "> - Height difference: 20\n",
    "> - Age difference: 20\n",
    ">\n",
    "> These look equal, but 20cm is a big height difference while 20 years is enormous!\n",
    ">\n",
    "> **Scaling puts everything on the same playing field** so that one feature\n",
    "> doesn't dominate just because it has bigger numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the current scales\n",
    "numeric_cols = ['age', 'income', 'credit_score', 'years_employed']\n",
    "\n",
    "print(\"Current feature ranges:\")\n",
    "print(df_imputed[numeric_cols].describe().loc[['min', 'max', 'mean', 'std']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: StandardScaler (Z-score normalization)\n",
    "# Transforms to mean=0, std=1\n",
    "# Best for: Algorithms that assume normal distribution (linear regression, SVM)\n",
    "\n",
    "def standard_scale(data):\n",
    "    \"\"\"Scale features to have mean=0 and std=1.\"\"\"\n",
    "    mean = data.mean(axis=0)\n",
    "    std = data.std(axis=0)\n",
    "    return (data - mean) / std, mean, std\n",
    "\n",
    "df_standard = df_imputed.copy()\n",
    "X = df_standard[numeric_cols].values\n",
    "\n",
    "X_scaled, means, stds = standard_scale(X)\n",
    "df_standard[numeric_cols] = X_scaled\n",
    "\n",
    "print(\"After StandardScaler:\")\n",
    "print(df_standard[numeric_cols].describe().loc[['min', 'max', 'mean', 'std']].round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: MinMaxScaler\n",
    "# Transforms to range [0, 1]\n",
    "# Best for: Neural networks, image data, when you need bounded values\n",
    "\n",
    "def minmax_scale(data):\n",
    "    \"\"\"Scale features to range [0, 1].\"\"\"\n",
    "    min_val = data.min(axis=0)\n",
    "    max_val = data.max(axis=0)\n",
    "    return (data - min_val) / (max_val - min_val), min_val, max_val\n",
    "\n",
    "df_minmax = df_imputed.copy()\n",
    "X = df_minmax[numeric_cols].values\n",
    "\n",
    "X_scaled, mins, maxs = minmax_scale(X)\n",
    "df_minmax[numeric_cols] = X_scaled\n",
    "\n",
    "print(\"After MinMaxScaler:\")\n",
    "print(df_minmax[numeric_cols].describe().loc[['min', 'max', 'mean', 'std']].round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: RobustScaler\n",
    "# Uses median and IQR - resistant to outliers!\n",
    "# Best for: Data with outliers that you want to keep\n",
    "\n",
    "def robust_scale(data):\n",
    "    \"\"\"Scale features using median and interquartile range.\"\"\"\n",
    "    median = np.median(data, axis=0)\n",
    "    q75, q25 = np.percentile(data, [75, 25], axis=0)\n",
    "    iqr = q75 - q25\n",
    "    return (data - median) / iqr, median, iqr\n",
    "\n",
    "df_robust = df_imputed.copy()\n",
    "X = df_robust[numeric_cols].values\n",
    "\n",
    "X_scaled, medians, iqrs = robust_scale(X)\n",
    "df_robust[numeric_cols] = X_scaled\n",
    "\n",
    "print(\"After RobustScaler:\")\n",
    "print(df_robust[numeric_cols].describe().loc[['min', 'max', 'mean', 'std']].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Comparison of Scaling Methods\n",
    "\n",
    "| Method | Range | Handles Outliers? | Best For |\n",
    "|--------|-------|-------------------|----------|\n",
    "| StandardScaler | Unbounded | No | Normal distributions |\n",
    "| MinMaxScaler | [0, 1] | No | Neural networks, bounded features |\n",
    "| RobustScaler | Unbounded | Yes | Data with outliers |\n",
    "\n",
    "Look at the min/max values - RobustScaler has the widest range because outliers aren't \"squished\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Building a Reusable Preprocessor Class\n",
    "\n",
    "Now let's use our production-ready Preprocessor class from the scripts folder!\n",
    "\n",
    "### üßí ELI5: Why a Preprocessor Class?\n",
    "\n",
    "> **Imagine you have a magic recipe book...** üìñ\n",
    ">\n",
    "> Every time you make cookies:\n",
    "> 1. Mix ingredients the same way\n",
    "> 2. Use the same oven temperature\n",
    "> 3. Bake for the same time\n",
    ">\n",
    "> A class is like that recipe book - it remembers exactly how to prepare data\n",
    "> so you can repeat it consistently on new data later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our custom Preprocessor\n",
    "from preprocessing_pipeline import Preprocessor\n",
    "\n",
    "print(\"Preprocessor class imported successfully!\")\n",
    "print(\"\\nDocstring:\")\n",
    "print(Preprocessor.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/test (in practice, do this before any preprocessing!)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {len(train_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit preprocessor\n",
    "preprocessor = Preprocessor(\n",
    "    numeric_features=['age', 'income', 'credit_score', 'years_employed'],\n",
    "    categorical_features=['education', 'employment_type'],\n",
    "    ordinal_mappings={\n",
    "        'education': {\n",
    "            'High School': 0,\n",
    "            'Bachelor': 1,\n",
    "            'Master': 2,\n",
    "            'PhD': 3\n",
    "        }\n",
    "    },\n",
    "    scaling='standard',\n",
    "    impute_strategy='median'\n",
    ")\n",
    "\n",
    "# Fit on training data only!\n",
    "train_processed = preprocessor.fit_transform(train_df)\n",
    "\n",
    "# Transform test data (using parameters learned from training)\n",
    "test_processed = preprocessor.transform(test_df)\n",
    "\n",
    "print(\"Preprocessing complete!\\n\")\n",
    "print(f\"Output features: {preprocessor.get_feature_names()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the output\n",
    "print(\"Training data after preprocessing:\")\n",
    "print(train_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that scaling is correct (should be mean~0, std~1 for training data)\n",
    "print(\"Training data statistics (should be ~0 mean, ~1 std):\")\n",
    "print(train_processed[['age', 'income', 'credit_score', 'years_employed']].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessor for later use\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Determine save path relative to notebook\n",
    "try:\n",
    "    save_dir = Path(__vsc_ipynb_file__).parent / '../data'\n",
    "except NameError:\n",
    "    save_dir = Path('../data')\n",
    "\n",
    "save_path = save_dir / 'preprocessor.pkl'\n",
    "\n",
    "try:\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(preprocessor, f)\n",
    "    print(f\"‚úÖ Preprocessor saved to {save_path}\")\n",
    "    print(\"\\nYou can load it later with:\")\n",
    "    print(\"  with open('preprocessor.pkl', 'rb') as f:\")\n",
    "    print(\"      preprocessor = pickle.load(f)\")\n",
    "except IOError as e:\n",
    "    print(f\"‚ö†Ô∏è Could not save preprocessor: {e}\")\n",
    "    print(\"   This is optional - you can continue without saving.\")\n",
    "    print(\"   The preprocessor object is still available in memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Fitting on test data (data leakage!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Fitting on all data before splitting\n",
    "# This causes \"data leakage\" - test data info leaks into training!\n",
    "\n",
    "# all_data_scaled = preprocessor.fit_transform(all_data)  # DON'T DO THIS\n",
    "# train, test = train_test_split(all_data_scaled)         # Test data influenced fit!\n",
    "\n",
    "# ‚úÖ Right: Split first, then fit on train only\n",
    "# train, test = train_test_split(raw_data)\n",
    "# train_processed = preprocessor.fit_transform(train)  # Fit on train\n",
    "# test_processed = preprocessor.transform(test)        # Only transform test\n",
    "\n",
    "print(\"üí° Always split data BEFORE preprocessing!\")\n",
    "print(\"   Fit on training data, transform on both.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Forgetting to handle new categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if test data has a category not in training?\n",
    "\n",
    "# Example: Training had ['A', 'B', 'C'], test has ['A', 'B', 'D']\n",
    "# One-hot encoding will create wrong columns!\n",
    "\n",
    "print(\"‚ö†Ô∏è Our Preprocessor handles this by only using categories from training.\")\n",
    "print(\"   New categories become all-zeros (which may or may not be desired).\")\n",
    "print(\"\")\n",
    "print(\"üí° For production, consider:\")\n",
    "print(\"   - Use an 'Unknown' category\")\n",
    "print(\"   - Map new categories to most similar known category\")\n",
    "print(\"   - Raise an error if unexpected category appears\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Scaling the target variable incorrectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For classification: DON'T scale the target (it's categorical: 0 or 1)\n",
    "# For regression: You CAN scale the target, but must inverse-transform predictions!\n",
    "\n",
    "print(\"üí° In our example, 'default' is binary (0/1) - no scaling needed!\")\n",
    "print(\"\")\n",
    "print(\"   For regression targets:\")\n",
    "print(\"   1. Scale target during training\")\n",
    "print(\"   2. Inverse-scale predictions to get real values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ Loading and exploring datasets with Pandas\n",
    "- ‚úÖ Multiple strategies for handling missing values\n",
    "- ‚úÖ Label vs One-Hot encoding for categorical features\n",
    "- ‚úÖ StandardScaler, MinMaxScaler, and RobustScaler\n",
    "- ‚úÖ Building and using a reusable Preprocessor class\n",
    "- ‚úÖ Avoiding data leakage in preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Build a complete preprocessing pipeline for the Titanic dataset!**\n",
    "\n",
    "1. Download the Titanic dataset from Kaggle or use seaborn's built-in version\n",
    "2. Handle missing values in Age, Cabin, and Embarked\n",
    "3. Engineer new features (e.g., Title from Name, FamilySize)\n",
    "4. Encode categorical variables\n",
    "5. Scale numeric features\n",
    "6. Save your preprocessor parameters for later use\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "titanic = sns.load_dataset('titanic')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- [Scikit-learn Preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "- [Feature Engineering for Machine Learning](https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "import gc\n",
    "\n",
    "del df, df_imputed, df_encoded, train_df, test_df, train_processed, test_processed\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Memory cleaned up!\")\n",
    "print(\"\\nüéâ Congratulations! You've mastered data preprocessing!\")\n",
    "print(\"   Next up: Lab 1.2.3 - Visualization Dashboard\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
