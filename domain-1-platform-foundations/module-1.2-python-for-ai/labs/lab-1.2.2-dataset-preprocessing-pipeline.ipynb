{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.2.2: Dataset Preprocessing Pipeline\n",
    "\n",
    "**Module:** 1.2 - Python for AI/ML  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Load and explore datasets using Pandas\n",
    "- [ ] Handle missing values with multiple strategies\n",
    "- [ ] Encode categorical variables properly\n",
    "- [ ] Implement feature scaling transformations\n",
    "- [ ] Use a production-ready `Preprocessor` class for ML pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Lab 1.2.1 (NumPy Broadcasting)\n",
    "- Knowledge of: Basic Python classes, NumPy basics\n",
    "\n",
    "### Required Packages\n",
    "- Python 3.9+\n",
    "- NumPy >= 1.21\n",
    "- Pandas >= 1.3\n",
    "- scikit-learn >= 1.0 (for train_test_split)\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**\"Garbage in, garbage out\"** - the oldest saying in data science.\n",
    "\n",
    "In the real world:\n",
    "- 80% of a data scientist's time is spent on data preparation\n",
    "- Raw data has missing values, inconsistent formats, outliers\n",
    "- A model is only as good as the data it's trained on\n",
    "\n",
    "**Examples:**\n",
    "- Medical records with missing patient data\n",
    "- E-commerce data with mixed currencies and formats\n",
    "- Sensor data with failed readings\n",
    "\n",
    "A solid preprocessing pipeline is the foundation of any ML project!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßí ELI5: Data Preprocessing\n",
    "\n",
    "> **Imagine you're making a recipe, but the ingredients are a mess...** üç≥\n",
    ">\n",
    "> - Some tomatoes are still in the fridge, some are rotten (missing data)\n",
    "> - The recipe uses cups but you only have a scale (different formats)\n",
    "> - Some ingredients are in grams, others in pounds (different scales)\n",
    ">\n",
    "> Before you can cook, you need to:\n",
    "> 1. Find and replace the bad tomatoes\n",
    "> 2. Convert everything to the same units\n",
    "> 3. Measure out equal portions\n",
    ">\n",
    "> **In AI terms:** ML models need clean, consistent, properly-scaled data.\n",
    "> Preprocessing transforms messy real-world data into something models can digest.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.3\n",
      "NumPy version: 2.1.0\n",
      "Pandas version: 2.3.2\n",
      "scikit-learn version: 1.7.1\n",
      "\n",
      "‚úÖ All data files present in /home/trosfy/projects/dgx-spark-ai-curriculum/domain-1-platform-foundations/module-1.2-python-for-ai/data\n",
      "\n",
      "==================================================\n",
      "Welcome to the Preprocessing Pipeline Lab! üîß\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Environment Setup and Dependency Checks\n",
    "# ============================================================\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Determine the notebook's directory for reliable path resolution\n",
    "# Works in Jupyter, VS Code, and command-line execution\n",
    "try:\n",
    "    # VS Code Jupyter\n",
    "    notebook_dir = Path(__vsc_ipynb_file__).parent\n",
    "except NameError:\n",
    "    try:\n",
    "        # Standard Jupyter with ipykernel\n",
    "        import IPython\n",
    "        notebook_dir = Path(IPython.get_ipython().kernel.session.config.get('IPKernelApp', {}).get('connection_file', '')).parent\n",
    "        if not (notebook_dir / '../scripts').exists():\n",
    "            notebook_dir = Path.cwd()\n",
    "    except:\n",
    "        notebook_dir = Path.cwd()\n",
    "\n",
    "# Add scripts directory to path (robust method)\n",
    "scripts_dir = (notebook_dir / '../scripts').resolve()\n",
    "if scripts_dir.exists() and str(scripts_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(scripts_dir))\n",
    "elif not scripts_dir.exists():\n",
    "    # Fallback: try relative to cwd\n",
    "    scripts_dir = Path('../scripts').resolve()\n",
    "    if scripts_dir.exists() and str(scripts_dir) not in sys.path:\n",
    "        sys.path.insert(0, str(scripts_dir))\n",
    "\n",
    "# Check required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "\n",
    "# Check sklearn availability (needed later)\n",
    "try:\n",
    "    import sklearn\n",
    "    print(f\"scikit-learn version: {sklearn.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è scikit-learn not installed!\")\n",
    "    print(\"   Install with: pip install scikit-learn\")\n",
    "    print(\"   Some cells will fail without it.\")\n",
    "\n",
    "# Check if data files exist\n",
    "data_dir = (notebook_dir / '../data').resolve()\n",
    "if not data_dir.exists():\n",
    "    data_dir = Path('../data').resolve()\n",
    "\n",
    "required_files = ['sample_customers.csv', 'sample_training_history.json', \n",
    "                  'sample_embeddings.npy', 'sample_confusion_data.json']\n",
    "missing_files = [f for f in required_files if not (data_dir / f).exists()]\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\n‚ö†Ô∏è Data files not found: {missing_files}\")\n",
    "    generator_script = data_dir / 'generate_sample_data.py'\n",
    "    if generator_script.exists():\n",
    "        print(\"   Generating sample data...\")\n",
    "        result = subprocess.run([sys.executable, str(generator_script)], \n",
    "                               capture_output=True, text=True, cwd=str(data_dir))\n",
    "        if result.returncode == 0:\n",
    "            print(\"   ‚úÖ Sample data generated successfully!\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Error generating data: {result.stderr}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Generator script not found at: {generator_script}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All data files present in {data_dir}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Welcome to the Preprocessing Pipeline Lab! üîß\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Loading and Exploring Data\n",
    "\n",
    "Let's create a realistic dataset with common data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created! Let's explore it...\n",
      "\n",
      "Shape: 1000 rows, 7 columns\n"
     ]
    }
   ],
   "source": [
    "# Create a synthetic dataset with realistic issues\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate synthetic customer data\n",
    "data = {\n",
    "    'age': np.random.randint(18, 80, n_samples).astype(float),\n",
    "    'income': np.random.lognormal(10.5, 0.5, n_samples),\n",
    "    'credit_score': np.random.randint(300, 850, n_samples).astype(float),\n",
    "    'years_employed': np.random.exponential(5, n_samples),\n",
    "    'education': np.random.choice(\n",
    "        ['High School', 'Bachelor', 'Master', 'PhD', None], \n",
    "        n_samples, \n",
    "        p=[0.3, 0.35, 0.2, 0.1, 0.05]\n",
    "    ),\n",
    "    'employment_type': np.random.choice(\n",
    "        ['Full-time', 'Part-time', 'Self-employed', 'Unemployed'],\n",
    "        n_samples,\n",
    "        p=[0.6, 0.15, 0.15, 0.1]\n",
    "    ),\n",
    "    'default': np.random.choice([0, 1], n_samples, p=[0.85, 0.15])\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introduce realistic data quality issues\n",
    "# Missing values\n",
    "missing_age_idx = np.random.choice(n_samples, 50, replace=False)\n",
    "missing_income_idx = np.random.choice(n_samples, 80, replace=False)\n",
    "missing_credit_idx = np.random.choice(n_samples, 30, replace=False)\n",
    "\n",
    "df.loc[missing_age_idx, 'age'] = np.nan\n",
    "df.loc[missing_income_idx, 'income'] = np.nan\n",
    "df.loc[missing_credit_idx, 'credit_score'] = np.nan\n",
    "\n",
    "# Some outliers\n",
    "df.loc[np.random.choice(n_samples, 5), 'income'] = 1e7  # Millionaires\n",
    "df.loc[np.random.choice(n_samples, 3), 'years_employed'] = 50  # Very long tenure\n",
    "\n",
    "print(\"Dataset created! Let's explore it...\")\n",
    "print(f\"\\nShape: {df.shape[0]} rows, {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>years_employed</th>\n",
       "      <th>education</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56.0</td>\n",
       "      <td>72127.372552</td>\n",
       "      <td>541.0</td>\n",
       "      <td>11.844744</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.0</td>\n",
       "      <td>25876.925765</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.894922</td>\n",
       "      <td>Master</td>\n",
       "      <td>Part-time</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.0</td>\n",
       "      <td>64651.457991</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.147562</td>\n",
       "      <td>High School</td>\n",
       "      <td>Part-time</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.0</td>\n",
       "      <td>30106.449167</td>\n",
       "      <td>499.0</td>\n",
       "      <td>8.299978</td>\n",
       "      <td>High School</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>25666.117690</td>\n",
       "      <td>378.0</td>\n",
       "      <td>14.724744</td>\n",
       "      <td>High School</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>472.0</td>\n",
       "      <td>2.057467</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>78.0</td>\n",
       "      <td>43097.852937</td>\n",
       "      <td>766.0</td>\n",
       "      <td>6.454579</td>\n",
       "      <td>Master</td>\n",
       "      <td>Part-time</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>38.0</td>\n",
       "      <td>94444.232145</td>\n",
       "      <td>715.0</td>\n",
       "      <td>3.869758</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>56.0</td>\n",
       "      <td>42606.784896</td>\n",
       "      <td>748.0</td>\n",
       "      <td>0.312723</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>75.0</td>\n",
       "      <td>40000.171914</td>\n",
       "      <td>386.0</td>\n",
       "      <td>5.855911</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age        income  credit_score  years_employed    education  \\\n",
       "0  56.0  72127.372552         541.0       11.844744     Bachelor   \n",
       "1  69.0  25876.925765         531.0        0.894922       Master   \n",
       "2  46.0  64651.457991           NaN        5.147562  High School   \n",
       "3  32.0  30106.449167         499.0        8.299978  High School   \n",
       "4   NaN  25666.117690         378.0       14.724744  High School   \n",
       "5  25.0           NaN         472.0        2.057467     Bachelor   \n",
       "6  78.0  43097.852937         766.0        6.454579       Master   \n",
       "7  38.0  94444.232145         715.0        3.869758          PhD   \n",
       "8  56.0  42606.784896         748.0        0.312723     Bachelor   \n",
       "9  75.0  40000.171914         386.0        5.855911          PhD   \n",
       "\n",
       "  employment_type  default  \n",
       "0       Full-time        0  \n",
       "1       Part-time        1  \n",
       "2       Part-time        0  \n",
       "3   Self-employed        0  \n",
       "4   Self-employed        1  \n",
       "5   Self-employed        0  \n",
       "6       Part-time        0  \n",
       "7       Full-time        0  \n",
       "8       Full-time        0  \n",
       "9       Full-time        0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First look at the data\n",
    "print(\"First 10 rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types:\n",
      "age                float64\n",
      "income             float64\n",
      "credit_score       float64\n",
      "years_employed     float64\n",
      "education           object\n",
      "employment_type     object\n",
      "default              int64\n",
      "dtype: object\n",
      "\n",
      "Memory usage: 151.1 KB\n"
     ]
    }
   ],
   "source": [
    "# Data types and basic info\n",
    "print(\"Data Types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      "              Missing Count  Missing %\n",
      "age                      50        5.0\n",
      "income                   79        7.9\n",
      "credit_score             30        3.0\n",
      "education                37        3.7\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (df.isnull().sum() / len(df) * 100).round(1)\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Missing %': missing_pct\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Features Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>years_employed</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>950.000000</td>\n",
       "      <td>9.210000e+02</td>\n",
       "      <td>970.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>49.727368</td>\n",
       "      <td>9.691093e+04</td>\n",
       "      <td>581.729897</td>\n",
       "      <td>5.296812</td>\n",
       "      <td>0.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18.138630</td>\n",
       "      <td>7.324511e+05</td>\n",
       "      <td>158.442221</td>\n",
       "      <td>5.451948</td>\n",
       "      <td>0.362086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>6.379949e+03</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>0.002214</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>34.250000</td>\n",
       "      <td>2.653831e+04</td>\n",
       "      <td>447.250000</td>\n",
       "      <td>1.677073</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>3.802134e+04</td>\n",
       "      <td>578.500000</td>\n",
       "      <td>3.697526</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>66.000000</td>\n",
       "      <td>5.379937e+04</td>\n",
       "      <td>721.500000</td>\n",
       "      <td>7.235312</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>79.000000</td>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>849.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age        income  credit_score  years_employed      default\n",
       "count  950.000000  9.210000e+02    970.000000     1000.000000  1000.000000\n",
       "mean    49.727368  9.691093e+04    581.729897        5.296812     0.155000\n",
       "std     18.138630  7.324511e+05    158.442221        5.451948     0.362086\n",
       "min     18.000000  6.379949e+03    300.000000        0.002214     0.000000\n",
       "25%     34.250000  2.653831e+04    447.250000        1.677073     0.000000\n",
       "50%     50.000000  3.802134e+04    578.500000        3.697526     0.000000\n",
       "75%     66.000000  5.379937e+04    721.500000        7.235312     0.000000\n",
       "max     79.000000  1.000000e+07    849.000000       50.000000     1.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistical summary\n",
    "print(\"Numerical Features Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Features:\n",
      "\n",
      "education:\n",
      "education\n",
      "Bachelor       337\n",
      "High School    332\n",
      "Master         195\n",
      "PhD             99\n",
      "None            37\n",
      "Name: count, dtype: int64\n",
      "\n",
      "employment_type:\n",
      "employment_type\n",
      "Full-time        611\n",
      "Self-employed    155\n",
      "Part-time        151\n",
      "Unemployed        83\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Categorical features\n",
    "print(\"Categorical Features:\")\n",
    "for col in ['education', 'employment_type']:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df[col].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Did We Find?\n",
    "\n",
    "Our dataset has several issues to address:\n",
    "1. **Missing values** in age (5%), income (8%), credit_score (3%), education (5%)\n",
    "2. **Outliers** in income (some very high values)\n",
    "3. **Categorical variables** that need encoding\n",
    "4. **Different scales** (age ~18-80, income ~thousands, credit_score ~300-850)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Handling Missing Values\n",
    "\n",
    "### üßí ELI5: Missing Data Strategies\n",
    "\n",
    "> **Imagine you're taking attendance, but some kids forgot to sign in...** üìã\n",
    ">\n",
    "> You have options:\n",
    "> - **Just remove them:** Don't count missing kids (deletion)\n",
    "> - **Use the average:** \"Most kids are 10 years old, so mark this one as 10\" (mean imputation)\n",
    "> - **Use the middle value:** \"Half the class is above/below 10\" (median imputation)\n",
    "> - **Use the most common:** \"Most kids chose pizza for lunch\" (mode imputation)\n",
    "> - **Ask a friend:** \"Johnny usually sits near missing kid, what would he guess?\" (model-based)\n",
    ">\n",
    "> Each method has trade-offs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age: Imputed 50 missing values with median = 50.0\n",
      "Income: Imputed 79 missing values with median = $38,021\n",
      "Credit Score: Imputed 30 missing values with mean = 582\n"
     ]
    }
   ],
   "source": [
    "# Strategy 1: Mean/Median imputation\n",
    "# Good for normally distributed data (mean) or skewed data (median)\n",
    "\n",
    "df_imputed = df.copy()\n",
    "\n",
    "# For age: Use median (less affected by outliers)\n",
    "age_median = df_imputed['age'].median()\n",
    "df_imputed['age'].fillna(age_median, inplace=True)\n",
    "print(f\"Age: Imputed {df['age'].isna().sum()} missing values with median = {age_median}\")\n",
    "\n",
    "# For income: Use median (it's right-skewed with outliers)\n",
    "income_median = df_imputed['income'].median()\n",
    "df_imputed['income'].fillna(income_median, inplace=True)\n",
    "print(f\"Income: Imputed {df['income'].isna().sum()} missing values with median = ${income_median:,.0f}\")\n",
    "\n",
    "# For credit_score: Use mean (approximately normal distribution)\n",
    "credit_mean = df_imputed['credit_score'].mean()\n",
    "df_imputed['credit_score'].fillna(credit_mean, inplace=True)\n",
    "print(f\"Credit Score: Imputed {df['credit_score'].isna().sum()} missing values with mean = {credit_mean:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Education: Imputed missing values with mode = 'Bachelor'\n",
      "\n",
      "Remaining missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Strategy 2: Mode imputation for categorical data\n",
    "# Fill with the most frequent value\n",
    "\n",
    "education_mode = df_imputed['education'].mode()[0]\n",
    "df_imputed['education'].fillna(education_mode, inplace=True)\n",
    "print(f\"Education: Imputed missing values with mode = '{education_mode}'\")\n",
    "\n",
    "# Verify no more missing values\n",
    "print(f\"\\nRemaining missing values: {df_imputed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added missing value indicators:\n",
      "         income  income_missing  credit_score  credit_missing\n",
      "0  72127.372552               0    541.000000               0\n",
      "1  25876.925765               0    531.000000               0\n",
      "2  64651.457991               0    581.729897               1\n",
      "3  30106.449167               0    499.000000               0\n",
      "4  25666.117690               0    378.000000               0\n",
      "5  38021.341036               1    472.000000               0\n",
      "6  43097.852937               0    766.000000               0\n",
      "7  94444.232145               0    715.000000               0\n",
      "8  42606.784896               0    748.000000               0\n",
      "9  40000.171914               0    386.000000               0\n"
     ]
    }
   ],
   "source": [
    "# Strategy 3: Create a \"missing\" indicator (sometimes useful!)\n",
    "# The fact that data is missing can itself be informative\n",
    "\n",
    "df_with_indicators = df.copy()\n",
    "\n",
    "# Add indicators for missing values\n",
    "df_with_indicators['income_missing'] = df['income'].isna().astype(int)\n",
    "df_with_indicators['credit_missing'] = df['credit_score'].isna().astype(int)\n",
    "\n",
    "# Then impute\n",
    "df_with_indicators['income'].fillna(df['income'].median(), inplace=True)\n",
    "df_with_indicators['credit_score'].fillna(df['credit_score'].mean(), inplace=True)\n",
    "\n",
    "print(\"Added missing value indicators:\")\n",
    "print(df_with_indicators[['income', 'income_missing', 'credit_score', 'credit_missing']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö Pandas GroupBy and Transform\n",
    "\n",
    "Before we try the exercise, let's learn a powerful Pandas pattern: **group-based operations**.\n",
    "\n",
    "**`groupby()`** - Splits data into groups based on column values\n",
    "**`transform()`** - Applies a function to each group and returns results aligned with the original index\n",
    "\n",
    "This is perfect for imputing missing values with group-specific statistics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "  category  value\n",
      "0        A   10.0\n",
      "1        A    NaN\n",
      "2        B   30.0\n",
      "3        B   40.0\n",
      "4        A   50.0\n",
      "\n",
      "--- Understanding groupby() ---\n",
      "\n",
      "Group 'A':\n",
      "  category  value\n",
      "0        A   10.0\n",
      "1        A    NaN\n",
      "4        A   50.0\n",
      "\n",
      "Group 'B':\n",
      "  category  value\n",
      "2        B   30.0\n",
      "3        B   40.0\n",
      "\n",
      "--- Using transform() ---\n",
      "\n",
      "Median per group (aligned with original rows):\n",
      "0    30.0\n",
      "1    30.0\n",
      "2    35.0\n",
      "3    35.0\n",
      "4    30.0\n",
      "Name: value, dtype: float64\n",
      "\n",
      "--- Group-based imputation ---\n",
      "  category  value  value_imputed\n",
      "0        A   10.0           10.0\n",
      "1        A    NaN           30.0\n",
      "2        B   30.0           30.0\n",
      "3        B   40.0           40.0\n",
      "4        A   50.0           50.0\n",
      "\n",
      "‚úÖ NaN in group 'A' was filled with median of group A (30.0)!\n"
     ]
    }
   ],
   "source": [
    "# GroupBy and Transform - A Powerful Combination\n",
    "# Let's understand how this works with a simple example\n",
    "\n",
    "# Create a small example DataFrame\n",
    "example_df = pd.DataFrame({\n",
    "    'category': ['A', 'A', 'B', 'B', 'A'],\n",
    "    'value': [10, np.nan, 30, 40, 50]\n",
    "})\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(example_df)\n",
    "\n",
    "# Step 1: groupby() splits data into groups\n",
    "print(\"\\n--- Understanding groupby() ---\")\n",
    "for name, group in example_df.groupby('category'):\n",
    "    print(f\"\\nGroup '{name}':\")\n",
    "    print(group)\n",
    "\n",
    "# Step 2: transform() applies a function to each group\n",
    "# and returns results with the SAME INDEX as the original\n",
    "print(\"\\n--- Using transform() ---\")\n",
    "\n",
    "# Calculate median for each group and broadcast back\n",
    "group_medians = example_df.groupby('category')['value'].transform('median')\n",
    "print(\"\\nMedian per group (aligned with original rows):\")\n",
    "print(group_medians)\n",
    "\n",
    "# Step 3: Use with fillna() to impute missing values per group\n",
    "print(\"\\n--- Group-based imputation ---\")\n",
    "example_df['value_imputed'] = example_df.groupby('category')['value'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "print(example_df)\n",
    "print(\"\\n‚úÖ NaN in group 'A' was filled with median of group A (30.0)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself: Exercise 1\n",
    "\n",
    "**Task:** Implement group-based imputation.\n",
    "\n",
    "Instead of using the global median for `income`, impute using the median income *for each education level*.\n",
    "\n",
    "This makes sense because income likely varies by education!\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "Use `df.groupby('education')['income'].transform()` with a function that returns the median where values are not null.\n",
    "\n",
    "```python\n",
    "df['income'] = df.groupby('education')['income'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median income by education (before imputation):\n",
      "education\n",
      "Bachelor       38243.882149\n",
      "High School    40751.323933\n",
      "Master         35390.042132\n",
      "PhD            37263.957723\n",
      "Name: income, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE - Exercise 1\n",
    "df_group_imputed = df.copy()\n",
    "\n",
    "# First, fill education missing values (we need groups)\n",
    "df_group_imputed['education'].fillna(df['education'].mode()[0], inplace=True)\n",
    "\n",
    "# TODO: Impute income using median per education group\n",
    "# Hint: Use groupby and transform\n",
    "df['income'] = df.groupby('education')['income'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "# Uncomment to verify:\n",
    "print(\"Median income by education (before imputation):\")\n",
    "print(df.groupby('education')['income'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Encoding Categorical Variables\n",
    "\n",
    "ML models work with numbers, not text. We need to convert categorical variables!\n",
    "\n",
    "### üßí ELI5: Why Encode Categories?\n",
    "\n",
    "> **Imagine teaching a robot to understand colors...** ü§ñ\n",
    ">\n",
    "> The robot only understands numbers. How do you explain \"red\", \"blue\", \"green\"?\n",
    ">\n",
    "> **Option 1 - Label Encoding:** Red=1, Blue=2, Green=3\n",
    "> - Problem: Robot thinks Green(3) > Blue(2) > Red(1). But colors aren't ordered!\n",
    ">\n",
    "> **Option 2 - One-Hot Encoding:** \n",
    "> - Red   = [1, 0, 0]\n",
    "> - Blue  = [0, 1, 0]  \n",
    "> - Green = [0, 0, 1]\n",
    "> - Now each color is equally different from the others!\n",
    ">\n",
    "> **When to use which:**\n",
    "> - Label: For ordinal data (Low < Medium < High)\n",
    "> - One-Hot: For nominal data (Red, Blue, Green - no order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö One-Hot Encoding with pd.get_dummies()\n",
    "\n",
    "Pandas provides `pd.get_dummies()` for one-hot encoding - it converts categorical columns into binary (0/1) columns.\n",
    "\n",
    "**Syntax:** `pd.get_dummies(data, prefix='column_prefix', dtype=int)`\n",
    "\n",
    "- `data`: Series or DataFrame to encode\n",
    "- `prefix`: String prefix for new column names  \n",
    "- `dtype`: Data type for output columns (use `int` for 0/1 values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before encoding:\n",
      "     education employment_type\n",
      "0     Bachelor       Full-time\n",
      "1       Master       Part-time\n",
      "2  High School       Part-time\n",
      "3  High School   Self-employed\n",
      "4  High School   Self-employed\n"
     ]
    }
   ],
   "source": [
    "# Start fresh with imputed data\n",
    "df_encoded = df_imputed.copy()\n",
    "\n",
    "print(\"Before encoding:\")\n",
    "print(df_encoded[['education', 'employment_type']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoded Education:\n",
      "     education  education_encoded\n",
      "2  High School                  0\n",
      "0     Bachelor                  1\n",
      "1       Master                  2\n",
      "7          PhD                  3\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Label Encoding (for ordinal data)\n",
    "# Education has a natural order: High School < Bachelor < Master < PhD\n",
    "\n",
    "education_order = {\n",
    "    'High School': 0,\n",
    "    'Bachelor': 1,\n",
    "    'Master': 2,\n",
    "    'PhD': 3\n",
    "}\n",
    "\n",
    "df_encoded['education_encoded'] = df_encoded['education'].map(education_order)\n",
    "\n",
    "print(\"Label Encoded Education:\")\n",
    "print(df_encoded[['education', 'education_encoded']].drop_duplicates().sort_values('education_encoded'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoded Employment Type:\n",
      "   emp_Full-time  emp_Part-time  emp_Self-employed  emp_Unemployed\n",
      "0              1              0                  0               0\n",
      "1              0              1                  0               0\n",
      "2              0              1                  0               0\n",
      "3              0              0                  1               0\n",
      "4              0              0                  1               0\n",
      "\n",
      "New columns created: ['emp_Full-time', 'emp_Part-time', 'emp_Self-employed', 'emp_Unemployed']\n"
     ]
    }
   ],
   "source": [
    "# Method 2: One-Hot Encoding (for nominal data)\n",
    "# Employment type has no natural order\n",
    "\n",
    "employment_dummies = pd.get_dummies(\n",
    "    df_encoded['employment_type'], \n",
    "    prefix='emp',\n",
    "    dtype=int\n",
    ")\n",
    "\n",
    "print(\"One-Hot Encoded Employment Type:\")\n",
    "print(employment_dummies.head())\n",
    "print(f\"\\nNew columns created: {list(employment_dummies.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final encoded dataframe:\n",
      "    age        income  credit_score  years_employed  default  \\\n",
      "0  56.0  72127.372552    541.000000       11.844744        0   \n",
      "1  69.0  25876.925765    531.000000        0.894922        1   \n",
      "2  46.0  64651.457991    581.729897        5.147562        0   \n",
      "3  32.0  30106.449167    499.000000        8.299978        0   \n",
      "4  50.0  25666.117690    378.000000       14.724744        1   \n",
      "\n",
      "   education_encoded  emp_Full-time  emp_Part-time  emp_Self-employed  \\\n",
      "0                  1              1              0                  0   \n",
      "1                  2              0              1                  0   \n",
      "2                  0              0              1                  0   \n",
      "3                  0              0              0                  1   \n",
      "4                  0              0              0                  1   \n",
      "\n",
      "   emp_Unemployed  \n",
      "0               0  \n",
      "1               0  \n",
      "2               0  \n",
      "3               0  \n",
      "4               0  \n"
     ]
    }
   ],
   "source": [
    "# Combine with original dataframe\n",
    "df_encoded = pd.concat([df_encoded, employment_dummies], axis=1)\n",
    "\n",
    "# Drop original categorical columns\n",
    "df_encoded = df_encoded.drop(['education', 'employment_type'], axis=1)\n",
    "\n",
    "print(\"Final encoded dataframe:\")\n",
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Feature Scaling\n",
    "\n",
    "Different features have different scales. Most ML algorithms work better when features are on similar scales.\n",
    "\n",
    "### üßí ELI5: Why Scale Features?\n",
    "\n",
    "> **Imagine comparing people by height and age...** üìè\n",
    ">\n",
    "> - Person A: Height = 180cm, Age = 25\n",
    "> - Person B: Height = 160cm, Age = 45\n",
    ">\n",
    "> If you calculate \"distance\" between them:\n",
    "> - Height difference: 20\n",
    "> - Age difference: 20\n",
    ">\n",
    "> These look equal, but 20cm is a big height difference while 20 years is enormous!\n",
    ">\n",
    "> **Scaling puts everything on the same playing field** so that one feature\n",
    "> doesn't dominate just because it has bigger numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current feature ranges:\n",
      "            age        income  credit_score  years_employed\n",
      "min   18.000000  6.379949e+03    300.000000        0.002214\n",
      "max   79.000000  1.000000e+07    849.000000       50.000000\n",
      "mean  49.741000  9.225865e+04    581.729897        5.296812\n",
      "std   17.678984  7.030736e+05    156.045075        5.451948\n"
     ]
    }
   ],
   "source": [
    "# Look at the current scales\n",
    "numeric_cols = ['age', 'income', 'credit_score', 'years_employed']\n",
    "\n",
    "print(\"Current feature ranges:\")\n",
    "print(df_imputed[numeric_cols].describe().loc[['min', 'max', 'mean', 'std']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After StandardScaler:\n",
      "       age  income  credit_score  years_employed\n",
      "min  -1.80   -0.12         -1.81           -0.97\n",
      "max   1.66   14.10          1.71            8.20\n",
      "mean  0.00   -0.00          0.00           -0.00\n",
      "std   1.00    1.00          1.00            1.00\n"
     ]
    }
   ],
   "source": [
    "# Method 1: StandardScaler (Z-score normalization)\n",
    "# Transforms to mean=0, std=1\n",
    "# Best for: Algorithms that assume normal distribution (linear regression, SVM)\n",
    "\n",
    "def standard_scale(data):\n",
    "    \"\"\"Scale features to have mean=0 and std=1.\"\"\"\n",
    "    mean = data.mean(axis=0)\n",
    "    std = data.std(axis=0)\n",
    "    return (data - mean) / std, mean, std\n",
    "\n",
    "df_standard = df_imputed.copy()\n",
    "X = df_standard[numeric_cols].values\n",
    "\n",
    "X_scaled, means, stds = standard_scale(X)\n",
    "df_standard[numeric_cols] = X_scaled\n",
    "\n",
    "print(\"After StandardScaler:\")\n",
    "print(df_standard[numeric_cols].describe().loc[['min', 'max', 'mean', 'std']].round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After MinMaxScaler:\n",
      "       age  income  credit_score  years_employed\n",
      "min   0.00    0.00          0.00            0.00\n",
      "max   1.00    1.00          1.00            1.00\n",
      "mean  0.52    0.01          0.51            0.11\n",
      "std   0.29    0.07          0.28            0.11\n"
     ]
    }
   ],
   "source": [
    "# Method 2: MinMaxScaler\n",
    "# Transforms to range [0, 1]\n",
    "# Best for: Neural networks, image data, when you need bounded values\n",
    "\n",
    "def minmax_scale(data):\n",
    "    \"\"\"Scale features to range [0, 1].\"\"\"\n",
    "    min_val = data.min(axis=0)\n",
    "    max_val = data.max(axis=0)\n",
    "    return (data - min_val) / (max_val - min_val), min_val, max_val\n",
    "\n",
    "df_minmax = df_imputed.copy()\n",
    "X = df_minmax[numeric_cols].values\n",
    "\n",
    "X_scaled, mins, maxs = minmax_scale(X)\n",
    "df_minmax[numeric_cols] = X_scaled\n",
    "\n",
    "print(\"After MinMaxScaler:\")\n",
    "print(df_minmax[numeric_cols].describe().loc[['min', 'max', 'mean', 'std']].round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After RobustScaler:\n",
      "       age  income  credit_score  years_employed\n",
      "min  -1.10   -1.28         -1.07           -0.66\n",
      "max   1.00  401.88          1.01            8.33\n",
      "mean -0.01    2.19          0.00            0.29\n",
      "std   0.61   28.36          0.59            0.98\n"
     ]
    }
   ],
   "source": [
    "# Method 3: RobustScaler\n",
    "# Uses median and IQR - resistant to outliers!\n",
    "# Best for: Data with outliers that you want to keep\n",
    "\n",
    "def robust_scale(data):\n",
    "    \"\"\"Scale features using median and interquartile range.\"\"\"\n",
    "    median = np.median(data, axis=0)\n",
    "    q75, q25 = np.percentile(data, [75, 25], axis=0)\n",
    "    iqr = q75 - q25\n",
    "    return (data - median) / iqr, median, iqr\n",
    "\n",
    "df_robust = df_imputed.copy()\n",
    "X = df_robust[numeric_cols].values\n",
    "\n",
    "X_scaled, medians, iqrs = robust_scale(X)\n",
    "df_robust[numeric_cols] = X_scaled\n",
    "\n",
    "print(\"After RobustScaler:\")\n",
    "print(df_robust[numeric_cols].describe().loc[['min', 'max', 'mean', 'std']].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö Splitting Data with scikit-learn\n",
    "\n",
    "**scikit-learn** (sklearn) is Python's most popular ML library. We'll use its `train_test_split` function to properly divide our data.\n",
    "\n",
    "**Why split data?**\n",
    "- **Training set**: Data used to fit/learn the model\n",
    "- **Test set**: Data held back to evaluate performance on unseen examples\n",
    "\n",
    "**`train_test_split(data, test_size, random_state)`**\n",
    "- `test_size`: Fraction of data for testing (e.g., 0.2 = 20%)\n",
    "- `random_state`: Seed for reproducibility (same split each run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Comparison of Scaling Methods\n",
    "\n",
    "| Method | Range | Handles Outliers? | Best For |\n",
    "|--------|-------|-------------------|----------|\n",
    "| StandardScaler | Unbounded | No | Normal distributions |\n",
    "| MinMaxScaler | [0, 1] | No | Neural networks, bounded features |\n",
    "| RobustScaler | Unbounded | Yes | Data with outliers |\n",
    "\n",
    "Look at the min/max values - RobustScaler has the widest range because outliers aren't \"squished\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Building a Reusable Preprocessor Class\n",
    "\n",
    "Now let's use our production-ready Preprocessor class from the scripts folder!\n",
    "\n",
    "### üßí ELI5: Why a Preprocessor Class?\n",
    "\n",
    "> **Imagine you have a magic recipe book...** üìñ\n",
    ">\n",
    "> Every time you make cookies:\n",
    "> 1. Mix ingredients the same way\n",
    "> 2. Use the same oven temperature\n",
    "> 3. Bake for the same time\n",
    ">\n",
    "> A class is like that recipe book - it remembers exactly how to prepare data\n",
    "> so you can repeat it consistently on new data later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor class imported successfully!\n",
      "\n",
      "Docstring:\n",
      "\n",
      "    A reusable data preprocessing pipeline for machine learning.\n",
      "\n",
      "    This class handles common preprocessing tasks:\n",
      "    - Missing value imputation (mean, median, mode, or constant)\n",
      "    - Categorical encoding (one-hot or ordinal/label encoding)\n",
      "    - Feature scaling (standard, minmax, or robust)\n",
      "    - Adding missing value indicators\n",
      "    - Log transformations for skewed features\n",
      "\n",
      "    The preprocessor follows sklearn's fit/transform pattern to prevent\n",
      "    data leakage: fit on training data, transform on all data.\n",
      "\n",
      "    Attributes:\n",
      "        numeric_features: List of numeric column names\n",
      "        categorical_features: List of categorical column names\n",
      "        ordinal_mappings: Dict mapping column names to ordinal encodings\n",
      "        scaling: Scaling method ('standard', 'minmax', 'robust', or None)\n",
      "        impute_strategy: Imputation strategy ('mean', 'median', or 'mode')\n",
      "        add_missing_indicators: Whether to add binary missing indicators\n",
      "        log_features: Features to apply log transformation to\n",
      "\n",
      "    Example:\n",
      "        >>> preprocessor = Preprocessor(\n",
      "        ...     numeric_features=['age', 'income', 'score'],\n",
      "        ...     categorical_features=['gender', 'department'],\n",
      "        ...     scaling='standard'\n",
      "        ... )\n",
      "        >>> train_processed = preprocessor.fit_transform(train_df)\n",
      "        >>> test_processed = preprocessor.transform(test_df)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Import our custom Preprocessor\n",
    "from preprocessing_pipeline import Preprocessor\n",
    "\n",
    "print(\"Preprocessor class imported successfully!\")\n",
    "print(\"\\nDocstring:\")\n",
    "print(Preprocessor.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 800 samples\n",
      "Test set: 200 samples\n"
     ]
    }
   ],
   "source": [
    "# Split data into train/test (in practice, do this before any preprocessing!)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {len(train_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete!\n",
      "\n",
      "Output features: ['age', 'income', 'credit_score', 'years_employed', 'education_encoded', 'employment_type_Full-time', 'employment_type_Part-time', 'employment_type_Self-employed', 'employment_type_Unemployed']\n"
     ]
    }
   ],
   "source": [
    "# Create and fit preprocessor\n",
    "preprocessor = Preprocessor(\n",
    "    numeric_features=['age', 'income', 'credit_score', 'years_employed'],\n",
    "    categorical_features=['education', 'employment_type'],\n",
    "    ordinal_mappings={\n",
    "        'education': {\n",
    "            'High School': 0,\n",
    "            'Bachelor': 1,\n",
    "            'Master': 2,\n",
    "            'PhD': 3\n",
    "        }\n",
    "    },\n",
    "    scaling='standard',\n",
    "    impute_strategy='median'\n",
    ")\n",
    "\n",
    "# Fit on training data only!\n",
    "train_processed = preprocessor.fit_transform(train_df)\n",
    "\n",
    "# Transform test data (using parameters learned from training)\n",
    "test_processed = preprocessor.transform(test_df)\n",
    "\n",
    "print(\"Preprocessing complete!\\n\")\n",
    "print(f\"Output features: {preprocessor.get_feature_names()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data after preprocessing:\n",
      "          age     income  credit_score  years_employed  default  \\\n",
      "29  -0.666457  -0.080887      0.715340       -0.379849        1   \n",
      "535  0.015995  14.099072      1.381789       -0.974596        0   \n",
      "695  1.039673  -0.113645     -1.141657       -0.356711        0   \n",
      "557  0.527834   0.054997      0.902981       -0.119911        0   \n",
      "836  0.755318  -0.104624     -0.132279        0.121997        0   \n",
      "\n",
      "     education_encoded  employment_type_Full-time  employment_type_Part-time  \\\n",
      "29                   2                          1                          0   \n",
      "535                  0                          1                          0   \n",
      "695                  2                          1                          0   \n",
      "557                  2                          1                          0   \n",
      "836                  1                          0                          0   \n",
      "\n",
      "     employment_type_Self-employed  employment_type_Unemployed  \n",
      "29                               0                           0  \n",
      "535                              0                           0  \n",
      "695                              0                           0  \n",
      "557                              0                           0  \n",
      "836                              1                           0  \n"
     ]
    }
   ],
   "source": [
    "# Verify the output\n",
    "print(\"Training data after preprocessing:\")\n",
    "print(train_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data statistics (should be ~0 mean, ~1 std):\n",
      "          age  income  credit_score  years_employed\n",
      "count  800.00  800.00        800.00          800.00\n",
      "mean     0.00   -0.00         -0.00            0.00\n",
      "std      1.00    1.00          1.00            1.00\n",
      "min     -1.80   -0.12         -1.81           -0.99\n",
      "25%     -0.78   -0.09         -0.82           -0.68\n",
      "50%      0.02   -0.08          0.00           -0.29\n",
      "75%      0.87   -0.06          0.84            0.38\n",
      "max      1.67   14.10          1.74            8.26\n"
     ]
    }
   ],
   "source": [
    "# Check that scaling is correct (should be mean~0, std~1 for training data)\n",
    "print(\"Training data statistics (should be ~0 mean, ~1 std):\")\n",
    "print(train_processed[['age', 'income', 'credit_score', 'years_employed']].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Preprocessor saved to ../data/preprocessor.pkl\n",
      "\n",
      "You can load it later with:\n",
      "  with open('preprocessor.pkl', 'rb') as f:\n",
      "      preprocessor = pickle.load(f)\n"
     ]
    }
   ],
   "source": [
    "# Save the preprocessor for later use\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Determine save path relative to notebook\n",
    "try:\n",
    "    save_dir = Path(__vsc_ipynb_file__).parent / '../data'\n",
    "except NameError:\n",
    "    save_dir = Path('../data')\n",
    "\n",
    "save_path = save_dir / 'preprocessor.pkl'\n",
    "\n",
    "try:\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(preprocessor, f)\n",
    "    print(f\"‚úÖ Preprocessor saved to {save_path}\")\n",
    "    print(\"\\nYou can load it later with:\")\n",
    "    print(\"  with open('preprocessor.pkl', 'rb') as f:\")\n",
    "    print(\"      preprocessor = pickle.load(f)\")\n",
    "except IOError as e:\n",
    "    print(f\"‚ö†Ô∏è Could not save preprocessor: {e}\")\n",
    "    print(\"   This is optional - you can continue without saving.\")\n",
    "    print(\"   The preprocessor object is still available in memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Fitting on test data (data leakage!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Fitting on all data before splitting\n",
    "# This causes \"data leakage\" - test data info leaks into training!\n",
    "\n",
    "# all_data_scaled = preprocessor.fit_transform(all_data)  # DON'T DO THIS\n",
    "# train, test = train_test_split(all_data_scaled)         # Test data influenced fit!\n",
    "\n",
    "# ‚úÖ Right: Split first, then fit on train only\n",
    "# train, test = train_test_split(raw_data)\n",
    "# train_processed = preprocessor.fit_transform(train)  # Fit on train\n",
    "# test_processed = preprocessor.transform(test)        # Only transform test\n",
    "\n",
    "print(\"üí° Always split data BEFORE preprocessing!\")\n",
    "print(\"   Fit on training data, transform on both.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Forgetting to handle new categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if test data has a category not in training?\n",
    "\n",
    "# Example: Training had ['A', 'B', 'C'], test has ['A', 'B', 'D']\n",
    "# One-hot encoding will create wrong columns!\n",
    "\n",
    "print(\"‚ö†Ô∏è Our Preprocessor handles this by only using categories from training.\")\n",
    "print(\"   New categories become all-zeros (which may or may not be desired).\")\n",
    "print(\"\")\n",
    "print(\"üí° For production, consider:\")\n",
    "print(\"   - Use an 'Unknown' category\")\n",
    "print(\"   - Map new categories to most similar known category\")\n",
    "print(\"   - Raise an error if unexpected category appears\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Scaling the target variable incorrectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For classification: DON'T scale the target (it's categorical: 0 or 1)\n",
    "# For regression: You CAN scale the target, but must inverse-transform predictions!\n",
    "\n",
    "print(\"üí° In our example, 'default' is binary (0/1) - no scaling needed!\")\n",
    "print(\"\")\n",
    "print(\"   For regression targets:\")\n",
    "print(\"   1. Scale target during training\")\n",
    "print(\"   2. Inverse-scale predictions to get real values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ Loading and exploring datasets with Pandas\n",
    "- ‚úÖ Multiple strategies for handling missing values\n",
    "- ‚úÖ Label vs One-Hot encoding for categorical features\n",
    "- ‚úÖ StandardScaler, MinMaxScaler, and RobustScaler\n",
    "- ‚úÖ Building and using a reusable Preprocessor class\n",
    "- ‚úÖ Avoiding data leakage in preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Build a complete preprocessing pipeline for the Titanic dataset!**\n",
    "\n",
    "1. Download the Titanic dataset from Kaggle or use seaborn's built-in version\n",
    "2. Handle missing values in Age, Cabin, and Embarked\n",
    "3. Engineer new features (e.g., Title from Name, FamilySize)\n",
    "4. Encode categorical variables\n",
    "5. Scale numeric features\n",
    "6. Save your preprocessor parameters for later use\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "titanic = sns.load_dataset('titanic')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- [Scikit-learn Preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "- [Feature Engineering for Machine Learning](https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Memory cleaned up!\n",
      "\n",
      "üéâ Congratulations! You've mastered data preprocessing!\n",
      "   Next up: Lab 1.2.3 - Visualization Dashboard\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "import gc\n",
    "\n",
    "del df, df_imputed, df_encoded, train_df, test_df, train_processed, test_processed\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Memory cleaned up!\")\n",
    "print(\"\\nüéâ Congratulations! You've mastered data preprocessing!\")\n",
    "print(\"   Next up: Lab 1.2.3 - Visualization Dashboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
