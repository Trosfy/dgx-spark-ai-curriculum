{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.2.5: Profiling Exercise\n",
    "\n",
    "**Module:** 1.2 - Python for AI/ML  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Profile Python code using cProfile and line_profiler\n",
    "- [ ] Identify performance bottlenecks\n",
    "- [ ] Optimize code using vectorization\n",
    "- [ ] Achieve 10x+ speedup on real code\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Labs 1.2.1-1.2.4\n",
    "- Knowledge of: NumPy, broadcasting, basic algorithms\n",
    "\n",
    "### Required Packages\n",
    "- Python 3.9+\n",
    "- NumPy >= 1.21\n",
    "- cProfile (stdlib)\n",
    "- tracemalloc (stdlib) or psutil (optional, for memory tracking)\n",
    "\n",
    "### Optional Packages\n",
    "- line_profiler (for line-by-line profiling)\n",
    "  - Note: On DGX Spark (ARM64), use the version from NGC container or install via conda\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**Why profile code?**\n",
    "\n",
    "\"Premature optimization is the root of all evil\" - Donald Knuth\n",
    "\n",
    "But *informed* optimization is essential:\n",
    "- Training a model in 2 hours vs 2 days\n",
    "- Serving 1000 req/s vs 10 req/s\n",
    "- Fitting a model on a laptop vs needing a cluster\n",
    "\n",
    "**The profiling workflow:**\n",
    "1. Write correct code first\n",
    "2. Measure to find bottlenecks\n",
    "3. Optimize only the slow parts\n",
    "4. Verify correctness after optimization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßí ELI5: What is Profiling?\n",
    "\n",
    "> **Imagine you're a detective solving \"The Case of the Slow Code\"...** üîç\n",
    ">\n",
    "> Your program takes 10 minutes to run. Where's the time going?\n",
    ">\n",
    "> **Without profiling:** You guess and optimize random things. Maybe it helps, maybe not.\n",
    ">\n",
    "> **With profiling:** You get a detailed report:\n",
    "> - Function A: 0.1 seconds (1%)\n",
    "> - Function B: 9.5 seconds (95%)  ‚Üê HERE'S YOUR CULPRIT!\n",
    "> - Function C: 0.4 seconds (4%)\n",
    ">\n",
    "> Now you know exactly where to focus!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Environment Setup and Dependency Checks\n",
    "# ============================================================\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Determine the notebook's directory for reliable path resolution\n",
    "try:\n",
    "    notebook_dir = Path(__vsc_ipynb_file__).parent  # VS Code\n",
    "except NameError:\n",
    "    notebook_dir = Path.cwd()  # Fallback\n",
    "\n",
    "# Add scripts directory to path (robust method)\n",
    "scripts_dir = (notebook_dir / '../scripts').resolve()\n",
    "if scripts_dir.exists() and str(scripts_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(scripts_dir))\n",
    "elif not scripts_dir.exists():\n",
    "    scripts_dir = Path('../scripts').resolve()\n",
    "    if scripts_dir.exists() and str(scripts_dir) not in sys.path:\n",
    "        sys.path.insert(0, str(scripts_dir))\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "from functools import wraps\n",
    "\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "# Import our profiling utilities\n",
    "# Note: memory_tracker requires either tracemalloc (stdlib, Python 3.4+) or psutil (pip install psutil)\n",
    "# If neither is available, memory tracking will show a warning but won't fail\n",
    "from profiling_utils import Timer, timeit, compare_implementations, memory_tracker\n",
    "\n",
    "# Check memory tracking capability\n",
    "try:\n",
    "    import tracemalloc\n",
    "    print(\"Memory tracking: tracemalloc available (stdlib)\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        import psutil\n",
    "        print(\"Memory tracking: psutil available\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è Memory tracking limited - install psutil for full support: pip install psutil\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Welcome to the Profiling Exercise! üî¨\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Basic Timing\n",
    "\n",
    "Let's start with simple timing before moving to profiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The slow function we'll optimize\n",
    "def compute_pairwise_distances_slow(points):\n",
    "    \"\"\"\n",
    "    Compute Euclidean distance between all pairs of points.\n",
    "    \n",
    "    This is intentionally slow to demonstrate profiling!\n",
    "    \n",
    "    Args:\n",
    "        points: Array of shape (n_points, n_dims)\n",
    "    \n",
    "    Returns:\n",
    "        Distance matrix of shape (n_points, n_points)\n",
    "    \"\"\"\n",
    "    n = len(points)\n",
    "    distances = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            # Compute Euclidean distance\n",
    "            diff = points[i] - points[j]\n",
    "            squared_diff = diff ** 2\n",
    "            sum_squared = np.sum(squared_diff)\n",
    "            distances[i, j] = np.sqrt(sum_squared)\n",
    "    \n",
    "    return distances\n",
    "\n",
    "# Test with small data\n",
    "np.random.seed(42)\n",
    "test_points = np.random.randn(100, 64).astype(np.float32)\n",
    "\n",
    "print(f\"Test data: {test_points.shape[0]} points, {test_points.shape[1]} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time the slow version\n",
    "with Timer(\"Slow pairwise distances (100 points)\"):\n",
    "    distances = compute_pairwise_distances_slow(test_points)\n",
    "\n",
    "print(f\"Result shape: {distances.shape}\")\n",
    "print(f\"Sample distances: {distances[0, :5].round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Profiling with cProfile\n",
    "\n",
    "Let's see WHERE the time is being spent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile the slow function\n",
    "def profile_function(func, *args, **kwargs):\n",
    "    \"\"\"Profile a function and return statistics.\"\"\"\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "    \n",
    "    result = func(*args, **kwargs)\n",
    "    \n",
    "    profiler.disable()\n",
    "    \n",
    "    # Format output\n",
    "    stream = io.StringIO()\n",
    "    stats = pstats.Stats(profiler, stream=stream)\n",
    "    stats.sort_stats('cumulative')\n",
    "    stats.print_stats(15)  # Top 15 functions\n",
    "    \n",
    "    print(stream.getvalue())\n",
    "    return result\n",
    "\n",
    "print(\"Profiling the slow function...\")\n",
    "print(\"=\"*70)\n",
    "_ = profile_function(compute_pairwise_distances_slow, test_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Reading the Profile Output\n",
    "\n",
    "| Column | Meaning |\n",
    "|--------|--------|\n",
    "| ncalls | Number of times function was called |\n",
    "| tottime | Total time IN this function (excluding sub-calls) |\n",
    "| percall | tottime / ncalls |\n",
    "| cumtime | Total time including sub-calls |\n",
    "\n",
    "**What to look for:**\n",
    "- Functions with high `tottime` are computation bottlenecks\n",
    "- Functions with high `cumtime` but low `tottime` call slow functions\n",
    "- Functions called many times (high `ncalls`) are loop candidates\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Optimizing Step by Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization 1: Reduce function call overhead\n",
    "# Problem: np.sum() called 10,000 times!\n",
    "\n",
    "def compute_pairwise_distances_v2(points):\n",
    "    \"\"\"Version 2: Inline the sum operation.\"\"\"\n",
    "    n = len(points)\n",
    "    distances = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            diff = points[i] - points[j]\n",
    "            # Use @ for dot product instead of sum of squares\n",
    "            distances[i, j] = np.sqrt(diff @ diff)\n",
    "    \n",
    "    return distances\n",
    "\n",
    "# Compare\n",
    "results = compare_implementations(\n",
    "    [compute_pairwise_distances_slow, compute_pairwise_distances_v2],\n",
    "    ['V1: Original', 'V2: Inline sum'],\n",
    "    args=(test_points,),\n",
    "    n_runs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization 2: Vectorize inner loop\n",
    "# Problem: Still iterating 10,000 times\n",
    "\n",
    "def compute_pairwise_distances_v3(points):\n",
    "    \"\"\"Version 3: Vectorize inner loop.\"\"\"\n",
    "    n = len(points)\n",
    "    distances = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Compute all distances from point i at once\n",
    "        diff = points - points[i]  # Broadcasting: (n, d) - (d,) = (n, d)\n",
    "        distances[i] = np.sqrt(np.sum(diff ** 2, axis=1))\n",
    "    \n",
    "    return distances\n",
    "\n",
    "# Compare\n",
    "results = compare_implementations(\n",
    "    [compute_pairwise_distances_slow, compute_pairwise_distances_v2, compute_pairwise_distances_v3],\n",
    "    ['V1: Original', 'V2: Inline sum', 'V3: Vectorize inner'],\n",
    "    args=(test_points,),\n",
    "    n_runs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization 3: Full vectorization (no loops!)\n",
    "\n",
    "def compute_pairwise_distances_v4(points):\n",
    "    \"\"\"Version 4: Fully vectorized using broadcasting.\"\"\"\n",
    "    # points: (n, d)\n",
    "    # points[:, np.newaxis, :]: (n, 1, d)\n",
    "    # points[np.newaxis, :, :]: (1, n, d)\n",
    "    # Difference: (n, n, d)\n",
    "    diff = points[:, np.newaxis, :] - points[np.newaxis, :, :]\n",
    "    return np.sqrt(np.sum(diff ** 2, axis=2))\n",
    "\n",
    "# Compare all versions\n",
    "results = compare_implementations(\n",
    "    [\n",
    "        compute_pairwise_distances_slow,\n",
    "        compute_pairwise_distances_v2,\n",
    "        compute_pairwise_distances_v3,\n",
    "        compute_pairwise_distances_v4\n",
    "    ],\n",
    "    ['V1: Original', 'V2: Inline', 'V3: Inner vec', 'V4: Full vec'],\n",
    "    args=(test_points,),\n",
    "    n_runs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization 4: Even smarter math!\n",
    "# ||a - b||^2 = ||a||^2 + ||b||^2 - 2*a¬∑b\n",
    "\n",
    "def compute_pairwise_distances_v5(points):\n",
    "    \"\"\"Version 5: Use the expansion trick for even better performance.\"\"\"\n",
    "    # ||a||^2 for all points\n",
    "    sq_norms = np.sum(points ** 2, axis=1)\n",
    "    \n",
    "    # ||a - b||^2 = ||a||^2 + ||b||^2 - 2*a¬∑b\n",
    "    # sq_norms[:, np.newaxis] broadcasts to (n, 1)\n",
    "    # sq_norms[np.newaxis, :] broadcasts to (1, n)\n",
    "    # points @ points.T gives all dot products (n, n)\n",
    "    sq_distances = sq_norms[:, np.newaxis] + sq_norms[np.newaxis, :] - 2 * (points @ points.T)\n",
    "    \n",
    "    # Handle numerical errors (small negative values)\n",
    "    sq_distances = np.maximum(sq_distances, 0)\n",
    "    \n",
    "    return np.sqrt(sq_distances)\n",
    "\n",
    "# Final comparison!\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL COMPARISON - All optimization levels\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "results = compare_implementations(\n",
    "    [\n",
    "        compute_pairwise_distances_slow,\n",
    "        compute_pairwise_distances_v3,\n",
    "        compute_pairwise_distances_v4,\n",
    "        compute_pairwise_distances_v5\n",
    "    ],\n",
    "    ['V1: Nested loops', 'V3: Inner vectorized', 'V4: Full broadcasting', 'V5: Math trick'],\n",
    "    args=(test_points,),\n",
    "    n_runs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéâ Amazing! \n",
    "\n",
    "We achieved massive speedups through systematic optimization:\n",
    "1. Profiled to find bottlenecks\n",
    "2. Reduced function call overhead\n",
    "3. Vectorized inner loop\n",
    "4. Fully vectorized with broadcasting\n",
    "5. Used smarter mathematics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Scaling Test\n",
    "\n",
    "Let's see how the optimized version scales to larger data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale test with larger data\n",
    "print(\"Scaling test: How do versions perform with more data?\\n\")\n",
    "\n",
    "sizes = [100, 500, 1000, 2000]\n",
    "\n",
    "for n in sizes:\n",
    "    points = np.random.randn(n, 64).astype(np.float32)\n",
    "    \n",
    "    # Only test fast versions for large n\n",
    "    if n <= 500:\n",
    "        v1_result = timeit(compute_pairwise_distances_slow, args=(points,), n_runs=1, warmup=0)\n",
    "        v1_str = f\"{v1_result.mean_time*1000:.1f} ms\"\n",
    "    else:\n",
    "        v1_str = \"(too slow)\"\n",
    "    \n",
    "    v4_result = timeit(compute_pairwise_distances_v4, args=(points,), n_runs=3)\n",
    "    v5_result = timeit(compute_pairwise_distances_v5, args=(points,), n_runs=3)\n",
    "    \n",
    "    print(f\"n={n:4d}: V1={v1_str:>12s}  V4={v4_result.mean_time*1000:6.1f} ms  V5={v5_result.mean_time*1000:6.1f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Memory Profiling\n",
    "\n",
    "On DGX Spark with 128GB unified memory, memory matters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory comparison\n",
    "n_large = 5000\n",
    "points_large = np.random.randn(n_large, 64).astype(np.float32)\n",
    "\n",
    "print(f\"Input: {n_large} points √ó 64 dims = {points_large.nbytes / 1e6:.1f} MB\")\n",
    "print(f\"Output: {n_large}√ó{n_large} matrix = {(n_large * n_large * 4) / 1e6:.1f} MB\")\n",
    "print()\n",
    "\n",
    "# V4 needs intermediate (n, n, d) array\n",
    "intermediate_v4 = n_large * n_large * 64 * 4  # float32\n",
    "print(f\"V4 intermediate array: {intermediate_v4 / 1e9:.2f} GB\")\n",
    "\n",
    "# V5 only needs (n, n) arrays\n",
    "intermediate_v5 = n_large * n_large * 4 * 2  # Two (n,n) arrays\n",
    "print(f\"V5 intermediate arrays: {intermediate_v5 / 1e6:.1f} MB\")\n",
    "\n",
    "print(f\"\\nüí° V5 uses {intermediate_v4 / intermediate_v5:.0f}x less memory!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with memory tracking\n",
    "print(\"\\nMemory usage during computation:\")\n",
    "\n",
    "# V5 (memory efficient)\n",
    "with memory_tracker(\"V5 (math trick)\"):\n",
    "    result_v5 = compute_pairwise_distances_v5(points_large)\n",
    "del result_v5\n",
    "\n",
    "# V4 - might need to skip if not enough memory\n",
    "if n_large <= 3000:  # Safe size for V4\n",
    "    with memory_tracker(\"V4 (broadcasting)\"):\n",
    "        result_v4 = compute_pairwise_distances_v4(points_large)\n",
    "    del result_v4\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping V4 - would use too much memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Your Turn - Optimize This Function!\n",
    "\n",
    "### ‚úã Exercise: Optimize K-Nearest Neighbors\n",
    "\n",
    "This slow function finds the k nearest neighbors for each point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö NumPy Functions for Efficient Top-K Selection\n",
    "\n",
    "Before the exercise, let's learn two powerful functions for finding the k smallest/largest elements:\n",
    "\n",
    "**`np.argpartition(arr, k)`** - Partially sorts array to find top-k indices\n",
    "- Much faster than full sort: O(n) instead of O(n log n)\n",
    "- Guarantees the k smallest elements are in positions `[:k]` (but not sorted among themselves)\n",
    "\n",
    "**`np.take_along_axis(arr, indices, axis)`** - Select elements using indices from another array\n",
    "- Perfect for gathering values based on argpartition/argsort results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: argpartition vs argsort for top-k selection\n",
    "\n",
    "# Example array\n",
    "arr = np.array([50, 10, 30, 80, 20, 60, 40])\n",
    "k = 3  # Want the 3 smallest elements\n",
    "\n",
    "print(f\"Original array: {arr}\")\n",
    "print(f\"Finding the {k} smallest elements...\\n\")\n",
    "\n",
    "# Method 1: Full sort (O(n log n))\n",
    "sorted_indices_full = np.argsort(arr)\n",
    "top_k_full = arr[sorted_indices_full[:k]]\n",
    "print(f\"Using argsort (full sort):\")\n",
    "print(f\"  All sorted indices: {sorted_indices_full}\")\n",
    "print(f\"  Top-{k} values: {top_k_full}\")\n",
    "\n",
    "# Method 2: Partial sort (O(n)) - FASTER!\n",
    "partitioned_indices = np.argpartition(arr, k)\n",
    "top_k_partial = arr[partitioned_indices[:k]]\n",
    "print(f\"\\nUsing argpartition (partial sort):\")\n",
    "print(f\"  Partitioned indices: {partitioned_indices}\")\n",
    "print(f\"  Top-{k} values (unordered): {top_k_partial}\")\n",
    "\n",
    "# If you need sorted within the k, sort just the small subset\n",
    "within_k_sorted = np.argsort(top_k_partial)\n",
    "top_k_sorted = top_k_partial[within_k_sorted]\n",
    "print(f\"  Top-{k} values (sorted): {top_k_sorted}\")\n",
    "\n",
    "# Using take_along_axis for 2D arrays\n",
    "print(\"\\n--- take_along_axis for 2D arrays ---\")\n",
    "matrix = np.array([[5, 2, 8], [1, 9, 3], [7, 4, 6]])\n",
    "print(f\"Matrix:\\n{matrix}\")\n",
    "\n",
    "# Get indices of sorted values per row\n",
    "sort_idx = np.argsort(matrix, axis=1)\n",
    "print(f\"\\nSort indices per row:\\n{sort_idx}\")\n",
    "\n",
    "# Use take_along_axis to get sorted values\n",
    "sorted_matrix = np.take_along_axis(matrix, sort_idx, axis=1)\n",
    "print(f\"\\nSorted rows using take_along_axis:\\n{sorted_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_slow(query_points, reference_points, k):\n",
    "    \"\"\"\n",
    "    Find k nearest neighbors for each query point.\n",
    "    \n",
    "    INTENTIONALLY SLOW - optimize this!\n",
    "    \n",
    "    Args:\n",
    "        query_points: (n_queries, dims)\n",
    "        reference_points: (n_reference, dims)\n",
    "        k: number of neighbors\n",
    "    \n",
    "    Returns:\n",
    "        indices: (n_queries, k) - indices of k nearest neighbors\n",
    "        distances: (n_queries, k) - distances to k nearest neighbors\n",
    "    \"\"\"\n",
    "    n_queries = len(query_points)\n",
    "    n_ref = len(reference_points)\n",
    "    \n",
    "    all_indices = []\n",
    "    all_distances = []\n",
    "    \n",
    "    for i in range(n_queries):\n",
    "        # Compute distance to all reference points\n",
    "        distances = []\n",
    "        for j in range(n_ref):\n",
    "            diff = query_points[i] - reference_points[j]\n",
    "            dist = np.sqrt(np.sum(diff ** 2))\n",
    "            distances.append(dist)\n",
    "        \n",
    "        # Sort and take top k\n",
    "        distances = np.array(distances)\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        \n",
    "        all_indices.append(sorted_indices[:k])\n",
    "        all_distances.append(distances[sorted_indices[:k]])\n",
    "    \n",
    "    return np.array(all_indices), np.array(all_distances)\n",
    "\n",
    "# Test data\n",
    "np.random.seed(42)\n",
    "queries = np.random.randn(50, 32).astype(np.float32)\n",
    "references = np.random.randn(200, 32).astype(np.float32)\n",
    "k = 5\n",
    "\n",
    "# Time the slow version\n",
    "with Timer(\"KNN slow (50 queries, 200 references)\"):\n",
    "    indices_slow, distances_slow = knn_slow(queries, references, k)\n",
    "\n",
    "print(f\"\\nResult shapes: indices={indices_slow.shape}, distances={distances_slow.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR OPTIMIZED VERSION HERE\n",
    "def knn_fast(query_points, reference_points, k):\n",
    "    \"\"\"\n",
    "    Optimized k-nearest neighbors.\n",
    "    \n",
    "    TODO: Implement this using vectorization!\n",
    "    \n",
    "    Hints:\n",
    "    1. Use the ||a-b||^2 = ||a||^2 + ||b||^2 - 2*a¬∑b trick\n",
    "    2. Compute all pairwise distances at once\n",
    "    3. Use np.argpartition instead of full sort (faster for top-k)\n",
    "    \"\"\"\n",
    "    # TODO: Your implementation here\n",
    "    pass\n",
    "\n",
    "# Uncomment when ready to test:\n",
    "# with Timer(\"KNN fast\"):\n",
    "#     indices_fast, distances_fast = knn_fast(queries, references, k)\n",
    "# \n",
    "# # Verify correctness\n",
    "# print(f\"Indices match: {np.allclose(indices_slow, indices_fast)}\")\n",
    "# print(f\"Distances match: {np.allclose(distances_slow, distances_fast)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Solution (click to reveal)</summary>\n",
    "\n",
    "```python\n",
    "def knn_fast(query_points, reference_points, k):\n",
    "    # Compute squared norms\n",
    "    query_sq = np.sum(query_points ** 2, axis=1, keepdims=True)  # (n_q, 1)\n",
    "    ref_sq = np.sum(reference_points ** 2, axis=1)  # (n_r,)\n",
    "    \n",
    "    # All pairwise squared distances: ||q - r||^2 = ||q||^2 + ||r||^2 - 2*q¬∑r\n",
    "    # query_sq: (n_q, 1), ref_sq: (n_r,) -> broadcasts to (n_q, n_r)\n",
    "    # query_points @ reference_points.T: (n_q, n_r)\n",
    "    sq_distances = query_sq + ref_sq - 2 * (query_points @ reference_points.T)\n",
    "    sq_distances = np.maximum(sq_distances, 0)  # Numerical stability\n",
    "    \n",
    "    # Use argpartition for efficiency (doesn't fully sort)\n",
    "    # This is O(n) instead of O(n log n) for full sort\n",
    "    indices = np.argpartition(sq_distances, k, axis=1)[:, :k]\n",
    "    \n",
    "    # Get the actual distances for these indices\n",
    "    row_indices = np.arange(len(query_points))[:, np.newaxis]\n",
    "    top_k_sq_distances = sq_distances[row_indices, indices]\n",
    "    \n",
    "    # Sort within the k (argpartition doesn't sort)\n",
    "    sorted_within_k = np.argsort(top_k_sq_distances, axis=1)\n",
    "    final_indices = np.take_along_axis(indices, sorted_within_k, axis=1)\n",
    "    final_distances = np.sqrt(np.take_along_axis(top_k_sq_distances, sorted_within_k, axis=1))\n",
    "    \n",
    "    return final_indices, final_distances\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Optimizing without measuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Guessing what's slow\n",
    "# \"I bet this sqrt is slow, let me optimize it\"\n",
    "\n",
    "# ‚úÖ Right: Measure first\n",
    "# Profile, find the ACTUAL bottleneck, then optimize\n",
    "\n",
    "print(\"üí° Always profile before optimizing!\")\n",
    "print(\"   You might be surprised where time actually goes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Breaking correctness while optimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Optimize and forget to check results\n",
    "# def fast_version(x):\n",
    "#     # Some \"clever\" optimization that introduces bugs\n",
    "#     pass\n",
    "\n",
    "# ‚úÖ Right: Always verify against original\n",
    "# assert np.allclose(fast_version(x), slow_version(x))\n",
    "\n",
    "print(\"üí° Always compare optimized results against the original!\")\n",
    "print(\"   Speed means nothing if the answer is wrong.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Over-optimizing small functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a function takes 0.01% of total time, optimizing it 10x\n",
    "# saves only 0.009% - not worth the complexity!\n",
    "\n",
    "# Amdahl's Law:\n",
    "# Speedup = 1 / ((1 - P) + P/S)\n",
    "# P = fraction of time in optimized part\n",
    "# S = speedup factor\n",
    "\n",
    "def amdahl_speedup(fraction, speedup_factor):\n",
    "    \"\"\"Calculate overall speedup using Amdahl's Law.\"\"\"\n",
    "    return 1 / ((1 - fraction) + fraction / speedup_factor)\n",
    "\n",
    "print(\"Amdahl's Law examples:\")\n",
    "print(f\"  Optimize 90% of code 10x ‚Üí {amdahl_speedup(0.9, 10):.1f}x overall speedup\")\n",
    "print(f\"  Optimize 10% of code 10x ‚Üí {amdahl_speedup(0.1, 10):.2f}x overall speedup\")\n",
    "print(f\"\\nüí° Focus on the biggest bottlenecks first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ Timing code with context managers\n",
    "- ‚úÖ Profiling with cProfile to find bottlenecks\n",
    "- ‚úÖ Systematic optimization: measure ‚Üí optimize ‚Üí verify\n",
    "- ‚úÖ Vectorization strategies for massive speedups\n",
    "- ‚úÖ Memory considerations for large computations\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [Python Profilers Documentation](https://docs.python.org/3/library/profile.html)\n",
    "- [High Performance Python (Book)](https://www.oreilly.com/library/view/high-performance-python/9781492055020/)\n",
    "- [NumPy Optimization Tips](https://numpy.org/doc/stable/user/basics.performance.html)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Clean up large arrays\n",
    "del test_points, points_large\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Memory cleaned up!\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéâ Congratulations! You've completed Module 1.2!\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nYou now have strong Python skills for AI/ML:\")\n",
    "print(\"  ‚úÖ NumPy broadcasting and vectorization\")\n",
    "print(\"  ‚úÖ Data preprocessing with Pandas\")\n",
    "print(\"  ‚úÖ Publication-quality visualizations\")\n",
    "print(\"  ‚úÖ Einsum for tensor operations\")\n",
    "print(\"  ‚úÖ Profiling and optimization\")\n",
    "print(\"\\nNext up: Module 1.3 - CUDA Python & GPU Programming!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
