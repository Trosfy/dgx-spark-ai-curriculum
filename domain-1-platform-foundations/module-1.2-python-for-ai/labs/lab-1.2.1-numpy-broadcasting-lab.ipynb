{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 1.2.1: NumPy Broadcasting Lab\n\n**Module:** 1.2 - Python for AI/ML  \n**Time:** 2 hours  \n**Difficulty:** ‚≠ê‚≠ê\n\n---\n\n## üéØ Learning Objectives\n\nBy the end of this notebook, you will:\n- [ ] Understand and apply NumPy broadcasting rules\n- [ ] Implement batch matrix multiplication using broadcasting\n- [ ] Create outer products without explicit loops\n- [ ] Achieve 100x+ speedup over loop-based implementations\n\n---\n\n## üìö Prerequisites\n\n- Completed: Module 1 (DGX Spark Platform Orientation)\n- Knowledge of: Basic Python, array concepts\n\n### Required Packages\n- Python 3.9+\n- NumPy >= 1.21\n\n---\n\n## üåç Real-World Context\n\n**Why does broadcasting matter for AI?**\n\nEvery neural network operation relies on efficient matrix math. When you:\n- Add bias to all neurons in a layer ‚Üí **Broadcasting**\n- Normalize a batch of images ‚Üí **Broadcasting**\n- Compute attention scores in a Transformer ‚Üí **Broadcasting**\n\nThe difference between loop-based and broadcasted code can mean:\n- Training in 2 hours vs. 2 weeks\n- Running on your laptop vs. needing a cluster\n\n**Real example:** GPT-style models compute attention for millions of token pairs simultaneously using broadcasting. Without it, language models would be impossibly slow.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßí ELI5: What is Broadcasting?\n",
    "\n",
    "> **Imagine you're baking cookies...** üç™\n",
    ">\n",
    "> You have a recipe that says \"add 1 teaspoon of vanilla to each cookie.\"\n",
    "> \n",
    "> You don't write \"1 tsp vanilla\" on a separate card for each of your 100 cookies.\n",
    "> Instead, you have ONE instruction that automatically applies to ALL cookies.\n",
    ">\n",
    "> That's broadcasting! NumPy takes a small array and automatically \"stretches\" \n",
    "> it to match a larger array, without actually copying the data.\n",
    ">\n",
    "> **In AI terms:** When you add a bias vector of shape `(128,)` to a batch of \n",
    "> 32 samples each with 128 features `(32, 128)`, NumPy broadcasts the bias \n",
    "> to add the same values to each row automatically.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Broadcasting Fundamentals\n",
    "\n",
    "### The Broadcasting Rules\n",
    "\n",
    "NumPy compares shapes **from right to left**. Two dimensions are compatible if:\n",
    "1. They are equal, OR\n",
    "2. One of them is 1\n",
    "\n",
    "```\n",
    "Array A:      (8, 1, 6, 1)\n",
    "Array B:         (7, 1, 5)\n",
    "Result:       (8, 7, 6, 5)  ‚úÖ Compatible!\n",
    "```\n",
    "\n",
    "Let's see this in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Check NumPy version\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Welcome to the Broadcasting Lab! üöÄ\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Adding a scalar to an array\n",
    "# The scalar is broadcast to match the array shape\n",
    "\n",
    "arr = np.array([1, 2, 3, 4, 5])\n",
    "scalar = 10\n",
    "\n",
    "result = arr + scalar\n",
    "\n",
    "print(\"Array:\", arr)\n",
    "print(\"Scalar:\", scalar)\n",
    "print(\"Result:\", result)\n",
    "print(f\"\\nWhat happened: {scalar} was 'stretched' to [10, 10, 10, 10, 10]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Adding a row vector to a matrix\n",
    "# This is EXACTLY what happens when adding bias in neural networks!\n",
    "\n",
    "# Simulating: batch of 4 samples, 3 features each\n",
    "batch = np.array([\n",
    "    [1.0, 2.0, 3.0],    # Sample 1\n",
    "    [4.0, 5.0, 6.0],    # Sample 2\n",
    "    [7.0, 8.0, 9.0],    # Sample 3\n",
    "    [10.0, 11.0, 12.0]  # Sample 4\n",
    "])\n",
    "\n",
    "# Bias: one value per feature\n",
    "bias = np.array([100, 200, 300])\n",
    "\n",
    "print(f\"Batch shape: {batch.shape}\")\n",
    "print(f\"Bias shape: {bias.shape}\")\n",
    "\n",
    "result = batch + bias\n",
    "\n",
    "print(f\"Result shape: {result.shape}\")\n",
    "print(\"\\nOriginal batch:\")\n",
    "print(batch)\n",
    "print(\"\\nAfter adding bias:\")\n",
    "print(result)\n",
    "print(\"\\n‚ú® Each column got its own bias value added to all rows!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "```\n",
    "batch shape: (4, 3)\n",
    "bias shape:     (3,)\n",
    "              -----\n",
    "Result:      (4, 3)  ‚Üê bias was stretched to (4, 3) automatically!\n",
    "```\n",
    "\n",
    "NumPy \"virtually\" repeated the bias for each row. No memory was actually copied!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Column operations using reshape\n",
    "# What if we want to add different values to each ROW instead of each column?\n",
    "\n",
    "# Per-sample scaling factors\n",
    "row_scales = np.array([1, 2, 3, 4]).reshape(-1, 1)  # Shape: (4, 1)\n",
    "\n",
    "print(f\"Batch shape: {batch.shape}\")\n",
    "print(f\"Row scales shape: {row_scales.shape}\")\n",
    "\n",
    "result = batch * row_scales\n",
    "\n",
    "print(f\"\\nRow scales:\\n{row_scales}\")\n",
    "print(f\"\\nResult (each row multiplied by its scale):\\n{result}\")\n",
    "print(\"\\n‚ú® Row 1 √ó 1, Row 2 √ó 2, Row 3 √ó 3, Row 4 √ó 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself: Exercise 1\n",
    "\n",
    "**Task:** Normalize each row to have zero mean (subtract the row mean from each element).\n",
    "\n",
    "Given a matrix of shape `(5, 4)`, compute the mean of each row and subtract it.\n",
    "\n",
    "```python\n",
    "# Your code here\n",
    "data = np.random.randn(5, 4)\n",
    "# Step 1: Compute mean of each row (should be shape (5,) or (5, 1))\n",
    "# Step 2: Subtract from data using broadcasting\n",
    "```\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "Use `np.mean(data, axis=1, keepdims=True)` to get a shape of `(5, 1)` which broadcasts correctly!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE - Exercise 1\n",
    "np.random.seed(42)\n",
    "data = np.random.randn(5, 4)\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(f\"\\nRow means before: {data.mean(axis=1)}\")\n",
    "\n",
    "# TODO: Normalize each row to zero mean\n",
    "# normalized = ?\n",
    "\n",
    "# Uncomment to verify:\n",
    "# print(f\"Row means after: {normalized.mean(axis=1)}\")  # Should be ~0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The Speed of Vectorization\n",
    "\n",
    "Now let's see WHY broadcasting matters: **speed**.\n",
    "\n",
    "### üßí ELI5: Why Loops are Slow\n",
    "\n",
    "> **Imagine two ways to fill a swimming pool...** üèä\n",
    ">\n",
    "> **Loop approach:** Walk to the pool with a cup, pour water, walk back, refill... repeat 1 million times.\n",
    ">\n",
    "> **Vectorized approach:** Turn on a fire hose and fill it all at once.\n",
    ">\n",
    "> Python loops are like the cup method - each iteration has overhead.\n",
    "> NumPy operations are like the fire hose - optimized C code processes everything in bulk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_function(func, *args, n_runs=5):\n",
    "    \"\"\"Time a function and return average execution time.\"\"\"\n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.perf_counter()\n",
    "        result = func(*args)\n",
    "        times.append(time.perf_counter() - start)\n",
    "    return np.mean(times), result\n",
    "\n",
    "# Test data\n",
    "N = 1000\n",
    "A = np.random.randn(N, N).astype(np.float32)\n",
    "B = np.random.randn(N, N).astype(np.float32)\n",
    "\n",
    "print(f\"Matrix size: {N}x{N} = {N*N:,} elements\")\n",
    "print(f\"Memory per matrix: {A.nbytes / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLOW: Element-wise addition with nested loops\n",
    "def add_with_loops(A, B):\n",
    "    \"\"\"Add two matrices using nested Python loops. (DON'T DO THIS!)\"\"\"\n",
    "    result = np.zeros_like(A)\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(A.shape[1]):\n",
    "            result[i, j] = A[i, j] + B[i, j]\n",
    "    return result\n",
    "\n",
    "# FAST: Vectorized addition\n",
    "def add_vectorized(A, B):\n",
    "    \"\"\"Add two matrices using NumPy vectorization.\"\"\"\n",
    "    return A + B\n",
    "\n",
    "# Let's use smaller matrices for the loop version (it's SLOW)\n",
    "N_small = 200\n",
    "A_small = A[:N_small, :N_small]\n",
    "B_small = B[:N_small, :N_small]\n",
    "\n",
    "print(f\"Testing with {N_small}x{N_small} matrices...\\n\")\n",
    "\n",
    "loop_time, loop_result = time_function(add_with_loops, A_small, B_small)\n",
    "vec_time, vec_result = time_function(add_vectorized, A_small, B_small)\n",
    "\n",
    "print(f\"üê¢ Loop version:       {loop_time*1000:.2f} ms\")\n",
    "print(f\"üöÄ Vectorized version: {vec_time*1000:.4f} ms\")\n",
    "print(f\"\\n‚ö° Speedup: {loop_time/vec_time:.0f}x faster!\")\n",
    "print(f\"\\n‚úÖ Results match: {np.allclose(loop_result, vec_result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéâ Whoa! \n",
    "\n",
    "That's a massive speedup! Now imagine this difference when training a neural network with millions of operations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do matrix multiplication - even more dramatic!\n",
    "\n",
    "def matmul_with_loops(A, B):\n",
    "    \"\"\"Matrix multiply using triple nested loops. (NEVER DO THIS!)\"\"\"\n",
    "    M, K = A.shape\n",
    "    K2, N = B.shape\n",
    "    assert K == K2, \"Dimensions don't match!\"\n",
    "    \n",
    "    result = np.zeros((M, N), dtype=A.dtype)\n",
    "    for i in range(M):\n",
    "        for j in range(N):\n",
    "            for k in range(K):\n",
    "                result[i, j] += A[i, k] * B[k, j]\n",
    "    return result\n",
    "\n",
    "def matmul_vectorized(A, B):\n",
    "    \"\"\"Matrix multiply using NumPy.\"\"\"\n",
    "    return A @ B  # or np.matmul(A, B) or np.dot(A, B)\n",
    "\n",
    "# Use tiny matrices for loop version\n",
    "N_tiny = 50\n",
    "A_tiny = A[:N_tiny, :N_tiny]\n",
    "B_tiny = B[:N_tiny, :N_tiny]\n",
    "\n",
    "print(f\"Testing matrix multiplication with {N_tiny}x{N_tiny} matrices...\\n\")\n",
    "\n",
    "loop_time, loop_result = time_function(matmul_with_loops, A_tiny, B_tiny, n_runs=1)\n",
    "vec_time, vec_result = time_function(matmul_vectorized, A_tiny, B_tiny)\n",
    "\n",
    "print(f\"üê¢ Triple loop:  {loop_time*1000:.2f} ms\")\n",
    "print(f\"üöÄ NumPy @ :     {vec_time*1000:.4f} ms\")\n",
    "print(f\"\\n‚ö° Speedup: {loop_time/vec_time:.0f}x faster!\")\n",
    "print(f\"\\n‚úÖ Results match: {np.allclose(loop_result, vec_result, rtol=1e-4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's see NumPy @ with full-size matrices\n",
    "print(f\"Full {N}x{N} matrix multiplication...\\n\")\n",
    "\n",
    "vec_time_full, _ = time_function(matmul_vectorized, A, B)\n",
    "print(f\"üöÄ NumPy @ : {vec_time_full*1000:.2f} ms for {N}x{N} matrices\")\n",
    "print(f\"\\nLoop version would take ~{(loop_time * (N/N_tiny)**3):.0f} seconds! üò±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Batch Matrix Multiplication\n",
    "\n",
    "In deep learning, we rarely work with single matrices. We work with **batches** - multiple samples processed simultaneously.\n",
    "\n",
    "### üßí ELI5: Batch Operations\n",
    "\n",
    "> **Imagine you're a teacher grading exams...** üìù\n",
    ">\n",
    "> You have 32 students, each with 10 questions.\n",
    "> \n",
    "> **Without batching:** Grade student 1's all questions, then student 2's, etc.\n",
    ">\n",
    "> **With batching:** Grade question 1 for ALL students at once, then question 2, etc.\n",
    ">\n",
    "> The second way is faster because you get into a rhythm with each question type!\n",
    ">\n",
    "> **In AI terms:** Processing a batch of 32 images together is more efficient than \n",
    "> processing them one at a time, because the GPU can parallelize the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch matrix multiplication example\n",
    "# Shape: (batch_size, rows, cols)\n",
    "\n",
    "batch_size = 32\n",
    "M, K, N = 64, 128, 64  # Matrix dimensions\n",
    "\n",
    "# Batch of A matrices: (32, 64, 128)\n",
    "A_batch = np.random.randn(batch_size, M, K).astype(np.float32)\n",
    "\n",
    "# Batch of B matrices: (32, 128, 64)\n",
    "B_batch = np.random.randn(batch_size, K, N).astype(np.float32)\n",
    "\n",
    "print(f\"A_batch shape: {A_batch.shape}\")\n",
    "print(f\"B_batch shape: {B_batch.shape}\")\n",
    "\n",
    "# Method 1: Loop over batch (slow)\n",
    "def batch_matmul_loop(A, B):\n",
    "    results = []\n",
    "    for i in range(A.shape[0]):\n",
    "        results.append(A[i] @ B[i])\n",
    "    return np.stack(results)\n",
    "\n",
    "# Method 2: Single broadcasted operation (fast!)\n",
    "def batch_matmul_broadcast(A, B):\n",
    "    return A @ B  # NumPy handles batch dimension automatically!\n",
    "\n",
    "loop_time, loop_result = time_function(batch_matmul_loop, A_batch, B_batch)\n",
    "broadcast_time, broadcast_result = time_function(batch_matmul_broadcast, A_batch, B_batch)\n",
    "\n",
    "print(f\"\\nResult shape: {loop_result.shape}\")\n",
    "print(f\"\\nüê¢ Loop over batch:    {loop_time*1000:.2f} ms\")\n",
    "print(f\"üöÄ Broadcast matmul:   {broadcast_time*1000:.2f} ms\")\n",
    "print(f\"\\n‚ö° Speedup: {loop_time/broadcast_time:.1f}x faster!\")\n",
    "print(f\"‚úÖ Results match: {np.allclose(loop_result, broadcast_result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "The `@` operator (and `np.matmul`) automatically handles batch dimensions!\n",
    "\n",
    "```\n",
    "A_batch: (32, 64, 128)  ‚Üí  32 matrices of size 64√ó128\n",
    "B_batch: (32, 128, 64)  ‚Üí  32 matrices of size 128√ó64\n",
    "Result:  (32, 64, 64)   ‚Üí  32 matrices of size 64√ó64\n",
    "```\n",
    "\n",
    "NumPy matched up corresponding matrices and multiplied them all at once!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Outer Products\n",
    "\n",
    "An **outer product** creates a matrix from two vectors by multiplying each element of the first with every element of the second.\n",
    "\n",
    "### üßí ELI5: Outer Product\n",
    "\n",
    "> **Imagine a multiplication table...** ‚úñÔ∏è\n",
    ">\n",
    "> Row headers: [1, 2, 3]\n",
    "> Column headers: [4, 5]\n",
    ">\n",
    "> The table shows every possible product:\n",
    "> ```\n",
    ">     4   5\n",
    "> 1   4   5\n",
    "> 2   8  10\n",
    "> 3  12  15\n",
    "> ```\n",
    ">\n",
    "> That's an outer product! Vector of 3 √ó Vector of 2 = Matrix of 3√ó2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple outer product example\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5])\n",
    "\n",
    "# Method 1: Using np.outer\n",
    "outer1 = np.outer(a, b)\n",
    "\n",
    "# Method 2: Using broadcasting with reshape\n",
    "outer2 = a[:, np.newaxis] * b[np.newaxis, :]  # (3,1) * (1,2) = (3,2)\n",
    "# Or equivalently:\n",
    "outer3 = a.reshape(-1, 1) * b.reshape(1, -1)\n",
    "\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"\\nOuter product:\")\n",
    "print(outer1)\n",
    "print(f\"\\nAll methods equal: {np.allclose(outer1, outer2) and np.allclose(outer1, outer3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why outer products matter: Computing distance matrices\n",
    "# Given N points, compute distance between every pair\n",
    "\n",
    "N_points = 1000\n",
    "D = 128  # Dimensionality (like embedding dimension)\n",
    "\n",
    "# Random points (like word embeddings)\n",
    "points = np.random.randn(N_points, D).astype(np.float32)\n",
    "\n",
    "# Slow: Double loop\n",
    "def distances_loop(points):\n",
    "    N = len(points)\n",
    "    dists = np.zeros((N, N), dtype=np.float32)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            dists[i, j] = np.sqrt(np.sum((points[i] - points[j])**2))\n",
    "    return dists\n",
    "\n",
    "# Fast: Broadcasting magic!\n",
    "def distances_broadcast(points):\n",
    "    # points: (N, D)\n",
    "    # points[:, np.newaxis, :]: (N, 1, D)\n",
    "    # points[np.newaxis, :, :]: (1, N, D)\n",
    "    # Difference: (N, N, D) - every pair's difference vector\n",
    "    diff = points[:, np.newaxis, :] - points[np.newaxis, :, :]\n",
    "    return np.sqrt(np.sum(diff**2, axis=2))\n",
    "\n",
    "# Test on smaller subset for loop version\n",
    "N_test = 100\n",
    "points_test = points[:N_test]\n",
    "\n",
    "print(f\"Computing {N_test}√ó{N_test} distance matrix...\\n\")\n",
    "\n",
    "loop_time, loop_result = time_function(distances_loop, points_test, n_runs=1)\n",
    "broadcast_time, broadcast_result = time_function(distances_broadcast, points_test)\n",
    "\n",
    "print(f\"üê¢ Double loop:    {loop_time*1000:.1f} ms\")\n",
    "print(f\"üöÄ Broadcasting:   {broadcast_time*1000:.2f} ms\")\n",
    "print(f\"\\n‚ö° Speedup: {loop_time/broadcast_time:.0f}x faster!\")\n",
    "print(f\"‚úÖ Results match: {np.allclose(loop_result, broadcast_result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full scale test with broadcasting only\n",
    "print(f\"\\nFull {N_points}√ó{N_points} distance matrix with broadcasting...\")\n",
    "full_time, full_result = time_function(distances_broadcast, points)\n",
    "print(f\"üöÄ Time: {full_time*1000:.1f} ms\")\n",
    "print(f\"   Result shape: {full_result.shape}\")\n",
    "print(f\"   Memory used: {full_result.nbytes / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself: Exercise 2\n",
    "\n",
    "**Task:** Implement cosine similarity between all pairs of vectors using broadcasting.\n",
    "\n",
    "Cosine similarity formula:\n",
    "$$\\text{cosine\\_sim}(a, b) = \\frac{a \\cdot b}{\\|a\\| \\cdot \\|b\\|}$$\n",
    "\n",
    "Given `embeddings` of shape `(100, 64)`, compute a `(100, 100)` similarity matrix.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "1. First normalize each embedding to unit length\n",
    "2. Then cosine similarity is just the dot product!\n",
    "3. Use `embeddings @ embeddings.T` after normalization\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE - Exercise 2\n",
    "np.random.seed(42)\n",
    "embeddings = np.random.randn(100, 64).astype(np.float32)\n",
    "\n",
    "# TODO: Compute cosine similarity matrix\n",
    "# Step 1: Compute norm of each embedding (shape: (100,) or (100, 1))\n",
    "# Step 2: Normalize embeddings\n",
    "# Step 3: Compute similarity matrix using @ operator\n",
    "\n",
    "# similarity_matrix = ?\n",
    "\n",
    "# Uncomment to verify:\n",
    "# print(f\"Shape: {similarity_matrix.shape}\")  # Should be (100, 100)\n",
    "# print(f\"Diagonal (self-similarity): {similarity_matrix.diagonal()[:5]}\")  # Should be ~1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Advanced Broadcasting Patterns\n",
    "\n",
    "Let's look at some patterns commonly used in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 1: Softmax with numerical stability\n",
    "# Used in attention mechanisms, classification outputs\n",
    "\n",
    "def softmax_naive(x):\n",
    "    \"\"\"Naive softmax - can overflow with large values!\"\"\"\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / exp_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "def softmax_stable(x):\n",
    "    \"\"\"Stable softmax using max subtraction trick.\"\"\"\n",
    "    # Subtract max for numerical stability (broadcasting!)\n",
    "    x_shifted = x - x.max(axis=-1, keepdims=True)\n",
    "    exp_x = np.exp(x_shifted)\n",
    "    return exp_x / exp_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "# Test with batch of logits\n",
    "batch_logits = np.random.randn(8, 10)  # 8 samples, 10 classes\n",
    "\n",
    "probs = softmax_stable(batch_logits)\n",
    "print(f\"Input shape: {batch_logits.shape}\")\n",
    "print(f\"Output shape: {probs.shape}\")\n",
    "print(f\"\\nProbabilities sum to 1? {np.allclose(probs.sum(axis=1), 1.0)}\")\n",
    "print(f\"Sample output: {probs[0].round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 2: Batch Normalization\n",
    "# Normalizes features across the batch dimension\n",
    "\n",
    "def batch_normalize(x, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Batch normalization.\n",
    "    x: (batch_size, features)\n",
    "    Returns normalized x with mean=0, std=1 per feature\n",
    "    \"\"\"\n",
    "    mean = x.mean(axis=0, keepdims=True)  # (1, features)\n",
    "    std = x.std(axis=0, keepdims=True)    # (1, features)\n",
    "    return (x - mean) / (std + eps)\n",
    "\n",
    "# Test\n",
    "batch = np.random.randn(32, 128) * 5 + 10  # Mean ~10, std ~5\n",
    "normalized = batch_normalize(batch)\n",
    "\n",
    "print(f\"Before normalization:\")\n",
    "print(f\"  Mean per feature: {batch.mean(axis=0)[:5].round(2)}\")\n",
    "print(f\"  Std per feature: {batch.std(axis=0)[:5].round(2)}\")\n",
    "\n",
    "print(f\"\\nAfter normalization:\")\n",
    "print(f\"  Mean per feature: {normalized.mean(axis=0)[:5].round(6)}\")\n",
    "print(f\"  Std per feature: {normalized.std(axis=0)[:5].round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 3: One-hot encoding using broadcasting\n",
    "\n",
    "def one_hot(labels, num_classes):\n",
    "    \"\"\"Convert labels to one-hot vectors using broadcasting.\"\"\"\n",
    "    # labels: (batch_size,) with values 0 to num_classes-1\n",
    "    # Create class indices: [0, 1, 2, ..., num_classes-1]\n",
    "    classes = np.arange(num_classes)\n",
    "    # Compare: (batch_size, 1) == (num_classes,) broadcasts to (batch_size, num_classes)\n",
    "    return (labels[:, np.newaxis] == classes).astype(np.float32)\n",
    "\n",
    "# Test\n",
    "labels = np.array([0, 2, 1, 3, 1])\n",
    "one_hot_labels = one_hot(labels, num_classes=4)\n",
    "\n",
    "print(\"Labels:\", labels)\n",
    "print(\"\\nOne-hot encoded:\")\n",
    "print(one_hot_labels)\n",
    "print(\"\\n‚ú® No loops needed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Memory Considerations on DGX Spark\n",
    "\n",
    "With 128GB of unified memory, you can work with large arrays. But it's still important to be memory-aware!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory efficiency comparison\n",
    "\n",
    "def check_memory(arr, name):\n",
    "    \"\"\"Print memory usage of an array.\"\"\"\n",
    "    mb = arr.nbytes / 1e6\n",
    "    print(f\"{name}: {arr.shape} {arr.dtype} = {mb:.1f} MB\")\n",
    "\n",
    "# Float64 (default) vs Float32 vs Float16\n",
    "N = 10000\n",
    "D = 1024\n",
    "\n",
    "arr_f64 = np.random.randn(N, D)  # Default float64\n",
    "arr_f32 = arr_f64.astype(np.float32)\n",
    "arr_f16 = arr_f64.astype(np.float16)\n",
    "\n",
    "check_memory(arr_f64, \"float64\")\n",
    "check_memory(arr_f32, \"float32\")\n",
    "check_memory(arr_f16, \"float16\")\n",
    "\n",
    "print(f\"\\nüí° Using float32 instead of float64 halves memory usage!\")\n",
    "print(f\"   Most ML models work fine with float32 (or even float16 for inference).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoiding unnecessary copies\n",
    "\n",
    "a = np.random.randn(1000, 1000).astype(np.float32)\n",
    "\n",
    "# BAD: Creates a copy\n",
    "b = a + 0  # New array allocated\n",
    "print(f\"a + 0 creates copy? {a is not b}\")\n",
    "\n",
    "# GOOD: View (no copy)\n",
    "c = a.reshape(1000000)  # Same data, different view\n",
    "print(f\"reshape creates copy? {not np.shares_memory(a, c)}\")\n",
    "\n",
    "# GOOD: In-place operations\n",
    "a_copy = a.copy()\n",
    "a_copy += 1  # In-place, no new allocation\n",
    "print(f\"\\nüí° Use in-place operations (+=, *=, etc.) when possible!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if array is contiguous (important for performance)\n",
    "\n",
    "a = np.random.randn(100, 100).astype(np.float32)\n",
    "\n",
    "print(\"Original array:\")\n",
    "print(f\"  C-contiguous (row-major): {a.flags['C_CONTIGUOUS']}\")\n",
    "print(f\"  F-contiguous (col-major): {a.flags['F_CONTIGUOUS']}\")\n",
    "\n",
    "# Transpose creates a view with different stride\n",
    "a_T = a.T\n",
    "print(\"\\nTransposed array (view):\")\n",
    "print(f\"  C-contiguous: {a_T.flags['C_CONTIGUOUS']}\")\n",
    "print(f\"  F-contiguous: {a_T.flags['F_CONTIGUOUS']}\")\n",
    "\n",
    "# Make it contiguous again\n",
    "a_T_contig = np.ascontiguousarray(a_T)\n",
    "print(\"\\nAfter np.ascontiguousarray:\")\n",
    "print(f\"  C-contiguous: {a_T_contig.flags['C_CONTIGUOUS']}\")\n",
    "\n",
    "print(\"\\nüí° NumPy operations are fastest on C-contiguous arrays!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Broadcasting dimension mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Trying to add vectors of different sizes\n",
    "a = np.array([1, 2, 3])      # shape (3,)\n",
    "b = np.array([1, 2, 3, 4])   # shape (4,)\n",
    "\n",
    "try:\n",
    "    result = a + b  # This will fail!\n",
    "except ValueError as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# ‚úÖ Right: Use compatible shapes\n",
    "a = np.array([1, 2, 3])      # shape (3,)\n",
    "b = np.array([[1], [2]])     # shape (2, 1)\n",
    "result = a + b               # shape (2, 3)\n",
    "print(f\"\\n‚úÖ Compatible broadcast: {a.shape} + {b.shape} = {result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Forgetting keepdims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(4, 5)\n",
    "\n",
    "# ‚ùå Wrong: Loses dimension, can't broadcast back\n",
    "mean_wrong = x.mean(axis=1)  # shape (4,)\n",
    "print(f\"‚ùå Without keepdims: {x.shape} - {mean_wrong.shape} needs reshape\")\n",
    "\n",
    "# ‚úÖ Right: Keeps dimension for broadcasting\n",
    "mean_right = x.mean(axis=1, keepdims=True)  # shape (4, 1)\n",
    "print(f\"‚úÖ With keepdims: {x.shape} - {mean_right.shape} broadcasts correctly\")\n",
    "\n",
    "# The difference in action:\n",
    "normalized = x - mean_right  # Works perfectly!\n",
    "print(f\"\\nNormalized shape: {normalized.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Modifying slices unintentionally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Slices are views, not copies!\n",
    "original = np.array([1, 2, 3, 4, 5])\n",
    "slice_view = original[1:4]\n",
    "slice_view[0] = 999  # This modifies original!\n",
    "print(f\"‚ùå Original modified: {original}\")\n",
    "\n",
    "# ‚úÖ Right: Explicitly copy if you need independence\n",
    "original = np.array([1, 2, 3, 4, 5])\n",
    "slice_copy = original[1:4].copy()\n",
    "slice_copy[0] = 999  # Original is safe\n",
    "print(f\"‚úÖ Original preserved: {original}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ Broadcasting rules: shapes are compared right-to-left\n",
    "- ‚úÖ Vectorization is 100x+ faster than Python loops\n",
    "- ‚úÖ Batch matrix multiplication with the `@` operator\n",
    "- ‚úÖ Outer products using broadcasting\n",
    "- ‚úÖ Common ML patterns: softmax, batch norm, one-hot\n",
    "- ‚úÖ Memory efficiency with dtypes and contiguity\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Implement a mini neural network forward pass using only NumPy broadcasting!**\n",
    "\n",
    "Create a 2-layer network:\n",
    "1. Input: (batch_size, 784) - flattened MNIST images\n",
    "2. Hidden: (784, 256) weights + (256,) bias with ReLU activation\n",
    "3. Output: (256, 10) weights + (10,) bias with softmax\n",
    "\n",
    "Use broadcasting for all operations - no loops allowed!\n",
    "\n",
    "```python\n",
    "def forward(x, w1, b1, w2, b2):\n",
    "    # TODO: Implement using broadcasting\n",
    "    # hidden = relu(x @ w1 + b1)\n",
    "    # output = softmax(hidden @ w2 + b2)\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE\n",
    "np.random.seed(42)\n",
    "\n",
    "# Start with random weights\n",
    "batch_size = 32\n",
    "x = np.random.randn(batch_size, 784).astype(np.float32)\n",
    "w1 = np.random.randn(784, 256).astype(np.float32) * 0.01\n",
    "b1 = np.zeros(256, dtype=np.float32)\n",
    "w2 = np.random.randn(256, 10).astype(np.float32) * 0.01\n",
    "b2 = np.zeros(10, dtype=np.float32)\n",
    "\n",
    "# TODO: Implement forward pass\n",
    "# probs = forward(x, w1, b1, w2, b2)\n",
    "# print(f\"Output shape: {probs.shape}\")  # Should be (32, 10)\n",
    "# print(f\"Probabilities sum: {probs.sum(axis=1)}\")  # Should be all 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [NumPy Broadcasting Documentation](https://numpy.org/doc/stable/user/basics.broadcasting.html)\n",
    "- [Array Broadcasting in NumPy (Stanford CS231n)](https://cs231n.github.io/python-numpy-tutorial/#numpy-broadcasting)\n",
    "- [Why Vectorization is Faster](https://realpython.com/numpy-array-programming/)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up large arrays\n",
    "import gc\n",
    "\n",
    "# Delete large arrays we created\n",
    "del A, B, A_batch, B_batch, points\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Memory cleaned up!\")\n",
    "print(\"\\nüéâ Congratulations! You've completed the NumPy Broadcasting Lab!\")\n",
    "print(\"   Next up: Lab 1.2.2 - Dataset Preprocessing Pipeline\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}