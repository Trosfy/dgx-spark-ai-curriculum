{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.2.5: Profiling Exercise - SOLUTIONS\n",
    "\n",
    "**Module:** 1.2 - Python for AI/ML  \n",
    "\n",
    "This notebook contains solutions to all exercises from the Profiling Exercise Lab.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"Solutions Notebook for Profiling Exercise\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise: Optimized K-Nearest Neighbors\n",
    "\n",
    "**Task:** Optimize the slow KNN implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original slow version (for reference)\n",
    "def knn_slow(query_points, reference_points, k):\n",
    "    \"\"\"Slow KNN with nested loops.\"\"\"\n",
    "    n_queries = len(query_points)\n",
    "    n_ref = len(reference_points)\n",
    "    \n",
    "    all_indices = []\n",
    "    all_distances = []\n",
    "    \n",
    "    for i in range(n_queries):\n",
    "        distances = []\n",
    "        for j in range(n_ref):\n",
    "            diff = query_points[i] - reference_points[j]\n",
    "            dist = np.sqrt(np.sum(diff ** 2))\n",
    "            distances.append(dist)\n",
    "        \n",
    "        distances = np.array(distances)\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        \n",
    "        all_indices.append(sorted_indices[:k])\n",
    "        all_distances.append(distances[sorted_indices[:k]])\n",
    "    \n",
    "    return np.array(all_indices), np.array(all_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Optimized KNN\n",
    "def knn_fast(query_points, reference_points, k):\n",
    "    \"\"\"\n",
    "    Optimized k-nearest neighbors using vectorization.\n",
    "    \n",
    "    Key optimizations:\n",
    "    1. Use ||a-b||^2 = ||a||^2 + ||b||^2 - 2*aÂ·b trick\n",
    "    2. Compute all pairwise distances at once\n",
    "    3. Use np.argpartition instead of full sort (O(n) vs O(n log n))\n",
    "    \n",
    "    Args:\n",
    "        query_points: (n_queries, dims)\n",
    "        reference_points: (n_reference, dims)\n",
    "        k: number of neighbors\n",
    "    \n",
    "    Returns:\n",
    "        indices: (n_queries, k) - indices of k nearest neighbors\n",
    "        distances: (n_queries, k) - distances to k nearest neighbors\n",
    "    \"\"\"\n",
    "    # Step 1: Compute squared norms\n",
    "    query_sq = np.sum(query_points ** 2, axis=1, keepdims=True)  # (n_q, 1)\n",
    "    ref_sq = np.sum(reference_points ** 2, axis=1)  # (n_r,)\n",
    "    \n",
    "    # Step 2: Compute all pairwise squared distances\n",
    "    # ||q - r||^2 = ||q||^2 + ||r||^2 - 2*qÂ·r\n",
    "    # Broadcasting: (n_q, 1) + (n_r,) - 2 * (n_q, n_r) -> (n_q, n_r)\n",
    "    sq_distances = query_sq + ref_sq - 2 * (query_points @ reference_points.T)\n",
    "    \n",
    "    # Handle numerical errors (small negative values from floating point)\n",
    "    sq_distances = np.maximum(sq_distances, 0)\n",
    "    \n",
    "    # Step 3: Find top-k using argpartition (faster than full sort!)\n",
    "    # argpartition puts the k smallest elements in the first k positions\n",
    "    # but doesn't sort them\n",
    "    indices = np.argpartition(sq_distances, k, axis=1)[:, :k]\n",
    "    \n",
    "    # Step 4: Get the actual squared distances for these indices\n",
    "    # Use advanced indexing\n",
    "    row_indices = np.arange(len(query_points))[:, np.newaxis]\n",
    "    top_k_sq_distances = sq_distances[row_indices, indices]\n",
    "    \n",
    "    # Step 5: Sort within the k (argpartition doesn't guarantee order)\n",
    "    sorted_within_k = np.argsort(top_k_sq_distances, axis=1)\n",
    "    \n",
    "    # Reorder indices and distances\n",
    "    final_indices = np.take_along_axis(indices, sorted_within_k, axis=1)\n",
    "    final_sq_distances = np.take_along_axis(top_k_sq_distances, sorted_within_k, axis=1)\n",
    "    \n",
    "    # Convert squared distances to distances\n",
    "    final_distances = np.sqrt(final_sq_distances)\n",
    "    \n",
    "    return final_indices, final_distances\n",
    "\n",
    "print(\"Optimized KNN function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and verify correctness\n",
    "np.random.seed(42)\n",
    "queries = np.random.randn(50, 32).astype(np.float32)\n",
    "references = np.random.randn(200, 32).astype(np.float32)\n",
    "k = 5\n",
    "\n",
    "# Run both versions\n",
    "start = time.perf_counter()\n",
    "indices_slow, distances_slow = knn_slow(queries, references, k)\n",
    "slow_time = time.perf_counter() - start\n",
    "\n",
    "start = time.perf_counter()\n",
    "indices_fast, distances_fast = knn_fast(queries, references, k)\n",
    "fast_time = time.perf_counter() - start\n",
    "\n",
    "# Verify correctness\n",
    "indices_match = np.allclose(indices_slow, indices_fast)\n",
    "distances_match = np.allclose(distances_slow, distances_fast, rtol=1e-5)\n",
    "\n",
    "print(\"Correctness check:\")\n",
    "print(f\"  Indices match: {indices_match}\")\n",
    "print(f\"  Distances match: {distances_match}\")\n",
    "\n",
    "print(f\"\\nPerformance comparison:\")\n",
    "print(f\"  Slow version: {slow_time*1000:.2f} ms\")\n",
    "print(f\"  Fast version: {fast_time*1000:.2f} ms\")\n",
    "print(f\"  Speedup: {slow_time/fast_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale test\n",
    "print(\"\\nScale test (larger data):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "sizes = [(100, 500), (200, 1000), (500, 2000), (1000, 5000)]\n",
    "\n",
    "for n_queries, n_refs in sizes:\n",
    "    queries = np.random.randn(n_queries, 64).astype(np.float32)\n",
    "    references = np.random.randn(n_refs, 64).astype(np.float32)\n",
    "    k = 10\n",
    "    \n",
    "    # Only time fast version for larger sizes\n",
    "    if n_queries <= 100 and n_refs <= 500:\n",
    "        start = time.perf_counter()\n",
    "        _ = knn_slow(queries, references, k)\n",
    "        slow_time = (time.perf_counter() - start) * 1000\n",
    "    else:\n",
    "        slow_time = float('nan')\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    _ = knn_fast(queries, references, k)\n",
    "    fast_time = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    if np.isnan(slow_time):\n",
    "        print(f\"  {n_queries:4d} queries, {n_refs:4d} refs: Fast = {fast_time:7.1f} ms (slow would be too slow)\")\n",
    "    else:\n",
    "        print(f\"  {n_queries:4d} queries, {n_refs:4d} refs: Slow = {slow_time:7.1f} ms, Fast = {fast_time:7.1f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Even Faster with scipy.spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def knn_kdtree(query_points, reference_points, k):\n",
    "    \"\"\"\n",
    "    KNN using KD-Tree (best for low-dimensional data).\n",
    "    \n",
    "    Note: KD-Trees are efficient for low dimensions (d < ~20).\n",
    "    For high-dimensional data, the brute-force vectorized approach may be faster.\n",
    "    \"\"\"\n",
    "    tree = cKDTree(reference_points)\n",
    "    distances, indices = tree.query(query_points, k=k)\n",
    "    return indices, distances\n",
    "\n",
    "# Compare all three methods on low-dimensional data\n",
    "print(\"Comparison with KD-Tree (low-dimensional):\")\n",
    "queries = np.random.randn(500, 10).astype(np.float32)  # Low dimensional\n",
    "references = np.random.randn(2000, 10).astype(np.float32)\n",
    "k = 10\n",
    "\n",
    "# Fast vectorized\n",
    "start = time.perf_counter()\n",
    "_, _ = knn_fast(queries, references, k)\n",
    "fast_time = (time.perf_counter() - start) * 1000\n",
    "\n",
    "# KD-Tree\n",
    "start = time.perf_counter()\n",
    "_, _ = knn_kdtree(queries, references, k)\n",
    "kdtree_time = (time.perf_counter() - start) * 1000\n",
    "\n",
    "print(f\"  Vectorized: {fast_time:.1f} ms\")\n",
    "print(f\"  KD-Tree:    {kdtree_time:.1f} ms\")\n",
    "\n",
    "# High-dimensional comparison\n",
    "print(\"\\nComparison with KD-Tree (high-dimensional):\")\n",
    "queries = np.random.randn(500, 128).astype(np.float32)  # High dimensional\n",
    "references = np.random.randn(2000, 128).astype(np.float32)\n",
    "\n",
    "start = time.perf_counter()\n",
    "_, _ = knn_fast(queries, references, k)\n",
    "fast_time = (time.perf_counter() - start) * 1000\n",
    "\n",
    "start = time.perf_counter()\n",
    "_, _ = knn_kdtree(queries, references, k)\n",
    "kdtree_time = (time.perf_counter() - start) * 1000\n",
    "\n",
    "print(f\"  Vectorized: {fast_time:.1f} ms\")\n",
    "print(f\"  KD-Tree:    {kdtree_time:.1f} ms\")\n",
    "print(f\"\\nðŸ’¡ KD-Trees suffer from 'curse of dimensionality' in high-D!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Memory-Efficient KNN for Very Large Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_chunked(query_points, reference_points, k, chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Memory-efficient KNN that processes in chunks.\n",
    "    \n",
    "    Useful when the full distance matrix doesn't fit in memory.\n",
    "    \n",
    "    Args:\n",
    "        query_points: (n_queries, dims)\n",
    "        reference_points: (n_reference, dims)\n",
    "        k: number of neighbors\n",
    "        chunk_size: number of queries to process at once\n",
    "    \n",
    "    Returns:\n",
    "        indices, distances: both (n_queries, k)\n",
    "    \"\"\"\n",
    "    n_queries = len(query_points)\n",
    "    all_indices = []\n",
    "    all_distances = []\n",
    "    \n",
    "    # Precompute reference squared norms\n",
    "    ref_sq = np.sum(reference_points ** 2, axis=1)\n",
    "    \n",
    "    for start in range(0, n_queries, chunk_size):\n",
    "        end = min(start + chunk_size, n_queries)\n",
    "        chunk = query_points[start:end]\n",
    "        \n",
    "        # Compute distances for this chunk\n",
    "        query_sq = np.sum(chunk ** 2, axis=1, keepdims=True)\n",
    "        sq_distances = query_sq + ref_sq - 2 * (chunk @ reference_points.T)\n",
    "        sq_distances = np.maximum(sq_distances, 0)\n",
    "        \n",
    "        # Find top-k\n",
    "        indices = np.argpartition(sq_distances, k, axis=1)[:, :k]\n",
    "        row_idx = np.arange(len(chunk))[:, np.newaxis]\n",
    "        top_k_sq = sq_distances[row_idx, indices]\n",
    "        \n",
    "        # Sort within k\n",
    "        sorted_within = np.argsort(top_k_sq, axis=1)\n",
    "        final_indices = np.take_along_axis(indices, sorted_within, axis=1)\n",
    "        final_distances = np.sqrt(np.take_along_axis(top_k_sq, sorted_within, axis=1))\n",
    "        \n",
    "        all_indices.append(final_indices)\n",
    "        all_distances.append(final_distances)\n",
    "    \n",
    "    return np.vstack(all_indices), np.vstack(all_distances)\n",
    "\n",
    "# Test chunked version\n",
    "indices_chunked, distances_chunked = knn_chunked(queries, references, k=10, chunk_size=100)\n",
    "indices_fast, distances_fast = knn_fast(queries, references, k=10)\n",
    "\n",
    "print(\"Chunked version matches fast version:\")\n",
    "print(f\"  Indices: {np.allclose(indices_chunked, indices_fast)}\")\n",
    "print(f\"  Distances: {np.allclose(distances_chunked, distances_fast)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Optimization Lessons\n",
    "\n",
    "1. **Vectorize loops** - NumPy operations are 100-1000x faster than Python loops\n",
    "\n",
    "2. **Use math tricks** - The ||a-b||Â² = ||a||Â² + ||b||Â² - 2aÂ·b trick avoids materializing difference arrays\n",
    "\n",
    "3. **Choose the right algorithm** - `argpartition` is O(n) vs `argsort` O(n log n) for top-k\n",
    "\n",
    "4. **Consider memory** - Chunking helps when data doesn't fit in memory\n",
    "\n",
    "5. **Know your data** - KD-Trees work for low-D, brute-force for high-D\n",
    "\n",
    "---\n",
    "\n",
    "**End of Solutions**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}