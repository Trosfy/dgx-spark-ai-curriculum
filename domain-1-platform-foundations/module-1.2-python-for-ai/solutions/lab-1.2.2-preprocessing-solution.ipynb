{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.2.2: Dataset Preprocessing Pipeline - SOLUTIONS\n",
    "\n",
    "**Module:** 1.2 - Python for AI/ML  \n",
    "\n",
    "This notebook contains solutions to all exercises from the Preprocessing Pipeline Lab.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seaborn version: 0.13.2\n",
      "scikit-learn: Available\n",
      "\n",
      "Solutions Notebook for Preprocessing Pipeline\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check seaborn availability (used for Titanic dataset challenge)\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    HAS_SEABORN = True\n",
    "    print(f\"Seaborn version: {sns.__version__}\")\n",
    "except ImportError:\n",
    "    HAS_SEABORN = False\n",
    "    sns = None\n",
    "    print(\"⚠️ Seaborn not installed. Titanic challenge requires seaborn.\")\n",
    "    print(\"   Install with: pip install seaborn\")\n",
    "\n",
    "# Check sklearn availability (used for train_test_split)\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    HAS_SKLEARN = True\n",
    "    print(\"scikit-learn: Available\")\n",
    "except ImportError:\n",
    "    HAS_SKLEARN = False\n",
    "    print(\"⚠️ scikit-learn not installed. Some examples will use manual train/test split.\")\n",
    "    print(\"   Install with: pip install scikit-learn\")\n",
    "\n",
    "print(\"\\nSolutions Notebook for Preprocessing Pipeline\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1000, 7)\n",
      "\n",
      "Missing values:\n",
      "age             50\n",
      "income          80\n",
      "credit_score    30\n",
      "education       37\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Recreate the sample data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "data = {\n",
    "    'age': np.random.randint(18, 80, n_samples).astype(float),\n",
    "    'income': np.random.lognormal(10.5, 0.5, n_samples),\n",
    "    'credit_score': np.random.randint(300, 850, n_samples).astype(float),\n",
    "    'years_employed': np.random.exponential(5, n_samples),\n",
    "    'education': np.random.choice(\n",
    "        ['High School', 'Bachelor', 'Master', 'PhD', None], \n",
    "        n_samples, \n",
    "        p=[0.3, 0.35, 0.2, 0.1, 0.05]\n",
    "    ),\n",
    "    'employment_type': np.random.choice(\n",
    "        ['Full-time', 'Part-time', 'Self-employed', 'Unemployed'],\n",
    "        n_samples,\n",
    "        p=[0.6, 0.15, 0.15, 0.1]\n",
    "    ),\n",
    "    'default': np.random.choice([0, 1], n_samples, p=[0.85, 0.15])\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add missing values\n",
    "df.loc[np.random.choice(n_samples, 50, replace=False), 'age'] = np.nan\n",
    "df.loc[np.random.choice(n_samples, 80, replace=False), 'income'] = np.nan\n",
    "df.loc[np.random.choice(n_samples, 30, replace=False), 'credit_score'] = np.nan\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nMissing values:\\n{df.isnull().sum()[df.isnull().sum() > 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Group-Based Imputation\n",
    "\n",
    "**Task:** Impute missing income using the median income for each education level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median income by education (before imputation):\n",
      "education\n",
      "Bachelor       38244.0\n",
      "High School    39794.0\n",
      "Master         35088.0\n",
      "PhD            37264.0\n",
      "Name: income, dtype: float64\n",
      "\n",
      "Missing values after imputation: 0\n",
      "\n",
      "Sample of imputed data (first 5 originally missing):\n",
      "      education        income\n",
      "5      Bachelor  37733.669035\n",
      "13  High School  39794.093323\n",
      "29       Master  35087.652845\n",
      "44       Master  35087.652845\n",
      "45     Bachelor  37733.669035\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION - Exercise 1\n",
    "df_group_imputed = df.copy()\n",
    "\n",
    "# First, fill education missing values (we need groups to exist)\n",
    "education_mode = df_group_imputed['education'].mode()[0]\n",
    "df_group_imputed['education'].fillna(education_mode, inplace=True)\n",
    "\n",
    "# Show median income by education BEFORE imputation\n",
    "print(\"Median income by education (before imputation):\")\n",
    "print(df.groupby('education')['income'].median().round(0))\n",
    "\n",
    "# Method 1: Using groupby + transform + fillna\n",
    "df_group_imputed['income'] = df_group_imputed.groupby('education')['income'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "\n",
    "print(f\"\\nMissing values after imputation: {df_group_imputed['income'].isna().sum()}\")\n",
    "\n",
    "# Verify by checking a sample\n",
    "print(\"\\nSample of imputed data (first 5 originally missing):\")\n",
    "original_missing = df[df['income'].isna()].head(5).index\n",
    "print(df_group_imputed.loc[original_missing, ['education', 'income']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Method: Manual Group Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group medians:\n",
      "education\n",
      "Bachelor       37734.0\n",
      "High School    39794.0\n",
      "Master         35088.0\n",
      "PhD            37264.0\n",
      "Name: income, dtype: float64\n",
      "\n",
      "Missing after: 0\n",
      "Same result? True\n"
     ]
    }
   ],
   "source": [
    "# Alternative: More explicit approach\n",
    "df_alt = df.copy()\n",
    "df_alt['education'].fillna(education_mode, inplace=True)\n",
    "\n",
    "# Calculate group medians\n",
    "group_medians = df_alt.groupby('education')['income'].median()\n",
    "print(\"Group medians:\")\n",
    "print(group_medians.round(0))\n",
    "\n",
    "# Fill missing values\n",
    "for education_level in df_alt['education'].unique():\n",
    "    mask = (df_alt['education'] == education_level) & (df_alt['income'].isna())\n",
    "    df_alt.loc[mask, 'income'] = group_medians[education_level]\n",
    "\n",
    "print(f\"\\nMissing after: {df_alt['income'].isna().sum()}\")\n",
    "print(f\"Same result? {np.allclose(df_group_imputed['income'], df_alt['income'], equal_nan=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Extended Preprocessor with Log Transform\n",
    "\n",
    "**Task:** Add log transformation capability to the Preprocessor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Preprocessor with log transform defined!\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION - Exercise 2: Extended Preprocessor\n",
    "\n",
    "class PreprocessorWithLog:\n",
    "    \"\"\"\n",
    "    Extended Preprocessor with log transformation support.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        numeric_features,\n",
    "        categorical_features=None,\n",
    "        ordinal_mappings=None,\n",
    "        scaling='standard',\n",
    "        impute_strategy='median',\n",
    "        log_features=None  # NEW PARAMETER\n",
    "    ):\n",
    "        self.numeric_features = numeric_features\n",
    "        self.categorical_features = categorical_features or []\n",
    "        self.ordinal_mappings = ordinal_mappings or {}\n",
    "        self.scaling = scaling\n",
    "        self.impute_strategy = impute_strategy\n",
    "        self.log_features = log_features or []  # NEW\n",
    "        \n",
    "        self.numeric_stats_ = {}\n",
    "        self.categorical_values_ = {}\n",
    "        self.scale_params_ = {}\n",
    "        self._is_fitted = False\n",
    "        \n",
    "    def fit(self, df):\n",
    "        # Learn imputation values\n",
    "        for col in self.numeric_features:\n",
    "            if self.impute_strategy == 'median':\n",
    "                self.numeric_stats_[col] = df[col].median()\n",
    "            else:\n",
    "                self.numeric_stats_[col] = df[col].mean()\n",
    "        \n",
    "        for col in self.categorical_features:\n",
    "            mode_values = df[col].mode()\n",
    "            self.categorical_values_[col] = {\n",
    "                'mode': mode_values[0] if len(mode_values) > 0 else 'Unknown',\n",
    "                'categories': sorted(df[col].dropna().unique())\n",
    "            }\n",
    "        \n",
    "        # Prepare data for scale param calculation\n",
    "        df_temp = df.copy()\n",
    "        for col in self.numeric_features:\n",
    "            df_temp[col].fillna(self.numeric_stats_[col], inplace=True)\n",
    "        \n",
    "        # Apply log transform BEFORE computing scale params\n",
    "        for col in self.log_features:\n",
    "            if col in df_temp.columns:\n",
    "                df_temp[col] = np.log1p(np.maximum(df_temp[col], 0))\n",
    "        \n",
    "        # Learn scaling parameters\n",
    "        X = df_temp[self.numeric_features].values\n",
    "        \n",
    "        if self.scaling == 'standard':\n",
    "            self.scale_params_ = {\n",
    "                'center': X.mean(axis=0),\n",
    "                'scale': X.std(axis=0)\n",
    "            }\n",
    "        elif self.scaling == 'minmax':\n",
    "            self.scale_params_ = {\n",
    "                'center': X.min(axis=0),\n",
    "                'scale': X.max(axis=0) - X.min(axis=0)\n",
    "            }\n",
    "        \n",
    "        # Prevent division by zero\n",
    "        self.scale_params_['scale'] = np.where(\n",
    "            self.scale_params_['scale'] == 0, 1, self.scale_params_['scale']\n",
    "        )\n",
    "        \n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        if not self._is_fitted:\n",
    "            raise ValueError(\"Not fitted!\")\n",
    "        \n",
    "        result = df.copy()\n",
    "        \n",
    "        # 1. Impute numeric\n",
    "        for col in self.numeric_features:\n",
    "            result[col].fillna(self.numeric_stats_[col], inplace=True)\n",
    "        \n",
    "        # 2. Impute categorical\n",
    "        for col in self.categorical_features:\n",
    "            result[col].fillna(self.categorical_values_[col]['mode'], inplace=True)\n",
    "        \n",
    "        # 3. Apply log transform (NEW!)\n",
    "        for col in self.log_features:\n",
    "            if col in result.columns:\n",
    "                result[f'log_{col}'] = np.log1p(np.maximum(result[col], 0))\n",
    "        \n",
    "        # 4. Ordinal encoding\n",
    "        for col, mapping in self.ordinal_mappings.items():\n",
    "            result[f'{col}_encoded'] = result[col].map(mapping).fillna(-1)\n",
    "            result = result.drop(col, axis=1)\n",
    "        \n",
    "        # 5. One-hot encoding\n",
    "        for col in self.categorical_features:\n",
    "            if col in self.ordinal_mappings:\n",
    "                continue\n",
    "            for cat in self.categorical_values_[col]['categories']:\n",
    "                result[f'{col}_{cat}'] = (result[col] == cat).astype(int)\n",
    "            result = result.drop(col, axis=1)\n",
    "        \n",
    "        # 6. Scale numeric (including log-transformed)\n",
    "        if self.scaling:\n",
    "            for i, col in enumerate(self.numeric_features):\n",
    "                # If this column was log-transformed, scale the log version\n",
    "                if col in self.log_features:\n",
    "                    target_col = f'log_{col}'\n",
    "                else:\n",
    "                    target_col = col\n",
    "                \n",
    "                result[target_col] = (\n",
    "                    (result[target_col] - self.scale_params_['center'][i]) / \n",
    "                    self.scale_params_['scale'][i]\n",
    "                )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def fit_transform(self, df):\n",
    "        return self.fit(df).transform(df)\n",
    "\n",
    "print(\"Extended Preprocessor with log transform defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed columns:\n",
      "['age', 'income', 'credit_score', 'years_employed', 'default', 'log_income', 'education_encoded', 'employment_type_Full-time', 'employment_type_Part-time', 'employment_type_Self-employed', 'employment_type_Unemployed']\n",
      "\n",
      "log_income statistics:\n",
      "count    8.000000e+02\n",
      "mean    -5.151435e-16\n",
      "std      1.000626e+00\n",
      "min     -3.441172e+00\n",
      "25%     -6.152823e-01\n",
      "50%      2.193601e-02\n",
      "75%      5.995075e-01\n",
      "max      3.063604e+00\n",
      "Name: log_income, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Test the extended preprocessor\n",
    "if HAS_SKLEARN:\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "else:\n",
    "    # Manual train/test split fallback\n",
    "    np.random.seed(42)\n",
    "    mask = np.random.rand(len(df)) < 0.8\n",
    "    train_df = df[mask]\n",
    "    test_df = df[~mask]\n",
    "    print(\"Using manual train/test split (sklearn not available)\")\n",
    "\n",
    "preprocessor = PreprocessorWithLog(\n",
    "    numeric_features=['age', 'income', 'credit_score', 'years_employed'],\n",
    "    categorical_features=['education', 'employment_type'],\n",
    "    ordinal_mappings={\n",
    "        'education': {'High School': 0, 'Bachelor': 1, 'Master': 2, 'PhD': 3}\n",
    "    },\n",
    "    scaling='standard',\n",
    "    log_features=['income']  # Apply log transform to income\n",
    ")\n",
    "\n",
    "train_processed = preprocessor.fit_transform(train_df)\n",
    "test_processed = preprocessor.transform(test_df)\n",
    "\n",
    "print(\"Processed columns:\")\n",
    "print(train_processed.columns.tolist())\n",
    "\n",
    "print(f\"\\nlog_income statistics:\")\n",
    "print(train_processed['log_income'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge: Titanic Dataset Preprocessing\n",
    "\n",
    "Complete preprocessing pipeline for the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic dataset shape: (891, 15)\n",
      "\n",
      "Columns: ['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town', 'alive', 'alone']\n",
      "\n",
      "Missing values:\n",
      "age            177\n",
      "embarked         2\n",
      "deck           688\n",
      "embark_town      2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION - Titanic Challenge\n",
    "if not HAS_SEABORN:\n",
    "    print(\"❌ Seaborn is required for the Titanic challenge.\")\n",
    "    print(\"   Install with: pip install seaborn\")\n",
    "    print(\"   Then restart the kernel and run again.\")\n",
    "else:\n",
    "    # Load Titanic data\n",
    "    titanic = sns.load_dataset('titanic')\n",
    "    print(f\"Titanic dataset shape: {titanic.shape}\")\n",
    "    print(f\"\\nColumns: {titanic.columns.tolist()}\")\n",
    "    print(f\"\\nMissing values:\\n{titanic.isnull().sum()[titanic.isnull().sum() > 0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New features created:\n",
      "   family_size  is_alone  fare_per_person\n",
      "0            2         0          3.62500\n",
      "1            2         0         35.64165\n",
      "2            1         1          7.92500\n",
      "3            2         0         26.55000\n",
      "4            1         1          8.05000\n"
     ]
    }
   ],
   "source": [
    "# Feature engineering (requires seaborn for Titanic dataset)\n",
    "if HAS_SEABORN:\n",
    "    df_titanic = titanic.copy()\n",
    "\n",
    "    # 1. Extract title from name\n",
    "    # Note: 'name' column is not in seaborn's version, so we'll skip this\n",
    "    # If using Kaggle version:\n",
    "    # df_titanic['title'] = df_titanic['name'].str.extract(' ([A-Za-z]+)\\.')\n",
    "\n",
    "    # 2. Create family size\n",
    "    df_titanic['family_size'] = df_titanic['sibsp'] + df_titanic['parch'] + 1\n",
    "\n",
    "    # 3. Is traveling alone?\n",
    "    df_titanic['is_alone'] = (df_titanic['family_size'] == 1).astype(int)\n",
    "\n",
    "    # 4. Fare per person\n",
    "    df_titanic['fare_per_person'] = df_titanic['fare'] / df_titanic['family_size']\n",
    "\n",
    "    print(\"New features created:\")\n",
    "    print(df_titanic[['family_size', 'is_alone', 'fare_per_person']].head())\n",
    "else:\n",
    "    print(\"⏭️ Skipped (seaborn not available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after imputation:\n",
      "deck           688\n",
      "embark_town      2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values (requires seaborn for Titanic dataset)\n",
    "if HAS_SEABORN:\n",
    "    # Age: Impute with median by class and sex\n",
    "    df_titanic['age'] = df_titanic.groupby(['pclass', 'sex'])['age'].transform(\n",
    "        lambda x: x.fillna(x.median())\n",
    "    )\n",
    "\n",
    "    # Embarked: Impute with mode\n",
    "    df_titanic['embarked'] = df_titanic['embarked'].fillna(df_titanic['embarked'].mode()[0])\n",
    "\n",
    "    # Deck (from cabin): Extract first letter, fill unknown\n",
    "    # df_titanic['deck'] = df_titanic['deck'].fillna('Unknown')\n",
    "\n",
    "    print(f\"Missing values after imputation:\")\n",
    "    print(df_titanic.isnull().sum()[df_titanic.isnull().sum() > 0])\n",
    "else:\n",
    "    print(\"⏭️ Skipped (seaborn not available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded features:\n",
      "   sex_encoded  embarked_C  embarked_Q  embarked_S\n",
      "0            1           0           0           1\n",
      "1            0           1           0           0\n",
      "2            0           0           0           1\n",
      "3            0           0           0           1\n",
      "4            1           0           0           1\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical variables (requires seaborn for Titanic dataset)\n",
    "if HAS_SEABORN:\n",
    "    # Sex: Binary encoding\n",
    "    df_titanic['sex_encoded'] = (df_titanic['sex'] == 'male').astype(int)\n",
    "\n",
    "    # Embarked: One-hot\n",
    "    embarked_dummies = pd.get_dummies(df_titanic['embarked'], prefix='embarked', dtype=int)\n",
    "    df_titanic = pd.concat([df_titanic, embarked_dummies], axis=1)\n",
    "\n",
    "    # Class: Already numeric (1, 2, 3)\n",
    "\n",
    "    print(\"Encoded features:\")\n",
    "    print(df_titanic[['sex_encoded', 'embarked_C', 'embarked_Q', 'embarked_S']].head())\n",
    "else:\n",
    "    print(\"⏭️ Skipped (seaborn not available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled features statistics:\n",
      "       age_scaled  fare_scaled  fare_per_person_scaled  family_size_scaled\n",
      "count      891.00       891.00                  891.00              891.00\n",
      "mean         0.00        -0.00                   -0.00               -0.00\n",
      "std          1.00         1.00                    1.00                1.00\n",
      "min         -2.16        -0.65                   -0.56               -0.56\n",
      "25%         -0.57        -0.49                   -0.35               -0.56\n",
      "50%         -0.23        -0.36                   -0.32               -0.56\n",
      "75%          0.52        -0.02                    0.10                0.06\n",
      "max          3.82         9.66                   13.74                5.64\n"
     ]
    }
   ],
   "source": [
    "# Scale numeric features (requires seaborn for Titanic dataset)\n",
    "if HAS_SEABORN:\n",
    "    numeric_cols = ['age', 'fare', 'fare_per_person', 'family_size']\n",
    "\n",
    "    # Standard scaling\n",
    "    for col in numeric_cols:\n",
    "        mean = df_titanic[col].mean()\n",
    "        std = df_titanic[col].std()\n",
    "        df_titanic[f'{col}_scaled'] = (df_titanic[col] - mean) / std\n",
    "\n",
    "    print(\"Scaled features statistics:\")\n",
    "    print(df_titanic[[f'{c}_scaled' for c in numeric_cols]].describe().round(2))\n",
    "else:\n",
    "    print(\"⏭️ Skipped (seaborn not available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature matrix shape: (891, 9)\n",
      "\n",
      "Features:\n",
      "['pclass', 'sex_encoded', 'age_scaled', 'fare_per_person_scaled', 'family_size', 'is_alone', 'embarked_C', 'embarked_Q', 'embarked_S']\n",
      "\n",
      "Target distribution:\n",
      "survived\n",
      "0    549\n",
      "1    342\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Final feature set for modeling (requires seaborn for Titanic dataset)\n",
    "if HAS_SEABORN:\n",
    "    feature_columns = [\n",
    "        'pclass',\n",
    "        'sex_encoded',\n",
    "        'age_scaled',\n",
    "        'fare_per_person_scaled',\n",
    "        'family_size',\n",
    "        'is_alone',\n",
    "        'embarked_C',\n",
    "        'embarked_Q',\n",
    "        'embarked_S'\n",
    "    ]\n",
    "\n",
    "    X = df_titanic[feature_columns]\n",
    "    y = df_titanic['survived']\n",
    "\n",
    "    print(f\"Final feature matrix shape: {X.shape}\")\n",
    "    print(f\"\\nFeatures:\\n{X.columns.tolist()}\")\n",
    "    print(f\"\\nTarget distribution:\\n{y.value_counts()}\")\n",
    "else:\n",
    "    print(\"⏭️ Skipped (seaborn not available)\")\n",
    "    print(\"\\nTo run the Titanic challenge, install seaborn:\")\n",
    "    print(\"   pip install seaborn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Group-based imputation** is more intelligent than global imputation\n",
    "2. **Log transforms** help with skewed distributions like income\n",
    "3. **Feature engineering** (family_size, is_alone) can be very valuable\n",
    "4. **Always fit on training data only** to prevent data leakage\n",
    "\n",
    "---\n",
    "\n",
    "**End of Solutions**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
