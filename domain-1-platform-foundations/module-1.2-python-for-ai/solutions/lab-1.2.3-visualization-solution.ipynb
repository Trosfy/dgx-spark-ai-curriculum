{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.2.3: Visualization Dashboard - SOLUTIONS\n",
    "\n",
    "**Module:** 1.2 - Python for AI/ML  \n",
    "\n",
    "This notebook contains solutions to all exercises from the Visualization Dashboard Lab.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "from sklearn.calibration import calibration_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Solutions Notebook for Visualization Dashboard\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise: Custom Dashboard with Advanced Metrics\n",
    "\n",
    "**Task:** Create a 2x2 dashboard with:\n",
    "1. Learning Rate Finder curve\n",
    "2. ROC Curve\n",
    "3. Precision-Recall Curve\n",
    "4. Calibration Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data for all plots\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Learning rate finder data\n",
    "lrs = np.logspace(-6, 0, 100)\n",
    "lr_losses = 2.0 - 0.5 * np.log10(lrs + 1e-7) + np.random.normal(0, 0.05, 100)\n",
    "lr_losses[70:] += 0.1 * (np.arange(30) ** 1.5)  # Divergence at high LR\n",
    "\n",
    "# 2. Classification predictions for ROC, PR, Calibration\n",
    "n_samples = 1000\n",
    "y_true = np.random.binomial(1, 0.3, n_samples)  # 30% positive\n",
    "\n",
    "# Generate realistic probabilities\n",
    "y_prob = np.zeros(n_samples)\n",
    "y_prob[y_true == 1] = np.random.beta(5, 2, y_true.sum())  # Higher for positives\n",
    "y_prob[y_true == 0] = np.random.beta(2, 5, (1-y_true).sum())  # Lower for negatives\n",
    "\n",
    "print(\"Sample data generated:\")\n",
    "print(f\"  Learning rate range: {lrs.min():.2e} to {lrs.max():.2e}\")\n",
    "print(f\"  Classification samples: {n_samples}\")\n",
    "print(f\"  Positive rate: {y_true.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Complete Advanced Dashboard\n",
    "\n",
    "fig = plt.figure(figsize=(14, 12))\n",
    "gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Color palette\n",
    "colors = {\n",
    "    'primary': '#3498db',\n",
    "    'secondary': '#e74c3c',\n",
    "    'success': '#2ecc71',\n",
    "    'gray': '#95a5a6'\n",
    "}\n",
    "\n",
    "# Panel 1: Learning Rate Finder (top-left)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.plot(lrs, lr_losses, color=colors['primary'], linewidth=2)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('Learning Rate (log scale)')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Learning Rate Finder', fontweight='bold')\n",
    "\n",
    "# Find suggested LR (steepest descent point)\n",
    "gradients = np.gradient(lr_losses)\n",
    "suggested_idx = np.argmin(gradients[:70])  # Before divergence\n",
    "suggested_lr = lrs[suggested_idx]\n",
    "ax1.axvline(suggested_lr, color=colors['secondary'], linestyle='--', \n",
    "            label=f'Suggested LR: {suggested_lr:.2e}')\n",
    "ax1.scatter([suggested_lr], [lr_losses[suggested_idx]], color=colors['secondary'], \n",
    "            s=100, zorder=5)\n",
    "ax1.legend()\n",
    "\n",
    "# Panel 2: ROC Curve (top-right)\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "ax2.plot(fpr, tpr, color=colors['primary'], linewidth=2, \n",
    "         label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "ax2.plot([0, 1], [0, 1], color=colors['gray'], linestyle='--', label='Random Classifier')\n",
    "ax2.fill_between(fpr, tpr, alpha=0.2, color=colors['primary'])\n",
    "\n",
    "# Mark optimal threshold (Youden's J)\n",
    "j_scores = tpr - fpr\n",
    "optimal_idx = np.argmax(j_scores)\n",
    "ax2.scatter([fpr[optimal_idx]], [tpr[optimal_idx]], color=colors['secondary'], \n",
    "            s=100, zorder=5, label=f'Optimal (thresh={thresholds[optimal_idx]:.2f})')\n",
    "\n",
    "ax2.set_xlabel('False Positive Rate')\n",
    "ax2.set_ylabel('True Positive Rate')\n",
    "ax2.set_title('ROC Curve', fontweight='bold')\n",
    "ax2.legend(loc='lower right')\n",
    "ax2.set_xlim(-0.02, 1.02)\n",
    "ax2.set_ylim(-0.02, 1.02)\n",
    "\n",
    "# Panel 3: Precision-Recall Curve (bottom-left)\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_true, y_prob)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "ax3.plot(recall, precision, color=colors['success'], linewidth=2,\n",
    "         label=f'PR Curve (AUC = {pr_auc:.3f})')\n",
    "ax3.fill_between(recall, precision, alpha=0.2, color=colors['success'])\n",
    "\n",
    "# Baseline (random classifier)\n",
    "baseline = y_true.mean()\n",
    "ax3.axhline(baseline, color=colors['gray'], linestyle='--', \n",
    "            label=f'Baseline (P={baseline:.2f})')\n",
    "\n",
    "ax3.set_xlabel('Recall')\n",
    "ax3.set_ylabel('Precision')\n",
    "ax3.set_title('Precision-Recall Curve', fontweight='bold')\n",
    "ax3.legend(loc='lower left')\n",
    "ax3.set_xlim(-0.02, 1.02)\n",
    "ax3.set_ylim(-0.02, 1.02)\n",
    "\n",
    "# Panel 4: Calibration Plot (bottom-right)\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "# Compute calibration curve\n",
    "prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=10)\n",
    "\n",
    "ax4.plot([0, 1], [0, 1], color=colors['gray'], linestyle='--', label='Perfectly Calibrated')\n",
    "ax4.plot(prob_pred, prob_true, color=colors['primary'], linewidth=2, \n",
    "         marker='o', markersize=8, label='Model')\n",
    "\n",
    "# Add histogram of predictions\n",
    "ax4_hist = ax4.twinx()\n",
    "ax4_hist.hist(y_prob, bins=20, alpha=0.3, color=colors['primary'])\n",
    "ax4_hist.set_ylabel('Count', color=colors['gray'])\n",
    "ax4_hist.tick_params(axis='y', labelcolor=colors['gray'])\n",
    "\n",
    "ax4.set_xlabel('Mean Predicted Probability')\n",
    "ax4.set_ylabel('Fraction of Positives')\n",
    "ax4.set_title('Calibration Plot', fontweight='bold')\n",
    "ax4.legend(loc='upper left')\n",
    "ax4.set_xlim(-0.02, 1.02)\n",
    "ax4.set_ylim(-0.02, 1.02)\n",
    "\n",
    "# Overall title\n",
    "fig.suptitle('Advanced Model Evaluation Dashboard', fontsize=16, fontweight='bold', y=1.01)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('advanced_dashboard.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Advanced dashboard complete!\")\n",
    "print(f\"   ROC AUC: {roc_auc:.3f}\")\n",
    "print(f\"   PR AUC: {pr_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Interactive Dashboard Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_dashboard(y_true, y_prob, lr_data=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Create a comprehensive model evaluation dashboard.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True binary labels\n",
    "        y_prob: Predicted probabilities\n",
    "        lr_data: Optional tuple of (learning_rates, losses) for LR finder\n",
    "        save_path: Optional path to save figure\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with computed metrics\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(14, 12))\n",
    "    gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Panel 1: ROC Curve\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    metrics['roc_auc'] = auc(fpr, tpr)\n",
    "    ax1.plot(fpr, tpr, 'b-', linewidth=2, label=f'AUC = {metrics[\"roc_auc\"]:.3f}')\n",
    "    ax1.plot([0, 1], [0, 1], 'k--')\n",
    "    ax1.set_xlabel('FPR')\n",
    "    ax1.set_ylabel('TPR')\n",
    "    ax1.set_title('ROC Curve', fontweight='bold')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Panel 2: Precision-Recall\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "    metrics['pr_auc'] = auc(recall, precision)\n",
    "    ax2.plot(recall, precision, 'g-', linewidth=2, label=f'AUC = {metrics[\"pr_auc\"]:.3f}')\n",
    "    ax2.set_xlabel('Recall')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_title('Precision-Recall Curve', fontweight='bold')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Panel 3: Calibration\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=10)\n",
    "    ax3.plot([0, 1], [0, 1], 'k--')\n",
    "    ax3.plot(prob_pred, prob_true, 'o-', linewidth=2)\n",
    "    ax3.set_xlabel('Predicted')\n",
    "    ax3.set_ylabel('Actual')\n",
    "    ax3.set_title('Calibration Plot', fontweight='bold')\n",
    "    \n",
    "    # Panel 4: Score Distribution\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    ax4.hist(y_prob[y_true == 0], bins=30, alpha=0.6, label='Negative', density=True)\n",
    "    ax4.hist(y_prob[y_true == 1], bins=30, alpha=0.6, label='Positive', density=True)\n",
    "    ax4.set_xlabel('Predicted Probability')\n",
    "    ax4.set_ylabel('Density')\n",
    "    ax4.set_title('Score Distribution', fontweight='bold')\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Use the function\n",
    "metrics = create_evaluation_dashboard(y_true, y_prob)\n",
    "print(f\"\\nMetrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Attention Heatmap Visualization\n",
    "\n",
    "Commonly used for Transformer interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample attention weights\n",
    "np.random.seed(42)\n",
    "tokens = ['The', 'cat', 'sat', 'on', 'the', 'mat', 'because', 'it', 'was', 'tired']\n",
    "n_tokens = len(tokens)\n",
    "\n",
    "# Create attention matrix (softmax over rows)\n",
    "raw_attention = np.random.randn(n_tokens, n_tokens)\n",
    "# Make \"it\" attend to \"cat\" more\n",
    "raw_attention[7, 1] = 3.0\n",
    "# Softmax\n",
    "attention = np.exp(raw_attention) / np.exp(raw_attention).sum(axis=1, keepdims=True)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.heatmap(attention, \n",
    "            xticklabels=tokens,\n",
    "            yticklabels=tokens,\n",
    "            cmap='YlOrRd',\n",
    "            annot=True,\n",
    "            fmt='.2f',\n",
    "            ax=ax)\n",
    "\n",
    "ax.set_xlabel('Key (attending to)', fontsize=12)\n",
    "ax.set_ylabel('Query (from)', fontsize=12)\n",
    "ax.set_title('Attention Weights Visualization', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Notice how 'it' (row 7) attends most to 'cat' (column 1)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **ROC curves** show TPR vs FPR trade-off (use AUC for single metric)\n",
    "2. **PR curves** are better for imbalanced datasets\n",
    "3. **Calibration plots** show if predicted probabilities are reliable\n",
    "4. **Attention heatmaps** visualize transformer interpretability\n",
    "\n",
    "---\n",
    "\n",
    "**End of Solutions**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}