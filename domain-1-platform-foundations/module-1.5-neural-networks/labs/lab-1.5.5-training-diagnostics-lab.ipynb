{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.5.5: Training Diagnostics Lab\n",
    "\n",
    "**Module:** 1.5 - Neural Network Fundamentals  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** â­â­â­â­ (Advanced Debugging Skills)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Identify common training problems from loss curves\n",
    "- [ ] Diagnose learning rate issues (too high, too low)\n",
    "- [ ] Detect and fix vanishing/exploding gradients\n",
    "- [ ] Create a debugging checklist for neural networks\n",
    "- [ ] Master the \"overfit one batch\" debugging technique\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Prerequisites\n",
    "\n",
    "- Completed: Notebooks 01-04\n",
    "- Knowledge of: Training loops, loss functions, activations\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ Real-World Context\n",
    "\n",
    "**Debugging neural networks is an essential skill:**\n",
    "\n",
    "Even experienced ML engineers spend significant time debugging training issues. The difference between a junior and senior ML engineer often comes down to debugging skills. This notebook teaches you to recognize symptoms and apply fixes quickly.\n",
    "\n",
    "Famous quote from Andrej Karpathy: *\"The most important skill for an ML engineer is the ability to debug models.\"*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§’ ELI5: Why Neural Networks Fail\n",
    "\n",
    "> **Imagine you're teaching a toddler to walk.**\n",
    ">\n",
    "> Things that can go wrong:\n",
    "> - **Steps too big** (learning rate too high): They fall over constantly\n",
    "> - **Steps too small** (learning rate too low): They barely make progress\n",
    "> - **Can't feel their legs** (vanishing gradients): They don't know how to move\n",
    "> - **Legs moving wildly** (exploding gradients): Completely out of control\n",
    "> - **Only practicing in kitchen** (overfitting): Can't walk anywhere else\n",
    ">\n",
    "> As a \"parent\" (ML engineer), you need to watch for these signs and adjust the training accordingly!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import List, Dict, Tuple\nimport time\nimport sys\nimport os\nfrom pathlib import Path\n\n# Add scripts directory to path (robust approach)\nnotebook_dir = Path().resolve()\nif notebook_dir.name == 'labs':\n    scripts_dir = notebook_dir.parent / 'scripts'\nelse:\n    scripts_dir = notebook_dir / 'scripts'\n    if not scripts_dir.exists():\n        scripts_dir = notebook_dir.parent / 'scripts'\n\nif scripts_dir.exists():\n    sys.path.insert(0, str(scripts_dir))\n\nnp.random.seed(42)\nplt.style.use('default')\n%matplotlib inline\n\nprint(\"Setup complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST for experiments\n",
    "import gzip\n",
    "import urllib.request\n",
    "\n",
    "def load_mnist(path='../data'):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    base_url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = {\n",
    "        'train_images': 'train-images-idx3-ubyte.gz',\n",
    "        'train_labels': 'train-labels-idx1-ubyte.gz',\n",
    "        'test_images': 't10k-images-idx3-ubyte.gz',\n",
    "        'test_labels': 't10k-labels-idx1-ubyte.gz'\n",
    "    }\n",
    "    \n",
    "    def download(filename):\n",
    "        filepath = os.path.join(path, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            urllib.request.urlretrieve(base_url + filename, filepath)\n",
    "        return filepath\n",
    "    \n",
    "    def load_images(fp):\n",
    "        with gzip.open(fp, 'rb') as f:\n",
    "            f.read(16)\n",
    "            return np.frombuffer(f.read(), dtype=np.uint8).reshape(-1, 784).astype(np.float32) / 255.0\n",
    "    \n",
    "    def load_labels(fp):\n",
    "        with gzip.open(fp, 'rb') as f:\n",
    "            f.read(8)\n",
    "            return np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    \n",
    "    return (load_images(download(files['train_images'])),\n",
    "            load_labels(download(files['train_labels'])),\n",
    "            load_images(download(files['test_images'])),\n",
    "            load_labels(download(files['test_labels'])))\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_mnist()\n",
    "print(f\"Loaded {len(X_train)} training samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable MLP class for experiments\n",
    "class DiagnosticMLP:\n",
    "    \"\"\"\n",
    "    MLP with diagnostic capabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes: List[int], activation: str = 'relu'):\n",
    "        self.layers = []\n",
    "        self.activation = activation\n",
    "        self.gradient_history = []  # Track gradient magnitudes\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # Standard initialization\n",
    "            W = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2.0 / layer_sizes[i])\n",
    "            b = np.zeros(layer_sizes[i + 1])\n",
    "            self.layers.append({'W': W, 'b': b, 'cache': {}, 'dW': None, 'db': None})\n",
    "    \n",
    "    def _activate(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        return x\n",
    "    \n",
    "    def _activate_backward(self, x, dout):\n",
    "        if self.activation == 'relu':\n",
    "            return dout * (x > 0)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            s = 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "            return dout * s * (1 - s)\n",
    "        elif self.activation == 'tanh':\n",
    "            return dout * (1 - np.tanh(x) ** 2)\n",
    "        return dout\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = X\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            layer['cache']['X'] = out\n",
    "            out = out @ layer['W'] + layer['b']\n",
    "            layer['cache']['Z'] = out\n",
    "            out = self._activate(out)\n",
    "        \n",
    "        self.layers[-1]['cache']['X'] = out\n",
    "        out = out @ self.layers[-1]['W'] + self.layers[-1]['b']\n",
    "        \n",
    "        out_shifted = out - np.max(out, axis=1, keepdims=True)\n",
    "        exp_out = np.exp(out_shifted)\n",
    "        self.probs = exp_out / np.sum(exp_out, axis=1, keepdims=True)\n",
    "        return self.probs\n",
    "    \n",
    "    def backward(self, targets, lr):\n",
    "        batch_size = len(targets)\n",
    "        grad = self.probs.copy()\n",
    "        grad[np.arange(batch_size), targets] -= 1\n",
    "        \n",
    "        layer_gradients = []\n",
    "        \n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            layer = self.layers[i]\n",
    "            X = layer['cache']['X']\n",
    "            \n",
    "            layer['dW'] = X.T @ grad / batch_size\n",
    "            layer['db'] = np.mean(grad, axis=0)\n",
    "            \n",
    "            # Track gradient magnitude\n",
    "            layer_gradients.append(np.abs(layer['dW']).mean())\n",
    "            \n",
    "            grad = grad @ layer['W'].T\n",
    "            \n",
    "            if i > 0:\n",
    "                Z = self.layers[i - 1]['cache']['Z']\n",
    "                grad = self._activate_backward(Z, grad)\n",
    "            \n",
    "            layer['W'] -= lr * layer['dW']\n",
    "            layer['b'] -= lr * layer['db']\n",
    "        \n",
    "        self.gradient_history.append(layer_gradients[::-1])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.forward(X), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Problem 1 - Learning Rate Too High\n",
    "\n",
    "### Symptoms:\n",
    "- Loss oscillates wildly or explodes to infinity\n",
    "- Training is unstable\n",
    "- Accuracy fluctuates randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, lr, epochs=20, batch_size=64, subset=5000):\n",
    "    \"\"\"Train model and return history.\"\"\"\n",
    "    history = {'loss': [], 'acc': []}\n",
    "    X_sub, y_sub = X_train[:subset], y_train[:subset]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        indices = np.random.permutation(len(X_sub))\n",
    "        epoch_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for start in range(0, len(X_sub), batch_size):\n",
    "            batch_idx = indices[start:start + batch_size]\n",
    "            X_batch = X_sub[batch_idx]\n",
    "            y_batch = y_sub[batch_idx]\n",
    "            \n",
    "            probs = model.forward(X_batch)\n",
    "            loss = -np.mean(np.log(probs[np.arange(len(y_batch)), y_batch] + 1e-10))\n",
    "            \n",
    "            # Clip loss for visualization (NaN becomes large number)\n",
    "            if np.isnan(loss) or loss > 100:\n",
    "                loss = 100\n",
    "            \n",
    "            model.backward(y_batch, lr)\n",
    "            epoch_loss += loss\n",
    "            n_batches += 1\n",
    "        \n",
    "        acc = np.mean(model.predict(X_test[:1000]) == y_test[:1000])\n",
    "        history['loss'].append(epoch_loss / n_batches)\n",
    "        history['acc'].append(acc)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate learning rate too high\n",
    "print(\"âŒ Problem: Learning Rate Too High\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "np.random.seed(42)\n",
    "model_high_lr = DiagnosticMLP([784, 256, 128, 10])\n",
    "history_high_lr = train_model(model_high_lr, lr=10.0, epochs=20)  # LR=10 is way too high!\n",
    "\n",
    "print(f\"Final Loss: {history_high_lr['loss'][-1]:.4f}\")\n",
    "print(f\"Final Accuracy: {history_high_lr['acc'][-1]:.2%}\")\n",
    "print(\"\\nâš ï¸ Notice: Loss explodes or oscillates wildly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the problem\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history_high_lr['loss'], 'r-', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('âŒ Loss with LR=10.0 (Too High)', fontsize=12)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history_high_lr['acc'], 'r-', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('âŒ Accuracy Fluctuates Randomly', fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fix: Lower learning rate\n",
    "print(\"\\nâœ… Fix: Lower the Learning Rate\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "np.random.seed(42)\n",
    "model_good_lr = DiagnosticMLP([784, 256, 128, 10])\n",
    "history_good_lr = train_model(model_good_lr, lr=0.1, epochs=20)  # LR=0.1 is reasonable\n",
    "\n",
    "print(f\"Final Loss: {history_good_lr['loss'][-1]:.4f}\")\n",
    "print(f\"Final Accuracy: {history_good_lr['acc'][-1]:.2%}\")\n",
    "\n",
    "# Compare\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history_high_lr['loss'], 'r-', linewidth=2, alpha=0.5, label='LR=10.0 (bad)')\n",
    "axes[0].plot(history_good_lr['loss'], 'g-', linewidth=2, label='LR=0.1 (good)')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history_high_lr['acc'], 'r-', linewidth=2, alpha=0.5, label='LR=10.0 (bad)')\n",
    "axes[1].plot(history_good_lr['acc'], 'g-', linewidth=2, label='LR=0.1 (good)')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Problem 2 - Learning Rate Too Low\n",
    "\n",
    "### Symptoms:\n",
    "- Loss decreases extremely slowly\n",
    "- Training takes forever\n",
    "- Model never reaches good performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"âŒ Problem: Learning Rate Too Low\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "np.random.seed(42)\n",
    "model_low_lr = DiagnosticMLP([784, 256, 128, 10])\n",
    "history_low_lr = train_model(model_low_lr, lr=0.0001, epochs=20)  # LR=0.0001 is too low\n",
    "\n",
    "print(f\"Final Loss: {history_low_lr['loss'][-1]:.4f}\")\n",
    "print(f\"Final Accuracy: {history_low_lr['acc'][-1]:.2%}\")\n",
    "print(\"\\nâš ï¸ Notice: Loss barely moves! Training is stuck.\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "ax.plot(history_low_lr['loss'], 'b-', linewidth=2, label='LR=0.0001 (too low)')\n",
    "ax.plot(history_good_lr['loss'], 'g-', linewidth=2, label='LR=0.1 (good)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Learning Rate Too Low vs Good')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Problem 3 - Vanishing Gradients (Deep Sigmoid Network)\n",
    "\n",
    "### Symptoms:\n",
    "- Gradients in early layers are nearly zero\n",
    "- Early layers don't learn\n",
    "- Loss decreases slowly even with good LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"âŒ Problem: Vanishing Gradients (Deep Sigmoid Network)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a deep network with sigmoid activation\n",
    "np.random.seed(42)\n",
    "model_sigmoid = DiagnosticMLP([784, 256, 256, 256, 256, 128, 10], activation='sigmoid')\n",
    "history_sigmoid = train_model(model_sigmoid, lr=0.1, epochs=20)\n",
    "\n",
    "print(f\"Final Accuracy: {history_sigmoid['acc'][-1]:.2%}\")\n",
    "\n",
    "# Show gradient magnitudes by layer\n",
    "print(\"\\nGradient magnitudes by layer (last epoch):\")\n",
    "last_grads = model_sigmoid.gradient_history[-1]\n",
    "for i, g in enumerate(last_grads):\n",
    "    bar = 'â–ˆ' * int(g * 1000)\n",
    "    print(f\"  Layer {i}: {g:.6f} {bar}\")\n",
    "\n",
    "print(\"\\nâš ï¸ Notice: Gradients vanish in early layers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient flow\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gradient magnitudes over training\n",
    "grads_over_time = np.array(model_sigmoid.gradient_history)\n",
    "for layer_idx in range(grads_over_time.shape[1]):\n",
    "    axes[0].plot(grads_over_time[:, layer_idx], label=f'Layer {layer_idx}')\n",
    "\n",
    "axes[0].set_xlabel('Training Step (batch)')\n",
    "axes[0].set_ylabel('Gradient Magnitude')\n",
    "axes[0].set_title('Gradient Magnitudes (Sigmoid Network)')\n",
    "axes[0].legend(fontsize=8)\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Compare final gradients\n",
    "layer_names = [f'Layer {i}' for i in range(len(last_grads))]\n",
    "axes[1].bar(layer_names, last_grads, color=['red' if g < 0.001 else 'green' for g in last_grads])\n",
    "axes[1].set_ylabel('Gradient Magnitude')\n",
    "axes[1].set_title('Final Gradient by Layer (Red = Vanishing)')\n",
    "axes[1].axhline(0.001, color='red', linestyle='--', label='Vanishing threshold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fix: Use ReLU activation\n",
    "print(\"\\nâœ… Fix: Use ReLU Activation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "np.random.seed(42)\n",
    "model_relu = DiagnosticMLP([784, 256, 256, 256, 256, 128, 10], activation='relu')\n",
    "history_relu = train_model(model_relu, lr=0.1, epochs=20)\n",
    "\n",
    "print(f\"Final Accuracy (ReLU): {history_relu['acc'][-1]:.2%}\")\n",
    "\n",
    "print(\"\\nGradient magnitudes by layer (ReLU):\")\n",
    "last_grads_relu = model_relu.gradient_history[-1]\n",
    "for i, g in enumerate(last_grads_relu):\n",
    "    bar = 'â–ˆ' * int(g * 100)\n",
    "    print(f\"  Layer {i}: {g:.6f} {bar}\")\n",
    "\n",
    "# Compare\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "x = np.arange(len(last_grads))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, last_grads, width, label='Sigmoid', color='red', alpha=0.7)\n",
    "ax.bar(x + width/2, last_grads_relu, width, label='ReLU', color='green', alpha=0.7)\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Gradient Magnitude')\n",
    "ax.set_title('Gradient Comparison: Sigmoid vs ReLU')\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: The \"Overfit One Batch\" Test\n",
    "\n",
    "### The Most Important Debugging Technique\n",
    "\n",
    "Before training on full data, always test if your model can memorize a single batch. If it can't, there's a bug in your implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”¬ The 'Overfit One Batch' Test\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Take a single small batch\n",
    "X_batch = X_train[:32]\n",
    "y_batch = y_train[:32]\n",
    "\n",
    "print(f\"Batch size: {len(X_batch)}\")\n",
    "print(\"\\nIf your model works, it should perfectly memorize this batch.\")\n",
    "print(\"Loss should go to ~0, accuracy should go to 100%.\")\n",
    "\n",
    "# Train on just this batch\n",
    "np.random.seed(42)\n",
    "model_overfit = DiagnosticMLP([784, 256, 128, 10])\n",
    "\n",
    "losses = []\n",
    "accs = []\n",
    "\n",
    "for i in range(200):  # Many iterations on same batch\n",
    "    probs = model_overfit.forward(X_batch)\n",
    "    loss = -np.mean(np.log(probs[np.arange(len(y_batch)), y_batch] + 1e-10))\n",
    "    model_overfit.backward(y_batch, lr=0.1)\n",
    "    \n",
    "    acc = np.mean(model_overfit.predict(X_batch) == y_batch)\n",
    "    losses.append(loss)\n",
    "    accs.append(acc)\n",
    "\n",
    "print(f\"\\nAfter 200 iterations:\")\n",
    "print(f\"  Loss: {losses[-1]:.6f} (should be ~0)\")\n",
    "print(f\"  Accuracy: {accs[-1]:.2%} (should be 100%)\")\n",
    "\n",
    "if accs[-1] > 0.99:\n",
    "    print(\"\\nâœ… Model can overfit one batch - implementation is correct!\")\n",
    "else:\n",
    "    print(\"\\nâŒ Model CAN'T overfit one batch - there's a bug!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the overfit test\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(losses, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Should Approach 0')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(accs, 'g-', linewidth=2)\n",
    "axes[1].axhline(1.0, color='red', linestyle='--', label='100% target')\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Should Reach 100%')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: The Complete Debugging Checklist\n",
    "\n",
    "Here's your go-to checklist when training fails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"                    NEURAL NETWORK DEBUGGING CHECKLIST\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "checklist = \"\"\"\n",
    "1. âœ… CAN YOU OVERFIT ONE BATCH?\n",
    "   â†’ Train on a single batch for 100+ iterations\n",
    "   â†’ Loss should â†’ 0, accuracy should â†’ 100%\n",
    "   â†’ If not: Bug in forward/backward pass\n",
    "\n",
    "2. âœ… IS YOUR DATA CORRECT?\n",
    "   â†’ Visualize some samples with labels\n",
    "   â†’ Check for NaN/Inf values\n",
    "   â†’ Verify normalization (mean~0, std~1)\n",
    "\n",
    "3. âœ… IS LOSS DECREASING AT ALL?\n",
    "   â†’ If no: Learning rate too low OR gradient issue\n",
    "   â†’ Try increasing LR by 10x\n",
    "   â†’ Check gradients aren't zero\n",
    "\n",
    "4. âœ… IS LOSS EXPLODING OR OSCILLATING?\n",
    "   â†’ Learning rate too high\n",
    "   â†’ Try decreasing LR by 10x\n",
    "   â†’ Add gradient clipping\n",
    "\n",
    "5. âœ… IS LOSS NaN OR Inf?\n",
    "   â†’ Numerical instability\n",
    "   â†’ Check for division by zero\n",
    "   â†’ Add epsilon to denominators\n",
    "   â†’ Clip activations/logits\n",
    "\n",
    "6. âœ… ARE GRADIENTS VANISHING?\n",
    "   â†’ Check gradient magnitudes per layer\n",
    "   â†’ Switch from Sigmoid/Tanh to ReLU\n",
    "   â†’ Use proper initialization (He for ReLU)\n",
    "   â†’ Add residual connections\n",
    "\n",
    "7. âœ… IS VALIDATION LOSS INCREASING WHILE TRAINING LOSS DECREASES?\n",
    "   â†’ Overfitting!\n",
    "   â†’ Add regularization (L2, Dropout)\n",
    "   â†’ Get more data\n",
    "   â†’ Reduce model size\n",
    "\n",
    "8. âœ… IS ACCURACY STUCK AT RANDOM CHANCE?\n",
    "   â†’ Labels might be shuffled incorrectly\n",
    "   â†’ Output layer issue\n",
    "   â†’ Check class imbalance\n",
    "\"\"\"\n",
    "\n",
    "print(checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Symptoms â†’ Diagnosis â†’ Fix Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"                              SYMPTOM â†’ DIAGNOSIS â†’ FIX\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ SYMPTOM                         â”‚ DIAGNOSIS                   â”‚ FIX                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Loss explodes to NaN/Inf        â”‚ LR too high OR              â”‚ Decrease LR by 10x              â”‚\n",
    "â”‚                                 â”‚ Numerical instability       â”‚ Add gradient clipping           â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Loss barely moves               â”‚ LR too low OR               â”‚ Increase LR by 10x              â”‚\n",
    "â”‚                                 â”‚ Vanishing gradients         â”‚ Use ReLU, check initialization  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Loss oscillates wildly          â”‚ LR too high OR              â”‚ Decrease LR                     â”‚\n",
    "â”‚                                 â”‚ Batch size too small        â”‚ Increase batch size             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Can't overfit single batch      â”‚ Bug in implementation       â”‚ Debug forward/backward pass     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Train loss â†“, Val loss â†‘        â”‚ Overfitting                 â”‚ Add regularization, get data    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Both losses high, not moving    â”‚ Underfitting                â”‚ Bigger model, train longer      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Accuracy stuck at random        â”‚ Labels issue OR             â”‚ Check data labels               â”‚\n",
    "â”‚                                 â”‚ Output layer bug            â”‚ Verify softmax/loss             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Early layer gradients = 0       â”‚ Vanishing gradients         â”‚ Use ReLU, He init, ResNets      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ All predictions are same class  â”‚ Class imbalance OR          â”‚ Balance data, check loss        â”‚\n",
    "â”‚                                 â”‚ Bug in output layer         â”‚                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ‹ Try It Yourself\n",
    "\n",
    "### Exercise: Diagnose These Training Curves\n",
    "\n",
    "Look at each curve and identify the problem. What would you fix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate example problematic curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Problem 1: Exploding loss\n",
    "x = np.arange(20)\n",
    "y1 = np.exp(x * 0.3) + np.random.randn(20) * 10\n",
    "axes[0, 0].plot(y1, 'r-', linewidth=2)\n",
    "axes[0, 0].set_title('Problem A: ?????', fontsize=12)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Problem 2: Flat loss\n",
    "y2 = 2.3 + np.random.randn(20) * 0.01\n",
    "axes[0, 1].plot(y2, 'b-', linewidth=2)\n",
    "axes[0, 1].set_title('Problem B: ?????', fontsize=12)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].set_ylim(0, 5)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Problem 3: Overfitting\n",
    "y3_train = 2.3 - np.log(x + 1) * 0.8 + np.random.randn(20) * 0.05\n",
    "y3_val = 2.3 - np.log(x + 1) * 0.3 + x * 0.05 + np.random.randn(20) * 0.05\n",
    "axes[1, 0].plot(y3_train, 'b-', linewidth=2, label='Train')\n",
    "axes[1, 0].plot(y3_val, 'r-', linewidth=2, label='Validation')\n",
    "axes[1, 0].set_title('Problem C: ?????', fontsize=12)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Problem 4: Oscillating\n",
    "y4 = 1.5 + np.sin(x * 0.5) * 0.5 + np.random.randn(20) * 0.3\n",
    "axes[1, 1].plot(y4, 'g-', linewidth=2)\n",
    "axes[1, 1].set_title('Problem D: ?????', fontsize=12)\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ¤” Can you identify each problem?\")\n",
    "print(\"   A: _____________\")\n",
    "print(\"   B: _____________\")\n",
    "print(\"   C: _____________\")\n",
    "print(\"   D: _____________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ğŸ’¡ Answers</summary>\n",
    "\n",
    "- **A**: Learning rate too high (loss exploding)\n",
    "- **B**: Learning rate too low (loss stuck)\n",
    "- **C**: Overfitting (train decreases, val increases)\n",
    "- **D**: Learning rate too high (oscillating)\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "\n",
    "- âœ… How to identify LR too high/low from loss curves\n",
    "- âœ… How vanishing gradients manifest and how to fix them\n",
    "- âœ… The \"overfit one batch\" debugging technique\n",
    "- âœ… A complete debugging checklist\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– Further Reading\n",
    "\n",
    "- [A Recipe for Training Neural Networks (Karpathy)](http://karpathy.github.io/2019/04/25/recipe/)\n",
    "- [Troubleshooting Deep Neural Networks (Josh Tobin)](https://josh-tobin.com/assets/pdf/troubleshooting-deep-neural-networks-01-19.pdf)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… Cleanup complete!\")\n",
    "print(\"\\nğŸ¯ Next: Proceed to notebook 06-gpu-acceleration.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}