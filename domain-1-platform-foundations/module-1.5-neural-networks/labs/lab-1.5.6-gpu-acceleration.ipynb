{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 1.5.6: GPU Acceleration on DGX Spark\n\n**Module:** 1.5 - Neural Network Fundamentals  \n**Time:** 2 hours  \n**Difficulty:** ‚≠ê‚≠ê‚≠ê (Intermediate)\n\n---\n\n## üéØ Learning Objectives\n\nBy the end of this notebook, you will:\n- [ ] Port your NumPy MLP to PyTorch\n- [ ] Measure CPU vs GPU training times\n- [ ] Understand the speedup from GPU acceleration\n- [ ] Find optimal batch sizes for DGX Spark's 128GB unified memory\n- [ ] Use mixed precision training with torch.cuda.amp\n- [ ] Appreciate why GPUs revolutionized deep learning\n\n---\n\n## üìö Prerequisites\n\n- Completed: Labs 1.5.1-1.5.5\n- Environment: DGX Spark with PyTorch NGC container\n\n---\n\n## üåç Real-World Context\n\n**Why GPUs transformed AI:**\n\nBefore GPUs, training a neural network on ImageNet took weeks on CPUs. With GPUs, it takes hours. This 100x+ speedup enabled the deep learning revolution!\n\n**Your DGX Spark advantage:**\n- **128GB unified memory**: No CPU‚ÜîGPU transfer bottleneck\n- **192 Tensor Cores**: Hardware-accelerated matrix operations\n- **1 PFLOP NVFP4**: Native low-precision inference\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßí ELI5: Why GPUs Are Faster\n",
    "\n",
    "> **Imagine you need to add up 1000 numbers.**\n",
    ">\n",
    "> **CPU approach (like one really smart mathematician):**\n",
    "> - Add numbers one by one: 1+2=3, 3+3=6, 6+4=10...\n",
    "> - Very fast at each addition, but does them sequentially\n",
    "> - 1000 operations total\n",
    ">\n",
    "> **GPU approach (like 500 average calculators working together):**\n",
    "> - Split into pairs: (1+2), (3+4), (5+6)...\n",
    "> - Each pair adds simultaneously\n",
    "> - 500 results ‚Üí pair those ‚Üí 250 results ‚Üí ... ‚Üí 1 result\n",
    "> - Only ~10 rounds total!\n",
    ">\n",
    "> Neural networks are mostly matrix multiplications, which are **embarrassingly parallel** - perfect for GPUs!\n",
    ">\n",
    "> **DGX Spark's special power:** Unified memory means the CPU and GPU share the same 128GB RAM - no time wasted copying data back and forth!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup\n\n**Important:** This notebook requires PyTorch. On DGX Spark, use the NGC container:\n\n```bash\ndocker run --gpus all -it --rm \\\n    -v $HOME/workspace:/workspace \\\n    -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\n    --ipc=host \\\n    -p 8888:8888 \\\n    nvcr.io/nvidia/pytorch:25.11-py3 \\\n    jupyter lab --ip=0.0.0.0 --allow-root --no-browser\n```\n\n### Understanding the Docker Flags\n\n| Flag | Purpose |\n|------|---------|\n| `--gpus all` | **Required!** Enables GPU access inside the container |\n| `-it` | Interactive terminal with TTY |\n| `--rm` | Clean up container on exit |\n| `-v $HOME/workspace:/workspace` | Mounts your workspace directory |\n| `-v $HOME/.cache/huggingface:/root/.cache/huggingface` | Mounts Hugging Face cache for model downloads |\n| `--ipc=host` | **Required for DataLoader workers!** PyTorch DataLoader uses shared memory for inter-process communication. Without this flag, you'll get errors when using `num_workers > 0` |\n| `-p 8888:8888` | **Required for Jupyter!** Maps container port 8888 to host port 8888 so you can access Jupyter Lab in your browser |\n| `nvcr.io/nvidia/pytorch:25.11-py3` | NGC container with PyTorch optimized for ARM64 |\n\n**Why `--ipc=host` matters:** PyTorch DataLoader creates worker processes that share data through shared memory (`/dev/shm`). By default, Docker containers have a small shared memory limit (64MB). The `--ipc=host` flag shares the host's IPC namespace, giving you access to the full shared memory space."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Check if PyTorch is available\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    PYTORCH_AVAILABLE = True\n",
    "    print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"‚ùå PyTorch not available. Please use NGC container.\")\n",
    "\n",
    "# Check GPU availability\n",
    "if PYTORCH_AVAILABLE:\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"‚úÖ CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è CUDA not available. GPU comparisons will be simulated.\")\n",
    "\n",
    "np.random.seed(42)\n",
    "if PYTORCH_AVAILABLE:\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "import gzip\n",
    "import urllib.request\n",
    "\n",
    "def load_mnist(path='../data'):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    base_url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = {\n",
    "        'train_images': 'train-images-idx3-ubyte.gz',\n",
    "        'train_labels': 'train-labels-idx1-ubyte.gz',\n",
    "        'test_images': 't10k-images-idx3-ubyte.gz',\n",
    "        'test_labels': 't10k-labels-idx1-ubyte.gz'\n",
    "    }\n",
    "    \n",
    "    def download(filename):\n",
    "        filepath = os.path.join(path, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            urllib.request.urlretrieve(base_url + filename, filepath)\n",
    "        return filepath\n",
    "    \n",
    "    def load_images(fp):\n",
    "        with gzip.open(fp, 'rb') as f:\n",
    "            f.read(16)\n",
    "            return np.frombuffer(f.read(), dtype=np.uint8).reshape(-1, 784).astype(np.float32) / 255.0\n",
    "    \n",
    "    def load_labels(fp):\n",
    "        with gzip.open(fp, 'rb') as f:\n",
    "            f.read(8)\n",
    "            return np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    \n",
    "    return (load_images(download(files['train_images'])),\n",
    "            load_labels(download(files['train_labels'])),\n",
    "            load_images(download(files['test_images'])),\n",
    "            load_labels(download(files['test_labels'])))\n",
    "\n",
    "X_train_np, y_train_np, X_test_np, y_test_np = load_mnist()\n",
    "print(f\"Loaded {len(X_train_np)} training samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: NumPy Implementation (CPU Baseline)\n",
    "\n",
    "First, let's establish our CPU baseline using the NumPy implementation from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumPyMLP:\n",
    "    \"\"\"\n",
    "    Our NumPy MLP from Notebook 01 - runs on CPU.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes):\n",
    "        self.layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            W = np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float32) * np.sqrt(2.0 / layer_sizes[i])\n",
    "            b = np.zeros(layer_sizes[i + 1], dtype=np.float32)\n",
    "            self.layers.append({'W': W, 'b': b, 'cache': {}})\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = X\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            layer['cache']['X'] = out\n",
    "            out = out @ layer['W'] + layer['b']\n",
    "            layer['cache']['Z'] = out\n",
    "            out = np.maximum(0, out)  # ReLU\n",
    "        \n",
    "        self.layers[-1]['cache']['X'] = out\n",
    "        out = out @ self.layers[-1]['W'] + self.layers[-1]['b']\n",
    "        \n",
    "        # Softmax\n",
    "        out_shifted = out - np.max(out, axis=1, keepdims=True)\n",
    "        exp_out = np.exp(out_shifted)\n",
    "        self.probs = exp_out / np.sum(exp_out, axis=1, keepdims=True)\n",
    "        return self.probs\n",
    "    \n",
    "    def backward(self, targets, lr):\n",
    "        batch_size = len(targets)\n",
    "        grad = self.probs.copy()\n",
    "        grad[np.arange(batch_size), targets] -= 1\n",
    "        \n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            layer = self.layers[i]\n",
    "            X = layer['cache']['X']\n",
    "            \n",
    "            dW = X.T @ grad / batch_size\n",
    "            db = np.mean(grad, axis=0)\n",
    "            grad = grad @ layer['W'].T\n",
    "            \n",
    "            if i > 0:\n",
    "                Z = self.layers[i - 1]['cache']['Z']\n",
    "                grad = grad * (Z > 0)\n",
    "            \n",
    "            layer['W'] -= lr * dW\n",
    "            layer['b'] -= lr * db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.forward(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_numpy(model, X_train, y_train, epochs, batch_size, lr):\n",
    "    \"\"\"Train NumPy model and return time.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        indices = np.random.permutation(len(X_train))\n",
    "        for start in range(0, len(X_train), batch_size):\n",
    "            batch_idx = indices[start:start + batch_size]\n",
    "            X_batch = X_train[batch_idx]\n",
    "            y_batch = y_train[batch_idx]\n",
    "            \n",
    "            model.forward(X_batch)\n",
    "            model.backward(y_batch, lr)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    return elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: PyTorch Implementation\n",
    "\n",
    "Now let's create the same model in PyTorch, which can run on CPU or GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE:\n",
    "    class PyTorchMLP(nn.Module):\n",
    "        \"\"\"\n",
    "        Same architecture as NumPyMLP, but in PyTorch.\n",
    "        \n",
    "        PyTorch handles:\n",
    "        - Automatic differentiation (no manual backward!)\n",
    "        - GPU acceleration (just move to device)\n",
    "        - Optimized kernels (cuDNN)\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, layer_sizes):\n",
    "            super().__init__()\n",
    "            \n",
    "            layers = []\n",
    "            for i in range(len(layer_sizes) - 1):\n",
    "                layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "                if i < len(layer_sizes) - 2:\n",
    "                    layers.append(nn.ReLU())\n",
    "            \n",
    "            self.model = nn.Sequential(*layers)\n",
    "            \n",
    "            # Initialize weights like NumPy version (He initialization)\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                    nn.init.zeros_(m.bias)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "    \n",
    "    print(\"‚úÖ PyTorch MLP class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE:\n",
    "    def train_pytorch(model, train_loader, epochs, lr, device):\n",
    "        \"\"\"Train PyTorch model and return time.\"\"\"\n",
    "        model = model.to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        \n",
    "        # Warmup (important for GPU timing!)\n",
    "        if device.type == 'cuda':\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                break\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Ensure GPU operations are complete\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        return elapsed\n",
    "    \n",
    "    print(\"‚úÖ Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: CPU vs GPU Comparison\n",
    "\n",
    "Let's measure the speedup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.1\n",
    "ARCHITECTURE = [784, 256, 128, 10]\n",
    "\n",
    "print(\"üèéÔ∏è CPU vs GPU Comparison\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Architecture: {ARCHITECTURE}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Training samples: {len(X_train_np)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. NumPy (CPU)\n",
    "print(\"\\n1Ô∏è‚É£ NumPy (CPU)...\")\n",
    "np.random.seed(42)\n",
    "model_numpy = NumPyMLP(ARCHITECTURE)\n",
    "time_numpy = train_numpy(model_numpy, X_train_np, y_train_np, EPOCHS, BATCH_SIZE, LR)\n",
    "acc_numpy = np.mean(model_numpy.predict(X_test_np) == y_test_np)\n",
    "print(f\"   Time: {time_numpy:.2f}s\")\n",
    "print(f\"   Accuracy: {acc_numpy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if PYTORCH_AVAILABLE:\n    # Prepare PyTorch data\n    X_train_torch = torch.FloatTensor(X_train_np)\n    y_train_torch = torch.LongTensor(y_train_np)\n    X_test_torch = torch.FloatTensor(X_test_np)\n    y_test_torch = torch.LongTensor(y_test_np)\n    \n    train_dataset = TensorDataset(X_train_torch, y_train_torch)\n    \n    # DataLoader options explained:\n    # - num_workers: Number of subprocesses for data loading (requires --ipc=host in Docker)\n    #   Use 0 for simple datasets, 2-4 for larger datasets with transforms\n    # - pin_memory: Copies data to CUDA pinned memory for faster CPU‚ÜíGPU transfer\n    #   Less important on DGX Spark due to unified memory architecture\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=BATCH_SIZE, \n        shuffle=True,\n        # num_workers=2,    # Uncomment for faster loading (requires --ipc=host)\n        # pin_memory=True,  # Uncomment for faster GPU transfer (less important on DGX Spark)\n    )\n    \n    # 2. PyTorch (CPU)\n    print(\"\\n2. PyTorch (CPU)...\")\n    torch.manual_seed(42)\n    model_cpu = PyTorchMLP(ARCHITECTURE)\n    device_cpu = torch.device('cpu')\n    time_pytorch_cpu = train_pytorch(model_cpu, train_loader, EPOCHS, LR, device_cpu)\n    \n    model_cpu.eval()\n    with torch.no_grad():\n        preds_cpu = model_cpu(X_test_torch).argmax(dim=1)\n        acc_pytorch_cpu = (preds_cpu == y_test_torch).float().mean().item()\n    \n    print(f\"   Time: {time_pytorch_cpu:.2f}s\")\n    print(f\"   Accuracy: {acc_pytorch_cpu:.2%}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE and torch.cuda.is_available():\n",
    "    # 3. PyTorch (GPU)\n",
    "    print(\"\\n3Ô∏è‚É£ PyTorch (GPU)...\")\n",
    "    torch.manual_seed(42)\n",
    "    model_gpu = PyTorchMLP(ARCHITECTURE)\n",
    "    device_gpu = torch.device('cuda')\n",
    "    time_pytorch_gpu = train_pytorch(model_gpu, train_loader, EPOCHS, LR, device_gpu)\n",
    "    \n",
    "    model_gpu.eval()\n",
    "    with torch.no_grad():\n",
    "        preds_gpu = model_gpu(X_test_torch.to(device_gpu)).argmax(dim=1).cpu()\n",
    "        acc_pytorch_gpu = (preds_gpu == y_test_torch).float().mean().item()\n",
    "    \n",
    "    print(f\"   Time: {time_pytorch_gpu:.2f}s\")\n",
    "    print(f\"   Accuracy: {acc_pytorch_gpu:.2%}\")\n",
    "else:\n",
    "    print(\"\\n3Ô∏è‚É£ PyTorch (GPU)... SKIPPED (no GPU available)\")\n",
    "    time_pytorch_gpu = None\n",
    "    acc_pytorch_gpu = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"                         RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n{'Method':<20} {'Time (s)':<12} {'Speedup':<12} {'Accuracy'}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'NumPy (CPU)':<20} {time_numpy:<12.2f} {'1.0x (baseline)':<12} {acc_numpy:.2%}\")\n",
    "\n",
    "if PYTORCH_AVAILABLE:\n",
    "    speedup_cpu = time_numpy / time_pytorch_cpu\n",
    "    print(f\"{'PyTorch (CPU)':<20} {time_pytorch_cpu:<12.2f} {speedup_cpu:.1f}x{'':<8} {acc_pytorch_cpu:.2%}\")\n",
    "    \n",
    "    if time_pytorch_gpu:\n",
    "        speedup_gpu = time_numpy / time_pytorch_gpu\n",
    "        print(f\"{'PyTorch (GPU)':<20} {time_pytorch_gpu:<12.2f} {speedup_gpu:.1f}x{'':<8} {acc_pytorch_gpu:.2%}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "if PYTORCH_AVAILABLE:\n",
    "    methods = ['NumPy (CPU)', 'PyTorch (CPU)']\n",
    "    times = [time_numpy, time_pytorch_cpu]\n",
    "    colors = ['#FF6B6B', '#4ECDC4']\n",
    "    \n",
    "    if time_pytorch_gpu:\n",
    "        methods.append('PyTorch (GPU)')\n",
    "        times.append(time_pytorch_gpu)\n",
    "        colors.append('#45B7D1')\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Time comparison\n",
    "    bars = axes[0].bar(methods, times, color=colors, edgecolor='black', linewidth=1.5)\n",
    "    axes[0].set_ylabel('Time (seconds)', fontsize=12)\n",
    "    axes[0].set_title('Training Time Comparison', fontsize=14)\n",
    "    for bar, t in zip(bars, times):\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                     f'{t:.2f}s', ha='center', fontsize=11)\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Speedup comparison\n",
    "    speedups = [1.0, time_numpy/time_pytorch_cpu]\n",
    "    if time_pytorch_gpu:\n",
    "        speedups.append(time_numpy/time_pytorch_gpu)\n",
    "    \n",
    "    bars = axes[1].bar(methods, speedups, color=colors, edgecolor='black', linewidth=1.5)\n",
    "    axes[1].set_ylabel('Speedup (vs NumPy)', fontsize=12)\n",
    "    axes[1].set_title('Speedup Factor', fontsize=14)\n",
    "    axes[1].axhline(y=1, color='red', linestyle='--', label='Baseline')\n",
    "    for bar, s in zip(bars, speedups):\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                     f'{s:.1f}x', ha='center', fontsize=11)\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Finding Optimal Batch Size for DGX Spark\n",
    "\n",
    "Batch size significantly affects training speed. Let's find the optimal value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE and torch.cuda.is_available():\n",
    "    print(\"üî¨ Finding Optimal Batch Size for DGX Spark\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    batch_sizes = [32, 64, 128, 256, 512, 1024, 2048]\n",
    "    gpu_times = []\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.manual_seed(42)\n",
    "            \n",
    "            train_loader = DataLoader(\n",
    "                TensorDataset(X_train_torch, y_train_torch), \n",
    "                batch_size=batch_size, \n",
    "                shuffle=True\n",
    "            )\n",
    "            \n",
    "            model = PyTorchMLP(ARCHITECTURE)\n",
    "            t = train_pytorch(model, train_loader, epochs=1, lr=0.1, device=torch.device('cuda'))\n",
    "            gpu_times.append(t)\n",
    "            \n",
    "            print(f\"Batch size {batch_size:4d}: {t:.3f}s\")\n",
    "        except RuntimeError as e:\n",
    "            if 'out of memory' in str(e):\n",
    "                print(f\"Batch size {batch_size:4d}: OUT OF MEMORY\")\n",
    "                gpu_times.append(None)\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    # Find optimal\n",
    "    valid_times = [(bs, t) for bs, t in zip(batch_sizes, gpu_times) if t is not None]\n",
    "    if valid_times:\n",
    "        optimal_bs, optimal_time = min(valid_times, key=lambda x: x[1])\n",
    "        print(f\"\\n‚úÖ Optimal batch size for speed: {optimal_bs}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU not available. Batch size optimization skipped.\")\n",
    "    batch_sizes = []\n",
    "    gpu_times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if batch_sizes and any(t is not None for t in gpu_times):\n",
    "    # Visualize batch size impact\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    valid_bs = [bs for bs, t in zip(batch_sizes, gpu_times) if t is not None]\n",
    "    valid_times = [t for t in gpu_times if t is not None]\n",
    "    \n",
    "    ax.plot(valid_bs, valid_times, 'bo-', linewidth=2, markersize=10)\n",
    "    ax.set_xlabel('Batch Size', fontsize=12)\n",
    "    ax.set_ylabel('Time per Epoch (seconds)', fontsize=12)\n",
    "    ax.set_title('Batch Size vs Training Speed (GPU)', fontsize=14)\n",
    "    ax.set_xscale('log', base=2)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mark optimal\n",
    "    min_idx = valid_times.index(min(valid_times))\n",
    "    ax.scatter([valid_bs[min_idx]], [valid_times[min_idx]], color='red', s=200, zorder=5, label='Optimal')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: DGX Spark's Unified Memory Advantage\n",
    "\n",
    "One of DGX Spark's key features is its **unified memory architecture**. Let's understand why this matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"                    DGX SPARK'S UNIFIED MEMORY ADVANTAGE\")\nprint(\"=\" * 80)\n\nprint(\"\"\"\nTraditional GPU System:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ    CPU      ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ PCIe ‚îÄ‚îÄ‚îÄ‚îÄ>   ‚îÇ    GPU      ‚îÇ\n‚îÇ   32GB RAM  ‚îÇ  <‚îÄ‚îÄ Transfer ‚îÄ‚îÄ   ‚îÇ   8GB VRAM  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     (SLOW!)        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nProblem: Moving data between CPU and GPU memory is slow!\n- PCIe bandwidth: ~16 GB/s\n- For a 70B model: Would need ~140GB, but GPU only has 8GB!\n- Constant swapping = very slow inference\n\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nDGX Spark (Unified Memory):\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                      128GB UNIFIED MEMORY (LPDDR5X)                           ‚îÇ\n‚îÇ              (CPU and GPU share the same memory pool!)                        ‚îÇ\n‚îÇ                                                                               ‚îÇ\n‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ\n‚îÇ     ‚îÇ Grace CPU       ‚îÇ<‚îÄ‚îÄ 273 GB/s ‚îÄ‚îÄ‚îÄ>   ‚îÇ Blackwell GPU   ‚îÇ               ‚îÇ\n‚îÇ     ‚îÇ 20 ARM v9.2     ‚îÇ    unified         ‚îÇ 6,144 CUDA cores‚îÇ               ‚îÇ\n‚îÇ     ‚îÇ cores           ‚îÇ    bandwidth       ‚îÇ 192 Tensor cores‚îÇ               ‚îÇ\n‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nDGX Spark Hardware Specifications:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Component              ‚îÇ Specification                                       ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ GPU                    ‚îÇ NVIDIA Blackwell GB10 Superchip                     ‚îÇ\n‚îÇ CPU                    ‚îÇ 20 ARM v9.2 cores (10 Cortex-X925 + 10 Cortex-A725) ‚îÇ\n‚îÇ Memory                 ‚îÇ 128GB unified memory (LPDDR5X)                      ‚îÇ\n‚îÇ Memory Bandwidth       ‚îÇ 273 GB/s                                            ‚îÇ\n‚îÇ CUDA Cores             ‚îÇ 6,144                                               ‚îÇ\n‚îÇ Tensor Cores           ‚îÇ 192 (5th generation)                                ‚îÇ\n‚îÇ NVFP4 Performance      ‚îÇ 1 PFLOP                                             ‚îÇ\n‚îÇ FP8 Performance        ‚îÇ ~209 TFLOPS                                         ‚îÇ\n‚îÇ BF16 Performance       ‚îÇ ~100 TFLOPS                                         ‚îÇ\n‚îÇ Architecture           ‚îÇ ARM64/aarch64                                       ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nAdvantage:\n- No data transfer needed between CPU and GPU\n- 70B model fits entirely in memory!\n- Memory bandwidth: 273 GB/s (LPDDR5X)\n- Perfect for large language models\n\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nWhat this means for you:\n1. Load models once, no swapping\n2. Larger batch sizes possible\n3. Run models that wouldn't fit on traditional GPUs\n4. Faster iteration during development\n\"\"\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Best Practices Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"                    GPU TRAINING BEST PRACTICES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "1. USE APPROPRIATE BATCH SIZES\n",
    "   - Too small: GPU underutilized, slower training\n",
    "   - Too large: May hurt generalization, memory issues\n",
    "   - Sweet spot: Usually 64-512 for most tasks\n",
    "   - DGX Spark: Can go larger (1024+) thanks to unified memory\n",
    "\n",
    "2. USE PROPER DATA TYPES\n",
    "   - float32: Default, good balance\n",
    "   - float16/bfloat16: 2x memory savings, often same accuracy\n",
    "   - DGX Spark: Native bfloat16 support in Blackwell\n",
    "\n",
    "3. PIN MEMORY FOR DATA LOADING\n",
    "   ```python\n",
    "   DataLoader(..., pin_memory=True)  # Faster CPU‚ÜíGPU transfer\n",
    "   ```\n",
    "   Note: Less important on DGX Spark due to unified memory!\n",
    "\n",
    "4. USE CUDA STREAMS FOR OVERLAP\n",
    "   - Overlap data loading with computation\n",
    "   - PyTorch does this automatically with DataLoader workers\n",
    "\n",
    "5. PROFILE YOUR CODE\n",
    "   ```python\n",
    "   with torch.profiler.profile() as prof:\n",
    "       model(x)\n",
    "   print(prof.key_averages().table())\n",
    "   ```\n",
    "\n",
    "6. CLEAR CACHE BEFORE LARGE MODELS\n",
    "   ```bash\n",
    "   sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'\n",
    "   ```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Part 8: Mixed Precision Training with torch.cuda.amp\n\n### What is Mixed Precision?\n\nMixed precision training uses a combination of float16 and float32 data types:\n- **Forward pass**: Use float16 (faster, less memory)\n- **Gradients**: Computed in float16\n- **Weight updates**: Done in float32 (maintains precision)\n\nThis gives you:\n- **~2x faster training** on Tensor Cores\n- **~2x less memory usage**\n- **Same accuracy** as float32 training\n\n### How torch.cuda.amp Works\n\nPyTorch's Automatic Mixed Precision (AMP) provides two key components:\n\n1. **`torch.autocast`**: Automatically casts operations to float16 where safe\n2. **`GradScaler`**: Scales gradients to prevent underflow in float16",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "if PYTORCH_AVAILABLE and torch.cuda.is_available():\n    from torch.cuda.amp import autocast, GradScaler\n\n    print(\"üî¨ Mixed Precision Training Demo\")\n    print(\"=\" * 60)\n\n    def train_mixed_precision(model, train_loader, epochs, lr, device):\n        \"\"\"Train with automatic mixed precision.\"\"\"\n        model = model.to(device)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.SGD(model.parameters(), lr=lr)\n\n        # GradScaler helps prevent gradient underflow in float16\n        scaler = GradScaler()\n\n        # Warmup\n        for X_batch, y_batch in train_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n\n            # autocast automatically uses float16 where appropriate\n            with autocast(device_type='cuda', dtype=torch.float16):\n                outputs = model(X_batch)\n                loss = criterion(outputs, y_batch)\n\n            # Scale loss and backward\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            break\n        torch.cuda.synchronize()\n\n        start_time = time.time()\n\n        for epoch in range(epochs):\n            for X_batch, y_batch in train_loader:\n                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n\n                optimizer.zero_grad()\n\n                # Forward pass with autocast\n                with autocast(device_type='cuda', dtype=torch.float16):\n                    outputs = model(X_batch)\n                    loss = criterion(outputs, y_batch)\n\n                # Backward pass with gradient scaling\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n\n        torch.cuda.synchronize()\n        return time.time() - start_time\n\n    # Compare FP32 vs Mixed Precision\n    torch.manual_seed(42)\n    model_fp32 = PyTorchMLP(ARCHITECTURE)\n    train_loader_amp = DataLoader(\n        TensorDataset(X_train_torch, y_train_torch),\n        batch_size=BATCH_SIZE,\n        shuffle=True\n    )\n    time_fp32 = train_pytorch(model_fp32, train_loader_amp, EPOCHS, LR, torch.device('cuda'))\n\n    torch.manual_seed(42)\n    model_amp = PyTorchMLP(ARCHITECTURE)\n    time_amp = train_mixed_precision(model_amp, train_loader_amp, EPOCHS, LR, torch.device('cuda'))\n\n    print(f\"\\nResults ({EPOCHS} epochs):\")\n    print(f\"   FP32:            {time_fp32:.3f}s\")\n    print(f\"   Mixed Precision: {time_amp:.3f}s\")\n    print(f\"   Speedup:         {time_fp32/time_amp:.2f}x\")\n\n    # Check accuracy\n    model_amp.eval()\n    with torch.no_grad():\n        preds = model_amp(X_test_torch.to('cuda')).argmax(dim=1).cpu()\n        acc = (preds == y_test_torch).float().mean().item()\n    print(f\"   Accuracy:        {acc:.2%}\")\nelse:\n    print(\"‚ö†Ô∏è GPU not available. Mixed precision demo skipped.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Train a Larger Model\n",
    "\n",
    "Train a model with architecture `[784, 1024, 512, 256, 128, 10]` and compare CPU vs GPU times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Train larger model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 2: Try bfloat16 Mixed Precision\n\nNow that you've seen float16 mixed precision in Part 8, try using **bfloat16** instead.\n\n**Why bfloat16?**\n- DGX Spark's Blackwell GPU has native bfloat16 support\n- bfloat16 has better numerical stability than float16\n- **Bonus:** bfloat16 doesn't require GradScaler!\n\n<details>\n<summary>üí° Hint: bfloat16 simplifies training</summary>\n\nWith bfloat16, you can skip the GradScaler entirely:\n\n```python\n# No scaler needed with bfloat16!\nfor X_batch, y_batch in train_loader:\n    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n    \n    optimizer.zero_grad()\n    \n    with autocast(device_type='cuda', dtype=torch.bfloat16):\n        outputs = model(X_batch)\n        loss = criterion(outputs, y_batch)\n    \n    loss.backward()  # No scaler.scale() needed!\n    optimizer.step()\n```\n</details>\n\n**Your task:** Modify the `train_mixed_precision` function to use bfloat16 and compare speed/accuracy with float16."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Implement mixed precision training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## üéâ Checkpoint\n\nYou've learned:\n\n- ‚úÖ How to port NumPy code to PyTorch\n- ‚úÖ The dramatic speedup from GPU acceleration\n- ‚úÖ How to find optimal batch sizes\n- ‚úÖ Why DGX Spark's unified memory is special\n- ‚úÖ Mixed precision training with torch.cuda.amp (autocast + GradScaler)\n- ‚úÖ Best practices for GPU training\n\n---\n\n## üìñ Further Reading\n\n- [PyTorch Performance Tuning Guide](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)\n- [NVIDIA DGX Spark Documentation](https://docs.nvidia.com/dgx/)\n- [Mixed Precision Training](https://pytorch.org/docs/stable/amp.html)\n\n---\n\n## üßπ Cleanup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import gc\n\nif PYTORCH_AVAILABLE and torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\ngc.collect()\n\nprint(\"‚úÖ Cleanup complete!\")\nprint(\"\\nüéâ Congratulations! You've completed Module 1.5: Neural Network Fundamentals!\")\nprint(\"\\nüéØ Next: Proceed to Module 1.6: Classical ML Foundations\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}