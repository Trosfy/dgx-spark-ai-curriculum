{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.5.4 Solution: Normalization Comparison\n",
    "\n",
    "This notebook contains solutions to the exercises from Lab 1.5.4.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Group Normalization Implementation\n",
    "\n",
    "Group Normalization divides channels into groups and normalizes within each group.\n",
    "It's a middle ground between LayerNorm (1 group) and InstanceNorm (C groups)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupNorm:\n",
    "    \"\"\"\n",
    "    Group Normalization implementation.\n",
    "    \n",
    "    Divides channels into groups and normalizes within each group.\n",
    "    Works well with small batch sizes where BatchNorm struggles.\n",
    "    \n",
    "    Args:\n",
    "        num_groups: Number of groups to divide channels into\n",
    "        num_channels: Number of channels (must be divisible by num_groups)\n",
    "        eps: Small constant for numerical stability\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_groups: int, num_channels: int, eps: float = 1e-5):\n",
    "        assert num_channels % num_groups == 0, \"num_channels must be divisible by num_groups\"\n",
    "        \n",
    "        self.num_groups = num_groups\n",
    "        self.num_channels = num_channels\n",
    "        self.eps = eps\n",
    "        self.channels_per_group = num_channels // num_groups\n",
    "        \n",
    "        # Learnable parameters (per channel)\n",
    "        self.gamma = np.ones(num_channels)\n",
    "        self.beta = np.zeros(num_channels)\n",
    "        \n",
    "        # Gradient accumulators\n",
    "        self.dgamma = np.zeros_like(self.gamma)\n",
    "        self.dbeta = np.zeros_like(self.beta)\n",
    "        \n",
    "        # Cache for backward pass\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            X: Input of shape (batch_size, num_channels) or (batch_size, num_channels, height, width)\n",
    "        \n",
    "        Returns:\n",
    "            Normalized output of same shape\n",
    "        \"\"\"\n",
    "        self.input_shape = X.shape\n",
    "        batch_size = X.shape[0]\n",
    "        \n",
    "        # Handle both 2D (batch, channels) and 4D (batch, channels, height, width)\n",
    "        if X.ndim == 2:\n",
    "            # Reshape to (batch, groups, channels_per_group)\n",
    "            X_grouped = X.reshape(batch_size, self.num_groups, self.channels_per_group)\n",
    "        else:\n",
    "            # 4D input: (batch, channels, height, width)\n",
    "            _, C, H, W = X.shape\n",
    "            X_grouped = X.reshape(batch_size, self.num_groups, self.channels_per_group, H, W)\n",
    "        \n",
    "        # Compute mean and variance per group\n",
    "        axis = tuple(range(2, X_grouped.ndim))  # All axes except batch and group\n",
    "        mean = X_grouped.mean(axis=axis, keepdims=True)\n",
    "        var = X_grouped.var(axis=axis, keepdims=True)\n",
    "        \n",
    "        # Normalize\n",
    "        X_norm = (X_grouped - mean) / np.sqrt(var + self.eps)\n",
    "        \n",
    "        # Reshape back\n",
    "        if X.ndim == 2:\n",
    "            X_norm = X_norm.reshape(batch_size, self.num_channels)\n",
    "            out = self.gamma * X_norm + self.beta\n",
    "        else:\n",
    "            X_norm = X_norm.reshape(batch_size, C, H, W)\n",
    "            out = self.gamma.reshape(1, C, 1, 1) * X_norm + self.beta.reshape(1, C, 1, 1)\n",
    "        \n",
    "        # Cache for backward\n",
    "        self.cache = {\n",
    "            'X_grouped': X_grouped,\n",
    "            'X_norm': X_norm if X.ndim == 2 else X_norm.reshape(batch_size, self.num_channels, -1),\n",
    "            'mean': mean,\n",
    "            'var': var,\n",
    "        }\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass.\n",
    "        \n",
    "        Args:\n",
    "            dout: Gradient of loss w.r.t. output\n",
    "        \n",
    "        Returns:\n",
    "            Gradient of loss w.r.t. input\n",
    "        \"\"\"\n",
    "        X_grouped = self.cache['X_grouped']\n",
    "        X_norm = self.cache['X_norm']\n",
    "        mean = self.cache['mean']\n",
    "        var = self.cache['var']\n",
    "        \n",
    "        batch_size = dout.shape[0]\n",
    "        \n",
    "        if dout.ndim == 2:\n",
    "            # Gradients for gamma and beta\n",
    "            self.dgamma = np.sum(dout * X_norm, axis=0)\n",
    "            self.dbeta = np.sum(dout, axis=0)\n",
    "            \n",
    "            # Gradient w.r.t. normalized input\n",
    "            dX_norm = dout * self.gamma\n",
    "            dX_norm_grouped = dX_norm.reshape(batch_size, self.num_groups, self.channels_per_group)\n",
    "        else:\n",
    "            _, C, H, W = dout.shape\n",
    "            # Gradients for gamma and beta\n",
    "            X_norm_flat = X_norm.reshape(batch_size, C, -1)\n",
    "            dout_flat = dout.reshape(batch_size, C, -1)\n",
    "            self.dgamma = np.sum(np.sum(dout_flat * X_norm_flat, axis=0), axis=1)\n",
    "            self.dbeta = np.sum(np.sum(dout_flat, axis=0), axis=1)\n",
    "            \n",
    "            # Gradient w.r.t. normalized input\n",
    "            dX_norm = dout * self.gamma.reshape(1, C, 1, 1)\n",
    "            dX_norm_grouped = dX_norm.reshape(batch_size, self.num_groups, self.channels_per_group, H, W)\n",
    "        \n",
    "        # Standard normalization backward pass\n",
    "        std = np.sqrt(var + self.eps)\n",
    "        axis = tuple(range(2, X_grouped.ndim))\n",
    "        N = np.prod([X_grouped.shape[i] for i in axis])\n",
    "        \n",
    "        dvar = np.sum(dX_norm_grouped * (X_grouped - mean) * -0.5 * (var + self.eps)**(-1.5), axis=axis, keepdims=True)\n",
    "        dmean = np.sum(dX_norm_grouped * -1/std, axis=axis, keepdims=True) + dvar * np.mean(-2 * (X_grouped - mean), axis=axis, keepdims=True)\n",
    "        \n",
    "        dX_grouped = dX_norm_grouped / std + dvar * 2 * (X_grouped - mean) / N + dmean / N\n",
    "        \n",
    "        # Reshape back to original shape\n",
    "        dX = dX_grouped.reshape(self.input_shape)\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [(self.gamma, self.dgamma), (self.beta, self.dbeta)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GroupNorm implementation\n",
    "print(\"Testing Group Normalization\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test with 2D input (batch, channels)\n",
    "batch_size = 4\n",
    "num_channels = 8\n",
    "num_groups = 2  # 4 channels per group\n",
    "\n",
    "gn = GroupNorm(num_groups=num_groups, num_channels=num_channels)\n",
    "X = np.random.randn(batch_size, num_channels) * 5 + 3  # Non-zero mean, large variance\n",
    "\n",
    "out = gn.forward(X)\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"\\nInput stats (per sample):\")\n",
    "print(f\"  Mean: {X.mean(axis=1)}\")\n",
    "print(f\"  Std:  {X.std(axis=1)}\")\n",
    "print(f\"\\nOutput stats (per sample, should be ~0 mean, ~1 std):\")\n",
    "print(f\"  Mean: {out.mean(axis=1)}\")\n",
    "print(f\"  Std:  {out.std(axis=1)}\")\n",
    "\n",
    "# Verify gradients with numerical check\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Gradient check...\")\n",
    "\n",
    "dout = np.random.randn(*out.shape)\n",
    "dX = gn.backward(dout)\n",
    "\n",
    "# Numerical gradient\n",
    "eps = 1e-5\n",
    "numerical_grad = np.zeros_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        X_plus = X.copy()\n",
    "        X_plus[i, j] += eps\n",
    "        out_plus = gn.forward(X_plus)\n",
    "        \n",
    "        X_minus = X.copy()\n",
    "        X_minus[i, j] -= eps\n",
    "        out_minus = gn.forward(X_minus)\n",
    "        \n",
    "        numerical_grad[i, j] = np.sum((out_plus - out_minus) * dout) / (2 * eps)\n",
    "\n",
    "# Reset and compute analytical gradient\n",
    "gn.forward(X)\n",
    "dX = gn.backward(dout)\n",
    "\n",
    "error = np.max(np.abs(dX - numerical_grad))\n",
    "print(f\"Max gradient error: {error:.2e}\")\n",
    "print(f\"Gradient check: {'PASSED' if error < 1e-4 else 'FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Normalization Method Comparison\n",
    "\n",
    "Compare all normalization methods on the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import or define all normalization layers\n",
    "class BatchNorm:\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        self.gamma = np.ones(num_features)\n",
    "        self.beta = np.zeros(num_features)\n",
    "        self.running_mean = np.zeros(num_features)\n",
    "        self.running_var = np.ones(num_features)\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, X):\n",
    "        if self.training:\n",
    "            mean = X.mean(axis=0)\n",
    "            var = X.var(axis=0)\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "        \n",
    "        X_norm = (X - mean) / np.sqrt(var + self.eps)\n",
    "        self.cache = {'X': X, 'X_norm': X_norm, 'mean': mean, 'var': var}\n",
    "        return self.gamma * X_norm + self.beta\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        X, X_norm, mean, var = self.cache['X'], self.cache['X_norm'], self.cache['mean'], self.cache['var']\n",
    "        N = X.shape[0]\n",
    "        self.dgamma = np.sum(dout * X_norm, axis=0)\n",
    "        self.dbeta = np.sum(dout, axis=0)\n",
    "        dX_norm = dout * self.gamma\n",
    "        std = np.sqrt(var + self.eps)\n",
    "        dX = (1/N) * (1/std) * (N * dX_norm - np.sum(dX_norm, axis=0) - X_norm * np.sum(dX_norm * X_norm, axis=0))\n",
    "        return dX\n",
    "\n",
    "\n",
    "class LayerNorm:\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        self.eps = eps\n",
    "        self.gamma = np.ones(normalized_shape)\n",
    "        self.beta = np.zeros(normalized_shape)\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, X):\n",
    "        mean = X.mean(axis=-1, keepdims=True)\n",
    "        var = X.var(axis=-1, keepdims=True)\n",
    "        X_norm = (X - mean) / np.sqrt(var + self.eps)\n",
    "        self.cache = {'X': X, 'X_norm': X_norm, 'mean': mean, 'var': var}\n",
    "        return self.gamma * X_norm + self.beta\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        X, X_norm, var = self.cache['X'], self.cache['X_norm'], self.cache['var']\n",
    "        N = X.shape[-1]\n",
    "        self.dgamma = np.sum(dout * X_norm, axis=0)\n",
    "        self.dbeta = np.sum(dout, axis=0)\n",
    "        dX_norm = dout * self.gamma\n",
    "        std = np.sqrt(var + self.eps)\n",
    "        dX = (1/N) * (1/std) * (N * dX_norm - np.sum(dX_norm, axis=-1, keepdims=True) - X_norm * np.sum(dX_norm * X_norm, axis=-1, keepdims=True))\n",
    "        return dX\n",
    "\n",
    "\n",
    "class RMSNorm:\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        self.eps = eps\n",
    "        self.gamma = np.ones(normalized_shape)\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, X):\n",
    "        rms = np.sqrt(np.mean(X**2, axis=-1, keepdims=True) + self.eps)\n",
    "        X_norm = X / rms\n",
    "        self.cache = {'X': X, 'X_norm': X_norm, 'rms': rms}\n",
    "        return self.gamma * X_norm\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        X, X_norm, rms = self.cache['X'], self.cache['X_norm'], self.cache['rms']\n",
    "        N = X.shape[-1]\n",
    "        self.dgamma = np.sum(dout * X_norm, axis=0)\n",
    "        dX_norm = dout * self.gamma\n",
    "        dX = (1/rms) * (dX_norm - X_norm * np.mean(dX_norm * X_norm, axis=-1, keepdims=True))\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "import gzip, os, urllib.request\n",
    "\n",
    "def load_mnist(path='../data'):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    base_url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = ['train-images-idx3-ubyte.gz', 'train-labels-idx1-ubyte.gz',\n",
    "             't10k-images-idx3-ubyte.gz', 't10k-labels-idx1-ubyte.gz']\n",
    "    for f in files:\n",
    "        fp = os.path.join(path, f)\n",
    "        if not os.path.exists(fp): urllib.request.urlretrieve(base_url + f, fp)\n",
    "    def load_img(fp): \n",
    "        with gzip.open(fp) as f: f.read(16); return np.frombuffer(f.read(), np.uint8).reshape(-1,784).astype(np.float32)/255\n",
    "    def load_lbl(fp): \n",
    "        with gzip.open(fp) as f: f.read(8); return np.frombuffer(f.read(), np.uint8)\n",
    "    return (load_img(os.path.join(path, files[0])), load_lbl(os.path.join(path, files[1])),\n",
    "            load_img(os.path.join(path, files[2])), load_lbl(os.path.join(path, files[3])))\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_mnist()\n",
    "X_train, y_train = X_train[:5000], y_train[:5000]  # Use subset for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedMLP:\n",
    "    \"\"\"MLP with configurable normalization.\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes, norm_type='none'):\n",
    "        self.norm_type = norm_type\n",
    "        self.layers = []\n",
    "        self.norms = []\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            W = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2.0 / layer_sizes[i])\n",
    "            b = np.zeros(layer_sizes[i+1])\n",
    "            self.layers.append({'W': W, 'b': b, 'cache': {}})\n",
    "            \n",
    "            # Add normalization layer (except for output layer)\n",
    "            if i < len(layer_sizes) - 2:\n",
    "                if norm_type == 'batch':\n",
    "                    self.norms.append(BatchNorm(layer_sizes[i+1]))\n",
    "                elif norm_type == 'layer':\n",
    "                    self.norms.append(LayerNorm(layer_sizes[i+1]))\n",
    "                elif norm_type == 'rms':\n",
    "                    self.norms.append(RMSNorm(layer_sizes[i+1]))\n",
    "                elif norm_type == 'group':\n",
    "                    num_groups = min(4, layer_sizes[i+1])  # Use 4 groups or fewer\n",
    "                    while layer_sizes[i+1] % num_groups != 0:\n",
    "                        num_groups -= 1\n",
    "                    self.norms.append(GroupNorm(num_groups, layer_sizes[i+1]))\n",
    "                else:\n",
    "                    self.norms.append(None)\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        out = X\n",
    "        \n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            layer['cache']['X'] = out\n",
    "            out = out @ layer['W'] + layer['b']\n",
    "            layer['cache']['Z'] = out\n",
    "            \n",
    "            # Apply normalization\n",
    "            if self.norms[i] is not None:\n",
    "                if hasattr(self.norms[i], 'training'):\n",
    "                    self.norms[i].training = training\n",
    "                out = self.norms[i].forward(out)\n",
    "            \n",
    "            out = np.maximum(0, out)  # ReLU\n",
    "            layer['cache']['A'] = out\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers[-1]['cache']['X'] = out\n",
    "        out = out @ self.layers[-1]['W'] + self.layers[-1]['b']\n",
    "        \n",
    "        # Softmax\n",
    "        out_shifted = out - np.max(out, axis=1, keepdims=True)\n",
    "        exp_out = np.exp(out_shifted)\n",
    "        self.probs = exp_out / np.sum(exp_out, axis=1, keepdims=True)\n",
    "        \n",
    "        return self.probs\n",
    "    \n",
    "    def backward(self, targets, lr):\n",
    "        batch_size = len(targets)\n",
    "        grad = self.probs.copy()\n",
    "        grad[np.arange(batch_size), targets] -= 1\n",
    "        \n",
    "        # Output layer\n",
    "        layer = self.layers[-1]\n",
    "        dW = layer['cache']['X'].T @ grad / batch_size\n",
    "        layer['W'] -= lr * dW\n",
    "        layer['b'] -= lr * np.mean(grad, axis=0)\n",
    "        grad = grad @ layer['W'].T\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(self.layers) - 2, -1, -1):\n",
    "            layer = self.layers[i]\n",
    "            \n",
    "            # ReLU backward\n",
    "            grad = grad * (layer['cache']['A'] > 0)\n",
    "            \n",
    "            # Normalization backward\n",
    "            if self.norms[i] is not None:\n",
    "                grad = self.norms[i].backward(grad)\n",
    "                # Update norm parameters\n",
    "                self.norms[i].gamma -= lr * self.norms[i].dgamma\n",
    "                if hasattr(self.norms[i], 'beta'):\n",
    "                    self.norms[i].beta -= lr * self.norms[i].dbeta\n",
    "            \n",
    "            X = layer['cache']['X']\n",
    "            dW = X.T @ grad / batch_size\n",
    "            layer['W'] -= lr * dW\n",
    "            layer['b'] -= lr * np.mean(grad, axis=0)\n",
    "            grad = grad @ layer['W'].T\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.forward(X, training=False), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare normalization methods\n",
    "print(\"Comparing Normalization Methods\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "norm_types = ['none', 'batch', 'layer', 'rms', 'group']\n",
    "all_histories = {}\n",
    "\n",
    "for norm_type in norm_types:\n",
    "    print(f\"\\nTraining with {norm_type.upper()} normalization...\")\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    model = NormalizedMLP([784, 256, 128, 10], norm_type=norm_type)\n",
    "    history = {'train_acc': [], 'test_acc': []}\n",
    "    \n",
    "    for epoch in range(20):\n",
    "        # Training\n",
    "        indices = np.random.permutation(len(X_train))\n",
    "        for start in range(0, len(X_train), 64):\n",
    "            batch_idx = indices[start:start+64]\n",
    "            model.forward(X_train[batch_idx], training=True)\n",
    "            model.backward(y_train[batch_idx], 0.1)\n",
    "        \n",
    "        train_acc = np.mean(model.predict(X_train) == y_train)\n",
    "        test_acc = np.mean(model.predict(X_test[:2000]) == y_test[:2000])\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"  Epoch {epoch+1:2d} | Train: {train_acc:.2%} | Test: {test_acc:.2%}\")\n",
    "    \n",
    "    all_histories[norm_type] = history\n",
    "    print(f\"  Final test accuracy: {test_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = {'none': 'gray', 'batch': 'blue', 'layer': 'green', 'rms': 'red', 'group': 'purple'}\n",
    "\n",
    "for norm_type, history in all_histories.items():\n",
    "    epochs = range(1, len(history['train_acc']) + 1)\n",
    "    axes[0].plot(epochs, history['train_acc'], color=colors[norm_type], \n",
    "                 label=f'{norm_type.upper()}', linewidth=2)\n",
    "    axes[1].plot(epochs, history['test_acc'], color=colors[norm_type], \n",
    "                 label=f'{norm_type.upper()}', linewidth=2)\n",
    "\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Training Accuracy by Normalization Method')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Test Accuracy by Normalization Method')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Final Results Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Method':<12} {'Final Train':<15} {'Final Test':<15}\")\n",
    "print(\"-\" * 50)\n",
    "for norm_type, history in all_histories.items():\n",
    "    print(f\"{norm_type.upper():<12} {history['train_acc'][-1]:<15.2%} {history['test_acc'][-1]:<15.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 Solution: Small Batch Training Comparison\n",
    "\n",
    "BatchNorm struggles with small batches because batch statistics are noisy.\n",
    "Let's compare normalization methods with batch size = 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small batch training experiment\n",
    "print(\"Small Batch Training (batch_size=4)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "small_batch_histories = {}\n",
    "batch_size = 4  # Very small batch size\n",
    "\n",
    "for norm_type in ['none', 'batch', 'layer', 'group']:\n",
    "    print(f\"\\nTraining with {norm_type.upper()} normalization (batch_size={batch_size})...\")\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    model = NormalizedMLP([784, 128, 64, 10], norm_type=norm_type)\n",
    "    history = {'train_acc': [], 'test_acc': []}\n",
    "    \n",
    "    for epoch in range(15):\n",
    "        indices = np.random.permutation(len(X_train))\n",
    "        for start in range(0, len(X_train), batch_size):\n",
    "            batch_idx = indices[start:start+batch_size]\n",
    "            if len(batch_idx) < batch_size:\n",
    "                continue  # Skip incomplete batches\n",
    "            model.forward(X_train[batch_idx], training=True)\n",
    "            model.backward(y_train[batch_idx], 0.05)  # Lower LR for small batches\n",
    "        \n",
    "        train_acc = np.mean(model.predict(X_train) == y_train)\n",
    "        test_acc = np.mean(model.predict(X_test[:2000]) == y_test[:2000])\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "    \n",
    "    small_batch_histories[norm_type] = history\n",
    "    print(f\"  Final test accuracy: {test_acc:.2%}\")\n",
    "\n",
    "# Plot small batch comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "for norm_type, history in small_batch_histories.items():\n",
    "    plt.plot(history['test_acc'], label=norm_type.upper(), linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Small Batch Training: BatchNorm vs LayerNorm vs GroupNorm')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Key Insight: BatchNorm struggles with small batches!\")\n",
    "print(\"LayerNorm and GroupNorm are more stable because they\")\n",
    "print(\"normalize within each sample, not across the batch.\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **BatchNorm** - Best for large batches, CNN training\n",
    "2. **LayerNorm** - Best for transformers, RNNs, small batches\n",
    "3. **GroupNorm** - Good middle ground, works well in vision with small batches\n",
    "4. **RMSNorm** - Simpler, faster, used in modern LLMs (Llama, etc.)\n",
    "\n",
    "### When to use what:\n",
    "- **Large batch CNN training**: BatchNorm\n",
    "- **Transformers/NLP**: LayerNorm or RMSNorm\n",
    "- **Small batch or instance-level processing**: LayerNorm or GroupNorm\n",
    "- **Maximum efficiency in LLMs**: RMSNorm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}