{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.5.3 Solution: Regularization Experiments\n",
    "\n",
    "This notebook contains solutions to the exercises from Lab 1.5.3.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Grid Search for Optimal Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import gzip, os, urllib.request\n",
    "\n",
    "def load_mnist(path='../data'):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    base_url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = ['train-images-idx3-ubyte.gz', 'train-labels-idx1-ubyte.gz',\n",
    "             't10k-images-idx3-ubyte.gz', 't10k-labels-idx1-ubyte.gz']\n",
    "    for f in files:\n",
    "        fp = os.path.join(path, f)\n",
    "        if not os.path.exists(fp): urllib.request.urlretrieve(base_url + f, fp)\n",
    "    def load_img(fp): \n",
    "        with gzip.open(fp) as f: f.read(16); return np.frombuffer(f.read(), np.uint8).reshape(-1,784).astype(np.float32)/255\n",
    "    def load_lbl(fp): \n",
    "        with gzip.open(fp) as f: f.read(8); return np.frombuffer(f.read(), np.uint8)\n",
    "    return (load_img(os.path.join(path, files[0])), load_lbl(os.path.join(path, files[1])),\n",
    "            load_img(os.path.join(path, files[2])), load_lbl(os.path.join(path, files[3])))\n",
    "\n",
    "X_train_full, y_train_full, X_test, y_test = load_mnist()\n",
    "\n",
    "# Use small training set for overfitting scenario\n",
    "X_train = X_train_full[:1000]\n",
    "y_train = y_train_full[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedMLP:\n",
    "    \"\"\"MLP with L2 regularization and Dropout.\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes, l2_lambda=0.0, dropout_rate=0.0):\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.training = True\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            W = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2.0 / layer_sizes[i])\n",
    "            b = np.zeros(layer_sizes[i + 1])\n",
    "            self.layers.append({'W': W, 'b': b, 'cache': {}, 'mask': None})\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = X\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            layer['cache']['X'] = out\n",
    "            out = out @ layer['W'] + layer['b']\n",
    "            layer['cache']['Z'] = out\n",
    "            out = np.maximum(0, out)  # ReLU\n",
    "            \n",
    "            # Dropout\n",
    "            if self.training and self.dropout_rate > 0:\n",
    "                mask = (np.random.rand(*out.shape) > self.dropout_rate).astype(float)\n",
    "                out = out * mask / (1 - self.dropout_rate)\n",
    "                layer['mask'] = mask\n",
    "        \n",
    "        self.layers[-1]['cache']['X'] = out\n",
    "        out = out @ self.layers[-1]['W'] + self.layers[-1]['b']\n",
    "        out_shifted = out - np.max(out, axis=1, keepdims=True)\n",
    "        exp_out = np.exp(out_shifted)\n",
    "        self.probs = exp_out / np.sum(exp_out, axis=1, keepdims=True)\n",
    "        return self.probs\n",
    "    \n",
    "    def compute_loss(self, targets):\n",
    "        batch_size = len(targets)\n",
    "        ce_loss = -np.mean(np.log(self.probs[np.arange(batch_size), targets] + 1e-10))\n",
    "        l2_loss = 0.5 * self.l2_lambda * sum(np.sum(l['W']**2) for l in self.layers) if self.l2_lambda > 0 else 0\n",
    "        return ce_loss + l2_loss\n",
    "    \n",
    "    def backward(self, targets, lr):\n",
    "        batch_size = len(targets)\n",
    "        grad = self.probs.copy()\n",
    "        grad[np.arange(batch_size), targets] -= 1\n",
    "        \n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            layer = self.layers[i]\n",
    "            X = layer['cache']['X']\n",
    "            \n",
    "            dW = X.T @ grad / batch_size\n",
    "            if self.l2_lambda > 0:\n",
    "                dW += self.l2_lambda * layer['W']\n",
    "            \n",
    "            layer['W'] -= lr * dW\n",
    "            layer['b'] -= lr * np.mean(grad, axis=0)\n",
    "            \n",
    "            grad = grad @ layer['W'].T\n",
    "            \n",
    "            if i > 0:\n",
    "                Z = self.layers[i - 1]['cache']['Z']\n",
    "                grad = grad * (Z > 0)\n",
    "                if self.layers[i - 1]['mask'] is not None:\n",
    "                    grad = grad * self.layers[i - 1]['mask'] / (1 - self.dropout_rate)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.training = False\n",
    "        probs = self.forward(X)\n",
    "        self.training = True\n",
    "        return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search\n",
    "print(\"Grid Search for Optimal Regularization\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "l2_values = [0.0, 0.0001, 0.001, 0.01]\n",
    "dropout_values = [0.0, 0.1, 0.2, 0.3]\n",
    "\n",
    "results = {}\n",
    "best_acc = 0\n",
    "best_config = None\n",
    "\n",
    "for l2 in l2_values:\n",
    "    for dropout in dropout_values:\n",
    "        np.random.seed(42)\n",
    "        model = RegularizedMLP([784, 512, 256, 10], l2_lambda=l2, dropout_rate=dropout)\n",
    "        \n",
    "        # Train\n",
    "        for epoch in range(30):\n",
    "            indices = np.random.permutation(len(X_train))\n",
    "            for start in range(0, len(X_train), 32):\n",
    "                batch_idx = indices[start:start+32]\n",
    "                model.forward(X_train[batch_idx])\n",
    "                model.backward(y_train[batch_idx], 0.1)\n",
    "        \n",
    "        # Evaluate\n",
    "        train_acc = np.mean(model.predict(X_train) == y_train)\n",
    "        test_acc = np.mean(model.predict(X_test[:2000]) == y_test[:2000])\n",
    "        gap = train_acc - test_acc\n",
    "        \n",
    "        results[(l2, dropout)] = {'train': train_acc, 'test': test_acc, 'gap': gap}\n",
    "        \n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_config = (l2, dropout)\n",
    "        \n",
    "        print(f\"L2={l2:.4f}, Dropout={dropout:.1f} | Train: {train_acc:.2%} | Test: {test_acc:.2%} | Gap: {gap:.2%}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n✅ Best configuration: L2={best_config[0]}, Dropout={best_config[1]}\")\n",
    "print(f\"   Best test accuracy: {best_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results as heatmap\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "metrics = ['train', 'test', 'gap']\n",
    "titles = ['Training Accuracy', 'Test Accuracy', 'Generalization Gap']\n",
    "cmaps = ['Blues', 'Greens', 'Reds']\n",
    "\n",
    "for ax, metric, title, cmap in zip(axes, metrics, titles, cmaps):\n",
    "    data = np.zeros((len(l2_values), len(dropout_values)))\n",
    "    for i, l2 in enumerate(l2_values):\n",
    "        for j, drop in enumerate(dropout_values):\n",
    "            data[i, j] = results[(l2, drop)][metric]\n",
    "    \n",
    "    im = ax.imshow(data, cmap=cmap, aspect='auto')\n",
    "    ax.set_xticks(range(len(dropout_values)))\n",
    "    ax.set_yticks(range(len(l2_values)))\n",
    "    ax.set_xticklabels([f'{d:.1f}' for d in dropout_values])\n",
    "    ax.set_yticklabels([f'{l:.4f}' for l in l2_values])\n",
    "    ax.set_xlabel('Dropout Rate')\n",
    "    ax.set_ylabel('L2 Lambda')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Add value annotations\n",
    "    for i in range(len(l2_values)):\n",
    "        for j in range(len(dropout_values)):\n",
    "            ax.text(j, i, f'{data[i,j]:.1%}', ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Early Stopping Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to prevent overfitting.\n",
    "    \n",
    "    Monitors validation loss and stops training when it stops improving.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, patience: int = 5, min_delta: float = 0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience: Number of epochs to wait for improvement\n",
    "            min_delta: Minimum change to qualify as improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "    \n",
    "    def __call__(self, val_loss, model_weights):\n",
    "        \"\"\"\n",
    "        Check if training should stop.\n",
    "        \n",
    "        Returns:\n",
    "            True if training should stop, False otherwise\n",
    "        \"\"\"\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            # Deep copy weights\n",
    "            self.best_weights = [(l['W'].copy(), l['b'].copy()) for l in model_weights]\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "            return False\n",
    "    \n",
    "    def restore_best_weights(self, model_weights):\n",
    "        \"\"\"Restore the best weights found during training.\"\"\"\n",
    "        if self.best_weights is not None:\n",
    "            for layer, (W, b) in zip(model_weights, self.best_weights):\n",
    "                layer['W'] = W.copy()\n",
    "                layer['b'] = b.copy()\n",
    "\n",
    "\n",
    "def train_with_early_stopping(model, X_train, y_train, X_val, y_val, \n",
    "                               max_epochs=100, patience=5, batch_size=32, lr=0.1):\n",
    "    \"\"\"\n",
    "    Train model with early stopping.\n",
    "    \n",
    "    Returns:\n",
    "        history: Dictionary with training metrics\n",
    "        stopped_epoch: Epoch at which training stopped\n",
    "    \"\"\"\n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # Training\n",
    "        model.training = True\n",
    "        indices = np.random.permutation(len(X_train))\n",
    "        train_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for start in range(0, len(X_train), batch_size):\n",
    "            batch_idx = indices[start:start+batch_size]\n",
    "            model.forward(X_train[batch_idx])\n",
    "            train_loss += model.compute_loss(y_train[batch_idx])\n",
    "            model.backward(y_train[batch_idx], lr)\n",
    "            n_batches += 1\n",
    "        \n",
    "        train_loss /= n_batches\n",
    "        \n",
    "        # Validation\n",
    "        model.training = False\n",
    "        model.forward(X_val)\n",
    "        val_loss = model.compute_loss(y_val)\n",
    "        \n",
    "        train_acc = np.mean(model.predict(X_train) == y_train)\n",
    "        val_acc = np.mean(model.predict(X_val) == y_val)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:3d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "              f\"Train Acc: {train_acc:.2%} | Val Acc: {val_acc:.2%}\")\n",
    "        \n",
    "        # Check early stopping\n",
    "        if early_stopping(val_loss, model.layers):\n",
    "            print(f\"\\n⚡ Early stopping triggered at epoch {epoch+1}!\")\n",
    "            print(f\"   Best validation loss: {early_stopping.best_loss:.4f}\")\n",
    "            early_stopping.restore_best_weights(model.layers)\n",
    "            return history, epoch + 1\n",
    "    \n",
    "    return history, max_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test early stopping\n",
    "print(\"Training with Early Stopping\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "np.random.seed(42)\n",
    "model = RegularizedMLP([784, 512, 256, 10], l2_lambda=0.0, dropout_rate=0.0)  # No regularization to see overfitting\n",
    "\n",
    "# Split training data for validation\n",
    "X_train_split = X_train[:800]\n",
    "y_train_split = y_train[:800]\n",
    "X_val_split = X_train[800:]\n",
    "y_val_split = y_train[800:]\n",
    "\n",
    "history, stopped_epoch = train_with_early_stopping(\n",
    "    model, X_train_split, y_train_split, X_val_split, y_val_split,\n",
    "    max_epochs=50, patience=5, batch_size=32, lr=0.1\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Training stopped at epoch {stopped_epoch}\")\n",
    "print(f\"   Final test accuracy: {np.mean(model.predict(X_test) == y_test):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot early stopping effect\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "axes[0].plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "axes[0].plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "axes[0].axvline(stopped_epoch - 5, color='green', linestyle='--', label='Best Model')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Curves with Early Stopping')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(epochs, history['train_acc'], 'b-', label='Train Acc', linewidth=2)\n",
    "axes[1].plot(epochs, history['val_acc'], 'r-', label='Val Acc', linewidth=2)\n",
    "axes[1].axvline(stopped_epoch - 5, color='green', linestyle='--', label='Best Model')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Curves with Early Stopping')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Grid search** helps find optimal hyperparameters systematically\n",
    "2. **L2 + Dropout together** often works better than either alone\n",
    "3. **Early stopping** is a simple but effective regularization technique\n",
    "4. Always **save the best model** during training, not just the final one"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}