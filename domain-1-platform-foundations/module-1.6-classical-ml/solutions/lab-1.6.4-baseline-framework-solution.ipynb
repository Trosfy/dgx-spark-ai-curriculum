{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.6.4: Baseline Comparison Framework - SOLUTIONS\n",
    "\n",
    "This notebook contains complete solutions to all exercises from Lab 1.6.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom time import time\nfrom typing import Dict, List, Any, Optional, Callable\nfrom dataclasses import dataclass\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.datasets import load_breast_cancer, fetch_california_housing\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, mean_squared_error, f1_score\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.linear_model import LogisticRegression, Ridge\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xgb\n\nnp.random.seed(42)\n\n# Plotting style with fallback for older matplotlib versions\ntry:\n    plt.style.use('seaborn-v0_8-whitegrid')\nexcept OSError:\n    try:\n        plt.style.use('seaborn-whitegrid')\n    except OSError:\n        pass  # Use default style\n\nprint(\"âœ… Setup complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Add More Models (LightGBM + MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Extended BaselineExperiment with more models\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LGBM_AVAILABLE = False\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "\n",
    "# Load breast cancer data for testing\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data.astype(np.float32), data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'XGBoost': (xgb.XGBClassifier(\n",
    "        n_estimators=100, max_depth=6, device='cuda',\n",
    "        random_state=42, verbosity=0\n",
    "    ), False),\n",
    "    \n",
    "    'Random Forest': (RandomForestClassifier(\n",
    "        n_estimators=100, max_depth=16, n_jobs=-1, random_state=42\n",
    "    ), False),\n",
    "    \n",
    "    'Logistic Regression': (LogisticRegression(\n",
    "        max_iter=1000, random_state=42, n_jobs=-1\n",
    "    ), True),  # needs scaling\n",
    "    \n",
    "    'MLP Neural Network': (MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 50, 25),\n",
    "        max_iter=500,\n",
    "        early_stopping=True,\n",
    "        random_state=42\n",
    "    ), True)  # needs scaling\n",
    "}\n",
    "\n",
    "# Add LightGBM if available\n",
    "if LGBM_AVAILABLE:\n",
    "    models['LightGBM'] = (lgb.LGBMClassifier(\n",
    "        n_estimators=100, max_depth=6, device='gpu',\n",
    "        random_state=42, verbose=-1\n",
    "    ), False)\n",
    "\n",
    "# Scale data for models that need it\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Run comparison\n",
    "print(\"Extended Model Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = []\n",
    "for name, (model, needs_scaling) in models.items():\n",
    "    print(f\"\\nTraining: {name}\")\n",
    "    \n",
    "    X_tr = X_train_scaled if needs_scaling else X_train\n",
    "    X_te = X_test_scaled if needs_scaling else X_test\n",
    "    \n",
    "    # Train\n",
    "    start = time()\n",
    "    model.fit(X_tr, y_train)\n",
    "    train_time = time() - start\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_te)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': acc,\n",
    "        'F1 Score': f1,\n",
    "        'Train Time (s)': train_time\n",
    "    })\n",
    "    \n",
    "    print(f\"  Accuracy: {acc:.4f}, F1: {f1:.4f}, Time: {train_time:.3f}s\")\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Summary (sorted by Accuracy):\")\n",
    "results_df = pd.DataFrame(results).sort_values('Accuracy', ascending=False)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Custom Metrics Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Custom profit-based metric\n",
    "\n",
    "def profit_metric(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Custom profit metric for a business application.\n",
    "    \n",
    "    Scenario: Fraud detection\n",
    "    - True Positive (caught fraud): +$100 (saved money)\n",
    "    - False Positive (false alarm): -$50 (investigation cost)\n",
    "    - False Negative (missed fraud): -$200 (lost money)\n",
    "    - True Negative: $0 (no action needed)\n",
    "    \n",
    "    Returns profit per prediction on average.\n",
    "    \"\"\"\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    \n",
    "    total_profit = 100 * tp - 50 * fp - 200 * fn + 0 * tn\n",
    "    profit_per_sample = total_profit / len(y_true)\n",
    "    \n",
    "    return profit_per_sample\n",
    "\n",
    "def specificity_metric(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"True Negative Rate (Specificity).\"\"\"\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    return tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "# Extended comparison with custom metrics\n",
    "print(\"Model Comparison with Custom Metrics\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "custom_results = []\n",
    "\n",
    "for name, (model, needs_scaling) in list(models.items())[:3]:  # Top 3 models\n",
    "    X_tr = X_train_scaled if needs_scaling else X_train\n",
    "    X_te = X_test_scaled if needs_scaling else X_test\n",
    "    \n",
    "    model.fit(X_tr, y_train)\n",
    "    y_pred = model.predict(X_te)\n",
    "    \n",
    "    custom_results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Profit per Sample': profit_metric(y_test, y_pred),\n",
    "        'Specificity': specificity_metric(y_test, y_pred)\n",
    "    })\n",
    "\n",
    "custom_df = pd.DataFrame(custom_results)\n",
    "print(custom_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - Best accuracy model may not be best for business profit!\")\n",
    "print(\"  - Specificity matters when false positives are costly.\")\n",
    "print(\"  - Always align metrics with business objectives.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 Solution: Automatic Hyperparameter Tuning Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Auto-tuning the best model\n",
    "\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def tune_best_model(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    best_model_name: str,\n",
    "    n_trials: int = 30\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Automatically tune the best-performing model using Optuna.\n",
    "    \n",
    "    Returns dict with tuned model and results.\n",
    "    \"\"\"\n",
    "    print(f\"\\nAuto-tuning {best_model_name} with Optuna ({n_trials} trials)...\")\n",
    "    \n",
    "    if best_model_name == 'XGBoost':\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'device': 'cuda',\n",
    "                'random_state': 42,\n",
    "                'verbosity': 0\n",
    "            }\n",
    "            \n",
    "            model = xgb.XGBClassifier(**params)\n",
    "            cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "            return cv_scores.mean()\n",
    "        \n",
    "    elif best_model_name == 'Random Forest':\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "                'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "                'n_jobs': -1,\n",
    "                'random_state': 42\n",
    "            }\n",
    "            \n",
    "            model = RandomForestClassifier(**params)\n",
    "            cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "            return cv_scores.mean()\n",
    "    else:\n",
    "        raise ValueError(f\"Tuning not implemented for {best_model_name}\")\n",
    "    \n",
    "    # Run optimization\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    \n",
    "    # Get best parameters and train final model\n",
    "    best_params = study.best_params\n",
    "    \n",
    "    if best_model_name == 'XGBoost':\n",
    "        best_params.update({'device': 'cuda', 'random_state': 42, 'verbosity': 0})\n",
    "        tuned_model = xgb.XGBClassifier(**best_params)\n",
    "    else:\n",
    "        best_params.update({'n_jobs': -1, 'random_state': 42})\n",
    "        tuned_model = RandomForestClassifier(**best_params)\n",
    "    \n",
    "    tuned_model.fit(X_train, y_train)\n",
    "    y_pred = tuned_model.predict(X_test)\n",
    "    tuned_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'model': tuned_model,\n",
    "        'best_params': best_params,\n",
    "        'cv_score': study.best_value,\n",
    "        'test_accuracy': tuned_accuracy,\n",
    "        'n_trials': n_trials\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "print(\"Baseline Comparison + Auto-Tuning\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# First, find best model\n",
    "best_name = results_df.iloc[0]['Model']\n",
    "best_baseline_acc = results_df.iloc[0]['Accuracy']\n",
    "print(f\"Best baseline model: {best_name} (Accuracy: {best_baseline_acc:.4f})\")\n",
    "\n",
    "# Auto-tune it\n",
    "tuning_result = tune_best_model(X_train, y_train, X_test, y_test, best_name, n_trials=30)\n",
    "\n",
    "print(f\"\\nTuning Results:\")\n",
    "print(f\"  CV Score: {tuning_result['cv_score']:.4f}\")\n",
    "print(f\"  Test Accuracy: {tuning_result['test_accuracy']:.4f}\")\n",
    "print(f\"  Improvement: {(tuning_result['test_accuracy'] - best_baseline_acc)*100:.2f}%\")\n",
    "print(f\"\\nBest Parameters:\")\n",
    "for key, val in tuning_result['best_params'].items():\n",
    "    if key not in ['device', 'random_state', 'verbosity', 'n_jobs']:\n",
    "        print(f\"  {key}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Extended Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final: Complete extended BaselineExperiment class\n",
    "\n",
    "@dataclass\n",
    "class ExtendedModelResult:\n",
    "    name: str\n",
    "    standard_metrics: Dict[str, float]\n",
    "    custom_metrics: Dict[str, float]\n",
    "    cv_scores: np.ndarray\n",
    "    train_time: float\n",
    "    tuned: bool = False\n",
    "    tuned_params: Optional[Dict] = None\n",
    "\n",
    "class ExtendedBaselineExperiment:\n",
    "    \"\"\"\n",
    "    Extended baseline framework with:\n",
    "    - More models (LightGBM, MLP)\n",
    "    - Custom metrics support\n",
    "    - Auto-tuning integration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, task='classification', custom_metrics=None):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y\n",
    "        self.task = task\n",
    "        self.custom_metrics = custom_metrics or {}\n",
    "        self.models = {}\n",
    "        self.results = []\n",
    "        \n",
    "        # Split data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "    def add_custom_metric(self, name: str, func: Callable):\n",
    "        \"\"\"Add a custom metric function.\"\"\"\n",
    "        self.custom_metrics[name] = func\n",
    "        \n",
    "    def add_all_models(self):\n",
    "        \"\"\"Add extended set of models.\"\"\"\n",
    "        if self.task == 'classification':\n",
    "            self.models = {\n",
    "                'XGBoost': xgb.XGBClassifier(n_estimators=100, device='cuda', verbosity=0),\n",
    "                'Random Forest': RandomForestClassifier(n_estimators=100, n_jobs=-1),\n",
    "                'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "                'MLP': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500)\n",
    "            }\n",
    "            if LGBM_AVAILABLE:\n",
    "                self.models['LightGBM'] = lgb.LGBMClassifier(n_estimators=100, verbose=-1)\n",
    "        return self\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Run all models.\"\"\"\n",
    "        for name, model in self.models.items():\n",
    "            # Train\n",
    "            start = time()\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "            train_time = time() - start\n",
    "            \n",
    "            # Predict\n",
    "            y_pred = model.predict(self.X_test)\n",
    "            \n",
    "            # Standard metrics\n",
    "            std_metrics = {'accuracy': accuracy_score(self.y_test, y_pred)}\n",
    "            \n",
    "            # Custom metrics\n",
    "            cust_metrics = {}\n",
    "            for metric_name, metric_func in self.custom_metrics.items():\n",
    "                cust_metrics[metric_name] = metric_func(self.y_test, y_pred)\n",
    "            \n",
    "            # CV scores\n",
    "            cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=5)\n",
    "            \n",
    "            self.results.append(ExtendedModelResult(\n",
    "                name=name,\n",
    "                standard_metrics=std_metrics,\n",
    "                custom_metrics=cust_metrics,\n",
    "                cv_scores=cv_scores,\n",
    "                train_time=train_time\n",
    "            ))\n",
    "        return self\n",
    "    \n",
    "    def report(self):\n",
    "        \"\"\"Generate report DataFrame.\"\"\"\n",
    "        data = []\n",
    "        for r in self.results:\n",
    "            row = {'Model': r.name, **r.standard_metrics, **r.custom_metrics,\n",
    "                   'CV Mean': r.cv_scores.mean(), 'Train Time': r.train_time}\n",
    "            data.append(row)\n",
    "        return pd.DataFrame(data).sort_values('accuracy', ascending=False)\n",
    "\n",
    "# Demo\n",
    "print(\"Extended Framework Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "exp = ExtendedBaselineExperiment(X, y, task='classification')\n",
    "exp.add_custom_metric('profit', profit_metric)\n",
    "exp.add_all_models()\n",
    "exp.run()\n",
    "\n",
    "print(exp.report().to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Extensibility matters** - frameworks should support new models and metrics easily\n",
    "2. **Custom metrics align ML with business goals** - accuracy isn't always the right target\n",
    "3. **Auto-tuning can improve results** - but start simple and tune only the best baseline\n",
    "4. **Reproducibility is key** - fixed seeds, documented splits, consistent evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}