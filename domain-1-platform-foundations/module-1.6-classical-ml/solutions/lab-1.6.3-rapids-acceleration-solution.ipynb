{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.6.3: RAPIDS Acceleration - SOLUTIONS\n",
    "\n",
    "This notebook contains complete solutions to all exercises from Lab 1.6.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom time import time\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier as SklearnRF\nfrom sklearn.metrics import accuracy_score\n\n# Plotting style with fallback for older matplotlib versions\ntry:\n    plt.style.use('seaborn-v0_8-whitegrid')\nexcept OSError:\n    try:\n        plt.style.use('seaborn-whitegrid')\n    except OSError:\n        pass  # Use default style\n\n# Check if RAPIDS is available\ntry:\n    import cudf\n    import cupy as cp\n    from cuml.ensemble import RandomForestClassifier as CumlRF\n    from cuml.linear_model import LogisticRegression as CumlLR\n    RAPIDS_AVAILABLE = True\n    print(\"✅ RAPIDS is available!\")\nexcept ImportError:\n    RAPIDS_AVAILABLE = False\n    print(\"❌ RAPIDS not available. Use NGC container:\")\n    print(\"   nvcr.io/nvidia/rapidsai/base:25.11-py3\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Benchmark Different Dataset Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: How speedup changes with dataset size\n",
    "\n",
    "if RAPIDS_AVAILABLE:\n",
    "    sizes = [10_000, 50_000, 100_000, 500_000, 1_000_000]\n",
    "    n_features = 30\n",
    "    \n",
    "    sklearn_times = []\n",
    "    cuml_times = []\n",
    "    speedups = []\n",
    "    \n",
    "    print(\"Benchmarking Random Forest across dataset sizes...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for size in sizes:\n",
    "        print(f\"\\nDataset size: {size:,}\")\n",
    "        \n",
    "        # Generate data\n",
    "        X, y = make_classification(\n",
    "            n_samples=size,\n",
    "            n_features=n_features,\n",
    "            n_informative=20,\n",
    "            random_state=42\n",
    "        )\n",
    "        X = X.astype(np.float32)\n",
    "        y = y.astype(np.int32)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # scikit-learn\n",
    "        sklearn_model = SklearnRF(n_estimators=50, max_depth=10, n_jobs=-1, random_state=42)\n",
    "        start = time()\n",
    "        sklearn_model.fit(X_train, y_train)\n",
    "        sklearn_time = time() - start\n",
    "        sklearn_times.append(sklearn_time)\n",
    "        print(f\"  sklearn: {sklearn_time:.2f}s\")\n",
    "        \n",
    "        # cuML\n",
    "        cuml_model = CumlRF(n_estimators=50, max_depth=10)\n",
    "        start = time()\n",
    "        cuml_model.fit(X_train, y_train)\n",
    "        cuml_time = time() - start\n",
    "        cuml_times.append(cuml_time)\n",
    "        print(f\"  cuML:    {cuml_time:.2f}s\")\n",
    "        \n",
    "        speedup = sklearn_time / cuml_time\n",
    "        speedups.append(speedup)\n",
    "        print(f\"  Speedup: {speedup:.1f}x\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Training time\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(sizes, sklearn_times, 'o-', label='sklearn (CPU)', linewidth=2)\n",
    "    ax1.plot(sizes, cuml_times, 'o-', label='cuML (GPU)', linewidth=2)\n",
    "    ax1.set_xlabel('Dataset Size')\n",
    "    ax1.set_ylabel('Training Time (seconds)')\n",
    "    ax1.set_title('Training Time vs Dataset Size')\n",
    "    ax1.legend()\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Speedup\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(sizes, speedups, 'o-', color='green', linewidth=2)\n",
    "    ax2.axhline(y=1, color='red', linestyle='--', label='Break-even')\n",
    "    ax2.set_xlabel('Dataset Size')\n",
    "    ax2.set_ylabel('Speedup (x times faster)')\n",
    "    ax2.set_title('GPU Speedup vs Dataset Size')\n",
    "    ax2.set_xscale('log')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nConclusion:\")\n",
    "    print(\"  GPU speedup increases with dataset size!\")\n",
    "    print(\"  Below 10K samples, overhead may dominate.\")\n",
    "    print(\"  For 1M+ samples, expect 10-50x speedup.\")\n",
    "else:\n",
    "    print(\"RAPIDS not available - showing expected results:\")\n",
    "    print(\"  10K samples: ~2x speedup (GPU overhead)\")\n",
    "    print(\"  100K samples: ~10x speedup\")\n",
    "    print(\"  1M samples: ~30-50x speedup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Port sklearn Pipeline to cuML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Porting sklearn pipeline to cuML\n",
    "\n",
    "if RAPIDS_AVAILABLE:\n",
    "    from cuml.preprocessing import StandardScaler as CumlScaler\n",
    "    from cuml.decomposition import PCA as CumlPCA\n",
    "    from cuml.pipeline import Pipeline as CumlPipeline\n",
    "    \n",
    "    # Generate test data\n",
    "    X, y = make_classification(n_samples=100_000, n_features=50, random_state=42)\n",
    "    X = X.astype(np.float32)\n",
    "    y = y.astype(np.int32)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Method 1: Manual chaining (more control)\n",
    "    print(\"Method 1: Manual Pipeline\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    # Scaling\n",
    "    scaler = CumlScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # PCA\n",
    "    pca = CumlPCA(n_components=20)\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "    \n",
    "    # Classifier\n",
    "    clf = CumlRF(n_estimators=100, max_depth=16)\n",
    "    clf.fit(X_train_pca, y_train)\n",
    "    y_pred = clf.predict(X_test_pca)\n",
    "    \n",
    "    manual_time = time() - start\n",
    "    manual_acc = accuracy_score(y_test, y_pred.to_numpy() if hasattr(y_pred, 'to_numpy') else y_pred)\n",
    "    \n",
    "    print(f\"  Time: {manual_time:.2f}s\")\n",
    "    print(f\"  Accuracy: {manual_acc:.4f}\")\n",
    "    \n",
    "    # Method 2: cuML Pipeline (cleaner)\n",
    "    print(\"\\nMethod 2: cuML Pipeline\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    cuml_pipeline = CumlPipeline([\n",
    "        ('scaler', CumlScaler()),\n",
    "        ('pca', CumlPCA(n_components=20)),\n",
    "        ('classifier', CumlRF(n_estimators=100, max_depth=16))\n",
    "    ])\n",
    "    \n",
    "    cuml_pipeline.fit(X_train, y_train)\n",
    "    y_pred_pipe = cuml_pipeline.predict(X_test)\n",
    "    \n",
    "    pipe_time = time() - start\n",
    "    pipe_acc = accuracy_score(y_test, y_pred_pipe.to_numpy() if hasattr(y_pred_pipe, 'to_numpy') else y_pred_pipe)\n",
    "    \n",
    "    print(f\"  Time: {pipe_time:.2f}s\")\n",
    "    print(f\"  Accuracy: {pipe_acc:.4f}\")\n",
    "    \n",
    "    # Compare with sklearn\n",
    "    print(\"\\nComparison with sklearn Pipeline:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    from sklearn.pipeline import Pipeline as SklearnPipeline\n",
    "    from sklearn.preprocessing import StandardScaler as SklearnScaler\n",
    "    from sklearn.decomposition import PCA as SklearnPCA\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    sklearn_pipeline = SklearnPipeline([\n",
    "        ('scaler', SklearnScaler()),\n",
    "        ('pca', SklearnPCA(n_components=20)),\n",
    "        ('classifier', SklearnRF(n_estimators=100, max_depth=16, n_jobs=-1))\n",
    "    ])\n",
    "    \n",
    "    sklearn_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    sklearn_time = time() - start\n",
    "    \n",
    "    print(f\"  sklearn time: {sklearn_time:.2f}s\")\n",
    "    print(f\"  cuML time: {pipe_time:.2f}s\")\n",
    "    print(f\"  Speedup: {sklearn_time/pipe_time:.1f}x\")\n",
    "else:\n",
    "    print(\"\"\"Expected pipeline code:\n",
    "    \n",
    "    from cuml.pipeline import Pipeline\n",
    "    from cuml.preprocessing import StandardScaler\n",
    "    from cuml.decomposition import PCA\n",
    "    from cuml.ensemble import RandomForestClassifier\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=20)),\n",
    "        ('classifier', RandomForestClassifier(n_estimators=100))\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 Solution: Memory Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: GPU Memory Profiling\n",
    "\n",
    "if RAPIDS_AVAILABLE:\n",
    "    import gc\n",
    "    \n",
    "    def get_gpu_memory_mb():\n",
    "        \"\"\"Get current GPU memory usage in MB.\"\"\"\n",
    "        return cp.get_default_memory_pool().used_bytes() / 1e6\n",
    "    \n",
    "    def free_gpu_memory():\n",
    "        \"\"\"Free unused GPU memory.\"\"\"\n",
    "        gc.collect()\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "    \n",
    "    print(\"GPU Memory Profiling\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Clean state\n",
    "    free_gpu_memory()\n",
    "    baseline_mem = get_gpu_memory_mb()\n",
    "    print(f\"Baseline: {baseline_mem:.1f} MB\")\n",
    "    \n",
    "    # Load data\n",
    "    X_large, y_large = make_classification(n_samples=1_000_000, n_features=50, random_state=42)\n",
    "    X_large = X_large.astype(np.float32)\n",
    "    print(f\"\\nAfter generating data (NumPy): {get_gpu_memory_mb():.1f} MB\")\n",
    "    print(f\"  Expected: ~0 MB (NumPy uses CPU RAM)\")\n",
    "    \n",
    "    # Convert to cuDF\n",
    "    X_gpu = cudf.DataFrame(X_large)\n",
    "    print(f\"\\nAfter cuDF conversion: {get_gpu_memory_mb():.1f} MB\")\n",
    "    print(f\"  Expected: ~{X_large.nbytes/1e6:.1f} MB (data copied to GPU)\")\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\nTraining Random Forest...\")\n",
    "    mem_before_train = get_gpu_memory_mb()\n",
    "    \n",
    "    model = CumlRF(n_estimators=50, max_depth=12)\n",
    "    model.fit(X_gpu, y_large)\n",
    "    \n",
    "    mem_after_train = get_gpu_memory_mb()\n",
    "    print(f\"  Before training: {mem_before_train:.1f} MB\")\n",
    "    print(f\"  After training: {mem_after_train:.1f} MB\")\n",
    "    print(f\"  Model overhead: {mem_after_train - mem_before_train:.1f} MB\")\n",
    "    \n",
    "    # Cleanup\n",
    "    print(f\"\\nCleaning up...\")\n",
    "    del X_gpu, model\n",
    "    free_gpu_memory()\n",
    "    print(f\"  After cleanup: {get_gpu_memory_mb():.1f} MB\")\n",
    "    \n",
    "    print(\"\\nBest Practices:\")\n",
    "    print(\"  1. Use float32 (not float64) to halve memory\")\n",
    "    print(\"  2. Delete unused objects and call free_gpu_memory()\")\n",
    "    print(\"  3. Process in batches for very large datasets\")\n",
    "    print(\"  4. DGX Spark's 128GB unified memory helps a lot!\")\n",
    "else:\n",
    "    print(\"\"\"Memory profiling code:\n",
    "    \n",
    "    import cupy as cp\n",
    "    \n",
    "    def get_gpu_memory_mb():\n",
    "        return cp.get_default_memory_pool().used_bytes() / 1e6\n",
    "    \n",
    "    def free_gpu_memory():\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "    \n",
    "    print(f\"Before: {get_gpu_memory_mb():.1f} MB\")\n",
    "    # ... do work ...\n",
    "    print(f\"After: {get_gpu_memory_mb():.1f} MB\")\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Speedup increases with dataset size** - GPU overhead becomes negligible for large datasets\n",
    "2. **cuML pipelines work like sklearn** - easy migration with minimal code changes\n",
    "3. **Memory management matters** - always clean up and use float32\n",
    "4. **DGX Spark is ideal** - 128GB unified memory eliminates many GPU memory constraints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}