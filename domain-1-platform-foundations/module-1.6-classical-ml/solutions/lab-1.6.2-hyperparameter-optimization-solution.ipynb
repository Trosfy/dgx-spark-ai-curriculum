{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.6.2: Hyperparameter Optimization - SOLUTIONS\n",
    "\n",
    "This notebook contains complete solutions to all exercises from Lab 1.6.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom time import time\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport optuna\n\nnp.random.seed(42)\n\n# Plotting style with fallback for older matplotlib versions\ntry:\n    plt.style.use('seaborn-v0_8-whitegrid')\nexcept OSError:\n    try:\n        plt.style.use('seaborn-whitegrid')\n    except OSError:\n        pass  # Use default style\n\n# Load data\nhousing = fetch_california_housing()\nX, y = housing.data, housing.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_sub, X_val, y_train_sub, y_val = train_test_split(\n    X_train, y_train, test_size=0.2, random_state=42\n)\n\nprint(\"âœ… Data loaded successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: XGBoost Pruning Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Using Optuna's pruning to speed up optimization\n",
    "\n",
    "def objective_with_pruning(trial):\n",
    "    \"\"\"\n",
    "    Objective function with early stopping/pruning.\n",
    "    Uses XGBoost's native API for fine-grained control.\n",
    "    \"\"\"\n",
    "    # Hyperparameter suggestions\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'device': 'cuda',\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    # Create DMatrix for XGBoost native API\n",
    "    dtrain = xgb.DMatrix(X_train_sub, label=y_train_sub)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    \n",
    "    # Pruning callback\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, 'validation-rmse')\n",
    "    \n",
    "    # Train with early stopping and pruning\n",
    "    try:\n",
    "        bst = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=500,\n",
    "            evals=[(dval, 'validation')],\n",
    "            early_stopping_rounds=20,\n",
    "            callbacks=[pruning_callback],\n",
    "            verbose_eval=False\n",
    "        )\n",
    "    except optuna.TrialPruned:\n",
    "        raise\n",
    "    \n",
    "    # Get best score\n",
    "    return bst.best_score\n",
    "\n",
    "# Run optimization with pruning\n",
    "print(\"Running Optuna with pruning...\")\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=50)\n",
    "\n",
    "study_pruning = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    pruner=pruner,\n",
    "    study_name='xgboost_pruning'\n",
    ")\n",
    "\n",
    "start = time()\n",
    "study_pruning.optimize(objective_with_pruning, n_trials=50, show_progress_bar=True)\n",
    "pruning_time = time() - start\n",
    "\n",
    "# Results\n",
    "print(f\"\\nOptimization with Pruning Complete!\")\n",
    "print(f\"Time: {pruning_time:.1f} seconds\")\n",
    "print(f\"Best RMSE: {study_pruning.best_value:.4f}\")\n",
    "print(f\"Pruned trials: {len([t for t in study_pruning.trials if t.state == optuna.trial.TrialState.PRUNED])}\")\n",
    "print(f\"Completed trials: {len([t for t in study_pruning.trials if t.state == optuna.trial.TrialState.COMPLETE])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: LightGBM Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Tuning LightGBM with Optuna\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LGBM_AVAILABLE = False\n",
    "    print(\"LightGBM not installed. Install with: pip install lightgbm\")\n",
    "\n",
    "if LGBM_AVAILABLE:\n",
    "    def lgb_objective(trial):\n",
    "        \"\"\"\n",
    "        Optuna objective for LightGBM.\n",
    "        LightGBM uses different parameter names!\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            # LightGBM-specific parameters\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "            'device': 'gpu',\n",
    "            'verbose': -1,\n",
    "            'random_state': 42\n",
    "        }\n",
    "        \n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        cv_scores = cross_val_score(\n",
    "            model, X_train, y_train,\n",
    "            cv=5,\n",
    "            scoring='neg_root_mean_squared_error',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        return -cv_scores.mean()\n",
    "    \n",
    "    # Run LightGBM optimization\n",
    "    print(\"Running Optuna for LightGBM...\")\n",
    "    study_lgb = optuna.create_study(direction='minimize', study_name='lightgbm_housing')\n",
    "    study_lgb.optimize(lgb_objective, n_trials=50, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"\\nLightGBM Results:\")\n",
    "    print(f\"Best RMSE: {study_lgb.best_value:.4f}\")\n",
    "    print(f\"\\nBest Parameters:\")\n",
    "    for key, value in study_lgb.best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Compare with XGBoost\n",
    "    print(f\"\\n Comparison:\")\n",
    "    print(f\"  XGBoost best RMSE: {study_pruning.best_value:.4f}\")\n",
    "    print(f\"  LightGBM best RMSE: {study_lgb.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 Solution: Multi-Objective Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Multi-objective optimization (accuracy AND speed)\n",
    "\n",
    "def multi_objective(trial):\n",
    "    \"\"\"\n",
    "    Optimize for both RMSE and training time.\n",
    "    Returns tuple: (rmse, training_time)\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'device': 'cuda',\n",
    "        'verbosity': 0,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    \n",
    "    # Time training\n",
    "    start = time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time() - start\n",
    "    \n",
    "    # Evaluate\n",
    "    pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "    \n",
    "    return rmse, train_time\n",
    "\n",
    "# Create multi-objective study\n",
    "print(\"Running multi-objective optimization...\")\n",
    "study_multi = optuna.create_study(\n",
    "    directions=['minimize', 'minimize'],  # Minimize both RMSE and time\n",
    "    study_name='xgboost_multi'\n",
    ")\n",
    "\n",
    "study_multi.optimize(multi_objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Get Pareto front\n",
    "pareto_trials = study_multi.best_trials\n",
    "\n",
    "print(f\"\\nMulti-objective Results:\")\n",
    "print(f\"Number of Pareto-optimal solutions: {len(pareto_trials)}\")\n",
    "print(f\"\\nPareto Front:\")\n",
    "print(f\"{'RMSE':>10} | {'Time (s)':>10}\")\n",
    "print(\"-\" * 25)\n",
    "for trial in pareto_trials:\n",
    "    print(f\"{trial.values[0]:>10.4f} | {trial.values[1]:>10.3f}\")\n",
    "\n",
    "# Visualize Pareto front\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# All trials\n",
    "all_rmse = [t.values[0] for t in study_multi.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "all_time = [t.values[1] for t in study_multi.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "ax.scatter(all_rmse, all_time, alpha=0.5, label='All Trials')\n",
    "\n",
    "# Pareto front\n",
    "pareto_rmse = [t.values[0] for t in pareto_trials]\n",
    "pareto_time = [t.values[1] for t in pareto_trials]\n",
    "ax.scatter(pareto_rmse, pareto_time, c='red', s=100, marker='*', label='Pareto Front')\n",
    "\n",
    "ax.set_xlabel('RMSE (lower is better)')\n",
    "ax.set_ylabel('Training Time (s) (lower is better)')\n",
    "ax.set_title('Multi-Objective Optimization: RMSE vs Training Time')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  The Pareto front shows the trade-off between accuracy and speed.\")\n",
    "print(\"  Points on the front are 'optimal' - improving one metric worsens the other.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Pruning dramatically speeds up optimization** by stopping unpromising trials early\n",
    "2. **LightGBM often matches XGBoost** with faster training (different parameter names!)\n",
    "3. **Multi-objective optimization** helps find the best trade-off between competing goals\n",
    "4. **The Pareto front** shows all optimal solutions - no single \"best\" when goals conflict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}