# Module 1.6: Classical ML Foundations - Study Guide

## ğŸ¯ Learning Objectives
By the end of this module, you will be able to:
1. **Train** and evaluate tree-based models (Random Forest, XGBoost)
2. **Explain** the bias-variance tradeoff in classical ML context
3. **Perform** hyperparameter tuning with cross-validation
4. **Accelerate** classical ML with RAPIDS cuML on DGX Spark

## ğŸ—ºï¸ Module Roadmap

| # | Lab | Focus | Time | Key Outcome |
|---|-----|-------|------|-------------|
| 1 | Tabular Data Challenge | XGBoost vs NN | ~2 hr | Know when to use each |
| 2 | Hyperparameter Optimization | Optuna tuning | ~2 hr | 100 trials with visualization |
| 3 | RAPIDS Acceleration | cuML on GPU | ~2 hr | 10-100x speedup |
| 4 | Baseline Comparison | Framework | ~2 hr | Reusable comparison tool |

**Total time**: ~8 hours

## ğŸ”‘ Core Concepts

### Tree-Based Methods
**What**: Decision trees, Random Forests, Gradient Boosting (XGBoost/LightGBM).
**Why it matters**: Often outperform neural networks on tabular data. Interpretable and fast.
**First appears in**: Lab 1

### Bias-Variance Tradeoff
**What**: Balance between underfitting (high bias) and overfitting (high variance).
**Why it matters**: Fundamental to model selection and hyperparameter tuning.
**First appears in**: Lab 1

### Gradient Boosting
**What**: Build models sequentially, each correcting the previous one's errors.
**Why it matters**: XGBoost dominates Kaggle competitions on tabular data.
**First appears in**: Lab 1, Lab 2

### RAPIDS cuML
**What**: GPU-accelerated scikit-learn-compatible ML library.
**Why it matters**: 10-100x faster training on DGX Spark's unified memory.
**First appears in**: Lab 3

## ğŸ”— How This Module Connects

```
    Module 1.5              Module 1.6                Module 1.7
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Neural Networks    â”€â”€â–º   Classical ML        â”€â”€â–º   Capstone

    Deep learning            When NOT to use DL        Combine everything
    Complex models           Fast baselines            Build autograd
    GPU training             XGBoost supremacy         MNIST example
```

**Builds on**:
- Module 1.5: Comparison point for neural networks

**Prepares for**:
- **Module 1.7**: Classical ML baselines for MicroGrad+ testing
- **All future modules**: Always start with an XGBoost baseline!

## ğŸ“Š When to Use What

### Decision Guide
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DECISION GUIDE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Data Type        â”‚ Recommendation                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Tabular (<100K)  â”‚ XGBoost first, neural net if needed     â”‚
â”‚ Tabular (>1M)    â”‚ XGBoost or LightGBM with GPU            â”‚
â”‚ Images           â”‚ Deep learning (CNNs, ViT)               â”‚
â”‚ Text             â”‚ Transformers (BERT, LLMs)               â”‚
â”‚ Time series      â”‚ Try both, XGBoost often wins            â”‚
â”‚ Need explainability â”‚ Trees, linear models                 â”‚
â”‚ Many features    â”‚ Random Forest, Lasso                    â”‚
â”‚ Few samples      â”‚ Classical ML (less overfit risk)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Method Comparison
| Method | Strengths | Weaknesses |
|--------|-----------|------------|
| XGBoost | Fast, accurate, handles missing values | Not for images/text |
| Random Forest | Robust, parallel | Slower than boosting |
| Logistic Regression | Interpretable, fast | Linear only |
| Neural Network | Universal, flexible | Needs lots of data |

## ğŸ“– Recommended Approach

**Standard path** (8 hours):
1. Lab 1: Compare XGBoost and neural networks
2. Lab 2: Learn Optuna for hyperparameter search
3. Lab 3: Experience RAPIDS GPU acceleration
4. Lab 4: Build reusable baseline framework

**Quick path** (if experienced with sklearn, 4-5 hours):
1. Focus on Lab 1 comparison insights
2. Skim Lab 2, focus on Optuna patterns
3. Complete Lab 3 RAPIDS (DGX Spark specific!)
4. Quick pass on Lab 4 framework

## ğŸ“‹ Before You Start
â†’ See [QUICKSTART.md](./QUICKSTART.md) for 5-minute XGBoost demo
â†’ See [QUICK_REFERENCE.md](./QUICK_REFERENCE.md) for XGBoost and cuML patterns
â†’ See [TROUBLESHOOTING.md](./TROUBLESHOOTING.md) for common questions
â†’ Ensure NGC container has XGBoost installed
