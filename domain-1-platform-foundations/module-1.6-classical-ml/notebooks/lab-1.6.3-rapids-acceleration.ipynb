{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 1.6.3: RAPIDS GPU Acceleration for Classical ML\n\n**Module:** 1.6 - Classical ML Foundations  \n**Time:** 2 hours  \n**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê (Advanced)\n\n---\n\n## üéØ Learning Objectives\n\nBy the end of this notebook, you will:\n- [ ] Understand RAPIDS cuML and cuDF architecture\n- [ ] Port scikit-learn pipelines to GPU with minimal code changes\n- [ ] Benchmark CPU vs GPU performance on large datasets\n- [ ] Know when GPU acceleration provides the biggest benefits\n- [ ] Handle memory management for GPU DataFrames\n\n---\n\n## üìö Prerequisites\n\n- Completed: Lab 1.6.1 and 1.6.2\n- Knowledge of: scikit-learn API, pandas basics\n- **Required**: RAPIDS installed (use NGC container)\n\n---\n\n## üåç Real-World Context\n\n**The Data Science Time Problem:**\n\nData scientists spend most of their time waiting:\n- Loading and preprocessing data: **30-40%** of time\n- Training models: **20-30%** of time\n- Hyperparameter tuning: **20-30%** of time\n\n**RAPIDS changes everything:**\n\n| Operation | CPU (sklearn) | GPU (cuML) | Speedup |\n|-----------|--------------|------------|----------|\n| Random Forest (1M rows) | 120 sec | 3 sec | **40x** |\n| K-Means Clustering | 45 sec | 0.5 sec | **90x** |\n| PCA | 30 sec | 0.3 sec | **100x** |\n| DataFrame operations | 10 sec | 0.1 sec | **100x** |\n\n**On your DGX Spark:**\n- 128GB unified memory = huge datasets in GPU memory\n- 6,144 CUDA cores = massive parallelism\n- 192 (5th generation) Tensor Cores for accelerated compute\n- ARM64/aarch64 architecture (use NGC containers, not pip)\n- No CPU‚ÜîGPU transfers needed with unified memory!\n\n---\n\n## üßí ELI5: GPU Acceleration for ML\n\n> **Imagine you need to count all the red M&Ms in a giant bowl...**\n>\n> **CPU approach** (scikit-learn):\n> - You have 20 helpers (CPU cores)\n> - Each helper picks up M&Ms one at a time\n> - They're very smart and can handle complex tasks\n> - But counting millions takes forever!\n>\n> **GPU approach** (RAPIDS cuML):\n> - You have 6,144 helpers (CUDA cores)!\n> - Each helper is simpler but checks M&Ms simultaneously\n> - For simple, repetitive tasks = massively faster\n>\n> **When does GPU help most?**\n> - Large datasets (more M&Ms = more parallelism)\n> - Simple operations (counting vs. solving puzzles)\n> - Many repetitive calculations (same task, different data)\n>\n> **In ML terms:** Matrix operations, distance calculations, and aggregations are perfect for GPUs because they do the same math on millions of data points.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 1: Environment Setup\n\nFirst, let's check our RAPIDS installation and DGX Spark capabilities.\n\n**Note:** If you don't have RAPIDS installed, use the NGC container:\n```bash\ndocker run --gpus all -it --rm \\\n    -v $HOME/workspace:/workspace \\\n    -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\n    --ipc=host \\\n    nvcr.io/nvidia/rapidsai/base:25.11-py3 \\\n    jupyter lab --ip=0.0.0.0 --allow-root --no-browser\n```\n\n**Important:** On DGX Spark (ARM64), always use NGC containers. Never use `pip install torch` - PyTorch ARM64 wheels require the NGC container."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check DGX Spark GPU info\n",
    "import subprocess\n",
    "\n",
    "print(\"üñ•Ô∏è DGX Spark GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,memory.free,compute_cap', \n",
    "                        '--format=csv,noheader'], capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "\n",
    "# Check unified memory\n",
    "print(\"\\nüíæ Unified Memory Info:\")\n",
    "print(\"   DGX Spark uses unified memory - CPU and GPU share 128GB!\")\n",
    "print(\"   This means no explicit CPU‚ÜîGPU transfers needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# scikit-learn (CPU)\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier as SklearnRF\n",
    "from sklearn.ensemble import RandomForestRegressor as SklearnRFReg\n",
    "from sklearn.linear_model import LogisticRegression as SklearnLR\n",
    "from sklearn.linear_model import Ridge as SklearnRidge\n",
    "from sklearn.cluster import KMeans as SklearnKMeans\n",
    "from sklearn.decomposition import PCA as SklearnPCA\n",
    "from sklearn.neighbors import KNeighborsClassifier as SklearnKNN\n",
    "from sklearn.preprocessing import StandardScaler as SklearnScaler\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "print(\"‚úÖ scikit-learn (CPU) libraries imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import RAPIDS cuML (GPU)\ntry:\n    import cudf\n    import cupy as cp\n    from cuml.ensemble import RandomForestClassifier as CumlRF\n    from cuml.ensemble import RandomForestRegressor as CumlRFReg\n    from cuml.linear_model import LogisticRegression as CumlLR\n    from cuml.linear_model import Ridge as CumlRidge\n    from cuml.cluster import KMeans as CumlKMeans\n    from cuml.decomposition import PCA as CumlPCA\n    from cuml.neighbors import KNeighborsClassifier as CumlKNN\n    from cuml.preprocessing import StandardScaler as CumlScaler\n    \n    RAPIDS_AVAILABLE = True\n    print(\"‚úÖ RAPIDS cuML (GPU) libraries imported!\")\n    print(f\"   cuDF version: {cudf.__version__}\")\n    \nexcept ImportError as e:\n    RAPIDS_AVAILABLE = False\n    print(\"‚ùå RAPIDS not available. Please use the NGC container.\")\n    print(f\"   Error: {e}\")\n    print(\"\\n   To run RAPIDS on DGX Spark, use the NGC container:\")\n    print(\"   docker run --gpus all -it --rm \\\\\")\n    print(\"       -v $HOME/workspace:/workspace \\\\\")\n    print(\"       -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\\\")\n    print(\"       --ipc=host \\\\\")\n    print(\"       nvcr.io/nvidia/rapidsai/base:25.11-py3 \\\\\")\n    print(\"       jupyter lab --ip=0.0.0.0 --allow-root --no-browser\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: cuDF - GPU DataFrames\n",
    "\n",
    "Before we benchmark ML algorithms, let's explore cuDF - GPU-accelerated DataFrames.\n",
    "\n",
    "### üßí ELI5: cuDF vs pandas\n",
    "\n",
    "> **pandas**: One person reading through a spreadsheet row by row\n",
    "> **cuDF**: 6,144 people each reading one cell simultaneously!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RAPIDS_AVAILABLE:\n",
    "    # Create a large pandas DataFrame\n",
    "    print(\"üìä Creating Large Dataset for DataFrame Benchmark...\")\n",
    "    n_rows = 5_000_000  # 5 million rows\n",
    "    n_cols = 20\n",
    "    \n",
    "    # Generate data\n",
    "    np.random.seed(42)\n",
    "    data = np.random.randn(n_rows, n_cols).astype(np.float32)\n",
    "    columns = [f'feature_{i}' for i in range(n_cols)]\n",
    "    \n",
    "    # Create pandas DataFrame\n",
    "    pdf = pd.DataFrame(data, columns=columns)\n",
    "    pdf['category'] = np.random.choice(['A', 'B', 'C', 'D'], size=n_rows)\n",
    "    \n",
    "    print(f\"   Shape: {pdf.shape}\")\n",
    "    print(f\"   Memory: {pdf.memory_usage(deep=True).sum() / 1e6:.1f} MB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è RAPIDS not available - skipping GPU DataFrame demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RAPIDS_AVAILABLE:\n",
    "    # Benchmark DataFrame operations\n",
    "    print(\"‚ö° Benchmarking DataFrame Operations\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # 1. GroupBy aggregation\n",
    "    print(\"\\n1Ô∏è‚É£ GroupBy Aggregation\")\n",
    "    \n",
    "    # pandas\n",
    "    start = time()\n",
    "    pdf_result = pdf.groupby('category').agg({'feature_0': ['mean', 'std', 'min', 'max']})\n",
    "    pandas_groupby_time = time() - start\n",
    "    print(f\"   pandas:  {pandas_groupby_time:.3f} seconds\")\n",
    "    \n",
    "    # cuDF\n",
    "    gdf = cudf.DataFrame.from_pandas(pdf)\n",
    "    start = time()\n",
    "    gdf_result = gdf.groupby('category').agg({'feature_0': ['mean', 'std', 'min', 'max']})\n",
    "    cudf_groupby_time = time() - start\n",
    "    print(f\"   cuDF:    {cudf_groupby_time:.3f} seconds\")\n",
    "    print(f\"   Speedup: {pandas_groupby_time/cudf_groupby_time:.1f}x\")\n",
    "    \n",
    "    results.append(('GroupBy', pandas_groupby_time, cudf_groupby_time))\n",
    "    \n",
    "    # 2. Sorting\n",
    "    print(\"\\n2Ô∏è‚É£ Sorting\")\n",
    "    \n",
    "    start = time()\n",
    "    pdf_sorted = pdf.sort_values('feature_0')\n",
    "    pandas_sort_time = time() - start\n",
    "    print(f\"   pandas:  {pandas_sort_time:.3f} seconds\")\n",
    "    \n",
    "    start = time()\n",
    "    gdf_sorted = gdf.sort_values('feature_0')\n",
    "    cudf_sort_time = time() - start\n",
    "    print(f\"   cuDF:    {cudf_sort_time:.3f} seconds\")\n",
    "    print(f\"   Speedup: {pandas_sort_time/cudf_sort_time:.1f}x\")\n",
    "    \n",
    "    results.append(('Sorting', pandas_sort_time, cudf_sort_time))\n",
    "    \n",
    "    # 3. Arithmetic operations\n",
    "    print(\"\\n3Ô∏è‚É£ Arithmetic Operations\")\n",
    "    \n",
    "    start = time()\n",
    "    pdf['new_feature'] = pdf['feature_0'] * pdf['feature_1'] + pdf['feature_2'] ** 2\n",
    "    pandas_arith_time = time() - start\n",
    "    print(f\"   pandas:  {pandas_arith_time:.3f} seconds\")\n",
    "    \n",
    "    start = time()\n",
    "    gdf['new_feature'] = gdf['feature_0'] * gdf['feature_1'] + gdf['feature_2'] ** 2\n",
    "    cudf_arith_time = time() - start\n",
    "    print(f\"   cuDF:    {cudf_arith_time:.3f} seconds\")\n",
    "    print(f\"   Speedup: {pandas_arith_time/cudf_arith_time:.1f}x\")\n",
    "    \n",
    "    results.append(('Arithmetic', pandas_arith_time, cudf_arith_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RAPIDS_AVAILABLE:\n",
    "    # Visualize DataFrame benchmarks\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    operations = [r[0] for r in results]\n",
    "    pandas_times = [r[1] for r in results]\n",
    "    cudf_times = [r[2] for r in results]\n",
    "    \n",
    "    x = np.arange(len(operations))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, pandas_times, width, label='pandas (CPU)', color='steelblue')\n",
    "    bars2 = ax.bar(x + width/2, cudf_times, width, label='cuDF (GPU)', color='coral')\n",
    "    \n",
    "    ax.set_ylabel('Time (seconds)')\n",
    "    ax.set_title('DataFrame Operations: pandas vs cuDF')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(operations)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add speedup annotations\n",
    "    for i, (p, c) in enumerate(zip(pandas_times, cudf_times)):\n",
    "        speedup = p / c\n",
    "        ax.annotate(f'{speedup:.0f}x', xy=(i + width/2, c), ha='center', va='bottom', \n",
    "                   fontsize=12, fontweight='bold', color='green')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Cleanup\n",
    "    del pdf, gdf\n",
    "    import gc\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: cuML - GPU Machine Learning\n",
    "\n",
    "Now let's benchmark ML algorithms. We'll compare scikit-learn (CPU) with cuML (GPU).\n",
    "\n",
    "### Key Insight: API Compatibility\n",
    "\n",
    "cuML is designed as a **drop-in replacement** for scikit-learn:\n",
    "\n",
    "```python\n",
    "# scikit-learn (CPU)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# cuML (GPU) - SAME API!\n",
    "from cuml.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate large classification dataset\n",
    "print(\"üìä Generating Large Classification Dataset...\")\n",
    "\n",
    "n_samples = 1_000_000  # 1 million samples\n",
    "n_features = 50\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_features,\n",
    "    n_informative=30,\n",
    "    n_redundant=10,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Convert to float32 (GPU-friendly)\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int32)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"   Samples: {n_samples:,}\")\n",
    "print(f\"   Features: {n_features}\")\n",
    "print(f\"   Training: {len(X_train):,}\")\n",
    "print(f\"   Testing: {len(X_test):,}\")\n",
    "print(f\"   Memory: {X.nbytes / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark function\n",
    "def benchmark_classifier(name, sklearn_cls, cuml_cls, sklearn_params, cuml_params,\n",
    "                         X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Benchmark scikit-learn vs cuML classifier.\n",
    "    \n",
    "    Returns dict with timing and accuracy results.\n",
    "    \"\"\"\n",
    "    results = {'name': name}\n",
    "    \n",
    "    # scikit-learn (CPU)\n",
    "    print(f\"\\nüîµ {name} - scikit-learn (CPU)\")\n",
    "    sklearn_model = sklearn_cls(**sklearn_params)\n",
    "    \n",
    "    start = time()\n",
    "    sklearn_model.fit(X_train, y_train)\n",
    "    results['sklearn_train'] = time() - start\n",
    "    print(f\"   Training: {results['sklearn_train']:.2f} seconds\")\n",
    "    \n",
    "    start = time()\n",
    "    sklearn_pred = sklearn_model.predict(X_test)\n",
    "    results['sklearn_infer'] = time() - start\n",
    "    results['sklearn_acc'] = accuracy_score(y_test, sklearn_pred)\n",
    "    print(f\"   Inference: {results['sklearn_infer']:.3f} seconds\")\n",
    "    print(f\"   Accuracy: {results['sklearn_acc']:.4f}\")\n",
    "    \n",
    "    # cuML (GPU)\n",
    "    if RAPIDS_AVAILABLE:\n",
    "        print(f\"\\nüü† {name} - cuML (GPU)\")\n",
    "        cuml_model = cuml_cls(**cuml_params)\n",
    "        \n",
    "        start = time()\n",
    "        cuml_model.fit(X_train, y_train)\n",
    "        results['cuml_train'] = time() - start\n",
    "        print(f\"   Training: {results['cuml_train']:.2f} seconds\")\n",
    "        \n",
    "        start = time()\n",
    "        cuml_pred = cuml_model.predict(X_test)\n",
    "        if hasattr(cuml_pred, 'to_numpy'):\n",
    "            cuml_pred = cuml_pred.to_numpy()\n",
    "        results['cuml_infer'] = time() - start\n",
    "        results['cuml_acc'] = accuracy_score(y_test, cuml_pred)\n",
    "        print(f\"   Inference: {results['cuml_infer']:.3f} seconds\")\n",
    "        print(f\"   Accuracy: {results['cuml_acc']:.4f}\")\n",
    "        \n",
    "        # Speedups\n",
    "        results['train_speedup'] = results['sklearn_train'] / results['cuml_train']\n",
    "        results['infer_speedup'] = results['sklearn_infer'] / results['cuml_infer']\n",
    "        print(f\"\\n   ‚ö° Training Speedup: {results['train_speedup']:.1f}x\")\n",
    "        print(f\"   ‚ö° Inference Speedup: {results['infer_speedup']:.1f}x\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark 1: Random Forest\n",
    "print(\"üå≤ Benchmark 1: Random Forest Classifier\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rf_results = benchmark_classifier(\n",
    "    name='Random Forest',\n",
    "    sklearn_cls=SklearnRF,\n",
    "    cuml_cls=CumlRF if RAPIDS_AVAILABLE else None,\n",
    "    sklearn_params={'n_estimators': 100, 'max_depth': 16, 'n_jobs': -1, 'random_state': 42},\n",
    "    cuml_params={'n_estimators': 100, 'max_depth': 16},\n",
    "    X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark 2: Logistic Regression\n",
    "print(\"\\nüìà Benchmark 2: Logistic Regression\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "lr_results = benchmark_classifier(\n",
    "    name='Logistic Regression',\n",
    "    sklearn_cls=SklearnLR,\n",
    "    cuml_cls=CumlLR if RAPIDS_AVAILABLE else None,\n",
    "    sklearn_params={'max_iter': 1000, 'n_jobs': -1},\n",
    "    cuml_params={'max_iter': 1000},\n",
    "    X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark 3: K-Nearest Neighbors\n",
    "# Note: KNN is very expensive on CPU for large datasets\n",
    "print(\"\\nüë• Benchmark 3: K-Nearest Neighbors\")\n",
    "print(\"=\" * 60)\n",
    "print(\"   (Using subset for CPU to avoid excessive wait time)\")\n",
    "\n",
    "# Use subset for sklearn to keep benchmark reasonable\n",
    "X_train_knn = X_train[:100_000]\n",
    "y_train_knn = y_train[:100_000]\n",
    "X_test_knn = X_test[:20_000]\n",
    "y_test_knn = y_test[:20_000]\n",
    "\n",
    "knn_results = benchmark_classifier(\n",
    "    name='K-Nearest Neighbors',\n",
    "    sklearn_cls=SklearnKNN,\n",
    "    cuml_cls=CumlKNN if RAPIDS_AVAILABLE else None,\n",
    "    sklearn_params={'n_neighbors': 5, 'n_jobs': -1},\n",
    "    cuml_params={'n_neighbors': 5},\n",
    "    X_train=X_train_knn, X_test=X_test_knn, y_train=y_train_knn, y_test=y_test_knn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark 4: K-Means Clustering\n",
    "print(\"\\nüéØ Benchmark 4: K-Means Clustering\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def benchmark_clustering(name, sklearn_cls, cuml_cls, sklearn_params, cuml_params, X):\n",
    "    results = {'name': name}\n",
    "    \n",
    "    # scikit-learn\n",
    "    print(f\"\\nüîµ {name} - scikit-learn (CPU)\")\n",
    "    sklearn_model = sklearn_cls(**sklearn_params)\n",
    "    start = time()\n",
    "    sklearn_model.fit(X)\n",
    "    results['sklearn_time'] = time() - start\n",
    "    print(f\"   Time: {results['sklearn_time']:.2f} seconds\")\n",
    "    print(f\"   Inertia: {sklearn_model.inertia_:.2f}\")\n",
    "    \n",
    "    # cuML\n",
    "    if RAPIDS_AVAILABLE and cuml_cls:\n",
    "        print(f\"\\nüü† {name} - cuML (GPU)\")\n",
    "        cuml_model = cuml_cls(**cuml_params)\n",
    "        start = time()\n",
    "        cuml_model.fit(X)\n",
    "        results['cuml_time'] = time() - start\n",
    "        print(f\"   Time: {results['cuml_time']:.2f} seconds\")\n",
    "        print(f\"   Inertia: {cuml_model.inertia_:.2f}\")\n",
    "        \n",
    "        results['speedup'] = results['sklearn_time'] / results['cuml_time']\n",
    "        print(f\"\\n   ‚ö° Speedup: {results['speedup']:.1f}x\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "kmeans_results = benchmark_clustering(\n",
    "    name='K-Means',\n",
    "    sklearn_cls=SklearnKMeans,\n",
    "    cuml_cls=CumlKMeans if RAPIDS_AVAILABLE else None,\n",
    "    sklearn_params={'n_clusters': 10, 'n_init': 10, 'max_iter': 300, 'random_state': 42},\n",
    "    cuml_params={'n_clusters': 10, 'n_init': 10, 'max_iter': 300},\n",
    "    X=X_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark 5: PCA\n",
    "print(\"\\nüìâ Benchmark 5: Principal Component Analysis (PCA)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def benchmark_pca(name, sklearn_cls, cuml_cls, sklearn_params, cuml_params, X):\n",
    "    results = {'name': name}\n",
    "    \n",
    "    # scikit-learn\n",
    "    print(f\"\\nüîµ {name} - scikit-learn (CPU)\")\n",
    "    sklearn_model = sklearn_cls(**sklearn_params)\n",
    "    start = time()\n",
    "    X_transformed_sklearn = sklearn_model.fit_transform(X)\n",
    "    results['sklearn_time'] = time() - start\n",
    "    print(f\"   Time: {results['sklearn_time']:.2f} seconds\")\n",
    "    print(f\"   Explained variance ratio sum: {sklearn_model.explained_variance_ratio_.sum():.4f}\")\n",
    "    \n",
    "    # cuML\n",
    "    if RAPIDS_AVAILABLE and cuml_cls:\n",
    "        print(f\"\\nüü† {name} - cuML (GPU)\")\n",
    "        cuml_model = cuml_cls(**cuml_params)\n",
    "        start = time()\n",
    "        X_transformed_cuml = cuml_model.fit_transform(X)\n",
    "        results['cuml_time'] = time() - start\n",
    "        print(f\"   Time: {results['cuml_time']:.2f} seconds\")\n",
    "        print(f\"   Explained variance ratio sum: {cuml_model.explained_variance_ratio_.sum():.4f}\")\n",
    "        \n",
    "        results['speedup'] = results['sklearn_time'] / results['cuml_time']\n",
    "        print(f\"\\n   ‚ö° Speedup: {results['speedup']:.1f}x\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "pca_results = benchmark_pca(\n",
    "    name='PCA',\n",
    "    sklearn_cls=SklearnPCA,\n",
    "    cuml_cls=CumlPCA if RAPIDS_AVAILABLE else None,\n",
    "    sklearn_params={'n_components': 10},\n",
    "    cuml_params={'n_components': 10},\n",
    "    X=X_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Summary Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RAPIDS_AVAILABLE:\n",
    "    # Collect all results\n",
    "    all_results = [\n",
    "        ('Random Forest', rf_results.get('sklearn_train', 0), rf_results.get('cuml_train', 0.001)),\n",
    "        ('Logistic Reg.', lr_results.get('sklearn_train', 0), lr_results.get('cuml_train', 0.001)),\n",
    "        ('KNN', knn_results.get('sklearn_train', 0), knn_results.get('cuml_train', 0.001)),\n",
    "        ('K-Means', kmeans_results.get('sklearn_time', 0), kmeans_results.get('cuml_time', 0.001)),\n",
    "        ('PCA', pca_results.get('sklearn_time', 0), pca_results.get('cuml_time', 0.001)),\n",
    "    ]\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 1. Training time comparison\n",
    "    ax1 = axes[0]\n",
    "    names = [r[0] for r in all_results]\n",
    "    sklearn_times = [r[1] for r in all_results]\n",
    "    cuml_times = [r[2] for r in all_results]\n",
    "    \n",
    "    x = np.arange(len(names))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, sklearn_times, width, label='scikit-learn (CPU)', color='steelblue')\n",
    "    bars2 = ax1.bar(x + width/2, cuml_times, width, label='cuML (GPU)', color='coral')\n",
    "    \n",
    "    ax1.set_ylabel('Time (seconds)')\n",
    "    ax1.set_title('Training Time: scikit-learn vs cuML')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(names, rotation=15)\n",
    "    ax1.legend()\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # 2. Speedup comparison\n",
    "    ax2 = axes[1]\n",
    "    speedups = [s/c if c > 0 else 0 for s, c in zip(sklearn_times, cuml_times)]\n",
    "    colors = plt.cm.Greens(np.linspace(0.4, 0.8, len(speedups)))\n",
    "    \n",
    "    bars = ax2.bar(names, speedups, color=colors)\n",
    "    ax2.axhline(y=1, color='red', linestyle='--', label='Break-even')\n",
    "    ax2.set_ylabel('Speedup (x times faster)')\n",
    "    ax2.set_title('GPU Speedup over CPU')\n",
    "    ax2.set_xticklabels(names, rotation=15)\n",
    "    \n",
    "    # Add speedup annotations\n",
    "    for bar, speedup in zip(bars, speedups):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{speedup:.0f}x', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('rapids_benchmark_summary.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üíæ Saved rapids_benchmark_summary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "if RAPIDS_AVAILABLE:\n",
    "    print(\"üìä Benchmark Summary Table\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    summary_data = {\n",
    "        'Algorithm': names,\n",
    "        'sklearn (CPU)': [f'{t:.2f}s' for t in sklearn_times],\n",
    "        'cuML (GPU)': [f'{t:.2f}s' for t in cuml_times],\n",
    "        'Speedup': [f'{s:.1f}x' for s in speedups]\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    avg_speedup = np.mean(speedups)\n",
    "    print(f\"\\nüöÄ Average Speedup: {avg_speedup:.1f}x\")\n",
    "    print(f\"   Dataset Size: {n_samples:,} samples √ó {n_features} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: When to Use GPU Acceleration\n",
    "\n",
    "GPU acceleration isn't always the right choice. Here's when it helps most:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When to use GPU acceleration\n",
    "print(\"üí° When to Use GPU Acceleration\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "guidance = \"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    GPU ACCELERATION DECISION GUIDE                   ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                      ‚îÇ\n",
    "‚îÇ  ‚úÖ USE GPU (cuML) WHEN:                                             ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                              ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Dataset > 100K rows                                              ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Many features (> 50)                                             ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Algorithms: KNN, K-Means, PCA, Random Forest                     ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Iterative training (hyperparameter tuning)                       ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Real-time inference requirements                                 ‚îÇ\n",
    "‚îÇ                                                                      ‚îÇ\n",
    "‚îÇ  ‚ùå STICK WITH CPU (sklearn) WHEN:                                   ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                   ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Dataset < 10K rows (GPU overhead dominates)                      ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Simple models (linear regression on small data)                  ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Memory-limited (GPU memory is smaller than system RAM)           ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Debugging/prototyping (sklearn has better error messages)        ‚îÇ\n",
    "‚îÇ                                                                      ‚îÇ\n",
    "‚îÇ  üí° DGX SPARK ADVANTAGE:                                             ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                             ‚îÇ\n",
    "‚îÇ  ‚Ä¢ 128GB unified memory = no CPU‚ÜîGPU transfers!                     ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Can fit huge datasets entirely in GPU memory                     ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Sweet spot: 100K-10M rows                                        ‚îÇ\n",
    "‚îÇ                                                                      ‚îÇ\n",
    "‚îÇ  ‚ö° BIGGEST SPEEDUPS:                                                ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                              ‚îÇ\n",
    "‚îÇ  ‚Ä¢ K-Nearest Neighbors: 50-100x (distance calculations)             ‚îÇ\n",
    "‚îÇ  ‚Ä¢ K-Means: 50-100x (many iterations)                               ‚îÇ\n",
    "‚îÇ  ‚Ä¢ PCA/SVD: 50-100x (matrix operations)                             ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Random Forest: 10-50x (tree building)                            ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Logistic Regression: 5-20x (iterative optimization)              ‚îÇ\n",
    "‚îÇ                                                                      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\"\"\"\n",
    "print(guidance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Complete Pipeline Example\n",
    "\n",
    "Let's create a complete ML pipeline using RAPIDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RAPIDS_AVAILABLE:\n",
    "    print(\"üîÑ Complete GPU-Accelerated ML Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Start pipeline timer\n",
    "    pipeline_start = time()\n",
    "    \n",
    "    # Step 1: Load data into GPU DataFrame\n",
    "    print(\"\\n1Ô∏è‚É£ Loading data into GPU...\")\n",
    "    X_train_gdf = cudf.DataFrame(X_train)\n",
    "    X_test_gdf = cudf.DataFrame(X_test)\n",
    "    y_train_gdf = cudf.Series(y_train)\n",
    "    y_test_gdf = cudf.Series(y_test)\n",
    "    print(f\"   ‚úÖ Data loaded to GPU\")\n",
    "    \n",
    "    # Step 2: Preprocessing - StandardScaler\n",
    "    print(\"\\n2Ô∏è‚É£ Scaling features (cuML StandardScaler)...\")\n",
    "    scaler = CumlScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_gdf)\n",
    "    X_test_scaled = scaler.transform(X_test_gdf)\n",
    "    print(f\"   ‚úÖ Features scaled\")\n",
    "    \n",
    "    # Step 3: Dimensionality reduction - PCA\n",
    "    print(\"\\n3Ô∏è‚É£ Reducing dimensions (cuML PCA)...\")\n",
    "    pca = CumlPCA(n_components=20)\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "    print(f\"   ‚úÖ Reduced to 20 components (variance retained: {pca.explained_variance_ratio_.sum():.2%})\")\n",
    "    \n",
    "    # Step 4: Train Random Forest\n",
    "    print(\"\\n4Ô∏è‚É£ Training Random Forest (cuML)...\")\n",
    "    rf = CumlRF(n_estimators=100, max_depth=16)\n",
    "    rf.fit(X_train_pca, y_train_gdf)\n",
    "    print(f\"   ‚úÖ Model trained\")\n",
    "    \n",
    "    # Step 5: Predictions\n",
    "    print(\"\\n5Ô∏è‚É£ Making predictions...\")\n",
    "    y_pred = rf.predict(X_test_pca)\n",
    "    y_pred_np = y_pred.to_numpy() if hasattr(y_pred, 'to_numpy') else np.array(y_pred)\n",
    "    \n",
    "    # Step 6: Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred_np)\n",
    "    \n",
    "    pipeline_time = time() - pipeline_start\n",
    "    \n",
    "    print(f\"\\n‚úÖ Pipeline Complete!\")\n",
    "    print(f\"   Total Time: {pipeline_time:.2f} seconds\")\n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"\\n   This entire pipeline ran on GPU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Benchmark on Different Dataset Sizes\n",
    "\n",
    "How does the speedup change with dataset size?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Try sizes: 10K, 100K, 500K, 1M, 5M. Plot speedup vs dataset size.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Benchmark different dataset sizes and plot speedup curve\n",
    "\n",
    "# sizes = [10_000, 100_000, 500_000, 1_000_000]\n",
    "# speedups = []\n",
    "# \n",
    "# for size in sizes:\n",
    "#     X, y = make_classification(n_samples=size, ...)\n",
    "#     # ... benchmark sklearn vs cuml ...\n",
    "#     speedups.append(sklearn_time / cuml_time)\n",
    "# \n",
    "# plt.plot(sizes, speedups, 'o-')\n",
    "# plt.xlabel('Dataset Size')\n",
    "# plt.ylabel('Speedup (x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Port a Full sklearn Pipeline\n",
    "\n",
    "Convert this sklearn pipeline to cuML:\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "sklearn_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=20)),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100))\n",
    "])\n",
    "```\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "cuML has a Pipeline class too, or you can chain the operations manually.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### üí° cuML Pipeline API\n\ncuML provides a Pipeline class similar to scikit-learn:\n\n```python\n# Import cuML Pipeline\nfrom cuml.pipeline import Pipeline as CumlPipeline\nfrom cuml.preprocessing import StandardScaler as CumlScaler\nfrom cuml.decomposition import PCA as CumlPCA\nfrom cuml.ensemble import RandomForestClassifier as CumlRF\n\n# Create a GPU-accelerated pipeline\ncuml_pipe = CumlPipeline([\n    ('scaler', CumlScaler()),\n    ('pca', CumlPCA(n_components=20)),\n    ('classifier', CumlRF(n_estimators=100))\n])\n\n# Use like sklearn Pipeline\ncuml_pipe.fit(X_train_cudf, y_train_cudf)\npredictions = cuml_pipe.predict(X_test_cudf)\n```\n\n**Key notes:**\n- Input should be cuDF DataFrames or cupy arrays\n- All transformers in the pipeline run on GPU\n- Output is typically a cupy array (use `.to_numpy()` if needed)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Port the sklearn pipeline to cuML\n",
    "\n",
    "# from cuml.pipeline import Pipeline as CumlPipeline\n",
    "# \n",
    "# cuml_pipe = CumlPipeline([\n",
    "#     ('scaler', CumlScaler()),\n",
    "#     ('pca', CumlPCA(n_components=20)),\n",
    "#     ('classifier', CumlRF(n_estimators=100))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Memory Profiling\n",
    "\n",
    "Monitor GPU memory usage during training.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Use `nvidia-smi` or `cupy.get_default_memory_pool().used_bytes()` to monitor memory.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### üí° GPU Memory Profiling with CuPy\n\nCuPy provides memory pool utilities to monitor GPU memory usage:\n\n```python\nimport cupy as cp\n\n# Get current GPU memory usage\ndef get_gpu_memory_gb():\n    \"\"\"Returns GPU memory used by CuPy in GB.\"\"\"\n    return cp.get_default_memory_pool().used_bytes() / 1e9\n\n# Check memory before/after operations\nprint(f\"Before training: {get_gpu_memory_gb():.2f} GB\")\nmodel.fit(X_train, y_train)\nprint(f\"After training: {get_gpu_memory_gb():.2f} GB\")\n\n# Free unused GPU memory\ncp.get_default_memory_pool().free_all_blocks()\nprint(f\"After cleanup: {get_gpu_memory_gb():.2f} GB\")\n```\n\n**Memory pool methods:**\n- `used_bytes()` - Currently allocated memory\n- `total_bytes()` - Total memory managed by the pool\n- `free_all_blocks()` - Release unused memory back to GPU",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "# Monitor GPU memory usage\n",
    "\n",
    "# import cupy as cp\n",
    "# \n",
    "# def get_gpu_memory():\n",
    "#     return cp.get_default_memory_pool().used_bytes() / 1e9\n",
    "# \n",
    "# print(f\"Before: {get_gpu_memory():.2f} GB\")\n",
    "# # ... train model ...\n",
    "# print(f\"After: {get_gpu_memory():.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting to Convert Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Using float64 (wastes GPU memory and is slower)\n",
    "# X = X.astype(np.float64)\n",
    "\n",
    "# ‚úÖ Right: Use float32 for GPU\n",
    "# X = X.astype(np.float32)\n",
    "\n",
    "print(\"üí° Always use float32 for GPU operations!\")\n",
    "print(\"   float64 ‚Üí float32 can halve memory usage and improve performance.\")\n",
    "print(\"   cuML often requires float32 anyway.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Not Cleaning Up GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Letting GPU memory accumulate\n",
    "# for i in range(100):\n",
    "#     model = CumlRF()\n",
    "#     model.fit(X, y)  # Memory keeps growing!\n",
    "\n",
    "# ‚úÖ Right: Clean up after each iteration\n",
    "# import gc\n",
    "# import cupy as cp\n",
    "# \n",
    "# for i in range(100):\n",
    "#     model = CumlRF()\n",
    "#     model.fit(X, y)\n",
    "#     del model\n",
    "#     gc.collect()\n",
    "#     cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "print(\"üí° Clean up GPU memory in loops!\")\n",
    "print(\"   Use: del model; gc.collect(); cp.get_default_memory_pool().free_all_blocks()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Unnecessary CPU‚ÜîGPU Transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Converting back and forth\n",
    "# gdf = cudf.from_pandas(pdf)\n",
    "# result = gdf.groupby('col').sum()\n",
    "# pdf_result = result.to_pandas()  # Unnecessary transfer!\n",
    "# another_result = cudf.from_pandas(pdf_result)  # Back to GPU??\n",
    "\n",
    "# ‚úÖ Right: Stay on GPU as long as possible\n",
    "# gdf = cudf.from_pandas(pdf)  # Transfer once\n",
    "# result = gdf.groupby('col').sum()  # Stay on GPU\n",
    "# final = result.merge(other_gdf)  # Still on GPU\n",
    "# pdf_final = final.to_pandas()  # Transfer at end only\n",
    "\n",
    "print(\"üí° Minimize CPU‚ÜîGPU transfers!\")\n",
    "print(\"   Transfer to GPU once at the start.\")\n",
    "print(\"   Do all processing on GPU.\")\n",
    "print(\"   Transfer back to CPU only at the end.\")\n",
    "print(\"\\n   DGX Spark's unified memory helps, but avoiding transfers is still faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "Congratulations! You've mastered GPU acceleration for classical ML. You've learned:\n",
    "\n",
    "- ‚úÖ **cuDF basics**: GPU-accelerated DataFrames with pandas-like API\n",
    "- ‚úÖ **cuML algorithms**: Drop-in sklearn replacements running on GPU\n",
    "- ‚úÖ **Benchmarking**: 10-100x speedups on large datasets\n",
    "- ‚úÖ **When to use GPU**: Large datasets, many iterations, distance calculations\n",
    "- ‚úÖ **Best practices**: float32, memory cleanup, minimize transfers\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**The Big Data Challenge:**\n",
    "\n",
    "1. Download the Higgs Boson dataset (11M samples): https://archive.ics.uci.edu/dataset/280/higgs\n",
    "2. Try loading it with pandas (will be slow!) vs cuDF\n",
    "3. Train a Random Forest classifier on the full dataset\n",
    "4. Report your speedups!\n",
    "\n",
    "This is a real-world ML challenge that would take hours on CPU but minutes on GPU.\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [RAPIDS cuML Documentation](https://docs.rapids.ai/api/cuml/stable/)\n",
    "- [RAPIDS cuDF Documentation](https://docs.rapids.ai/api/cudf/stable/)\n",
    "- [RAPIDS AI Getting Started](https://rapids.ai/start.html)\n",
    "- [cuML vs scikit-learn Benchmarks](https://medium.com/rapids-ai/)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "import gc\n",
    "\n",
    "# Delete large arrays\n",
    "del X, y, X_train, X_test, y_train, y_test\n",
    "\n",
    "if RAPIDS_AVAILABLE:\n",
    "    import cupy as cp\n",
    "    # Free GPU memory\n",
    "    gc.collect()\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Memory cleaned up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚û°Ô∏è Next Steps\n",
    "\n",
    "Continue to **Lab 1.6.4: Baseline Comparison Framework** to create a reusable framework for comparing models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}