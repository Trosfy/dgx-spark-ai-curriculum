{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.6.1: Tabular Data Challenge - XGBoost vs Neural Networks\n",
    "\n",
    "**Module:** 1.6 - Classical ML Foundations  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Train XGBoost models on tabular data with GPU acceleration\n",
    "- [ ] Build equivalent neural networks for fair comparison\n",
    "- [ ] Understand why XGBoost often wins on tabular data\n",
    "- [ ] Extract and interpret feature importance from tree models\n",
    "- [ ] Make informed decisions about when to use each approach\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Module 1.5 (Neural Network Fundamentals)\n",
    "- Knowledge of: Basic Python, NumPy, basic neural network concepts\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The $1 Million Question:** When should you use XGBoost vs a neural network?\n",
    "\n",
    "This isn't academic‚Äîit's the question that determines whether your model ships in 2 days or 2 months:\n",
    "\n",
    "- **Banks** use XGBoost for credit scoring (interpretability required by law!)\n",
    "- **Kaggle competitions** are dominated by XGBoost on tabular data\n",
    "- **Tech companies** start every ML project with an XGBoost baseline\n",
    "- **Healthcare** prefers tree models because doctors need to understand decisions\n",
    "\n",
    "In 2022, a landmark paper \"Why do tree-based models still outperform deep learning on tabular data?\" confirmed what practitioners knew: **XGBoost wins on tabular data most of the time**.\n",
    "\n",
    "Today, you'll see this firsthand and understand *why*.\n",
    "\n",
    "---\n",
    "\n",
    "## üßí ELI5: Decision Trees and Gradient Boosting\n",
    "\n",
    "> **Imagine you're playing 20 Questions...**\n",
    ">\n",
    "> In 20 Questions, you ask yes/no questions to guess what someone is thinking:\n",
    "> - \"Is it alive?\" ‚Üí Yes\n",
    "> - \"Is it a mammal?\" ‚Üí Yes  \n",
    "> - \"Does it live in water?\" ‚Üí Yes\n",
    "> - \"Is it a whale?\" ‚Üí Yes! üêã\n",
    ">\n",
    "> A **Decision Tree** works exactly like this! It asks questions about your data:\n",
    "> - \"Is income > $50K?\" ‚Üí Yes\n",
    "> - \"Is age > 30?\" ‚Üí No\n",
    "> - \"Has credit history > 5 years?\" ‚Üí Yes\n",
    "> - Prediction: **Approve loan** ‚úÖ\n",
    ">\n",
    "> **But what about XGBoost (Gradient Boosting)?**\n",
    ">\n",
    "> Imagine you're terrible at 20 Questions. So you ask 100 friends to play, and each friend:\n",
    "> 1. Plays the game\n",
    "> 2. Sees where the previous friends made mistakes\n",
    "> 3. Focuses on fixing THOSE specific mistakes\n",
    ">\n",
    "> XGBoost is like this team of 100 friends, where each new tree specifically learns to fix the errors of all previous trees!\n",
    ">\n",
    "> **In AI terms:** Gradient boosting builds trees sequentially, with each new tree trained to predict the *residual errors* of the ensemble so far.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup & Data Loading\n",
    "\n",
    "Let's set up our environment and load a classic tabular dataset: the California Housing dataset.\n",
    "\n",
    "### Why California Housing?\n",
    "- Real estate prediction is a perfect tabular problem\n",
    "- Mix of numerical features (median income, house age, etc.)\n",
    "- Reasonable size (~20K samples) for quick experiments\n",
    "- Well-understood baseline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check our DGX Spark GPU!\n",
    "import torch\n",
    "\n",
    "print(\"üîç System Check\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn for data and preprocessing\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# XGBoost - The Kaggle Champion!\n",
    "import xgboost as xgb\n",
    "\n",
    "# PyTorch for neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California Housing dataset\n",
    "print(\"üì¶ Loading California Housing Dataset...\")\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Create a DataFrame for easier exploration\n",
    "df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "df['Target'] = housing.target  # Median house value in $100,000s\n",
    "\n",
    "print(f\"\\nüìä Dataset Shape: {df.shape}\")\n",
    "print(f\"üìù Features: {list(housing.feature_names)}\")\n",
    "print(f\"üéØ Target: Median house value (in $100,000s)\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's understand our features better\n",
    "print(\"üìä Feature Descriptions:\")\n",
    "print(\"=\" * 60)\n",
    "feature_descriptions = {\n",
    "    'MedInc': 'Median income in block group (in $10,000s)',\n",
    "    'HouseAge': 'Median house age in block group (years)',\n",
    "    'AveRooms': 'Average number of rooms per household',\n",
    "    'AveBedrms': 'Average number of bedrooms per household',\n",
    "    'Population': 'Block group population',\n",
    "    'AveOccup': 'Average number of household members',\n",
    "    'Latitude': 'Block group latitude',\n",
    "    'Longitude': 'Block group longitude'\n",
    "}\n",
    "\n",
    "for feature, desc in feature_descriptions.items():\n",
    "    print(f\"  ‚Ä¢ {feature}: {desc}\")\n",
    "\n",
    "print(\"\\nüìà Statistical Summary:\")\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Target distribution\n",
    "axes[0].hist(df['Target'], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].axvline(df['Target'].median(), color='red', linestyle='--', linewidth=2, label=f'Median: ${df[\"Target\"].median()*100000:,.0f}')\n",
    "axes[0].set_xlabel('Median House Value ($100,000s)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of House Prices')\n",
    "axes[0].legend()\n",
    "\n",
    "# Correlation with target\n",
    "correlations = df.corr()['Target'].drop('Target').sort_values()\n",
    "colors = ['green' if c > 0 else 'red' for c in correlations]\n",
    "axes[1].barh(correlations.index, correlations.values, color=colors, alpha=0.7)\n",
    "axes[1].set_xlabel('Correlation with House Price')\n",
    "axes[1].set_title('Feature Correlations with Target')\n",
    "axes[1].axvline(0, color='black', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Key Insight: MedInc (median income) has the strongest correlation with house prices!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "We loaded the California Housing dataset and discovered:\n",
    "1. **20,640 samples** with 8 features each\n",
    "2. **MedInc** (median income) is the strongest predictor of house prices\n",
    "3. **Location** (Lat/Long) also matters (beachfront property costs more!)\n",
    "4. The target is capped at $500K (values above were truncated)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Preparation\n",
    "\n",
    "Now let's prepare our data for both XGBoost and neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df.drop('Target', axis=1).values\n",
    "y = df['Target'].values\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "# 70% train, 15% validation, 15% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"üìä Data Split:\")\n",
    "print(f\"  ‚Ä¢ Training:   {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"  ‚Ä¢ Validation: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"  ‚Ä¢ Test:       {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features for neural network (XGBoost doesn't need this!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚ö†Ô∏è Important Note:\")\n",
    "print(\"  ‚Ä¢ XGBoost: Does NOT need feature scaling (tree-based!)\")\n",
    "print(\"  ‚Ä¢ Neural Network: NEEDS feature scaling (gradient-based!)\")\n",
    "print(\"\\nThis is one reason XGBoost is easier to use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: XGBoost - The Gradient Boosting Champion\n",
    "\n",
    "### üßí ELI5: What Makes XGBoost Special?\n",
    "\n",
    "> **Think of XGBoost as a smart study group...**\n",
    ">\n",
    "> Imagine a class where students help each other prepare for an exam:\n",
    "> 1. **Student 1** studies everything, but makes some mistakes on practice problems\n",
    "> 2. **Student 2** only studies the problems Student 1 got wrong\n",
    "> 3. **Student 3** focuses on problems both previous students struggled with\n",
    "> 4. And so on...\n",
    ">\n",
    "> By the end, this study group can solve any problem because each student specializes in fixing the previous students' weaknesses!\n",
    ">\n",
    "> **XGBoost works the same way:**\n",
    "> - Each tree focuses on the errors made by previous trees\n",
    "> - Trees are shallow (usually 3-10 levels) but there are many of them\n",
    "> - The final prediction is the sum of all trees' predictions\n",
    "\n",
    "### Why XGBoost Dominates Tabular Data\n",
    "\n",
    "1. **No preprocessing needed**: Handles raw features directly\n",
    "2. **Built-in regularization**: L1/L2 prevents overfitting\n",
    "3. **Missing values**: Learns optimal direction for missing data\n",
    "4. **Feature importance**: Free interpretability!\n",
    "5. **Fast**: Highly optimized C++ with GPU support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train XGBoost with GPU acceleration\nprint(\"üöÄ Training XGBoost on GPU...\")\nprint(\"=\" * 60)\n\n# XGBoost parameters - good defaults for tabular regression\nxgb_params = {\n    'objective': 'reg:squarederror',  # Regression task\n    'eval_metric': 'rmse',             # Root Mean Squared Error\n    'max_depth': 6,                    # Tree depth (controls complexity)\n    'learning_rate': 0.1,              # Step size (Œ∑)\n    'n_estimators': 100,               # Number of trees\n    'subsample': 0.8,                  # Row sampling\n    'colsample_bytree': 0.8,           # Column sampling\n    'tree_method': 'hist',             # Fast histogram-based algorithm\n    'device': 'cuda' if torch.cuda.is_available() else 'cpu',  # Use GPU if available\n    'random_state': 42,\n    'verbosity': 0\n}\n\n# Create and train model\nxgb_model = xgb.XGBRegressor(**xgb_params)\n\n# Time the training\nstart_time = time()\nxgb_model.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)],\n    verbose=False\n)\nxgb_train_time = time() - start_time\n\n# Predictions\nstart_time = time()\nxgb_pred_train = xgb_model.predict(X_train)\nxgb_pred_val = xgb_model.predict(X_val)\nxgb_pred_test = xgb_model.predict(X_test)\nxgb_inference_time = time() - start_time\n\n# Calculate metrics\nxgb_metrics = {\n    'train_rmse': np.sqrt(mean_squared_error(y_train, xgb_pred_train)),\n    'val_rmse': np.sqrt(mean_squared_error(y_val, xgb_pred_val)),\n    'test_rmse': np.sqrt(mean_squared_error(y_test, xgb_pred_test)),\n    'test_r2': r2_score(y_test, xgb_pred_test),\n    'test_mae': mean_absolute_error(y_test, xgb_pred_test),\n    'train_time': xgb_train_time,\n    'inference_time': xgb_inference_time\n}\n\nprint(f\"\\n‚úÖ XGBoost Training Complete!\")\nprint(f\"\\nüìä Results:\")\nprint(f\"  ‚Ä¢ Training RMSE:   ${xgb_metrics['train_rmse']*100000:,.0f}\")\nprint(f\"  ‚Ä¢ Validation RMSE: ${xgb_metrics['val_rmse']*100000:,.0f}\")\nprint(f\"  ‚Ä¢ Test RMSE:       ${xgb_metrics['test_rmse']*100000:,.0f}\")\nprint(f\"  ‚Ä¢ Test R¬≤ Score:   {xgb_metrics['test_r2']:.4f}\")\nprint(f\"  ‚Ä¢ Test MAE:        ${xgb_metrics['test_mae']*100000:,.0f}\")\nprint(f\"\\n‚è±Ô∏è Timing:\")\nprint(f\"  ‚Ä¢ Training Time:   {xgb_metrics['train_time']:.2f} seconds\")\nprint(f\"  ‚Ä¢ Inference Time:  {xgb_metrics['inference_time']*1000:.2f} ms\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize XGBoost Feature Importance\n",
    "print(\"üìä XGBoost Feature Importance\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': housing.feature_names,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.barh(importance_df['Feature'], importance_df['Importance'], \n",
    "               color='steelblue', alpha=0.8)\n",
    "ax.set_xlabel('Feature Importance (Gain)')\n",
    "ax.set_title('XGBoost Feature Importance - Which Features Matter Most?')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, importance_df['Importance']):\n",
    "    ax.text(val + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "            f'{val:.3f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Insight: MedInc (median income) is by far the most important feature!\")\n",
    "print(\"   This matches our correlation analysis and makes intuitive sense.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "XGBoost trained in under a second on GPU and achieved:\n",
    "- **R¬≤ of ~0.83**: Explains 83% of the variance in house prices!\n",
    "- **RMSE of ~$47,000**: Average error in predictions\n",
    "- **Automatic feature importance**: We know MedInc is the key driver\n",
    "\n",
    "Now let's see if a neural network can do better...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Neural Network Approach\n",
    "\n",
    "### üßí ELI5: Neural Networks vs Decision Trees\n",
    "\n",
    "> **Trees ask questions. Neural networks learn patterns.**\n",
    ">\n",
    "> Think of it this way:\n",
    "> - **Decision Tree**: \"Is the income above $50K? Yes? Then check if age > 30...\" (Explicit rules)\n",
    "> - **Neural Network**: \"I've seen millions of examples. There's a complex pattern here...\" (Learned features)\n",
    ">\n",
    "> **Why neural networks sometimes struggle with tabular data:**\n",
    "> 1. Tabular features are often **heterogeneous** (income, age, location = different scales/meanings)\n",
    "> 2. Important patterns may be **axis-aligned** (income > threshold) - trees handle this naturally\n",
    "> 3. Need more data to learn what trees \"know\" implicitly\n",
    "\n",
    "Let's build a fair comparison neural network:\n",
    "- Similar number of parameters to XGBoost's effective capacity\n",
    "- Modern techniques: BatchNorm, Dropout, Adam optimizer\n",
    "- Similar training time budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a modern MLP for tabular data\n",
    "class TabularMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Modern MLP architecture for tabular data.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input ‚Üí Linear ‚Üí BatchNorm ‚Üí ReLU ‚Üí Dropout\n",
    "    - Repeat for hidden layers\n",
    "    - Final Linear ‚Üí Output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims=[256, 128, 64], dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Build hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze(-1)\n",
    "\n",
    "# Create model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "nn_model = TabularMLP(input_dim=8, hidden_dims=[256, 128, 64], dropout=0.2).to(device)\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in nn_model.parameters())\n",
    "print(f\"üìê Neural Network Architecture:\")\n",
    "print(f\"  ‚Ä¢ Input: 8 features\")\n",
    "print(f\"  ‚Ä¢ Hidden: 256 ‚Üí 128 ‚Üí 64\")\n",
    "print(f\"  ‚Ä¢ Output: 1 (house price)\")\n",
    "print(f\"  ‚Ä¢ Total Parameters: {n_params:,}\")\n",
    "print(f\"  ‚Ä¢ Device: {device}\")\n",
    "print(\"\\n\" + str(nn_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare PyTorch datasets\n",
    "def to_tensor(x):\n",
    "    return torch.FloatTensor(x)\n",
    "\n",
    "# Create datasets (using scaled data for neural network!)\n",
    "train_dataset = TensorDataset(to_tensor(X_train_scaled), to_tensor(y_train))\n",
    "val_dataset = TensorDataset(to_tensor(X_val_scaled), to_tensor(y_val))\n",
    "test_dataset = TensorDataset(to_tensor(X_test_scaled), to_tensor(y_test))\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"üì¶ DataLoaders created with batch_size={batch_size}\")\n",
    "print(f\"  ‚Ä¢ Training batches: {len(train_loader)}\")\n",
    "print(f\"  ‚Ä¢ Validation batches: {len(val_loader)}\")\n",
    "print(f\"  ‚Ä¢ Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * len(y_batch)\n",
    "    \n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            pred = model(X_batch)\n",
    "            loss = criterion(pred, y_batch)\n",
    "            total_loss += loss.item() * len(y_batch)\n",
    "            predictions.append(pred.cpu().numpy())\n",
    "            targets.append(y_batch.cpu().numpy())\n",
    "    \n",
    "    predictions = np.concatenate(predictions)\n",
    "    targets = np.concatenate(targets)\n",
    "    \n",
    "    return total_loss / len(loader.dataset), predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the neural network\n",
    "print(\"üöÄ Training Neural Network on GPU...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Setup\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(nn_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
    "\n",
    "# Training history\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "best_val_loss = float('inf')\n",
    "patience = 20\n",
    "patience_counter = 0\n",
    "n_epochs = 200\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Train\n",
    "    train_loss = train_epoch(nn_model, train_loader, optimizer, criterion, device)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss, _, _ = evaluate(nn_model, val_loader, criterion, device)\n",
    "    \n",
    "    # Record history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        best_model_state = nn_model.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\n‚èπÔ∏è Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{n_epochs}: Train Loss={train_loss:.6f}, Val Loss={val_loss:.6f}\")\n",
    "\n",
    "nn_train_time = time() - start_time\n",
    "\n",
    "# Load best model\n",
    "nn_model.load_state_dict(best_model_state)\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete in {nn_train_time:.2f} seconds!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate neural network\n",
    "print(\"üìä Evaluating Neural Network...\")\n",
    "\n",
    "# Get predictions\n",
    "start_time = time()\n",
    "_, nn_pred_train, y_train_check = evaluate(nn_model, train_loader, criterion, device)\n",
    "_, nn_pred_val, y_val_check = evaluate(nn_model, val_loader, criterion, device)\n",
    "_, nn_pred_test, y_test_check = evaluate(nn_model, test_loader, criterion, device)\n",
    "nn_inference_time = time() - start_time\n",
    "\n",
    "# Calculate metrics\n",
    "nn_metrics = {\n",
    "    'train_rmse': np.sqrt(mean_squared_error(y_train, nn_pred_train)),\n",
    "    'val_rmse': np.sqrt(mean_squared_error(y_val, nn_pred_val)),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, nn_pred_test)),\n",
    "    'test_r2': r2_score(y_test, nn_pred_test),\n",
    "    'test_mae': mean_absolute_error(y_test, nn_pred_test),\n",
    "    'train_time': nn_train_time,\n",
    "    'inference_time': nn_inference_time\n",
    "}\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"  ‚Ä¢ Training RMSE:   ${nn_metrics['train_rmse']*100000:,.0f}\")\n",
    "print(f\"  ‚Ä¢ Validation RMSE: ${nn_metrics['val_rmse']*100000:,.0f}\")\n",
    "print(f\"  ‚Ä¢ Test RMSE:       ${nn_metrics['test_rmse']*100000:,.0f}\")\n",
    "print(f\"  ‚Ä¢ Test R¬≤ Score:   {nn_metrics['test_r2']:.4f}\")\n",
    "print(f\"  ‚Ä¢ Test MAE:        ${nn_metrics['test_mae']*100000:,.0f}\")\n",
    "print(f\"\\n‚è±Ô∏è Timing:\")\n",
    "print(f\"  ‚Ä¢ Training Time:   {nn_metrics['train_time']:.2f} seconds\")\n",
    "print(f\"  ‚Ä¢ Inference Time:  {nn_metrics['inference_time']*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(history['train_loss'], label='Train Loss', alpha=0.8)\n",
    "ax.plot(history['val_loss'], label='Validation Loss', alpha=0.8)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MSE Loss')\n",
    "ax.set_title('Neural Network Training Curves')\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Head-to-Head Comparison\n",
    "\n",
    "Now let's compare XGBoost and Neural Network side-by-side!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Test RMSE ($)',\n",
    "        'Test R¬≤ Score',\n",
    "        'Test MAE ($)',\n",
    "        'Training Time (s)',\n",
    "        'Inference Time (ms)',\n",
    "        'Feature Importance',\n",
    "        'Preprocessing Required'\n",
    "    ],\n",
    "    'XGBoost': [\n",
    "        f\"${xgb_metrics['test_rmse']*100000:,.0f}\",\n",
    "        f\"{xgb_metrics['test_r2']:.4f}\",\n",
    "        f\"${xgb_metrics['test_mae']*100000:,.0f}\",\n",
    "        f\"{xgb_metrics['train_time']:.2f}\",\n",
    "        f\"{xgb_metrics['inference_time']*1000:.2f}\",\n",
    "        '‚úÖ Built-in',\n",
    "        '‚ùå No'\n",
    "    ],\n",
    "    'Neural Network': [\n",
    "        f\"${nn_metrics['test_rmse']*100000:,.0f}\",\n",
    "        f\"{nn_metrics['test_r2']:.4f}\",\n",
    "        f\"${nn_metrics['test_mae']*100000:,.0f}\",\n",
    "        f\"{nn_metrics['train_time']:.2f}\",\n",
    "        f\"{nn_metrics['inference_time']*1000:.2f}\",\n",
    "        '‚ö†Ô∏è Requires extra work',\n",
    "        '‚úÖ Yes (scaling)'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Determine winner for each metric\n",
    "winners = []\n",
    "for i, metric in enumerate(comparison['Metric']):\n",
    "    if metric in ['Test RMSE ($)', 'Test MAE ($)', 'Training Time (s)', 'Inference Time (ms)']:\n",
    "        # Lower is better\n",
    "        xgb_val = float(comparison['XGBoost'].iloc[i].replace('$', '').replace(',', '').replace(' s', '').replace(' ms', ''))\n",
    "        nn_val = float(comparison['Neural Network'].iloc[i].replace('$', '').replace(',', '').replace(' s', '').replace(' ms', ''))\n",
    "        winners.append('XGBoost' if xgb_val < nn_val else 'Neural Network')\n",
    "    elif metric == 'Test R¬≤ Score':\n",
    "        # Higher is better\n",
    "        xgb_val = float(comparison['XGBoost'].iloc[i])\n",
    "        nn_val = float(comparison['Neural Network'].iloc[i])\n",
    "        winners.append('XGBoost' if xgb_val > nn_val else 'Neural Network')\n",
    "    else:\n",
    "        winners.append('-')\n",
    "\n",
    "comparison['Winner'] = winners\n",
    "\n",
    "print(\"üèÜ XGBoost vs Neural Network: Head-to-Head Comparison\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 1. Predictions vs Actual\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(y_test, xgb_pred_test, alpha=0.5, label='XGBoost', s=20)\n",
    "ax1.scatter(y_test, nn_pred_test, alpha=0.5, label='Neural Net', s=20)\n",
    "ax1.plot([0, 5], [0, 5], 'r--', linewidth=2, label='Perfect')\n",
    "ax1.set_xlabel('Actual Price ($100K)')\n",
    "ax1.set_ylabel('Predicted Price ($100K)')\n",
    "ax1.set_title('Predictions vs Actual')\n",
    "ax1.legend()\n",
    "ax1.set_xlim(0, 5.5)\n",
    "ax1.set_ylim(0, 5.5)\n",
    "\n",
    "# 2. Residual distributions\n",
    "ax2 = axes[1]\n",
    "xgb_residuals = y_test - xgb_pred_test\n",
    "nn_residuals = y_test - nn_pred_test\n",
    "ax2.hist(xgb_residuals, bins=50, alpha=0.6, label='XGBoost', density=True)\n",
    "ax2.hist(nn_residuals, bins=50, alpha=0.6, label='Neural Net', density=True)\n",
    "ax2.axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "ax2.set_xlabel('Residual ($100K)')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.set_title('Residual Distribution')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Timing comparison\n",
    "ax3 = axes[2]\n",
    "metrics_names = ['Training\\nTime (s)', 'Inference\\nTime (ms)']\n",
    "xgb_times = [xgb_metrics['train_time'], xgb_metrics['inference_time']*1000]\n",
    "nn_times = [nn_metrics['train_time'], nn_metrics['inference_time']*1000]\n",
    "\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "ax3.bar(x - width/2, xgb_times, width, label='XGBoost', color='steelblue')\n",
    "ax3.bar(x + width/2, nn_times, width, label='Neural Net', color='coral')\n",
    "ax3.set_ylabel('Time')\n",
    "ax3.set_title('Training & Inference Time')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(metrics_names)\n",
    "ax3.legend()\n",
    "ax3.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Analysis - When Does Each Excel?\n",
    "\n",
    "### Why XGBoost Usually Wins on Tabular Data\n",
    "\n",
    "Based on our experiment and research, here's why:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary analysis\n",
    "print(\"üìä Analysis: XGBoost vs Neural Networks on Tabular Data\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "analysis = \"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    WHY XGBOOST WINS ON TABULAR DATA                  ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                      ‚îÇ\n",
    "‚îÇ  1. AXIS-ALIGNED SPLITS                                             ‚îÇ\n",
    "‚îÇ     Trees naturally handle \"if income > $50K\" decisions             ‚îÇ\n",
    "‚îÇ     Neural nets must learn these boundaries from scratch            ‚îÇ\n",
    "‚îÇ                                                                      ‚îÇ\n",
    "‚îÇ  2. HETEROGENEOUS FEATURES                                          ‚îÇ\n",
    "‚îÇ     Age, income, location are fundamentally different               ‚îÇ\n",
    "‚îÇ     Trees handle each feature independently                         ‚îÇ\n",
    "‚îÇ     Neural nets struggle with mixed feature semantics               ‚îÇ\n",
    "‚îÇ                                                                      ‚îÇ\n",
    "‚îÇ  3. NO PREPROCESSING NEEDED                                         ‚îÇ\n",
    "‚îÇ     XGBoost: Raw data in, predictions out                           ‚îÇ\n",
    "‚îÇ     Neural nets: Need scaling, encoding, careful initialization     ‚îÇ\n",
    "‚îÇ                                                                      ‚îÇ\n",
    "‚îÇ  4. BUILT-IN REGULARIZATION                                         ‚îÇ\n",
    "‚îÇ     Tree depth, min samples, L1/L2 penalties                        ‚îÇ\n",
    "‚îÇ     Neural nets: Dropout, weight decay, early stopping              ‚îÇ\n",
    "‚îÇ                                                                      ‚îÇ\n",
    "‚îÇ  5. TRAINING EFFICIENCY                                             ‚îÇ\n",
    "‚îÇ     XGBoost: Seconds to minutes                                     ‚îÇ\n",
    "‚îÇ     Neural nets: Minutes to hours                                   ‚îÇ\n",
    "‚îÇ                                                                      ‚îÇ\n",
    "‚îÇ  6. INTERPRETABILITY                                                ‚îÇ\n",
    "‚îÇ     XGBoost: Feature importance is built-in and meaningful          ‚îÇ\n",
    "‚îÇ     Neural nets: Require SHAP, LIME, or other tools                 ‚îÇ\n",
    "‚îÇ                                                                      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    WHEN NEURAL NETWORKS WIN                          ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                      ‚îÇ\n",
    "‚îÇ  1. VERY LARGE DATASETS (>1M samples)                               ‚îÇ\n",
    "‚îÇ     Neural nets can keep improving with more data                   ‚îÇ\n",
    "‚îÇ                                                                      ‚îÇ\n",
    "‚îÇ  2. COMPLEX FEATURE INTERACTIONS                                    ‚îÇ\n",
    "‚îÇ     When relationships aren't axis-aligned                          ‚îÇ\n",
    "‚îÇ                                                                      ‚îÇ\n",
    "‚îÇ  3. TRANSFER LEARNING                                               ‚îÇ\n",
    "‚îÇ     Pre-trained embeddings from related tasks                       ‚îÇ\n",
    "‚îÇ     (e.g., text/image features as columns)                          ‚îÇ\n",
    "‚îÇ                                                                      ‚îÇ\n",
    "‚îÇ  4. MULTI-MODAL DATA                                                ‚îÇ\n",
    "‚îÇ     Tables + Images + Text combined                                 ‚îÇ\n",
    "‚îÇ                                                                      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\"\"\"\n",
    "print(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself\n",
    "\n",
    "Now it's your turn! Complete these exercises to solidify your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Tune XGBoost Hyperparameters\n",
    "\n",
    "Try different hyperparameters and see if you can beat the default XGBoost model.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Try adjusting:\n",
    "- `max_depth`: 3-10 (lower = less overfitting)\n",
    "- `learning_rate`: 0.01-0.3 (lower = needs more trees)\n",
    "- `n_estimators`: 100-1000 (more trees = better, but slower)\n",
    "- `min_child_weight`: 1-10 (higher = more conservative)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 1: Your code here\n# Try to beat the default XGBoost score!\n\n# Suggested starting point - uncomment and fill in the blanks:\n# tuned_params = {\n#     'objective': 'reg:squarederror',\n#     'max_depth': 5,            # Try: 4, 5, 6, 7, 8\n#     'learning_rate': 0.1,      # Try: 0.05, 0.1, 0.2\n#     'n_estimators': 200,       # Try: 200, 300, 500\n#     'min_child_weight': 3,     # Try: 1, 3, 5\n#     'subsample': 0.8,\n#     'colsample_bytree': 0.8,\n#     'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n#     'random_state': 42\n# }\n#\n# tuned_model = xgb.XGBRegressor(**tuned_params)\n# tuned_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n# tuned_pred = tuned_model.predict(X_test)\n# tuned_rmse = np.sqrt(mean_squared_error(y_test, tuned_pred))\n# print(f\"Tuned RMSE: ${tuned_rmse*100000:,.0f} vs Original: ${xgb_metrics['test_rmse']*100000:,.0f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Experiment with Neural Network Architecture\n",
    "\n",
    "Try different architectures and see if a neural network can match XGBoost.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Try:\n",
    "- Deeper networks: [512, 256, 128, 64]\n",
    "- Wider networks: [512, 512, 256]\n",
    "- Different activations: LeakyReLU, GELU\n",
    "- More/less dropout\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Try different neural network architectures\n",
    "\n",
    "# Example: Create a deeper model\n",
    "# deep_model = TabularMLP(\n",
    "#     input_dim=8, \n",
    "#     hidden_dims=[____],  # Try different architectures\n",
    "#     dropout=____         # Try different dropout rates\n",
    "# ).to(device)\n",
    "\n",
    "# Train and evaluate..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Cross-Validation Comparison\n",
    "\n",
    "Use 5-fold cross-validation to get a more robust comparison.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Use `sklearn.model_selection.cross_val_score` with `scoring='neg_root_mean_squared_error'`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "# Use cross-validation for a fairer comparison\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# cv_model = xgb.XGBRegressor(**xgb_params)\n",
    "# cv_scores = cross_val_score(cv_model, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "# print(f\"Cross-validation RMSE: ${-cv_scores.mean()*100000:,.0f} (+/- ${cv_scores.std()*100000:,.0f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Using Early Stopping with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Training until all n_estimators complete (may overfit)\n",
    "# bad_model = xgb.XGBRegressor(n_estimators=1000)\n",
    "# bad_model.fit(X_train, y_train)  # No validation set, no early stopping!\n",
    "\n",
    "# ‚úÖ Right: Use early stopping with validation set\n",
    "good_model = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    early_stopping_rounds=10,  # Stop if no improvement for 10 rounds\n",
    "    device='cuda'\n",
    ")\n",
    "good_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "print(f\"‚úÖ Stopped at {good_model.best_iteration} trees (instead of 1000)!\")\n",
    "print(f\"   This prevents overfitting and saves time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Scaling Features for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Scaling features for tree-based models (unnecessary!)\n",
    "# Tree-based models split on feature values - scaling changes nothing!\n",
    "# model = xgb.XGBRegressor()\n",
    "# model.fit(StandardScaler().fit_transform(X_train), y_train)  # Wastes time\n",
    "\n",
    "# ‚úÖ Right: Use raw features\n",
    "print(\"üí° XGBoost doesn't need feature scaling!\")\n",
    "print(\"   Trees split on 'feature > threshold', so scaling doesn't change anything.\")\n",
    "print(\"   Save StandardScaler for neural networks only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Not Using GPU Acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Using CPU on DGX Spark (wastes GPU!)\n",
    "# model = xgb.XGBRegressor(tree_method='hist')  # CPU-only\n",
    "\n",
    "# ‚úÖ Right: Use GPU acceleration\n",
    "model = xgb.XGBRegressor(\n",
    "    tree_method='hist',\n",
    "    device='cuda'  # Use GPU!\n",
    ")\n",
    "print(\"‚úÖ XGBoost is using GPU acceleration!\")\n",
    "print(\"   On DGX Spark, this can be 2-5x faster than CPU for large datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 4: Forgetting to Scale for Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Using raw features for neural networks\n",
    "# Neural networks are VERY sensitive to feature scale!\n",
    "# model.fit(X_train, y_train)  # Features have wildly different ranges\n",
    "\n",
    "# ‚úÖ Right: Always scale features for neural networks\n",
    "print(\"üí° Neural networks NEED feature scaling!\")\n",
    "print(\"   Income ranges from 0.5-15, but Population can be 3-35,000\")\n",
    "print(\"   Without scaling, gradients will be dominated by large features.\")\n",
    "print(\"\\n   Use: X_scaled = StandardScaler().fit_transform(X)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "Congratulations! You've completed the Tabular Data Challenge. You've learned:\n",
    "\n",
    "- ‚úÖ **XGBoost basics**: Training, prediction, and feature importance\n",
    "- ‚úÖ **Neural network comparison**: Fair benchmark with modern architecture\n",
    "- ‚úÖ **Why XGBoost wins**: Axis-aligned splits, heterogeneous features, no preprocessing\n",
    "- ‚úÖ **When to use each**: Tabular ‚Üí XGBoost, Complex/Multi-modal ‚Üí Neural Networks\n",
    "- ‚úÖ **GPU acceleration**: Both models run faster on your DGX Spark!\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**The Ultimate Challenge:** Can you find a tabular dataset where a neural network beats XGBoost?\n",
    "\n",
    "Some candidates to try:\n",
    "1. **Forest Cover Type** (sklearn.datasets.fetch_covtype) - 581K samples\n",
    "2. **Higgs Boson** (from UCI) - 11M samples\n",
    "3. **Click-Through Rate Prediction** - Kaggle datasets\n",
    "\n",
    "Neural networks tend to win when:\n",
    "- Dataset is very large (>1M samples)\n",
    "- Features have complex interactions\n",
    "- You can use pre-trained embeddings\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [Why do tree-based models still outperform deep learning on tabular data? (2022)](https://arxiv.org/abs/2207.08815)\n",
    "- [XGBoost: A Scalable Tree Boosting System (Original Paper)](https://arxiv.org/abs/1603.02754)\n",
    "- [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n",
    "- [Deep Neural Networks and Tabular Data: A Survey](https://arxiv.org/abs/2110.01889)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Delete models\n",
    "del nn_model\n",
    "del xgb_model\n",
    "\n",
    "# Clear CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Memory cleaned up!\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU Memory: {torch.cuda.memory_allocated()/1e6:.1f} MB allocated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚û°Ô∏è Next Steps\n",
    "\n",
    "Continue to **Lab 1.6.2: Hyperparameter Optimization** to learn how to use Optuna to automatically find the best hyperparameters!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}