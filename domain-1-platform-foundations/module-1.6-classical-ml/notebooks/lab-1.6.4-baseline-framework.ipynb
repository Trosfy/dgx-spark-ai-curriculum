{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.6.4: Baseline Comparison Framework\n",
    "\n",
    "**Module:** 1.6 - Classical ML Foundations  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê (Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Build a reusable `BaselineExperiment` class for ML comparisons\n",
    "- [ ] Implement automated model comparison with consistent metrics\n",
    "- [ ] Generate comprehensive comparison reports\n",
    "- [ ] Test the framework on multiple real-world datasets\n",
    "- [ ] Create visualizations for model performance analysis\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Labs 1.6.1, 1.6.2, 1.6.3\n",
    "- Knowledge of: XGBoost, scikit-learn, cross-validation\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The Scientific Method for ML:**\n",
    "\n",
    "In industry, every ML project should start with a baseline comparison:\n",
    "1. **Establish baselines**: Simple models that are hard to beat\n",
    "2. **Fair comparison**: Same splits, same metrics, same preprocessing\n",
    "3. **Document everything**: Training time, inference time, memory usage\n",
    "4. **Reproducibility**: Anyone can replicate your results\n",
    "\n",
    "**Why This Matters:**\n",
    "- At Google, all ML models must beat a baseline to ship\n",
    "- At Kaggle, XGBoost baseline often beats complex neural networks\n",
    "- In research papers, reviewers expect rigorous baseline comparisons\n",
    "- For startups, a simple model that works beats a complex one that doesn't\n",
    "\n",
    "Today we'll build a framework that enforces these best practices automatically!\n",
    "\n",
    "---\n",
    "\n",
    "## üßí ELI5: Why Baselines Matter\n",
    "\n",
    "> **Imagine you're trying to run faster...**\n",
    ">\n",
    "> You could:\n",
    "> - Buy $500 running shoes\n",
    "> - Hire an expensive coach\n",
    "> - Follow a complex training program\n",
    ">\n",
    "> But first, you should know:\n",
    "> - How fast can you already run? (baseline)\n",
    "> - How fast does a regular person run? (simple baseline)\n",
    "> - Did the expensive shoes actually help? (fair comparison)\n",
    ">\n",
    "> **In ML terms:**\n",
    "> - Complex model = expensive running shoes\n",
    "> - XGBoost baseline = your current running speed\n",
    "> - If the complex model doesn't beat the baseline, it's not worth the cost!\n",
    ">\n",
    "> **The shocking truth:** On tabular data, XGBoost baseline beats deep learning ~70% of the time!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Core imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom time import time\nfrom datetime import datetime\nfrom typing import Dict, List, Any, Optional, Union, Callable\nfrom dataclasses import dataclass\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# PyTorch for GPU detection\nimport torch\n\n# scikit-learn\nfrom sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n    mean_squared_error, mean_absolute_error, r2_score\n)\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.linear_model import LogisticRegression, Ridge, Lasso\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.datasets import (\n    fetch_california_housing, load_breast_cancer, load_wine,\n    make_classification, make_regression\n)\n\n# XGBoost\nimport xgboost as xgb\n\n# Plotting\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\n\n# Set random seed\nnp.random.seed(42)\n\n# Determine device for XGBoost\nXGB_DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"‚úÖ All imports successful!\")\nprint(f\"   XGBoost device: {XGB_DEVICE}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Building the BaselineExperiment Class\n",
    "\n",
    "Let's build a professional-grade experiment framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelResult:\n",
    "    \"\"\"\n",
    "    Stores results from a single model evaluation.\n",
    "    \n",
    "    Attributes:\n",
    "        name: Model name\n",
    "        metrics: Dictionary of metric names to values\n",
    "        cv_scores: Cross-validation scores array\n",
    "        train_time: Training time in seconds\n",
    "        inference_time: Inference time in seconds\n",
    "        feature_importance: Feature importance array (if available)\n",
    "        model: The trained model object\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    metrics: Dict[str, float]\n",
    "    cv_scores: np.ndarray\n",
    "    train_time: float\n",
    "    inference_time: float\n",
    "    feature_importance: Optional[np.ndarray] = None\n",
    "    model: Any = None\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ModelResult(name='{self.name}', metrics={self.metrics})\"\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convert to dictionary (for JSON serialization).\"\"\"\n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'metrics': self.metrics,\n",
    "            'cv_mean': float(self.cv_scores.mean()),\n",
    "            'cv_std': float(self.cv_scores.std()),\n",
    "            'train_time': self.train_time,\n",
    "            'inference_time': self.inference_time\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ ModelResult dataclass defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class BaselineExperiment:\n    \"\"\"\n    A reusable framework for comparing ML models on tabular data.\n    \n    Features:\n    - Automatic cross-validation\n    - Consistent metrics across models\n    - Training and inference timing\n    - Feature importance extraction\n    - Visualization and reporting\n    \n    Example:\n        >>> exp = BaselineExperiment(\n        ...     X=X, y=y,\n        ...     task='classification',\n        ...     feature_names=['feat1', 'feat2']\n        ... )\n        >>> exp.add_model('XGBoost', xgb.XGBClassifier())\n        >>> exp.add_model('Random Forest', RandomForestClassifier())\n        >>> exp.run()\n        >>> exp.report()\n    \"\"\"\n    \n    def __init__(\n        self,\n        X: np.ndarray,\n        y: np.ndarray,\n        task: str = 'classification',\n        feature_names: Optional[List[str]] = None,\n        cv_folds: int = 5,\n        test_size: float = 0.2,\n        random_state: int = 42,\n        scale_features: bool = True\n    ):\n        \"\"\"\n        Initialize the experiment.\n        \n        Args:\n            X: Feature matrix\n            y: Target vector\n            task: 'classification' or 'regression'\n            feature_names: Optional list of feature names\n            cv_folds: Number of cross-validation folds\n            test_size: Proportion of data for test set\n            random_state: Random seed for reproducibility\n            scale_features: Whether to scale features (for non-tree models)\n        \"\"\"\n        self.X = X.astype(np.float32)\n        self.y = y\n        self.task = task\n        self.feature_names = feature_names or [f'feature_{i}' for i in range(X.shape[1])]\n        self.cv_folds = cv_folds\n        self.test_size = test_size\n        self.random_state = random_state\n        self.scale_features = scale_features\n        \n        # Store models to evaluate\n        self.models: Dict[str, Any] = {}\n        self.needs_scaling: Dict[str, bool] = {}\n        \n        # Results storage\n        self.results: List[ModelResult] = []\n        \n        # Train/test split\n        self._setup_data()\n        \n        # Set up cross-validation\n        if task == 'classification':\n            self.cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n        else:\n            self.cv = KFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n        \n        print(f\"‚úÖ BaselineExperiment initialized!\")\n        print(f\"   Task: {task}\")\n        print(f\"   Samples: {len(X):,}\")\n        print(f\"   Features: {X.shape[1]}\")\n        print(f\"   CV Folds: {cv_folds}\")\n    \n    def _setup_data(self):\n        \"\"\"Split data into train and test sets.\"\"\"\n        from sklearn.model_selection import train_test_split\n        \n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n            self.X, self.y,\n            test_size=self.test_size,\n            random_state=self.random_state,\n            stratify=self.y if self.task == 'classification' else None\n        )\n        \n        # Prepare scaled versions\n        if self.scale_features:\n            self.scaler = StandardScaler()\n            self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n            self.X_test_scaled = self.scaler.transform(self.X_test)\n        else:\n            self.X_train_scaled = self.X_train\n            self.X_test_scaled = self.X_test\n    \n    def add_model(self, name: str, model: Any, needs_scaling: bool = False):\n        \"\"\"\n        Add a model to the experiment.\n        \n        Args:\n            name: Display name for the model\n            model: sklearn-compatible model instance\n            needs_scaling: Whether this model needs scaled features\n        \"\"\"\n        self.models[name] = model\n        self.needs_scaling[name] = needs_scaling\n        print(f\"   Added model: {name}\")\n    \n    def add_default_models(self):\n        \"\"\"\n        Add default baseline models for the task.\n        \"\"\"\n        print(\"\\nüì¶ Adding default baseline models...\")\n        \n        if self.task == 'classification':\n            # XGBoost (with GPU support if available)\n            self.add_model(\n                'XGBoost',\n                xgb.XGBClassifier(\n                    n_estimators=100,\n                    max_depth=6,\n                    learning_rate=0.1,\n                    device=XGB_DEVICE,\n                    random_state=self.random_state,\n                    verbosity=0\n                ),\n                needs_scaling=False\n            )\n            \n            # Random Forest\n            self.add_model(\n                'Random Forest',\n                RandomForestClassifier(\n                    n_estimators=100,\n                    max_depth=16,\n                    n_jobs=-1,\n                    random_state=self.random_state\n                ),\n                needs_scaling=False\n            )\n            \n            # Logistic Regression\n            self.add_model(\n                'Logistic Regression',\n                LogisticRegression(\n                    max_iter=1000,\n                    random_state=self.random_state,\n                    n_jobs=-1\n                ),\n                needs_scaling=True\n            )\n        \n        else:  # regression\n            # XGBoost (with GPU support if available)\n            self.add_model(\n                'XGBoost',\n                xgb.XGBRegressor(\n                    n_estimators=100,\n                    max_depth=6,\n                    learning_rate=0.1,\n                    device=XGB_DEVICE,\n                    random_state=self.random_state,\n                    verbosity=0\n                ),\n                needs_scaling=False\n            )\n            \n            # Random Forest\n            self.add_model(\n                'Random Forest',\n                RandomForestRegressor(\n                    n_estimators=100,\n                    max_depth=16,\n                    n_jobs=-1,\n                    random_state=self.random_state\n                ),\n                needs_scaling=False\n            )\n            \n            # Ridge Regression\n            self.add_model(\n                'Ridge Regression',\n                Ridge(alpha=1.0, random_state=self.random_state),\n                needs_scaling=True\n            )\n    \n    def _get_metrics(self, y_true: np.ndarray, y_pred: np.ndarray, y_proba: Optional[np.ndarray] = None) -> Dict[str, float]:\n        \"\"\"Calculate metrics for the task.\"\"\"\n        if self.task == 'classification':\n            metrics = {\n                'accuracy': accuracy_score(y_true, y_pred),\n                'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n                'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n                'f1': f1_score(y_true, y_pred, average='weighted', zero_division=0)\n            }\n            if y_proba is not None and len(np.unique(y_true)) == 2:\n                try:\n                    metrics['roc_auc'] = roc_auc_score(y_true, y_proba[:, 1] if y_proba.ndim > 1 else y_proba)\n                except:\n                    pass\n        else:\n            metrics = {\n                'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n                'mae': mean_absolute_error(y_true, y_pred),\n                'r2': r2_score(y_true, y_pred)\n            }\n        return metrics\n    \n    def _evaluate_model(self, name: str, model: Any) -> ModelResult:\n        \"\"\"Evaluate a single model.\"\"\"\n        # Select appropriate data\n        if self.needs_scaling.get(name, False):\n            X_train = self.X_train_scaled\n            X_test = self.X_test_scaled\n        else:\n            X_train = self.X_train\n            X_test = self.X_test\n        \n        # Cross-validation scoring\n        if self.task == 'classification':\n            scoring = 'accuracy'\n        else:\n            scoring = 'neg_root_mean_squared_error'\n        \n        cv_scores = cross_val_score(model, X_train, self.y_train, cv=self.cv, scoring=scoring, n_jobs=-1)\n        \n        # Training\n        start_time = time()\n        model.fit(X_train, self.y_train)\n        train_time = time() - start_time\n        \n        # Inference\n        start_time = time()\n        y_pred = model.predict(X_test)\n        inference_time = time() - start_time\n        \n        # Get probabilities if available\n        y_proba = None\n        if self.task == 'classification' and hasattr(model, 'predict_proba'):\n            try:\n                y_proba = model.predict_proba(X_test)\n            except:\n                pass\n        \n        # Calculate metrics\n        metrics = self._get_metrics(self.y_test, y_pred, y_proba)\n        \n        # Feature importance\n        feature_importance = None\n        if hasattr(model, 'feature_importances_'):\n            feature_importance = model.feature_importances_\n        elif hasattr(model, 'coef_'):\n            feature_importance = np.abs(model.coef_).flatten()\n        \n        return ModelResult(\n            name=name,\n            metrics=metrics,\n            cv_scores=cv_scores if self.task == 'classification' else -cv_scores,\n            train_time=train_time,\n            inference_time=inference_time,\n            feature_importance=feature_importance,\n            model=model\n        )\n    \n    def run(self, verbose: bool = True):\n        \"\"\"\n        Run the experiment on all models.\n        \"\"\"\n        print(\"\\nüöÄ Running Baseline Experiment...\")\n        print(\"=\" * 60)\n        \n        self.results = []\n        \n        for name, model in self.models.items():\n            if verbose:\n                print(f\"\\nüìä Evaluating: {name}\")\n            \n            result = self._evaluate_model(name, model)\n            self.results.append(result)\n            \n            if verbose:\n                if self.task == 'classification':\n                    print(f\"   CV Accuracy: {result.cv_scores.mean():.4f} (+/- {result.cv_scores.std():.4f})\")\n                    print(f\"   Test Accuracy: {result.metrics['accuracy']:.4f}\")\n                else:\n                    print(f\"   CV RMSE: {result.cv_scores.mean():.4f} (+/- {result.cv_scores.std():.4f})\")\n                    print(f\"   Test RMSE: {result.metrics['rmse']:.4f}\")\n                print(f\"   Train Time: {result.train_time:.3f}s\")\n        \n        print(\"\\n‚úÖ Experiment complete!\")\n        return self\n    \n    def get_best_model(self) -> ModelResult:\n        \"\"\"Return the best performing model.\"\"\"\n        if not self.results:\n            raise ValueError(\"No results yet. Run the experiment first!\")\n        \n        if self.task == 'classification':\n            return max(self.results, key=lambda r: r.metrics['accuracy'])\n        else:\n            return min(self.results, key=lambda r: r.metrics['rmse'])\n    \n    def report(self) -> pd.DataFrame:\n        \"\"\"\n        Generate a comparison report.\n        \"\"\"\n        if not self.results:\n            raise ValueError(\"No results yet. Run the experiment first!\")\n        \n        print(\"\\nüìã Baseline Comparison Report\")\n        print(\"=\" * 70)\n        \n        # Build comparison DataFrame\n        data = []\n        for result in self.results:\n            row = {\n                'Model': result.name,\n                'CV Mean': result.cv_scores.mean(),\n                'CV Std': result.cv_scores.std(),\n                **result.metrics,\n                'Train Time (s)': result.train_time,\n                'Inference Time (s)': result.inference_time\n            }\n            data.append(row)\n        \n        df = pd.DataFrame(data)\n        \n        # Sort by primary metric\n        if self.task == 'classification':\n            df = df.sort_values('accuracy', ascending=False)\n        else:\n            df = df.sort_values('rmse', ascending=True)\n        \n        print(df.to_string(index=False))\n        \n        # Best model\n        best = self.get_best_model()\n        print(f\"\\nüèÜ Best Model: {best.name}\")\n        \n        return df\n    \n    def plot_comparison(self, save_path: Optional[str] = None):\n        \"\"\"\n        Create visualization of model comparison.\n        \"\"\"\n        if not self.results:\n            raise ValueError(\"No results yet. Run the experiment first!\")\n        \n        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n        \n        names = [r.name for r in self.results]\n        \n        # 1. Primary metric comparison\n        ax1 = axes[0, 0]\n        if self.task == 'classification':\n            metric_values = [r.metrics['accuracy'] for r in self.results]\n            metric_name = 'Accuracy'\n        else:\n            metric_values = [r.metrics['rmse'] for r in self.results]\n            metric_name = 'RMSE'\n        \n        colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(names)))\n        bars = ax1.bar(names, metric_values, color=colors)\n        ax1.set_ylabel(metric_name)\n        ax1.set_title(f'Model Comparison: {metric_name}')\n        ax1.tick_params(axis='x', rotation=15)\n        \n        # Add value labels\n        for bar, val in zip(bars, metric_values):\n            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n                    f'{val:.4f}', ha='center', va='bottom', fontsize=10)\n        \n        # 2. Cross-validation boxplot\n        ax2 = axes[0, 1]\n        cv_data = [r.cv_scores for r in self.results]\n        bp = ax2.boxplot(cv_data, labels=names, patch_artist=True)\n        for patch, color in zip(bp['boxes'], colors):\n            patch.set_facecolor(color)\n        ax2.set_ylabel('CV Score')\n        ax2.set_title('Cross-Validation Score Distribution')\n        ax2.tick_params(axis='x', rotation=15)\n        \n        # 3. Training time comparison\n        ax3 = axes[1, 0]\n        train_times = [r.train_time for r in self.results]\n        bars = ax3.bar(names, train_times, color=colors)\n        ax3.set_ylabel('Time (seconds)')\n        ax3.set_title('Training Time')\n        ax3.tick_params(axis='x', rotation=15)\n        \n        # 4. Feature importance (for best model)\n        ax4 = axes[1, 1]\n        best = self.get_best_model()\n        if best.feature_importance is not None:\n            # Get top 10 features\n            top_k = min(10, len(self.feature_names))\n            indices = np.argsort(best.feature_importance)[-top_k:]\n            top_features = [self.feature_names[i] for i in indices]\n            top_importance = best.feature_importance[indices]\n            \n            ax4.barh(top_features, top_importance, color='steelblue')\n            ax4.set_xlabel('Importance')\n            ax4.set_title(f'Feature Importance ({best.name})')\n        else:\n            ax4.text(0.5, 0.5, 'Feature importance\\nnot available',\n                    ha='center', va='center', transform=ax4.transAxes)\n            ax4.set_title('Feature Importance')\n        \n        plt.tight_layout()\n        \n        if save_path:\n            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n            print(f\"üíæ Saved: {save_path}\")\n        \n        plt.show()\n    \n    def save_results(self, filepath: str):\n        \"\"\"Save results to JSON file.\"\"\"\n        data = {\n            'experiment_time': datetime.now().isoformat(),\n            'task': self.task,\n            'n_samples': len(self.X),\n            'n_features': self.X.shape[1],\n            'cv_folds': self.cv_folds,\n            'results': [r.to_dict() for r in self.results]\n        }\n        \n        with open(filepath, 'w') as f:\n            json.dump(data, f, indent=2)\n        \n        print(f\"üíæ Results saved to: {filepath}\")\n\nprint(\"‚úÖ BaselineExperiment class defined!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Testing on Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Breast Cancer Classification\n",
    "print(\"üî¨ Test 1: Breast Cancer Classification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load data\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "print(f\"Dataset: {data.DESCR.split(chr(10))[0]}\")\n",
    "print(f\"Samples: {len(X):,}\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Classes: {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment\n",
    "exp_cancer = BaselineExperiment(\n",
    "    X=X, y=y,\n",
    "    task='classification',\n",
    "    feature_names=list(data.feature_names),\n",
    "    cv_folds=5\n",
    ")\n",
    "\n",
    "# Add default models\n",
    "exp_cancer.add_default_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiment\n",
    "exp_cancer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate report\n",
    "report_cancer = exp_cancer.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "exp_cancer.plot_comparison(save_path='cancer_comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Testing on Regression Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: California Housing Regression\n",
    "print(\"\\nüè† Test 2: California Housing Regression\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load data\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "print(f\"Dataset: California Housing\")\n",
    "print(f\"Samples: {len(X):,}\")\n",
    "print(f\"Features: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment\n",
    "exp_housing = BaselineExperiment(\n",
    "    X=X, y=y,\n",
    "    task='regression',\n",
    "    feature_names=list(housing.feature_names),\n",
    "    cv_folds=5\n",
    ")\n",
    "\n",
    "# Add default models\n",
    "exp_housing.add_default_models()\n",
    "\n",
    "# Run\n",
    "exp_housing.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report and visualize\n",
    "report_housing = exp_housing.report()\n",
    "exp_housing.plot_comparison(save_path='housing_comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Testing on Large Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Large Synthetic Dataset\n",
    "print(\"\\nüìä Test 3: Large Synthetic Dataset\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate large dataset\n",
    "X_large, y_large = make_classification(\n",
    "    n_samples=100_000,\n",
    "    n_features=50,\n",
    "    n_informative=30,\n",
    "    n_redundant=10,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Samples: {len(X_large):,}\")\n",
    "print(f\"Features: {X_large.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment\n",
    "exp_large = BaselineExperiment(\n",
    "    X=X_large, y=y_large,\n",
    "    task='classification',\n",
    "    cv_folds=3  # Fewer folds for speed\n",
    ")\n",
    "\n",
    "# Add default models\n",
    "exp_large.add_default_models()\n",
    "\n",
    "# Run\n",
    "exp_large.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report and visualize\n",
    "report_large = exp_large.report()\n",
    "exp_large.plot_comparison(save_path='large_comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Summary Across All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary across datasets\n",
    "print(\"üìä Summary: XGBoost vs Other Models\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for exp, name in [(exp_cancer, 'Breast Cancer'), \n",
    "                  (exp_housing, 'California Housing'),\n",
    "                  (exp_large, 'Large Synthetic')]:\n",
    "    \n",
    "    best = exp.get_best_model()\n",
    "    xgb_result = [r for r in exp.results if r.name == 'XGBoost'][0]\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dataset': name,\n",
    "        'Task': exp.task,\n",
    "        'Samples': len(exp.X),\n",
    "        'Best Model': best.name,\n",
    "        'XGBoost Rank': sorted([r.name for r in exp.results], \n",
    "                               key=lambda n: [r.metrics.get('accuracy', -r.metrics.get('rmse', 0)) \n",
    "                                             for r in exp.results if r.name == n][0],\n",
    "                               reverse=True).index('XGBoost') + 1,\n",
    "        'XGBoost Time': f\"{xgb_result.train_time:.3f}s\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Key insight\n",
    "xgb_wins = sum(1 for d in summary_data if d['Best Model'] == 'XGBoost')\n",
    "print(f\"\\nüèÜ XGBoost won {xgb_wins}/{len(summary_data)} experiments!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Add More Models\n",
    "\n",
    "Extend the framework to include LightGBM and a simple neural network.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Use `lightgbm.LGBMClassifier` and create a simple sklearn MLP.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Add LightGBM and neural network to the experiment\n",
    "\n",
    "# import lightgbm as lgb\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# \n",
    "# exp = BaselineExperiment(X=X, y=y, task='classification')\n",
    "# exp.add_default_models()\n",
    "# exp.add_model('LightGBM', lgb.LGBMClassifier(n_estimators=100, verbose=-1))\n",
    "# exp.add_model('MLP', MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500), needs_scaling=True)\n",
    "# exp.run()\n",
    "# exp.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Custom Metrics\n",
    "\n",
    "Modify the framework to support custom metrics (e.g., profit-based metric for business applications).\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Add a `custom_metrics` parameter that accepts a dictionary of metric functions.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Add custom metric support\n",
    "\n",
    "# def profit_metric(y_true, y_pred):\n",
    "#     \"\"\"Custom profit metric: TP=$100, FP=-$50, FN=-$200\"\"\"\n",
    "#     tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "#     fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "#     fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "#     return 100*tp - 50*fp - 200*fn\n",
    "# \n",
    "# # Add to experiment..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Hyperparameter Tuning Integration\n",
    "\n",
    "Create a method that automatically tunes the best-performing model.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Use Optuna within a `tune_best_model()` method.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "# Add automatic tuning for best model\n",
    "\n",
    "# def tune_best_model(self, n_trials=50):\n",
    "#     best = self.get_best_model()\n",
    "#     # Use Optuna to tune the best model\n",
    "#     ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Data Leakage in Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Fitting scaler on all data\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)  # Leaks test info!\n",
    "# X_train, X_test = train_test_split(X_scaled, ...)\n",
    "\n",
    "# ‚úÖ Right: Fit only on training data\n",
    "# X_train, X_test = train_test_split(X, ...)\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)  # Only transform!\n",
    "\n",
    "print(\"üí° Always fit preprocessing on training data only!\")\n",
    "print(\"   The test set should be 'unseen' in every way.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Inconsistent Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Wrong: Different splits for different models\n",
    "# model1.fit(X_train1, y_train1)\n",
    "# model2.fit(X_train2, y_train2)  # Different split!\n",
    "\n",
    "# ‚úÖ Right: Same splits, same evaluation\n",
    "# Use our BaselineExperiment class - it handles this automatically!\n",
    "\n",
    "print(\"üí° Fair comparison requires identical data splits!\")\n",
    "print(\"   BaselineExperiment ensures all models see the same data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "Congratulations! You've built a professional baseline comparison framework. You've learned:\n",
    "\n",
    "- ‚úÖ **BaselineExperiment class**: Reusable, extensible experiment framework\n",
    "- ‚úÖ **Consistent evaluation**: Same data, same metrics, fair comparison\n",
    "- ‚úÖ **Automatic reporting**: Visualizations and summaries\n",
    "- ‚úÖ **Best practices**: Proper train/test splits, no data leakage\n",
    "- ‚úÖ **Key insight**: XGBoost is a powerful baseline for tabular data!\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Module Complete!\n",
    "\n",
    "You've finished Module 1.6: Classical ML Foundations! You now know:\n",
    "\n",
    "1. **Lab 1.6.1**: XGBoost often beats neural networks on tabular data\n",
    "2. **Lab 1.6.2**: Optuna makes hyperparameter tuning efficient\n",
    "3. **Lab 1.6.3**: RAPIDS provides 10-100x GPU acceleration\n",
    "4. **Lab 1.6.4**: Always start with baselines and compare fairly\n",
    "\n",
    "**Key Takeaway:** Start every ML project with an XGBoost baseline. It's fast, powerful, and often wins!\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [Why do tree-based models still outperform deep learning on tabular data?](https://arxiv.org/abs/2207.08815)\n",
    "- [XGBoost Documentation](https://xgboost.readthedocs.io/)\n",
    "- [scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html)\n",
    "- [RAPIDS cuML Documentation](https://docs.rapids.ai/api/cuml/stable/)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "import gc\n",
    "\n",
    "del exp_cancer, exp_housing, exp_large\n",
    "del X_large, y_large\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")\n",
    "print(\"\\nüéâ Congratulations on completing Module 1.6!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚û°Ô∏è Next Steps\n",
    "\n",
    "Continue to **Module 1.7: Capstone - MicroGrad+** to build your own deep learning framework from scratch!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}