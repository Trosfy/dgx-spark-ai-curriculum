{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.6.2: Hyperparameter Optimization with Optuna\n",
    "\n",
    "**Module:** 1.6 - Classical ML Foundations  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** â­â­â­ (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand why hyperparameter tuning matters\n",
    "- [ ] Use Optuna for Bayesian hyperparameter optimization\n",
    "- [ ] Define effective search spaces for XGBoost\n",
    "- [ ] Visualize optimization history and parameter importance\n",
    "- [ ] Apply cross-validation within hyperparameter search\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Prerequisites\n",
    "\n",
    "- Completed: Lab 1.6.1 (Tabular Data Challenge)\n",
    "- Knowledge of: XGBoost basics, cross-validation concept\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ Real-World Context\n",
    "\n",
    "**The $100K Question:** How much accuracy can you squeeze out of hyperparameter tuning?\n",
    "\n",
    "In Kaggle competitions, the difference between winning and losing is often in the hyperparameters:\n",
    "- A 2019 Kaggle competition was won by improving XGBoost's AUC from 0.87 to 0.91 through tuning alone\n",
    "- That 4% improvement represented a $100,000 prize difference!\n",
    "\n",
    "In production:\n",
    "- **Banks** tune credit scoring models to optimize profit/loss tradeoffs\n",
    "- **E-commerce** tunes recommendation models for click-through rates\n",
    "- **Healthcare** tunes diagnostic models for sensitivity/specificity balance\n",
    "\n",
    "**Why Optuna?**\n",
    "- Bayesian optimization (smarter than random search)\n",
    "- Pruning (stops bad trials early)\n",
    "- Visualization tools built-in\n",
    "- State-of-the-art algorithms (TPE, CMA-ES)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§’ ELI5: Hyperparameter Optimization\n",
    "\n",
    "> **Imagine you're learning to bake the perfect chocolate chip cookie...**\n",
    ">\n",
    "> You have many \"knobs\" to adjust:\n",
    "> - How much sugar? (sweetness)\n",
    "> - What temperature? (crispiness)\n",
    "> - How long to bake? (doneness)\n",
    "> - How many chocolate chips? (richness)\n",
    ">\n",
    "> **Grid Search** = \"I'll try EVERY combination\"\n",
    "> - 5 sugar levels Ã— 5 temps Ã— 5 times Ã— 5 chip amounts = 625 batches! ğŸ˜±\n",
    ">\n",
    "> **Random Search** = \"I'll try random combinations\"\n",
    "> - Better than grid! Often finds good recipes faster\n",
    "> - But still wastes time on obviously bad recipes\n",
    ">\n",
    "> **Bayesian Optimization (Optuna)** = \"I'll learn from each batch!\"\n",
    "> - Batch 1: Too sweet â†’ \"Hmm, less sugar next time\"\n",
    "> - Batch 2: Perfect sweetness, but raw â†’ \"Keep sugar, increase temp\"\n",
    "> - Batch 3: Golden brown, delicious! â†’ \"Let's explore nearby settings\"\n",
    ">\n",
    "> **In ML terms:** Optuna builds a probabilistic model of \"which hyperparameters â†’ good scores\" and uses it to pick promising combinations to try next.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Preparation\n",
    "\n",
    "Let's set up our environment and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Optuna is pre-installed in the NGC PyTorch container\n# If running outside NGC, install with: pip install optuna plotly\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom time import time\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Scikit-learn\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# XGBoost\nimport xgboost as xgb\n\n# Optuna - The star of the show!\nimport optuna\nfrom optuna.visualization import (\n    plot_optimization_history,\n    plot_param_importances,\n    plot_contour,\n    plot_parallel_coordinate\n)\n\n# Set random seed\nnp.random.seed(42)\n\n# Plotting style with fallback for older matplotlib versions\ntry:\n    plt.style.use('seaborn-v0_8-whitegrid')\nexcept OSError:\n    try:\n        plt.style.use('seaborn-whitegrid')\n    except OSError:\n        pass  # Use default style\n\nprint(\"âœ… Libraries imported!\")\nprint(f\"ğŸ“¦ Optuna version: {optuna.__version__}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California Housing dataset\n",
    "print(\"ğŸ“¦ Loading California Housing Dataset...\")\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "feature_names = housing.feature_names\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset Info:\")\n",
    "print(f\"  â€¢ Training samples: {len(X_train):,}\")\n",
    "print(f\"  â€¢ Test samples: {len(X_test):,}\")\n",
    "print(f\"  â€¢ Features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Baseline Model (Default Hyperparameters)\n",
    "\n",
    "First, let's establish a baseline with default XGBoost hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline model with defaults\n",
    "print(\"ğŸ¯ Training Baseline XGBoost (Default Parameters)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "baseline_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42,\n",
    "    device='cuda',\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "# Cross-validation for robust estimate\n",
    "cv_scores = cross_val_score(\n",
    "    baseline_model, X_train, y_train,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "baseline_rmse = -cv_scores.mean()\n",
    "baseline_std = cv_scores.std()\n",
    "\n",
    "# Also train on full training set for test evaluation\n",
    "baseline_model.fit(X_train, y_train)\n",
    "baseline_test_rmse = np.sqrt(mean_squared_error(y_test, baseline_model.predict(X_test)))\n",
    "\n",
    "print(f\"\\nğŸ“Š Baseline Results (Default Hyperparameters):\")\n",
    "print(f\"  â€¢ CV RMSE: ${baseline_rmse*100000:,.0f} (+/- ${baseline_std*100000:,.0f})\")\n",
    "print(f\"  â€¢ Test RMSE: ${baseline_test_rmse*100000:,.0f}\")\n",
    "print(f\"\\nğŸ¯ Our Goal: Beat ${baseline_rmse*100000:,.0f} CV RMSE through tuning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Understanding XGBoost Hyperparameters\n",
    "\n",
    "Before we tune, let's understand what we're tuning!\n",
    "\n",
    "### ğŸ§’ ELI5: Key XGBoost Hyperparameters\n",
    "\n",
    "> **Think of XGBoost as building a team of advisors...**\n",
    ">\n",
    "> | Parameter | Cookie Analogy | What It Does |\n",
    "> |-----------|---------------|---------------|\n",
    "> | `n_estimators` | Number of taste testers | More trees = more opinions, but slower |\n",
    "> | `max_depth` | How picky each tester is | Deeper = more detailed, but may overthink |\n",
    "> | `learning_rate` | How much to trust each opinion | Lower = need more testers to decide |\n",
    "> | `min_child_weight` | Minimum confidence to speak up | Higher = only sure opinions count |\n",
    "> | `subsample` | What % of cookies each tester tries | Less = more variety in opinions |\n",
    "> | `colsample_bytree` | What % of criteria each tester checks | Less = each tester specializes |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Hyperparameter Guide\n",
    "print(\"ğŸ“š XGBoost Hyperparameter Guide\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "params_guide = \"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    XGBOOST HYPERPARAMETERS                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  ğŸŒ² TREE STRUCTURE                                                   â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚\n",
    "â”‚  max_depth (3-10)                                                   â”‚\n",
    "â”‚    â€¢ Controls tree complexity                                       â”‚\n",
    "â”‚    â€¢ Higher = more complex, risk of overfitting                     â”‚\n",
    "â”‚    â€¢ Default: 6                                                     â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  min_child_weight (1-10)                                            â”‚\n",
    "â”‚    â€¢ Minimum sum of instance weight in a child                      â”‚\n",
    "â”‚    â€¢ Higher = more conservative                                     â”‚\n",
    "â”‚    â€¢ Default: 1                                                     â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  gamma (0-5)                                                        â”‚\n",
    "â”‚    â€¢ Minimum loss reduction to make a split                         â”‚\n",
    "â”‚    â€¢ Higher = more conservative                                     â”‚\n",
    "â”‚    â€¢ Default: 0                                                     â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  ğŸ“‰ LEARNING RATE & TREES                                           â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚\n",
    "â”‚  learning_rate / eta (0.01-0.3)                                     â”‚\n",
    "â”‚    â€¢ Step size shrinkage                                            â”‚\n",
    "â”‚    â€¢ Lower = need more trees, but more robust                       â”‚\n",
    "â”‚    â€¢ Default: 0.3                                                   â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  n_estimators (100-1000)                                            â”‚\n",
    "â”‚    â€¢ Number of boosting rounds                                      â”‚\n",
    "â”‚    â€¢ More = better, but diminishing returns                         â”‚\n",
    "â”‚    â€¢ Use early stopping!                                            â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  ğŸ² SAMPLING                                                         â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                       â”‚\n",
    "â”‚  subsample (0.5-1.0)                                                â”‚\n",
    "â”‚    â€¢ Fraction of samples used per tree                              â”‚\n",
    "â”‚    â€¢ Lower = more regularization                                    â”‚\n",
    "â”‚    â€¢ Default: 1.0                                                   â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  colsample_bytree (0.5-1.0)                                         â”‚\n",
    "â”‚    â€¢ Fraction of features used per tree                             â”‚\n",
    "â”‚    â€¢ Lower = more regularization                                    â”‚\n",
    "â”‚    â€¢ Default: 1.0                                                   â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  ğŸ”§ REGULARIZATION                                                   â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                â”‚\n",
    "â”‚  reg_alpha (0-10)                                                   â”‚\n",
    "â”‚    â€¢ L1 regularization                                              â”‚\n",
    "â”‚    â€¢ Can drive feature weights to zero                              â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  reg_lambda (0-10)                                                  â”‚\n",
    "â”‚    â€¢ L2 regularization                                              â”‚\n",
    "â”‚    â€¢ Shrinks weights, doesn't eliminate                             â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\"\n",
    "print(params_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Optuna Hyperparameter Optimization\n",
    "\n",
    "Now let's use Optuna's Bayesian optimization to find the best hyperparameters!\n",
    "\n",
    "### How Optuna Works\n",
    "\n",
    "1. **Define an objective function** that returns a score to minimize/maximize\n",
    "2. **Create a study** that manages the optimization process\n",
    "3. **Run trials** where Optuna suggests hyperparameters\n",
    "4. **Optuna learns** which regions of the search space are promising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for XGBoost hyperparameter tuning.\n",
    "    \n",
    "    This function:\n",
    "    1. Suggests hyperparameters from defined search spaces\n",
    "    2. Creates an XGBoost model with those parameters\n",
    "    3. Evaluates using cross-validation\n",
    "    4. Returns the score (RMSE to minimize)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        # Tree structure\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
    "        \n",
    "        # Learning\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        \n",
    "        # Sampling\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        \n",
    "        # Regularization\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        \n",
    "        # Fixed parameters\n",
    "        'objective': 'reg:squarederror',\n",
    "        'device': 'cuda',\n",
    "        'random_state': 42,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    # Create model\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train, y_train,\n",
    "        cv=5,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Return mean RMSE (negated back to positive)\n",
    "    return -cv_scores.mean()\n",
    "\n",
    "print(\"âœ… Objective function defined!\")\n",
    "print(\"\\nğŸ“Š Search Space:\")\n",
    "print(\"  â€¢ max_depth: [3, 10]\")\n",
    "print(\"  â€¢ min_child_weight: [1, 10]\")\n",
    "print(\"  â€¢ gamma: [0.0, 5.0]\")\n",
    "print(\"  â€¢ learning_rate: [0.01, 0.3] (log scale)\")\n",
    "print(\"  â€¢ n_estimators: [100, 500]\")\n",
    "print(\"  â€¢ subsample: [0.5, 1.0]\")\n",
    "print(\"  â€¢ colsample_bytree: [0.5, 1.0]\")\n",
    "print(\"  â€¢ reg_alpha: [1e-8, 10.0] (log scale)\")\n",
    "print(\"  â€¢ reg_lambda: [1e-8, 10.0] (log scale)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run the Optuna study\n",
    "print(\"ğŸš€ Starting Optuna Hyperparameter Optimization...\")\n",
    "print(\"=\" * 60)\n",
    "print(\"This will run 100 trials with Bayesian optimization.\")\n",
    "print(\"Each trial uses 5-fold cross-validation.\")\n",
    "print(\"\\nâ³ Estimated time: 2-5 minutes on GPU...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set up sampler (TPE = Tree-structured Parzen Estimator)\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "\n",
    "# Create study (minimize RMSE)\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    sampler=sampler,\n",
    "    study_name='xgboost_housing'\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "start_time = time()\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=100,\n",
    "    show_progress_bar=True,\n",
    "    n_jobs=1  # Sequential for GPU\n",
    ")\n",
    "\n",
    "optimization_time = time() - start_time\n",
    "\n",
    "print(f\"\\nâœ… Optimization complete in {optimization_time:.1f} seconds!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best results\n",
    "print(\"ğŸ† Best Trial Results\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(f\"\\nğŸ“Š Best CV RMSE: ${best_trial.value*100000:,.0f}\")\n",
    "print(f\"   Baseline RMSE: ${baseline_rmse*100000:,.0f}\")\n",
    "print(f\"   Improvement: ${(baseline_rmse - best_trial.value)*100000:,.0f} ({(baseline_rmse - best_trial.value)/baseline_rmse*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nğŸ”§ Best Hyperparameters:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"   â€¢ {key}: {value:.6f}\")\n",
    "    else:\n",
    "        print(f\"   â€¢ {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Visualizing the Optimization Process\n",
    "\n",
    "Optuna provides powerful visualization tools to understand the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization History\n",
    "print(\"ğŸ“ˆ Optimization History\")\n",
    "print(\"Shows how the best score improved over trials.\")\n",
    "\n",
    "fig = plot_optimization_history(study)\n",
    "fig.update_layout(title='XGBoost Hyperparameter Optimization History')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Importance\n",
    "print(\"ğŸ“Š Hyperparameter Importance\")\n",
    "print(\"Shows which hyperparameters had the biggest impact on performance.\")\n",
    "\n",
    "fig = plot_param_importances(study)\n",
    "fig.update_layout(title='Which Hyperparameters Matter Most?')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contour plot of top 2 parameters\n",
    "print(\"ğŸ—ºï¸ Contour Plot\")\n",
    "print(\"Shows interaction between the two most important hyperparameters.\")\n",
    "\n",
    "# Get top 2 important parameters\n",
    "importances = optuna.importance.get_param_importances(study)\n",
    "top_params = list(importances.keys())[:2]\n",
    "\n",
    "fig = plot_contour(study, params=top_params)\n",
    "fig.update_layout(title=f'Interaction: {top_params[0]} vs {top_params[1]}')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel Coordinate Plot\n",
    "print(\"ğŸ“‰ Parallel Coordinate Plot\")\n",
    "print(\"Shows relationships between all hyperparameters and the objective.\")\n",
    "\n",
    "fig = plot_parallel_coordinate(study)\n",
    "fig.update_layout(title='Parallel Coordinate View of All Trials')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary visualization with matplotlib\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Optimization history\n",
    "ax1 = axes[0, 0]\n",
    "trial_numbers = [t.number for t in study.trials]\n",
    "values = [t.value for t in study.trials]\n",
    "best_values = [min(values[:i+1]) for i in range(len(values))]\n",
    "\n",
    "ax1.scatter(trial_numbers, values, alpha=0.5, s=30, label='Trial RMSE')\n",
    "ax1.plot(trial_numbers, best_values, 'r-', linewidth=2, label='Best so far')\n",
    "ax1.axhline(baseline_rmse, color='green', linestyle='--', label='Baseline')\n",
    "ax1.set_xlabel('Trial Number')\n",
    "ax1.set_ylabel('CV RMSE')\n",
    "ax1.set_title('Optimization History')\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Parameter importance (bar chart)\n",
    "ax2 = axes[0, 1]\n",
    "importances = optuna.importance.get_param_importances(study)\n",
    "params = list(importances.keys())\n",
    "imp_values = list(importances.values())\n",
    "colors = plt.cm.viridis(np.linspace(0, 0.8, len(params)))\n",
    "ax2.barh(params, imp_values, color=colors)\n",
    "ax2.set_xlabel('Importance')\n",
    "ax2.set_title('Hyperparameter Importance')\n",
    "\n",
    "# 3. Distribution of learning_rate values tried\n",
    "ax3 = axes[1, 0]\n",
    "lr_values = [t.params['learning_rate'] for t in study.trials]\n",
    "rmse_values = [t.value for t in study.trials]\n",
    "scatter = ax3.scatter(lr_values, rmse_values, c=trial_numbers, cmap='viridis', alpha=0.6)\n",
    "ax3.set_xlabel('Learning Rate')\n",
    "ax3.set_ylabel('CV RMSE')\n",
    "ax3.set_title('Learning Rate vs Performance')\n",
    "ax3.set_xscale('log')\n",
    "plt.colorbar(scatter, ax=ax3, label='Trial #')\n",
    "\n",
    "# 4. Distribution of max_depth values tried\n",
    "ax4 = axes[1, 1]\n",
    "depth_values = [t.params['max_depth'] for t in study.trials]\n",
    "ax4.scatter(depth_values, rmse_values, c=trial_numbers, cmap='viridis', alpha=0.6)\n",
    "ax4.set_xlabel('Max Depth')\n",
    "ax4.set_ylabel('CV RMSE')\n",
    "ax4.set_title('Max Depth vs Performance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('optimization_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¾ Saved optimization_summary.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Evaluate the Tuned Model\n",
    "\n",
    "Now let's train the final model with the best hyperparameters and evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best hyperparameters\n",
    "print(\"ğŸ¯ Training Final Model with Best Hyperparameters...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get best params and add fixed params\n",
    "best_params = best_trial.params.copy()\n",
    "best_params['objective'] = 'reg:squarederror'\n",
    "best_params['device'] = 'cuda'\n",
    "best_params['random_state'] = 42\n",
    "best_params['verbosity'] = 0\n",
    "\n",
    "# Create final model\n",
    "final_model = xgb.XGBRegressor(**best_params)\n",
    "\n",
    "# Train on full training set\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_pred = final_model.predict(X_train)\n",
    "test_pred = final_model.predict(X_test)\n",
    "\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "test_r2 = r2_score(y_test, test_pred)\n",
    "\n",
    "print(f\"\\nğŸ“Š Final Model Performance:\")\n",
    "print(f\"  â€¢ Train RMSE: ${train_rmse*100000:,.0f}\")\n",
    "print(f\"  â€¢ Test RMSE:  ${test_rmse*100000:,.0f}\")\n",
    "print(f\"  â€¢ Test RÂ²:    {test_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ†š Comparison with Baseline:\")\n",
    "print(f\"  â€¢ Baseline Test RMSE: ${baseline_test_rmse*100000:,.0f}\")\n",
    "print(f\"  â€¢ Tuned Test RMSE:    ${test_rmse*100000:,.0f}\")\n",
    "print(f\"  â€¢ Improvement:        ${(baseline_test_rmse - test_rmse)*100000:,.0f} ({(baseline_test_rmse - test_rmse)/baseline_test_rmse*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from tuned model\n",
    "print(\"ğŸ“Š Feature Importance (Tuned Model)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': final_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.barh(importance_df['Feature'], importance_df['Importance'], \n",
    "               color='steelblue', alpha=0.8)\n",
    "ax.set_xlabel('Feature Importance (Gain)')\n",
    "ax.set_title('XGBoost Feature Importance (Tuned Model)')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, importance_df['Importance']):\n",
    "    ax.text(val + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "            f'{val:.3f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline vs tuned predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Baseline predictions\n",
    "ax1 = axes[0]\n",
    "baseline_pred = baseline_model.predict(X_test)\n",
    "ax1.scatter(y_test, baseline_pred, alpha=0.5, s=20)\n",
    "ax1.plot([0, 5], [0, 5], 'r--', linewidth=2)\n",
    "ax1.set_xlabel('Actual Price ($100K)')\n",
    "ax1.set_ylabel('Predicted Price ($100K)')\n",
    "ax1.set_title(f'Baseline Model (RMSE: ${baseline_test_rmse*100000:,.0f})')\n",
    "ax1.set_xlim(0, 5.5)\n",
    "ax1.set_ylim(0, 5.5)\n",
    "\n",
    "# Tuned predictions\n",
    "ax2 = axes[1]\n",
    "ax2.scatter(y_test, test_pred, alpha=0.5, s=20, color='green')\n",
    "ax2.plot([0, 5], [0, 5], 'r--', linewidth=2)\n",
    "ax2.set_xlabel('Actual Price ($100K)')\n",
    "ax2.set_ylabel('Predicted Price ($100K)')\n",
    "ax2.set_title(f'Tuned Model (RMSE: ${test_rmse*100000:,.0f})')\n",
    "ax2.set_xlim(0, 5.5)\n",
    "ax2.set_ylim(0, 5.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Best Practices and Tips\n",
    "\n",
    "### Key Takeaways from Our Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of optimization insights\n",
    "print(\"ğŸ’¡ Optimization Insights\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get parameter importance\n",
    "importances = optuna.importance.get_param_importances(study)\n",
    "\n",
    "print(\"\\nğŸ“Š Most Important Hyperparameters (in order):\")\n",
    "for i, (param, imp) in enumerate(importances.items(), 1):\n",
    "    print(f\"   {i}. {param}: {imp:.3f}\")\n",
    "\n",
    "print(\"\\nğŸ¯ Optimal Ranges Found:\")\n",
    "for param in list(importances.keys())[:3]:\n",
    "    values = [t.params[param] for t in study.trials]\n",
    "    best_val = best_trial.params[param]\n",
    "    print(f\"   â€¢ {param}: Best={best_val:.4f}, Range tried=[{min(values):.4f}, {max(values):.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices summary\n",
    "print(\"ğŸ“š Best Practices for Hyperparameter Tuning\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "best_practices = \"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    HYPERPARAMETER TUNING BEST PRACTICES             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  1ï¸âƒ£  USE CROSS-VALIDATION                                            â”‚\n",
    "â”‚     â€¢ Single train/val split can be misleading                      â”‚\n",
    "â”‚     â€¢ 5-fold CV gives more robust estimates                         â”‚\n",
    "â”‚     â€¢ Takes longer, but worth it!                                   â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  2ï¸âƒ£  USE LOG SCALE FOR CERTAIN PARAMETERS                            â”‚\n",
    "â”‚     â€¢ learning_rate: [0.001, 0.3] should be log scale               â”‚\n",
    "â”‚     â€¢ regularization: often spans orders of magnitude               â”‚\n",
    "â”‚     â€¢ Log scale explores more efficiently                           â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  3ï¸âƒ£  START WITH WIDE RANGES, THEN NARROW                             â”‚\n",
    "â”‚     â€¢ First run: explore wide ranges (100 trials)                   â”‚\n",
    "â”‚     â€¢ Second run: narrow to promising regions                       â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  4ï¸âƒ£  USE PRUNING FOR EXPENSIVE EVALUATIONS                           â”‚\n",
    "â”‚     â€¢ Optuna can stop bad trials early                              â”‚\n",
    "â”‚     â€¢ Saves compute on obviously poor configurations                â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  5ï¸âƒ£  SAVE YOUR STUDIES                                               â”‚\n",
    "â”‚     â€¢ Use optuna.create_study(storage='sqlite:///study.db')         â”‚\n",
    "â”‚     â€¢ Resume optimization later                                     â”‚\n",
    "â”‚     â€¢ Compare across experiments                                    â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  6ï¸âƒ£  DON'T OVERTUNE                                                  â”‚\n",
    "â”‚     â€¢ Diminishing returns after ~100 trials                         â”‚\n",
    "â”‚     â€¢ Risk of overfitting to validation set                         â”‚\n",
    "â”‚     â€¢ Keep a hold-out test set untouched!                           â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\"\n",
    "print(best_practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ‹ Try It Yourself\n",
    "\n",
    "### Exercise 1: Tune with Pruning\n",
    "\n",
    "Use Optuna's pruning feature to speed up optimization.\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Hint</summary>\n",
    "Use `optuna.integration.XGBoostPruningCallback` with the XGBoost callback system.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### ğŸ’¡ Advanced: Optuna Pruning with XGBoost\n\nOptuna can **prune** (early stop) poorly performing trials to save time. This requires using XGBoost's native API instead of the sklearn-compatible API:\n\n```python\n# XGBoost has TWO APIs:\n# 1. sklearn API (we've been using): xgb.XGBRegressor - easy, familiar\n# 2. Native API: xgb.train() with DMatrix - more control, supports pruning\n\nimport xgboost as xgb\n\n# Convert data to XGBoost's native format\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndval = xgb.DMatrix(X_val, label=y_val)\n\n# Native parameters (slightly different names)\nparams = {\n    'objective': 'reg:squarederror',\n    'max_depth': 6,\n    'eta': 0.1,  # Called 'learning_rate' in sklearn API\n}\n\n# Train with native API\nbst = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=500,\n    evals=[(dval, 'validation')],\n    early_stopping_rounds=10,\n    verbose_eval=False\n)\n\n# Predict\npreds = bst.predict(dval)\n```\n\n**Pruning callback:**\n```python\nfrom optuna.integration import XGBoostPruningCallback\n\n# Inside your objective function:\npruning_callback = XGBoostPruningCallback(trial, 'validation-rmse')\nbst = xgb.train(params, dtrain, callbacks=[pruning_callback], ...)\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Implement pruning to stop bad trials early\n",
    "\n",
    "# def objective_with_pruning(trial):\n",
    "#     params = { ... }  # Same as before\n",
    "#     \n",
    "#     # Add pruning callback\n",
    "#     pruning_callback = optuna.integration.XGBoostPruningCallback(trial, 'validation-rmse')\n",
    "#     \n",
    "#     # Use XGBoost's native API with callback\n",
    "#     dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "#     dval = xgb.DMatrix(X_val, label=y_val)\n",
    "#     \n",
    "#     bst = xgb.train(\n",
    "#         params,\n",
    "#         dtrain,\n",
    "#         num_boost_round=500,\n",
    "#         evals=[(dval, 'validation')],\n",
    "#         callbacks=[pruning_callback],\n",
    "#         verbose_eval=False\n",
    "#     )\n",
    "#     \n",
    "#     return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Tune LightGBM\n",
    "\n",
    "Apply the same optimization technique to LightGBM.\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Hint</summary>\n",
    "LightGBM has similar parameters but different names:\n",
    "- `num_leaves` instead of focusing on `max_depth`\n",
    "- `min_child_samples` instead of `min_child_weight`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### ğŸ’¡ LightGBM Introduction\n\nLightGBM is another gradient boosting library, often faster than XGBoost:\n\n```python\n# Install: pip install lightgbm (or use NGC container)\nimport lightgbm as lgb\n\n# LightGBM uses slightly different parameter names:\nmodel = lgb.LGBMRegressor(\n    num_leaves=31,          # Max leaves per tree (replaces max_depth focus)\n    min_child_samples=20,   # Similar to min_child_weight\n    learning_rate=0.1,\n    n_estimators=100,\n    verbosity=-1            # Suppress output\n)\n\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n```\n\n**Key differences from XGBoost:**\n- `num_leaves` controls complexity (instead of primarily `max_depth`)\n- Leaf-wise tree growth (vs level-wise in XGBoost)\n- Often faster for large datasets\n- `min_child_samples` instead of `min_child_weight`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Tune LightGBM and compare with XGBoost\n",
    "\n",
    "# import lightgbm as lgb\n",
    "# \n",
    "# def lgb_objective(trial):\n",
    "#     params = {\n",
    "#         'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "#         'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "#         ...\n",
    "#     }\n",
    "#     ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Multi-Objective Optimization\n",
    "\n",
    "Optimize for both accuracy AND training speed.\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Hint</summary>\n",
    "Use `optuna.create_study(directions=['minimize', 'minimize'])` for multi-objective.\n",
    "Return a tuple: (rmse, training_time)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### ğŸ’¡ Multi-Objective Optimization with Optuna\n\nSometimes you want to optimize for multiple objectives (e.g., accuracy AND speed):\n\n```python\n# Create a multi-objective study\nstudy = optuna.create_study(\n    directions=['minimize', 'minimize']  # Both objectives to minimize\n    # Use ['minimize', 'maximize'] if second objective should be maximized\n)\n\n# Objective function returns a TUPLE of values\ndef multi_objective(trial):\n    params = {...}\n    \n    start = time()\n    model.fit(X_train, y_train)\n    train_time = time() - start\n    \n    rmse = compute_rmse(model, X_val, y_val)\n    \n    return rmse, train_time  # Return tuple of objectives\n\nstudy.optimize(multi_objective, n_trials=50)\n\n# Get Pareto front (best trade-offs)\npareto_trials = study.best_trials  # Note: best_trials (plural) for multi-objective\n```\n\n**Key differences from single-objective:**\n- Use `directions` (plural) with a list\n- Return a tuple from objective function\n- Use `study.best_trials` to get Pareto-optimal solutions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "# Multi-objective: minimize both RMSE and training time\n",
    "\n",
    "# def multi_objective(trial):\n",
    "#     params = { ... }\n",
    "#     \n",
    "#     start = time()\n",
    "#     # ... train model ...\n",
    "#     train_time = time() - start\n",
    "#     \n",
    "#     return rmse, train_time  # Return tuple\n",
    "#\n",
    "# study = optuna.create_study(directions=['minimize', 'minimize'])\n",
    "# study.optimize(multi_objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš ï¸ Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Using Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âŒ Wrong: Single train/val split\n",
    "# def bad_objective(trial):\n",
    "#     model = xgb.XGBRegressor(**params)\n",
    "#     model.fit(X_train_split, y_train_split)  # Fixed split!\n",
    "#     return model.score(X_val_split, y_val_split)  # Overfits to this split\n",
    "\n",
    "# âœ… Right: Use cross-validation\n",
    "print(\"ğŸ’¡ Always use cross-validation in hyperparameter tuning!\")\n",
    "print(\"   A single split can give misleading results.\")\n",
    "print(\"   CV gives a more robust estimate of generalization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Searching Linear Scale for Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âŒ Wrong: Linear scale for learning rate\n",
    "# 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "# This spends 90% of samples between 0.1-0.3, ignoring 0.01-0.1!\n",
    "\n",
    "# âœ… Right: Log scale\n",
    "# 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
    "\n",
    "print(\"ğŸ’¡ Use log scale for parameters that span orders of magnitude!\")\n",
    "print(\"   â€¢ learning_rate: 0.001 to 0.3 â†’ log scale\")\n",
    "print(\"   â€¢ reg_alpha/reg_lambda: 1e-8 to 10 â†’ log scale\")\n",
    "print(\"   â€¢ max_depth: 3 to 10 â†’ linear scale (small range)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Using Test Set During Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âŒ Wrong: Using test set to select hyperparameters\n",
    "# def bad_objective(trial):\n",
    "#     ...\n",
    "#     return model.score(X_TEST, y_TEST)  # NEVER DO THIS!\n",
    "\n",
    "# âœ… Right: Keep test set completely separate\n",
    "print(\"âš ï¸ CRITICAL: Never use the test set during hyperparameter tuning!\")\n",
    "print(\"\")\n",
    "print(\"   The test set must be completely untouched until final evaluation.\")\n",
    "print(\"   Otherwise, you're overfitting to the test set!\")\n",
    "print(\"\")\n",
    "print(\"   Correct workflow:\")\n",
    "print(\"   1. Split data: train/val/test\")\n",
    "print(\"   2. Use CV on train+val for tuning\")\n",
    "print(\"   3. Train final model on train+val\")\n",
    "print(\"   4. Evaluate ONCE on test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ Checkpoint\n",
    "\n",
    "Congratulations! You've mastered hyperparameter optimization with Optuna. You've learned:\n",
    "\n",
    "- âœ… **Optuna basics**: Studies, trials, and objectives\n",
    "- âœ… **Search space design**: Integer, float, categorical, and log scale\n",
    "- âœ… **XGBoost parameters**: What they do and how to tune them\n",
    "- âœ… **Visualization**: Understanding optimization history and parameter importance\n",
    "- âœ… **Best practices**: Cross-validation, log scale, and avoiding test set leakage\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Challenge (Optional)\n",
    "\n",
    "**The Kaggle Challenge:** Can you beat the leaderboard?\n",
    "\n",
    "1. Download a Kaggle tabular competition dataset\n",
    "2. Use Optuna to tune XGBoost, LightGBM, and CatBoost\n",
    "3. Ensemble the best models\n",
    "4. Submit to Kaggle and compare with the leaderboard!\n",
    "\n",
    "Good starter competitions:\n",
    "- House Prices: Advanced Regression Techniques\n",
    "- Titanic: Machine Learning from Disaster\n",
    "- Spaceship Titanic\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– Further Reading\n",
    "\n",
    "- [Optuna Documentation](https://optuna.readthedocs.io/)\n",
    "- [Optuna Tutorial](https://optuna.readthedocs.io/en/stable/tutorial/index.html)\n",
    "- [XGBoost Parameter Tuning Guide](https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html)\n",
    "- [Bayesian Optimization Primer](https://distill.pub/2020/bayesian-optimization/)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "import gc\n",
    "\n",
    "# Delete models\n",
    "del baseline_model, final_model\n",
    "del study\n",
    "\n",
    "# Garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## â¡ï¸ Next Steps\n",
    "\n",
    "Continue to **Lab 1.6.3: RAPIDS Acceleration** to learn how to speed up classical ML by 10-100x using GPU acceleration with cuML!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}