{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.1.1: System Exploration - SOLUTIONS\n",
    "\n",
    "This notebook contains solutions to the exercises in the System Exploration notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try It Yourself #1 Solution\n",
    "\n",
    "**Task:** Run nvidia-smi topo command. What interconnect does the DGX Spark use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: GPU topology information\n",
    "!nvidia-smi topo --matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "        GPU0    CPU Affinity    NUMA Affinity\n",
    "GPU0     X      0-19            N/A\n",
    "\n",
    "Legend:\n",
    "  X    = Self\n",
    "  SYS  = Connection traversing PCIe as well as the SMP interconnect\n",
    "  NODE = Connection traversing PCIe as well as the NUMA node interconnect\n",
    "  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge\n",
    "  NV#  = Connection traversing NVLink\n",
    "```\n",
    "\n",
    "**Answer:** The DGX Spark uses NVLink-C2C (Chip-to-Chip) interconnect between the Grace CPU and Blackwell GPU. This is shown in the topology as a direct connection with no PCIe traversal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself #2 Solution\n",
    "\n",
    "**Task:** Find out the cache sizes on your CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Filter for cache information\n",
    "!lscpu | grep -i cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "L1d cache:                       1 MiB (20 instances)\n",
    "L1i cache:                       1 MiB (20 instances)\n",
    "L2 cache:                        20 MiB (20 instances)\n",
    "L3 cache:                        24 MiB (1 instance)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- L1d (data) and L1i (instruction) caches: 1 MiB each per core\n",
    "- L2 cache: 1 MiB per core (20 total)\n",
    "- L3 cache: 24 MiB shared across all cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Solution\n",
    "\n",
    "**Task:** Create a monitoring script that runs every 5 seconds and displays GPU memory, temperature, and system memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: Using watch command (run in terminal)\n",
    "print(\"Run this in terminal:\")\n",
    "print(\"\")\n",
    "print('watch -n 5 \"nvidia-smi --query-gpu=memory.used,memory.free,temperature.gpu --format=csv && echo \\'---\\' && free -h | head -2\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2: Python version with continuous updates\n",
    "import subprocess\n",
    "import time\n",
    "import sys\n",
    "\n",
    "def monitor_once():\n",
    "    \"\"\"Display system status once.\"\"\"\n",
    "    # Clear output (works in Jupyter)\n",
    "    from IPython.display import clear_output\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"System Monitor - {time.strftime('%H:%M:%S')}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # GPU info - with timeout to prevent hanging\n",
    "    try:\n",
    "        gpu_info = subprocess.run(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.used,memory.free,temperature.gpu,power.draw\", \n",
    "             \"--format=csv,noheader\"],\n",
    "            capture_output=True, text=True, timeout=30\n",
    "        )\n",
    "        if gpu_info.returncode == 0:\n",
    "            parts = gpu_info.stdout.strip().split(\", \")\n",
    "            if len(parts) >= 4:\n",
    "                print(f\"\\nGPU Memory Used:  {parts[0]}\")\n",
    "                print(f\"GPU Memory Free:  {parts[1]}\")\n",
    "                print(f\"GPU Temperature:  {parts[2]}C\")\n",
    "                print(f\"GPU Power:        {parts[3]}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"\\nGPU info: Command timed out\")\n",
    "    \n",
    "    # System memory - with timeout to prevent hanging\n",
    "    try:\n",
    "        mem_info = subprocess.run([\"free\", \"-h\"], capture_output=True, text=True, timeout=30)\n",
    "        if mem_info.returncode == 0:\n",
    "            lines = mem_info.stdout.strip().split(\"\\n\")\n",
    "            print(\"\\nSystem Memory:\")\n",
    "            for line in lines[:2]:\n",
    "                print(f\"  {line}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"\\nMemory info: Command timed out\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"Press Ctrl+C to stop (or interrupt kernel)\")\n",
    "\n",
    "def continuous_monitor(interval=5, max_iterations=10):\n",
    "    \"\"\"Monitor continuously for a number of iterations.\"\"\"\n",
    "    for i in range(max_iterations):\n",
    "        monitor_once()\n",
    "        time.sleep(interval)\n",
    "    print(\"\\nMonitoring complete!\")\n",
    "\n",
    "# Run once to show output\n",
    "monitor_once()\n",
    "\n",
    "# Uncomment to run continuously:\n",
    "# continuous_monitor(interval=5, max_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3: Using the memory_monitor script from scripts/\n",
    "print(\"You can also use the provided memory_monitor.py script:\")\n",
    "print(\"\")\n",
    "print(\"python ../scripts/memory_monitor.py --interval 5\")\n",
    "print(\"python ../scripts/memory_monitor.py --interval 5 --processes  # Include GPU processes\")\n",
    "print(\"python ../scripts/memory_monitor.py --interval 5 --log memory.csv  # Log to file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **nvidia-smi** is your primary tool for GPU monitoring\n",
    "2. **lscpu** shows CPU architecture - ARM64 on DGX Spark\n",
    "3. **free** shows unified memory shared between CPU and GPU\n",
    "4. The Grace-Blackwell architecture uses NVLink-C2C for fast interconnect\n",
    "5. Buffer cache can compete with GPU memory - clear before large models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup resources\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
