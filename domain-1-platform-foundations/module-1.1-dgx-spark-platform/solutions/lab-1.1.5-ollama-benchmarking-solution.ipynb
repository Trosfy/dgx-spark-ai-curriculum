{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.1.5: Ollama Benchmarking - SOLUTIONS\n",
    "\n",
    "This notebook contains solutions to the exercises in the Ollama Benchmarking notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Solution\n",
    "\n",
    "**Task:** Create a benchmark that tests different quantization levels, context lengths, and batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import statistics\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    model: str\n",
    "    config: str\n",
    "    prefill_tps: float\n",
    "    decode_tps: float\n",
    "    prompt_tokens: int\n",
    "    generated_tokens: int\n",
    "    total_time_s: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution Part 1: Quantization Level Comparison\n",
    "\n",
    "def benchmark_quantization_levels():\n",
    "    \"\"\"\n",
    "    Compare different quantization levels of the same model.\n",
    "    \n",
    "    You'll need to pull these models first:\n",
    "        ollama pull llama3.1:8b-q4_0\n",
    "        ollama pull llama3.1:8b-q5_0  \n",
    "        ollama pull llama3.1:8b-q8_0\n",
    "        ollama pull llama3.1:8b-fp16 (if available)\n",
    "    \"\"\"\n",
    "    # Models to compare\n",
    "    quant_models = [\n",
    "        \"llama3.1:8b-q4_0\",   # 4-bit quantization\n",
    "        \"llama3.1:8b-q5_0\",   # 5-bit quantization\n",
    "        \"llama3.1:8b-q8_0\",   # 8-bit quantization\n",
    "    ]\n",
    "    \n",
    "    # Check which models are available\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_BASE_URL}/api/tags\", timeout=30)\n",
    "        available = [m[\"name\"] for m in response.json().get(\"models\", [])]\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"Error: Ollama server connection timed out\")\n",
    "        return []\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Error: Could not connect to Ollama server\")\n",
    "        return []\n",
    "    \n",
    "    models_to_test = [m for m in quant_models if m in available]\n",
    "    \n",
    "    if not models_to_test:\n",
    "        print(\"No quantized models found. Pull them first:\")\n",
    "        for m in quant_models:\n",
    "            print(f\"  ollama pull {m}\")\n",
    "        return []\n",
    "    \n",
    "    print(\"Quantization Level Comparison\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = []\n",
    "    prompt = \"Explain the concept of machine learning in detail.\"\n",
    "    \n",
    "    for model in models_to_test:\n",
    "        print(f\"\\nBenchmarking {model}...\")\n",
    "        \n",
    "        try:\n",
    "            # Warmup\n",
    "            requests.post(f\"{OLLAMA_BASE_URL}/api/generate\", \n",
    "                         json={\"model\": model, \"prompt\": \"Hi\", \"stream\": False},\n",
    "                         timeout=60)\n",
    "            \n",
    "            # Benchmark\n",
    "            speeds = []\n",
    "            for _ in range(3):\n",
    "                resp = requests.post(f\"{OLLAMA_BASE_URL}/api/generate\",\n",
    "                    json={\"model\": model, \"prompt\": prompt, \"stream\": False,\n",
    "                          \"options\": {\"num_predict\": 100}},\n",
    "                    timeout=120)\n",
    "                data = resp.json()\n",
    "                \n",
    "                eval_count = data.get(\"eval_count\", 0)\n",
    "                eval_duration = data.get(\"eval_duration\", 1) / 1e9\n",
    "                speeds.append(eval_count / eval_duration if eval_duration > 0 else 0)\n",
    "            \n",
    "            avg_speed = statistics.mean(speeds)\n",
    "            print(f\"  Decode speed: {avg_speed:.1f} tok/s\")\n",
    "            \n",
    "            results.append({\n",
    "                \"model\": model,\n",
    "                \"decode_tps\": avg_speed\n",
    "            })\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"  Timeout while benchmarking {model}\")\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"  Connection error while benchmarking {model}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run quantization comparison\n",
    "quant_results = benchmark_quantization_levels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution Part 2: Context Length Comparison\n",
    "\n",
    "def benchmark_context_lengths(model: str = None):\n",
    "    \"\"\"\n",
    "    Compare performance at different context lengths.\n",
    "    \n",
    "    Longer contexts = slower prefill but same decode speed.\n",
    "    \"\"\"\n",
    "    # Get first available model if not specified\n",
    "    if model is None:\n",
    "        try:\n",
    "            response = requests.get(f\"{OLLAMA_BASE_URL}/api/tags\", timeout=30)\n",
    "            models = response.json().get(\"models\", [])\n",
    "            if not models:\n",
    "                print(\"No models available!\")\n",
    "                return []\n",
    "            model = models[0][\"name\"]\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(\"Error: Ollama server connection timed out\")\n",
    "            return []\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(\"Error: Could not connect to Ollama server\")\n",
    "            return []\n",
    "    \n",
    "    # Context lengths to test\n",
    "    context_configs = [\n",
    "        (512, \"short\"),\n",
    "        (2048, \"medium\"),\n",
    "        (4096, \"long\"),\n",
    "        (8192, \"very_long\"),\n",
    "    ]\n",
    "    \n",
    "    # Generate prompts of different lengths\n",
    "    base_text = \"Please analyze and respond to this text. \" * 50\n",
    "    \n",
    "    print(f\"\\nContext Length Comparison: {model}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for num_ctx, label in context_configs:\n",
    "        print(f\"\\nTesting context length: {num_ctx} ({label})\")\n",
    "        \n",
    "        try:\n",
    "            # Use shorter prompt for shorter contexts\n",
    "            prompt_len = min(num_ctx // 4, len(base_text))\n",
    "            prompt = base_text[:prompt_len]\n",
    "            \n",
    "            resp = requests.post(f\"{OLLAMA_BASE_URL}/api/generate\",\n",
    "                json={\n",
    "                    \"model\": model,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\n",
    "                        \"num_ctx\": num_ctx,\n",
    "                        \"num_predict\": 50\n",
    "                    }\n",
    "                },\n",
    "                timeout=120\n",
    "            )\n",
    "            data = resp.json()\n",
    "            \n",
    "            prompt_count = data.get(\"prompt_eval_count\", 0)\n",
    "            prompt_duration = data.get(\"prompt_eval_duration\", 1) / 1e9\n",
    "            eval_count = data.get(\"eval_count\", 0)\n",
    "            eval_duration = data.get(\"eval_duration\", 1) / 1e9\n",
    "            \n",
    "            prefill_tps = prompt_count / prompt_duration if prompt_duration > 0 else 0\n",
    "            decode_tps = eval_count / eval_duration if eval_duration > 0 else 0\n",
    "            \n",
    "            print(f\"  Prompt tokens: {prompt_count}\")\n",
    "            print(f\"  Prefill: {prefill_tps:.1f} tok/s\")\n",
    "            print(f\"  Decode: {decode_tps:.1f} tok/s\")\n",
    "            \n",
    "            results.append({\n",
    "                \"context_length\": num_ctx,\n",
    "                \"label\": label,\n",
    "                \"prompt_tokens\": prompt_count,\n",
    "                \"prefill_tps\": prefill_tps,\n",
    "                \"decode_tps\": decode_tps\n",
    "            })\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"  Timeout for context length {num_ctx}\")\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"  Connection error for context length {num_ctx}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run context length comparison\n",
    "ctx_results = benchmark_context_lengths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution Part 3: Concurrent Request Simulation\n",
    "\n",
    "import concurrent.futures\n",
    "import threading\n",
    "\n",
    "def benchmark_concurrent_requests(model: str = None, num_concurrent: int = 4):\n",
    "    \"\"\"\n",
    "    Test performance under concurrent load.\n",
    "    \n",
    "    Note: Ollama processes requests serially by default,\n",
    "    so this measures queuing behavior more than true parallelism.\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        try:\n",
    "            response = requests.get(f\"{OLLAMA_BASE_URL}/api/tags\", timeout=30)\n",
    "            models = response.json().get(\"models\", [])\n",
    "            if not models:\n",
    "                print(\"No models available!\")\n",
    "                return {}\n",
    "            model = models[0][\"name\"]\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(\"Error: Ollama server connection timed out\")\n",
    "            return {}\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(\"Error: Could not connect to Ollama server\")\n",
    "            return {}\n",
    "    \n",
    "    print(f\"\\nConcurrent Request Test: {model}\")\n",
    "    print(f\"Simulating {num_concurrent} concurrent requests\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    prompt = \"What is machine learning?\"\n",
    "    results = []\n",
    "    results_lock = threading.Lock()\n",
    "    \n",
    "    def make_request(request_id: int):\n",
    "        start = time.time()\n",
    "        try:\n",
    "            resp = requests.post(f\"{OLLAMA_BASE_URL}/api/generate\",\n",
    "                json={\"model\": model, \"prompt\": prompt, \"stream\": False,\n",
    "                      \"options\": {\"num_predict\": 50}},\n",
    "                timeout=60\n",
    "            )\n",
    "            elapsed = time.time() - start\n",
    "            data = resp.json()\n",
    "            \n",
    "            with results_lock:\n",
    "                results.append({\n",
    "                    \"request_id\": request_id,\n",
    "                    \"success\": True,\n",
    "                    \"time_s\": elapsed,\n",
    "                    \"tokens\": data.get(\"eval_count\", 0)\n",
    "                })\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            elapsed = time.time() - start\n",
    "            with results_lock:\n",
    "                results.append({\n",
    "                    \"request_id\": request_id,\n",
    "                    \"success\": False,\n",
    "                    \"time_s\": elapsed,\n",
    "                    \"error\": \"Request timed out\"\n",
    "                })\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            elapsed = time.time() - start\n",
    "            with results_lock:\n",
    "                results.append({\n",
    "                    \"request_id\": request_id,\n",
    "                    \"success\": False,\n",
    "                    \"time_s\": elapsed,\n",
    "                    \"error\": \"Connection error\"\n",
    "                })\n",
    "        except Exception as e:\n",
    "            elapsed = time.time() - start\n",
    "            with results_lock:\n",
    "                results.append({\n",
    "                    \"request_id\": request_id,\n",
    "                    \"success\": False,\n",
    "                    \"time_s\": elapsed,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "    \n",
    "    # Launch concurrent requests\n",
    "    overall_start = time.time()\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_concurrent) as executor:\n",
    "        futures = [executor.submit(make_request, i) for i in range(num_concurrent)]\n",
    "        concurrent.futures.wait(futures)\n",
    "    \n",
    "    overall_time = time.time() - overall_start\n",
    "    \n",
    "    # Analyze results\n",
    "    successful = [r for r in results if r[\"success\"]]\n",
    "    \n",
    "    if successful:\n",
    "        avg_time = statistics.mean(r[\"time_s\"] for r in successful)\n",
    "        total_tokens = sum(r[\"tokens\"] for r in successful)\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Successful requests: {len(successful)}/{num_concurrent}\")\n",
    "        print(f\"  Total time: {overall_time:.2f}s\")\n",
    "        print(f\"  Average request time: {avg_time:.2f}s\")\n",
    "        print(f\"  Total tokens generated: {total_tokens}\")\n",
    "        print(f\"  Overall throughput: {total_tokens/overall_time:.1f} tok/s\")\n",
    "    \n",
    "    return {\n",
    "        \"concurrent_requests\": num_concurrent,\n",
    "        \"overall_time_s\": overall_time,\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "# Run concurrent test\n",
    "concurrent_results = benchmark_concurrent_requests(num_concurrent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution Part 4: Comprehensive Report\n",
    "\n",
    "def generate_comprehensive_report():\n",
    "    \"\"\"Generate a full benchmark report.\"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"          DGX SPARK COMPREHENSIVE BENCHMARK REPORT\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print()\n",
    "    \n",
    "    # Run all benchmarks\n",
    "    print(\"\\n[1/3] Testing available models...\")\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_BASE_URL}/api/tags\", timeout=30)\n",
    "        models = [m[\"name\"] for m in response.json().get(\"models\", [])]\n",
    "        print(f\"Found {len(models)} models: {', '.join(models[:5])}{'...' if len(models) > 5 else ''}\")\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"Error: Ollama server connection timed out\")\n",
    "        models = []\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Error: Could not connect to Ollama server\")\n",
    "        models = []\n",
    "    \n",
    "    print(\"\\n[2/3] Running context length tests...\")\n",
    "    ctx_results = benchmark_context_lengths(models[0] if models else None)\n",
    "    \n",
    "    print(\"\\n[3/3] Running concurrent request tests...\")\n",
    "    conc_results = benchmark_concurrent_requests(models[0] if models else None, num_concurrent=3)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"                        SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if ctx_results:\n",
    "        print(\"\\nContext Length Impact:\")\n",
    "        for r in ctx_results:\n",
    "            print(f\"  {r['context_length']:>5} tokens: Prefill {r['prefill_tps']:>7.1f} tok/s, Decode {r['decode_tps']:>6.1f} tok/s\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Report complete!\")\n",
    "\n",
    "# Generate report\n",
    "# generate_comprehensive_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Insights from Solutions\n",
    "\n",
    "### Quantization Trade-offs\n",
    "| Quantization | Speed | Quality | Memory |\n",
    "|--------------|-------|---------|--------|\n",
    "| Q4 | Fastest | Lower | Smallest |\n",
    "| Q5 | Fast | Good | Small |\n",
    "| Q8 | Medium | Very Good | Medium |\n",
    "| FP16 | Slower | Best | Large |\n",
    "\n",
    "### Context Length Impact\n",
    "- **Prefill time** scales roughly linearly with context length\n",
    "- **Decode speed** stays relatively constant\n",
    "- **Memory usage** increases with context (KV cache)\n",
    "\n",
    "### Concurrent Requests\n",
    "- Ollama processes requests serially by default\n",
    "- Multiple requests queue up, increasing latency\n",
    "- For true parallelism, use multiple Ollama instances or vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup resources\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
