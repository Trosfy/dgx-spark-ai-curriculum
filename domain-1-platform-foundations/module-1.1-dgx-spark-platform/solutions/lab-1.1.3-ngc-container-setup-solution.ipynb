{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.1.3: NGC Container Setup - SOLUTIONS\n",
    "\n",
    "This notebook contains solutions to the exercises in the NGC Container Setup notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try It Yourself #1 Solution\n",
    "\n",
    "**Task:** Pull the PyTorch container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Pull the PyTorch NGC container\n",
    "# Run this in terminal for progress visibility:\n",
    "#   docker pull nvcr.io/nvidia/pytorch:25.11-py3\n",
    "\n",
    "# Or run directly (may take 10-30 minutes first time):\n",
    "!docker pull nvcr.io/nvidia/pytorch:25.11-py3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "25.11-py3: Pulling from nvidia/pytorch\n",
    "...\n",
    "Status: Downloaded newer image for nvcr.io/nvidia/pytorch:25.11-py3\n",
    "nvcr.io/nvidia/pytorch:25.11-py3\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- The image is ~20GB+ so initial download takes time\n",
    "- Subsequent runs will use cached layers\n",
    "- Always use specific version tags (e.g., `25.11-py3`) not `latest`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Solution\n",
    "\n",
    "**Task:** Create a custom Dockerfile that extends the NGC PyTorch image with additional packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Custom Dockerfile content\n",
    "dockerfile_content = '''# Custom DGX Spark Development Environment\n",
    "# Extends NGC PyTorch with common AI packages\n",
    "\n",
    "FROM nvcr.io/nvidia/pytorch:25.11-py3\n",
    "\n",
    "# Set environment variables\n",
    "ENV DEBIAN_FRONTEND=noninteractive\n",
    "ENV PYTHONUNBUFFERED=1\n",
    "\n",
    "# Install additional Python packages\n",
    "RUN pip install --no-cache-dir \\\\\n",
    "    transformers>=4.40.0 \\\\\n",
    "    datasets>=2.18.0 \\\\\n",
    "    accelerate>=0.28.0 \\\\\n",
    "    peft>=0.10.0 \\\\\n",
    "    bitsandbytes>=0.43.0 \\\\\n",
    "    wandb>=0.16.0 \\\\\n",
    "    tensorboard>=2.16.0 \\\\\n",
    "    gradio>=4.0.0 \\\\\n",
    "    langchain>=0.1.0 \\\\\n",
    "    chromadb>=0.4.0\n",
    "\n",
    "# Configure Jupyter\n",
    "RUN mkdir -p /root/.jupyter\n",
    "RUN echo \"c.ServerApp.token = ''\" >> /root/.jupyter/jupyter_lab_config.py\n",
    "RUN echo \"c.ServerApp.password = ''\" >> /root/.jupyter/jupyter_lab_config.py\n",
    "RUN echo \"c.ServerApp.allow_root = True\" >> /root/.jupyter/jupyter_lab_config.py\n",
    "\n",
    "# Create startup script\n",
    "COPY startup.sh /startup.sh\n",
    "RUN chmod +x /startup.sh\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /workspace\n",
    "\n",
    "# Default command\n",
    "CMD [\"/startup.sh\"]\n",
    "'''\n",
    "\n",
    "# Save Dockerfile\n",
    "with open('Dockerfile.custom', 'w') as f:\n",
    "    f.write(dockerfile_content)\n",
    "\n",
    "print(\"Dockerfile.custom created!\")\n",
    "print(\"\\nContent preview:\")\n",
    "print(dockerfile_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Startup script that shows GPU status\n",
    "startup_script = '''#!/bin/bash\n",
    "# DGX Spark Custom Container Startup Script\n",
    "\n",
    "echo \"========================================\"\n",
    "echo \"  DGX Spark Development Environment\"\n",
    "echo \"========================================\"\n",
    "echo \"\"\n",
    "\n",
    "# Show GPU info\n",
    "echo \"GPU Status:\"\n",
    "nvidia-smi --query-gpu=name,memory.total,memory.free,temperature.gpu --format=csv\n",
    "echo \"\"\n",
    "\n",
    "# Show PyTorch CUDA status\n",
    "echo \"PyTorch CUDA Check:\"\n",
    "python -c \"import torch; print(f'  CUDA Available: {torch.cuda.is_available()}'); print(f'  Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else None}')\"\n",
    "echo \"\"\n",
    "\n",
    "# Show memory\n",
    "echo \"System Memory:\"\n",
    "free -h | head -2\n",
    "echo \"\"\n",
    "\n",
    "echo \"========================================\"\n",
    "echo \"  Starting Jupyter Lab...\"\n",
    "echo \"========================================\"\n",
    "\n",
    "# Start Jupyter Lab\n",
    "exec jupyter lab --ip=0.0.0.0 --port=8888 --allow-root --no-browser\n",
    "'''\n",
    "\n",
    "# Save startup script\n",
    "with open('startup.sh', 'w') as f:\n",
    "    f.write(startup_script)\n",
    "\n",
    "import os\n",
    "os.chmod('startup.sh', 0o755)\n",
    "\n",
    "print(\"startup.sh created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Build and run commands\n",
    "print(\"To build and run your custom container:\")\n",
    "print(\"=\"*50)\n",
    "print()\n",
    "print(\"# Build the custom image\")\n",
    "print(\"docker build -f Dockerfile.custom -t dgx-spark-custom:latest .\")\n",
    "print()\n",
    "print(\"# Run the custom container\")\n",
    "print(\"docker run --gpus all -it --rm \\\\\")\n",
    "print(\"    -v $HOME/workspace:/workspace \\\\\")\n",
    "print(\"    -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\\\")\n",
    "print(\"    -p 8888:8888 \\\\\")\n",
    "print(\"    --ipc=host \\\\\")\n",
    "print(\"    dgx-spark-custom:latest\")\n",
    "print()\n",
    "print(\"# Alternative: Just run bash\")\n",
    "print(\"docker run --gpus all -it --rm \\\\\")\n",
    "print(\"    -v $HOME/workspace:/workspace \\\\\")\n",
    "print(\"    --ipc=host \\\\\")\n",
    "print(\"    dgx-spark-custom:latest bash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Points:**\n",
    "\n",
    "1. **Always extend NGC base images** - They have the correct CUDA/cuDNN setup for ARM64\n",
    "\n",
    "2. **Use `--no-cache-dir`** for pip to reduce image size\n",
    "\n",
    "3. **Include `--ipc=host`** when running for PyTorch DataLoader compatibility\n",
    "\n",
    "4. **Configure Jupyter security** appropriately for your environment\n",
    "\n",
    "5. **Use version pinning** for reproducible builds (e.g., `transformers>=4.40.0`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **NGC containers are required** - pip install torch won't work on ARM64\n",
    "2. **Always use `--gpus all`** - GPU access must be explicitly enabled\n",
    "3. **Always use `--ipc=host`** - Required for PyTorch multiprocessing\n",
    "4. **Mount cache directories** - Avoid re-downloading models\n",
    "5. **Custom containers extend NGC** - Never try to install CUDA/PyTorch yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup resources and generated files\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Remove generated files if they exist\n",
    "for f in [\"Dockerfile.custom\", \"startup.sh\"]:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "        print(f\"Removed {f}\")\n",
    "\n",
    "gc.collect()\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
