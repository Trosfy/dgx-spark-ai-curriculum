{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.1.5: Ollama Benchmarking\n",
    "\n",
    "**Module:** 1.1 - DGX Spark Platform Mastery  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** â­â­â­ (Intermediate-Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand LLM inference metrics (prefill vs decode)\n",
    "- [ ] Use Ollama's API for accurate benchmarking\n",
    "- [ ] Benchmark multiple model sizes (3B, 8B, 70B)\n",
    "- [ ] Compare your results with NVIDIA published specs\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Prerequisites\n",
    "\n",
    "- Completed: Tasks 1.1-1.4\n",
    "- Ollama installed and running\n",
    "- At least one model pulled (llama3.2:3b recommended to start)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ Real-World Context\n",
    "\n",
    "When choosing an LLM for your application, speed matters:\n",
    "- **Chatbots** need low latency for good user experience\n",
    "- **Batch processing** needs high throughput to be cost-effective\n",
    "- **Edge deployment** has strict memory constraints\n",
    "\n",
    "Understanding how to benchmark helps you make informed decisions about model selection and hardware utilization.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§’ ELI5: LLM Inference Metrics\n",
    "\n",
    "> **Imagine you're reading a question and writing an answer...**\n",
    ">\n",
    "> **Prefill (Prompt Processing)**: This is like reading and understanding the question. You need to read ALL the words before you can start answering. Speed = how fast you can read.\n",
    ">\n",
    "> **Decode (Token Generation)**: This is like writing your answer, one word at a time. Each word depends on what you wrote before. Speed = how fast you can write.\n",
    ">\n",
    "> **Tokens**: Words or pieces of words. \"Hello world\" = 2 tokens. \"Unbelievable\" might be 3 tokens (\"Un\", \"believ\", \"able\").\n",
    ">\n",
    "> **In AI terms:**\n",
    "> - Prefill (tok/s): How fast the model processes your input prompt\n",
    "> - Decode (tok/s): How fast the model generates new tokens (the response)\n",
    ">\n",
    "> Prefill is usually much faster because we can process many tokens in parallel. Decode is slower because each token depends on all previous tokens.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Verify Ollama Setup\n",
    "\n",
    "### Checking Ollama Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Ollama API configuration - update if using a different host/port\n",
    "OLLAMA_BASE_URL: str = \"http://localhost:11434\"\n",
    "\n",
    "def check_ollama_status(base_url: str = OLLAMA_BASE_URL) -> bool:\n",
    "    \"\"\"\n",
    "    Check if Ollama is running and accessible.\n",
    "    \n",
    "    Args:\n",
    "        base_url: Ollama API base URL\n",
    "        \n",
    "    Returns:\n",
    "        True if Ollama is accessible, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{base_url}/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(\"Ollama is running!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Ollama responded with status {response.status_code}\")\n",
    "            return False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Cannot connect to Ollama!\")\n",
    "        print(\"   Start it with: ollama serve\")\n",
    "        print(\"   Or check: systemctl status ollama\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n",
    "\n",
    "check_ollama_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# List available models\ndef list_models() -> List[str]:\n    \"\"\"\n    List models available in Ollama.\n    \n    Returns:\n        List of model names, or empty list if none found or error.\n    \"\"\"\n    try:\n        response = requests.get(f\"{OLLAMA_BASE_URL}/api/tags\", timeout=10)\n        response.raise_for_status()  # Raise exception for bad status codes\n        data = response.json()\n        models = data.get(\"models\", [])\n        \n        if not models:\n            print(\"No models found. Pull some models first:\")\n            print(\"  ollama pull llama3.2:3b\")\n            print(\"  ollama pull llama3.1:8b\")\n            print(\"  ollama pull llama3.1:70b\")\n            return []\n        \n        print(\"Available Models:\")\n        print(\"=\" * 60)\n        print(f\"{'Name':<35} {'Size':<15} {'Modified'}\")\n        print(\"-\" * 60)\n        \n        for model in models:\n            name = model.get(\"name\", \"unknown\")\n            size_bytes = model.get(\"size\", 0)\n            size_gb = size_bytes / (1024**3)\n            modified = model.get(\"modified_at\", \"\")[:10]\n            print(f\"{name:<35} {size_gb:.1f} GB{'':<8} {modified}\")\n        \n        return [m[\"name\"] for m in models]\n        \n    except requests.exceptions.ConnectionError:\n        print(\"âŒ Cannot connect to Ollama!\")\n        print(\"   Start it with: ollama serve\")\n        print(\"   Or check: systemctl status ollama\")\n        return []\n    except requests.exceptions.Timeout:\n        print(\"âŒ Request timed out - Ollama may be busy\")\n        return []\n    except requests.exceptions.RequestException as e:\n        print(f\"âŒ Request error: {e}\")\n        return []\n    except Exception as e:\n        print(f\"âŒ Error listing models: {e}\")\n        return []\n\navailable_models = list_models()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ‹ Try It Yourself #1\n",
    "\n",
    "If you don't have models yet, pull them now. Run in a terminal:\n",
    "\n",
    "```bash\n",
    "# Start with the smallest model\n",
    "ollama pull llama3.2:3b\n",
    "\n",
    "# Medium model (8 billion parameters)\n",
    "ollama pull llama3.1:8b\n",
    "\n",
    "# Large model (70 billion parameters) - only if you have enough storage!\n",
    "ollama pull llama3.1:70b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding the API Response\n",
    "\n",
    "### Key Metrics from Ollama\n",
    "\n",
    "Ollama's `/api/generate` endpoint returns timing information in nanoseconds:\n",
    "\n",
    "| Field | Description |\n",
    "|-------|-------------|\n",
    "| `prompt_eval_count` | Number of tokens in the prompt |\n",
    "| `prompt_eval_duration` | Time to process prompt (nanoseconds) |\n",
    "| `eval_count` | Number of tokens generated |\n",
    "| `eval_duration` | Time to generate response (nanoseconds) |\n",
    "| `total_duration` | Total request time (nanoseconds) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_inference(model: str, prompt: str, max_tokens: int = 50) -> Dict:\n",
    "    \"\"\"\n",
    "    Run a single inference and return raw timing data.\n",
    "    \n",
    "    Args:\n",
    "        model: Model name (e.g., \"llama3.2:3b\")\n",
    "        prompt: Input prompt\n",
    "        max_tokens: Maximum tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with timing metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_BASE_URL}/api/generate\",\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False,\n",
    "                \"options\": {\n",
    "                    \"num_predict\": max_tokens\n",
    "                }\n",
    "            },\n",
    "            timeout=300\n",
    "        )\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            return {\"error\": f\"Status {response.status_code}: {response.text[:100]}\"}\n",
    "        \n",
    "        return response.json()\n",
    "    \n",
    "    except requests.exceptions.Timeout:\n",
    "        return {\"error\": \"Request timed out (300s limit)\"}\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return {\"error\": \"Cannot connect to Ollama - is it running?\"}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\"error\": f\"Request failed: {e}\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Unexpected error: {type(e).__name__}: {e}\"}\n",
    "\n",
    "# Test with a simple prompt\n",
    "if available_models:\n",
    "    test_model = available_models[0]\n",
    "    print(f\"Testing with {test_model}...\\n\")\n",
    "    \n",
    "    result = single_inference(test_model, \"Hello, how are you?\", max_tokens=20)\n",
    "    \n",
    "    if \"error\" not in result:\n",
    "        print(\"Response:\", result.get(\"response\", \"\")[:100])\n",
    "        print(\"\\nRaw timing data:\")\n",
    "        print(f\"  prompt_eval_count:    {result.get('prompt_eval_count', 0)} tokens\")\n",
    "        print(f\"  prompt_eval_duration: {result.get('prompt_eval_duration', 0) / 1e9:.3f} seconds\")\n",
    "        print(f\"  eval_count:           {result.get('eval_count', 0)} tokens\")\n",
    "        print(f\"  eval_duration:        {result.get('eval_duration', 0) / 1e9:.3f} seconds\")\n",
    "        print(f\"  total_duration:       {result.get('total_duration', 0) / 1e9:.3f} seconds\")\n",
    "    else:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "else:\n",
    "    print(\"No models available. Pull a model first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Benchmark Utility Functions\n",
    "\n",
    "> **Note:** A more complete version of these utilities is available in `utils/benchmark_utils.py`.\n",
    "> For production use, you can import: `from utils.benchmark_utils import OllamaBenchmark, BenchmarkResult, quick_benchmark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    \"\"\"Container for benchmark results.\"\"\"\n",
    "    model: str\n",
    "    prompt_tokens: int\n",
    "    generated_tokens: int\n",
    "    prefill_tps: float  # tokens per second (prompt processing)\n",
    "    decode_tps: float   # tokens per second (generation)\n",
    "    total_time_s: float\n",
    "    memory_gb: float = 0.0\n",
    "    \n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"{self.model}: \"\n",
    "            f\"Prefill: {self.prefill_tps:.1f} tok/s, \"\n",
    "            f\"Decode: {self.decode_tps:.1f} tok/s, \"\n",
    "            f\"Memory: {self.memory_gb:.1f} GB\"\n",
    "        )\n",
    "\n",
    "def get_gpu_memory() -> float:\n",
    "    \"\"\"Get GPU memory usage in GB.\"\"\"\n",
    "    import subprocess\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.used\", \"--format=csv,noheader,nounits\"],\n",
    "            capture_output=True, text=True, timeout=5\n",
    "        )\n",
    "        return float(result.stdout.strip()) / 1024  # MiB to GB\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def benchmark_model(\n",
    "    model: str,\n",
    "    prompt: str = \"Explain the concept of machine learning in simple terms.\",\n",
    "    max_tokens: int = 100,\n",
    "    num_runs: int = 3,\n",
    "    warmup: bool = True\n",
    ") -> BenchmarkResult:\n",
    "    \"\"\"\n",
    "    Benchmark a model with multiple runs for accuracy.\n",
    "    \n",
    "    Args:\n",
    "        model: Model name\n",
    "        prompt: Test prompt\n",
    "        max_tokens: Tokens to generate\n",
    "        num_runs: Number of runs to average\n",
    "        warmup: Whether to do warmup run\n",
    "    \n",
    "    Returns:\n",
    "        BenchmarkResult with averaged metrics\n",
    "    \"\"\"\n",
    "    # Warmup run (GPU needs to \"wake up\")\n",
    "    if warmup:\n",
    "        print(f\"  Warming up {model}...\")\n",
    "        single_inference(model, \"Hello\", max_tokens=5)\n",
    "    \n",
    "    # Get memory usage after model is loaded\n",
    "    memory_gb = get_gpu_memory()\n",
    "    \n",
    "    # Collect metrics from multiple runs\n",
    "    prefill_speeds = []\n",
    "    decode_speeds = []\n",
    "    total_times = []\n",
    "    prompt_tokens = 0\n",
    "    gen_tokens = []\n",
    "    \n",
    "    print(f\"  Running {num_runs} benchmark iterations...\")\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        result = single_inference(model, prompt, max_tokens)\n",
    "        \n",
    "        if \"error\" in result:\n",
    "            print(f\"    Run {i+1}: ERROR - {result['error']}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract timing (convert ns to seconds)\n",
    "        prompt_count = result.get(\"prompt_eval_count\", 0)\n",
    "        prompt_duration = result.get(\"prompt_eval_duration\", 0) / 1e9\n",
    "        eval_count = result.get(\"eval_count\", 0)\n",
    "        eval_duration = result.get(\"eval_duration\", 0) / 1e9\n",
    "        total_duration = result.get(\"total_duration\", 0) / 1e9\n",
    "        \n",
    "        # Calculate tokens per second\n",
    "        prefill_tps = prompt_count / prompt_duration if prompt_duration > 0 else 0\n",
    "        decode_tps = eval_count / eval_duration if eval_duration > 0 else 0\n",
    "        \n",
    "        prefill_speeds.append(prefill_tps)\n",
    "        decode_speeds.append(decode_tps)\n",
    "        total_times.append(total_duration)\n",
    "        prompt_tokens = prompt_count\n",
    "        gen_tokens.append(eval_count)\n",
    "        \n",
    "        print(f\"    Run {i+1}: Prefill: {prefill_tps:.1f} tok/s, Decode: {decode_tps:.1f} tok/s\")\n",
    "    \n",
    "    if not prefill_speeds:\n",
    "        return BenchmarkResult(\n",
    "            model=model,\n",
    "            prompt_tokens=0,\n",
    "            generated_tokens=0,\n",
    "            prefill_tps=0,\n",
    "            decode_tps=0,\n",
    "            total_time_s=0,\n",
    "            memory_gb=memory_gb\n",
    "        )\n",
    "    \n",
    "    # Average the results\n",
    "    return BenchmarkResult(\n",
    "        model=model,\n",
    "        prompt_tokens=prompt_tokens,\n",
    "        generated_tokens=int(statistics.mean(gen_tokens)),\n",
    "        prefill_tps=statistics.mean(prefill_speeds),\n",
    "        decode_tps=statistics.mean(decode_speeds),\n",
    "        total_time_s=statistics.mean(total_times),\n",
    "        memory_gb=memory_gb\n",
    "    )\n",
    "\n",
    "print(\"Benchmark functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Run Benchmarks\n",
    "\n",
    "### Single Model Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark a single model\n",
    "if available_models:\n",
    "    test_model = available_models[0]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Benchmarking: {test_model}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    result = benchmark_model(test_model, num_runs=3)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Results:\")\n",
    "    print(f\"  Prompt tokens:     {result.prompt_tokens}\")\n",
    "    print(f\"  Generated tokens:  {result.generated_tokens}\")\n",
    "    print(f\"  Prefill speed:     {result.prefill_tps:.1f} tokens/second\")\n",
    "    print(f\"  Decode speed:      {result.decode_tps:.1f} tokens/second\")\n",
    "    print(f\"  Total time:        {result.total_time_s:.2f} seconds\")\n",
    "    print(f\"  GPU memory:        {result.memory_gb:.1f} GB\")\n",
    "else:\n",
    "    print(\"No models available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Model Benchmark Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark_suite(models: List[str] = None) -> List[BenchmarkResult]:\n",
    "    \"\"\"\n",
    "    Run benchmarks on multiple models.\n",
    "    \n",
    "    Args:\n",
    "        models: List of model names (None = all available)\n",
    "    \n",
    "    Returns:\n",
    "        List of BenchmarkResult objects\n",
    "    \"\"\"\n",
    "    if models is None:\n",
    "        models = available_models\n",
    "    \n",
    "    if not models:\n",
    "        print(\"No models to benchmark!\")\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"                    DGX SPARK BENCHMARK SUITE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nModels to test: {len(models)}\")\n",
    "    print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    for i, model in enumerate(models, 1):\n",
    "        print(f\"\\n[{i}/{len(models)}] Benchmarking: {model}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        result = benchmark_model(model, num_runs=3)\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"  âœ… {result}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run suite on all available models\n",
    "all_results = run_benchmark_suite()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results_table(results: List[BenchmarkResult]):\n",
    "    \"\"\"Print results in a formatted table.\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to display.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"                         BENCHMARK RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Model':<30} {'Prefill (tok/s)':<18} {'Decode (tok/s)':<18} {'Memory (GB)':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for r in results:\n",
    "        print(f\"{r.model:<30} {r.prefill_tps:<18.1f} {r.decode_tps:<18.1f} {r.memory_gb:<12.1f}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "\n",
    "print_results_table(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected results for comparison\n",
    "EXPECTED_RESULTS = {\n",
    "    \"3b\": {\"prefill\": 5000, \"decode\": 80, \"memory\": 3},\n",
    "    \"8b\": {\"prefill\": 3000, \"decode\": 45, \"memory\": 6},\n",
    "    \"70b\": {\"prefill\": 500, \"decode\": 15, \"memory\": 45},\n",
    "}\n",
    "\n",
    "def compare_with_expected(results: List[BenchmarkResult]):\n",
    "    \"\"\"Compare results with expected DGX Spark performance.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"                    COMPARISON WITH EXPECTED RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Note: Expected values are approximate and may vary with quantization.\\n\")\n",
    "    \n",
    "    for r in results:\n",
    "        # Try to match model size\n",
    "        model_lower = r.model.lower()\n",
    "        expected = None\n",
    "        \n",
    "        for size_key, exp in EXPECTED_RESULTS.items():\n",
    "            if size_key in model_lower:\n",
    "                expected = exp\n",
    "                break\n",
    "        \n",
    "        if expected:\n",
    "            print(f\"{r.model}:\")\n",
    "            \n",
    "            # Prefill comparison\n",
    "            prefill_pct = (r.prefill_tps / expected[\"prefill\"]) * 100\n",
    "            prefill_status = \"âœ…\" if prefill_pct >= 80 else \"âš ï¸\"\n",
    "            print(f\"  Prefill: {r.prefill_tps:.0f} tok/s (expected ~{expected['prefill']}) {prefill_status} {prefill_pct:.0f}%\")\n",
    "            \n",
    "            # Decode comparison\n",
    "            decode_pct = (r.decode_tps / expected[\"decode\"]) * 100\n",
    "            decode_status = \"âœ…\" if decode_pct >= 80 else \"âš ï¸\"\n",
    "            print(f\"  Decode:  {r.decode_tps:.0f} tok/s (expected ~{expected['decode']}) {decode_status} {decode_pct:.0f}%\")\n",
    "            \n",
    "            # Memory comparison\n",
    "            print(f\"  Memory:  {r.memory_gb:.1f} GB (expected ~{expected['memory']} GB)\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"{r.model}: No expected results for comparison\")\n",
    "            print()\n",
    "\n",
    "compare_with_expected(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Save Benchmark Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def save_results(results: List[BenchmarkResult], filename: str = None):\n",
    "    \"\"\"Save benchmark results to JSON file.\"\"\"\n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"benchmark_results_{timestamp}.json\"\n",
    "    \n",
    "    data = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"platform\": \"NVIDIA DGX Spark\",\n",
    "        \"results\": [\n",
    "            {\n",
    "                \"model\": r.model,\n",
    "                \"prompt_tokens\": r.prompt_tokens,\n",
    "                \"generated_tokens\": r.generated_tokens,\n",
    "                \"prefill_tokens_per_sec\": r.prefill_tps,\n",
    "                \"decode_tokens_per_sec\": r.decode_tps,\n",
    "                \"total_time_seconds\": r.total_time_s,\n",
    "                \"memory_gb\": r.memory_gb\n",
    "            }\n",
    "            for r in results\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ… Results saved to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "# Save results\n",
    "if all_results:\n",
    "    save_results(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate markdown report\n",
    "def generate_markdown_report(results: List[BenchmarkResult]) -> str:\n",
    "    \"\"\"Generate a markdown report of benchmark results.\"\"\"\n",
    "    lines = [\n",
    "        \"# DGX Spark Ollama Benchmark Results\\n\",\n",
    "        f\"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\",\n",
    "        f\"**Platform:** NVIDIA DGX Spark (128GB Unified Memory)\\n\",\n",
    "        \"\",\n",
    "        \"## Results\\n\",\n",
    "        \"| Model | Prefill (tok/s) | Decode (tok/s) | Memory (GB) |\",\n",
    "        \"|-------|-----------------|----------------|-------------|\"\n",
    "    ]\n",
    "    \n",
    "    for r in results:\n",
    "        lines.append(\n",
    "            f\"| {r.model} | {r.prefill_tps:.1f} | {r.decode_tps:.1f} | {r.memory_gb:.1f} |\"\n",
    "        )\n",
    "    \n",
    "    lines.extend([\n",
    "        \"\",\n",
    "        \"## Notes\\n\",\n",
    "        \"- Prefill: Prompt processing speed (higher is better)\",\n",
    "        \"- Decode: Token generation speed (higher is better)\",\n",
    "        \"- Memory: GPU memory used by the model\",\n",
    "        \"\",\n",
    "        \"## Test Configuration\\n\",\n",
    "        \"- Prompt: 'Explain the concept of machine learning in simple terms.'\",\n",
    "        \"- Max tokens: 100\",\n",
    "        \"- Runs per model: 3 (averaged)\"\n",
    "    ])\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Generate and save report\n",
    "if all_results:\n",
    "    report = generate_markdown_report(all_results)\n",
    "    \n",
    "    with open(\"benchmark_report.md\", \"w\") as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(\"âœ… Report saved to: benchmark_report.md\")\n",
    "    print(\"\\nPreview:\")\n",
    "    print(report[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Different Prompt Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different prompt lengths\n",
    "PROMPTS = {\n",
    "    \"short\": \"Hello!\",\n",
    "    \"medium\": \"Explain the concept of machine learning in simple terms. What are the main types?\",\n",
    "    \"long\": \"\"\"You are an expert AI researcher. Provide a comprehensive analysis of:\n",
    "    1. The evolution of transformer architectures from 2017 to present\n",
    "    2. Key innovations enabling billion-parameter models\n",
    "    3. The role of attention in capturing dependencies\n",
    "    4. Trade-offs between model size and efficiency\n",
    "    5. Future directions for more efficient architectures\n",
    "    Structure your response with clear sections and examples.\"\"\"\n",
    "}\n",
    "\n",
    "def benchmark_prompt_lengths(model: str):\n",
    "    \"\"\"Benchmark a model with different prompt lengths.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt Length Analysis: {model}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, prompt in PROMPTS.items():\n",
    "        print(f\"\\nTesting '{name}' prompt ({len(prompt)} chars)...\")\n",
    "        result = benchmark_model(model, prompt=prompt, num_runs=2, warmup=False)\n",
    "        results[name] = result\n",
    "        print(f\"  Prefill: {result.prefill_tps:.1f} tok/s, Decode: {result.decode_tps:.1f} tok/s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run prompt length analysis on first available model\n",
    "if available_models:\n",
    "    prompt_results = benchmark_prompt_lengths(available_models[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš ï¸ Common Mistakes\n",
    "\n",
    "### Mistake 1: Measuring via web UI or CLI timing\n",
    "\n",
    "```bash\n",
    "# âŒ Wrong - Includes rendering overhead\n",
    "time ollama run llama3.1:8b \"Hello\"\n",
    "\n",
    "# âœ… Right - Use API for accurate timing\n",
    "curl http://localhost:11434/api/generate -d '{\n",
    "  \"model\": \"llama3.1:8b\",\n",
    "  \"prompt\": \"Hello\",\n",
    "  \"stream\": false\n",
    "}'\n",
    "```\n",
    "\n",
    "### Mistake 2: Not doing warmup runs\n",
    "\n",
    "```python\n",
    "# âŒ Wrong - First run may be slow (model loading)\n",
    "result = single_inference(model, prompt)\n",
    "\n",
    "# âœ… Right - Warmup first\n",
    "single_inference(model, \"Hello\", max_tokens=5)  # Warmup\n",
    "result = single_inference(model, prompt)  # Real benchmark\n",
    "```\n",
    "\n",
    "### Mistake 3: Single run measurements\n",
    "\n",
    "```python\n",
    "# âŒ Wrong - High variance\n",
    "result = benchmark(model)  # Single run\n",
    "\n",
    "# âœ… Right - Average multiple runs\n",
    "results = [benchmark(model) for _ in range(3)]\n",
    "avg_speed = statistics.mean(r.decode_tps for r in results)\n",
    "```\n",
    "\n",
    "### Mistake 4: Not clearing buffer cache for large models\n",
    "\n",
    "```bash\n",
    "# âŒ Wrong - May OOM on 70B\n",
    "ollama run llama3.1:70b\n",
    "\n",
    "# âœ… Right - Clear cache first\n",
    "sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'\n",
    "ollama run llama3.1:70b\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- âœ… The difference between prefill and decode metrics\n",
    "- âœ… How to use Ollama's API for accurate benchmarking\n",
    "- âœ… How to run multi-model benchmark suites\n",
    "- âœ… How to compare results with expected performance\n",
    "- âœ… How to save and report benchmark results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Challenge (Optional)\n",
    "\n",
    "Create a benchmark that tests:\n",
    "1. Different quantization levels (Q4, Q5, Q8, FP16)\n",
    "2. Different context lengths (512, 2048, 8192 tokens)\n",
    "3. Batch processing (multiple prompts at once)\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ’¡ Hint</summary>\n",
    "\n",
    "For quantization comparison:\n",
    "```bash\n",
    "ollama pull llama3.1:8b-q4_0\n",
    "ollama pull llama3.1:8b-q8_0\n",
    "```\n",
    "\n",
    "For context length, use the `num_ctx` option:\n",
    "```python\n",
    "response = requests.post(url, json={\n",
    "    \"model\": model,\n",
    "    \"prompt\": long_prompt,\n",
    "    \"options\": {\"num_ctx\": 8192}\n",
    "})\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“– Further Reading\n",
    "\n",
    "- [Ollama API Documentation](https://github.com/ollama/ollama/blob/main/docs/api.md)\n",
    "- [LLM Inference Optimization](https://huggingface.co/docs/transformers/llm_optims)\n",
    "- [NVIDIA DGX Spark Playbooks](https://build.nvidia.com/spark)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Files created in this notebook:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"  - benchmark_results_[timestamp].json\")\n",
    "print(\"  - benchmark_report.md\")\n",
    "print(\"\\nThese are your benchmark records - keep them!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸŽ‰ Congratulations! You've completed Module 1!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nYou now have a fully configured DGX Spark ready for AI development!\")\n",
    "print(\"\\nNext: Module 1.2 - Python for AI/ML\")\n",
    "print(\"You'll learn Python best practices for high-performance AI code.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}