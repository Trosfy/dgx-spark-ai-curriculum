{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.1.5: Ollama Benchmarking\n",
    "\n",
    "**Module:** 1.1 - DGX Spark Platform Mastery  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê (Intermediate-Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand LLM inference metrics (prefill tok/s vs decode tok/s)\n",
    "- [ ] Use Ollama's API for accurate benchmarking\n",
    "- [ ] Benchmark multiple model sizes (3B, 8B, 70B)\n",
    "- [ ] Compare your results with NVIDIA published specs via Ollama Web UI\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Labs 1.1.1-1.1.4\n",
    "- Ollama installed and running (verify via Ollama Web UI at http://localhost:11434)\n",
    "- At least one model pulled (qwen3:4b recommended to start)\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "When choosing an LLM for your application, speed matters:\n",
    "- **Chatbots** need low latency for good user experience\n",
    "- **Batch processing** needs high throughput to be cost-effective\n",
    "- **Edge deployment** has strict memory constraints\n",
    "\n",
    "Understanding how to benchmark helps you make informed decisions about model selection and hardware utilization. You can verify your benchmarks through the Ollama Web UI.\n",
    "\n",
    "---\n",
    "\n",
    "## üßí ELI5: LLM Inference Metrics\n",
    "\n",
    "> **Imagine you're reading a question and writing an answer...**\n",
    ">\n",
    "> **Prefill (Prompt Processing)**: This is like reading and understanding the question. You need to read ALL the words before you can start answering. Speed = how fast you can read.\n",
    ">\n",
    "> **Decode (Token Generation)**: This is like writing your answer, one word at a time. Each word depends on what you wrote before. Speed = how fast you can write.\n",
    ">\n",
    "> **Tokens**: Words or pieces of words. \"Hello world\" = 2 tokens. \"Unbelievable\" might be 3 tokens (\"Un\", \"believ\", \"able\").\n",
    ">\n",
    "> **In AI terms:**\n",
    "> - **Prefill tok/s**: How fast the model processes your input prompt (tokens per second)\n",
    "> - **Decode tok/s**: How fast the model generates new tokens (tokens per second)\n",
    ">\n",
    "> Prefill is usually much faster because we can process many tokens in parallel. Decode is slower because each token depends on all previous tokens.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Verify Ollama Setup\n",
    "\n",
    "### Checking Ollama Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Ollama API configuration - update if using a different host/port\n",
    "OLLAMA_BASE_URL: str = \"http://localhost:11434\"\n",
    "\n",
    "def check_ollama_status(base_url: str = OLLAMA_BASE_URL) -> bool:\n",
    "    \"\"\"\n",
    "    Check if Ollama is running and accessible.\n",
    "    \n",
    "    Args:\n",
    "        base_url: Ollama API base URL\n",
    "        \n",
    "    Returns:\n",
    "        True if Ollama is accessible, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{base_url}/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(\"Ollama is running!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Ollama responded with status {response.status_code}\")\n",
    "            return False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Cannot connect to Ollama!\")\n",
    "        print(\"   Start it with: ollama serve\")\n",
    "        print(\"   Or check: systemctl status ollama\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n",
    "\n",
    "check_ollama_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Models:\n",
      "============================================================\n",
      "Name                                Size            Modified\n",
      "------------------------------------------------------------\n",
      "qwen3:32b                           18.8 GB         2026-01-02\n",
      "qwen3:8b                            4.9 GB         2026-01-02\n",
      "qwen3:4b                            2.3 GB         2025-12-27\n",
      "magistral:24b                       13.3 GB         2025-12-21\n",
      "qwen3-vl:8b                         5.7 GB         2025-12-20\n",
      "rnj-1:8b                            4.8 GB         2025-12-20\n",
      "qwen3-embedding:4b                  2.3 GB         2025-12-20\n",
      "ministral-3:14b                     8.5 GB         2025-12-20\n",
      "gpt-oss:20b                         12.8 GB         2025-12-20\n",
      "devstral-small-2:24b                14.1 GB         2025-12-19\n",
      "devstral-2:123b                     69.8 GB         2025-12-19\n",
      "deepseek-r1:70b                     39.6 GB         2025-12-19\n",
      "nemotron-3-nano:30b                 22.6 GB         2025-12-18\n",
      "deepseek-ocr:3b                     6.2 GB         2025-12-18\n",
      "gpt-oss:120b                        60.9 GB         2025-12-18\n"
     ]
    }
   ],
   "source": [
    "# List available models\n",
    "def list_models() -> List[str]:\n",
    "    \"\"\"\n",
    "    List models available in Ollama.\n",
    "    \n",
    "    Returns:\n",
    "        List of model names, or empty list if none found or error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_BASE_URL}/api/tags\", timeout=10)\n",
    "        response.raise_for_status()  # Raise exception for bad status codes\n",
    "        data = response.json()\n",
    "        models = data.get(\"models\", [])\n",
    "        \n",
    "        if not models:\n",
    "            print(\"No models found. Pull some models first:\")\n",
    "            print(\"  ollama pull qwen3:4b\")\n",
    "            print(\"  ollama pull qwen3:8b\")\n",
    "            print(\"  ollama pull qwen3:32b\")\n",
    "            return []\n",
    "        \n",
    "        print(\"Available Models:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"{'Name':<35} {'Size':<15} {'Modified'}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for model in models:\n",
    "            name = model.get(\"name\", \"unknown\")\n",
    "            size_bytes = model.get(\"size\", 0)\n",
    "            size_gb = size_bytes / (1024**3)\n",
    "            modified = model.get(\"modified_at\", \"\")[:10]\n",
    "            print(f\"{name:<35} {size_gb:.1f} GB{'':<8} {modified}\")\n",
    "        \n",
    "        return [m[\"name\"] for m in models]\n",
    "        \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"‚ùå Cannot connect to Ollama!\")\n",
    "        print(\"   Start it with: ollama serve\")\n",
    "        print(\"   Or check: systemctl status ollama\")\n",
    "        return []\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"‚ùå Request timed out - Ollama may be busy\")\n",
    "        return []\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Request error: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error listing models: {e}\")\n",
    "        return []\n",
    "\n",
    "available_models = list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself #1\n",
    "\n",
    "If you don't have models yet, pull them now. Run in a terminal:\n",
    "\n",
    "```bash\n",
    "# Start with the smallest model\n",
    "ollama pull qwen3:4b\n",
    "\n",
    "# Medium model (8 billion parameters)\n",
    "ollama pull qwen3:8b\n",
    "\n",
    "# Large model (70 billion parameters) - only if you have enough storage!\n",
    "ollama pull qwen3:32b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding the API Response\n",
    "\n",
    "### Key Metrics from Ollama\n",
    "\n",
    "Ollama's `/api/generate` endpoint returns timing information in nanoseconds:\n",
    "\n",
    "| Field | Description | Standard Term |\n",
    "|-------|-------------|---------------|\n",
    "| `prompt_eval_count` | Number of tokens in the prompt | - |\n",
    "| `prompt_eval_duration` | Time to process prompt (ns) | prefill time |\n",
    "| `eval_count` | Number of tokens generated | - |\n",
    "| `eval_duration` | Time to generate response (ns) | decode time |\n",
    "| `total_duration` | Total request time (ns) | - |\n",
    "\n",
    "**Metrics we calculate:**\n",
    "- **Prefill tok/s** = `prompt_eval_count / prompt_eval_duration` (prompt processing speed)\n",
    "- **Decode tok/s** = `eval_count / eval_duration` (token generation speed)\n",
    "\n",
    "> **Note:** These metrics match what you'll see in the Ollama Web UI performance stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with qwen3:32b...\n",
      "\n",
      "Response: \n",
      "\n",
      "Raw timing data:\n",
      "  prompt_eval_count:    16 tokens\n",
      "  prompt_eval_duration: 0.180 seconds\n",
      "  eval_count:           20 tokens\n",
      "  eval_duration:        1.862 seconds\n",
      "  total_duration:       4.888 seconds\n"
     ]
    }
   ],
   "source": [
    "def single_inference(model: str, prompt: str, max_tokens: int = 50) -> Dict:\n",
    "    \"\"\"\n",
    "    Run a single inference and return raw timing data.\n",
    "    \n",
    "    Args:\n",
    "        model: Model name (e.g., \"qwen3:4b\")\n",
    "        prompt: Input prompt\n",
    "        max_tokens: Maximum tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with timing metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_BASE_URL}/api/generate\",\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False,\n",
    "                \"options\": {\n",
    "                    \"num_predict\": max_tokens\n",
    "                }\n",
    "            },\n",
    "            timeout=300\n",
    "        )\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            return {\"error\": f\"Status {response.status_code}: {response.text[:100]}\"}\n",
    "        \n",
    "        return response.json()\n",
    "    \n",
    "    except requests.exceptions.Timeout:\n",
    "        return {\"error\": \"Request timed out (300s limit)\"}\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return {\"error\": \"Cannot connect to Ollama - is it running?\"}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\"error\": f\"Request failed: {e}\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Unexpected error: {type(e).__name__}: {e}\"}\n",
    "\n",
    "# Test with a simple prompt\n",
    "if available_models:\n",
    "    test_model = available_models[0]\n",
    "    print(f\"Testing with {test_model}...\\n\")\n",
    "    \n",
    "    result = single_inference(test_model, \"Hello, how are you?\", max_tokens=20)\n",
    "    \n",
    "    if \"error\" not in result:\n",
    "        print(\"Response:\", result.get(\"response\", \"\")[:100])\n",
    "        print(\"\\nRaw timing data:\")\n",
    "        print(f\"  prompt_eval_count:    {result.get('prompt_eval_count', 0)} tokens\")\n",
    "        print(f\"  prompt_eval_duration: {result.get('prompt_eval_duration', 0) / 1e9:.3f} seconds\")\n",
    "        print(f\"  eval_count:           {result.get('eval_count', 0)} tokens\")\n",
    "        print(f\"  eval_duration:        {result.get('eval_duration', 0) / 1e9:.3f} seconds\")\n",
    "        print(f\"  total_duration:       {result.get('total_duration', 0) / 1e9:.3f} seconds\")\n",
    "    else:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "else:\n",
    "    print(\"No models available. Pull a model first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Benchmark Utility Functions\n",
    "\n",
    "> **Note:** A more complete version of these utilities is available in `utils/benchmarks/`.\n",
    "> For production use, you can import: `from utils.benchmarks import OllamaBenchmark, BenchmarkResult, quick_benchmark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark functions defined!\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    \"\"\"Container for benchmark results.\"\"\"\n",
    "    model: str\n",
    "    prompt_tokens: int\n",
    "    generated_tokens: int\n",
    "    prefill_tps: float  # tokens per second (prompt processing)\n",
    "    decode_tps: float   # tokens per second (generation)\n",
    "    total_time_s: float\n",
    "    memory_gb: float = 0.0\n",
    "    \n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"{self.model}: \"\n",
    "            f\"Prefill: {self.prefill_tps:.1f} tok/s, \"\n",
    "            f\"Decode: {self.decode_tps:.1f} tok/s, \"\n",
    "            f\"Memory: {self.memory_gb:.1f} GB\"\n",
    "        )\n",
    "\n",
    "def get_gpu_memory() -> float:\n",
    "    \"\"\"Get GPU memory usage in GB.\"\"\"\n",
    "    import subprocess\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.used\", \"--format=csv,noheader,nounits\"],\n",
    "            capture_output=True, text=True, timeout=5\n",
    "        )\n",
    "        return float(result.stdout.strip()) / 1024  # MiB to GB\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def benchmark_model(\n",
    "    model: str,\n",
    "    prompt: str = \"Explain the concept of machine learning in simple terms.\",\n",
    "    max_tokens: int = 100,\n",
    "    num_runs: int = 3,\n",
    "    warmup: bool = True\n",
    ") -> BenchmarkResult:\n",
    "    \"\"\"\n",
    "    Benchmark a model with multiple runs for accuracy.\n",
    "    \n",
    "    Args:\n",
    "        model: Model name\n",
    "        prompt: Test prompt\n",
    "        max_tokens: Tokens to generate\n",
    "        num_runs: Number of runs to average\n",
    "        warmup: Whether to do warmup run\n",
    "    \n",
    "    Returns:\n",
    "        BenchmarkResult with averaged metrics\n",
    "    \"\"\"\n",
    "    # Warmup run (GPU needs to \"wake up\")\n",
    "    if warmup:\n",
    "        print(f\"  Warming up {model}...\")\n",
    "        single_inference(model, \"Hello\", max_tokens=5)\n",
    "    \n",
    "    # Get memory usage after model is loaded\n",
    "    memory_gb = get_gpu_memory()\n",
    "    \n",
    "    # Collect metrics from multiple runs\n",
    "    prefill_speeds = []\n",
    "    decode_speeds = []\n",
    "    total_times = []\n",
    "    prompt_tokens = 0\n",
    "    gen_tokens = []\n",
    "    \n",
    "    print(f\"  Running {num_runs} benchmark iterations...\")\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        result = single_inference(model, prompt, max_tokens)\n",
    "        \n",
    "        if \"error\" in result:\n",
    "            print(f\"    Run {i+1}: ERROR - {result['error']}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract timing (convert ns to seconds)\n",
    "        prompt_count = result.get(\"prompt_eval_count\", 0)\n",
    "        prompt_duration = result.get(\"prompt_eval_duration\", 0) / 1e9\n",
    "        eval_count = result.get(\"eval_count\", 0)\n",
    "        eval_duration = result.get(\"eval_duration\", 0) / 1e9\n",
    "        total_duration = result.get(\"total_duration\", 0) / 1e9\n",
    "        \n",
    "        # Calculate tokens per second\n",
    "        prefill_tps = prompt_count / prompt_duration if prompt_duration > 0 else 0\n",
    "        decode_tps = eval_count / eval_duration if eval_duration > 0 else 0\n",
    "        \n",
    "        prefill_speeds.append(prefill_tps)\n",
    "        decode_speeds.append(decode_tps)\n",
    "        total_times.append(total_duration)\n",
    "        prompt_tokens = prompt_count\n",
    "        gen_tokens.append(eval_count)\n",
    "        \n",
    "        print(f\"    Run {i+1}: Prefill: {prefill_tps:.1f} tok/s, Decode: {decode_tps:.1f} tok/s\")\n",
    "    \n",
    "    if not prefill_speeds:\n",
    "        return BenchmarkResult(\n",
    "            model=model,\n",
    "            prompt_tokens=0,\n",
    "            generated_tokens=0,\n",
    "            prefill_tps=0,\n",
    "            decode_tps=0,\n",
    "            total_time_s=0,\n",
    "            memory_gb=memory_gb\n",
    "        )\n",
    "    \n",
    "    # Average the results\n",
    "    return BenchmarkResult(\n",
    "        model=model,\n",
    "        prompt_tokens=prompt_tokens,\n",
    "        generated_tokens=int(statistics.mean(gen_tokens)),\n",
    "        prefill_tps=statistics.mean(prefill_speeds),\n",
    "        decode_tps=statistics.mean(decode_speeds),\n",
    "        total_time_s=statistics.mean(total_times),\n",
    "        memory_gb=memory_gb\n",
    "    )\n",
    "\n",
    "print(\"Benchmark functions defined!\")"
   ]
   "outputs": [],
   "source": "import statistics\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\n@dataclass\nclass BenchmarkResult:\n    \"\"\"Container for benchmark results.\"\"\"\n    model: str\n    prompt_tokens: int\n    generated_tokens: int\n    prefill_tps: float  # tokens per second (prompt processing)\n    decode_tps: float   # tokens per second (generation)\n    total_time_s: float\n    memory_gb: float = 0.0\n    \n    def __str__(self):\n        mem_str = f\"{self.memory_gb:.1f} GB\" if self.memory_gb > 0 else \"N/A (unified)\"\n        return (\n            f\"{self.model}: \"\n            f\"Prefill: {self.prefill_tps:.1f} tok/s, \"\n            f\"Decode: {self.decode_tps:.1f} tok/s, \"\n            f\"Memory: {mem_str}\"\n        )\n\ndef get_ollama_model_memory() -> float:\n    \"\"\"\n    Get memory usage of currently loaded Ollama models.\n    \n    On DGX Spark with unified memory, nvidia-smi returns [N/A] for memory.\n    This function uses Ollama's API to get model memory instead.\n    \n    Returns:\n        Memory in GB, or 0.0 if unavailable\n    \"\"\"\n    try:\n        response = requests.get(f\"{OLLAMA_BASE_URL}/api/ps\", timeout=5)\n        if response.status_code == 200:\n            data = response.json()\n            models = data.get(\"models\", [])\n            if models:\n                # Sum memory of all loaded models (size is in bytes)\n                total_bytes = sum(m.get(\"size\", 0) for m in models)\n                return total_bytes / (1024**3)  # Convert to GB\n        return 0.0\n    except:\n        return 0.0\n\ndef get_system_memory_used() -> float:\n    \"\"\"\n    Get system memory used (fallback for unified memory systems).\n    \n    On DGX Spark, system memory includes GPU allocations.\n    \n    Returns:\n        Used memory in GB\n    \"\"\"\n    import subprocess\n    try:\n        result = subprocess.run(['free', '-b'], capture_output=True, text=True, timeout=5)\n        lines = result.stdout.strip().split('\\n')\n        if len(lines) >= 2:\n            parts = lines[1].split()\n            return int(parts[2]) / (1024**3)  # used column in GB\n        return 0.0\n    except:\n        return 0.0\n\ndef get_gpu_memory() -> float:\n    \"\"\"\n    Get GPU memory usage in GB.\n    \n    Note: On DGX Spark with unified memory, nvidia-smi returns [N/A].\n    This function tries nvidia-smi first, then falls back to Ollama API.\n    \"\"\"\n    import subprocess\n    try:\n        result = subprocess.run(\n            [\"nvidia-smi\", \"--query-gpu=memory.used\", \"--format=csv,noheader,nounits\"],\n            capture_output=True, text=True, timeout=5\n        )\n        output = result.stdout.strip()\n        # Check for [N/A] or empty (unified memory case)\n        if output and output != \"[N/A]\" and output != \"[Not Supported]\":\n            return float(output) / 1024  # MiB to GB\n        else:\n            # Fall back to Ollama model memory on unified memory systems\n            return get_ollama_model_memory()\n    except:\n        return get_ollama_model_memory()\n\ndef benchmark_model(\n    model: str,\n    prompt: str = \"Explain the concept of machine learning in simple terms.\",\n    max_tokens: int = 100,\n    num_runs: int = 3,\n    warmup: bool = True\n) -> BenchmarkResult:\n    \"\"\"\n    Benchmark a model with multiple runs for accuracy.\n    \n    Args:\n        model: Model name\n        prompt: Test prompt\n        max_tokens: Tokens to generate\n        num_runs: Number of runs to average\n        warmup: Whether to do warmup run\n    \n    Returns:\n        BenchmarkResult with averaged metrics\n    \"\"\"\n    # Warmup run (GPU needs to \"wake up\")\n    if warmup:\n        print(f\"  Warming up {model}...\")\n        single_inference(model, \"Hello\", max_tokens=5)\n    \n    # Get memory usage after model is loaded\n    memory_gb = get_gpu_memory()\n    \n    # Collect metrics from multiple runs\n    prefill_speeds = []\n    decode_speeds = []\n    total_times = []\n    prompt_tokens = 0\n    gen_tokens = []\n    \n    print(f\"  Running {num_runs} benchmark iterations...\")\n    \n    for i in range(num_runs):\n        result = single_inference(model, prompt, max_tokens)\n        \n        if \"error\" in result:\n            print(f\"    Run {i+1}: ERROR - {result['error']}\")\n            continue\n        \n        # Extract timing (convert ns to seconds)\n        prompt_count = result.get(\"prompt_eval_count\", 0)\n        prompt_duration = result.get(\"prompt_eval_duration\", 0) / 1e9\n        eval_count = result.get(\"eval_count\", 0)\n        eval_duration = result.get(\"eval_duration\", 0) / 1e9\n        total_duration = result.get(\"total_duration\", 0) / 1e9\n        \n        # Calculate tokens per second\n        prefill_tps = prompt_count / prompt_duration if prompt_duration > 0 else 0\n        decode_tps = eval_count / eval_duration if eval_duration > 0 else 0\n        \n        prefill_speeds.append(prefill_tps)\n        decode_speeds.append(decode_tps)\n        total_times.append(total_duration)\n        prompt_tokens = prompt_count\n        gen_tokens.append(eval_count)\n        \n        print(f\"    Run {i+1}: Prefill: {prefill_tps:.1f} tok/s, Decode: {decode_tps:.1f} tok/s\")\n    \n    if not prefill_speeds:\n        return BenchmarkResult(\n            model=model,\n            prompt_tokens=0,\n            generated_tokens=0,\n            prefill_tps=0,\n            decode_tps=0,\n            total_time_s=0,\n            memory_gb=memory_gb\n        )\n    \n    # Average the results\n    return BenchmarkResult(\n        model=model,\n        prompt_tokens=prompt_tokens,\n        generated_tokens=int(statistics.mean(gen_tokens)),\n        prefill_tps=statistics.mean(prefill_speeds),\n        decode_tps=statistics.mean(decode_speeds),\n        total_time_s=statistics.mean(total_times),\n        memory_gb=memory_gb\n    )\n\nprint(\"Benchmark functions defined!\")\nprint(\"Note: Memory tracking uses Ollama API on unified memory systems.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Run Benchmarks\n",
    "\n",
    "### Single Model Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Benchmarking: qwen3:32b\n",
      "============================================================\n",
      "  Warming up qwen3:32b...\n",
      "  Running 3 benchmark iterations...\n",
      "    Run 1: Prefill: 127.8 tok/s, Decode: 9.9 tok/s\n",
      "    Run 2: Prefill: 191.4 tok/s, Decode: 9.9 tok/s\n",
      "    Run 3: Prefill: 201.4 tok/s, Decode: 9.9 tok/s\n",
      "\n",
      "üìä Results:\n",
      "  Prompt tokens:     21\n",
      "  Generated tokens:  100\n",
      "  Prefill speed:     173.5 tokens/second\n",
      "  Decode speed:      9.9 tokens/second\n",
      "  Total time:        10.34 seconds\n",
      "  GPU memory:        0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Benchmark a single model\n",
    "if available_models:\n",
    "    test_model = available_models[0]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Benchmarking: {test_model}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    result = benchmark_model(test_model, num_runs=3)\n",
    "    \n",
    "    print(f\"\\nüìä Results:\")\n",
    "    print(f\"  Prompt tokens:     {result.prompt_tokens}\")\n",
    "    print(f\"  Generated tokens:  {result.generated_tokens}\")\n",
    "    print(f\"  Prefill speed:     {result.prefill_tps:.1f} tokens/second\")\n",
    "    print(f\"  Decode speed:      {result.decode_tps:.1f} tokens/second\")\n",
    "    print(f\"  Total time:        {result.total_time_s:.2f} seconds\")\n",
    "    print(f\"  GPU memory:        {result.memory_gb:.1f} GB\")\n",
    "else:\n",
    "    print(\"No models available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Model Benchmark Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "                    DGX SPARK BENCHMARK SUITE\n",
      "======================================================================\n",
      "\n",
      "Models to test: 15\n",
      "Timestamp: 2026-01-02 15:48:43\n",
      "\n",
      "[1/15] Benchmarking: qwen3:32b\n",
      "--------------------------------------------------\n",
      "  Warming up qwen3:32b...\n",
      "  Running 3 benchmark iterations...\n",
      "    Run 1: Prefill: 154.6 tok/s, Decode: 9.9 tok/s\n",
      "    Run 2: Prefill: 200.0 tok/s, Decode: 9.9 tok/s\n",
      "    Run 3: Prefill: 193.3 tok/s, Decode: 9.9 tok/s\n",
      "  ‚úÖ qwen3:32b: Prefill: 182.7 tok/s, Decode: 9.9 tok/s, Memory: 0.0 GB\n",
      "\n",
      "[2/15] Benchmarking: qwen3:8b\n",
      "--------------------------------------------------\n",
      "  Warming up qwen3:8b...\n",
      "  Running 3 benchmark iterations...\n",
      "    Run 1: ERROR - Status 500: {\"error\":\"do load request: Post \\\"http://127.0.0.1:38367/load\\\": EOF\"}\n",
      "    Run 2: Prefill: 341.4 tok/s, Decode: 41.9 tok/s\n",
      "    Run 3: Prefill: 802.9 tok/s, Decode: 41.9 tok/s\n",
      "  ‚úÖ qwen3:8b: Prefill: 572.1 tok/s, Decode: 41.9 tok/s, Memory: 0.0 GB\n",
      "\n",
      "[3/15] Benchmarking: qwen3:4b\n",
      "--------------------------------------------------\n",
      "  Warming up qwen3:4b...\n",
      "  Running 3 benchmark iterations...\n",
      "    Run 1: Prefill: 612.6 tok/s, Decode: 75.9 tok/s\n",
      "    Run 2: Prefill: 1447.9 tok/s, Decode: 76.5 tok/s\n",
      "    Run 3: Prefill: 1454.5 tok/s, Decode: 76.6 tok/s\n",
      "  ‚úÖ qwen3:4b: Prefill: 1171.7 tok/s, Decode: 76.3 tok/s, Memory: 0.0 GB\n",
      "\n",
      "[4/15] Benchmarking: magistral:24b\n",
      "--------------------------------------------------\n",
      "  Warming up magistral:24b...\n",
      "  Running 3 benchmark iterations...\n",
      "    Run 1: Prefill: 1745.3 tok/s, Decode: 14.0 tok/s\n",
      "    Run 2: Prefill: 2406.0 tok/s, Decode: 13.9 tok/s\n",
      "    Run 3: Prefill: 2368.3 tok/s, Decode: 13.9 tok/s\n",
      "  ‚úÖ magistral:24b: Prefill: 2173.2 tok/s, Decode: 13.9 tok/s, Memory: 0.0 GB\n",
      "\n",
      "[5/15] Benchmarking: qwen3-vl:8b\n",
      "--------------------------------------------------\n",
      "  Warming up qwen3-vl:8b...\n",
      "  Running 3 benchmark iterations...\n",
      "    Run 1: Prefill: 475.5 tok/s, Decode: 41.9 tok/s\n",
      "    Run 2: Prefill: 835.5 tok/s, Decode: 43.0 tok/s\n",
      "    Run 3: Prefill: 821.7 tok/s, Decode: 43.1 tok/s\n",
      "  ‚úÖ qwen3-vl:8b: Prefill: 710.9 tok/s, Decode: 42.7 tok/s, Memory: 0.0 GB\n",
      "\n",
      "[6/15] Benchmarking: rnj-1:8b\n",
      "--------------------------------------------------\n",
      "  Warming up rnj-1:8b...\n",
      "  Running 3 benchmark iterations...\n",
      "    Run 1: Prefill: 884.7 tok/s, Decode: 36.8 tok/s\n",
      "    Run 2: Prefill: 1471.7 tok/s, Decode: 37.0 tok/s\n",
      "    Run 3: Prefill: 1358.9 tok/s, Decode: 37.2 tok/s\n",
      "  ‚úÖ rnj-1:8b: Prefill: 1238.4 tok/s, Decode: 37.0 tok/s, Memory: 0.0 GB\n",
      "\n",
      "[7/15] Benchmarking: qwen3-embedding:4b\n",
      "--------------------------------------------------\n",
      "  Warming up qwen3-embedding:4b...\n",
      "  Running 3 benchmark iterations...\n",
      "    Run 1: ERROR - Status 400: {\"error\":\"\\\"qwen3-embedding:4b\\\" does not support generate\"}\n",
      "    Run 2: ERROR - Status 400: {\"error\":\"\\\"qwen3-embedding:4b\\\" does not support generate\"}\n",
      "    Run 3: ERROR - Status 400: {\"error\":\"\\\"qwen3-embedding:4b\\\" does not support generate\"}\n",
      "  ‚úÖ qwen3-embedding:4b: Prefill: 0.0 tok/s, Decode: 0.0 tok/s, Memory: 0.0 GB\n",
      "\n",
      "[8/15] Benchmarking: ministral-3:14b\n",
      "--------------------------------------------------\n",
      "  Warming up ministral-3:14b...\n",
      "  Running 3 benchmark iterations...\n",
      "    Run 1: Prefill: 8059.5 tok/s, Decode: 25.4 tok/s\n",
      "    Run 2: Prefill: 13894.0 tok/s, Decode: 25.4 tok/s\n",
      "    Run 3: Prefill: 13826.7 tok/s, Decode: 25.4 tok/s\n",
      "  ‚úÖ ministral-3:14b: Prefill: 11926.7 tok/s, Decode: 25.4 tok/s, Memory: 0.0 GB\n",
      "\n",
      "[9/15] Benchmarking: gpt-oss:20b\n",
      "--------------------------------------------------\n",
      "  Warming up gpt-oss:20b...\n",
      "  Running 3 benchmark iterations...\n",
      "    Run 1: Prefill: 968.6 tok/s, Decode: 58.8 tok/s\n",
      "    Run 2: Prefill: 4223.5 tok/s, Decode: 59.4 tok/s\n",
      "    Run 3: Prefill: 4199.0 tok/s, Decode: 59.2 tok/s\n",
      "  ‚úÖ gpt-oss:20b: Prefill: 3130.4 tok/s, Decode: 59.1 tok/s, Memory: 0.0 GB\n",
      "\n",
      "[10/15] Benchmarking: devstral-small-2:24b\n",
      "--------------------------------------------------\n",
      "  Warming up devstral-small-2:24b...\n",
      "  Running 3 benchmark iterations...\n",
      "    Run 1: Prefill: 4025.9 tok/s, Decode: 13.7 tok/s\n",
      "    Run 2: Prefill: 7413.3 tok/s, Decode: 13.8 tok/s\n",
      "    Run 3: Prefill: 7513.2 tok/s, Decode: 13.7 tok/s\n",
      "  ‚úÖ devstral-small-2:24b: Prefill: 6317.5 tok/s, Decode: 13.7 tok/s, Memory: 0.0 GB\n",
      "\n",
      "[11/15] Benchmarking: devstral-2:123b\n",
      "--------------------------------------------------\n",
      "  Warming up devstral-2:123b...\n",
      "  Running 3 benchmark iterations...\n",
      "    Run 1: Prefill: 1120.0 tok/s, Decode: 2.6 tok/s\n",
      "    Run 2: Prefill: 1332.8 tok/s, Decode: 2.6 tok/s\n",
      "    Run 3: Prefill: 1334.9 tok/s, Decode: 2.6 tok/s\n",
      "  ‚úÖ devstral-2:123b: Prefill: 1262.6 tok/s, Decode: 2.6 tok/s, Memory: 0.0 GB\n",
      "\n",
      "[12/15] Benchmarking: deepseek-r1:70b\n",
      "--------------------------------------------------\n",
      "  Warming up deepseek-r1:70b...\n",
      "  Running 3 benchmark iterations...\n",
      "    Run 1: Prefill: 45.8 tok/s, Decode: 4.5 tok/s\n",
      "    Run 2: Prefill: 63.4 tok/s, Decode: 4.6 tok/s\n",
      "    Run 3: Prefill: 61.9 tok/s, Decode: 4.6 tok/s\n",
      "  ‚úÖ deepseek-r1:70b: Prefill: 57.0 tok/s, Decode: 4.5 tok/s, Memory: 0.0 GB\n",
      "\n",
      "[13/15] Benchmarking: nemotron-3-nano:30b\n",
      "--------------------------------------------------\n",
      "  Warming up nemotron-3-nano:30b...\n",
      "  Running 3 benchmark iterations...\n",
      "    Run 1: Prefill: 340.0 tok/s, Decode: 62.5 tok/s\n",
      "    Run 2: Prefill: 328.1 tok/s, Decode: 63.0 tok/s\n",
      "    Run 3: Prefill: 346.3 tok/s, Decode: 62.8 tok/s\n",
      "  ‚úÖ nemotron-3-nano:30b: Prefill: 338.1 tok/s, Decode: 62.8 tok/s, Memory: 0.0 GB\n",
      "\n",
      "[14/15] Benchmarking: deepseek-ocr:3b\n",
      "--------------------------------------------------\n",
      "  Warming up deepseek-ocr:3b...\n",
      "  Running 3 benchmark iterations...\n",
      "    Run 1: Prefill: 218.4 tok/s, Decode: 175.1 tok/s\n",
      "    Run 2: Prefill: 1702.7 tok/s, Decode: 176.4 tok/s\n",
      "    Run 3: Prefill: 1786.6 tok/s, Decode: 176.2 tok/s\n",
      "  ‚úÖ deepseek-ocr:3b: Prefill: 1235.9 tok/s, Decode: 175.9 tok/s, Memory: 0.0 GB\n",
      "\n",
      "[15/15] Benchmarking: gpt-oss:120b\n",
      "--------------------------------------------------\n",
      "  Warming up gpt-oss:120b...\n",
      "  Running 3 benchmark iterations...\n",
      "    Run 1: Prefill: 597.1 tok/s, Decode: 42.4 tok/s\n",
      "    Run 2: Prefill: 2958.6 tok/s, Decode: 42.6 tok/s\n",
      "    Run 3: Prefill: 3082.2 tok/s, Decode: 42.5 tok/s\n",
      "  ‚úÖ gpt-oss:120b: Prefill: 2212.6 tok/s, Decode: 42.5 tok/s, Memory: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "def run_benchmark_suite(models: List[str] = None) -> List[BenchmarkResult]:\n",
    "    \"\"\"\n",
    "    Run benchmarks on multiple models.\n",
    "    \n",
    "    Args:\n",
    "        models: List of model names (None = all available)\n",
    "    \n",
    "    Returns:\n",
    "        List of BenchmarkResult objects\n",
    "    \"\"\"\n",
    "    if models is None:\n",
    "        models = available_models\n",
    "    \n",
    "    if not models:\n",
    "        print(\"No models to benchmark!\")\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"                    DGX SPARK BENCHMARK SUITE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nModels to test: {len(models)}\")\n",
    "    print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    for i, model in enumerate(models, 1):\n",
    "        print(f\"\\n[{i}/{len(models)}] Benchmarking: {model}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        result = benchmark_model(model, num_runs=3)\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"  ‚úÖ {result}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run suite on all available models\n",
    "all_results = run_benchmark_suite()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                         BENCHMARK RESULTS\n",
      "================================================================================\n",
      "Model                          Prefill (tok/s)    Decode (tok/s)     Memory (GB) \n",
      "--------------------------------------------------------------------------------\n",
      "qwen3:32b                      182.7              9.9                0.0         \n",
      "qwen3:8b                       572.1              41.9               0.0         \n",
      "qwen3:4b                       1171.7             76.3               0.0         \n",
      "magistral:24b                  2173.2             13.9               0.0         \n",
      "qwen3-vl:8b                    710.9              42.7               0.0         \n",
      "rnj-1:8b                       1238.4             37.0               0.0         \n",
      "qwen3-embedding:4b             0.0                0.0                0.0         \n",
      "ministral-3:14b                11926.7            25.4               0.0         \n",
      "gpt-oss:20b                    3130.4             59.1               0.0         \n",
      "devstral-small-2:24b           6317.5             13.7               0.0         \n",
      "devstral-2:123b                1262.6             2.6                0.0         \n",
      "deepseek-r1:70b                57.0               4.5                0.0         \n",
      "nemotron-3-nano:30b            338.1              62.8               0.0         \n",
      "deepseek-ocr:3b                1235.9             175.9              0.0         \n",
      "gpt-oss:120b                   2212.6             42.5               0.0         \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def print_results_table(results: List[BenchmarkResult]):\n",
    "    \"\"\"Print results in a formatted table.\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to display.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"                         BENCHMARK RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Model':<30} {'Prefill (tok/s)':<18} {'Decode (tok/s)':<18} {'Memory (GB)':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for r in results:\n",
    "        print(f\"{r.model:<30} {r.prefill_tps:<18.1f} {r.decode_tps:<18.1f} {r.memory_gb:<12.1f}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "\n",
    "print_results_table(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                    COMPARISON WITH EXPECTED RESULTS\n",
      "================================================================================\n",
      "Note: Expected values are approximate and may vary with quantization.\n",
      "\n",
      "qwen3:32b: No expected results for comparison\n",
      "\n",
      "qwen3:8b:\n",
      "  Prefill: 572 tok/s (expected ~3000) ‚ö†Ô∏è 19%\n",
      "  Decode:  42 tok/s (expected ~45) ‚úÖ 93%\n",
      "  Memory:  0.0 GB (expected ~6 GB)\n",
      "\n",
      "qwen3:4b: No expected results for comparison\n",
      "\n",
      "magistral:24b: No expected results for comparison\n",
      "\n",
      "qwen3-vl:8b:\n",
      "  Prefill: 711 tok/s (expected ~3000) ‚ö†Ô∏è 24%\n",
      "  Decode:  43 tok/s (expected ~45) ‚úÖ 95%\n",
      "  Memory:  0.0 GB (expected ~6 GB)\n",
      "\n",
      "rnj-1:8b:\n",
      "  Prefill: 1238 tok/s (expected ~3000) ‚ö†Ô∏è 41%\n",
      "  Decode:  37 tok/s (expected ~45) ‚úÖ 82%\n",
      "  Memory:  0.0 GB (expected ~6 GB)\n",
      "\n",
      "qwen3-embedding:4b: No expected results for comparison\n",
      "\n",
      "ministral-3:14b: No expected results for comparison\n",
      "\n",
      "gpt-oss:20b: No expected results for comparison\n",
      "\n",
      "devstral-small-2:24b: No expected results for comparison\n",
      "\n",
      "devstral-2:123b:\n",
      "  Prefill: 1263 tok/s (expected ~5000) ‚ö†Ô∏è 25%\n",
      "  Decode:  3 tok/s (expected ~80) ‚ö†Ô∏è 3%\n",
      "  Memory:  0.0 GB (expected ~3 GB)\n",
      "\n",
      "deepseek-r1:70b:\n",
      "  Prefill: 57 tok/s (expected ~500) ‚ö†Ô∏è 11%\n",
      "  Decode:  5 tok/s (expected ~15) ‚ö†Ô∏è 30%\n",
      "  Memory:  0.0 GB (expected ~45 GB)\n",
      "\n",
      "nemotron-3-nano:30b: No expected results for comparison\n",
      "\n",
      "deepseek-ocr:3b:\n",
      "  Prefill: 1236 tok/s (expected ~5000) ‚ö†Ô∏è 25%\n",
      "  Decode:  176 tok/s (expected ~80) ‚úÖ 220%\n",
      "  Memory:  0.0 GB (expected ~3 GB)\n",
      "\n",
      "gpt-oss:120b: No expected results for comparison\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Expected results for comparison\n",
    "EXPECTED_RESULTS = {\n",
    "    \"3b\": {\"prefill\": 5000, \"decode\": 80, \"memory\": 3},\n",
    "    \"8b\": {\"prefill\": 3000, \"decode\": 45, \"memory\": 6},\n",
    "    \"70b\": {\"prefill\": 500, \"decode\": 15, \"memory\": 45},\n",
    "}\n",
    "\n",
    "def compare_with_expected(results: List[BenchmarkResult]):\n",
    "    \"\"\"Compare results with expected DGX Spark performance.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"                    COMPARISON WITH EXPECTED RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Note: Expected values are approximate and may vary with quantization.\\n\")\n",
    "    \n",
    "    for r in results:\n",
    "        # Try to match model size\n",
    "        model_lower = r.model.lower()\n",
    "        expected = None\n",
    "        \n",
    "        for size_key, exp in EXPECTED_RESULTS.items():\n",
    "            if size_key in model_lower:\n",
    "                expected = exp\n",
    "                break\n",
    "        \n",
    "        if expected:\n",
    "            print(f\"{r.model}:\")\n",
    "            \n",
    "            # Prefill comparison\n",
    "            prefill_pct = (r.prefill_tps / expected[\"prefill\"]) * 100\n",
    "            prefill_status = \"‚úÖ\" if prefill_pct >= 80 else \"‚ö†Ô∏è\"\n",
    "            print(f\"  Prefill: {r.prefill_tps:.0f} tok/s (expected ~{expected['prefill']}) {prefill_status} {prefill_pct:.0f}%\")\n",
    "            \n",
    "            # Decode comparison\n",
    "            decode_pct = (r.decode_tps / expected[\"decode\"]) * 100\n",
    "            decode_status = \"‚úÖ\" if decode_pct >= 80 else \"‚ö†Ô∏è\"\n",
    "            print(f\"  Decode:  {r.decode_tps:.0f} tok/s (expected ~{expected['decode']}) {decode_status} {decode_pct:.0f}%\")\n",
    "            \n",
    "            # Memory comparison\n",
    "            print(f\"  Memory:  {r.memory_gb:.1f} GB (expected ~{expected['memory']} GB)\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"{r.model}: No expected results for comparison\")\n",
    "            print()\n",
    "\n",
    "compare_with_expected(all_results)"
   ]
   "outputs": [],
   "source": "# Expected results for comparison - now includes architecture awareness\n# Different model architectures have different performance characteristics\n\n# Model architecture detection patterns\nMODEL_ARCHITECTURE = {\n    # MoE (Mixture of Experts) - high prefill, lower decode due to expert routing\n    \"moe\": [\"devstral\", \"magistral\", \"mixtral\", \"gpt-oss\", \"dbrx\", \"arctic\"],\n    # Reasoning models - much slower due to long chain-of-thought\n    \"reasoning\": [\"deepseek-r1\", \"qwq\", \"o1\", \"reasoning\"],\n    # Dense models - standard performance\n    \"dense\": [\"qwen\", \"llama\", \"gemma\", \"phi\", \"mistral\", \"nemotron\", \"yi\"]\n}\n\n# Expected performance by architecture and size\nEXPECTED_RESULTS = {\n    # Dense models (standard transformers)\n    \"dense\": {\n        \"3b\":  {\"prefill\": 5000, \"decode\": 80,  \"memory\": 3},\n        \"4b\":  {\"prefill\": 4500, \"decode\": 70,  \"memory\": 4},\n        \"7b\":  {\"prefill\": 4000, \"decode\": 55,  \"memory\": 6},\n        \"8b\":  {\"prefill\": 3000, \"decode\": 45,  \"memory\": 6},\n        \"14b\": {\"prefill\": 2000, \"decode\": 35,  \"memory\": 10},\n        \"32b\": {\"prefill\": 1000, \"decode\": 25,  \"memory\": 20},\n        \"70b\": {\"prefill\": 500,  \"decode\": 15,  \"memory\": 45},\n    },\n    # MoE models (Mixture of Experts) - higher prefill, lower decode\n    \"moe\": {\n        \"8b\":   {\"prefill\": 3500, \"decode\": 35, \"memory\": 5},\n        \"22b\":  {\"prefill\": 2500, \"decode\": 25, \"memory\": 14},\n        \"47b\":  {\"prefill\": 2000, \"decode\": 18, \"memory\": 28},\n        \"120b\": {\"prefill\": 2000, \"decode\": 40, \"memory\": 65},\n        \"123b\": {\"prefill\": 1200, \"decode\": 3,  \"memory\": 70},\n    },\n    # Reasoning models - much slower (long chain-of-thought)\n    \"reasoning\": {\n        \"7b\":  {\"prefill\": 200, \"decode\": 20, \"memory\": 6},\n        \"14b\": {\"prefill\": 150, \"decode\": 15, \"memory\": 10},\n        \"32b\": {\"prefill\": 100, \"decode\": 8,  \"memory\": 20},\n        \"70b\": {\"prefill\": 60,  \"decode\": 5,  \"memory\": 45},\n    }\n}\n\ndef detect_architecture(model_name: str) -> str:\n    \"\"\"Detect model architecture from name.\"\"\"\n    model_lower = model_name.lower()\n    for arch, patterns in MODEL_ARCHITECTURE.items():\n        for pattern in patterns:\n            if pattern in model_lower:\n                return arch\n    return \"dense\"  # Default to dense\n\ndef extract_size(model_name: str) -> str:\n    \"\"\"Extract model size from name (e.g., 'qwen3:8b' -> '8b').\"\"\"\n    import re\n    # Try to find patterns like :8b, -8b, _8b, or just 8b\n    match = re.search(r'[:\\-_]?(\\d+)b', model_name.lower())\n    if match:\n        return f\"{match.group(1)}b\"\n    return None\n\ndef compare_with_expected(results: List[BenchmarkResult]):\n    \"\"\"Compare results with expected DGX Spark performance (architecture-aware).\"\"\"\n    print(\"\\n\" + \"=\" * 85)\n    print(\"                    COMPARISON WITH EXPECTED RESULTS\")\n    print(\"=\" * 85)\n    \n    # Architecture legend\n    print(\"\\nüìä Model Architecture Guide:\")\n    print(\"‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n    print(\"‚îÇ Dense      ‚îÇ Standard transformers (Qwen, Llama, Gemma) - balanced    ‚îÇ\")\n    print(\"‚îÇ MoE        ‚îÇ Mixture of Experts (Mixtral, Devstral) - high prefill    ‚îÇ\")\n    print(\"‚îÇ Reasoning  ‚îÇ Chain-of-thought (DeepSeek-R1, QwQ) - slower but deeper  ‚îÇ\")\n    print(\"‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\\n\")\n    \n    for r in results:\n        # Detect architecture and size\n        arch = detect_architecture(r.model)\n        size = extract_size(r.model)\n        \n        print(f\"üì¶ {r.model}\")\n        print(f\"   Architecture: {arch.upper()}\")\n        \n        if size and size in EXPECTED_RESULTS.get(arch, {}):\n            expected = EXPECTED_RESULTS[arch][size]\n            \n            # Prefill comparison\n            prefill_pct = (r.prefill_tps / expected[\"prefill\"]) * 100\n            prefill_status = \"‚úÖ\" if prefill_pct >= 70 else \"‚ö†Ô∏è\" if prefill_pct >= 50 else \"‚ùå\"\n            print(f\"   Prefill: {r.prefill_tps:>7.0f} tok/s (expected ~{expected['prefill']:>5}) {prefill_status} {prefill_pct:>5.0f}%\")\n            \n            # Decode comparison  \n            decode_pct = (r.decode_tps / expected[\"decode\"]) * 100\n            decode_status = \"‚úÖ\" if decode_pct >= 70 else \"‚ö†Ô∏è\" if decode_pct >= 50 else \"‚ùå\"\n            print(f\"   Decode:  {r.decode_tps:>7.1f} tok/s (expected ~{expected['decode']:>5}) {decode_status} {decode_pct:>5.0f}%\")\n            \n            # Memory info\n            if r.memory_gb > 0:\n                print(f\"   Memory:  {r.memory_gb:>7.1f} GB   (expected ~{expected['memory']:>5} GB)\")\n            else:\n                print(f\"   Memory:  N/A (unified memory - use 'ollama ps' or 'free -h')\")\n        else:\n            print(f\"   Prefill: {r.prefill_tps:>7.0f} tok/s\")\n            print(f\"   Decode:  {r.decode_tps:>7.1f} tok/s\")\n            if r.memory_gb > 0:\n                print(f\"   Memory:  {r.memory_gb:>7.1f} GB\")\n            print(f\"   ‚ÑπÔ∏è  No baseline for {arch}/{size} - results recorded for reference\")\n        print()\n\ncompare_with_expected(all_results)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Save Benchmark Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Results saved to: benchmark_results_20260102_161056.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def save_results(results: List[BenchmarkResult], filename: str = None):\n",
    "    \"\"\"Save benchmark results to JSON file.\"\"\"\n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"benchmark_results_{timestamp}.json\"\n",
    "    \n",
    "    data = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"platform\": \"NVIDIA DGX Spark\",\n",
    "        \"results\": [\n",
    "            {\n",
    "                \"model\": r.model,\n",
    "                \"prompt_tokens\": r.prompt_tokens,\n",
    "                \"generated_tokens\": r.generated_tokens,\n",
    "                \"prefill_tokens_per_sec\": r.prefill_tps,\n",
    "                \"decode_tokens_per_sec\": r.decode_tps,\n",
    "                \"total_time_seconds\": r.total_time_s,\n",
    "                \"memory_gb\": r.memory_gb\n",
    "            }\n",
    "            for r in results\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Results saved to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "# Save results\n",
    "if all_results:\n",
    "    save_results(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Report saved to: benchmark_report.md\n",
      "\n",
      "Preview:\n",
      "# DGX Spark Ollama Benchmark Results\n",
      "\n",
      "**Date:** 2026-01-02 16:10:59\n",
      "\n",
      "**Platform:** NVIDIA DGX Spark (128GB Unified Memory)\n",
      "\n",
      "\n",
      "## Results\n",
      "\n",
      "| Model | Prefill (tok/s) | Decode (tok/s) | Memory (GB) |\n",
      "|-------|-----------------|----------------|-------------|\n",
      "| qwen3:32b | 182.7 | 9.9 | 0.0 |\n",
      "| qwen3:8b | 572.1 | 41.9 | 0.0 |\n",
      "| qwen3:4b | 1171.7 | 76.3 | 0.0 |\n",
      "| magistral:24b | 2173.2 | 13.9 | 0.0 |\n",
      "| qwen3-vl:8b | 710.9 | 42.7 | 0.0 |\n",
      "| rnj-1:8b | 1238.4 | 37.0 | 0.0 |\n",
      "| qwen3-embedding:4b | 0.0 | 0\n"
     ]
    }
   ],
   "source": [
    "# Generate markdown report\n",
    "def generate_markdown_report(results: List[BenchmarkResult]) -> str:\n",
    "    \"\"\"Generate a markdown report of benchmark results.\"\"\"\n",
    "    lines = [\n",
    "        \"# DGX Spark Ollama Benchmark Results\\n\",\n",
    "        f\"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\",\n",
    "        f\"**Platform:** NVIDIA DGX Spark (128GB Unified Memory)\\n\",\n",
    "        \"\",\n",
    "        \"## Results\\n\",\n",
    "        \"| Model | Prefill (tok/s) | Decode (tok/s) | Memory (GB) |\",\n",
    "        \"|-------|-----------------|----------------|-------------|\"\n",
    "    ]\n",
    "    \n",
    "    for r in results:\n",
    "        lines.append(\n",
    "            f\"| {r.model} | {r.prefill_tps:.1f} | {r.decode_tps:.1f} | {r.memory_gb:.1f} |\"\n",
    "        )\n",
    "    \n",
    "    lines.extend([\n",
    "        \"\",\n",
    "        \"## Notes\\n\",\n",
    "        \"- Prefill: Prompt processing speed (higher is better)\",\n",
    "        \"- Decode: Token generation speed (higher is better)\",\n",
    "        \"- Memory: GPU memory used by the model\",\n",
    "        \"\",\n",
    "        \"## Test Configuration\\n\",\n",
    "        \"- Prompt: 'Explain the concept of machine learning in simple terms.'\",\n",
    "        \"- Max tokens: 100\",\n",
    "        \"- Runs per model: 3 (averaged)\"\n",
    "    ])\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Generate and save report\n",
    "if all_results:\n",
    "    report = generate_markdown_report(all_results)\n",
    "    \n",
    "    with open(\"benchmark_report.md\", \"w\") as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(\"‚úÖ Report saved to: benchmark_report.md\")\n",
    "    print(\"\\nPreview:\")\n",
    "    print(report[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Different Prompt Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Prompt Length Analysis: qwen3:32b\n",
      "============================================================\n",
      "\n",
      "Testing 'short' prompt (6 chars)...\n",
      "  Running 2 benchmark iterations...\n",
      "    Run 1: Prefill: 6.2 tok/s, Decode: 9.9 tok/s\n",
      "    Run 2: Prefill: 114.0 tok/s, Decode: 9.9 tok/s\n",
      "  Prefill: 60.1 tok/s, Decode: 9.9 tok/s\n",
      "\n",
      "Testing 'medium' prompt (81 chars)...\n",
      "  Running 2 benchmark iterations...\n",
      "    Run 1: Prefill: 150.6 tok/s, Decode: 9.9 tok/s\n",
      "    Run 2: Prefill: 261.5 tok/s, Decode: 9.9 tok/s\n",
      "  Prefill: 206.0 tok/s, Decode: 9.9 tok/s\n",
      "\n",
      "Testing 'long' prompt (424 chars)...\n",
      "  Running 2 benchmark iterations...\n",
      "    Run 1: Prefill: 552.6 tok/s, Decode: 9.9 tok/s\n",
      "    Run 2: Prefill: 917.6 tok/s, Decode: 9.9 tok/s\n",
      "  Prefill: 735.1 tok/s, Decode: 9.9 tok/s\n"
     ]
    }
   ],
   "source": [
    "# Test with different prompt lengths\n",
    "PROMPTS = {\n",
    "    \"short\": \"Hello!\",\n",
    "    \"medium\": \"Explain the concept of machine learning in simple terms. What are the main types?\",\n",
    "    \"long\": \"\"\"You are an expert AI researcher. Provide a comprehensive analysis of:\n",
    "    1. The evolution of transformer architectures from 2017 to present\n",
    "    2. Key innovations enabling billion-parameter models\n",
    "    3. The role of attention in capturing dependencies\n",
    "    4. Trade-offs between model size and efficiency\n",
    "    5. Future directions for more efficient architectures\n",
    "    Structure your response with clear sections and examples.\"\"\"\n",
    "}\n",
    "\n",
    "def benchmark_prompt_lengths(model: str):\n",
    "    \"\"\"Benchmark a model with different prompt lengths.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt Length Analysis: {model}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, prompt in PROMPTS.items():\n",
    "        print(f\"\\nTesting '{name}' prompt ({len(prompt)} chars)...\")\n",
    "        result = benchmark_model(model, prompt=prompt, num_runs=2, warmup=False)\n",
    "        results[name] = result\n",
    "        print(f\"  Prefill: {result.prefill_tps:.1f} tok/s, Decode: {result.decode_tps:.1f} tok/s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run prompt length analysis on first available model\n",
    "if available_models:\n",
    "    prompt_results = benchmark_prompt_lengths(available_models[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Measuring via web UI or CLI timing\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong - Includes rendering overhead\n",
    "time ollama run qwen3:8b \"Hello\"\n",
    "\n",
    "# ‚úÖ Right - Use API for accurate timing\n",
    "curl http://localhost:11434/api/generate -d '{\n",
    "  \"model\": \"qwen3:8b\",\n",
    "  \"prompt\": \"Hello\",\n",
    "  \"stream\": false\n",
    "}'\n",
    "```\n",
    "\n",
    "### Mistake 2: Not doing warmup runs\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - First run may be slow (model loading)\n",
    "result = single_inference(model, prompt)\n",
    "\n",
    "# ‚úÖ Right - Warmup first\n",
    "single_inference(model, \"Hello\", max_tokens=5)  # Warmup\n",
    "result = single_inference(model, prompt)  # Real benchmark\n",
    "```\n",
    "\n",
    "### Mistake 3: Single run measurements\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - High variance\n",
    "result = benchmark(model)  # Single run\n",
    "\n",
    "# ‚úÖ Right - Average multiple runs\n",
    "results = [benchmark(model) for _ in range(3)]\n",
    "avg_speed = statistics.mean(r.decode_tps for r in results)\n",
    "```\n",
    "\n",
    "### Mistake 4: Not clearing buffer cache for large models\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong - May OOM on 70B\n",
    "ollama run qwen3:32b\n",
    "\n",
    "# ‚úÖ Right - Clear cache first\n",
    "sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'\n",
    "ollama run qwen3:32b\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<cell_type>markdown</cell_type>## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ The difference between prefill tok/s and decode tok/s metrics\n",
    "- ‚úÖ How to use Ollama's API for accurate benchmarking\n",
    "- ‚úÖ How to run multi-model benchmark suites\n",
    "- ‚úÖ How to compare results with expected performance via Ollama Web UI\n",
    "- ‚úÖ How to save and report benchmark results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "Create a benchmark that tests:\n",
    "1. Different quantization levels (Q4, Q5, Q8, FP16)\n",
    "2. Different context lengths (512, 2048, 8192 tokens)\n",
    "3. Batch processing (multiple prompts at once)\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "For quantization comparison:\n",
    "```bash\n",
    "ollama pull qwen3:8b-q4_0\n",
    "ollama pull qwen3:8b-q8_0\n",
    "```\n",
    "\n",
    "For context length, use the `num_ctx` option:\n",
    "```python\n",
    "response = requests.post(url, json={\n",
    "    \"model\": model,\n",
    "    \"prompt\": long_prompt,\n",
    "    \"options\": {\"num_ctx\": 8192}\n",
    "})\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [Ollama API Documentation](https://github.com/ollama/ollama/blob/main/docs/api.md)\n",
    "- [LLM Inference Optimization](https://huggingface.co/docs/transformers/llm_optims)\n",
    "- [NVIDIA DGX Spark Playbooks](https://build.nvidia.com/spark)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files created in this notebook:\n",
      "----------------------------------------\n",
      "  - benchmark_results_[timestamp].json\n",
      "  - benchmark_report.md\n",
      "\n",
      "These are your benchmark records - keep them!\n",
      "\n",
      "============================================================\n",
      "üéâ Congratulations! You've completed Module 1!\n",
      "============================================================\n",
      "\n",
      "You now have a fully configured DGX Spark ready for AI development!\n",
      "\n",
      "Next: Module 1.2 - Python for AI/ML\n",
      "You'll learn Python best practices for high-performance AI code.\n"
     ]
    }
   ],
   "source": [
    "print(\"Files created in this notebook:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"  - benchmark_results_[timestamp].json\")\n",
    "print(\"  - benchmark_report.md\")\n",
    "print(\"\\nThese are your benchmark records - keep them!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ Congratulations! You've completed Module 1!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nYou now have a fully configured DGX Spark ready for AI development!\")\n",
    "print(\"\\nNext: Module 1.2 - Python for AI/ML\")\n",
    "print(\"You'll learn Python best practices for high-performance AI code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
