{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.1.1: System Exploration\n",
    "\n",
    "**Module:** 1.1 - DGX Spark Platform Mastery  \n",
    "**Time:** 1 hour  \n",
    "**Difficulty:** ‚≠ê (Beginner)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand the DGX Spark hardware architecture\n",
    "- [ ] Use system commands to explore your hardware\n",
    "- [ ] Document your system's capabilities\n",
    "- [ ] Know the key specifications that matter for AI workloads\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Basic command line knowledge\n",
    "- Access to a DGX Spark system\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "Before you can train AI models or run inference, you need to understand your hardware. Just like a chef needs to know their kitchen before cooking a feast, an AI engineer needs to know their compute environment.\n",
    "\n",
    "The DGX Spark is NVIDIA's first desktop AI supercomputer, designed specifically for AI developers. Understanding its unique architecture will help you:\n",
    "- Choose the right model sizes for your hardware\n",
    "- Optimize memory usage for maximum performance\n",
    "- Troubleshoot issues when things don't work as expected\n",
    "\n",
    "---\n",
    "\n",
    "## üßí ELI5: What is the DGX Spark?\n",
    "\n",
    "> **Imagine you have a super-powered gaming computer...**\n",
    ">\n",
    "> But instead of being good at video games, it's amazing at doing math - billions of calculations per second! The DGX Spark has a special brain (the GPU) that can do many math problems at the same time, like having 6,144 calculators working together.\n",
    ">\n",
    "> The really cool part? It has a HUGE memory (128GB) that both the regular brain (CPU) and the math brain (GPU) can share. It's like having one giant desk where everyone can work together, instead of passing papers back and forth between desks.\n",
    ">\n",
    "> **In AI terms:** This unified memory architecture means you can load enormous AI models (like a 70 billion parameter LLM) without worrying about GPU memory limits!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: GPU Information with nvidia-smi\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "The `nvidia-smi` (NVIDIA System Management Interface) command is your window into the GPU. It shows:\n",
    "- GPU model and architecture\n",
    "- Memory usage (total, used, free)\n",
    "- Running processes using the GPU\n",
    "- Temperature and power consumption\n",
    "\n",
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  1 21:49:15 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GB10                    On  |   0000000F:01:00.0 Off |                  N/A |\n",
      "| N/A   39C    P8              3W /  N/A  | Not Supported          |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Let's start by checking our GPU!\n",
    "# This is the most important command for any AI developer\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "You should see output showing:\n",
    "- **GPU Name:** NVIDIA Graphics Device (or GB10 Superchip)\n",
    "- **Memory:** 128GB (this is the unified memory!)\n",
    "- **CUDA Version:** 13.0 or higher\n",
    "- **Driver Version:** Latest NVIDIA driver\n",
    "\n",
    "Let's get a more detailed view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name, memory.total [MiB], memory.free [MiB], memory.used [MiB], temperature.gpu, power.draw [W]\n",
      "NVIDIA GB10, [N/A], [N/A], [N/A], 39, 3.74 W\n"
     ]
    }
   ],
   "source": [
    "# Get detailed GPU information in a query format\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free,memory.used,temperature.gpu,power.draw --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n"
     ]
    }
   ],
   "source": [
    "# Let's also check CUDA compute capability\n",
    "!nvidia-smi --query-gpu=compute_cap --format=csv,noheader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself #1\n",
    "\n",
    "Run the following cell to get GPU topology information. What interconnect does the DGX Spark use?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Look for \"NVLink\" in the output - this is the high-speed interconnect between CPU and GPU.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\u001b[4mGPU0\tNIC0\tNIC1\tNIC2\tNIC3\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\u001b[0m\n",
      "GPU0\t X \tNODE\tNODE\tNODE\tNODE\t0-19\t0\t\tN/A\n",
      "NIC0\tNODE\t X \tPIX\tNODE\tNODE\t\t\t\t\n",
      "NIC1\tNODE\tPIX\t X \tNODE\tNODE\t\t\t\t\n",
      "NIC2\tNODE\tNODE\tNODE\t X \tPIX\t\t\t\t\n",
      "NIC3\tNODE\tNODE\tNODE\tPIX\t X \t\t\t\t\n",
      "\n",
      "Legend:\n",
      "\n",
      "  X    = Self\n",
      "  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n",
      "  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n",
      "  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n",
      "  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n",
      "  PIX  = Connection traversing at most a single PCIe bridge\n",
      "  NV#  = Connection traversing a bonded set of # NVLinks\n",
      "\n",
      "NIC Legend:\n",
      "\n",
      "  NIC0: rocep1s0f0\n",
      "  NIC1: rocep1s0f1\n",
      "  NIC2: roceP2p1s0f0\n",
      "  NIC3: roceP2p1s0f1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE: Run nvidia-smi topo command\n",
    "# Hint: nvidia-smi topo --matrix\n",
    "\n",
    "!nvidia-smi topo --matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: CPU Information with lscpu\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "The DGX Spark uses NVIDIA's Grace CPU - an ARM-based processor designed specifically for AI workloads. This is different from typical x86 processors (Intel/AMD) found in most computers.\n",
    "\n",
    "**Why ARM matters:**\n",
    "- More power-efficient\n",
    "- Designed to work seamlessly with the Blackwell GPU\n",
    "- Connected via NVLink-C2C for maximum bandwidth\n",
    "\n",
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:                aarch64\n",
      "  CPU op-mode(s):            64-bit\n",
      "  Byte Order:                Little Endian\n",
      "CPU(s):                      20\n",
      "  On-line CPU(s) list:       0-19\n",
      "Vendor ID:                   ARM\n",
      "  Model name:                Cortex-X925\n",
      "    Model:                   1\n",
      "    Thread(s) per core:      1\n",
      "    Core(s) per socket:      10\n",
      "    Socket(s):               1\n",
      "    Stepping:                r0p1\n",
      "    CPU(s) scaling MHz:      101%\n",
      "    CPU max MHz:             4004.0000\n",
      "    CPU min MHz:             1378.0000\n",
      "    BogoMIPS:                2000.00\n",
      "    Flags:                   fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics \n",
      "                             fphp asimdhp cpuid asimdrdm jscvt fcma lrcpc dcpop \n",
      "                             sha3 sm3 sm4 asimddp sha512 sve asimdfhm dit uscat \n",
      "                             ilrcpc flagm sb paca pacg dcpodp sve2 sveaes svepmu\n",
      "                             ll svebitperm svesha3 svesm4 flagm2 frint svei8mm s\n",
      "                             vebf16 i8mm bf16 dgh bti ecv afp wfxt\n",
      "  Model name:                Cortex-A725\n",
      "    Model:                   1\n",
      "    Thread(s) per core:      1\n",
      "    Core(s) per socket:      10\n",
      "    Socket(s):               1\n",
      "    Stepping:                r0p1\n",
      "    CPU(s) scaling MHz:      79%\n",
      "    CPU max MHz:             2860.0000\n",
      "    CPU min MHz:             338.0000\n",
      "    BogoMIPS:                2000.00\n",
      "    Flags:                   fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics \n",
      "                             fphp asimdhp cpuid asimdrdm jscvt fcma lrcpc dcpop \n",
      "                             sha3 sm3 sm4 asimddp sha512 sve asimdfhm dit uscat \n",
      "                             ilrcpc flagm sb paca pacg dcpodp sve2 sveaes svepmu\n",
      "                             ll svebitperm svesha3 svesm4 flagm2 frint svei8mm s\n",
      "                             vebf16 i8mm bf16 dgh bti ecv afp wfxt\n",
      "Caches (sum of all):         \n",
      "  L1d:                       1.3 MiB (20 instances)\n",
      "  L1i:                       1.3 MiB (20 instances)\n",
      "  L2:                        25 MiB (20 instances)\n",
      "  L3:                        24 MiB (2 instances)\n",
      "NUMA:                        \n",
      "  NUMA node(s):              1\n",
      "  NUMA node0 CPU(s):         0-19\n",
      "Vulnerabilities:             \n",
      "  Gather data sampling:      Not affected\n",
      "  Ghostwrite:                Not affected\n",
      "  Indirect target selection: Not affected\n",
      "  Itlb multihit:             Not affected\n",
      "  L1tf:                      Not affected\n",
      "  Mds:                       Not affected\n",
      "  Meltdown:                  Not affected\n",
      "  Mmio stale data:           Not affected\n",
      "  Reg file data sampling:    Not affected\n",
      "  Retbleed:                  Not affected\n",
      "  Spec rstack overflow:      Not affected\n",
      "  Spec store bypass:         Mitigation; Speculative Store Bypass disabled via p\n",
      "                             rctl\n",
      "  Spectre v1:                Mitigation; __user pointer sanitization\n",
      "  Spectre v2:                Mitigation; CSV2, BHB\n",
      "  Srbds:                     Not affected\n",
      "  Tsa:                       Not affected\n",
      "  Tsx async abort:           Not affected\n",
      "  Vmscape:                   Not affected\n"
     ]
    }
   ],
   "source": [
    "# Check CPU information\n",
    "!lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "Key things to note from the output:\n",
    "- **Architecture:** aarch64 (ARM 64-bit)\n",
    "- **CPU(s):** 20 cores\n",
    "- **Model name:** ARM v9.2 cores\n",
    "- **Core types:** Mix of performance (Cortex-X925) and efficiency (Cortex-A725) cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:                            aarch64\n",
      "CPU(s):                                  20\n",
      "On-line CPU(s) list:                     0-19\n",
      "Model name:                              Cortex-X925\n",
      "Thread(s) per core:                      1\n",
      "Core(s) per socket:                      10\n",
      "Socket(s):                               1\n",
      "CPU(s) scaling MHz:                      94%\n",
      "CPU max MHz:                             4004.0000\n",
      "CPU min MHz:                             1378.0000\n",
      "Model name:                              Cortex-A725\n",
      "Thread(s) per core:                      1\n",
      "Core(s) per socket:                      10\n",
      "Socket(s):                               1\n",
      "CPU(s) scaling MHz:                      99%\n",
      "CPU max MHz:                             2860.0000\n",
      "CPU min MHz:                             338.0000\n",
      "NUMA node0 CPU(s):                       0-19\n"
     ]
    }
   ],
   "source": [
    "# Get a summary of CPU info in a cleaner format\n",
    "!lscpu | grep -E \"Architecture|CPU\\(s\\)|Model name|Thread|Core|Socket|MHz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself #2\n",
    "\n",
    "Find out the cache sizes on your CPU. Large caches help with AI workloads!\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Use `lscpu | grep -i cache` to filter for cache information\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1d cache:                               1.3 MiB (20 instances)\n",
      "L1i cache:                               1.3 MiB (20 instances)\n",
      "L2 cache:                                25 MiB (20 instances)\n",
      "L3 cache:                                24 MiB (2 instances)\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE: Find the cache sizes\n",
    "!lscpu | grep -E \"cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Memory Information with free\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "The DGX Spark's **unified memory** is its superpower. Unlike traditional systems where CPU and GPU have separate memory pools, here they share 128GB of LPDDR5X memory.\n",
    "\n",
    "**Why this matters:**\n",
    "- No need to copy data between CPU and GPU\n",
    "- Can load larger models than typical GPU memory allows\n",
    "- Simpler programming model\n",
    "\n",
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\n",
      "Mem:           119Gi        12Gi        66Gi       3.6Mi        42Gi       107Gi\n",
      "Swap:             0B          0B          0B\n"
     ]
    }
   ],
   "source": [
    "# Check memory information\n",
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "You should see:\n",
    "- **total:** ~128GB (the unified memory pool)\n",
    "- **used:** Memory currently in use by system and applications\n",
    "- **free:** Available memory\n",
    "- **buff/cache:** Memory used for disk caching (can be reclaimed)\n",
    "\n",
    "‚ö†Ô∏è **Important:** The \"buff/cache\" memory can compete with GPU allocations! We'll learn how to clear it before loading large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemTotal:       125511968 kB\n",
      "MemFree:        69472388 kB\n",
      "MemAvailable:   112483768 kB\n",
      "Buffers:          583712 kB\n",
      "Cached:         41396448 kB\n",
      "SwapCached:            0 kB\n",
      "Active:          7702780 kB\n",
      "Inactive:       43433692 kB\n",
      "Active(anon):    6795344 kB\n",
      "Inactive(anon):  2372016 kB\n",
      "Active(file):     907436 kB\n",
      "Inactive(file): 41061676 kB\n",
      "Unevictable:       26436 kB\n",
      "Mlocked:           26436 kB\n",
      "SwapTotal:             0 kB\n",
      "SwapFree:              0 kB\n",
      "Zswap:                 0 kB\n",
      "Zswapped:              0 kB\n",
      "Dirty:               408 kB\n",
      "Writeback:             0 kB\n"
     ]
    }
   ],
   "source": [
    "# Get more detailed memory info\n",
    "!cat /proc/meminfo | head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for trosfy: \n"
     ]
    }
   ],
   "source": [
    "# Check memory bandwidth info\n",
    "!sudo dmidecode -t memory 2>/dev/null | grep -E \"Size|Speed|Type\" | head -10 || echo \"Note: dmidecode requires root access\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Storage Information with df\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "AI models and datasets can be HUGE. A single 70B parameter model can take 40-140GB depending on quantization. Knowing your storage is essential for planning.\n",
    "\n",
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "tmpfs            12G  3.1M   12G   1% /run\n",
      "efivarfs        256K   20K  237K   8% /sys/firmware/efi/efivars\n",
      "/dev/nvme0n1p2  3.7T  753G  2.8T  22% /\n",
      "tmpfs            60G   96K   60G   1% /dev/shm\n",
      "tmpfs           5.0M  8.0K  5.0M   1% /run/lock\n",
      "/dev/nvme0n1p1  298M  7.4M  291M   3% /boot/efi\n",
      "tmpfs            12G  156K   12G   1% /run/user/1001\n"
     ]
    }
   ],
   "source": [
    "# Check disk space\n",
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "/dev/nvme0n1p2  3.7T  753G  2.8T  22% /\n",
      "/dev/nvme0n1p1  298M  7.4M  291M   3% /boot/efi\n"
     ]
    }
   ],
   "source": [
    "# Check just the main partitions (filter out snap and temporary filesystems)\n",
    "!df -h | grep -E \"^/dev|Filesystem\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185G\t/home/trosfy/.cache/huggingface\n",
      "Ollama models not found yet\n"
     ]
    }
   ],
   "source": [
    "# Check where models are typically stored\n",
    "!du -sh ~/.cache/huggingface 2>/dev/null || echo \"Hugging Face cache not found yet\"\n",
    "!du -sh ~/.ollama 2>/dev/null || echo \"Ollama models not found yet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Software Environment\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "The DGX Spark runs DGX OS (based on Ubuntu 24.04 LTS). It comes with many AI tools pre-installed, but there's a critical thing to remember:\n",
    "\n",
    "‚ö†Ô∏è **Standard pip-installed PyTorch does NOT work** on DGX Spark because it's ARM64 + CUDA. You must use NGC containers!\n",
    "\n",
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRETTY_NAME=\"Ubuntu 24.04.3 LTS\"\n",
      "NAME=\"Ubuntu\"\n",
      "VERSION_ID=\"24.04\"\n",
      "VERSION=\"24.04.3 LTS (Noble Numbat)\"\n",
      "VERSION_CODENAME=noble\n",
      "ID=ubuntu\n",
      "ID_LIKE=debian\n",
      "HOME_URL=\"https://www.ubuntu.com/\"\n",
      "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
      "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
      "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
      "UBUNTU_CODENAME=noble\n",
      "LOGO=ubuntu-logo\n"
     ]
    }
   ],
   "source": [
    "# Check OS version\n",
    "!cat /etc/os-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux dgx-spark 6.14.0-1015-nvidia #15-Ubuntu SMP PREEMPT_DYNAMIC Tue Nov 25 18:02:16 UTC 2025 aarch64 aarch64 aarch64 GNU/Linux\n"
     ]
    }
   ],
   "source": [
    "# Check kernel version\n",
    "!uname -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2025 NVIDIA Corporation\n",
      "Built on Wed_Aug_20_01:57:39_PM_PDT_2025\n",
      "Cuda compilation tools, release 13.0, V13.0.88\n",
      "Build cuda_13.0.r13.0/compiler.36424714_0\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA version\n",
    "!nvcc --version 2>/dev/null || echo \"CUDA compiler not directly accessible - use NGC containers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker version 28.5.1, build e180ab8\n"
     ]
    }
   ],
   "source": [
    "# Check Docker installation\n",
    "!docker --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  cdi: nvidia.com/gpu=0\n",
      "  cdi: nvidia.com/gpu=GPU-3d29cd16-ca97-2c38-7d79-d462cfa45fed\n",
      "  cdi: nvidia.com/gpu=all\n",
      " Runtimes: runc io.containerd.runc.v2 nvidia\n",
      " Kernel Version: 6.14.0-1015-nvidia\n"
     ]
    }
   ],
   "source": [
    "# Check if NVIDIA container runtime is installed\n",
    "!docker info 2>/dev/null | grep -i nvidia || echo \"Check docker info for NVIDIA runtime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama version is 0.13.4\n"
     ]
    }
   ],
   "source": [
    "# Check if Ollama is installed\n",
    "!ollama --version 2>/dev/null || echo \"Ollama not found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Create Your System Specification Document\n",
    "\n",
    "Now let's compile all this information into a reusable Python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DGX SPARK SYSTEM SPECIFICATION\n",
      "============================================================\n",
      "\n",
      "Generated: 2026-01-02T07:44:26.707442\n",
      "Hostname: dgx-spark\n",
      "\n",
      "----------------------------------------\n",
      "OPERATING SYSTEM\n",
      "----------------------------------------\n",
      "  OS: \"Ubuntu 24.04.3 LTS\"\n",
      "  Kernel: 6.14.0-1015-nvidia\n",
      "  Architecture: aarch64\n",
      "\n",
      "----------------------------------------\n",
      "CPU\n",
      "----------------------------------------\n",
      "  Model: Cortex-X925\n",
      "                              Cortex-A725\n",
      "  Cores: 20\n",
      "  Architecture: aarch64\n",
      "\n",
      "----------------------------------------\n",
      "MEMORY\n",
      "----------------------------------------\n",
      "  Total: 119 GB\n",
      "  Available: 107 GB\n",
      "\n",
      "----------------------------------------\n",
      "GPU\n",
      "----------------------------------------\n",
      "  Model: NVIDIA GB10\n",
      "  Memory: [N/A]\n",
      "  Driver: 580.95.05\n",
      "  CUDA: 13.0\n",
      "\n",
      "----------------------------------------\n",
      "STORAGE\n",
      "----------------------------------------\n",
      "  Root Partition: 3.7T total, 2.8T available\n",
      "\n",
      "----------------------------------------\n",
      "SOFTWARE\n",
      "----------------------------------------\n",
      "  Docker: 28.5.1\n",
      "  Ollama: ollama version is 0.13.4\n",
      "  Python: 3.12.3\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import platform\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def run_command(cmd):\n",
    "    \"\"\"Run a shell command and return its output.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)\n",
    "        return result.stdout.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def get_system_info():\n",
    "    \"\"\"Gather comprehensive system information for DGX Spark.\"\"\"\n",
    "    info = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"hostname\": platform.node(),\n",
    "        \"os\": {},\n",
    "        \"cpu\": {},\n",
    "        \"memory\": {},\n",
    "        \"gpu\": {},\n",
    "        \"storage\": {},\n",
    "        \"software\": {}\n",
    "    }\n",
    "    \n",
    "    # OS Information\n",
    "    info[\"os\"][\"system\"] = platform.system()\n",
    "    info[\"os\"][\"release\"] = platform.release()\n",
    "    info[\"os\"][\"version\"] = run_command(\"cat /etc/os-release | grep PRETTY_NAME | cut -d'=' -f2\")\n",
    "    info[\"os\"][\"architecture\"] = platform.machine()\n",
    "    \n",
    "    # CPU Information\n",
    "    info[\"cpu\"][\"model\"] = run_command(\"lscpu | grep 'Model name' | cut -d':' -f2\").strip()\n",
    "    info[\"cpu\"][\"cores\"] = run_command(\"nproc\")\n",
    "    info[\"cpu\"][\"architecture\"] = run_command(\"lscpu | grep 'Architecture' | cut -d':' -f2\").strip()\n",
    "    \n",
    "    # Memory Information\n",
    "    mem_total = run_command(\"free -g | grep Mem | awk '{print $2}'\")\n",
    "    mem_available = run_command(\"free -g | grep Mem | awk '{print $7}'\")\n",
    "    info[\"memory\"][\"total_gb\"] = mem_total\n",
    "    info[\"memory\"][\"available_gb\"] = mem_available\n",
    "    \n",
    "    # GPU Information\n",
    "    info[\"gpu\"][\"name\"] = run_command(\"nvidia-smi --query-gpu=name --format=csv,noheader\")\n",
    "    info[\"gpu\"][\"memory_total\"] = run_command(\"nvidia-smi --query-gpu=memory.total --format=csv,noheader\")\n",
    "    info[\"gpu\"][\"driver_version\"] = run_command(\"nvidia-smi --query-gpu=driver_version --format=csv,noheader\")\n",
    "    info[\"gpu\"][\"cuda_version\"] = run_command(\"nvidia-smi | grep 'CUDA Version' | awk '{print $9}'\")\n",
    "    \n",
    "    # Storage Information\n",
    "    info[\"storage\"][\"root_total\"] = run_command(\"df -h / | tail -1 | awk '{print $2}'\")\n",
    "    info[\"storage\"][\"root_available\"] = run_command(\"df -h / | tail -1 | awk '{print $4}'\")\n",
    "    \n",
    "    # Software Versions\n",
    "    info[\"software\"][\"docker\"] = run_command(\"docker --version 2>/dev/null | cut -d' ' -f3 | tr -d ','\")\n",
    "    info[\"software\"][\"ollama\"] = run_command(\"ollama --version 2>/dev/null\")\n",
    "    info[\"software\"][\"python\"] = platform.python_version()\n",
    "    \n",
    "    return info\n",
    "\n",
    "# Get and display system information\n",
    "system_info = get_system_info()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DGX SPARK SYSTEM SPECIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nGenerated: {system_info['timestamp']}\")\n",
    "print(f\"Hostname: {system_info['hostname']}\")\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"OPERATING SYSTEM\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  OS: {system_info['os']['version']}\")\n",
    "print(f\"  Kernel: {system_info['os']['release']}\")\n",
    "print(f\"  Architecture: {system_info['os']['architecture']}\")\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"CPU\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Model: {system_info['cpu']['model']}\")\n",
    "print(f\"  Cores: {system_info['cpu']['cores']}\")\n",
    "print(f\"  Architecture: {system_info['cpu']['architecture']}\")\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"MEMORY\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Total: {system_info['memory']['total_gb']} GB\")\n",
    "print(f\"  Available: {system_info['memory']['available_gb']} GB\")\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"GPU\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Model: {system_info['gpu']['name']}\")\n",
    "print(f\"  Memory: {system_info['gpu']['memory_total']}\")\n",
    "print(f\"  Driver: {system_info['gpu']['driver_version']}\")\n",
    "print(f\"  CUDA: {system_info['gpu']['cuda_version']}\")\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"STORAGE\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Root Partition: {system_info['storage']['root_total']} total, {system_info['storage']['root_available']} available\")\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"SOFTWARE\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Docker: {system_info['software']['docker']}\")\n",
    "print(f\"  Ollama: {system_info['software']['ollama']}\")\n",
    "print(f\"  Python: {system_info['software']['python']}\")\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Trying to pip install PyTorch\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong way - This will NOT work on DGX Spark!\n",
    "pip install torch torchvision\n",
    "\n",
    "# ‚úÖ Right way - Use NGC containers\n",
    "docker run --gpus all -it --rm nvcr.io/nvidia/pytorch:25.11-py3 python -c \"import torch; print(torch.cuda.is_available())\"\n",
    "```\n",
    "**Why:** Standard PyTorch wheels are built for x86 architecture. DGX Spark uses ARM64, so you need specially compiled versions from NGC.\n",
    "\n",
    "### Mistake 2: Not clearing buffer cache before loading large models\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong way - Loading 70B model with cached memory\n",
    "ollama run qwen3:32b  # Might fail with OOM!\n",
    "\n",
    "# ‚úÖ Right way - Clear cache first\n",
    "sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'\n",
    "ollama run qwen3:32b  # Works!\n",
    "```\n",
    "**Why:** Linux aggressively caches disk data in RAM. This memory competes with GPU allocations on unified memory systems.\n",
    "\n",
    "### Mistake 3: Forgetting --gpus all flag\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong way - No GPU access\n",
    "docker run -it nvcr.io/nvidia/pytorch:25.11-py3 bash\n",
    "\n",
    "# ‚úÖ Right way - GPU enabled\n",
    "docker run --gpus all -it --rm nvcr.io/nvidia/pytorch:25.11-py3 bash\n",
    "```\n",
    "**Why:** Docker doesn't expose GPUs by default. The `--gpus all` flag enables NVIDIA container runtime.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How to use `nvidia-smi` to check GPU status\n",
    "- ‚úÖ How to use `lscpu` to understand the ARM64 CPU\n",
    "- ‚úÖ How to use `free` to monitor unified memory\n",
    "- ‚úÖ How to use `df` to check storage capacity\n",
    "- ‚úÖ The importance of NGC containers for PyTorch\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "Create a monitoring script that runs every 5 seconds and displays:\n",
    "1. GPU memory usage\n",
    "2. GPU temperature\n",
    "3. System memory usage\n",
    "\n",
    "Use `watch` command or write a Python loop with `time.sleep()`.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Solution Hint</summary>\n",
    "\n",
    "```bash\n",
    "watch -n 5 \"nvidia-smi --query-gpu=memory.used,memory.free,temperature.gpu --format=csv && echo '---' && free -h | head -2\"\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [DGX Spark User Guide](https://docs.nvidia.com/dgx/dgx-spark/)\n",
    "- [NVIDIA Grace CPU Architecture](https://www.nvidia.com/en-us/data-center/grace-cpu/)\n",
    "- [Blackwell GPU Architecture](https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/)\n",
    "- [NGC Container Catalog](https://catalog.ngc.nvidia.com/)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup\n",
    "\n",
    "No cleanup needed for this notebook - we only ran read-only commands!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleanup complete!\n"
     ]
    }
   ],
   "source": [
    "# Cleanup cell - good practice even for read-only notebooks\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"‚úÖ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great job completing Lab 1.1.1: System Exploration!\n",
      "\n",
      "Next up: Lab 1.1.2 - Memory Architecture Lab\n",
      "You'll learn how unified memory really works by allocating tensors of various sizes.\n"
     ]
    }
   ],
   "source": [
    "print(\"Great job completing Lab 1.1.1: System Exploration!\")\n",
    "print(\"\\nNext up: Lab 1.1.2 - Memory Architecture Lab\")\n",
    "print(\"You'll learn how unified memory really works by allocating tensors of various sizes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
