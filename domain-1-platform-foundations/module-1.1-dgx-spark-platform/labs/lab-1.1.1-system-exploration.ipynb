{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.1.1: System Exploration\n",
    "\n",
    "**Module:** 1.1 - DGX Spark Platform Mastery  \n",
    "**Time:** 1 hour  \n",
    "**Difficulty:** ‚≠ê (Beginner)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand the DGX Spark hardware architecture\n",
    "- [ ] Use system commands to explore your hardware\n",
    "- [ ] Document your system's capabilities\n",
    "- [ ] Know the key specifications that matter for AI workloads\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Basic command line knowledge\n",
    "- Access to a DGX Spark system\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "Before you can train AI models or run inference, you need to understand your hardware. Just like a chef needs to know their kitchen before cooking a feast, an AI engineer needs to know their compute environment.\n",
    "\n",
    "The DGX Spark is NVIDIA's first desktop AI supercomputer, designed specifically for AI developers. Understanding its unique architecture will help you:\n",
    "- Choose the right model sizes for your hardware\n",
    "- Optimize memory usage for maximum performance\n",
    "- Troubleshoot issues when things don't work as expected\n",
    "\n",
    "---\n",
    "\n",
    "## üßí ELI5: What is the DGX Spark?\n",
    "\n",
    "> **Imagine you have a super-powered gaming computer...**\n",
    ">\n",
    "> But instead of being good at video games, it's amazing at doing math - billions of calculations per second! The DGX Spark has a special brain (the GPU) that can do many math problems at the same time, like having 6,144 calculators working together.\n",
    ">\n",
    "> The really cool part? It has a HUGE memory (128GB) that both the regular brain (CPU) and the math brain (GPU) can share. It's like having one giant desk where everyone can work together, instead of passing papers back and forth between desks.\n",
    ">\n",
    "> **In AI terms:** This unified memory architecture means you can load enormous AI models (like a 70 billion parameter LLM) without worrying about GPU memory limits!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: GPU Information with nvidia-smi\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "The `nvidia-smi` (NVIDIA System Management Interface) command is your window into the GPU. It shows:\n",
    "- GPU model and architecture\n",
    "- Memory usage (total, used, free)\n",
    "- Running processes using the GPU\n",
    "- Temperature and power consumption\n",
    "\n",
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by checking our GPU!\n",
    "# This is the most important command for any AI developer\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "You should see output showing:\n",
    "- **GPU Name:** NVIDIA Graphics Device (or GB10 Superchip)\n",
    "- **Memory:** 128GB (this is the unified memory!)\n",
    "- **CUDA Version:** 13.0 or higher\n",
    "- **Driver Version:** Latest NVIDIA driver\n",
    "\n",
    "Let's get a more detailed view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed GPU information in a query format\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free,memory.used,temperature.gpu,power.draw --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also check CUDA compute capability\n",
    "!nvidia-smi --query-gpu=compute_cap --format=csv,noheader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself #1\n",
    "\n",
    "Run the following cell to get GPU topology information. What interconnect does the DGX Spark use?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Look for \"NVLink\" in the output - this is the high-speed interconnect between CPU and GPU.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Run nvidia-smi topo command\n",
    "# Hint: nvidia-smi topo --matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: CPU Information with lscpu\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "The DGX Spark uses NVIDIA's Grace CPU - an ARM-based processor designed specifically for AI workloads. This is different from typical x86 processors (Intel/AMD) found in most computers.\n",
    "\n",
    "**Why ARM matters:**\n",
    "- More power-efficient\n",
    "- Designed to work seamlessly with the Blackwell GPU\n",
    "- Connected via NVLink-C2C for maximum bandwidth\n",
    "\n",
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CPU information\n",
    "!lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "Key things to note from the output:\n",
    "- **Architecture:** aarch64 (ARM 64-bit)\n",
    "- **CPU(s):** 20 cores\n",
    "- **Model name:** ARM v9.2 cores\n",
    "- **Core types:** Mix of performance (Cortex-X925) and efficiency (Cortex-A725) cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary of CPU info in a cleaner format\n",
    "!lscpu | grep -E \"Architecture|CPU\\(s\\)|Model name|Thread|Core|Socket|MHz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself #2\n",
    "\n",
    "Find out the cache sizes on your CPU. Large caches help with AI workloads!\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Use `lscpu | grep -i cache` to filter for cache information\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Find the cache sizes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Memory Information with free\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "The DGX Spark's **unified memory** is its superpower. Unlike traditional systems where CPU and GPU have separate memory pools, here they share 128GB of LPDDR5X memory.\n",
    "\n",
    "**Why this matters:**\n",
    "- No need to copy data between CPU and GPU\n",
    "- Can load larger models than typical GPU memory allows\n",
    "- Simpler programming model\n",
    "\n",
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory information\n",
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "You should see:\n",
    "- **total:** ~128GB (the unified memory pool)\n",
    "- **used:** Memory currently in use by system and applications\n",
    "- **free:** Available memory\n",
    "- **buff/cache:** Memory used for disk caching (can be reclaimed)\n",
    "\n",
    "‚ö†Ô∏è **Important:** The \"buff/cache\" memory can compete with GPU allocations! We'll learn how to clear it before loading large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get more detailed memory info\n",
    "!cat /proc/meminfo | head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory bandwidth info\n",
    "!sudo dmidecode -t memory 2>/dev/null | grep -E \"Size|Speed|Type\" | head -10 || echo \"Note: dmidecode requires root access\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Storage Information with df\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "AI models and datasets can be HUGE. A single 70B parameter model can take 40-140GB depending on quantization. Knowing your storage is essential for planning.\n",
    "\n",
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check disk space\n",
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check just the main partitions (filter out snap and temporary filesystems)\n",
    "!df -h | grep -E \"^/dev|Filesystem\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check where models are typically stored\n",
    "!du -sh ~/.cache/huggingface 2>/dev/null || echo \"Hugging Face cache not found yet\"\n",
    "!du -sh ~/.ollama 2>/dev/null || echo \"Ollama models not found yet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Software Environment\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "The DGX Spark runs DGX OS (based on Ubuntu 24.04 LTS). It comes with many AI tools pre-installed, but there's a critical thing to remember:\n",
    "\n",
    "‚ö†Ô∏è **Standard pip-installed PyTorch does NOT work** on DGX Spark because it's ARM64 + CUDA. You must use NGC containers!\n",
    "\n",
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check OS version\n",
    "!cat /etc/os-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check kernel version\n",
    "!uname -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA version\n",
    "!nvcc --version 2>/dev/null || echo \"CUDA compiler not directly accessible - use NGC containers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Docker installation\n",
    "!docker --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if NVIDIA container runtime is installed\n",
    "!docker info 2>/dev/null | grep -i nvidia || echo \"Check docker info for NVIDIA runtime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Ollama is installed\n",
    "!ollama --version 2>/dev/null || echo \"Ollama not found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Create Your System Specification Document\n",
    "\n",
    "Now let's compile all this information into a reusable Python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import platform\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def run_command(cmd):\n",
    "    \"\"\"Run a shell command and return its output.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)\n",
    "        return result.stdout.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def get_system_info():\n",
    "    \"\"\"Gather comprehensive system information for DGX Spark.\"\"\"\n",
    "    info = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"hostname\": platform.node(),\n",
    "        \"os\": {},\n",
    "        \"cpu\": {},\n",
    "        \"memory\": {},\n",
    "        \"gpu\": {},\n",
    "        \"storage\": {},\n",
    "        \"software\": {}\n",
    "    }\n",
    "    \n",
    "    # OS Information\n",
    "    info[\"os\"][\"system\"] = platform.system()\n",
    "    info[\"os\"][\"release\"] = platform.release()\n",
    "    info[\"os\"][\"version\"] = run_command(\"cat /etc/os-release | grep PRETTY_NAME | cut -d'=' -f2\")\n",
    "    info[\"os\"][\"architecture\"] = platform.machine()\n",
    "    \n",
    "    # CPU Information\n",
    "    info[\"cpu\"][\"model\"] = run_command(\"lscpu | grep 'Model name' | cut -d':' -f2\").strip()\n",
    "    info[\"cpu\"][\"cores\"] = run_command(\"nproc\")\n",
    "    info[\"cpu\"][\"architecture\"] = run_command(\"lscpu | grep 'Architecture' | cut -d':' -f2\").strip()\n",
    "    \n",
    "    # Memory Information\n",
    "    mem_total = run_command(\"free -g | grep Mem | awk '{print $2}'\")\n",
    "    mem_available = run_command(\"free -g | grep Mem | awk '{print $7}'\")\n",
    "    info[\"memory\"][\"total_gb\"] = mem_total\n",
    "    info[\"memory\"][\"available_gb\"] = mem_available\n",
    "    \n",
    "    # GPU Information\n",
    "    info[\"gpu\"][\"name\"] = run_command(\"nvidia-smi --query-gpu=name --format=csv,noheader\")\n",
    "    info[\"gpu\"][\"memory_total\"] = run_command(\"nvidia-smi --query-gpu=memory.total --format=csv,noheader\")\n",
    "    info[\"gpu\"][\"driver_version\"] = run_command(\"nvidia-smi --query-gpu=driver_version --format=csv,noheader\")\n",
    "    info[\"gpu\"][\"cuda_version\"] = run_command(\"nvidia-smi | grep 'CUDA Version' | awk '{print $9}'\")\n",
    "    \n",
    "    # Storage Information\n",
    "    info[\"storage\"][\"root_total\"] = run_command(\"df -h / | tail -1 | awk '{print $2}'\")\n",
    "    info[\"storage\"][\"root_available\"] = run_command(\"df -h / | tail -1 | awk '{print $4}'\")\n",
    "    \n",
    "    # Software Versions\n",
    "    info[\"software\"][\"docker\"] = run_command(\"docker --version 2>/dev/null | cut -d' ' -f3 | tr -d ','\")\n",
    "    info[\"software\"][\"ollama\"] = run_command(\"ollama --version 2>/dev/null\")\n",
    "    info[\"software\"][\"python\"] = platform.python_version()\n",
    "    \n",
    "    return info\n",
    "\n",
    "# Get and display system information\n",
    "system_info = get_system_info()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DGX SPARK SYSTEM SPECIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nGenerated: {system_info['timestamp']}\")\n",
    "print(f\"Hostname: {system_info['hostname']}\")\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"OPERATING SYSTEM\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  OS: {system_info['os']['version']}\")\n",
    "print(f\"  Kernel: {system_info['os']['release']}\")\n",
    "print(f\"  Architecture: {system_info['os']['architecture']}\")\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"CPU\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Model: {system_info['cpu']['model']}\")\n",
    "print(f\"  Cores: {system_info['cpu']['cores']}\")\n",
    "print(f\"  Architecture: {system_info['cpu']['architecture']}\")\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"MEMORY\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Total: {system_info['memory']['total_gb']} GB\")\n",
    "print(f\"  Available: {system_info['memory']['available_gb']} GB\")\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"GPU\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Model: {system_info['gpu']['name']}\")\n",
    "print(f\"  Memory: {system_info['gpu']['memory_total']}\")\n",
    "print(f\"  Driver: {system_info['gpu']['driver_version']}\")\n",
    "print(f\"  CUDA: {system_info['gpu']['cuda_version']}\")\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"STORAGE\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Root Partition: {system_info['storage']['root_total']} total, {system_info['storage']['root_available']} available\")\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"SOFTWARE\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Docker: {system_info['software']['docker']}\")\n",
    "print(f\"  Ollama: {system_info['software']['ollama']}\")\n",
    "print(f\"  Python: {system_info['software']['python']}\")\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Trying to pip install PyTorch\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong way - This will NOT work on DGX Spark!\n",
    "pip install torch torchvision\n",
    "\n",
    "# ‚úÖ Right way - Use NGC containers\n",
    "docker run --gpus all -it --rm nvcr.io/nvidia/pytorch:25.11-py3 python -c \"import torch; print(torch.cuda.is_available())\"\n",
    "```\n",
    "**Why:** Standard PyTorch wheels are built for x86 architecture. DGX Spark uses ARM64, so you need specially compiled versions from NGC.\n",
    "\n",
    "### Mistake 2: Not clearing buffer cache before loading large models\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong way - Loading 70B model with cached memory\n",
    "ollama run llama3.1:70b  # Might fail with OOM!\n",
    "\n",
    "# ‚úÖ Right way - Clear cache first\n",
    "sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'\n",
    "ollama run llama3.1:70b  # Works!\n",
    "```\n",
    "**Why:** Linux aggressively caches disk data in RAM. This memory competes with GPU allocations on unified memory systems.\n",
    "\n",
    "### Mistake 3: Forgetting --gpus all flag\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong way - No GPU access\n",
    "docker run -it nvcr.io/nvidia/pytorch:25.11-py3 bash\n",
    "\n",
    "# ‚úÖ Right way - GPU enabled\n",
    "docker run --gpus all -it --rm nvcr.io/nvidia/pytorch:25.11-py3 bash\n",
    "```\n",
    "**Why:** Docker doesn't expose GPUs by default. The `--gpus all` flag enables NVIDIA container runtime.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How to use `nvidia-smi` to check GPU status\n",
    "- ‚úÖ How to use `lscpu` to understand the ARM64 CPU\n",
    "- ‚úÖ How to use `free` to monitor unified memory\n",
    "- ‚úÖ How to use `df` to check storage capacity\n",
    "- ‚úÖ The importance of NGC containers for PyTorch\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "Create a monitoring script that runs every 5 seconds and displays:\n",
    "1. GPU memory usage\n",
    "2. GPU temperature\n",
    "3. System memory usage\n",
    "\n",
    "Use `watch` command or write a Python loop with `time.sleep()`.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Solution Hint</summary>\n",
    "\n",
    "```bash\n",
    "watch -n 5 \"nvidia-smi --query-gpu=memory.used,memory.free,temperature.gpu --format=csv && echo '---' && free -h | head -2\"\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [DGX Spark User Guide](https://docs.nvidia.com/dgx/dgx-spark/)\n",
    "- [NVIDIA Grace CPU Architecture](https://www.nvidia.com/en-us/data-center/grace-cpu/)\n",
    "- [Blackwell GPU Architecture](https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/)\n",
    "- [NGC Container Catalog](https://catalog.ngc.nvidia.com/)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup\n",
    "\n",
    "No cleanup needed for this notebook - we only ran read-only commands!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup cell - good practice even for read-only notebooks\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"‚úÖ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Great job completing Lab 1.1.1: System Exploration!\")\n",
    "print(\"\\nNext up: Lab 1.1.2 - Memory Architecture Lab\")\n",
    "print(\"You'll learn how unified memory really works by allocating tensors of various sizes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
