{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.1.2: Memory Architecture Lab\n",
    "\n",
    "**Module:** 1.1 - DGX Spark Platform Mastery  \n",
    "**Time:** 1.5 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand how unified memory works on DGX Spark\n",
    "- [ ] Allocate and monitor GPU tensors of various sizes\n",
    "- [ ] Identify the memory limits and behavior patterns\n",
    "- [ ] Learn to clear buffer cache for optimal memory availability\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Lab 1.1.1 (System Exploration)\n",
    "- Running inside NGC PyTorch container\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "When you want to run a 70 billion parameter language model, you need about 35-140GB of memory depending on quantization. Traditional GPUs max out at 24-80GB. The DGX Spark's 128GB unified memory is a game-changer because:\n",
    "\n",
    "1. **No memory copy overhead** - Data doesn't need to transfer between CPU and GPU\n",
    "2. **Larger models fit** - Run models that wouldn't fit on traditional GPUs\n",
    "3. **Simpler programming** - No need to manage separate memory pools\n",
    "\n",
    "In this lab, you'll see this in action by allocating tensors of various sizes and watching how memory behaves.\n",
    "\n",
    "---\n",
    "\n",
    "## üßí ELI5: Unified Memory\n",
    "\n",
    "> **Imagine you have two desks in your room...**\n",
    ">\n",
    "> One desk (CPU) is where you read books and make plans. The other desk (GPU) is where you do thousands of math problems super fast. Normally, if you need to use something from one desk on the other, you have to walk over and carry it - which takes time!\n",
    ">\n",
    "> **Unified memory is like pushing both desks together into one GIANT desk.** Now everything is right there - no more walking back and forth! Both \"workers\" (CPU and GPU) can reach everything instantly.\n",
    ">\n",
    "> That's why DGX Spark can load huge AI brains (models) that wouldn't fit on a normal GPU desk - because it has one enormous shared desk (128GB of memory)!\n",
    ">\n",
    "> **In AI terms:** The CPU and GPU share the same physical memory pool, eliminating the PCIe transfer bottleneck that limits traditional systems.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Important: Run This Notebook in NGC Container\n",
    "\n",
    "This notebook **must** be run inside an NGC PyTorch container. If you're not already in one, start it with:\n",
    "\n",
    "```bash\n",
    "docker run --gpus all -it --rm \\\n",
    "    -v $HOME/workspace:/workspace \\\n",
    "    -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\n",
    "    -p 8888:8888 \\\n",
    "    --ipc=host \\\n",
    "    nvcr.io/nvidia/pytorch:25.11-py3 \\\n",
    "    jupyter lab --ip=0.0.0.0 --allow-root --no-browser\n",
    "```\n",
    "\n",
    "> **Note:** The `-p 8888:8888` flag is only needed when running Jupyter Lab inside the container.\n",
    "> For interactive bash sessions, you can omit the port mapping.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Verify PyTorch CUDA Setup\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "First, let's confirm we have GPU access through PyTorch. This is the most critical check for any deep learning work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PyTorch CUDA Configuration\n",
      "============================================================\n",
      "PyTorch version: 2.9.0+cu130\n",
      "CUDA available: True\n",
      "CUDA version: 13.0\n",
      "cuDNN version: 91300\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GB10\n",
      "\n",
      "GPU Memory: 128.5 GB\n",
      "Compute capability: 12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trosfy/projects/dgx-spark-ai-curriculum/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Check CUDA availability\n",
    "print(\"=\" * 60)\n",
    "print(\"PyTorch CUDA Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Memory info\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"\\nGPU Memory: {props.total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"Compute capability: {props.major}.{props.minor}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå CUDA is not available!\")\n",
    "    print(\"Make sure you're running inside an NGC container with --gpus all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully created tensor on GPU\n",
      "   Shape: torch.Size([1000, 1000])\n",
      "   Device: cuda:0\n",
      "   Memory used: 4.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Quick test: Create a tensor on GPU\n",
    "if torch.cuda.is_available():\n",
    "    x = torch.randn(1000, 1000, device='cuda')\n",
    "    print(f\"‚úÖ Successfully created tensor on GPU\")\n",
    "    print(f\"   Shape: {x.shape}\")\n",
    "    print(f\"   Device: {x.device}\")\n",
    "    print(f\"   Memory used: {x.element_size() * x.nelement() / 1e6:.2f} MB\")\n",
    "    del x\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Memory Monitoring Utilities\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "PyTorch provides detailed memory tracking. Let's create helper functions to monitor memory usage throughout our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Initial Memory Status:\n",
      "   Allocated: 0.00 GB\n",
      "   Reserved:  0.00 GB\n",
      "   Free:      128.52 GB\n",
      "   Total:     128.5 GB\n",
      "   [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 0.0%\n"
     ]
    }
   ],
   "source": [
    "def get_memory_stats():\n",
    "    \"\"\"\n",
    "    Get current GPU memory statistics.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Memory statistics in GB\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    \n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    max_allocated = torch.cuda.max_memory_allocated() / 1e9\n",
    "    max_reserved = torch.cuda.max_memory_reserved() / 1e9\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    return {\n",
    "        'allocated_gb': allocated,\n",
    "        'reserved_gb': reserved,\n",
    "        'max_allocated_gb': max_allocated,\n",
    "        'max_reserved_gb': max_reserved,\n",
    "        'total_gb': total,\n",
    "        'free_gb': total - reserved\n",
    "    }\n",
    "\n",
    "def print_memory_stats(label=\"Current\"):\n",
    "    \"\"\"\n",
    "    Print formatted memory statistics.\n",
    "    \"\"\"\n",
    "    stats = get_memory_stats()\n",
    "    if stats:\n",
    "        print(f\"\\nüìä {label} Memory Status:\")\n",
    "        print(f\"   Allocated: {stats['allocated_gb']:.2f} GB\")\n",
    "        print(f\"   Reserved:  {stats['reserved_gb']:.2f} GB\")\n",
    "        print(f\"   Free:      {stats['free_gb']:.2f} GB\")\n",
    "        print(f\"   Total:     {stats['total_gb']:.1f} GB\")\n",
    "        \n",
    "        # Visual bar\n",
    "        used_pct = stats['reserved_gb'] / stats['total_gb'] * 100\n",
    "        bar_len = 40\n",
    "        filled = int(bar_len * used_pct / 100)\n",
    "        bar = '‚ñà' * filled + '‚ñë' * (bar_len - filled)\n",
    "        print(f\"   [{bar}] {used_pct:.1f}%\")\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"\n",
    "    Clear GPU memory cache.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    print(\"üßπ Memory cleared\")\n",
    "\n",
    "# Test our functions\n",
    "print_memory_stats(\"Initial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Understanding Memory Types\n",
    "\n",
    "- **Allocated**: Memory actually used by tensors\n",
    "- **Reserved**: Memory reserved by PyTorch (includes caching)\n",
    "- **Free**: Memory available for new allocations\n",
    "- **Total**: Total GPU memory (128GB on DGX Spark)\n",
    "\n",
    "PyTorch reserves more memory than strictly needed to speed up future allocations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Small Tensor Allocations (1-10 GB)\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "Let's start with allocations that would fit on any modern GPU. We'll create tensors of increasing size and observe memory behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Memory cleared\n",
      "Testing 1GB allocation...\n",
      "‚úÖ Allocated 1.00 GB in 35.8 ms\n",
      "\n",
      "üìä After 1GB Memory Status:\n",
      "   Allocated: 1.00 GB\n",
      "   Reserved:  1.00 GB\n",
      "   Free:      127.52 GB\n",
      "   Total:     128.5 GB\n",
      "   [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 0.8%\n"
     ]
    }
   ],
   "source": [
    "# Clear any existing allocations\n",
    "clear_memory()\n",
    "\n",
    "def allocate_tensor_gb(size_gb: float, dtype=torch.float32):\n",
    "    \"\"\"\n",
    "    Allocate a tensor of approximately the specified size in GB.\n",
    "    \n",
    "    Args:\n",
    "        size_gb: Desired size in gigabytes\n",
    "        dtype: Data type (default: float32 = 4 bytes per element)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (tensor, actual_size_gb, time_taken)\n",
    "    \"\"\"\n",
    "    bytes_per_element = torch.tensor([], dtype=dtype).element_size()\n",
    "    num_elements = int(size_gb * 1e9 / bytes_per_element)\n",
    "    \n",
    "    # Use a 1D tensor for simplicity\n",
    "    start_time = time.time()\n",
    "    tensor = torch.empty(num_elements, dtype=dtype, device='cuda')\n",
    "    torch.cuda.synchronize()  # Wait for allocation to complete\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    actual_size = tensor.element_size() * tensor.nelement() / 1e9\n",
    "    \n",
    "    return tensor, actual_size, elapsed\n",
    "\n",
    "# Test with 1GB\n",
    "print(\"Testing 1GB allocation...\")\n",
    "tensor_1gb, size, alloc_time = allocate_tensor_gb(1.0)\n",
    "print(f\"‚úÖ Allocated {size:.2f} GB in {alloc_time*1000:.1f} ms\")\n",
    "print_memory_stats(\"After 1GB\")\n",
    "\n",
    "# Clean up\n",
    "del tensor_1gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Memory cleared\n",
      "\n",
      "============================================================\n",
      "Small Tensor Allocation Test (1-10 GB)\n",
      "============================================================\n",
      "Size (GB)    Alloc Time (ms)    Memory Used (GB)  \n",
      "------------------------------------------------\n",
      "üßπ Memory cleared\n",
      "1            35.67              1.00              \n",
      "üßπ Memory cleared\n",
      "2            78.09              2.00              \n",
      "üßπ Memory cleared\n",
      "4            148.63             4.00              \n",
      "üßπ Memory cleared\n",
      "8            291.55             8.00              \n",
      "üßπ Memory cleared\n",
      "10           344.19             10.00             \n",
      "üßπ Memory cleared\n",
      "\n",
      "‚úÖ All small allocations successful!\n"
     ]
    }
   ],
   "source": [
    "# Now let's test a range of sizes\n",
    "clear_memory()\n",
    "\n",
    "test_sizes = [1, 2, 4, 8, 10]  # GB\n",
    "results = []\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Small Tensor Allocation Test (1-10 GB)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Size (GB)':<12} {'Alloc Time (ms)':<18} {'Memory Used (GB)':<18}\")\n",
    "print(\"-\" * 48)\n",
    "\n",
    "for size_gb in test_sizes:\n",
    "    clear_memory()\n",
    "    \n",
    "    try:\n",
    "        tensor, actual_size, alloc_time = allocate_tensor_gb(size_gb)\n",
    "        stats = get_memory_stats()\n",
    "        \n",
    "        results.append({\n",
    "            'requested_gb': size_gb,\n",
    "            'actual_gb': actual_size,\n",
    "            'alloc_ms': alloc_time * 1000,\n",
    "            'reserved_gb': stats['reserved_gb']\n",
    "        })\n",
    "        \n",
    "        print(f\"{size_gb:<12} {alloc_time*1000:<18.2f} {stats['reserved_gb']:<18.2f}\")\n",
    "        \n",
    "        del tensor\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(f\"{size_gb:<12} FAILED: {str(e)[:40]}\")\n",
    "\n",
    "clear_memory()\n",
    "print(\"\\n‚úÖ All small allocations successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "Notice that:\n",
    "1. **Allocation is fast** - No data transfer between CPU and GPU\n",
    "2. **Reserved > Allocated** - PyTorch caches memory for efficiency\n",
    "3. **Linear scaling** - Memory usage scales linearly with tensor size\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Medium Tensor Allocations (20-50 GB)\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "Now we enter territory that would fail on most GPUs! A typical RTX 4090 has 24GB. Let's allocate 30, 40, and 50 GB tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Memory cleared\n",
      "\n",
      "============================================================\n",
      "Medium Tensor Allocation Test (20-50 GB)\n",
      "============================================================\n",
      "‚ö†Ô∏è  These sizes would FAIL on most consumer GPUs!\n",
      "Size (GB)    Alloc Time (ms)    Memory Used (GB)  \n",
      "------------------------------------------------\n",
      "üßπ Memory cleared\n",
      "20           672.04             20.00             \n",
      "üßπ Memory cleared\n",
      "30           978.98             30.00             \n",
      "üßπ Memory cleared\n",
      "40           1287.50            40.00             \n",
      "üßπ Memory cleared\n",
      "50           1544.87            50.00             \n",
      "üßπ Memory cleared\n",
      "\n",
      "üéâ DGX Spark handled these like a champ!\n"
     ]
    }
   ],
   "source": [
    "clear_memory()\n",
    "\n",
    "medium_sizes = [20, 30, 40, 50]  # GB\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Medium Tensor Allocation Test (20-50 GB)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚ö†Ô∏è  These sizes would FAIL on most consumer GPUs!\")\n",
    "print(f\"{'Size (GB)':<12} {'Alloc Time (ms)':<18} {'Memory Used (GB)':<18}\")\n",
    "print(\"-\" * 48)\n",
    "\n",
    "for size_gb in medium_sizes:\n",
    "    clear_memory()\n",
    "    \n",
    "    try:\n",
    "        tensor, actual_size, alloc_time = allocate_tensor_gb(size_gb)\n",
    "        stats = get_memory_stats()\n",
    "        \n",
    "        print(f\"{size_gb:<12} {alloc_time*1000:<18.2f} {stats['reserved_gb']:<18.2f}\")\n",
    "        \n",
    "        del tensor\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(f\"{size_gb:<12} ‚ùå FAILED: Out of memory\")\n",
    "        print(f\"   Hint: Clear buffer cache and try again\")\n",
    "\n",
    "clear_memory()\n",
    "print(\"\\nüéâ DGX Spark handled these like a champ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself #1\n",
    "\n",
    "What's the largest single tensor you can allocate? Try sizes between 60-100 GB.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "You should be able to allocate up to ~100GB, but the exact limit depends on:\n",
    "- Current buffer cache usage\n",
    "- Other running processes\n",
    "- PyTorch's memory overhead\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Try allocating larger tensors\n",
    "# What's the maximum size you can allocate?\n",
    "\n",
    "clear_memory()\n",
    "\n",
    "# Try these sizes:\n",
    "# large_sizes = [60, 70, 80, 90, 100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Large Tensor Allocations (70-100 GB)\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "This is where the DGX Spark truly shines. We can allocate tensors that represent full 70B parameter models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Memory cleared\n",
      "System Memory Status:\n",
      "  Total:     119 GB\n",
      "  Available: 106 GB\n",
      "  Used:      12 GB\n"
     ]
    }
   ],
   "source": [
    "clear_memory()\n",
    "\n",
    "# Before large allocations, let's check system memory\n",
    "import subprocess\n",
    "from typing import Dict, Optional\n",
    "\n",
    "def get_system_memory() -> Optional[Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Get system memory info.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with memory info (total, used, free, available in GB),\n",
    "        or None if unable to retrieve.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(['free', '-g'], capture_output=True, text=True, timeout=10)\n",
    "        lines = result.stdout.strip().split('\\n')\n",
    "        if len(lines) >= 2:\n",
    "            parts = lines[1].split()\n",
    "            return {\n",
    "                'total': int(parts[1]),\n",
    "                'used': int(parts[2]),\n",
    "                'free': int(parts[3]),\n",
    "                'available': int(parts[6]) if len(parts) > 6 else int(parts[3])\n",
    "            }\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"‚ö†Ô∏è Command timed out\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error getting system memory: {e}\")\n",
    "    return None\n",
    "\n",
    "sys_mem = get_system_memory()\n",
    "if sys_mem:\n",
    "    print(\"System Memory Status:\")\n",
    "    print(f\"  Total:     {sys_mem['total']} GB\")\n",
    "    print(f\"  Available: {sys_mem['available']} GB\")\n",
    "    print(f\"  Used:      {sys_mem['used']} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Could not retrieve system memory info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Memory cleared\n",
      "\n",
      "============================================================\n",
      "Large Tensor Allocation Test (60-80 GB)\n",
      "============================================================\n",
      "üöÄ This is IMPOSSIBLE on most GPUs!\n",
      "Size (GB)    Status          Time (s)     Memory (GB)    \n",
      "------------------------------------------------------\n",
      "üßπ Memory cleared\n",
      "60           ‚úÖ SUCCESS       3.108        60.00          \n",
      "üßπ Memory cleared\n",
      "70           ‚úÖ SUCCESS       2.626        70.00          \n",
      "üßπ Memory cleared\n",
      "80           ‚úÖ SUCCESS       2.778        80.00          \n",
      "üßπ Memory cleared\n"
     ]
    }
   ],
   "source": [
    "# Large allocations\n",
    "clear_memory()\n",
    "\n",
    "large_sizes = [60, 70, 80]  # GB - Conservative to ensure success\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Large Tensor Allocation Test (60-80 GB)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üöÄ This is IMPOSSIBLE on most GPUs!\")\n",
    "print(f\"{'Size (GB)':<12} {'Status':<15} {'Time (s)':<12} {'Memory (GB)':<15}\")\n",
    "print(\"-\" * 54)\n",
    "\n",
    "for size_gb in large_sizes:\n",
    "    clear_memory()\n",
    "    \n",
    "    try:\n",
    "        tensor, actual_size, alloc_time = allocate_tensor_gb(size_gb)\n",
    "        stats = get_memory_stats()\n",
    "        \n",
    "        print(f\"{size_gb:<12} {'‚úÖ SUCCESS':<15} {alloc_time:<12.3f} {stats['reserved_gb']:<15.2f}\")\n",
    "        \n",
    "        del tensor\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(f\"{size_gb:<12} {'‚ùå FAILED':<15} {'N/A':<12} {'N/A':<15}\")\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<cell_type></cell_type>\n",
    "### üîç What This Means for AI Models\n",
    "\n",
    "**DGX Spark v2.0 Model Capacity Matrix:**\n",
    "\n",
    "| Scenario | Maximum Model Size | Memory Usage | Notes |\n",
    "|----------|-------------------|--------------|-------|\n",
    "| Full Fine-Tuning (FP16) | 12-16B | ~100-128GB | With gradient checkpointing |\n",
    "| QLoRA Fine-Tuning | 100-120B | ~50-70GB | 4-bit quantized + adapters |\n",
    "| FP16 Inference | 50-55B | ~110-120GB | Including KV cache headroom |\n",
    "| FP8 Inference | 90-100B | ~90-100GB | Native Blackwell support |\n",
    "| NVFP4 Inference | ~200B | ~100GB | Blackwell exclusive format |\n",
    "\n",
    "**Standard Memory Requirements:**\n",
    "\n",
    "| Model Size | FP16 | INT8 | INT4/NVFP4 |\n",
    "|------------|------|------|------------|\n",
    "| 7B params  | ~14 GB | ~7 GB | ~3.5 GB |\n",
    "| 13B params | ~26 GB | ~13 GB | ~6.5 GB |\n",
    "| 70B params | ~140 GB | ~70 GB | ~35 GB |\n",
    "\n",
    "The 128GB unified memory means **70B models fit entirely in INT4/NVFP4**, even without aggressive optimization!\n",
    "\n",
    "> **Note:** NVFP4 is Blackwell's native 4-bit format, offering better accuracy than standard INT4 quantization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Buffer Cache Impact\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "Linux aggressively caches disk data in RAM to speed up file access. On unified memory systems, this cache competes with GPU allocations.\n",
    "\n",
    "> **ELI5:** Imagine your shared desk has papers scattered around from previous work. Before doing a big new project, you need to clean up those old papers to make room!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current buffer cache: 19 GB\n",
      "\n",
      "‚ö†Ô∏è  Buffer cache is using 19 GB!\n",
      "   This may limit GPU memory for large models.\n",
      "   Clear with: sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'\n"
     ]
    }
   ],
   "source": [
    "# Check current buffer cache usage\n",
    "def check_buffer_cache() -> int:\n",
    "    \"\"\"\n",
    "    Check Linux buffer cache usage.\n",
    "    \n",
    "    Returns:\n",
    "        Buffer cache size in GB, or 0 if unable to determine.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['free', '-g'], \n",
    "            capture_output=True, \n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        lines = result.stdout.strip().split('\\n')\n",
    "        if len(lines) >= 2:\n",
    "            parts = lines[1].split()\n",
    "            # buff/cache is typically column 6\n",
    "            if len(parts) >= 6:\n",
    "                return int(parts[5])  # buff/cache column\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"‚ö†Ô∏è Command timed out\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error checking buffer cache: {e}\")\n",
    "    return 0\n",
    "\n",
    "cache_size = check_buffer_cache()\n",
    "print(f\"Current buffer cache: {cache_size} GB\")\n",
    "\n",
    "if cache_size > 10:\n",
    "    print(f\"\\n‚ö†Ô∏è  Buffer cache is using {cache_size} GB!\")\n",
    "    print(\"   This may limit GPU memory for large models.\")\n",
    "    print(\"   Clear with: sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Buffer cache is minimal - good for large allocations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clear buffer cache (requires sudo)\n",
    "def clear_buffer_cache() -> bool:\n",
    "    \"\"\"\n",
    "    Clear Linux buffer cache.\n",
    "    \n",
    "    NOTE: Requires sudo access.\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise.\n",
    "    \"\"\"\n",
    "    before = check_buffer_cache()\n",
    "    \n",
    "    try:\n",
    "        # This command requires sudo\n",
    "        result = subprocess.run(\n",
    "            [\"sudo\", \"sh\", \"-c\", \"sync; echo 3 > /proc/sys/vm/drop_caches\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            after = check_buffer_cache()\n",
    "            print(f\"‚úÖ Buffer cache cleared: {before} GB ‚Üí {after} GB\")\n",
    "            print(f\"   Freed approximately {before - after} GB\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Failed to clear cache (sudo required)\")\n",
    "            print(\"   Run manually: sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'\")\n",
    "            return False\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"‚ùå Command timed out\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error clearing cache: {e}\")\n",
    "        return False\n",
    "\n",
    "# Uncomment to clear cache:\n",
    "# clear_buffer_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Multiple Tensors and Memory Fragmentation\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "In real AI workloads, you don't have one giant tensor - you have many tensors for model weights, activations, gradients, optimizer states, etc. Let's simulate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Memory cleared\n",
      "\n",
      "============================================================\n",
      "Simulating Real Model Memory Usage\n",
      "============================================================\n",
      "\n",
      "Expected total: 25.5 GB\n",
      "----------------------------------------\n",
      "‚úÖ embeddings      : 2.00 GB allocated\n",
      "‚úÖ attention       : 8.00 GB allocated\n",
      "‚úÖ feedforward     : 10.00 GB allocated\n",
      "‚úÖ layer_norms     : 0.50 GB allocated\n",
      "‚úÖ output_head     : 1.00 GB allocated\n",
      "‚úÖ activations     : 4.00 GB allocated\n",
      "\n",
      "üìä After all components Memory Status:\n",
      "   Allocated: 25.50 GB\n",
      "   Reserved:  25.51 GB\n",
      "   Free:      103.02 GB\n",
      "   Total:     128.5 GB\n",
      "   [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 19.8%\n",
      "\n",
      "üìä Summary:\n",
      "   Expected: 25.5 GB\n",
      "   Allocated: 25.50 GB\n",
      "   Reserved: 25.51 GB\n",
      "   Overhead: 0.01 GB\n"
     ]
    }
   ],
   "source": [
    "clear_memory()\n",
    "\n",
    "# Simulate a typical model memory layout\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Simulating Real Model Memory Usage\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Model components (simulated 13B parameter model)\n",
    "components = {\n",
    "    'embeddings': 2.0,      # GB - embedding layers\n",
    "    'attention': 8.0,       # GB - attention weights\n",
    "    'feedforward': 10.0,    # GB - FFN layers\n",
    "    'layer_norms': 0.5,     # GB - normalization\n",
    "    'output_head': 1.0,     # GB - output projection\n",
    "    'activations': 4.0,     # GB - intermediate activations\n",
    "}\n",
    "\n",
    "total_expected = sum(components.values())\n",
    "print(f\"\\nExpected total: {total_expected:.1f} GB\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "tensors = {}\n",
    "for name, size_gb in components.items():\n",
    "    tensor, actual, alloc_time = allocate_tensor_gb(size_gb)\n",
    "    tensors[name] = tensor\n",
    "    print(f\"‚úÖ {name:15} : {actual:.2f} GB allocated\")\n",
    "\n",
    "print_memory_stats(\"After all components\")\n",
    "\n",
    "# Verify total\n",
    "stats = get_memory_stats()\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   Expected: {total_expected:.1f} GB\")\n",
    "print(f\"   Allocated: {stats['allocated_gb']:.2f} GB\")\n",
    "print(f\"   Reserved: {stats['reserved_gb']:.2f} GB\")\n",
    "print(f\"   Overhead: {(stats['reserved_gb'] - total_expected):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Memory cleared\n",
      "\n",
      "üìä After cleanup Memory Status:\n",
      "   Allocated: 4.00 GB\n",
      "   Reserved:  4.00 GB\n",
      "   Free:      124.52 GB\n",
      "   Total:     128.5 GB\n",
      "   [‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 3.1%\n"
     ]
    }
   ],
   "source": [
    "# Clean up the simulated model\n",
    "for name in list(tensors.keys()):\n",
    "    del tensors[name]\n",
    "\n",
    "clear_memory()\n",
    "print_memory_stats(\"After cleanup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Different Data Types\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "Different data types use different amounts of memory per element:\n",
    "\n",
    "| Type | Bytes | Use Case |\n",
    "|------|-------|----------|\n",
    "| float32 | 4 | Training (traditional) |\n",
    "| float16 | 2 | Inference, mixed precision |\n",
    "| bfloat16 | 2 | Training (modern, recommended for Blackwell) |\n",
    "| int8 | 1 | Quantized inference |\n",
    "| int4/NVFP4 | 0.5 | Aggressive quantization (Blackwell native) |\n",
    "\n",
    "**DGX Spark's Blackwell GPU has native support for bfloat16, FP8, and NVFP4!**\n",
    "\n",
    "> **Tip:** Use `bfloat16` as the default dtype for training and inference on DGX Spark.\n",
    "> NVFP4 (Blackwell's native 4-bit format) provides better accuracy than standard INT4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Memory cleared\n",
      "\n",
      "============================================================\n",
      "Memory Usage for 1B Elements\n",
      "============================================================\n",
      "Data Type    Bytes/Elem   Memory (GB)     Time (ms)   \n",
      "---------------------------------------------------\n",
      "üßπ Memory cleared\n",
      "float32      4            4.00            144.9       \n",
      "üßπ Memory cleared\n",
      "float16      2            2.00            71.6        \n",
      "üßπ Memory cleared\n",
      "bfloat16     2            2.00            71.6        \n",
      "üßπ Memory cleared\n",
      "int8         1            1.00            35.6        \n",
      "üßπ Memory cleared\n",
      "\n",
      "üí° Tip: Use bfloat16 as default on DGX Spark (native Blackwell support)\n"
     ]
    }
   ],
   "source": [
    "clear_memory()\n",
    "\n",
    "# Compare memory usage for different dtypes\n",
    "target_elements = 1_000_000_000  # 1 billion elements\n",
    "\n",
    "dtypes = [\n",
    "    ('float32', torch.float32),\n",
    "    ('float16', torch.float16),\n",
    "    ('bfloat16', torch.bfloat16),\n",
    "    ('int8', torch.int8),\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Memory Usage for {target_elements/1e9:.0f}B Elements\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Data Type':<12} {'Bytes/Elem':<12} {'Memory (GB)':<15} {'Time (ms)':<12}\")\n",
    "print(\"-\" * 51)\n",
    "\n",
    "for name, dtype in dtypes:\n",
    "    clear_memory()\n",
    "    \n",
    "    start = time.time()\n",
    "    tensor = torch.empty(target_elements, dtype=dtype, device='cuda')\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = (time.time() - start) * 1000\n",
    "    \n",
    "    size_gb = tensor.element_size() * tensor.nelement() / 1e9\n",
    "    bytes_per = tensor.element_size()\n",
    "    \n",
    "    print(f\"{name:<12} {bytes_per:<12} {size_gb:<15.2f} {elapsed:<12.1f}\")\n",
    "    \n",
    "    del tensor\n",
    "\n",
    "clear_memory()\n",
    "print(\"\\nüí° Tip: Use bfloat16 as default on DGX Spark (native Blackwell support)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself #2\n",
    "\n",
    "Calculate how much memory a 70B parameter model needs in different precisions.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "- 70B parameters = 70,000,000,000 elements\n",
    "- float32: 70B √ó 4 bytes = 280 GB (doesn't fit!)\n",
    "- float16: 70B √ó 2 bytes = 140 GB (barely fits)\n",
    "- int4: 70B √ó 0.5 bytes = 35 GB (fits easily!)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Calculate memory requirements for 70B model\n",
    "\n",
    "params_70b = 70_000_000_000  # 70 billion parameters\n",
    "\n",
    "# Calculate memory for each precision:\n",
    "# memory_fp32 = ...\n",
    "# memory_fp16 = ...\n",
    "# memory_int4 = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Not clearing memory between experiments\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong way - Memory accumulates\n",
    "tensor1 = torch.randn(10000, 10000, device='cuda')\n",
    "tensor2 = torch.randn(10000, 10000, device='cuda')  # Uses MORE memory\n",
    "\n",
    "# ‚úÖ Right way - Clean up first\n",
    "tensor1 = torch.randn(10000, 10000, device='cuda')\n",
    "del tensor1\n",
    "torch.cuda.empty_cache()\n",
    "tensor2 = torch.randn(10000, 10000, device='cuda')  # Reuses memory\n",
    "```\n",
    "\n",
    "### Mistake 2: Forgetting about buffer cache for large models\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong way - May fail with OOM on 70B model\n",
    "model = load_model(\"70b-model\")\n",
    "\n",
    "# ‚úÖ Right way - Clear cache first\n",
    "!sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'\n",
    "model = load_model(\"70b-model\")\n",
    "```\n",
    "\n",
    "### Mistake 3: Using float32 when you don't need it\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong way - Wastes memory\n",
    "model = model.to(device='cuda', dtype=torch.float32)\n",
    "\n",
    "# ‚úÖ Right way - Use bfloat16 on Blackwell\n",
    "model = model.to(device='cuda', dtype=torch.bfloat16)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How unified memory enables allocations impossible on other GPUs\n",
    "- ‚úÖ How to monitor GPU memory with PyTorch\n",
    "- ‚úÖ The impact of buffer cache on available memory\n",
    "- ‚úÖ How different data types affect memory usage\n",
    "- ‚úÖ Why DGX Spark can run 70B parameter models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "Create a \"memory stress test\" that:\n",
    "1. Allocates tensors in a loop until allocation fails\n",
    "2. Records the maximum memory achieved\n",
    "3. Cleans up gracefully\n",
    "\n",
    "<details>\n",
    "<summary>üí° Solution Hint</summary>\n",
    "\n",
    "```python\n",
    "def memory_stress_test(increment_gb=5):\n",
    "    tensors = []\n",
    "    total_gb = 0\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            t, size, _ = allocate_tensor_gb(increment_gb)\n",
    "            tensors.append(t)\n",
    "            total_gb += size\n",
    "            print(f\"Allocated: {total_gb:.1f} GB\")\n",
    "    except RuntimeError:\n",
    "        print(f\"Max allocation: {total_gb:.1f} GB\")\n",
    "    finally:\n",
    "        for t in tensors:\n",
    "            del t\n",
    "        clear_memory()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [NVIDIA Unified Memory Architecture](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/)\n",
    "- [PyTorch CUDA Memory Management](https://pytorch.org/docs/stable/notes/cuda.html)\n",
    "- [Blackwell Architecture Whitepaper](https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Memory cleared\n",
      "\n",
      "üìä Final Memory Status:\n",
      "   Allocated: 0.00 GB\n",
      "   Reserved:  0.00 GB\n",
      "   Free:      128.52 GB\n",
      "   Total:     128.5 GB\n",
      "   [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 0.0%\n",
      "\n",
      "============================================================\n",
      "üéâ Great job completing Lab 1.1.2: Memory Architecture Lab!\n",
      "============================================================\n",
      "\n",
      "Next up: Lab 1.1.3 - NGC Container Setup\n",
      "You'll learn to configure Docker containers for AI development.\n"
     ]
    }
   ],
   "source": [
    "# Final cleanup\n",
    "clear_memory()\n",
    "print_memory_stats(\"Final\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ Great job completing Lab 1.1.2: Memory Architecture Lab!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nNext up: Lab 1.1.3 - NGC Container Setup\")\n",
    "print(\"You'll learn to configure Docker containers for AI development.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
