{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2: Memory Architecture Lab\n",
    "\n",
    "**Module:** 1 - DGX Spark Platform Mastery  \n",
    "**Time:** 1.5 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand how unified memory works on DGX Spark\n",
    "- [ ] Allocate and monitor GPU tensors of various sizes\n",
    "- [ ] Identify the memory limits and behavior patterns\n",
    "- [ ] Learn to clear buffer cache for optimal memory availability\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Task 1.1 (System Exploration)\n",
    "- Running inside NGC PyTorch container\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "When you want to run a 70 billion parameter language model, you need about 35-140GB of memory depending on quantization. Traditional GPUs max out at 24-80GB. The DGX Spark's 128GB unified memory is a game-changer because:\n",
    "\n",
    "1. **No memory copy overhead** - Data doesn't need to transfer between CPU and GPU\n",
    "2. **Larger models fit** - Run models that wouldn't fit on traditional GPUs\n",
    "3. **Simpler programming** - No need to manage separate memory pools\n",
    "\n",
    "In this lab, you'll see this in action by allocating tensors of various sizes and watching how memory behaves.\n",
    "\n",
    "---\n",
    "\n",
    "## üßí ELI5: Unified Memory\n",
    "\n",
    "> **Imagine you have two desks in your room...**\n",
    ">\n",
    "> One desk (CPU) is where you read books and make plans. The other desk (GPU) is where you do thousands of math problems super fast. Normally, if you need to use something from one desk on the other, you have to walk over and carry it - which takes time!\n",
    ">\n",
    "> **Unified memory is like pushing both desks together into one GIANT desk.** Now everything is right there - no more walking back and forth! Both \"workers\" (CPU and GPU) can reach everything instantly.\n",
    ">\n",
    "> That's why DGX Spark can load huge AI brains (models) that wouldn't fit on a normal GPU desk - because it has one enormous shared desk (128GB of memory)!\n",
    ">\n",
    "> **In AI terms:** The CPU and GPU share the same physical memory pool, eliminating the PCIe transfer bottleneck that limits traditional systems.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ‚ö†Ô∏è Important: Run This Notebook in NGC Container\n\nThis notebook **must** be run inside an NGC PyTorch container. If you're not already in one, start it with:\n\n```bash\ndocker run --gpus all -it --rm \\\n    -v $HOME/workspace:/workspace \\\n    -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\n    -p 8888:8888 \\\n    --ipc=host \\\n    nvcr.io/nvidia/pytorch:25.11-py3 \\\n    jupyter lab --ip=0.0.0.0 --allow-root --no-browser\n```\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Verify PyTorch CUDA Setup\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "First, let's confirm we have GPU access through PyTorch. This is the most critical check for any deep learning work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Check CUDA availability\n",
    "print(\"=\" * 60)\n",
    "print(\"PyTorch CUDA Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Memory info\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"\\nGPU Memory: {props.total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"Compute capability: {props.major}.{props.minor}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå CUDA is not available!\")\n",
    "    print(\"Make sure you're running inside an NGC container with --gpus all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test: Create a tensor on GPU\n",
    "if torch.cuda.is_available():\n",
    "    x = torch.randn(1000, 1000, device='cuda')\n",
    "    print(f\"‚úÖ Successfully created tensor on GPU\")\n",
    "    print(f\"   Shape: {x.shape}\")\n",
    "    print(f\"   Device: {x.device}\")\n",
    "    print(f\"   Memory used: {x.element_size() * x.nelement() / 1e6:.2f} MB\")\n",
    "    del x\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Memory Monitoring Utilities\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "PyTorch provides detailed memory tracking. Let's create helper functions to monitor memory usage throughout our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_stats():\n",
    "    \"\"\"\n",
    "    Get current GPU memory statistics.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Memory statistics in GB\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    \n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    max_allocated = torch.cuda.max_memory_allocated() / 1e9\n",
    "    max_reserved = torch.cuda.max_memory_reserved() / 1e9\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    return {\n",
    "        'allocated_gb': allocated,\n",
    "        'reserved_gb': reserved,\n",
    "        'max_allocated_gb': max_allocated,\n",
    "        'max_reserved_gb': max_reserved,\n",
    "        'total_gb': total,\n",
    "        'free_gb': total - reserved\n",
    "    }\n",
    "\n",
    "def print_memory_stats(label=\"Current\"):\n",
    "    \"\"\"\n",
    "    Print formatted memory statistics.\n",
    "    \"\"\"\n",
    "    stats = get_memory_stats()\n",
    "    if stats:\n",
    "        print(f\"\\nüìä {label} Memory Status:\")\n",
    "        print(f\"   Allocated: {stats['allocated_gb']:.2f} GB\")\n",
    "        print(f\"   Reserved:  {stats['reserved_gb']:.2f} GB\")\n",
    "        print(f\"   Free:      {stats['free_gb']:.2f} GB\")\n",
    "        print(f\"   Total:     {stats['total_gb']:.1f} GB\")\n",
    "        \n",
    "        # Visual bar\n",
    "        used_pct = stats['reserved_gb'] / stats['total_gb'] * 100\n",
    "        bar_len = 40\n",
    "        filled = int(bar_len * used_pct / 100)\n",
    "        bar = '‚ñà' * filled + '‚ñë' * (bar_len - filled)\n",
    "        print(f\"   [{bar}] {used_pct:.1f}%\")\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"\n",
    "    Clear GPU memory cache.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    print(\"üßπ Memory cleared\")\n",
    "\n",
    "# Test our functions\n",
    "print_memory_stats(\"Initial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Understanding Memory Types\n",
    "\n",
    "- **Allocated**: Memory actually used by tensors\n",
    "- **Reserved**: Memory reserved by PyTorch (includes caching)\n",
    "- **Free**: Memory available for new allocations\n",
    "- **Total**: Total GPU memory (128GB on DGX Spark)\n",
    "\n",
    "PyTorch reserves more memory than strictly needed to speed up future allocations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Small Tensor Allocations (1-10 GB)\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "Let's start with allocations that would fit on any modern GPU. We'll create tensors of increasing size and observe memory behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any existing allocations\n",
    "clear_memory()\n",
    "\n",
    "def allocate_tensor_gb(size_gb: float, dtype=torch.float32):\n",
    "    \"\"\"\n",
    "    Allocate a tensor of approximately the specified size in GB.\n",
    "    \n",
    "    Args:\n",
    "        size_gb: Desired size in gigabytes\n",
    "        dtype: Data type (default: float32 = 4 bytes per element)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (tensor, actual_size_gb, time_taken)\n",
    "    \"\"\"\n",
    "    bytes_per_element = torch.tensor([], dtype=dtype).element_size()\n",
    "    num_elements = int(size_gb * 1e9 / bytes_per_element)\n",
    "    \n",
    "    # Use a 1D tensor for simplicity\n",
    "    start_time = time.time()\n",
    "    tensor = torch.empty(num_elements, dtype=dtype, device='cuda')\n",
    "    torch.cuda.synchronize()  # Wait for allocation to complete\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    actual_size = tensor.element_size() * tensor.nelement() / 1e9\n",
    "    \n",
    "    return tensor, actual_size, elapsed\n",
    "\n",
    "# Test with 1GB\n",
    "print(\"Testing 1GB allocation...\")\n",
    "tensor_1gb, size, alloc_time = allocate_tensor_gb(1.0)\n",
    "print(f\"‚úÖ Allocated {size:.2f} GB in {alloc_time*1000:.1f} ms\")\n",
    "print_memory_stats(\"After 1GB\")\n",
    "\n",
    "# Clean up\n",
    "del tensor_1gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's test a range of sizes\n",
    "clear_memory()\n",
    "\n",
    "test_sizes = [1, 2, 4, 8, 10]  # GB\n",
    "results = []\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Small Tensor Allocation Test (1-10 GB)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Size (GB)':<12} {'Alloc Time (ms)':<18} {'Memory Used (GB)':<18}\")\n",
    "print(\"-\" * 48)\n",
    "\n",
    "for size_gb in test_sizes:\n",
    "    clear_memory()\n",
    "    \n",
    "    try:\n",
    "        tensor, actual_size, alloc_time = allocate_tensor_gb(size_gb)\n",
    "        stats = get_memory_stats()\n",
    "        \n",
    "        results.append({\n",
    "            'requested_gb': size_gb,\n",
    "            'actual_gb': actual_size,\n",
    "            'alloc_ms': alloc_time * 1000,\n",
    "            'reserved_gb': stats['reserved_gb']\n",
    "        })\n",
    "        \n",
    "        print(f\"{size_gb:<12} {alloc_time*1000:<18.2f} {stats['reserved_gb']:<18.2f}\")\n",
    "        \n",
    "        del tensor\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(f\"{size_gb:<12} FAILED: {str(e)[:40]}\")\n",
    "\n",
    "clear_memory()\n",
    "print(\"\\n‚úÖ All small allocations successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "Notice that:\n",
    "1. **Allocation is fast** - No data transfer between CPU and GPU\n",
    "2. **Reserved > Allocated** - PyTorch caches memory for efficiency\n",
    "3. **Linear scaling** - Memory usage scales linearly with tensor size\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Medium Tensor Allocations (20-50 GB)\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "Now we enter territory that would fail on most GPUs! A typical RTX 4090 has 24GB. Let's allocate 30, 40, and 50 GB tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_memory()\n",
    "\n",
    "medium_sizes = [20, 30, 40, 50]  # GB\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Medium Tensor Allocation Test (20-50 GB)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚ö†Ô∏è  These sizes would FAIL on most consumer GPUs!\")\n",
    "print(f\"{'Size (GB)':<12} {'Alloc Time (ms)':<18} {'Memory Used (GB)':<18}\")\n",
    "print(\"-\" * 48)\n",
    "\n",
    "for size_gb in medium_sizes:\n",
    "    clear_memory()\n",
    "    \n",
    "    try:\n",
    "        tensor, actual_size, alloc_time = allocate_tensor_gb(size_gb)\n",
    "        stats = get_memory_stats()\n",
    "        \n",
    "        print(f\"{size_gb:<12} {alloc_time*1000:<18.2f} {stats['reserved_gb']:<18.2f}\")\n",
    "        \n",
    "        del tensor\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(f\"{size_gb:<12} ‚ùå FAILED: Out of memory\")\n",
    "        print(f\"   Hint: Clear buffer cache and try again\")\n",
    "\n",
    "clear_memory()\n",
    "print(\"\\nüéâ DGX Spark handled these like a champ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself #1\n",
    "\n",
    "What's the largest single tensor you can allocate? Try sizes between 60-100 GB.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "You should be able to allocate up to ~100GB, but the exact limit depends on:\n",
    "- Current buffer cache usage\n",
    "- Other running processes\n",
    "- PyTorch's memory overhead\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Try allocating larger tensors\n",
    "# What's the maximum size you can allocate?\n",
    "\n",
    "clear_memory()\n",
    "\n",
    "# Try these sizes:\n",
    "# large_sizes = [60, 70, 80, 90, 100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Large Tensor Allocations (70-100 GB)\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "This is where the DGX Spark truly shines. We can allocate tensors that represent full 70B parameter models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_memory()\n",
    "\n",
    "# Before large allocations, let's check system memory\n",
    "import subprocess\n",
    "\n",
    "def get_system_memory():\n",
    "    \"\"\"Get system memory info.\"\"\"\n",
    "    result = subprocess.run(['free', '-g'], capture_output=True, text=True)\n",
    "    lines = result.stdout.strip().split('\\n')\n",
    "    if len(lines) >= 2:\n",
    "        parts = lines[1].split()\n",
    "        return {\n",
    "            'total': int(parts[1]),\n",
    "            'used': int(parts[2]),\n",
    "            'free': int(parts[3]),\n",
    "            'available': int(parts[6]) if len(parts) > 6 else int(parts[3])\n",
    "        }\n",
    "    return None\n",
    "\n",
    "sys_mem = get_system_memory()\n",
    "print(\"System Memory Status:\")\n",
    "print(f\"  Total:     {sys_mem['total']} GB\")\n",
    "print(f\"  Available: {sys_mem['available']} GB\")\n",
    "print(f\"  Used:      {sys_mem['used']} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large allocations\n",
    "clear_memory()\n",
    "\n",
    "large_sizes = [60, 70, 80]  # GB - Conservative to ensure success\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Large Tensor Allocation Test (60-80 GB)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üöÄ This is IMPOSSIBLE on most GPUs!\")\n",
    "print(f\"{'Size (GB)':<12} {'Status':<15} {'Time (s)':<12} {'Memory (GB)':<15}\")\n",
    "print(\"-\" * 54)\n",
    "\n",
    "for size_gb in large_sizes:\n",
    "    clear_memory()\n",
    "    \n",
    "    try:\n",
    "        tensor, actual_size, alloc_time = allocate_tensor_gb(size_gb)\n",
    "        stats = get_memory_stats()\n",
    "        \n",
    "        print(f\"{size_gb:<12} {'‚úÖ SUCCESS':<15} {alloc_time:<12.3f} {stats['reserved_gb']:<15.2f}\")\n",
    "        \n",
    "        del tensor\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(f\"{size_gb:<12} {'‚ùå FAILED':<15} {'N/A':<12} {'N/A':<15}\")\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What This Means for AI Models\n",
    "\n",
    "| Model Size | Memory Needed (FP16) | Memory Needed (INT4) | DGX Spark? |\n",
    "|------------|---------------------|---------------------|------------|\n",
    "| 7B params  | ~14 GB              | ~3.5 GB             | ‚úÖ Easy    |\n",
    "| 13B params | ~26 GB              | ~7 GB               | ‚úÖ Easy    |\n",
    "| 30B params | ~60 GB              | ~15 GB              | ‚úÖ Works   |\n",
    "| 70B params | ~140 GB             | ~35 GB              | ‚úÖ Works!  |\n",
    "\n",
    "The 128GB unified memory means **70B models fit entirely**, even without aggressive quantization!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Buffer Cache Impact\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "Linux aggressively caches disk data in RAM to speed up file access. On unified memory systems, this cache competes with GPU allocations.\n",
    "\n",
    "> **ELI5:** Imagine your shared desk has papers scattered around from previous work. Before doing a big new project, you need to clean up those old papers to make room!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current buffer cache usage\n",
    "def check_buffer_cache():\n",
    "    \"\"\"Check Linux buffer cache usage.\"\"\"\n",
    "    result = subprocess.run(['free', '-g'], capture_output=True, text=True)\n",
    "    lines = result.stdout.strip().split('\\n')\n",
    "    if len(lines) >= 2:\n",
    "        parts = lines[1].split()\n",
    "        # buff/cache is typically column 6\n",
    "        if len(parts) >= 6:\n",
    "            return int(parts[5])  # buff/cache column\n",
    "    return 0\n",
    "\n",
    "cache_size = check_buffer_cache()\n",
    "print(f\"Current buffer cache: {cache_size} GB\")\n",
    "\n",
    "if cache_size > 10:\n",
    "    print(f\"\\n‚ö†Ô∏è  Buffer cache is using {cache_size} GB!\")\n",
    "    print(\"   This may limit GPU memory for large models.\")\n",
    "    print(\"   Clear with: sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Buffer cache is minimal - good for large allocations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clear buffer cache (requires sudo)\n",
    "def clear_buffer_cache():\n",
    "    \"\"\"\n",
    "    Clear Linux buffer cache.\n",
    "    NOTE: Requires sudo access.\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    \n",
    "    before = check_buffer_cache()\n",
    "    \n",
    "    # This command requires sudo\n",
    "    result = subprocess.run(\n",
    "        [\"sudo\", \"sh\", \"-c\", \"sync; echo 3 > /proc/sys/vm/drop_caches\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        after = check_buffer_cache()\n",
    "        print(f\"‚úÖ Buffer cache cleared: {before} GB ‚Üí {after} GB\")\n",
    "        print(f\"   Freed approximately {before - after} GB\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to clear cache (sudo required)\")\n",
    "        print(\"   Run manually: sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'\")\n",
    "\n",
    "# Uncomment to clear cache:\n",
    "# clear_buffer_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Multiple Tensors and Memory Fragmentation\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "In real AI workloads, you don't have one giant tensor - you have many tensors for model weights, activations, gradients, optimizer states, etc. Let's simulate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_memory()\n",
    "\n",
    "# Simulate a typical model memory layout\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Simulating Real Model Memory Usage\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Model components (simulated 13B parameter model)\n",
    "components = {\n",
    "    'embeddings': 2.0,      # GB - embedding layers\n",
    "    'attention': 8.0,       # GB - attention weights\n",
    "    'feedforward': 10.0,    # GB - FFN layers\n",
    "    'layer_norms': 0.5,     # GB - normalization\n",
    "    'output_head': 1.0,     # GB - output projection\n",
    "    'activations': 4.0,     # GB - intermediate activations\n",
    "}\n",
    "\n",
    "total_expected = sum(components.values())\n",
    "print(f\"\\nExpected total: {total_expected:.1f} GB\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "tensors = {}\n",
    "for name, size_gb in components.items():\n",
    "    tensor, actual, alloc_time = allocate_tensor_gb(size_gb)\n",
    "    tensors[name] = tensor\n",
    "    print(f\"‚úÖ {name:15} : {actual:.2f} GB allocated\")\n",
    "\n",
    "print_memory_stats(\"After all components\")\n",
    "\n",
    "# Verify total\n",
    "stats = get_memory_stats()\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   Expected: {total_expected:.1f} GB\")\n",
    "print(f\"   Allocated: {stats['allocated_gb']:.2f} GB\")\n",
    "print(f\"   Reserved: {stats['reserved_gb']:.2f} GB\")\n",
    "print(f\"   Overhead: {(stats['reserved_gb'] - total_expected):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the simulated model\n",
    "for name in list(tensors.keys()):\n",
    "    del tensors[name]\n",
    "\n",
    "clear_memory()\n",
    "print_memory_stats(\"After cleanup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Different Data Types\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "Different data types use different amounts of memory per element:\n",
    "\n",
    "| Type | Bytes | Use Case |\n",
    "|------|-------|----------|\n",
    "| float32 | 4 | Training (traditional) |\n",
    "| float16 | 2 | Inference, mixed precision |\n",
    "| bfloat16 | 2 | Training (modern) |\n",
    "| int8 | 1 | Quantized inference |\n",
    "| int4 | 0.5 | Aggressive quantization |\n",
    "\n",
    "**DGX Spark's Blackwell GPU has native support for bfloat16 and FP4!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_memory()\n",
    "\n",
    "# Compare memory usage for different dtypes\n",
    "target_elements = 1_000_000_000  # 1 billion elements\n",
    "\n",
    "dtypes = [\n",
    "    ('float32', torch.float32),\n",
    "    ('float16', torch.float16),\n",
    "    ('bfloat16', torch.bfloat16),\n",
    "    ('int8', torch.int8),\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Memory Usage for {target_elements/1e9:.0f}B Elements\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Data Type':<12} {'Bytes/Elem':<12} {'Memory (GB)':<15} {'Time (ms)':<12}\")\n",
    "print(\"-\" * 51)\n",
    "\n",
    "for name, dtype in dtypes:\n",
    "    clear_memory()\n",
    "    \n",
    "    start = time.time()\n",
    "    tensor = torch.empty(target_elements, dtype=dtype, device='cuda')\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = (time.time() - start) * 1000\n",
    "    \n",
    "    size_gb = tensor.element_size() * tensor.nelement() / 1e9\n",
    "    bytes_per = tensor.element_size()\n",
    "    \n",
    "    print(f\"{name:<12} {bytes_per:<12} {size_gb:<15.2f} {elapsed:<12.1f}\")\n",
    "    \n",
    "    del tensor\n",
    "\n",
    "clear_memory()\n",
    "print(\"\\nüí° Tip: Use bfloat16 as default on DGX Spark (native Blackwell support)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself #2\n",
    "\n",
    "Calculate how much memory a 70B parameter model needs in different precisions.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "- 70B parameters = 70,000,000,000 elements\n",
    "- float32: 70B √ó 4 bytes = 280 GB (doesn't fit!)\n",
    "- float16: 70B √ó 2 bytes = 140 GB (barely fits)\n",
    "- int4: 70B √ó 0.5 bytes = 35 GB (fits easily!)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Calculate memory requirements for 70B model\n",
    "\n",
    "params_70b = 70_000_000_000  # 70 billion parameters\n",
    "\n",
    "# Calculate memory for each precision:\n",
    "# memory_fp32 = ...\n",
    "# memory_fp16 = ...\n",
    "# memory_int4 = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Not clearing memory between experiments\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong way - Memory accumulates\n",
    "tensor1 = torch.randn(10000, 10000, device='cuda')\n",
    "tensor2 = torch.randn(10000, 10000, device='cuda')  # Uses MORE memory\n",
    "\n",
    "# ‚úÖ Right way - Clean up first\n",
    "tensor1 = torch.randn(10000, 10000, device='cuda')\n",
    "del tensor1\n",
    "torch.cuda.empty_cache()\n",
    "tensor2 = torch.randn(10000, 10000, device='cuda')  # Reuses memory\n",
    "```\n",
    "\n",
    "### Mistake 2: Forgetting about buffer cache for large models\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong way - May fail with OOM on 70B model\n",
    "model = load_model(\"70b-model\")\n",
    "\n",
    "# ‚úÖ Right way - Clear cache first\n",
    "!sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'\n",
    "model = load_model(\"70b-model\")\n",
    "```\n",
    "\n",
    "### Mistake 3: Using float32 when you don't need it\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong way - Wastes memory\n",
    "model = model.to(device='cuda', dtype=torch.float32)\n",
    "\n",
    "# ‚úÖ Right way - Use bfloat16 on Blackwell\n",
    "model = model.to(device='cuda', dtype=torch.bfloat16)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How unified memory enables allocations impossible on other GPUs\n",
    "- ‚úÖ How to monitor GPU memory with PyTorch\n",
    "- ‚úÖ The impact of buffer cache on available memory\n",
    "- ‚úÖ How different data types affect memory usage\n",
    "- ‚úÖ Why DGX Spark can run 70B parameter models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "Create a \"memory stress test\" that:\n",
    "1. Allocates tensors in a loop until allocation fails\n",
    "2. Records the maximum memory achieved\n",
    "3. Cleans up gracefully\n",
    "\n",
    "<details>\n",
    "<summary>üí° Solution Hint</summary>\n",
    "\n",
    "```python\n",
    "def memory_stress_test(increment_gb=5):\n",
    "    tensors = []\n",
    "    total_gb = 0\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            t, size, _ = allocate_tensor_gb(increment_gb)\n",
    "            tensors.append(t)\n",
    "            total_gb += size\n",
    "            print(f\"Allocated: {total_gb:.1f} GB\")\n",
    "    except RuntimeError:\n",
    "        print(f\"Max allocation: {total_gb:.1f} GB\")\n",
    "    finally:\n",
    "        for t in tensors:\n",
    "            del t\n",
    "        clear_memory()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [NVIDIA Unified Memory Architecture](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/)\n",
    "- [PyTorch CUDA Memory Management](https://pytorch.org/docs/stable/notes/cuda.html)\n",
    "- [Blackwell Architecture Whitepaper](https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup\n",
    "clear_memory()\n",
    "print_memory_stats(\"Final\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ Great job completing Task 1.2: Memory Architecture Lab!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nNext up: Task 1.3 - NGC Container Setup\")\n",
    "print(\"You'll learn to configure Docker containers for AI development.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}