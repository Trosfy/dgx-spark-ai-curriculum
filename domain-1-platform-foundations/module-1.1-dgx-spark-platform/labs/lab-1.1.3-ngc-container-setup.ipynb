{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.1.3: NGC Container Setup\n",
    "\n",
    "**Module:** 1.1 - DGX Spark Platform Mastery  \n",
    "**Time:** 1.5 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand why NGC containers are essential for DGX Spark\n",
    "- [ ] Pull and configure the PyTorch NGC container\n",
    "- [ ] Create a docker-compose.yml for easy development\n",
    "- [ ] Verify GPU access inside containers\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Lab 1.1.1 (System Exploration)\n",
    "- Docker installed and running\n",
    "- Internet access for container pulls\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "Here's a frustrating scenario every AI developer has faced:\n",
    "\n",
    "1. Install PyTorch with `pip install torch`\n",
    "2. Run `torch.cuda.is_available()` ‚Üí Returns `False`\n",
    "3. Spend hours debugging driver issues, CUDA versions, wheel compatibility...\n",
    "\n",
    "On DGX Spark, this is even worse because **standard pip wheels don't work at all** - they're built for x86, not ARM64.\n",
    "\n",
    "**NGC containers solve this completely.** They're pre-built, tested, and optimized by NVIDIA. Just pull and run!\n",
    "\n",
    "---\n",
    "\n",
    "## üßí ELI5: What are NGC Containers?\n",
    "\n",
    "> **Imagine you want to bake a really complicated cake...**\n",
    ">\n",
    "> You could buy all the ingredients separately, find the right recipe, measure everything perfectly, and hope it works. Or... you could get a \"cake kit\" where everything is pre-measured and packaged together!\n",
    ">\n",
    "> **NGC containers are like cake kits for AI.** NVIDIA has already:\n",
    "> - Compiled PyTorch correctly for your hardware\n",
    "> - Set up CUDA and cuDNN perfectly\n",
    "> - Tested everything to make sure it works\n",
    "> - Optimized it for maximum speed\n",
    ">\n",
    "> You just \"open the box\" (pull the container) and start cooking (coding)!\n",
    ">\n",
    "> **In AI terms:** NGC containers are Docker images pre-configured with AI frameworks, drivers, and optimizations for NVIDIA hardware.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Why NGC Containers are Required\n",
    "\n",
    "### The ARM64 + CUDA Challenge\n",
    "\n",
    "DGX Spark uses:\n",
    "- **ARM64 CPU** (not x86_64 like most computers)\n",
    "- **CUDA 13+** (cutting edge)\n",
    "- **Blackwell GPU** (brand new architecture)\n",
    "\n",
    "Standard PyPI wheels are compiled for `x86_64 + CUDA 11/12`. They simply **cannot run** on DGX Spark.\n",
    "\n",
    "Let's demonstrate this problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture: aarch64\n",
      "System: Linux\n",
      "\n",
      "‚ö†Ô∏è  You're on ARM64 - standard pip PyTorch won't work!\n",
      "   You MUST use NGC containers for GPU support.\n"
     ]
    }
   ],
   "source": [
    "# Check our architecture\n",
    "import platform\n",
    "\n",
    "arch = platform.machine()\n",
    "system = platform.system()\n",
    "\n",
    "print(f\"Architecture: {arch}\")\n",
    "print(f\"System: {system}\")\n",
    "\n",
    "if arch == 'aarch64':\n",
    "    print(\"\\n‚ö†Ô∏è  You're on ARM64 - standard pip PyTorch won't work!\")\n",
    "    print(\"   You MUST use NGC containers for GPU support.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ You're on x86_64 - but NGC containers are still recommended.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker version 28.5.1, build e180ab8\n"
     ]
    }
   ],
   "source": [
    "# Check Docker status\n",
    "!docker --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  cdi: nvidia.com/gpu=0\n",
      "  cdi: nvidia.com/gpu=GPU-3d29cd16-ca97-2c38-7d79-d462cfa45fed\n",
      "  cdi: nvidia.com/gpu=all\n",
      " Runtimes: runc io.containerd.runc.v2 nvidia\n",
      " Kernel Version: 6.14.0-1015-nvidia\n",
      "\n",
      "---\n",
      "cli-version: 1.18.1\n",
      "lib-version: 1.18.1\n",
      "build date: 2025-11-24T14:47+00:00\n",
      "build revision: 889a3bb5408c195ed7897ba2cb8341c7d249672f\n",
      "build compiler: aarch64-linux-gnu-gcc-7 7.5.0\n",
      "build platform: aarch64\n",
      "build flags: -D_GNU_SOURCE -D_FORTIFY_SOURCE=2 -DNDEBUG -std=gnu11 -O2 -g -fdata-sections -ffunction-sections -fplan9-extensions -fstack-protector -fno-strict-aliasing -fvisibility=hidden -Wall -Wextra -Wcast-align -Wpointer-arith -Wmissing-prototypes -Wnonnull -Wwrite-strings -Wlogical-op -Wformat=2 -Wmissing-format-attribute -Winit-self -Wshadow -Wstrict-prototypes -Wunreachable-code -Wconversion -Wsign-conversion -Wno-unknown-warning-option -Wno-format-extra-args -Wno-gnu-alignof-expression -Wl,-zrelro -Wl,-znow -Wl,-zdefs -Wl,--gc-sections\n"
     ]
    }
   ],
   "source": [
    "# Check NVIDIA container toolkit\n",
    "!docker info 2>/dev/null | grep -i nvidia\n",
    "print(\"\\n---\")\n",
    "!nvidia-container-cli --version 2>/dev/null || echo \"nvidia-container-cli not directly accessible\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding NGC Container Catalog\n",
    "\n",
    "### Available Containers\n",
    "\n",
    "The NGC Catalog (https://catalog.ngc.nvidia.com/) offers many pre-built containers:\n",
    "\n",
    "| Container | Use Case | Image |\n",
    "|-----------|----------|-------|\n",
    "| PyTorch | Deep learning | `nvcr.io/nvidia/pytorch:25.11-py3` |\n",
    "| TensorFlow | Deep learning | `nvcr.io/nvidia/tensorflow:25.11-tf2-py3` |\n",
    "| Triton | Inference server | `nvcr.io/nvidia/tritonserver:25.11-py3` |\n",
    "| NeMo | NLP/ASR | `nvcr.io/nvidia/nemo:25.11` |\n",
    "| RAPIDS | Data science | `nvcr.io/nvidia/rapidsai/base:25.11-cuda13.0-py3.11` |\n",
    "\n",
    "### Version Naming\n",
    "\n",
    "NGC uses `YY.MM` versioning:\n",
    "- `25.11` = November 2025 release\n",
    "- `25.06` = June 2025 release\n",
    "\n",
    "**Always use the latest compatible version for best performance!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently downloaded NVIDIA containers:\n",
      "==================================================\n",
      "REPOSITORY                                TAG                        IMAGE ID       CREATED         SIZE\n",
      "nvcr.io/nvidia/cuda                       13.0.1-devel-ubuntu24.04   d1f3dc428c53   3 months ago    6.59GB\n"
     ]
    }
   ],
   "source": [
    "# List currently downloaded NGC containers\n",
    "print(\"Currently downloaded NVIDIA containers:\")\n",
    "print(\"=\" * 50)\n",
    "!docker images | grep -E \"nvcr.io|REPOSITORY\" | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Pulling the PyTorch Container\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "The PyTorch NGC container is the most commonly used for AI development. It includes:\n",
    "- PyTorch (latest version, compiled for your GPU)\n",
    "- CUDA and cuDNN\n",
    "- TensorRT for inference optimization\n",
    "- Common libraries (numpy, pandas, etc.)\n",
    "- Jupyter Lab\n",
    "\n",
    "**Note:** This cell will download ~20GB+ the first time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target container: nvcr.io/nvidia/pytorch:25.11-py3\n",
      "\n",
      "First pull can take 10-30 minutes depending on connection speed!\n",
      "\n",
      "To check for newer versions:\n",
      "  Visit: https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# NGC CONTAINER VERSION CONFIGURATION\n",
    "# =============================================================================\n",
    "# Update this variable when newer NGC containers become available.\n",
    "# Check https://catalog.ngc.nvidia.com for the latest versions.\n",
    "# Version format: YY.MM (e.g., 25.11 = November 2025 release)\n",
    "# =============================================================================\n",
    "\n",
    "PYTORCH_IMAGE = \"nvcr.io/nvidia/pytorch:25.11-py3\"\n",
    "\n",
    "print(f\"Target container: {PYTORCH_IMAGE}\")\n",
    "print(\"\\nFirst pull can take 10-30 minutes depending on connection speed!\")\n",
    "print(\"\\nTo check for newer versions:\")\n",
    "print(\"  Visit: https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container not found locally.\n",
      "Run this command to pull it:\n",
      "\n",
      "    docker pull nvcr.io/nvidia/pytorch:25.11-py3\n"
     ]
    }
   ],
   "source": [
    "# Check if already downloaded\n",
    "import subprocess\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"docker\", \"images\", \"-q\", PYTORCH_IMAGE],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.stdout.strip():\n",
    "    print(f\"‚úÖ Container already downloaded!\")\n",
    "    print(f\"   Image ID: {result.stdout.strip()}\")\n",
    "else:\n",
    "    print(f\"Container not found locally.\")\n",
    "    print(f\"Run this command to pull it:\")\n",
    "    print(f\"\\n    docker pull {PYTORCH_IMAGE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself #1\n",
    "\n",
    "Pull the PyTorch container. Run this in a terminal (not this notebook) to see progress:\n",
    "\n",
    "```bash\n",
    "docker pull nvcr.io/nvidia/pytorch:25.11-py3\n",
    "```\n",
    "\n",
    "Or uncomment and run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.11-py3: Pulling from nvidia/pytorch\n",
      "\n",
      "\u001b[1B5db46e38: Pulling fs layer \n",
      "\u001b[1Bb700ef54: Pulling fs layer \n",
      "\u001b[1B3c1ba463: Pulling fs layer \n",
      "\u001b[1B72e6e08d: Pulling fs layer \n",
      "\u001b[1B63993349: Pulling fs layer \n",
      "\u001b[1Bf314d125: Pulling fs layer \n",
      "\u001b[1Bddcc87c2: Pulling fs layer \n",
      "\u001b[1B67646c1c: Pulling fs layer \n",
      "\u001b[1B138847d4: Pulling fs layer \n",
      "\u001b[1B51a7be9a: Pulling fs layer \n",
      "\u001b[1B0625c4f2: Pulling fs layer \n",
      "\u001b[1Ba2cea4f8: Pulling fs layer \n",
      "\u001b[1Bd52a000b: Pulling fs layer \n",
      "\u001b[1B0efc374f: Pulling fs layer \n",
      "\u001b[1B04cbfd01: Pulling fs layer \n",
      "\u001b[1B9b7f7ff1: Pulling fs layer \n",
      "\u001b[1Bda052f10: Pulling fs layer \n",
      "\u001b[1B69c44cc4: Pulling fs layer \n",
      "\u001b[1B761d0a1e: Pulling fs layer \n",
      "\u001b[1Bbb3e15dc: Pulling fs layer \n",
      "\u001b[1Ba87108e5: Pulling fs layer \n",
      "\u001b[1B31d931ca: Pulling fs layer \n",
      "\u001b[1B9db912fd: Pulling fs layer \n",
      "\u001b[1B2f1e370b: Pulling fs layer \n",
      "\u001b[1B272720e7: Pulling fs layer \n",
      "\u001b[1B26379d7c: Pulling fs layer \n",
      "\u001b[1Bcb5580f9: Pulling fs layer \n",
      "\u001b[1B779dcb1a: Pulling fs layer \n",
      "\u001b[1Be5459342: Pulling fs layer \n",
      "\u001b[1B45a29a11: Pulling fs layer \n",
      "\u001b[28B2e6e08d: Waiting fs layer \n",
      "\u001b[1B1b47f24e: Pulling fs layer \n",
      "\u001b[1Be96ba825: Pulling fs layer \n",
      "\u001b[1B498a88ea: Pulling fs layer \n",
      "\u001b[1B367f06d5: Pulling fs layer \n",
      "\u001b[1Ba508baeb: Pulling fs layer \n",
      "\u001b[1B236eaa77: Pulling fs layer \n",
      "\u001b[1B6862be76: Pulling fs layer \n",
      "\u001b[1B1a8e0994: Pulling fs layer \n",
      "\u001b[1Bef775d0d: Pulling fs layer \n",
      "\u001b[1B92c8f888: Pulling fs layer \n",
      "\u001b[1B3b3da9e6: Pulling fs layer \n",
      "\u001b[1Bf30407fc: Pulling fs layer \n",
      "\u001b[1Be30b4d34: Pulling fs layer \n",
      "\u001b[1B4e677009: Pulling fs layer \n",
      "\u001b[1B32c062f2: Pulling fs layer \n",
      "\u001b[1B07c9e0e0: Pulling fs layer \n",
      "\u001b[1B8883ee03: Pulling fs layer \n",
      "\u001b[1Be47a66fe: Pulling fs layer \n",
      "\u001b[43B7646c1c: Waiting fs layer \n",
      "\u001b[43B38847d4: Waiting fs layer \n",
      "\u001b[43B1a7be9a: Waiting fs layer \n",
      "\u001b[43B625c4f2: Waiting fs layer \n",
      "\u001b[43B2cea4f8: Waiting fs layer \n",
      "\u001b[42Befc374f: Waiting fs layer \n",
      "\u001b[1Bd5ec78e3: Pulling fs layer \n",
      "\u001b[17B2c8f888: Waiting fs layer \n",
      "\u001b[1Bdc71c829: Pulling fs layer \n",
      "\u001b[1Be82bc90b: Pulling fs layer \n",
      "\u001b[19Bb3da9e6: Waiting fs layer \n",
      "\u001b[1B01537ce5: Pulling fs layer \n",
      "\u001b[19B30b4d34: Waiting fs layer \n",
      "\u001b[1B9310f93e: Pulling fs layer \n",
      "\u001b[20Be677009: Waiting fs layer \n",
      "\u001b[1BDigest: sha256:417cbf33f87b5378849df37983552cd1f8bc8b62fe1ceabe004de816a55dff21\n",
      "Status: Downloaded newer image for nvcr.io/nvidia/pytorch:25.11-py3\n",
      "nvcr.io/nvidia/pytorch:25.11-py3\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to pull (this may take a while!)\n",
    "!docker pull {PYTORCH_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Running Containers with GPU Access\n",
    "\n",
    "### Key Flags Explained\n",
    "\n",
    "```bash\n",
    "docker run \\\n",
    "    --gpus all \\           # Enable GPU access\n",
    "    -it \\                   # Interactive terminal\n",
    "    --rm \\                  # Remove container on exit\n",
    "    -v $HOME/workspace:/workspace \\  # Mount your code\n",
    "    --ipc=host \\           # Shared memory for PyTorch DataLoader\n",
    "    nvcr.io/nvidia/pytorch:25.11-py3 \\\n",
    "    bash\n",
    "```\n",
    "\n",
    "Let's break down each flag:\n",
    "\n",
    "| Flag | Purpose | When Required |\n",
    "|------|----------|---------------|\n",
    "| `--gpus all` | Makes GPU visible inside container | Always |\n",
    "| `-it` | Interactive mode with terminal | For interactive sessions |\n",
    "| `--rm` | Auto-cleanup when container exits | Recommended always |\n",
    "| `-v` | Mount host directory into container | To persist your work |\n",
    "| `--ipc=host` | Needed for PyTorch multi-worker data loading | Always for PyTorch |\n",
    "| `-p 8888:8888` | Port mapping for Jupyter Lab | Only when running Jupyter |\n",
    "\n",
    "> **Note:** The `-p 8888:8888` flag is only needed when running Jupyter Lab. For interactive bash sessions, port mapping is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run this command to start the container:\n",
      "==================================================\n",
      "docker run --gpus all -it --rm \\\n",
      "    -v /home/trosfy/workspace:/workspace \\\n",
      "    -v /home/trosfy/.cache/huggingface:/root/.cache/huggingface \\\n",
      "    --ipc=host \\\n",
      "    nvcr.io/nvidia/pytorch:25.11-py3 \\\n",
      "    bash\n"
     ]
    }
   ],
   "source": [
    "# Generate the docker run command\n",
    "import os\n",
    "\n",
    "home = os.environ.get('HOME', '/home/user')\n",
    "\n",
    "run_command = f\"\"\"docker run --gpus all -it --rm \\\\\n",
    "    -v {home}/workspace:/workspace \\\\\n",
    "    -v {home}/.cache/huggingface:/root/.cache/huggingface \\\\\n",
    "    --ipc=host \\\\\n",
    "    {PYTORCH_IMAGE} \\\\\n",
    "    bash\"\"\"\n",
    "\n",
    "print(\"Run this command to start the container:\")\n",
    "print(\"=\" * 50)\n",
    "print(run_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing GPU access in container...\n",
      "--------------------------------------------------\n",
      "\n",
      "=============\n",
      "== PyTorch ==\n",
      "=============\n",
      "\n",
      "NVIDIA Release 25.11 (build 231036168)\n",
      "PyTorch Version 2.10.0a0+b558c98\n",
      "Container image Copyright (c) 2025, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "Copyright (c) 2014-2024 Facebook Inc.\n",
      "Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)\n",
      "Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)\n",
      "Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)\n",
      "Copyright (c) 2011-2013 NYU                      (Clement Farabet)\n",
      "Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)\n",
      "Copyright (c) 2006      Idiap Research Institute (Samy Bengio)\n",
      "Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)\n",
      "Copyright (c) 2015      Google Inc.\n",
      "Copyright (c) 2015      Yangqing Jia\n",
      "Copyright (c) 2013-2016 The Caffe contributors\n",
      "All rights reserved.\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
      "\n",
      "GOVERNING TERMS: The software and materials are governed by the NVIDIA Software License Agreement\n",
      "(found at https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/)\n",
      "and the Product-Specific Terms for NVIDIA AI Products\n",
      "(found at https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/).\n",
      "\n",
      "NOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n",
      "   insufficient for PyTorch.  NVIDIA recommends the use of the following flags:\n",
      "   docker run --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 ...\n",
      "\n",
      "CUDA available: True\n",
      "Device: NVIDIA GB10\n"
     ]
    }
   ],
   "source": [
    "# Test GPU access in container (quick test)\n",
    "test_command = f'''docker run --gpus all --rm {PYTORCH_IMAGE} \\\n",
    "    python -c \"import torch; print(f'CUDA available: {{torch.cuda.is_available()}}'); print(f'Device: {{torch.cuda.get_device_name(0) if torch.cuda.is_available() else None}}')\"\n",
    "'''\n",
    "\n",
    "print(\"Testing GPU access in container...\")\n",
    "print(\"-\" * 50)\n",
    "!{test_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Creating docker-compose.yml\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "Docker Compose makes it easy to run containers with complex configurations. Instead of typing long `docker run` commands, you define everything in a YAML file.\n",
    "\n",
    "> **Important: File Locations**\n",
    ">\n",
    "> The following cells will create configuration files in your **current working directory**.\n",
    "> For best organization, ensure you're running this notebook from your project root directory\n",
    "> (e.g., `$HOME/workspace/dgx-spark-project/`).\n",
    ">\n",
    "> Files created:\n",
    "> - `docker-compose.yml` - Docker Compose configuration\n",
    "> - `start_pytorch.sh` - Shell script launcher\n",
    "> - `verify_gpu.py` - GPU verification script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker-compose.yml content:\n",
      "==================================================\n",
      "# DGX Spark AI Development Environment\n",
      "# Generated for NGC PyTorch container\n",
      "\n",
      "services:\n",
      "  pytorch:\n",
      "    image: nvcr.io/nvidia/pytorch:25.11-py3\n",
      "    container_name: dgx-spark-pytorch\n",
      "\n",
      "    # GPU access\n",
      "    deploy:\n",
      "      resources:\n",
      "        reservations:\n",
      "          devices:\n",
      "            - driver: nvidia\n",
      "              count: all\n",
      "              capabilities: [gpu]\n",
      "\n",
      "    # Volume mounts\n",
      "    volumes:\n",
      "      - /home/trosfy/workspace:/workspace\n",
      "      - /home/trosfy/.cache/huggingface:/root/.cache/huggingface\n",
      "      - /home/trosfy/.cache/torch:/root/.cache/torch\n",
      "\n",
      "    # Networking\n",
      "    ports:\n",
      "      - \"8888:8888\"   # Jupyter Lab\n",
      "      - \"6006:6006\"   # TensorBoard\n",
      "\n",
      "    # Required for PyTorch DataLoader\n",
      "    ipc: host\n",
      "\n",
      "    # Keep container running\n",
      "    stdin_open: true\n",
      "    tty: true\n",
      "\n",
      "    # Working directory\n",
      "    working_dir: /workspace\n",
      "\n",
      "    # Default command (can override)\n",
      "    command: jupyter lab --ip=0.0.0.0 --port=8888 --allow-root --no-browser\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate docker-compose.yml content\n",
    "import os\n",
    "\n",
    "home = os.environ.get('HOME', '/home/user')\n",
    "\n",
    "docker_compose_content = f\"\"\"# DGX Spark AI Development Environment\n",
    "# Generated for NGC PyTorch container\n",
    "\n",
    "services:\n",
    "  pytorch:\n",
    "    image: {PYTORCH_IMAGE}\n",
    "    container_name: dgx-spark-pytorch\n",
    "    \n",
    "    # GPU access\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              count: all\n",
    "              capabilities: [gpu]\n",
    "    \n",
    "    # Volume mounts\n",
    "    volumes:\n",
    "      - {home}/workspace:/workspace\n",
    "      - {home}/.cache/huggingface:/root/.cache/huggingface\n",
    "      - {home}/.cache/torch:/root/.cache/torch\n",
    "    \n",
    "    # Networking\n",
    "    ports:\n",
    "      - \"8888:8888\"   # Jupyter Lab\n",
    "      - \"6006:6006\"   # TensorBoard\n",
    "    \n",
    "    # Required for PyTorch DataLoader\n",
    "    ipc: host\n",
    "    \n",
    "    # Keep container running\n",
    "    stdin_open: true\n",
    "    tty: true\n",
    "    \n",
    "    # Working directory\n",
    "    working_dir: /workspace\n",
    "    \n",
    "    # Default command (can override)\n",
    "    command: jupyter lab --ip=0.0.0.0 --port=8888 --allow-root --no-browser\n",
    "\"\"\"\n",
    "\n",
    "print(\"docker-compose.yml content:\")\n",
    "print(\"=\" * 50)\n",
    "print(docker_compose_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved to: /home/trosfy/projects/dgx-spark-ai-curriculum/domain-1-platform-foundations/module-1.1-dgx-spark-platform/labs/docker-compose.yml\n",
      "\n",
      "Usage:\n",
      "  docker compose up -d      # Start in background\n",
      "  docker compose logs -f    # View logs\n",
      "  docker compose down       # Stop and remove\n",
      "  docker compose exec pytorch bash  # Get shell\n"
     ]
    }
   ],
   "source": [
    "# Save docker-compose.yml\n",
    "import os\n",
    "\n",
    "# Save to current directory\n",
    "compose_path = \"docker-compose.yml\"\n",
    "\n",
    "with open(compose_path, 'w') as f:\n",
    "    f.write(docker_compose_content)\n",
    "\n",
    "print(f\"‚úÖ Saved to: {os.path.abspath(compose_path)}\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  docker compose up -d      # Start in background\")\n",
    "print(\"  docker compose logs -f    # View logs\")\n",
    "print(\"  docker compose down       # Stop and remove\")\n",
    "print(\"  docker compose exec pytorch bash  # Get shell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Shell Script Launcher\n",
    "\n",
    "Some developers prefer a simple shell script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved to: /home/trosfy/projects/dgx-spark-ai-curriculum/domain-1-platform-foundations/module-1.1-dgx-spark-platform/labs/start_pytorch.sh\n",
      "\n",
      "Usage:\n",
      "  ./start_pytorch.sh           # Start Jupyter Lab\n",
      "  ./start_pytorch.sh bash      # Get shell\n",
      "  ./start_pytorch.sh python    # Python REPL\n"
     ]
    }
   ],
   "source": [
    "# Generate shell script launcher\n",
    "shell_script = f\"\"\"#!/bin/bash\n",
    "# DGX Spark PyTorch Development Environment\n",
    "# Usage: ./start_pytorch.sh [command]\n",
    "#   ./start_pytorch.sh           # Start Jupyter Lab\n",
    "#   ./start_pytorch.sh bash      # Get shell\n",
    "#   ./start_pytorch.sh python    # Python REPL\n",
    "\n",
    "IMAGE=\"{PYTORCH_IMAGE}\"\n",
    "CONTAINER_NAME=\"dgx-spark-pytorch\"\n",
    "\n",
    "# Default command is Jupyter Lab\n",
    "CMD=\"${{@:-jupyter lab --ip=0.0.0.0 --allow-root --no-browser}}\"\n",
    "\n",
    "# Stop existing container if running\n",
    "docker stop $CONTAINER_NAME 2>/dev/null\n",
    "docker rm $CONTAINER_NAME 2>/dev/null\n",
    "\n",
    "echo \"Starting DGX Spark PyTorch environment...\"\n",
    "echo \"Image: $IMAGE\"\n",
    "echo \"Command: $CMD\"\n",
    "echo \"\"\n",
    "\n",
    "docker run --gpus all -it --rm \\\\\n",
    "    --name $CONTAINER_NAME \\\\\n",
    "    -v $HOME/workspace:/workspace \\\\\n",
    "    -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\\\n",
    "    -v $HOME/.cache/torch:/root/.cache/torch \\\\\n",
    "    -p 8888:8888 \\\\\n",
    "    -p 6006:6006 \\\\\n",
    "    --ipc=host \\\\\n",
    "    -w /workspace \\\\\n",
    "    $IMAGE \\\\\n",
    "    $CMD\n",
    "\"\"\"\n",
    "\n",
    "# Save script\n",
    "script_path = \"start_pytorch.sh\"\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(shell_script)\n",
    "\n",
    "# Make executable\n",
    "os.chmod(script_path, 0o755)\n",
    "\n",
    "print(f\"‚úÖ Saved to: {os.path.abspath(script_path)}\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  ./start_pytorch.sh           # Start Jupyter Lab\")\n",
    "print(\"  ./start_pytorch.sh bash      # Get shell\")\n",
    "print(\"  ./start_pytorch.sh python    # Python REPL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Verifying GPU Access\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "Once your container is running, you need to verify that:\n",
    "1. PyTorch can see the GPU\n",
    "2. Tensor operations work on GPU\n",
    "3. Memory is accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved verification script to: verify_gpu.py\n",
      "\n",
      "Run inside container:\n",
      "  python verify_gpu.py\n"
     ]
    }
   ],
   "source": [
    "# Create a verification script\n",
    "verification_script = '''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "DGX Spark GPU Verification Script\n",
    "Run this inside the NGC container to verify GPU access.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "def check_torch():\n",
    "    \"\"\"Check PyTorch GPU access.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PyTorch GPU Verification\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"CUDA version: {torch.version.cuda}\")\n",
    "            print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "            print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "            print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "            \n",
    "            # Memory info\n",
    "            props = torch.cuda.get_device_properties(0)\n",
    "            print(f\"Total memory: {props.total_memory / 1e9:.1f} GB\")\n",
    "            \n",
    "            # Test tensor operation\n",
    "            print(\"\\nTesting tensor operations...\")\n",
    "            x = torch.randn(1000, 1000, device=\"cuda\")\n",
    "            y = torch.randn(1000, 1000, device=\"cuda\")\n",
    "            z = torch.matmul(x, y)\n",
    "            print(f\"‚úÖ Matrix multiplication successful!\")\n",
    "            print(f\"   Result shape: {z.shape}\")\n",
    "            print(f\"   Memory used: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå CUDA not available!\")\n",
    "            return False\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"‚ùå PyTorch not installed!\")\n",
    "        return False\n",
    "\n",
    "def check_cudnn():\n",
    "    \"\"\"Check cuDNN status.\"\"\"\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"cuDNN Status\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"cuDNN available: {torch.backends.cudnn.is_available()}\")\n",
    "        print(f\"cuDNN enabled: {torch.backends.cudnn.enabled}\")\n",
    "        print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return False\n",
    "\n",
    "def check_bfloat16():\n",
    "    \"\"\"Check bfloat16 support (important for Blackwell).\"\"\"\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"BFloat16 Support (Blackwell Optimized)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            x = torch.randn(100, 100, dtype=torch.bfloat16, device=\"cuda\")\n",
    "            y = torch.randn(100, 100, dtype=torch.bfloat16, device=\"cuda\")\n",
    "            z = torch.matmul(x, y)\n",
    "            print(f\"‚úÖ BFloat16 operations work!\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå BFloat16 error: {e}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    all_passed = True\n",
    "    all_passed &= check_torch()\n",
    "    all_passed &= check_cudnn()\n",
    "    all_passed &= check_bfloat16()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    if all_passed:\n",
    "        print(\"‚úÖ All checks passed! Your DGX Spark is ready for AI!\")\n",
    "    else:\n",
    "        print(\"‚ùå Some checks failed. Please review the output above.\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    sys.exit(0 if all_passed else 1)\n",
    "'''\n",
    "\n",
    "# Save verification script\n",
    "verify_path = \"verify_gpu.py\"\n",
    "with open(verify_path, 'w') as f:\n",
    "    f.write(verification_script)\n",
    "\n",
    "print(f\"‚úÖ Saved verification script to: {verify_path}\")\n",
    "print(\"\\nRun inside container:\")\n",
    "print(\"  python verify_gpu.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Common Container Configurations\n",
    "\n",
    "### Configuration Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Container Configurations\n",
      "============================================================\n",
      "\n",
      "### Development (Interactive)\n",
      "----------------------------------------\n",
      "\n",
      "docker run --gpus all -it --rm \\\n",
      "    -v $HOME/workspace:/workspace \\\n",
      "    -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\n",
      "    --ipc=host \\\n",
      "    nvcr.io/nvidia/pytorch:25.11-py3 bash\n",
      "\n",
      "\n",
      "### Jupyter Lab\n",
      "----------------------------------------\n",
      "\n",
      "docker run --gpus all -it --rm \\\n",
      "    -v $HOME/workspace:/workspace \\\n",
      "    -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\n",
      "    -p 8888:8888 \\\n",
      "    --ipc=host \\\n",
      "    nvcr.io/nvidia/pytorch:25.11-py3 \\\n",
      "    jupyter lab --ip=0.0.0.0 --allow-root --no-browser\n",
      "\n",
      "\n",
      "### Training Script\n",
      "----------------------------------------\n",
      "\n",
      "docker run --gpus all --rm \\\n",
      "    -v $HOME/workspace:/workspace \\\n",
      "    -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\n",
      "    --ipc=host \\\n",
      "    nvcr.io/nvidia/pytorch:25.11-py3 \\\n",
      "    python /workspace/train.py\n",
      "\n",
      "\n",
      "### With TensorBoard\n",
      "----------------------------------------\n",
      "\n",
      "docker run --gpus all -it --rm \\\n",
      "    -v $HOME/workspace:/workspace \\\n",
      "    -p 8888:8888 -p 6006:6006 \\\n",
      "    --ipc=host \\\n",
      "    nvcr.io/nvidia/pytorch:25.11-py3 bash -c \"\n",
      "        tensorboard --logdir=/workspace/logs --bind_all &\n",
      "        jupyter lab --ip=0.0.0.0 --allow-root --no-browser\n",
      "    \"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print common container configurations\n",
    "\n",
    "configs = {\n",
    "    \"Development (Interactive)\": f\"\"\"\n",
    "docker run --gpus all -it --rm \\\\\n",
    "    -v $HOME/workspace:/workspace \\\\\n",
    "    -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\\\n",
    "    --ipc=host \\\\\n",
    "    {PYTORCH_IMAGE} bash\n",
    "\"\"\",\n",
    "\n",
    "    \"Jupyter Lab\": f\"\"\"\n",
    "docker run --gpus all -it --rm \\\\\n",
    "    -v $HOME/workspace:/workspace \\\\\n",
    "    -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\\\n",
    "    -p 8888:8888 \\\\\n",
    "    --ipc=host \\\\\n",
    "    {PYTORCH_IMAGE} \\\\\n",
    "    jupyter lab --ip=0.0.0.0 --allow-root --no-browser\n",
    "\"\"\",\n",
    "\n",
    "    \"Training Script\": f\"\"\"\n",
    "docker run --gpus all --rm \\\\\n",
    "    -v $HOME/workspace:/workspace \\\\\n",
    "    -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\\\n",
    "    --ipc=host \\\\\n",
    "    {PYTORCH_IMAGE} \\\\\n",
    "    python /workspace/train.py\n",
    "\"\"\",\n",
    "\n",
    "    \"With TensorBoard\": f\"\"\"\n",
    "docker run --gpus all -it --rm \\\\\n",
    "    -v $HOME/workspace:/workspace \\\\\n",
    "    -p 8888:8888 -p 6006:6006 \\\\\n",
    "    --ipc=host \\\\\n",
    "    {PYTORCH_IMAGE} bash -c \"\n",
    "        tensorboard --logdir=/workspace/logs --bind_all &\n",
    "        jupyter lab --ip=0.0.0.0 --allow-root --no-browser\n",
    "    \"\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "print(\"Common Container Configurations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, cmd in configs.items():\n",
    "    print(f\"\\n### {name}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Persisting Container Changes\n",
    "\n",
    "### Concept Explanation\n",
    "\n",
    "By default, containers are ephemeral - changes are lost when they stop. Here's how to persist data:\n",
    "\n",
    "1. **Volume mounts** (recommended): Mount directories from host\n",
    "2. **Docker volumes**: Named volumes managed by Docker\n",
    "3. **Custom image**: Build your own image with modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Volume Mounts for AI Development\n",
      "============================================================\n",
      "\n",
      "Mount                                  | Purpose\n",
      "---------------------------------------|------------------------\n",
      "$HOME/workspace:/workspace             | Your code and projects\n",
      "$HOME/.cache/huggingface:/root/.cache/ | HuggingFace models\n",
      "  huggingface                          |   (saves re-downloads)\n",
      "$HOME/.cache/torch:/root/.cache/torch  | PyTorch model cache\n",
      "$HOME/data:/data                       | Large datasets\n",
      "$HOME/models:/models                   | Saved model checkpoints\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show recommended volume mounts\n",
    "print(\"Recommended Volume Mounts for AI Development\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "Mount                                  | Purpose\n",
    "---------------------------------------|------------------------\n",
    "$HOME/workspace:/workspace             | Your code and projects\n",
    "$HOME/.cache/huggingface:/root/.cache/ | HuggingFace models\n",
    "  huggingface                          |   (saves re-downloads)\n",
    "$HOME/.cache/torch:/root/.cache/torch  | PyTorch model cache\n",
    "$HOME/data:/data                       | Large datasets\n",
    "$HOME/models:/models                   | Saved model checkpoints\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating recommended directories...\n",
      "  ‚úÖ /home/trosfy/workspace\n",
      "  ‚úÖ /home/trosfy/.cache/huggingface\n",
      "  ‚úÖ /home/trosfy/.cache/torch\n",
      "  ‚úÖ /home/trosfy/data\n",
      "  ‚úÖ /home/trosfy/models\n",
      "\n",
      "Done! These directories will persist your work.\n"
     ]
    }
   ],
   "source": [
    "# Create recommended directories\n",
    "import os\n",
    "\n",
    "directories = [\n",
    "    os.path.expanduser(\"~/workspace\"),\n",
    "    os.path.expanduser(\"~/.cache/huggingface\"),\n",
    "    os.path.expanduser(\"~/.cache/torch\"),\n",
    "    os.path.expanduser(\"~/data\"),\n",
    "    os.path.expanduser(\"~/models\"),\n",
    "]\n",
    "\n",
    "print(\"Creating recommended directories...\")\n",
    "for d in directories:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "    print(f\"  ‚úÖ {d}\")\n",
    "\n",
    "print(\"\\nDone! These directories will persist your work.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting `--gpus all`\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong - No GPU access\n",
    "docker run -it nvcr.io/nvidia/pytorch:25.11-py3 bash\n",
    "\n",
    "# ‚úÖ Right - GPU enabled\n",
    "docker run --gpus all -it nvcr.io/nvidia/pytorch:25.11-py3 bash\n",
    "```\n",
    "\n",
    "### Mistake 2: Missing `--ipc=host`\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong - DataLoader workers may crash\n",
    "docker run --gpus all -it nvcr.io/nvidia/pytorch:25.11-py3 bash\n",
    "\n",
    "# ‚úÖ Right - Shared memory enabled\n",
    "docker run --gpus all --ipc=host -it nvcr.io/nvidia/pytorch:25.11-py3 bash\n",
    "```\n",
    "**Why:** PyTorch DataLoader uses shared memory for inter-process communication.\n",
    "\n",
    "### Mistake 3: Not mounting cache directories\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong - Re-downloads models every time\n",
    "docker run --gpus all -it nvcr.io/nvidia/pytorch:25.11-py3 bash\n",
    "\n",
    "# ‚úÖ Right - Persist model cache\n",
    "docker run --gpus all -it \\\n",
    "    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n",
    "    nvcr.io/nvidia/pytorch:25.11-py3 bash\n",
    "```\n",
    "\n",
    "### Mistake 4: Using wrong port for Jupyter\n",
    "\n",
    "```bash\n",
    "# ‚ùå Wrong - Can't access from host\n",
    "docker run --gpus all -it IMAGE jupyter lab --allow-root\n",
    "\n",
    "# ‚úÖ Right - Bind to all interfaces and expose port\n",
    "docker run --gpus all -it -p 8888:8888 IMAGE \\\n",
    "    jupyter lab --ip=0.0.0.0 --allow-root --no-browser\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ Why NGC containers are required for DGX Spark\n",
    "- ‚úÖ How to pull the PyTorch NGC container\n",
    "- ‚úÖ How to run containers with GPU access\n",
    "- ‚úÖ How to create docker-compose.yml for easy development\n",
    "- ‚úÖ How to verify GPU access inside containers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "Create a custom Dockerfile that extends the NGC PyTorch image with:\n",
    "1. Your favorite Python packages (e.g., transformers, datasets)\n",
    "2. Custom Jupyter configuration\n",
    "3. A startup script that shows GPU status\n",
    "\n",
    "<details>\n",
    "<summary>üí° Solution Hint</summary>\n",
    "\n",
    "```dockerfile\n",
    "FROM nvcr.io/nvidia/pytorch:25.11-py3\n",
    "\n",
    "# Install additional packages\n",
    "RUN pip install transformers datasets accelerate\n",
    "\n",
    "# Jupyter config\n",
    "RUN mkdir -p /root/.jupyter\n",
    "RUN echo \"c.NotebookApp.token = ''\" >> /root/.jupyter/jupyter_notebook_config.py\n",
    "\n",
    "# Startup script\n",
    "COPY startup.sh /startup.sh\n",
    "ENTRYPOINT [\"/startup.sh\"]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [NGC Container Catalog](https://catalog.ngc.nvidia.com/)\n",
    "- [Docker GPU Documentation](https://docs.docker.com/config/containers/resource_constraints/#gpu)\n",
    "- [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files created in this notebook:\n",
      "----------------------------------------\n",
      "  - docker-compose.yml\n",
      "  - start_pytorch.sh\n",
      "  - verify_gpu.py\n",
      "\n",
      "These files are useful - keep them!\n",
      "\n",
      "============================================================\n",
      "üéâ Great job completing Lab 1.1.3: NGC Container Setup!\n",
      "============================================================\n",
      "\n",
      "Next up: Lab 1.1.4 - Compatibility Matrix\n",
      "You'll research which AI tools work on DGX Spark.\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"Files created in this notebook:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"  - docker-compose.yml\")\n",
    "print(\"  - start_pytorch.sh\")\n",
    "print(\"  - verify_gpu.py\")\n",
    "print(\"\\nThese files are useful - keep them!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ Great job completing Lab 1.1.3: NGC Container Setup!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nNext up: Lab 1.1.4 - Compatibility Matrix\")\n",
    "print(\"You'll research which AI tools work on DGX Spark.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
