# Domain 1: Platform Foundations - Overview

## ğŸ¯ Domain Purpose

Domain 1 establishes the essential foundation for AI/ML development on DGX Spark. You'll master the platform, programming fundamentals, mathematical concepts, and neural network building blocks needed for advanced deep learning work in later domains.

---

## ğŸ“Š Domain at a Glance

| Aspect | Details |
|--------|---------|
| **Modules** | 7 (1.1 through 1.7) |
| **Total Duration** | ~6-7 weeks |
| **Prerequisites** | Basic Python, high school math |
| **Capstone** | MicroGrad+ autograd library |

---

## ğŸ—ºï¸ Module Progression

```
Week 1          Week 2          Week 3          Week 4          Week 5          Week 6          Week 7
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1.1    â”‚    â”‚  1.2    â”‚    â”‚  1.3    â”‚    â”‚  1.4    â”‚    â”‚  1.5    â”‚    â”‚  1.6    â”‚    â”‚  1.7    â”‚
â”‚ Platformâ”‚â”€â”€â”€â–ºâ”‚ Python  â”‚â”€â”€â”€â–ºâ”‚  CUDA   â”‚â”€â”€â”€â–ºâ”‚  Math   â”‚â”€â”€â”€â–ºâ”‚ Neural  â”‚â”€â”€â”€â–ºâ”‚Classicalâ”‚â”€â”€â”€â–ºâ”‚Capstone â”‚
â”‚ Setup   â”‚    â”‚ NumPy   â”‚    â”‚ Python  â”‚    â”‚  Fndns  â”‚    â”‚Networks â”‚    â”‚   ML    â”‚    â”‚MicroGradâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚              â”‚              â”‚              â”‚              â”‚              â”‚              â”‚
     â–¼              â–¼              â–¼              â–¼              â–¼              â–¼              â–¼
 Container      Vectorized     GPU kernels    Gradients     Forward/      XGBoost      Build your
 & GPU ready    operations     & memory       & chain       Backward      baselines    own autograd
                                              rule          passes
```

---

## ğŸ“š Module Details

### Module 1.1: DGX Spark Platform
**Duration:** ~4-6 hours | **Priority:** P0 Critical

Understand your hardware and set up the development environment.

- GB10 Superchip specs (128GB unified memory, 6,144 CUDA cores)
- NGC container ecosystem
- Docker commands for GPU workloads
- Resource monitoring and troubleshooting

**Key Outcome:** Can launch NGC containers and verify GPU access

---

### Module 1.2: Python for AI/ML
**Duration:** ~6-8 hours | **Priority:** P0 Critical

Master NumPy vectorization and efficient data manipulation.

- NumPy array operations and broadcasting
- Einsum notation for tensor operations
- Pandas for data preprocessing
- Vectorization for 100x speedups

**Key Outcome:** Write vectorized code without Python loops

---

### Module 1.3: CUDA Python Introduction
**Duration:** ~8-10 hours | **Priority:** P1 High

Write GPU-accelerated code with Numba and CuPy.

- GPU architecture (SMs, warps, memory hierarchy)
- Numba CUDA kernels
- CuPy as drop-in NumPy replacement
- Memory coalescing and optimization

**Key Outcome:** Write custom GPU kernels that outperform CPU

---

### Module 1.4: Math Foundations for Deep Learning
**Duration:** ~6-8 hours | **Priority:** P0 Critical

Build the mathematical intuition for neural networks.

- Derivatives and gradients
- Chain rule and backpropagation
- Optimization (gradient descent, Adam)
- Linear algebra for deep learning

**Key Outcome:** Compute gradients by hand and verify with code

---

### Module 1.5: Neural Network Fundamentals
**Duration:** ~6-8 hours | **Priority:** P0 Critical

Implement neural networks from scratch.

- Layers, activations, and forward pass
- Backpropagation implementation
- Loss functions (MSE, Cross-Entropy)
- Training loops and batching

**Key Outcome:** Train a neural network from scratch on MNIST

---

### Module 1.6: Classical ML Foundations
**Duration:** ~6-8 hours | **Priority:** P2 Medium

Know when classical ML beats deep learning.

- XGBoost and gradient boosting
- RAPIDS cuML for GPU acceleration
- Hyperparameter tuning with Optuna
- When to use classical vs deep learning

**Key Outcome:** Create XGBoost baselines for comparison

---

### Module 1.7: Capstone â€” MicroGrad+
**Duration:** ~8-10 hours | **Priority:** P0 Critical

Build your own autograd library from scratch.

- Tensor class with automatic differentiation
- Layer, loss, and optimizer implementations
- Comprehensive testing with gradient checks
- MNIST example achieving >95% accuracy

**Key Outcome:** Working autograd library demonstrating Domain 1 mastery

---

## ğŸ”— How Modules Connect

```
Platform (1.1) â”€â”€â”€â”€â–º Required for everything else

Python (1.2) â”€â”€â”€â”€â”€â”€â–º NumPy arrays used in all modules
                     â”œâ”€â”€ CUDA Python (1.3): Array operations on GPU
                     â”œâ”€â”€ Math (1.4): Gradient computations
                     â”œâ”€â”€ Neural Networks (1.5): Layer implementations
                     â””â”€â”€ Classical ML (1.6): cuML DataFrames

CUDA Python (1.3) â”€â–º Understanding GPU for:
                     â”œâ”€â”€ Neural Networks (1.5): Why GPU training matters
                     â””â”€â”€ Capstone (1.7): Appreciate PyTorch's optimizations

Math (1.4) â”€â”€â”€â”€â”€â”€â”€â”€â–º Foundation for:
                     â”œâ”€â”€ Neural Networks (1.5): Backpropagation
                     â””â”€â”€ Capstone (1.7): Autograd implementation

Neural Networks (1.5) â–º Implement components for Capstone (1.7)

Classical ML (1.6) â”€â–º Baselines to compare against:
                     â””â”€â”€ Capstone (1.7): MicroGrad+ vs XGBoost
```

---

## ğŸ“ˆ Skills Progression

| Skill | Module 1.1 | Module 1.2 | Module 1.3 | Module 1.4 | Module 1.5 | Module 1.6 | Module 1.7 |
|-------|------------|------------|------------|------------|------------|------------|------------|
| Docker/Containers | â—â—â—â—‹ | â—â—â—â—‹ | â—â—â—â— | â—â—â—â—‹ | â—â—â—â—‹ | â—â—â—â—‹ | â—â—â—â—‹ |
| NumPy/Vectorization | â—‹â—‹â—‹â—‹ | â—â—â—â— | â—â—â—â— | â—â—â—â— | â—â—â—â— | â—â—â—â— | â—â—â—â— |
| GPU Programming | â—â—‹â—‹â—‹ | â—‹â—‹â—‹â—‹ | â—â—â—â— | â—‹â—‹â—‹â—‹ | â—â—‹â—‹â—‹ | â—â—â—‹â—‹ | â—â—‹â—‹â—‹ |
| Math/Calculus | â—‹â—‹â—‹â—‹ | â—â—‹â—‹â—‹ | â—‹â—‹â—‹â—‹ | â—â—â—â— | â—â—â—â— | â—â—â—‹â—‹ | â—â—â—â— |
| Neural Networks | â—‹â—‹â—‹â—‹ | â—‹â—‹â—‹â—‹ | â—‹â—‹â—‹â—‹ | â—â—â—‹â—‹ | â—â—â—â— | â—â—â—â—‹ | â—â—â—â— |
| Software Engineering | â—â—â—‹â—‹ | â—â—â—‹â—‹ | â—â—â—â—‹ | â—â—â—‹â—‹ | â—â—â—â—‹ | â—â—â—â—‹ | â—â—â—â— |

Legend: â—‹ = Not covered, â— = Basic, â—â— = Intermediate, â—â—â— = Advanced, â—â—â—â— = Expert

---

## âœ… Domain Completion Checklist

### Module 1.1
- [ ] NGC container pulled and running
- [ ] GPU verified with nvidia-smi and torch.cuda
- [ ] Understand DGX Spark architecture

### Module 1.2
- [ ] NumPy broadcasting mastered
- [ ] Einsum notation comfortable
- [ ] Replaced loops with vectorization

### Module 1.3
- [ ] Custom CUDA kernel written
- [ ] Memory coalescing understood
- [ ] CuPy speedup demonstrated

### Module 1.4
- [ ] Gradients computed by hand
- [ ] Chain rule applied to multi-layer networks
- [ ] Numerical gradient verification working

### Module 1.5
- [ ] Neural network trained from scratch
- [ ] Backpropagation implemented
- [ ] MNIST accuracy >85%

### Module 1.6
- [ ] XGBoost model trained
- [ ] RAPIDS cuML speedup measured
- [ ] Know when to use classical vs deep learning

### Module 1.7
- [ ] Tensor autograd implemented
- [ ] All layers, losses, optimizers working
- [ ] Test coverage >80%
- [ ] MNIST accuracy >95%

---

## ğŸ“ What You'll Be Able to Do

After completing Domain 1, you will:

1. **Operate DGX Spark** â€” Launch containers, monitor resources, troubleshoot issues
2. **Write Efficient Code** â€” Use vectorization and GPU acceleration
3. **Understand Math** â€” Apply chain rule, compute gradients, optimize functions
4. **Build Neural Networks** â€” Implement from scratch or use frameworks effectively
5. **Choose Wisely** â€” Know when classical ML beats deep learning
6. **Debug Effectively** â€” Verify gradients numerically, test thoroughly

---

## ğŸš€ Preparing for Domain 2

Domain 1 builds the foundation that Domain 2 (Deep Learning Frameworks) depends on:

| Domain 1 Skill | Domain 2 Application |
|----------------|----------------------|
| Container management | PyTorch NGC containers |
| NumPy vectorization | Understanding tensor operations |
| GPU basics | Multi-GPU training |
| Math foundations | Understanding optimizer behavior |
| Manual backprop | Appreciating autograd |
| Classical ML baselines | Model comparison |

With Domain 1 complete, you'll understand what PyTorch does under the hoodâ€”making you a more effective deep learning practitioner.

---

## ğŸ“– Study Resources

Each module includes:
- **QUICKSTART.md** â€” 5-minute hands-on introduction
- **STUDY_GUIDE.md** â€” Learning roadmap and objectives
- **QUICK_REFERENCE.md** â€” Commands and code patterns

Selected modules also include:
- **ELI5.md** â€” Jargon-free explanations for complex concepts
- **LAB_PREP.md** â€” Environment setup checklist
- **TROUBLESHOOTING.md** â€” Common issues and solutions
- **FAQ.md** â€” Frequently asked questions

---

## â­ï¸ Next Domain

After completing Domain 1, proceed to:

**[Domain 2: Deep Learning Frameworks](../domain-2-deep-learning-frameworks/)**

Where you'll master PyTorch for production deep learning work.
