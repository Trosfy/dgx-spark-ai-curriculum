{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MicroGrad+ CIFAR-10 Example\n",
    "\n",
    "This notebook demonstrates training on CIFAR-10 using MicroGrad+.\n",
    "\n",
    "**Note**: CIFAR-10 is more challenging than MNIST because:\n",
    "- Images are 32x32 color (3 channels) = 3072 features\n",
    "- 10 diverse object classes (not just digits)\n",
    "- Requires larger networks for good performance\n",
    "\n",
    "With an MLP, we can expect ~50-55% accuracy (CNNs can reach ~90%+).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport sys\nfrom pathlib import Path\n\n# Robust path resolution - works regardless of working directory\ndef _find_module_root():\n    \"\"\"Find the module root directory containing micrograd_plus.\"\"\"\n    current = Path.cwd()\n    for parent in [current] + list(current.parents):\n        if (parent / 'micrograd_plus' / '__init__.py').exists():\n            return str(parent)\n    return str(Path.cwd().parent)\n\nsys.path.insert(0, _find_module_root())\n\nfrom micrograd_plus import (\n    Tensor, Linear, ReLU, Dropout, Sequential,\n    CrossEntropyLoss, Adam\n)\nfrom micrograd_plus.utils import set_seed, DataLoader\n\nset_seed(42)\n\nprint(\"MicroGrad+ CIFAR-10 Example\")\nprint(\"=\" * 50)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load CIFAR-10 Dataset\n",
    "\n",
    "CIFAR-10 contains 60,000 32x32 color images in 10 classes:\n",
    "- airplane, automobile, bird, cat, deer\n",
    "- dog, frog, horse, ship, truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "CIFAR10_CLASSES = [\n    'airplane', 'automobile', 'bird', 'cat', 'deer',\n    'dog', 'frog', 'horse', 'ship', 'truck'\n]\n\ndef load_cifar10():\n    \"\"\"\n    Load CIFAR-10 dataset.\n    \n    Requires TensorFlow/Keras to be installed for real data.\n    Falls back to synthetic data with clear warning if unavailable.\n    \n    Returns:\n        X_train, X_test: (N, 3072) normalized images (flattened)\n        y_train, y_test: (N,) integer labels 0-9\n    \"\"\"\n    try:\n        from tensorflow.keras.datasets import cifar10\n        print(\"Loading CIFAR-10 from keras...\")\n        (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n        \n        # Flatten and normalize\n        X_train = X_train.reshape(-1, 3072).astype(np.float32) / 255.0\n        X_test = X_test.reshape(-1, 3072).astype(np.float32) / 255.0\n        y_train = y_train.flatten().astype(np.int32)\n        y_test = y_test.flatten().astype(np.int32)\n        \n        print(\"CIFAR-10 loaded successfully!\")\n        return X_train, X_test, y_train, y_test\n        \n    except ImportError:\n        print(\"=\" * 60)\n        print(\"WARNING: TensorFlow not installed!\")\n        print(\"=\" * 60)\n        print(\"\")\n        print(\"To use real CIFAR-10 data, install TensorFlow:\")\n        print(\"  pip install tensorflow\")\n        print(\"\")\n        print(\"Falling back to SYNTHETIC random data.\")\n        print(\"Training results will NOT be meaningful!\")\n        print(\"=\" * 60)\n        print(\"\")\n        \n        # Generate synthetic data that looks somewhat like CIFAR-10\n        np.random.seed(42)\n        X_train = np.random.randn(5000, 3072).astype(np.float32) * 0.3 + 0.5\n        X_train = np.clip(X_train, 0, 1)\n        y_train = np.random.randint(0, 10, 5000).astype(np.int32)\n        X_test = np.random.randn(1000, 3072).astype(np.float32) * 0.3 + 0.5\n        X_test = np.clip(X_test, 0, 1)\n        y_test = np.random.randint(0, 10, 1000).astype(np.int32)\n        \n        return X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = load_cifar10()\n\nprint(f\"\\nDataset loaded:\")\nprint(f\"  Training: {X_train.shape[0]} samples\")\nprint(f\"  Test:     {X_test.shape[0]} samples\")\nprint(f\"  Image shape: 32x32x3 = 3072 features\")\nprint(f\"  Classes: {', '.join(CIFAR10_CLASSES)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Some Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some training examples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # Reshape to 32x32x3\n",
    "    img = X_train[i].reshape(32, 32, 3)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'{CIFAR10_CLASSES[y_train[i]]}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Sample CIFAR-10 Images', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing\n",
    "\n",
    "Apply normalization per channel (standard practice for CIFAR-10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_cifar10(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Normalize CIFAR-10 using per-channel mean and std.\n",
    "    \"\"\"\n",
    "    # Reshape to (N, 3, 1024) for channel-wise normalization\n",
    "    X_train_reshaped = X_train.reshape(-1, 3, 1024)\n",
    "    X_test_reshaped = X_test.reshape(-1, 3, 1024)\n",
    "    \n",
    "    # Compute mean and std per channel from training set\n",
    "    mean = X_train_reshaped.mean(axis=(0, 2), keepdims=True)\n",
    "    std = X_train_reshaped.std(axis=(0, 2), keepdims=True)\n",
    "    \n",
    "    # Normalize\n",
    "    X_train_norm = (X_train_reshaped - mean) / (std + 1e-8)\n",
    "    X_test_norm = (X_test_reshaped - mean) / (std + 1e-8)\n",
    "    \n",
    "    # Flatten back\n",
    "    return X_train_norm.reshape(-1, 3072), X_test_norm.reshape(-1, 3072)\n",
    "\n",
    "X_train_norm, X_test_norm = normalize_cifar10(X_train, X_test)\n",
    "\n",
    "print(f\"Normalized data:\")\n",
    "print(f\"  Train mean: {X_train_norm.mean():.4f}\")\n",
    "print(f\"  Train std:  {X_train_norm.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a subset for faster training\n",
    "TRAIN_SIZE = 10000  # Use 10000 for quick demo, 50000 for full\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(X_train_norm[:TRAIN_SIZE], y_train[:TRAIN_SIZE],\n",
    "                          batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(X_test_norm, y_test,\n",
    "                         batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define the Model\n",
    "\n",
    "We need a larger network for CIFAR-10 since it's more complex than MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a deeper MLP for CIFAR-10\n",
    "model = Sequential(\n",
    "    # First hidden layer\n",
    "    Linear(3072, 1024),\n",
    "    ReLU(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Second hidden layer\n",
    "    Linear(1024, 512),\n",
    "    ReLU(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Third hidden layer\n",
    "    Linear(512, 256),\n",
    "    ReLU(),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    # Output layer\n",
    "    Linear(256, 10)\n",
    ")\n",
    "\n",
    "total_params = sum(p.data.size for p in model.parameters())\n",
    "print(f\"Model Architecture:\")\n",
    "print(f\"  Input:  3072 (32x32x3 pixels)\")\n",
    "print(f\"  Hidden: 1024 -> ReLU -> Dropout(0.3)\")\n",
    "print(f\"  Hidden: 512 -> ReLU -> Dropout(0.3)\")\n",
    "print(f\"  Hidden: 256 -> ReLU -> Dropout(0.2)\")\n",
    "print(f\"  Output: 10 classes\")\n",
    "print(f\"\\nTotal Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "EPOCHS = 20\n",
    "\n",
    "def train_epoch(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X = Tensor(X_batch, requires_grad=True)\n",
    "        y = Tensor(y_batch)\n",
    "        \n",
    "        logits = model(X)\n",
    "        loss = loss_fn(logits, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * len(y_batch)\n",
    "        predictions = np.argmax(logits.data, axis=1)\n",
    "        correct += np.sum(predictions == y_batch)\n",
    "        total += len(y_batch)\n",
    "    \n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "def evaluate(model, test_loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X = Tensor(X_batch)\n",
    "        y = Tensor(y_batch)\n",
    "        \n",
    "        logits = model(X)\n",
    "        loss = loss_fn(logits, y)\n",
    "        \n",
    "        total_loss += loss.item() * len(y_batch)\n",
    "        predictions = np.argmax(logits.data, axis=1)\n",
    "        correct += np.sum(predictions == y_batch)\n",
    "        total += len(y_batch)\n",
    "    \n",
    "    return total_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"\\nStarting Training...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, loss_fn, optimizer)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, loss_fn)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}% | \"\n",
    "          f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc*100:.2f}%\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Final Test Accuracy: {history['test_acc'][-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "ax1.plot(history['train_loss'], 'b-', label='Train', linewidth=2)\n",
    "ax1.plot(history['test_loss'], 'r-', label='Test', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Test Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot([a*100 for a in history['train_acc']], 'b-', label='Train', linewidth=2)\n",
    "ax2.plot([a*100 for a in history['test_acc']], 'r-', label='Test', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Test Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Get predictions for first 20 test images\n",
    "X_sample = Tensor(X_test_norm[:20])\n",
    "logits = model(X_sample)\n",
    "predictions = np.argmax(logits.data, axis=1)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 10, figsize=(16, 4))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = X_test[i].reshape(32, 32, 3)  # Use original (not normalized) for display\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    pred_class = CIFAR10_CLASSES[predictions[i]]\n",
    "    true_class = CIFAR10_CLASSES[y_test[i]]\n",
    "    \n",
    "    color = 'green' if predictions[i] == y_test[i] else 'red'\n",
    "    ax.set_title(f'P:{pred_class[:4]}\\nT:{true_class[:4]}', \n",
    "                 color=color, fontsize=8)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Predictions (Green=Correct, Red=Wrong)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Per-Class Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-class accuracy\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "for X_batch, y_batch in test_loader:\n",
    "    X = Tensor(X_batch)\n",
    "    logits = model(X)\n",
    "    predictions = np.argmax(logits.data, axis=1)\n",
    "    all_predictions.extend(predictions)\n",
    "    all_labels.extend(y_batch)\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "class_accuracies = []\n",
    "for i, class_name in enumerate(CIFAR10_CLASSES):\n",
    "    mask = all_labels == i\n",
    "    if mask.sum() > 0:\n",
    "        acc = (all_predictions[mask] == i).mean() * 100\n",
    "        class_accuracies.append(acc)\n",
    "        print(f\"  {class_name:12s}: {acc:.1f}%\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"  {'Average':12s}: {np.mean(class_accuracies):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-class accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "bars = plt.bar(CIFAR10_CLASSES, class_accuracies, color='steelblue')\n",
    "\n",
    "# Color bars based on accuracy\n",
    "for bar, acc in zip(bars, class_accuracies):\n",
    "    if acc >= 60:\n",
    "        bar.set_color('green')\n",
    "    elif acc >= 40:\n",
    "        bar.set_color('orange')\n",
    "    else:\n",
    "        bar.set_color('red')\n",
    "\n",
    "plt.axhline(y=np.mean(class_accuracies), color='black', linestyle='--', label='Average')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Per-Class Accuracy on CIFAR-10')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **CIFAR-10 is harder than MNIST** - Color images with complex objects\n",
    "\n",
    "2. **MLPs have limitations** - Can't capture spatial patterns like CNNs\n",
    "\n",
    "3. **Normalization matters** - Per-channel normalization helps training\n",
    "\n",
    "4. **Some classes are harder** - Often confusions like cat/dog, car/truck\n",
    "\n",
    "### Expected Results:\n",
    "- **Random baseline**: 10% (guessing)\n",
    "- **Simple MLP**: 40-50%\n",
    "- **Deep MLP with dropout**: 50-55%\n",
    "- **CNN (not implemented here)**: 80-95%\n",
    "\n",
    "### Why CNNs Would Be Better:\n",
    "- Capture local spatial patterns (edges, textures)\n",
    "- Parameter sharing (same filter applied everywhere)\n",
    "- Translation invariance\n",
    "- Hierarchical feature learning\n",
    "\n",
    "This example shows the limits of MLPs on image classification tasks,\n",
    "motivating the need for more advanced architectures like CNNs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}