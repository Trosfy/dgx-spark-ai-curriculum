{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.5.5 Solutions: MNIST Classification\n",
    "\n",
    "This notebook contains solutions to the exercises from notebook 05.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from micrograd_plus import (\n",
    "    Tensor, Linear, ReLU, Sigmoid, Tanh, Dropout, Sequential,\n",
    "    MSELoss, CrossEntropyLoss, SGD, Adam\n",
    ")\n",
    "from micrograd_plus.utils import set_seed, DataLoader\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Improved Architecture\n",
    "\n",
    "Design an improved MLP that achieves >97% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedMLP:\n",
    "    \"\"\"\n",
    "    Improved MLP architecture for MNIST.\n",
    "    \n",
    "    Key improvements:\n",
    "    1. Deeper network (more layers)\n",
    "    2. Dropout for regularization\n",
    "    3. Better hidden dimensions\n",
    "    4. BatchNorm for stable training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=784, num_classes=10, dropout_rate=0.3):\n",
    "        self.layers = Sequential(\n",
    "            # First block: 784 -> 512\n",
    "            Linear(input_dim, 512),\n",
    "            ReLU(),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            # Second block: 512 -> 256\n",
    "            Linear(512, 256),\n",
    "            ReLU(),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            # Third block: 256 -> 128\n",
    "            Linear(256, 128),\n",
    "            ReLU(),\n",
    "            Dropout(dropout_rate / 2),  # Less dropout near output\n",
    "            \n",
    "            # Output: 128 -> 10\n",
    "            Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.layers.parameters()\n",
    "    \n",
    "    def train(self):\n",
    "        self.layers.train()\n",
    "        return self\n",
    "    \n",
    "    def eval(self):\n",
    "        self.layers.eval()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters\n",
    "model = ImprovedMLP()\n",
    "total_params = sum(p.data.size for p in model.parameters())\n",
    "print(f\"Improved MLP Parameters: {total_params:,}\")\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(\"  784 -> 512 -> 256 -> 128 -> 10\")\n",
    "print(\"  With Dropout and ReLU activations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2 Solution: Data Augmentation\n",
    "\n",
    "Implement simple data augmentation for MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTAugmenter:\n",
    "    \"\"\"\n",
    "    Data augmentation for MNIST images.\n",
    "    \n",
    "    Augmentations:\n",
    "    1. Random shifts (translation)\n",
    "    2. Random noise\n",
    "    3. Random scaling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_shift=2, noise_std=0.1, scale_range=(0.9, 1.1)):\n",
    "        self.max_shift = max_shift\n",
    "        self.noise_std = noise_std\n",
    "        self.scale_range = scale_range\n",
    "    \n",
    "    def random_shift(self, image):\n",
    "        \"\"\"Apply random translation.\"\"\"\n",
    "        # Reshape to 28x28\n",
    "        img = image.reshape(28, 28)\n",
    "        \n",
    "        # Random shifts\n",
    "        shift_x = np.random.randint(-self.max_shift, self.max_shift + 1)\n",
    "        shift_y = np.random.randint(-self.max_shift, self.max_shift + 1)\n",
    "        \n",
    "        # Apply shift using roll (wrapping at edges)\n",
    "        shifted = np.roll(img, shift_x, axis=1)\n",
    "        shifted = np.roll(shifted, shift_y, axis=0)\n",
    "        \n",
    "        return shifted.flatten()\n",
    "    \n",
    "    def add_noise(self, image):\n",
    "        \"\"\"Add Gaussian noise.\"\"\"\n",
    "        noise = np.random.randn(*image.shape) * self.noise_std\n",
    "        noisy = np.clip(image + noise, 0, 1)\n",
    "        return noisy\n",
    "    \n",
    "    def random_scale(self, image):\n",
    "        \"\"\"Apply random brightness scaling.\"\"\"\n",
    "        scale = np.random.uniform(*self.scale_range)\n",
    "        scaled = np.clip(image * scale, 0, 1)\n",
    "        return scaled\n",
    "    \n",
    "    def augment(self, image, probability=0.5):\n",
    "        \"\"\"Apply random augmentations.\"\"\"\n",
    "        aug = image.copy()\n",
    "        \n",
    "        if np.random.random() < probability:\n",
    "            aug = self.random_shift(aug)\n",
    "        \n",
    "        if np.random.random() < probability:\n",
    "            aug = self.add_noise(aug)\n",
    "        \n",
    "        if np.random.random() < probability:\n",
    "            aug = self.random_scale(aug)\n",
    "        \n",
    "        return aug.astype(np.float32)\n",
    "    \n",
    "    def augment_batch(self, batch, probability=0.5):\n",
    "        \"\"\"Augment a batch of images.\"\"\"\n",
    "        return np.array([self.augment(img, probability) for img in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate augmentation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a sample digit (handwritten 5)\n",
    "sample = np.zeros((28, 28), dtype=np.float32)\n",
    "sample[5:23, 8:20] = 0.5  # Body\n",
    "sample[5:10, 8:20] = 1.0  # Top\n",
    "sample[12:16, 8:20] = 1.0  # Middle\n",
    "sample[20:23, 8:20] = 1.0  # Bottom\n",
    "sample = sample.flatten()\n",
    "\n",
    "augmenter = MNISTAugmenter(max_shift=3, noise_std=0.15)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "axes[0, 0].imshow(sample.reshape(28, 28), cmap='gray')\n",
    "axes[0, 0].set_title('Original')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "for i in range(1, 10):\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    aug = augmenter.augment(sample, probability=0.8)\n",
    "    ax.imshow(aug.reshape(28, 28), cmap='gray')\n",
    "    ax.set_title(f'Augmented {i}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3 Solution: Learning Rate Scheduling\n",
    "\n",
    "Implement and use learning rate scheduling for better convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineAnnealingWarmRestarts:\n",
    "    \"\"\"\n",
    "    Cosine annealing with warm restarts.\n",
    "    \n",
    "    The learning rate follows a cosine curve and resets periodically.\n",
    "    Good for escaping local minima.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_min=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            optimizer: The optimizer\n",
    "            T_0: Number of epochs for the first restart\n",
    "            T_mult: Factor to increase T_0 after each restart\n",
    "            eta_min: Minimum learning rate\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.eta_min = eta_min\n",
    "        self.eta_max = optimizer.lr\n",
    "        \n",
    "        self.T_cur = 0\n",
    "        self.T_i = T_0\n",
    "        self.cycle = 0\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Update learning rate.\"\"\"\n",
    "        # Compute current learning rate\n",
    "        lr = self.eta_min + 0.5 * (self.eta_max - self.eta_min) * (\n",
    "            1 + np.cos(np.pi * self.T_cur / self.T_i)\n",
    "        )\n",
    "        \n",
    "        self.optimizer.lr = lr\n",
    "        \n",
    "        # Update epoch counter\n",
    "        self.T_cur += 1\n",
    "        \n",
    "        # Check for restart\n",
    "        if self.T_cur >= self.T_i:\n",
    "            self.T_cur = 0\n",
    "            self.T_i = self.T_i * self.T_mult\n",
    "            self.cycle += 1\n",
    "        \n",
    "        return lr\n",
    "    \n",
    "    def get_lr(self):\n",
    "        return self.optimizer.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learning rate schedule\n",
    "x = Tensor([0.0], requires_grad=True)\n",
    "opt = Adam([x], lr=0.01)\n",
    "\n",
    "scheduler = CosineAnnealingWarmRestarts(opt, T_0=10, T_mult=2)\n",
    "\n",
    "lrs = []\n",
    "for epoch in range(100):\n",
    "    lr = scheduler.step()\n",
    "    lrs.append(lr)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(lrs, 'b-', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Cosine Annealing with Warm Restarts (T_0=10, T_mult=2)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark restarts\n",
    "restarts = [10, 30, 70]  # Cumulative: 10, 10+20=30, 30+40=70\n",
    "for r in restarts:\n",
    "    plt.axvline(x=r, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4 Solution: Early Stopping\n",
    "\n",
    "Implement early stopping to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to terminate training when validation loss stops improving.\n",
    "    \n",
    "    Features:\n",
    "    - Patience: number of epochs to wait before stopping\n",
    "    - Min delta: minimum improvement to count as improvement\n",
    "    - Restore best weights: option to restore best model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, patience=10, min_delta=0.0, restore_best=True, mode='min'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience: Epochs to wait before stopping\n",
    "            min_delta: Minimum change to qualify as improvement\n",
    "            restore_best: Whether to restore best weights\n",
    "            mode: 'min' for loss, 'max' for accuracy\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best = restore_best\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.best_score = None\n",
    "        self.best_weights = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = 0\n",
    "    \n",
    "    def _is_improvement(self, score):\n",
    "        \"\"\"Check if score is an improvement.\"\"\"\n",
    "        if self.best_score is None:\n",
    "            return True\n",
    "        \n",
    "        if self.mode == 'min':\n",
    "            return score < self.best_score - self.min_delta\n",
    "        else:\n",
    "            return score > self.best_score + self.min_delta\n",
    "    \n",
    "    def __call__(self, score, model, epoch):\n",
    "        \"\"\"Update early stopping state.\"\"\"\n",
    "        if self._is_improvement(score):\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            self.best_epoch = epoch\n",
    "            \n",
    "            if self.restore_best:\n",
    "                # Save weights\n",
    "                self.best_weights = [\n",
    "                    p.data.copy() for p in model.parameters()\n",
    "                ]\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                \n",
    "                if self.restore_best and self.best_weights is not None:\n",
    "                    # Restore best weights\n",
    "                    for p, best_w in zip(model.parameters(), self.best_weights):\n",
    "                        p.data = best_w\n",
    "                    print(f\"Restored best weights from epoch {self.best_epoch}\")\n",
    "        \n",
    "        return self.early_stop\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset early stopping state.\"\"\"\n",
    "        self.best_score = None\n",
    "        self.best_weights = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate early stopping behavior\n",
    "print(\"Early Stopping Demo\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Simulate validation losses\n",
    "val_losses = [1.0, 0.8, 0.7, 0.65, 0.63, 0.62, 0.61, 0.615, 0.62, 0.625, 0.63, 0.635, 0.64]\n",
    "\n",
    "# Create a dummy model\n",
    "model = Sequential(Linear(10, 5))\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.01)\n",
    "\n",
    "for epoch, loss in enumerate(val_losses):\n",
    "    stopped = early_stopping(loss, model, epoch)\n",
    "    status = \"STOP\" if stopped else f\"counter={early_stopping.counter}\"\n",
    "    print(f\"Epoch {epoch}: loss={loss:.4f}, best={early_stopping.best_score:.4f}, {status}\")\n",
    "    \n",
    "    if stopped:\n",
    "        break\n",
    "\n",
    "print(f\"\\nBest epoch: {early_stopping.best_epoch}\")\n",
    "print(f\"Best loss:  {early_stopping.best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Solution: Complete Training Pipeline\n",
    "\n",
    "Combine all improvements into a production-ready training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingPipeline:\n",
    "    \"\"\"\n",
    "    Complete training pipeline with all improvements.\n",
    "    \n",
    "    Features:\n",
    "    - Data augmentation\n",
    "    - Learning rate scheduling\n",
    "    - Early stopping\n",
    "    - Progress logging\n",
    "    - Best model checkpointing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, loss_fn, optimizer, scheduler=None,\n",
    "                 augmenter=None, early_stopping=None):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.augmenter = augmenter\n",
    "        self.early_stopping = early_stopping\n",
    "        \n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_acc': [],\n",
    "            'lr': []\n",
    "        }\n",
    "    \n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # Apply augmentation if available\n",
    "            if self.augmenter is not None:\n",
    "                X_batch = self.augmenter.augment_batch(X_batch)\n",
    "            \n",
    "            X = Tensor(X_batch, requires_grad=True)\n",
    "            y = Tensor(y_batch)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = self.model(X)\n",
    "            loss = self.loss_fn(logits, y)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item() * len(y_batch)\n",
    "            predictions = np.argmax(logits.data, axis=1)\n",
    "            correct += np.sum(predictions == y_batch)\n",
    "            total += len(y_batch)\n",
    "        \n",
    "        return total_loss / total, correct / total\n",
    "    \n",
    "    def evaluate(self, val_loader):\n",
    "        \"\"\"Evaluate on validation set.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X = Tensor(X_batch)\n",
    "            y = Tensor(y_batch)\n",
    "            \n",
    "            logits = self.model(X)\n",
    "            loss = self.loss_fn(logits, y)\n",
    "            \n",
    "            total_loss += loss.item() * len(y_batch)\n",
    "            predictions = np.argmax(logits.data, axis=1)\n",
    "            correct += np.sum(predictions == y_batch)\n",
    "            total += len(y_batch)\n",
    "        \n",
    "        return total_loss / total, correct / total\n",
    "    \n",
    "    def fit(self, train_loader, val_loader, epochs, verbose=True):\n",
    "        \"\"\"Run full training.\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Train\n",
    "            train_loss, train_acc = self.train_epoch(train_loader)\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_acc = self.evaluate(val_loader)\n",
    "            \n",
    "            # Update scheduler\n",
    "            current_lr = self.optimizer.lr\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "            \n",
    "            # Record history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            self.history['lr'].append(current_lr)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}: \"\n",
    "                      f\"train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, \"\n",
    "                      f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}, \"\n",
    "                      f\"lr={current_lr:.6f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if self.early_stopping is not None:\n",
    "                if self.early_stopping(val_loss, self.model, epoch):\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "        \n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (with synthetic data)\n",
    "print(\"Training Pipeline Demo (Synthetic Data)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create synthetic dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "X_train = np.random.randn(n_samples, 784).astype(np.float32)\n",
    "y_train = np.random.randint(0, 10, n_samples)\n",
    "X_val = np.random.randn(200, 784).astype(np.float32)\n",
    "y_val = np.random.randint(0, 10, 200)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(X_train, y_train, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(X_val, y_val, batch_size=32, shuffle=False)\n",
    "\n",
    "# Create model and training components\n",
    "model = ImprovedMLP(dropout_rate=0.2)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "augmenter = MNISTAugmenter(max_shift=2, noise_std=0.1)\n",
    "\n",
    "# Create and run pipeline\n",
    "pipeline = TrainingPipeline(\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    augmenter=augmenter,\n",
    "    early_stopping=early_stopping\n",
    ")\n",
    "\n",
    "history = pipeline.fit(train_loader, val_loader, epochs=20, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Deeper Networks**: More layers with proper regularization (dropout) often improve performance\n",
    "\n",
    "2. **Data Augmentation**: Simple transforms (shifts, noise, scaling) increase effective dataset size\n",
    "\n",
    "3. **Learning Rate Scheduling**: Warm restarts help escape local minima\n",
    "\n",
    "4. **Early Stopping**: Prevents overfitting by monitoring validation loss\n",
    "\n",
    "5. **Complete Pipeline**: Combining all techniques creates robust training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}