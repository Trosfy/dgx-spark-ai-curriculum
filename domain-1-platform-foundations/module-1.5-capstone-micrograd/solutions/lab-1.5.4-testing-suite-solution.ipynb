{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5.4 Solutions: Testing Suite\n",
    "\n",
    "This notebook contains solutions to the exercises from notebook 04.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from micrograd_plus import Tensor, Linear, ReLU, Sequential\n",
    "from micrograd_plus.utils import numerical_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Enhanced Gradient Checker\n",
    "\n",
    "Implement a comprehensive gradient checker with detailed error reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_gradient_check(f, x, eps=1e-5, rtol=1e-3, atol=1e-5, verbose=True):\n",
    "    \"\"\"\n",
    "    Enhanced gradient checker with detailed error reporting.\n",
    "    \n",
    "    Args:\n",
    "        f: Function taking Tensor and returning scalar Tensor\n",
    "        x: Input tensor with requires_grad=True\n",
    "        eps: Perturbation for finite differences\n",
    "        rtol: Relative tolerance\n",
    "        atol: Absolute tolerance\n",
    "        verbose: Print detailed information\n",
    "        \n",
    "    Returns:\n",
    "        dict with pass/fail status and detailed metrics\n",
    "    \"\"\"\n",
    "    # Compute analytical gradient\n",
    "    x.zero_grad()\n",
    "    y = f(x)\n",
    "    y.backward()\n",
    "    analytical = x.grad.copy()\n",
    "    \n",
    "    # Compute numerical gradient\n",
    "    def numpy_f(arr):\n",
    "        return f(Tensor(arr)).data.item()\n",
    "    \n",
    "    numerical = numerical_gradient(numpy_f, x.data.copy(), eps)\n",
    "    \n",
    "    # Compute errors\n",
    "    abs_error = np.abs(analytical - numerical)\n",
    "    rel_error = abs_error / (np.abs(numerical) + 1e-8)\n",
    "    \n",
    "    max_abs_error = np.max(abs_error)\n",
    "    max_rel_error = np.max(rel_error)\n",
    "    mean_abs_error = np.mean(abs_error)\n",
    "    mean_rel_error = np.mean(rel_error)\n",
    "    \n",
    "    # Check if passed\n",
    "    passed = np.allclose(analytical, numerical, rtol=rtol, atol=atol)\n",
    "    \n",
    "    # Find worst element\n",
    "    worst_idx = np.unravel_index(np.argmax(abs_error), abs_error.shape)\n",
    "    \n",
    "    result = {\n",
    "        'passed': passed,\n",
    "        'max_abs_error': max_abs_error,\n",
    "        'max_rel_error': max_rel_error,\n",
    "        'mean_abs_error': mean_abs_error,\n",
    "        'mean_rel_error': mean_rel_error,\n",
    "        'worst_index': worst_idx,\n",
    "        'analytical': analytical,\n",
    "        'numerical': numerical\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        status = \"PASSED\" if passed else \"FAILED\"\n",
    "        print(f\"Gradient Check: {status}\")\n",
    "        print(f\"  Max Absolute Error: {max_abs_error:.2e}\")\n",
    "        print(f\"  Max Relative Error: {max_rel_error:.2e}\")\n",
    "        print(f\"  Mean Absolute Error: {mean_abs_error:.2e}\")\n",
    "        print(f\"  Mean Relative Error: {mean_rel_error:.2e}\")\n",
    "        if not passed:\n",
    "            print(f\"  Worst element at index: {worst_idx}\")\n",
    "            print(f\"    Analytical: {analytical[worst_idx]:.6e}\")\n",
    "            print(f\"    Numerical:  {numerical[worst_idx]:.6e}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the enhanced gradient checker\n",
    "print(\"Testing Enhanced Gradient Checker\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test 1: Simple function (should pass)\n",
    "print(\"\\nTest 1: f(x) = x^2\")\n",
    "x = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "result = enhanced_gradient_check(lambda t: (t ** 2).sum(), x)\n",
    "\n",
    "# Test 2: Complex function\n",
    "print(\"\\nTest 2: f(x) = sigmoid(x^2 + 2x)\")\n",
    "x = Tensor([0.5, 1.0, 1.5], requires_grad=True)\n",
    "result = enhanced_gradient_check(lambda t: (t ** 2 + t * 2).sigmoid().sum(), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2 Solution: Layer-wise Testing Framework\n",
    "\n",
    "Create a comprehensive test for any layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerTester:\n",
    "    \"\"\"\n",
    "    Comprehensive testing framework for neural network layers.\n",
    "    \n",
    "    Tests:\n",
    "    1. Forward pass produces correct shapes\n",
    "    2. Backward pass computes gradients\n",
    "    3. Gradients are numerically correct\n",
    "    4. Parameters are updated correctly\n",
    "    5. Train/eval mode behavior\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer, input_shape, seed=42):\n",
    "        self.layer = layer\n",
    "        self.input_shape = input_shape\n",
    "        self.seed = seed\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def test_forward_shape(self):\n",
    "        \"\"\"Test that forward pass produces output.\"\"\"\n",
    "        x = Tensor(np.random.randn(*self.input_shape).astype(np.float32))\n",
    "        try:\n",
    "            y = self.layer(x)\n",
    "            return {'passed': True, 'output_shape': y.shape}\n",
    "        except Exception as e:\n",
    "            return {'passed': False, 'error': str(e)}\n",
    "    \n",
    "    def test_backward_exists(self):\n",
    "        \"\"\"Test that backward pass computes gradients.\"\"\"\n",
    "        x = Tensor(np.random.randn(*self.input_shape).astype(np.float32), requires_grad=True)\n",
    "        try:\n",
    "            y = self.layer(x)\n",
    "            y.sum().backward()\n",
    "            has_grad = x.grad is not None\n",
    "            return {'passed': has_grad, 'has_input_grad': has_grad}\n",
    "        except Exception as e:\n",
    "            return {'passed': False, 'error': str(e)}\n",
    "    \n",
    "    def test_gradient_numerical(self, eps=1e-5, atol=1e-4):\n",
    "        \"\"\"Test gradients against numerical computation.\"\"\"\n",
    "        x = Tensor(np.random.randn(*self.input_shape).astype(np.float32), requires_grad=True)\n",
    "        \n",
    "        def f(t):\n",
    "            return self.layer(t).sum()\n",
    "        \n",
    "        result = enhanced_gradient_check(f, x, eps=eps, atol=atol, verbose=False)\n",
    "        return {'passed': result['passed'], 'max_error': result['max_abs_error']}\n",
    "    \n",
    "    def test_parameters(self):\n",
    "        \"\"\"Test that parameters are tracked and updatable.\"\"\"\n",
    "        params = self.layer.parameters()\n",
    "        has_params = len(params) > 0\n",
    "        \n",
    "        if has_params:\n",
    "            # Test parameter update\n",
    "            original = [p.data.copy() for p in params]\n",
    "            \n",
    "            x = Tensor(np.random.randn(*self.input_shape).astype(np.float32), requires_grad=True)\n",
    "            y = self.layer(x)\n",
    "            y.sum().backward()\n",
    "            \n",
    "            # Manual update\n",
    "            for p in params:\n",
    "                if p.grad is not None:\n",
    "                    p.data -= 0.01 * p.grad\n",
    "            \n",
    "            changed = any(\n",
    "                not np.allclose(orig, p.data)\n",
    "                for orig, p in zip(original, params)\n",
    "            )\n",
    "            \n",
    "            return {'passed': True, 'num_params': len(params), 'params_updated': changed}\n",
    "        else:\n",
    "            return {'passed': True, 'num_params': 0, 'params_updated': False}\n",
    "    \n",
    "    def test_train_eval_modes(self):\n",
    "        \"\"\"Test train/eval mode behavior.\"\"\"\n",
    "        x = Tensor(np.random.randn(*self.input_shape).astype(np.float32))\n",
    "        \n",
    "        self.layer.train()\n",
    "        train_output = self.layer(x).data.copy()\n",
    "        \n",
    "        self.layer.eval()\n",
    "        eval_output = self.layer(x).data.copy()\n",
    "        \n",
    "        # Check if outputs differ (e.g., dropout)\n",
    "        outputs_differ = not np.allclose(train_output, eval_output)\n",
    "        \n",
    "        return {\n",
    "            'passed': True,\n",
    "            'outputs_differ': outputs_differ,\n",
    "            'note': 'Outputs should differ if layer has dropout/batchnorm'\n",
    "        }\n",
    "    \n",
    "    def run_all_tests(self):\n",
    "        \"\"\"Run all tests and return summary.\"\"\"\n",
    "        results = {\n",
    "            'forward_shape': self.test_forward_shape(),\n",
    "            'backward_exists': self.test_backward_exists(),\n",
    "            'gradient_numerical': self.test_gradient_numerical(),\n",
    "            'parameters': self.test_parameters(),\n",
    "            'train_eval_modes': self.test_train_eval_modes()\n",
    "        }\n",
    "        \n",
    "        all_passed = all(r['passed'] for r in results.values())\n",
    "        results['all_passed'] = all_passed\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def print_report(self):\n",
    "        \"\"\"Run tests and print formatted report.\"\"\"\n",
    "        results = self.run_all_tests()\n",
    "        \n",
    "        print(f\"\\nLayer Test Report: {self.layer.__class__.__name__}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for test_name, result in results.items():\n",
    "            if test_name == 'all_passed':\n",
    "                continue\n",
    "            status = \"PASS\" if result['passed'] else \"FAIL\"\n",
    "            print(f\"\\n{test_name}: {status}\")\n",
    "            for key, value in result.items():\n",
    "                if key != 'passed':\n",
    "                    print(f\"  {key}: {value}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        final_status = \"ALL TESTS PASSED\" if results['all_passed'] else \"SOME TESTS FAILED\"\n",
    "        print(f\"Final Status: {final_status}\")\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the LayerTester\n",
    "print(\"Testing LayerTester Framework\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test Linear layer\n",
    "linear = Linear(10, 5)\n",
    "tester = LayerTester(linear, input_shape=(4, 10))\n",
    "tester.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Sequential model\n",
    "model = Sequential(\n",
    "    Linear(10, 20),\n",
    "    ReLU(),\n",
    "    Linear(20, 5)\n",
    ")\n",
    "\n",
    "tester = LayerTester(model, input_shape=(4, 10))\n",
    "tester.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3 Solution: Performance Benchmarking\n",
    "\n",
    "Create a benchmarking utility for measuring performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class PerformanceBenchmark:\n",
    "    \"\"\"\n",
    "    Benchmark forward/backward pass performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, input_shape, num_warmup=5, num_runs=20):\n",
    "        self.model = model\n",
    "        self.input_shape = input_shape\n",
    "        self.num_warmup = num_warmup\n",
    "        self.num_runs = num_runs\n",
    "    \n",
    "    def benchmark_forward(self):\n",
    "        \"\"\"Benchmark forward pass.\"\"\"\n",
    "        x = Tensor(np.random.randn(*self.input_shape).astype(np.float32))\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(self.num_warmup):\n",
    "            _ = self.model(x)\n",
    "        \n",
    "        # Benchmark\n",
    "        times = []\n",
    "        for _ in range(self.num_runs):\n",
    "            start = time.perf_counter()\n",
    "            _ = self.model(x)\n",
    "            end = time.perf_counter()\n",
    "            times.append(end - start)\n",
    "        \n",
    "        return {\n",
    "            'mean_ms': np.mean(times) * 1000,\n",
    "            'std_ms': np.std(times) * 1000,\n",
    "            'min_ms': np.min(times) * 1000,\n",
    "            'max_ms': np.max(times) * 1000\n",
    "        }\n",
    "    \n",
    "    def benchmark_backward(self):\n",
    "        \"\"\"Benchmark backward pass.\"\"\"\n",
    "        # Warmup\n",
    "        for _ in range(self.num_warmup):\n",
    "            x = Tensor(np.random.randn(*self.input_shape).astype(np.float32), requires_grad=True)\n",
    "            y = self.model(x)\n",
    "            y.sum().backward()\n",
    "        \n",
    "        # Benchmark\n",
    "        times = []\n",
    "        for _ in range(self.num_runs):\n",
    "            x = Tensor(np.random.randn(*self.input_shape).astype(np.float32), requires_grad=True)\n",
    "            y = self.model(x)\n",
    "            \n",
    "            start = time.perf_counter()\n",
    "            y.sum().backward()\n",
    "            end = time.perf_counter()\n",
    "            times.append(end - start)\n",
    "        \n",
    "        return {\n",
    "            'mean_ms': np.mean(times) * 1000,\n",
    "            'std_ms': np.std(times) * 1000,\n",
    "            'min_ms': np.min(times) * 1000,\n",
    "            'max_ms': np.max(times) * 1000\n",
    "        }\n",
    "    \n",
    "    def benchmark_full_step(self):\n",
    "        \"\"\"Benchmark full training step (forward + backward).\"\"\"\n",
    "        from micrograd_plus import Adam\n",
    "        \n",
    "        optimizer = Adam(self.model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(self.num_warmup):\n",
    "            x = Tensor(np.random.randn(*self.input_shape).astype(np.float32), requires_grad=True)\n",
    "            y = self.model(x)\n",
    "            optimizer.zero_grad()\n",
    "            y.sum().backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Benchmark\n",
    "        times = []\n",
    "        for _ in range(self.num_runs):\n",
    "            x = Tensor(np.random.randn(*self.input_shape).astype(np.float32), requires_grad=True)\n",
    "            \n",
    "            start = time.perf_counter()\n",
    "            y = self.model(x)\n",
    "            optimizer.zero_grad()\n",
    "            y.sum().backward()\n",
    "            optimizer.step()\n",
    "            end = time.perf_counter()\n",
    "            \n",
    "            times.append(end - start)\n",
    "        \n",
    "        return {\n",
    "            'mean_ms': np.mean(times) * 1000,\n",
    "            'std_ms': np.std(times) * 1000,\n",
    "            'min_ms': np.min(times) * 1000,\n",
    "            'max_ms': np.max(times) * 1000\n",
    "        }\n",
    "    \n",
    "    def run_all(self):\n",
    "        \"\"\"Run all benchmarks.\"\"\"\n",
    "        return {\n",
    "            'forward': self.benchmark_forward(),\n",
    "            'backward': self.benchmark_backward(),\n",
    "            'full_step': self.benchmark_full_step()\n",
    "        }\n",
    "    \n",
    "    def print_report(self):\n",
    "        \"\"\"Print formatted benchmark report.\"\"\"\n",
    "        results = self.run_all()\n",
    "        \n",
    "        print(f\"\\nPerformance Benchmark Report\")\n",
    "        print(f\"Model: {self.model.__class__.__name__}\")\n",
    "        print(f\"Input shape: {self.input_shape}\")\n",
    "        print(f\"Runs: {self.num_runs} (warmup: {self.num_warmup})\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for benchmark, result in results.items():\n",
    "            print(f\"\\n{benchmark.upper()}:\")\n",
    "            print(f\"  Mean:  {result['mean_ms']:.3f} ms\")\n",
    "            print(f\"  Std:   {result['std_ms']:.3f} ms\")\n",
    "            print(f\"  Range: [{result['min_ms']:.3f}, {result['max_ms']:.3f}] ms\")\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run performance benchmark\n",
    "model = Sequential(\n",
    "    Linear(784, 256),\n",
    "    ReLU(),\n",
    "    Linear(256, 128),\n",
    "    ReLU(),\n",
    "    Linear(128, 10)\n",
    ")\n",
    "\n",
    "benchmark = PerformanceBenchmark(model, input_shape=(32, 784))\n",
    "benchmark.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4 Solution: Test Coverage Analysis\n",
    "\n",
    "Implement a simple test coverage tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoverageTracker:\n",
    "    \"\"\"\n",
    "    Simple coverage tracker for operations tested.\n",
    "    \"\"\"\n",
    "    \n",
    "    # All operations that should be tested\n",
    "    TENSOR_OPS = [\n",
    "        '__add__', '__sub__', '__mul__', '__truediv__', '__pow__',\n",
    "        '__neg__', '__matmul__',\n",
    "        'sum', 'mean', 'max', 'min',\n",
    "        'reshape', 'flatten', 'transpose', 'squeeze', 'unsqueeze',\n",
    "        'relu', 'sigmoid', 'tanh', 'softmax', 'log_softmax',\n",
    "        'exp', 'log', 'sqrt', 'abs'\n",
    "    ]\n",
    "    \n",
    "    LAYERS = [\n",
    "        'Linear', 'ReLU', 'Sigmoid', 'Tanh', 'Softmax',\n",
    "        'Dropout', 'BatchNorm', 'LayerNorm', 'Embedding', 'Sequential'\n",
    "    ]\n",
    "    \n",
    "    LOSSES = [\n",
    "        'MSELoss', 'CrossEntropyLoss', 'BCELoss', 'L1Loss', 'HuberLoss'\n",
    "    ]\n",
    "    \n",
    "    OPTIMIZERS = [\n",
    "        'SGD', 'Adam', 'AdamW', 'RMSprop'\n",
    "    ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tested = {\n",
    "            'tensor_ops': set(),\n",
    "            'layers': set(),\n",
    "            'losses': set(),\n",
    "            'optimizers': set()\n",
    "        }\n",
    "    \n",
    "    def mark_tested(self, category, name):\n",
    "        \"\"\"Mark an operation as tested.\"\"\"\n",
    "        if category in self.tested:\n",
    "            self.tested[category].add(name)\n",
    "    \n",
    "    def get_coverage(self, category):\n",
    "        \"\"\"Get coverage for a category.\"\"\"\n",
    "        all_items = getattr(self, category.upper(), [])\n",
    "        tested = self.tested.get(category, set())\n",
    "        \n",
    "        return {\n",
    "            'tested': list(tested),\n",
    "            'not_tested': [item for item in all_items if item not in tested],\n",
    "            'coverage': len(tested) / len(all_items) if all_items else 1.0\n",
    "        }\n",
    "    \n",
    "    def print_report(self):\n",
    "        \"\"\"Print coverage report.\"\"\"\n",
    "        print(\"\\nTest Coverage Report\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        total_items = 0\n",
    "        total_tested = 0\n",
    "        \n",
    "        categories = ['tensor_ops', 'layers', 'losses', 'optimizers']\n",
    "        for category in categories:\n",
    "            coverage = self.get_coverage(category)\n",
    "            all_items = getattr(self, category.upper(), [])\n",
    "            tested = len(coverage['tested'])\n",
    "            total = len(all_items)\n",
    "            \n",
    "            total_items += total\n",
    "            total_tested += tested\n",
    "            \n",
    "            pct = coverage['coverage'] * 100\n",
    "            print(f\"\\n{category.upper()}: {tested}/{total} ({pct:.1f}%)\")\n",
    "            \n",
    "            if coverage['not_tested']:\n",
    "                print(f\"  Not tested: {', '.join(coverage['not_tested'][:5])}...\" \n",
    "                      if len(coverage['not_tested']) > 5 \n",
    "                      else f\"  Not tested: {', '.join(coverage['not_tested'])}\")\n",
    "        \n",
    "        overall = total_tested / total_items if total_items > 0 else 0\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(f\"OVERALL COVERAGE: {total_tested}/{total_items} ({overall*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo coverage tracker\n",
    "tracker = CoverageTracker()\n",
    "\n",
    "# Simulate running tests\n",
    "for op in ['__add__', '__sub__', '__mul__', 'sum', 'mean', 'relu', 'sigmoid']:\n",
    "    tracker.mark_tested('tensor_ops', op)\n",
    "\n",
    "for layer in ['Linear', 'ReLU', 'Sequential']:\n",
    "    tracker.mark_tested('layers', layer)\n",
    "\n",
    "for loss in ['MSELoss', 'CrossEntropyLoss']:\n",
    "    tracker.mark_tested('losses', loss)\n",
    "\n",
    "for opt in ['SGD', 'Adam']:\n",
    "    tracker.mark_tested('optimizers', opt)\n",
    "\n",
    "tracker.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Gradient Checking**: Always verify analytical gradients against numerical approximations\n",
    "\n",
    "2. **Layer Testing**: Comprehensive tests should cover shapes, gradients, parameters, and modes\n",
    "\n",
    "3. **Benchmarking**: Include warmup runs, measure multiple iterations, report statistics\n",
    "\n",
    "4. **Coverage Tracking**: Know what you've tested and what's missing\n",
    "\n",
    "5. **Reproducibility**: Always set random seeds for consistent test results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
