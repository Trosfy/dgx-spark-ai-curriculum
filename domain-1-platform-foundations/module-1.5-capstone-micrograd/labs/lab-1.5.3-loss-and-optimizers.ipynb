{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5.3: Loss Functions and Optimizers\n",
    "\n",
    "**Module:** 5 - Phase 1 Capstone: MicroGrad+  \n",
    "**Time:** 1.5 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand how loss functions guide learning\n",
    "- [ ] Implement MSE Loss for regression and Cross-Entropy Loss for classification\n",
    "- [ ] Build SGD optimizer with momentum\n",
    "- [ ] Implement the Adam optimizer\n",
    "- [ ] See how optimizers update parameters to minimize loss\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Task 5.1 and 5.2 (Tensor and Layers)\n",
    "- Knowledge of: Basic calculus, gradient descent concept\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "Loss functions and optimizers are the \"brain\" of neural network training:\n",
    "- **Loss functions** tell the network \"how wrong\" its predictions are\n",
    "- **Optimizers** use gradients to update parameters and improve predictions\n",
    "\n",
    "Choosing the right loss function and optimizer can make or break your model's performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: Loss Functions and Optimizers\n",
    "\n",
    "> **Imagine you're learning to throw darts at a bullseye.**\n",
    ">\n",
    "> **Loss Function = Your Coach's Feedback**\n",
    "> - MSE Loss: \"You missed by 5 centimeters\" (measures the distance squared)\n",
    "> - Cross-Entropy Loss: \"You were 80% confident it would hit, but it missed completely!\" (penalizes confident wrong predictions)\n",
    ">\n",
    "> **Optimizer = How You Adjust Your Aim**\n",
    "> - SGD: \"Move your arm 1cm to the left\" (simple correction)\n",
    "> - SGD + Momentum: \"You've been moving left for the last 5 throws, keep that trend going!\" (builds up speed)\n",
    "> - Adam: \"Based on how you've been doing and how consistent your throws are, try this specific adjustment\" (smart, adaptive)\n",
    ">\n",
    "> **In AI terms:** The loss function computes a number that represents how bad the predictions are. The optimizer uses the gradients of this loss to figure out how to adjust each weight to make the predictions better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> **üìù Learning Note:**\n>\n> In this notebook, we implement loss functions and optimizers from scratch for educational purposes.\n> The complete, tested implementations are already available in the `micrograd_plus` package.\n> After understanding how they work, you can import them directly:\n>\n> ```python\n> from micrograd_plus import MSELoss, CrossEntropyLoss, SGD, Adam\n> ```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sys\nfrom pathlib import Path\n\n# Robust path resolution - works regardless of working directory\ndef _find_module_root():\n    \"\"\"Find the module root directory containing micrograd_plus.\"\"\"\n    current = Path.cwd()\n    for parent in [current] + list(current.parents):\n        if (parent / 'micrograd_plus' / '__init__.py').exists():\n            return str(parent)\n    return str(Path.cwd().parent)\n\nsys.path.insert(0, _find_module_root())\n\nfrom micrograd_plus import Tensor\nfrom micrograd_plus.utils import set_seed\n\nset_seed(42)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Mean Squared Error (MSE) Loss\n",
    "\n",
    "**MSE Loss** is used for regression problems. It measures the average squared difference between predictions and targets:\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "### Why Squared?\n",
    "- Always positive (can't have negative loss)\n",
    "- Penalizes large errors more than small ones\n",
    "- Smooth gradient (derivative is `2(y - ≈∑)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    \"\"\"\n",
    "    Mean Squared Error Loss.\n",
    "    \n",
    "    MSE = mean((pred - target)^2)\n",
    "    \n",
    "    Used for regression problems where you predict continuous values.\n",
    "    \n",
    "    Args:\n",
    "        reduction: How to combine losses ('mean', 'sum', or 'none')\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reduction: str = 'mean'):\n",
    "        if reduction not in ('mean', 'sum', 'none'):\n",
    "            raise ValueError(f\"reduction must be 'mean', 'sum', or 'none'\")\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def __call__(self, pred: Tensor, target: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute MSE loss.\n",
    "        \n",
    "        Args:\n",
    "            pred: Predictions from the model\n",
    "            target: Ground truth values\n",
    "        \n",
    "        Returns:\n",
    "            Scalar loss value\n",
    "        \"\"\"\n",
    "        if not isinstance(target, Tensor):\n",
    "            target = Tensor(target)\n",
    "        \n",
    "        diff = pred - target\n",
    "        squared = diff ** 2\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return squared.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return squared.sum()\n",
    "        else:\n",
    "            return squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MSE Loss\n",
    "mse_loss = MSELoss()\n",
    "\n",
    "# Simple example\n",
    "pred = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "target = Tensor([1.5, 2.0, 2.5])\n",
    "\n",
    "loss = mse_loss(pred, target)\n",
    "print(f\"Predictions: {pred.data}\")\n",
    "print(f\"Targets: {target.data}\")\n",
    "print(f\"Differences: {(pred.data - target.data)}\")\n",
    "print(f\"Squared: {(pred.data - target.data)**2}\")\n",
    "print(f\"MSE Loss: {loss.item():.4f}\")\n",
    "print(f\"Expected: {np.mean((pred.data - target.data)**2):.4f}\")\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "print(f\"\\nGradient: {pred.grad}\")\n",
    "print(f\"Expected (2*(pred-target)/N): {2 * (pred.data - target.data) / 3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Cross-Entropy Loss\n",
    "\n",
    "**Cross-Entropy Loss** is used for classification. It measures how different the predicted probability distribution is from the true distribution.\n",
    "\n",
    "$$\\text{CE} = -\\sum_{c=1}^{C} y_c \\log(\\hat{y}_c)$$\n",
    "\n",
    "For single-label classification with integer targets:\n",
    "$$\\text{CE} = -\\log(\\hat{y}_{\\text{true class}})$$\n",
    "\n",
    "### Why Cross-Entropy?\n",
    "- Penalizes confident wrong predictions heavily\n",
    "- Works well with softmax output\n",
    "- Natural fit for probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    \"\"\"\n",
    "    Cross-Entropy Loss for classification.\n",
    "    \n",
    "    Expects raw logits (not softmax) as predictions.\n",
    "    Targets should be class indices (integers).\n",
    "    \n",
    "    Internally computes: -log(softmax(logits)[target_class])\n",
    "    \n",
    "    Args:\n",
    "        reduction: How to combine losses ('mean', 'sum', or 'none')\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reduction: str = 'mean'):\n",
    "        if reduction not in ('mean', 'sum', 'none'):\n",
    "            raise ValueError(f\"reduction must be 'mean', 'sum', or 'none'\")\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def __call__(self, logits: Tensor, targets: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute cross-entropy loss.\n",
    "        \n",
    "        Args:\n",
    "            logits: Raw predictions of shape (batch_size, num_classes)\n",
    "            targets: Class indices of shape (batch_size,)\n",
    "        \n",
    "        Returns:\n",
    "            Scalar loss value\n",
    "        \"\"\"\n",
    "        if not isinstance(targets, Tensor):\n",
    "            targets = Tensor(targets)\n",
    "        \n",
    "        batch_size = logits.shape[0]\n",
    "        num_classes = logits.shape[1]\n",
    "        \n",
    "        # Compute log-softmax (numerically stable)\n",
    "        log_probs = logits.log_softmax(axis=1)\n",
    "        \n",
    "        # Create one-hot encoding of targets\n",
    "        target_indices = targets.data.astype(np.int32)\n",
    "        one_hot = np.zeros((batch_size, num_classes), dtype=np.float32)\n",
    "        one_hot[np.arange(batch_size), target_indices] = 1.0\n",
    "        \n",
    "        # Negative log likelihood\n",
    "        nll = -(log_probs * Tensor(one_hot)).sum(axis=1)\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return nll.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return nll.sum()\n",
    "        else:\n",
    "            return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Cross-Entropy Loss\n",
    "ce_loss = CrossEntropyLoss()\n",
    "\n",
    "# Example: 2 samples, 3 classes\n",
    "# Sample 1: Strong prediction for class 0 (correct)\n",
    "# Sample 2: Wrong prediction (predicts class 0, true is class 2)\n",
    "logits = Tensor([\n",
    "    [2.0, 1.0, 0.1],  # Predicts class 0\n",
    "    [2.0, 1.0, 0.1],  # Predicts class 0 (wrong!)\n",
    "], requires_grad=True)\n",
    "\n",
    "targets = Tensor([0, 2])  # True classes\n",
    "\n",
    "loss = ce_loss(logits, targets)\n",
    "\n",
    "# Show what's happening\n",
    "softmax_probs = np.exp(logits.data - logits.data.max(axis=1, keepdims=True))\n",
    "softmax_probs = softmax_probs / softmax_probs.sum(axis=1, keepdims=True)\n",
    "\n",
    "print(f\"Logits:\\n{logits.data}\")\n",
    "print(f\"\\nSoftmax probabilities:\\n{softmax_probs.round(3)}\")\n",
    "print(f\"\\nTrue classes: {targets.data}\")\n",
    "print(f\"\\nLoss breakdown:\")\n",
    "print(f\"  Sample 0: -log({softmax_probs[0, 0]:.3f}) = {-np.log(softmax_probs[0, 0]):.3f} (correct prediction)\")\n",
    "print(f\"  Sample 1: -log({softmax_probs[1, 2]:.3f}) = {-np.log(softmax_probs[1, 2]):.3f} (wrong prediction)\")\n",
    "print(f\"\\nCross-Entropy Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Backward\n",
    "loss.backward()\n",
    "print(f\"\\nGradient on logits:\\n{logits.grad}\")\n",
    "print(\"(Gradient = softmax - one_hot for each sample)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how cross-entropy penalizes confident wrong predictions\n",
    "probs = np.linspace(0.01, 0.99, 100)\n",
    "ce_values = -np.log(probs)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(probs, ce_values, 'b-', linewidth=2)\n",
    "plt.xlabel('Predicted Probability for Correct Class')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.title('Cross-Entropy Loss vs Confidence')\n",
    "plt.axvline(x=0.5, color='r', linestyle='--', alpha=0.5, label='50% confident')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Compare MSE vs CE for classification\n",
    "mse_values = (1 - probs) ** 2\n",
    "plt.plot(probs, ce_values, 'b-', linewidth=2, label='Cross-Entropy')\n",
    "plt.plot(probs, mse_values, 'r-', linewidth=2, label='MSE')\n",
    "plt.xlabel('Predicted Probability for Correct Class')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Cross-Entropy vs MSE')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Cross-entropy penalizes low-confidence correct predictions more than MSE.\")\n",
    "print(\"This is why CE is preferred for classification - it pushes for confident correct predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: SGD Optimizer\n",
    "\n",
    "**Stochastic Gradient Descent** updates parameters in the direction that reduces loss:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla L(\\theta_t)$$\n",
    "\n",
    "Where:\n",
    "- $\\theta$ are the parameters\n",
    "- $\\eta$ is the learning rate\n",
    "- $\\nabla L$ is the gradient of the loss\n",
    "\n",
    "### With Momentum\n",
    "\n",
    "Momentum helps SGD \"build up speed\" in consistent directions:\n",
    "\n",
    "$$v_t = \\mu \\cdot v_{t-1} + \\nabla L(\\theta_t)$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta \\cdot v_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent optimizer with optional momentum.\n",
    "    \n",
    "    Args:\n",
    "        params: List of parameters to optimize\n",
    "        lr: Learning rate (step size)\n",
    "        momentum: Momentum factor (default: 0, meaning no momentum)\n",
    "        weight_decay: L2 regularization factor (default: 0)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params: List[Tensor], lr: float = 0.01, \n",
    "                 momentum: float = 0.0, weight_decay: float = 0.0):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # Velocity buffers for momentum\n",
    "        self.velocities: Dict[int, np.ndarray] = {}\n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        \"\"\"Reset all parameter gradients to zero.\"\"\"\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.grad = np.zeros_like(p.data)\n",
    "    \n",
    "    def step(self) -> None:\n",
    "        \"\"\"Update parameters using their gradients.\"\"\"\n",
    "        for i, p in enumerate(self.params):\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            \n",
    "            grad = p.grad.copy()\n",
    "            \n",
    "            # Apply weight decay (L2 regularization)\n",
    "            if self.weight_decay != 0:\n",
    "                grad = grad + self.weight_decay * p.data\n",
    "            \n",
    "            # Apply momentum\n",
    "            if self.momentum != 0:\n",
    "                if i not in self.velocities:\n",
    "                    self.velocities[i] = np.zeros_like(p.data)\n",
    "                \n",
    "                v = self.velocities[i]\n",
    "                v = self.momentum * v + grad\n",
    "                self.velocities[i] = v\n",
    "                grad = v\n",
    "            \n",
    "            # Update parameter\n",
    "            p.data = p.data - self.lr * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate SGD optimization\n",
    "set_seed(42)\n",
    "\n",
    "# Simple optimization problem: minimize (x - 3)^2\n",
    "# The minimum is at x = 3\n",
    "x = Tensor([0.0], requires_grad=True)\n",
    "target = 3.0\n",
    "\n",
    "optimizer = SGD([x], lr=0.1)\n",
    "\n",
    "print(\"Optimizing (x - 3)^2 with SGD:\")\n",
    "print(f\"Initial x = {x.item():.4f}\")\n",
    "\n",
    "history = [x.item()]\n",
    "for step in range(20):\n",
    "    # Forward: compute loss\n",
    "    loss = (x - target) ** 2\n",
    "    \n",
    "    # Backward: compute gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update\n",
    "    optimizer.step()\n",
    "    \n",
    "    history.append(x.item())\n",
    "    \n",
    "    if step < 5 or step >= 18:\n",
    "        print(f\"Step {step+1}: x = {x.item():.4f}, loss = {loss.item():.6f}, grad = {x.grad[0]:.4f}\")\n",
    "    elif step == 5:\n",
    "        print(\"...\")\n",
    "\n",
    "print(f\"\\nFinal x = {x.item():.4f} (target: {target})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SGD with and without momentum\n",
    "def optimize_with_sgd(momentum, lr=0.1, steps=50):\n",
    "    x = Tensor([0.0], requires_grad=True)\n",
    "    optimizer = SGD([x], lr=lr, momentum=momentum)\n",
    "    history = [x.item()]\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        loss = (x - 3.0) ** 2\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        history.append(x.item())\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Run both\n",
    "history_no_momentum = optimize_with_sgd(momentum=0.0)\n",
    "history_momentum = optimize_with_sgd(momentum=0.9)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(history_no_momentum, 'b-', label='SGD (no momentum)', linewidth=2)\n",
    "plt.plot(history_momentum, 'r-', label='SGD + momentum (0.9)', linewidth=2)\n",
    "plt.axhline(y=3.0, color='k', linestyle='--', alpha=0.3, label='Target')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('x value')\n",
    "plt.title('SGD Optimization: Effect of Momentum')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Momentum helps converge faster by building up velocity!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Adam Optimizer\n",
    "\n",
    "**Adam** (Adaptive Moment Estimation) is the most popular optimizer. It combines:\n",
    "- **Momentum**: Uses exponential moving average of gradients\n",
    "- **RMSprop**: Adapts learning rate per-parameter based on gradient history\n",
    "\n",
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$$\n",
    "$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$$\n",
    "$$\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}$$\n",
    "$$\\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "class Adam:\n",
    "    \"\"\"\n",
    "    Adam optimizer.\n",
    "    \n",
    "    Combines momentum and adaptive learning rates for fast, stable convergence.\n",
    "    \n",
    "    Args:\n",
    "        params: List of parameters to optimize\n",
    "        lr: Learning rate (default: 0.001)\n",
    "        betas: Coefficients for computing running averages (default: (0.9, 0.999))\n",
    "        eps: Small constant for numerical stability (default: 1e-8)\n",
    "        weight_decay: L2 regularization factor (default: 0)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params: List[Tensor], lr: float = 0.001,\n",
    "                 betas: Tuple[float, float] = (0.9, 0.999),\n",
    "                 eps: float = 1e-8, weight_decay: float = 0.0):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # State\n",
    "        self.m: Dict[int, np.ndarray] = {}  # First moment\n",
    "        self.v: Dict[int, np.ndarray] = {}  # Second moment\n",
    "        self.t = 0  # Timestep\n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        \"\"\"Reset all parameter gradients to zero.\"\"\"\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.grad = np.zeros_like(p.data)\n",
    "    \n",
    "    def step(self) -> None:\n",
    "        \"\"\"Update parameters using Adam algorithm.\"\"\"\n",
    "        self.t += 1\n",
    "        beta1, beta2 = self.betas\n",
    "        \n",
    "        for i, p in enumerate(self.params):\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            \n",
    "            grad = p.grad.copy()\n",
    "            \n",
    "            # Apply weight decay\n",
    "            if self.weight_decay != 0:\n",
    "                p.data = p.data - self.lr * self.weight_decay * p.data\n",
    "            \n",
    "            # Initialize moment buffers\n",
    "            if i not in self.m:\n",
    "                self.m[i] = np.zeros_like(p.data)\n",
    "                self.v[i] = np.zeros_like(p.data)\n",
    "            \n",
    "            m = self.m[i]\n",
    "            v = self.v[i]\n",
    "            \n",
    "            # Update biased first moment estimate\n",
    "            m = beta1 * m + (1 - beta1) * grad\n",
    "            self.m[i] = m\n",
    "            \n",
    "            # Update biased second moment estimate\n",
    "            v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
    "            self.v[i] = v\n",
    "            \n",
    "            # Bias correction\n",
    "            m_hat = m / (1 - beta1 ** self.t)\n",
    "            v_hat = v / (1 - beta2 ** self.t)\n",
    "            \n",
    "            # Update parameter\n",
    "            p.data = p.data - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare SGD vs Adam on a harder optimization problem\ndef rosenbrock(x: Tensor, y: Tensor) -> Tensor:\n    \"\"\"\n    Rosenbrock function - a classic optimization test function.\n    \n    The Rosenbrock function is a non-convex function used as a performance test\n    for optimization algorithms. It's also known as the \"banana function\" due\n    to its curved valley shape.\n    \n    Formula: f(x, y) = (1 - x)^2 + 100 * (y - x^2)^2\n    \n    Properties:\n        - Global minimum at (1, 1) where f(1, 1) = 0\n        - The global minimum lies inside a long, narrow, parabolic-shaped valley\n        - Finding the valley is trivial, but converging to the minimum is difficult\n    \n    Args:\n        x: First coordinate (Tensor)\n        y: Second coordinate (Tensor)\n    \n    Returns:\n        Tensor: Rosenbrock function value at (x, y)\n    \"\"\"\n    return (1 - x) ** 2 + 100 * (y - x ** 2) ** 2\n\ndef optimize_rosenbrock(optimizer_class, **kwargs) -> Tuple[List[Tuple[float, float]], float]:\n    \"\"\"\n    Optimize the Rosenbrock function using a given optimizer.\n    \n    Args:\n        optimizer_class: The optimizer class to use (SGD or Adam)\n        **kwargs: Arguments to pass to the optimizer\n        \n    Returns:\n        Tuple of (optimization history, final loss value)\n    \"\"\"\n    x = Tensor([-1.0], requires_grad=True)\n    y = Tensor([1.0], requires_grad=True)\n    \n    optimizer = optimizer_class([x, y], **kwargs)\n    \n    history = [(x.item(), y.item())]\n    \n    for _ in range(1000):\n        # Compute loss\n        loss = rosenbrock(x, y)\n        \n        # Backward\n        optimizer.zero_grad()\n        loss.backward()\n        \n        # Update\n        optimizer.step()\n        \n        history.append((x.item(), y.item()))\n    \n    return history, loss.item()\n\n# Run both optimizers\nsgd_history, sgd_final_loss = optimize_rosenbrock(SGD, lr=0.0001)\nadam_history, adam_final_loss = optimize_rosenbrock(Adam, lr=0.01)\n\nprint(f\"Final loss - SGD: {sgd_final_loss:.6f}, Adam: {adam_final_loss:.6f}\")\nprint(f\"Minimum is at (1, 1) with loss 0\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimization paths\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Create contour plot\n",
    "x_range = np.linspace(-2, 2, 100)\n",
    "y_range = np.linspace(-1, 3, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = (1 - X) ** 2 + 100 * (Y - X ** 2) ** 2\n",
    "\n",
    "for ax, history, name in [(axes[0], sgd_history, 'SGD'),\n",
    "                           (axes[1], adam_history, 'Adam')]:\n",
    "    ax.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), alpha=0.7)\n",
    "    \n",
    "    # Plot optimization path\n",
    "    xs, ys = zip(*history[::10])  # Every 10th point\n",
    "    ax.plot(xs, ys, 'r.-', markersize=2, linewidth=0.5, alpha=0.7)\n",
    "    ax.plot(history[0][0], history[0][1], 'go', markersize=10, label='Start')\n",
    "    ax.plot(history[-1][0], history[-1][1], 'r*', markersize=15, label='End')\n",
    "    ax.plot(1, 1, 'bx', markersize=15, label='Global min')\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(f'{name} Optimization Path')\n",
    "    ax.legend()\n",
    "    ax.set_xlim(-2, 2)\n",
    "    ax.set_ylim(-1, 3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Putting It All Together\n",
    "\n",
    "Let's train a simple model using our loss functions and optimizers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our layers\n",
    "from micrograd_plus import Linear, ReLU, Sequential\n",
    "\n",
    "# Generate simple classification data\n",
    "set_seed(42)\n",
    "\n",
    "def generate_spiral_data(n_points=100, n_classes=3):\n",
    "    \"\"\"Generate spiral dataset for classification\"\"\"\n",
    "    X = np.zeros((n_points * n_classes, 2), dtype=np.float32)\n",
    "    y = np.zeros(n_points * n_classes, dtype=np.int32)\n",
    "    \n",
    "    for class_idx in range(n_classes):\n",
    "        ix = range(n_points * class_idx, n_points * (class_idx + 1))\n",
    "        r = np.linspace(0.0, 1, n_points)\n",
    "        t = np.linspace(class_idx * 4, (class_idx + 1) * 4, n_points) + np.random.randn(n_points) * 0.2\n",
    "        X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]\n",
    "        y[ix] = class_idx\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_spiral_data()\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', s=20)\n",
    "plt.title('Spiral Dataset')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.colorbar(label='Class')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train a model\n",
    "set_seed(42)\n",
    "\n",
    "# Model\n",
    "model = Sequential(\n",
    "    Linear(2, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, 3)\n",
    ")\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Convert data to tensors\n",
    "X_tensor = Tensor(X, requires_grad=True)\n",
    "y_tensor = Tensor(y)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "print(\"Training...\")\n",
    "for epoch in range(200):\n",
    "    # Forward pass\n",
    "    logits = model(X_tensor)\n",
    "    loss = loss_fn(logits, y_tensor)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Track metrics\n",
    "    losses.append(loss.item())\n",
    "    predictions = np.argmax(logits.data, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch:3d}: Loss = {loss.item():.4f}, Accuracy = {accuracy:.2%}\")\n",
    "\n",
    "print(f\"\\nFinal: Loss = {losses[-1]:.4f}, Accuracy = {accuracies[-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training and decision boundary\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(losses)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curve\n",
    "axes[1].plot(accuracies)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training Accuracy')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Decision boundary\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Predict on grid\n",
    "model.eval()\n",
    "grid_tensor = Tensor(np.c_[xx.ravel(), yy.ravel()].astype(np.float32))\n",
    "Z = model(grid_tensor)\n",
    "Z = np.argmax(Z.data, axis=1).reshape(xx.shape)\n",
    "\n",
    "axes[2].contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "axes[2].scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', s=20, edgecolors='k', linewidth=0.5)\n",
    "axes[2].set_xlabel('x1')\n",
    "axes[2].set_ylabel('x2')\n",
    "axes[2].set_title('Decision Boundary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Learning Rate Too High or Too Low\n",
    "\n",
    "```python\n",
    "# ‚ùå Too high: loss explodes or oscillates\n",
    "optimizer = SGD(params, lr=10.0)\n",
    "\n",
    "# ‚ùå Too low: training takes forever\n",
    "optimizer = SGD(params, lr=0.00001)\n",
    "\n",
    "# ‚úÖ Start with reasonable defaults\n",
    "optimizer = SGD(params, lr=0.01)  # For SGD\n",
    "optimizer = Adam(params, lr=0.001)  # For Adam\n",
    "```\n",
    "\n",
    "### Mistake 2: Not Zeroing Gradients\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: gradients accumulate!\n",
    "for epoch in range(epochs):\n",
    "    loss = model(x)\n",
    "    loss.backward()  # Gradients keep growing!\n",
    "    optimizer.step()\n",
    "\n",
    "# ‚úÖ Right: zero gradients each iteration\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(x)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How MSE Loss measures regression error\n",
    "- ‚úÖ How Cross-Entropy Loss works for classification\n",
    "- ‚úÖ How SGD updates parameters with optional momentum\n",
    "- ‚úÖ How Adam adaptively adjusts learning rates\n",
    "- ‚úÖ How to combine everything to train a neural network!\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cleanup - release memory\nfrom micrograd_plus.utils import cleanup_notebook\ncleanup_notebook(globals())"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}