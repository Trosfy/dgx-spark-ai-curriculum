{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5.4: Testing Suite\n",
    "\n",
    "**Module:** 5 - Phase 1 Capstone: MicroGrad+  \n",
    "**Time:** 1.5 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand why testing is crucial for neural network code\n",
    "- [ ] Write unit tests for tensor operations\n",
    "- [ ] Implement gradient checking to verify backpropagation\n",
    "- [ ] Test neural network layers and training loops\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Tasks 5.1-5.3\n",
    "- Knowledge of: Python testing basics\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "Testing neural network code is notoriously difficult because:\n",
    "1. **Silent failures**: Wrong gradients don't crash - they just train slowly or incorrectly\n",
    "2. **Randomness**: Neural networks have random initialization and stochastic training\n",
    "3. **Numerical issues**: Floating-point math can hide bugs\n",
    "\n",
    "Good tests catch bugs before they waste days of training time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: Why Test Neural Networks?\n",
    "\n",
    "> **Imagine you're building a very complicated LEGO spaceship** from a 1000-page instruction book.\n",
    ">\n",
    "> - **Without tests**: You build the whole thing, then discover it doesn't fly because one tiny piece on page 347 was wrong. Now you have to check everything!\n",
    ">\n",
    "> - **With tests**: After each section, you test that part works. \"Wheels spin? Check! Wings attach? Check!\" If something breaks, you know exactly which section to fix.\n",
    ">\n",
    "> **In AI terms:** Neural network bugs often don't cause errors - they just make the model learn slowly or learn wrong things. Tests help us catch these subtle bugs early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nimport numpy as np\nimport sys\nfrom pathlib import Path\n\n# Robust path resolution - works regardless of working directory\ndef _find_module_root():\n    \"\"\"Find the module root directory containing micrograd_plus.\"\"\"\n    current = Path.cwd()\n    for parent in [current] + list(current.parents):\n        if (parent / 'micrograd_plus' / '__init__.py').exists():\n            return str(parent)\n    return str(Path.cwd().parent)\n\nsys.path.insert(0, _find_module_root())\n\nfrom micrograd_plus import (\n    Tensor, Linear, ReLU, Sigmoid, Softmax, Dropout,\n    MSELoss, CrossEntropyLoss, SGD, Adam, Sequential\n)\nfrom micrograd_plus.utils import set_seed, numerical_gradient\n\nset_seed(42)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Testing Framework\n",
    "\n",
    "We'll build a simple testing framework. In production, you'd use `pytest`, but this shows the concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestResult:\n",
    "    \"\"\"Store results of a test run.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.passed = 0\n",
    "        self.failed = 0\n",
    "        self.errors = []\n",
    "    \n",
    "    def add_pass(self, name):\n",
    "        self.passed += 1\n",
    "        print(f\"  ‚úÖ {name}\")\n",
    "    \n",
    "    def add_fail(self, name, message):\n",
    "        self.failed += 1\n",
    "        self.errors.append((name, message))\n",
    "        print(f\"  ‚ùå {name}: {message}\")\n",
    "    \n",
    "    def summary(self):\n",
    "        total = self.passed + self.failed\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Results: {self.passed}/{total} tests passed\")\n",
    "        if self.failed > 0:\n",
    "            print(f\"\\nFailed tests:\")\n",
    "            for name, msg in self.errors:\n",
    "                print(f\"  - {name}: {msg}\")\n",
    "        return self.failed == 0\n",
    "\n",
    "def assert_close(actual, expected, name, atol=1e-5):\n",
    "    \"\"\"Assert two values are close.\"\"\"\n",
    "    if isinstance(actual, Tensor):\n",
    "        actual = actual.data\n",
    "    if isinstance(expected, Tensor):\n",
    "        expected = expected.data\n",
    "    \n",
    "    actual = np.array(actual)\n",
    "    expected = np.array(expected)\n",
    "    \n",
    "    if not np.allclose(actual, expected, atol=atol):\n",
    "        max_diff = np.max(np.abs(actual - expected))\n",
    "        raise AssertionError(f\"max diff = {max_diff:.2e}, expected < {atol}\")\n",
    "\n",
    "def run_test(test_fn, result):\n",
    "    \"\"\"Run a single test function.\"\"\"\n",
    "    name = test_fn.__name__\n",
    "    try:\n",
    "        test_fn()\n",
    "        result.add_pass(name)\n",
    "    except AssertionError as e:\n",
    "        result.add_fail(name, str(e))\n",
    "    except Exception as e:\n",
    "        result.add_fail(name, f\"Error: {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Testing Tensor Operations\n",
    "\n",
    "Let's test that basic tensor operations work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tensor_creation():\n",
    "    \"\"\"Test tensor can be created from various inputs.\"\"\"\n",
    "    # From list\n",
    "    t1 = Tensor([1, 2, 3])\n",
    "    assert t1.shape == (3,)\n",
    "    \n",
    "    # From numpy array\n",
    "    t2 = Tensor(np.array([[1, 2], [3, 4]]))\n",
    "    assert t2.shape == (2, 2)\n",
    "    \n",
    "    # From scalar\n",
    "    t3 = Tensor(5.0)\n",
    "    assert t3.shape == ()\n",
    "\n",
    "def test_tensor_addition():\n",
    "    \"\"\"Test element-wise addition.\"\"\"\n",
    "    a = Tensor([1, 2, 3])\n",
    "    b = Tensor([4, 5, 6])\n",
    "    c = a + b\n",
    "    assert_close(c, [5, 7, 9], \"addition result\")\n",
    "\n",
    "def test_tensor_multiplication():\n",
    "    \"\"\"Test element-wise multiplication.\"\"\"\n",
    "    a = Tensor([1, 2, 3])\n",
    "    b = Tensor([4, 5, 6])\n",
    "    c = a * b\n",
    "    assert_close(c, [4, 10, 18], \"multiplication result\")\n",
    "\n",
    "def test_tensor_matmul():\n",
    "    \"\"\"Test matrix multiplication.\"\"\"\n",
    "    a = Tensor([[1, 2], [3, 4]])\n",
    "    b = Tensor([[5, 6], [7, 8]])\n",
    "    c = a @ b\n",
    "    expected = np.array([[19, 22], [43, 50]])\n",
    "    assert_close(c, expected, \"matmul result\")\n",
    "\n",
    "def test_tensor_broadcasting():\n",
    "    \"\"\"Test broadcasting in operations.\"\"\"\n",
    "    a = Tensor([[1, 2, 3], [4, 5, 6]])  # (2, 3)\n",
    "    b = Tensor([10, 20, 30])  # (3,) broadcasts to (2, 3)\n",
    "    c = a + b\n",
    "    expected = np.array([[11, 22, 33], [14, 25, 36]])\n",
    "    assert_close(c, expected, \"broadcast addition\")\n",
    "\n",
    "def test_tensor_sum():\n",
    "    \"\"\"Test sum reduction.\"\"\"\n",
    "    a = Tensor([[1, 2], [3, 4]])\n",
    "    assert_close(a.sum(), 10, \"total sum\")\n",
    "    assert_close(a.sum(axis=0), [4, 6], \"sum axis 0\")\n",
    "    assert_close(a.sum(axis=1), [3, 7], \"sum axis 1\")\n",
    "\n",
    "def test_tensor_mean():\n",
    "    \"\"\"Test mean reduction.\"\"\"\n",
    "    a = Tensor([[1, 2], [3, 4]])\n",
    "    assert_close(a.mean(), 2.5, \"mean\")\n",
    "\n",
    "# Run tensor tests\n",
    "print(\"Testing Tensor Operations:\")\n",
    "print(\"-\" * 50)\n",
    "result = TestResult()\n",
    "\n",
    "for test in [test_tensor_creation, test_tensor_addition, test_tensor_multiplication,\n",
    "             test_tensor_matmul, test_tensor_broadcasting, test_tensor_sum, test_tensor_mean]:\n",
    "    run_test(test, result)\n",
    "\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Gradient Checking\n",
    "\n",
    "The most important tests for an autograd system verify that **analytical gradients match numerical gradients**.\n",
    "\n",
    "### The Idea\n",
    "\n",
    "Numerical gradient using finite differences:\n",
    "$$\\frac{\\partial f}{\\partial x} \\approx \\frac{f(x + \\epsilon) - f(x - \\epsilon)}{2\\epsilon}$$\n",
    "\n",
    "This is slow but reliable. We compare it to our fast analytical gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(f, x, eps=1e-5, atol=1e-4):\n",
    "    \"\"\"\n",
    "    Check that analytical gradient matches numerical gradient.\n",
    "    \n",
    "    Args:\n",
    "        f: Function that takes a Tensor and returns a scalar Tensor\n",
    "        x: Input tensor with requires_grad=True\n",
    "        eps: Perturbation for numerical gradient\n",
    "        atol: Absolute tolerance for comparison\n",
    "    \n",
    "    Returns:\n",
    "        (passed: bool, max_error: float)\n",
    "    \"\"\"\n",
    "    # Compute analytical gradient\n",
    "    x.zero_grad()\n",
    "    y = f(x)\n",
    "    y.backward()\n",
    "    analytical = x.grad.copy()\n",
    "    \n",
    "    # Compute numerical gradient\n",
    "    def numpy_f(arr):\n",
    "        return f(Tensor(arr)).data.item()\n",
    "    \n",
    "    numerical = numerical_gradient(numpy_f, x.data.copy(), eps)\n",
    "    \n",
    "    # Compare\n",
    "    max_error = np.max(np.abs(analytical - numerical))\n",
    "    passed = np.allclose(analytical, numerical, atol=atol)\n",
    "    \n",
    "    return passed, max_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gradient_addition():\n",
    "    \"\"\"Test gradient of addition.\"\"\"\n",
    "    x = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "    passed, error = gradient_check(lambda t: (t + 5).sum(), x)\n",
    "    assert passed, f\"gradient error: {error:.2e}\"\n",
    "\n",
    "def test_gradient_multiplication():\n",
    "    \"\"\"Test gradient of element-wise multiplication.\"\"\"\n",
    "    x = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "    y = Tensor([4.0, 5.0, 6.0])\n",
    "    passed, error = gradient_check(lambda t: (t * y).sum(), x)\n",
    "    assert passed, f\"gradient error: {error:.2e}\"\n",
    "\n",
    "def test_gradient_power():\n",
    "    \"\"\"Test gradient of power operation.\"\"\"\n",
    "    x = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "    passed, error = gradient_check(lambda t: (t ** 2).sum(), x)\n",
    "    assert passed, f\"gradient error: {error:.2e}\"\n",
    "\n",
    "def test_gradient_matmul():\n",
    "    \"\"\"Test gradient of matrix multiplication.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    x = Tensor(np.random.randn(3, 4).astype(np.float32), requires_grad=True)\n",
    "    w = Tensor(np.random.randn(4, 2).astype(np.float32))\n",
    "    passed, error = gradient_check(lambda t: (t @ w).sum(), x)\n",
    "    assert passed, f\"gradient error: {error:.2e}\"\n",
    "\n",
    "def test_gradient_relu():\n",
    "    \"\"\"Test gradient of ReLU.\"\"\"\n",
    "    # Avoid x=0 where gradient is undefined\n",
    "    x = Tensor([-2.0, -0.5, 0.5, 2.0], requires_grad=True)\n",
    "    passed, error = gradient_check(lambda t: t.relu().sum(), x)\n",
    "    assert passed, f\"gradient error: {error:.2e}\"\n",
    "\n",
    "def test_gradient_sigmoid():\n",
    "    \"\"\"Test gradient of sigmoid.\"\"\"\n",
    "    x = Tensor([-2.0, 0.0, 2.0], requires_grad=True)\n",
    "    passed, error = gradient_check(lambda t: t.sigmoid().sum(), x)\n",
    "    assert passed, f\"gradient error: {error:.2e}\"\n",
    "\n",
    "def test_gradient_softmax():\n",
    "    \"\"\"Test gradient of softmax.\"\"\"\n",
    "    x = Tensor([[1.0, 2.0, 3.0]], requires_grad=True)\n",
    "    passed, error = gradient_check(lambda t: t.softmax().sum(), x)\n",
    "    assert passed, f\"gradient error: {error:.2e}\"\n",
    "\n",
    "def test_gradient_log():\n",
    "    \"\"\"Test gradient of log.\"\"\"\n",
    "    x = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "    passed, error = gradient_check(lambda t: t.log().sum(), x)\n",
    "    assert passed, f\"gradient error: {error:.2e}\"\n",
    "\n",
    "def test_gradient_mean():\n",
    "    \"\"\"Test gradient of mean.\"\"\"\n",
    "    x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "    passed, error = gradient_check(lambda t: t.mean(), x)\n",
    "    assert passed, f\"gradient error: {error:.2e}\"\n",
    "\n",
    "# Run gradient tests\n",
    "print(\"\\nTesting Gradients:\")\n",
    "print(\"-\" * 50)\n",
    "result = TestResult()\n",
    "\n",
    "for test in [test_gradient_addition, test_gradient_multiplication, test_gradient_power,\n",
    "             test_gradient_matmul, test_gradient_relu, test_gradient_sigmoid,\n",
    "             test_gradient_softmax, test_gradient_log, test_gradient_mean]:\n",
    "    run_test(test, result)\n",
    "\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Testing Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_linear_layer_shape():\n",
    "    \"\"\"Test Linear layer output shape.\"\"\"\n",
    "    layer = Linear(10, 5)\n",
    "    x = Tensor(np.random.randn(3, 10).astype(np.float32))\n",
    "    y = layer(x)\n",
    "    assert y.shape == (3, 5), f\"expected (3, 5), got {y.shape}\"\n",
    "\n",
    "def test_linear_layer_gradient():\n",
    "    \"\"\"Test Linear layer gradients.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    layer = Linear(4, 3)\n",
    "    x = Tensor(np.random.randn(2, 4).astype(np.float32), requires_grad=True)\n",
    "    \n",
    "    # Forward and backward\n",
    "    y = layer(x)\n",
    "    loss = y.sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Check gradients exist\n",
    "    assert layer.weight.grad is not None, \"weight gradient is None\"\n",
    "    assert layer.bias.grad is not None, \"bias gradient is None\"\n",
    "    assert x.grad is not None, \"input gradient is None\"\n",
    "    \n",
    "    # Check gradient shapes\n",
    "    assert layer.weight.grad.shape == (4, 3), f\"weight grad shape wrong\"\n",
    "    assert layer.bias.grad.shape == (3,), f\"bias grad shape wrong\"\n",
    "    assert x.grad.shape == (2, 4), f\"input grad shape wrong\"\n",
    "\n",
    "def test_dropout_training_mode():\n",
    "    \"\"\"Test Dropout drops values in training mode.\"\"\"\n",
    "    set_seed(42)\n",
    "    dropout = Dropout(p=0.5)\n",
    "    dropout.train()\n",
    "    \n",
    "    x = Tensor(np.ones((100, 100)))\n",
    "    y = dropout(x)\n",
    "    \n",
    "    # Some values should be 0\n",
    "    num_zeros = np.sum(y.data == 0)\n",
    "    assert num_zeros > 0, \"no values dropped\"\n",
    "    \n",
    "    # Non-zero values should be scaled (approximately 2x for p=0.5)\n",
    "    non_zero_vals = y.data[y.data != 0]\n",
    "    mean_non_zero = np.mean(non_zero_vals)\n",
    "    assert 1.8 < mean_non_zero < 2.2, f\"scaling wrong: {mean_non_zero}\"\n",
    "\n",
    "def test_dropout_eval_mode():\n",
    "    \"\"\"Test Dropout is identity in eval mode.\"\"\"\n",
    "    dropout = Dropout(p=0.5)\n",
    "    dropout.eval()\n",
    "    \n",
    "    x = Tensor(np.ones((10, 10)))\n",
    "    y = dropout(x)\n",
    "    \n",
    "    assert_close(y, x, \"dropout in eval mode should be identity\")\n",
    "\n",
    "def test_sequential_forward():\n",
    "    \"\"\"Test Sequential container forward pass.\"\"\"\n",
    "    model = Sequential(\n",
    "        Linear(10, 5),\n",
    "        ReLU(),\n",
    "        Linear(5, 2)\n",
    "    )\n",
    "    \n",
    "    x = Tensor(np.random.randn(3, 10).astype(np.float32))\n",
    "    y = model(x)\n",
    "    \n",
    "    assert y.shape == (3, 2), f\"expected (3, 2), got {y.shape}\"\n",
    "\n",
    "def test_sequential_parameters():\n",
    "    \"\"\"Test Sequential collects all parameters.\"\"\"\n",
    "    model = Sequential(\n",
    "        Linear(10, 5),  # 10*5 + 5 = 55 params\n",
    "        ReLU(),  # 0 params\n",
    "        Linear(5, 2)  # 5*2 + 2 = 12 params\n",
    "    )\n",
    "    \n",
    "    params = model.parameters()\n",
    "    total = sum(p.size for p in params)\n",
    "    \n",
    "    assert total == 67, f\"expected 67 params, got {total}\"\n",
    "\n",
    "# Run layer tests\n",
    "print(\"\\nTesting Layers:\")\n",
    "print(\"-\" * 50)\n",
    "result = TestResult()\n",
    "\n",
    "for test in [test_linear_layer_shape, test_linear_layer_gradient,\n",
    "             test_dropout_training_mode, test_dropout_eval_mode,\n",
    "             test_sequential_forward, test_sequential_parameters]:\n",
    "    run_test(test, result)\n",
    "\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Testing Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mse_loss_value():\n",
    "    \"\"\"Test MSE loss computes correct value.\"\"\"\n",
    "    pred = Tensor([1.0, 2.0, 3.0])\n",
    "    target = Tensor([1.5, 2.0, 2.5])\n",
    "    \n",
    "    loss_fn = MSELoss()\n",
    "    loss = loss_fn(pred, target)\n",
    "    \n",
    "    expected = np.mean([0.25, 0, 0.25])  # (0.5^2 + 0 + 0.5^2) / 3\n",
    "    assert_close(loss, expected, \"MSE value\")\n",
    "\n",
    "def test_mse_loss_gradient():\n",
    "    \"\"\"Test MSE loss gradient.\"\"\"\n",
    "    pred = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "    target = Tensor([1.5, 2.0, 2.5])\n",
    "    \n",
    "    loss_fn = MSELoss()\n",
    "    passed, error = gradient_check(\n",
    "        lambda p: loss_fn(p, target), \n",
    "        pred\n",
    "    )\n",
    "    assert passed, f\"gradient error: {error:.2e}\"\n",
    "\n",
    "def test_cross_entropy_loss_gradient():\n",
    "    \"\"\"Test cross-entropy loss gradient.\"\"\"\n",
    "    logits = Tensor([[2.0, 1.0, 0.1], [0.5, 2.0, 0.3]], requires_grad=True)\n",
    "    targets = Tensor([0, 1])\n",
    "    \n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    \n",
    "    # Gradient check\n",
    "    passed, error = gradient_check(\n",
    "        lambda l: loss_fn(l, targets),\n",
    "        logits,\n",
    "        atol=1e-3  # Slightly higher tolerance for CE\n",
    "    )\n",
    "    assert passed, f\"gradient error: {error:.2e}\"\n",
    "\n",
    "# Run loss tests\n",
    "print(\"\\nTesting Loss Functions:\")\n",
    "print(\"-\" * 50)\n",
    "result = TestResult()\n",
    "\n",
    "for test in [test_mse_loss_value, test_mse_loss_gradient, test_cross_entropy_loss_gradient]:\n",
    "    run_test(test, result)\n",
    "\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Testing Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sgd_basic_step():\n",
    "    \"\"\"Test SGD performs correct update.\"\"\"\n",
    "    x = Tensor([1.0], requires_grad=True)\n",
    "    optimizer = SGD([x], lr=0.1)\n",
    "    \n",
    "    # Simulate gradient\n",
    "    x.grad = np.array([2.0])  # Gradient of 2\n",
    "    \n",
    "    # Step\n",
    "    optimizer.step()\n",
    "    \n",
    "    # x = x - lr * grad = 1.0 - 0.1 * 2.0 = 0.8\n",
    "    assert_close(x.data, [0.8], \"SGD step\")\n",
    "\n",
    "def test_sgd_momentum():\n",
    "    \"\"\"Test SGD with momentum accumulates velocity.\"\"\"\n",
    "    x = Tensor([1.0], requires_grad=True)\n",
    "    optimizer = SGD([x], lr=0.1, momentum=0.9)\n",
    "    \n",
    "    # Two steps with same gradient\n",
    "    x.grad = np.array([1.0])\n",
    "    optimizer.step()  # v = 1.0, x = 1.0 - 0.1 * 1.0 = 0.9\n",
    "    \n",
    "    x.grad = np.array([1.0])\n",
    "    optimizer.step()  # v = 0.9 * 1.0 + 1.0 = 1.9, x = 0.9 - 0.1 * 1.9 = 0.71\n",
    "    \n",
    "    assert_close(x.data, [0.71], \"SGD with momentum\", atol=1e-4)\n",
    "\n",
    "def test_adam_converges():\n",
    "    \"\"\"Test Adam converges on simple problem.\"\"\"\n",
    "    x = Tensor([0.0], requires_grad=True)\n",
    "    optimizer = Adam([x], lr=0.1)\n",
    "    \n",
    "    # Minimize (x - 3)^2\n",
    "    for _ in range(100):\n",
    "        loss = (x - 3) ** 2\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    assert abs(x.item() - 3.0) < 0.1, f\"Adam didn't converge: x = {x.item()}\"\n",
    "\n",
    "def test_optimizer_zero_grad():\n",
    "    \"\"\"Test zero_grad resets gradients.\"\"\"\n",
    "    x = Tensor([1.0], requires_grad=True)\n",
    "    optimizer = SGD([x], lr=0.1)\n",
    "    \n",
    "    # Set gradient\n",
    "    x.grad = np.array([5.0])\n",
    "    \n",
    "    # Zero it\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    assert_close(x.grad, [0.0], \"zero_grad\")\n",
    "\n",
    "# Run optimizer tests\n",
    "print(\"\\nTesting Optimizers:\")\n",
    "print(\"-\" * 50)\n",
    "result = TestResult()\n",
    "\n",
    "for test in [test_sgd_basic_step, test_sgd_momentum, test_adam_converges, test_optimizer_zero_grad]:\n",
    "    run_test(test, result)\n",
    "\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: End-to-End Training Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_end_to_end_training():\n",
    "    \"\"\"Test full training loop works and improves loss.\"\"\"\n",
    "    set_seed(42)\n",
    "    \n",
    "    # Simple XOR problem\n",
    "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "    y = np.array([0, 1, 1, 0], dtype=np.int32)  # XOR\n",
    "    \n",
    "    # Model\n",
    "    model = Sequential(\n",
    "        Linear(2, 8),\n",
    "        ReLU(),\n",
    "        Linear(8, 2)\n",
    "    )\n",
    "    \n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=0.1)\n",
    "    \n",
    "    # Training\n",
    "    X_tensor = Tensor(X, requires_grad=True)\n",
    "    y_tensor = Tensor(y)\n",
    "    \n",
    "    initial_loss = loss_fn(model(X_tensor), y_tensor).item()\n",
    "    \n",
    "    for _ in range(500):\n",
    "        logits = model(X_tensor)\n",
    "        loss = loss_fn(logits, y_tensor)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    final_loss = loss_fn(model(X_tensor), y_tensor).item()\n",
    "    \n",
    "    # Check loss decreased\n",
    "    assert final_loss < initial_loss * 0.1, \\\n",
    "        f\"loss didn't decrease enough: {initial_loss:.4f} -> {final_loss:.4f}\"\n",
    "    \n",
    "    # Check accuracy\n",
    "    model.eval()\n",
    "    predictions = np.argmax(model(X_tensor).data, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    assert accuracy == 1.0, f\"didn't solve XOR: accuracy = {accuracy}\"\n",
    "\n",
    "# Run end-to-end test\n",
    "print(\"\\nTesting End-to-End Training:\")\n",
    "print(\"-\" * 50)\n",
    "result = TestResult()\n",
    "run_test(test_end_to_end_training, result)\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Running All Tests\n",
    "\n",
    "Let's run the complete test suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete test suite\n",
    "all_tests = [\n",
    "    # Tensor operations\n",
    "    test_tensor_creation, test_tensor_addition, test_tensor_multiplication,\n",
    "    test_tensor_matmul, test_tensor_broadcasting, test_tensor_sum, test_tensor_mean,\n",
    "    \n",
    "    # Gradients\n",
    "    test_gradient_addition, test_gradient_multiplication, test_gradient_power,\n",
    "    test_gradient_matmul, test_gradient_relu, test_gradient_sigmoid,\n",
    "    test_gradient_softmax, test_gradient_log, test_gradient_mean,\n",
    "    \n",
    "    # Layers\n",
    "    test_linear_layer_shape, test_linear_layer_gradient,\n",
    "    test_dropout_training_mode, test_dropout_eval_mode,\n",
    "    test_sequential_forward, test_sequential_parameters,\n",
    "    \n",
    "    # Loss functions\n",
    "    test_mse_loss_value, test_mse_loss_gradient, test_cross_entropy_loss_gradient,\n",
    "    \n",
    "    # Optimizers\n",
    "    test_sgd_basic_step, test_sgd_momentum, test_adam_converges, test_optimizer_zero_grad,\n",
    "    \n",
    "    # End-to-end\n",
    "    test_end_to_end_training,\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FULL TEST SUITE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result = TestResult()\n",
    "for test in all_tests:\n",
    "    run_test(test, result)\n",
    "\n",
    "all_passed = result.summary()\n",
    "\n",
    "if all_passed:\n",
    "    print(\"\\nüéâ All tests passed! Your MicroGrad+ implementation is correct!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Some tests failed. Review the errors above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself: Write Your Own Tests\n",
    "\n",
    "Write tests for:\n",
    "1. Tensor `reshape` operation\n",
    "2. Tensor `tanh` activation gradient\n",
    "3. `BatchNorm` layer (if you implemented it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def test_tensor_reshape():\n",
    "    \"\"\"Test reshape operation.\"\"\"\n",
    "    # TODO: Create a tensor, reshape it, verify shape and values\n",
    "    pass\n",
    "\n",
    "def test_gradient_tanh():\n",
    "    \"\"\"Test tanh gradient.\"\"\"\n",
    "    # TODO: Use gradient_check on tanh\n",
    "    pass\n",
    "\n",
    "# Run your tests\n",
    "# result = TestResult()\n",
    "# run_test(test_tensor_reshape, result)\n",
    "# run_test(test_gradient_tanh, result)\n",
    "# result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How to structure tests for neural network code\n",
    "- ‚úÖ How to verify tensor operations work correctly\n",
    "- ‚úÖ How to use gradient checking to validate backpropagation\n",
    "- ‚úÖ How to test layers, loss functions, and optimizers\n",
    "- ‚úÖ How to run end-to-end training tests\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cleanup - release memory\nfrom micrograd_plus.utils import cleanup_notebook\ncleanup_notebook(globals())"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}