{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 1.4.5: Probability Distributions Lab\n\n**Module: 1.4.4\n- Knowledge of: Basic probability, logarithms\n\n---\n\n## üåç Real-World Context\n\n**Why does probability matter in deep learning?**\n\nEvery loss function comes from probability theory!\n\n| Task | Distribution | Loss Function |\n|------|--------------|---------------|\n| Regression | Gaussian | Mean Squared Error |\n| Binary Classification | Bernoulli | Binary Cross-Entropy |\n| Multi-class Classification | Categorical | Cross-Entropy |\n| Language Modeling | Categorical | Cross-Entropy |\n\nUnderstanding this connection helps you:\n- Choose the right loss for your problem\n- Interpret model outputs as probabilities\n- Understand techniques like temperature sampling in LLMs"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What is Probability?\n",
    "\n",
    "> **Imagine you're a weather predictor...**\n",
    ">\n",
    "> You say: \"There's a 70% chance of rain tomorrow.\"\n",
    ">\n",
    "> This means:\n",
    "> - If we had 100 tomorrows just like this one\n",
    "> - About 70 of them would be rainy\n",
    "> - About 30 would be sunny\n",
    ">\n",
    "> **In neural networks:**\n",
    "> - The network outputs probabilities: \"80% chance this is a cat\"\n",
    "> - Training = making these predictions more accurate\n",
    "> - Loss functions measure \"how wrong were your probabilities?\"\n",
    ">\n",
    "> **Maximum Likelihood:**\n",
    "> \"What probability distribution makes the data we observed most likely?\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"üöÄ Probability Distributions Lab\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Key Probability Distributions\n",
    "\n",
    "Let's implement and visualize the distributions used in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. GAUSSIAN (NORMAL) DISTRIBUTION\n",
    "# Used for: Regression, weight initialization, noise modeling\n",
    "\n",
    "def gaussian_pdf(x, mu=0, sigma=1):\n",
    "    \"\"\"\n",
    "    Gaussian probability density function.\n",
    "    \n",
    "    p(x) = (1 / œÉ‚àö(2œÄ)) √ó exp(-(x-Œº)¬≤ / 2œÉ¬≤)\n",
    "    \n",
    "    Args:\n",
    "        x: Input values\n",
    "        mu: Mean\n",
    "        sigma: Standard deviation\n",
    "    \"\"\"\n",
    "    coeff = 1 / (sigma * np.sqrt(2 * np.pi))\n",
    "    exponent = -((x - mu) ** 2) / (2 * sigma ** 2)\n",
    "    return coeff * np.exp(exponent)\n",
    "\n",
    "def gaussian_log_pdf(x, mu=0, sigma=1):\n",
    "    \"\"\"Log of Gaussian PDF (more numerically stable)\"\"\"\n",
    "    return -0.5 * np.log(2 * np.pi) - np.log(sigma) - ((x - mu) ** 2) / (2 * sigma ** 2)\n",
    "\n",
    "# Visualize\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Different means\n",
    "for mu in [-2, 0, 2]:\n",
    "    axes[0].plot(x, gaussian_pdf(x, mu=mu, sigma=1), linewidth=2, label=f'Œº={mu}, œÉ=1')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('p(x)')\n",
    "axes[0].set_title('Gaussian Distribution (varying mean)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Different variances\n",
    "for sigma in [0.5, 1, 2]:\n",
    "    axes[1].plot(x, gaussian_pdf(x, mu=0, sigma=sigma), linewidth=2, label=f'Œº=0, œÉ={sigma}')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('p(x)')\n",
    "axes[1].set_title('Gaussian Distribution (varying std)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Gaussian Distribution:\")\n",
    "print(\"  - Symmetric bell curve\")\n",
    "print(\"  - Œº controls center, œÉ controls spread\")\n",
    "print(\"  - Used for regression outputs and weight init\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. BERNOULLI DISTRIBUTION\n",
    "# Used for: Binary classification\n",
    "\n",
    "def bernoulli_pmf(k, p):\n",
    "    \"\"\"\n",
    "    Bernoulli probability mass function.\n",
    "    \n",
    "    p(k) = p^k √ó (1-p)^(1-k), where k ‚àà {0, 1}\n",
    "    \n",
    "    Args:\n",
    "        k: Outcome (0 or 1)\n",
    "        p: Probability of k=1\n",
    "    \"\"\"\n",
    "    return (p ** k) * ((1 - p) ** (1 - k))\n",
    "\n",
    "def bernoulli_log_pmf(k, p):\n",
    "    \"\"\"Log of Bernoulli PMF\"\"\"\n",
    "    # Add small epsilon for numerical stability\n",
    "    eps = 1e-10\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    return k * np.log(p) + (1 - k) * np.log(1 - p)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "probs = [0.2, 0.5, 0.8]\n",
    "x_pos = np.arange(len(probs))\n",
    "width = 0.25\n",
    "\n",
    "for i, p in enumerate(probs):\n",
    "    prob_0 = bernoulli_pmf(0, p)\n",
    "    prob_1 = bernoulli_pmf(1, p)\n",
    "    \n",
    "    ax.bar(i - width/2, prob_0, width, label=f'k=0 (p={p})' if i == 0 else '', \n",
    "           color='red', alpha=0.7)\n",
    "    ax.bar(i + width/2, prob_1, width, label=f'k=1 (p={p})' if i == 0 else '',\n",
    "           color='blue', alpha=0.7)\n",
    "    \n",
    "    # Annotate\n",
    "    ax.annotate(f'{prob_0:.2f}', (i - width/2, prob_0 + 0.02), ha='center')\n",
    "    ax.annotate(f'{prob_1:.2f}', (i + width/2, prob_1 + 0.02), ha='center')\n",
    "\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([f'p={p}' for p in probs])\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_title('Bernoulli Distribution for Different p values')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add legend\n",
    "ax.bar([], [], color='red', alpha=0.7, label='k=0 (failure)')\n",
    "ax.bar([], [], color='blue', alpha=0.7, label='k=1 (success)')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Bernoulli Distribution:\")\n",
    "print(\"  - Models binary outcomes (yes/no, spam/not spam)\")\n",
    "print(\"  - p = probability of success (k=1)\")\n",
    "print(\"  - Network outputs p via sigmoid activation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. CATEGORICAL DISTRIBUTION\n",
    "# Used for: Multi-class classification, language modeling\n",
    "\n",
    "def categorical_pmf(k, probs):\n",
    "    \"\"\"\n",
    "    Categorical probability mass function.\n",
    "    \n",
    "    p(k) = probs[k]\n",
    "    \n",
    "    Args:\n",
    "        k: Class index (0 to K-1)\n",
    "        probs: Probability vector (must sum to 1)\n",
    "    \"\"\"\n",
    "    return probs[k]\n",
    "\n",
    "def categorical_log_pmf(k, probs):\n",
    "    \"\"\"Log of categorical PMF\"\"\"\n",
    "    eps = 1e-10\n",
    "    return np.log(probs[k] + eps)\n",
    "\n",
    "def softmax(logits):\n",
    "    \"\"\"Convert logits to probabilities\"\"\"\n",
    "    exp_logits = np.exp(logits - np.max(logits))  # Subtract max for stability\n",
    "    return exp_logits / exp_logits.sum()\n",
    "\n",
    "# Visualize with example: ImageNet classes\n",
    "classes = ['cat', 'dog', 'bird', 'fish', 'horse']\n",
    "\n",
    "# Network output (logits) ‚Üí softmax ‚Üí probabilities\n",
    "logits = np.array([2.5, 1.2, 0.5, -0.3, -1.0])\n",
    "probs = softmax(logits)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Logits\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(classes)))\n",
    "axes[0].bar(classes, logits, color=colors)\n",
    "axes[0].set_ylabel('Logit Value')\n",
    "axes[0].set_title('Network Output (Logits)')\n",
    "axes[0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Probabilities after softmax\n",
    "axes[1].bar(classes, probs, color=colors)\n",
    "axes[1].set_ylabel('Probability')\n",
    "axes[1].set_title('After Softmax (Probabilities)')\n",
    "for i, (c, p) in enumerate(zip(classes, probs)):\n",
    "    axes[1].annotate(f'{p:.2%}', (i, p + 0.02), ha='center')\n",
    "axes[1].set_ylim(0, max(probs) * 1.2)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Categorical Distribution:\")\n",
    "print(f\"  - Logits: {logits}\")\n",
    "print(f\"  - After softmax: {probs.round(3)}\")\n",
    "print(f\"  - Probabilities sum to: {probs.sum():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "**The Big Idea:** Find parameters that maximize the probability of observed data.\n",
    "\n",
    "### Likelihood vs Probability\n",
    "\n",
    "- **Probability:** P(data | parameters) - \"Given these parameters, how likely is this data?\"\n",
    "- **Likelihood:** L(parameters | data) - \"Given this data, which parameters are most likely?\"\n",
    "\n",
    "### üßí ELI5: Maximum Likelihood\n",
    "\n",
    "> **Imagine you found some ancient coins...**\n",
    ">\n",
    "> You flip them 10 times: H H T H H H T H H H (8 heads, 2 tails)\n",
    ">\n",
    "> **Question:** Is this a fair coin (p=0.5) or biased?\n",
    ">\n",
    "> **MLE approach:** Find the p that makes 8/10 heads most likely.\n",
    "> - If p=0.5: Probability of exactly 8 heads = small\n",
    "> - If p=0.8: Probability of exactly 8 heads = large!\n",
    "> - MLE says: p = 0.8 (best explains the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE for Bernoulli: Binary Classification\n",
    "\n",
    "# Observed data: coin flips\n",
    "data = np.array([1, 1, 0, 1, 1, 1, 0, 1, 1, 1])  # 8 heads (1), 2 tails (0)\n",
    "\n",
    "def bernoulli_likelihood(p, data):\n",
    "    \"\"\"Likelihood of data under Bernoulli with parameter p\"\"\"\n",
    "    return np.prod([bernoulli_pmf(k, p) for k in data])\n",
    "\n",
    "def bernoulli_log_likelihood(p, data):\n",
    "    \"\"\"Log-likelihood (more stable)\"\"\"\n",
    "    return np.sum([bernoulli_log_pmf(k, p) for k in data])\n",
    "\n",
    "# Compute likelihood for different p values\n",
    "p_values = np.linspace(0.01, 0.99, 100)\n",
    "likelihoods = [bernoulli_likelihood(p, data) for p in p_values]\n",
    "log_likelihoods = [bernoulli_log_likelihood(p, data) for p in p_values]\n",
    "\n",
    "# Find MLE\n",
    "mle_idx = np.argmax(log_likelihoods)\n",
    "mle_p = p_values[mle_idx]\n",
    "\n",
    "# Analytical MLE for Bernoulli: p = mean(data)\n",
    "analytical_mle = data.mean()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Likelihood\n",
    "axes[0].plot(p_values, likelihoods, 'b-', linewidth=2)\n",
    "axes[0].axvline(x=mle_p, color='red', linestyle='--', label=f'MLE: p={mle_p:.2f}')\n",
    "axes[0].set_xlabel('p (probability of heads)')\n",
    "axes[0].set_ylabel('Likelihood')\n",
    "axes[0].set_title('Likelihood Function')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Log-likelihood (easier to work with)\n",
    "axes[1].plot(p_values, log_likelihoods, 'b-', linewidth=2)\n",
    "axes[1].axvline(x=mle_p, color='red', linestyle='--', label=f'MLE: p={mle_p:.2f}')\n",
    "axes[1].set_xlabel('p (probability of heads)')\n",
    "axes[1].set_ylabel('Log-Likelihood')\n",
    "axes[1].set_title('Log-Likelihood Function')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Data: {data} ({data.sum()} heads, {len(data)-data.sum()} tails)\")\n",
    "print(f\"MLE (numerical): p = {mle_p:.4f}\")\n",
    "print(f\"MLE (analytical): p = {analytical_mle:.4f}\")\n",
    "print(f\"\\nThe MLE is simply the fraction of heads!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Deriving MSE Loss from Gaussian MLE\n",
    "\n",
    "**Key insight:** MSE loss = negative log-likelihood of a Gaussian!\n",
    "\n",
    "### The Derivation\n",
    "\n",
    "Assume our model's predictions follow: $y \\sim \\mathcal{N}(\\hat{y}, \\sigma^2)$\n",
    "\n",
    "**Log-likelihood for one sample:**\n",
    "$$\\log p(y|\\hat{y}, \\sigma) = -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(y-\\hat{y})^2}{2\\sigma^2}$$\n",
    "\n",
    "**Negative log-likelihood:**\n",
    "$$-\\log p(y|\\hat{y}) \\propto (y - \\hat{y})^2$$\n",
    "\n",
    "This is exactly **Mean Squared Error** (up to constants)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the connection between Gaussian MLE and MSE\n",
    "\n",
    "# True values and predictions\n",
    "y_true = np.array([1.0, 2.5, 3.0, 4.5, 5.0])\n",
    "y_pred = np.array([1.2, 2.3, 3.5, 4.2, 5.1])\n",
    "\n",
    "# MSE Loss (what we usually use)\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Gaussian Negative Log-Likelihood (with œÉ=1)\n",
    "def gaussian_nll(y_true, y_pred, sigma=1.0):\n",
    "    \"\"\"Negative log-likelihood under Gaussian assumption\"\"\"\n",
    "    n = len(y_true)\n",
    "    const = 0.5 * n * np.log(2 * np.pi * sigma**2)\n",
    "    squared_errors = np.sum((y_true - y_pred)**2) / (2 * sigma**2)\n",
    "    return const + squared_errors\n",
    "\n",
    "# Compute both\n",
    "mse = mse_loss(y_true, y_pred)\n",
    "nll = gaussian_nll(y_true, y_pred, sigma=1.0)\n",
    "\n",
    "print(\"Gaussian MLE ‚Üí MSE Connection\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"True values:  {y_true}\")\n",
    "print(f\"Predictions:  {y_pred}\")\n",
    "print()\n",
    "print(f\"MSE Loss:     {mse:.6f}\")\n",
    "print(f\"Gaussian NLL: {nll:.6f}\")\n",
    "print()\n",
    "print(\"Notice: Minimizing MSE = Maximizing Gaussian likelihood!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual demonstration\n",
    "\n",
    "# Single prediction example\n",
    "y_true_single = 3.0\n",
    "y_preds = np.linspace(0, 6, 100)\n",
    "\n",
    "# Compute MSE and NLL for each prediction\n",
    "mses = (y_true_single - y_preds) ** 2\n",
    "nlls = -gaussian_log_pdf(y_true_single, mu=y_preds, sigma=1.0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# MSE vs prediction\n",
    "axes[0].plot(y_preds, mses, 'b-', linewidth=2)\n",
    "axes[0].axvline(x=y_true_single, color='red', linestyle='--', label=f'True y = {y_true_single}')\n",
    "axes[0].set_xlabel('Prediction ≈∑')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('MSE Loss vs Prediction')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# NLL vs prediction\n",
    "axes[1].plot(y_preds, nlls, 'b-', linewidth=2)\n",
    "axes[1].axvline(x=y_true_single, color='red', linestyle='--', label=f'True y = {y_true_single}')\n",
    "axes[1].set_xlabel('Prediction ≈∑')\n",
    "axes[1].set_ylabel('Negative Log-Likelihood')\n",
    "axes[1].set_title('Gaussian NLL vs Prediction')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Both curves have the same shape!\")\n",
    "print(\"They only differ by constants (which don't affect optimization).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Deriving Cross-Entropy from Categorical MLE\n",
    "\n",
    "**Key insight:** Cross-entropy loss = negative log-likelihood of Categorical!\n",
    "\n",
    "### The Derivation\n",
    "\n",
    "For classification with one-hot label $y$ and predicted probabilities $\\hat{p}$:\n",
    "\n",
    "**Log-likelihood:**\n",
    "$$\\log p(y|\\hat{p}) = \\sum_k y_k \\log \\hat{p}_k$$\n",
    "\n",
    "**Negative log-likelihood (Cross-Entropy):**\n",
    "$$H(y, \\hat{p}) = -\\sum_k y_k \\log \\hat{p}_k$$\n",
    "\n",
    "This is the **Cross-Entropy Loss**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Entropy Loss implementation\n",
    "\n",
    "def cross_entropy_loss(y_true_onehot, y_pred_probs):\n",
    "    \"\"\"\n",
    "    Cross-entropy loss for classification.\n",
    "    \n",
    "    H(y, p) = -Œ£ y_k log(p_k)\n",
    "    \n",
    "    Args:\n",
    "        y_true_onehot: One-hot encoded true labels\n",
    "        y_pred_probs: Predicted probabilities (after softmax)\n",
    "    \"\"\"\n",
    "    eps = 1e-10  # Prevent log(0)\n",
    "    return -np.sum(y_true_onehot * np.log(y_pred_probs + eps))\n",
    "\n",
    "def categorical_nll(y_true_class, y_pred_probs):\n",
    "    \"\"\"\n",
    "    Negative log-likelihood for categorical distribution.\n",
    "    \n",
    "    NLL = -log(p_true_class)\n",
    "    \n",
    "    Args:\n",
    "        y_true_class: True class index\n",
    "        y_pred_probs: Predicted probabilities\n",
    "    \"\"\"\n",
    "    eps = 1e-10\n",
    "    return -np.log(y_pred_probs[y_true_class] + eps)\n",
    "\n",
    "# Example: 5-class classification\n",
    "n_classes = 5\n",
    "true_class = 2  # True label is class 2\n",
    "\n",
    "# One-hot encoding\n",
    "y_true_onehot = np.zeros(n_classes)\n",
    "y_true_onehot[true_class] = 1\n",
    "\n",
    "# Predicted probabilities (from softmax)\n",
    "logits = np.array([0.5, 1.0, 2.5, 0.3, -0.2])  # Network output\n",
    "y_pred_probs = softmax(logits)\n",
    "\n",
    "# Compute both losses\n",
    "ce_loss = cross_entropy_loss(y_true_onehot, y_pred_probs)\n",
    "nll = categorical_nll(true_class, y_pred_probs)\n",
    "\n",
    "print(\"Categorical MLE ‚Üí Cross-Entropy Connection\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"True class: {true_class}\")\n",
    "print(f\"One-hot:    {y_true_onehot}\")\n",
    "print(f\"Logits:     {logits}\")\n",
    "print(f\"Probs:      {y_pred_probs.round(4)}\")\n",
    "print()\n",
    "print(f\"Cross-Entropy Loss: {ce_loss:.6f}\")\n",
    "print(f\"Categorical NLL:    {nll:.6f}\")\n",
    "print()\n",
    "print(\"They're identical! Cross-entropy IS the categorical NLL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how cross-entropy penalizes wrong predictions\n",
    "\n",
    "# True class is 0\n",
    "# Vary the predicted probability for class 0\n",
    "p_true_class = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "# Cross-entropy = -log(p_true_class)\n",
    "ce_losses = -np.log(p_true_class)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Cross-entropy vs probability\n",
    "axes[0].plot(p_true_class, ce_losses, 'b-', linewidth=2)\n",
    "axes[0].axvline(x=1.0, color='green', linestyle='--', label='Perfect prediction')\n",
    "axes[0].axhline(y=0, color='green', linestyle='--')\n",
    "axes[0].set_xlabel('Probability assigned to true class')\n",
    "axes[0].set_ylabel('Cross-Entropy Loss')\n",
    "axes[0].set_title('Cross-Entropy vs Correct Class Probability')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xlim(0, 1)\n",
    "\n",
    "# Show gradient (steeper when very wrong)\n",
    "gradient = 1 / p_true_class\n",
    "axes[1].plot(p_true_class, gradient, 'r-', linewidth=2)\n",
    "axes[1].set_xlabel('Probability assigned to true class')\n",
    "axes[1].set_ylabel('Gradient magnitude')\n",
    "axes[1].set_title('Gradient of Cross-Entropy')\n",
    "axes[1].set_ylim(0, 20)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight:\")\n",
    "print(\"  - Loss ‚Üí 0 as probability ‚Üí 1 (correct prediction)\")\n",
    "print(\"  - Loss ‚Üí ‚àû as probability ‚Üí 0 (wrong prediction)\")\n",
    "print(\"  - Gradient is larger for worse predictions (faster learning!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: KL Divergence and Information Theory\n",
    "\n",
    "### What is KL Divergence?\n",
    "\n",
    "KL Divergence measures how different two probability distributions are:\n",
    "\n",
    "$$D_{KL}(P || Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}$$\n",
    "\n",
    "### üßí ELI5: KL Divergence\n",
    "\n",
    "> **Imagine you're using a map to navigate...**\n",
    ">\n",
    "> - P = the real terrain\n",
    "> - Q = your map\n",
    ">\n",
    "> KL Divergence measures: \"How much trouble will I get into using this map?\"\n",
    "> - KL = 0: Perfect map! No trouble.\n",
    "> - KL > 0: Map is wrong. Higher = more trouble.\n",
    "\n",
    "### Key Relationship\n",
    "\n",
    "$$\\text{Cross-Entropy}(P, Q) = \\text{Entropy}(P) + D_{KL}(P || Q)$$\n",
    "\n",
    "Since Entropy(P) is constant during training:\n",
    "- **Minimizing Cross-Entropy = Minimizing KL Divergence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    \"\"\"Compute entropy of distribution p\"\"\"\n",
    "    eps = 1e-10\n",
    "    p = np.clip(p, eps, 1-eps)\n",
    "    return -np.sum(p * np.log(p))\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    \"\"\"Compute KL divergence D_KL(p || q)\"\"\"\n",
    "    eps = 1e-10\n",
    "    p = np.clip(p, eps, 1-eps)\n",
    "    q = np.clip(q, eps, 1-eps)\n",
    "    return np.sum(p * np.log(p / q))\n",
    "\n",
    "def cross_entropy_probs(p, q):\n",
    "    \"\"\"Compute cross-entropy H(p, q)\"\"\"\n",
    "    eps = 1e-10\n",
    "    q = np.clip(q, eps, 1-eps)\n",
    "    return -np.sum(p * np.log(q))\n",
    "\n",
    "# Example: True distribution vs predicted\n",
    "p_true = np.array([0.7, 0.2, 0.1])  # True distribution\n",
    "q_pred = np.array([0.5, 0.3, 0.2])  # Predicted distribution\n",
    "\n",
    "H_p = entropy(p_true)\n",
    "D_kl = kl_divergence(p_true, q_pred)\n",
    "H_pq = cross_entropy_probs(p_true, q_pred)\n",
    "\n",
    "print(\"Information Theory Relationships\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"True distribution P:      {p_true}\")\n",
    "print(f\"Predicted distribution Q: {q_pred}\")\n",
    "print()\n",
    "print(f\"Entropy H(P):        {H_p:.6f}\")\n",
    "print(f\"KL Divergence:       {D_kl:.6f}\")\n",
    "print(f\"Cross-Entropy H(P,Q): {H_pq:.6f}\")\n",
    "print()\n",
    "print(f\"H(P) + D_KL(P||Q) = {H_p + D_kl:.6f}\")\n",
    "print(f\"This equals Cross-Entropy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize KL divergence as Q approaches P\n",
    "\n",
    "# Fix P, vary Q\n",
    "p = np.array([0.7, 0.3])\n",
    "\n",
    "q_probs = np.linspace(0.1, 0.9, 100)  # Probability of first class in Q\n",
    "kl_values = []\n",
    "ce_values = []\n",
    "\n",
    "for q1 in q_probs:\n",
    "    q = np.array([q1, 1 - q1])\n",
    "    kl_values.append(kl_divergence(p, q))\n",
    "    ce_values.append(cross_entropy_probs(p, q))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# KL Divergence\n",
    "axes[0].plot(q_probs, kl_values, 'b-', linewidth=2)\n",
    "axes[0].axvline(x=p[0], color='red', linestyle='--', label=f'P = [{p[0]}, {p[1]}]')\n",
    "axes[0].set_xlabel('Q[0] (probability of first class)')\n",
    "axes[0].set_ylabel('KL Divergence')\n",
    "axes[0].set_title('KL Divergence D_KL(P || Q)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cross-entropy\n",
    "axes[1].plot(q_probs, ce_values, 'g-', linewidth=2, label='Cross-Entropy')\n",
    "axes[1].axhline(y=entropy(p), color='orange', linestyle='--', label='Entropy H(P)')\n",
    "axes[1].axvline(x=p[0], color='red', linestyle='--', label=f'P = [{p[0]}, {p[1]}]')\n",
    "axes[1].set_xlabel('Q[0] (probability of first class)')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].set_title('Cross-Entropy = Entropy + KL Divergence')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(f\"  - KL = 0 when Q = P (distributions match)\")\n",
    "print(f\"  - Cross-Entropy is minimized when Q = P\")\n",
    "print(f\"  - At minimum, CE = Entropy(P) = {entropy(p):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Using MSE for Classification\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: MSE for classification (probabilities)\n",
    "loss = np.mean((y_pred_probs - y_true_onehot) ** 2)\n",
    "\n",
    "# ‚úÖ Right: Cross-entropy for classification\n",
    "loss = -np.sum(y_true_onehot * np.log(y_pred_probs))\n",
    "```\n",
    "\n",
    "**Why:** Cross-entropy has better gradient properties for classification.\n",
    "\n",
    "### Mistake 2: Log of Zero\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: No protection against log(0)\n",
    "ce = -np.sum(y * np.log(p))  # Crashes if p contains 0!\n",
    "\n",
    "# ‚úÖ Right: Add small epsilon\n",
    "eps = 1e-10\n",
    "ce = -np.sum(y * np.log(p + eps))\n",
    "```\n",
    "\n",
    "### Mistake 3: Forgetting Softmax\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Cross-entropy on raw logits\n",
    "ce = cross_entropy(y_true, logits)  # Logits aren't probabilities!\n",
    "\n",
    "# ‚úÖ Right: Apply softmax first\n",
    "probs = softmax(logits)\n",
    "ce = cross_entropy(y_true, probs)\n",
    "\n",
    "# Or use PyTorch's combined version (numerically stable)\n",
    "import torch.nn.functional as F\n",
    "ce = F.cross_entropy(logits, labels)  # Handles softmax internally\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise: Implement Binary Cross-Entropy\n",
    "\n",
    "Implement BCE and show it's equivalent to the negative log-likelihood of a Bernoulli distribution.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "Binary cross-entropy:\n",
    "```python\n",
    "BCE = -(y * log(p) + (1-y) * log(1-p))\n",
    "```\n",
    "\n",
    "Compare with Bernoulli log-PMF:\n",
    "```python\n",
    "log_pmf = y * log(p) + (1-y) * log(1-p)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Binary cross-entropy loss.\n",
    "    \n",
    "    BCE = -[y*log(p) + (1-y)*log(1-p)]\n",
    "    \n",
    "    Args:\n",
    "        y_true: Binary labels (0 or 1)\n",
    "        y_pred: Predicted probabilities (0 to 1)\n",
    "    \"\"\"\n",
    "    eps = 1e-10\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# Test data\n",
    "y_true_binary = np.array([1, 0, 1, 1, 0])\n",
    "y_pred_probs_binary = np.array([0.9, 0.2, 0.8, 0.7, 0.3])\n",
    "\n",
    "# Compute BCE\n",
    "bce = binary_cross_entropy(y_true_binary, y_pred_probs_binary)\n",
    "\n",
    "# Compute as Bernoulli NLL\n",
    "bernoulli_nll_values = [-bernoulli_log_pmf(y, p) for y, p in zip(y_true_binary, y_pred_probs_binary)]\n",
    "bernoulli_nll_mean = np.mean(bernoulli_nll_values)\n",
    "\n",
    "print(f\"Binary Cross-Entropy: {bce:.6f}\")\n",
    "print(f\"Bernoulli NLL (mean): {bernoulli_nll_mean:.6f}\")\n",
    "print(f\"\\nThey're the same! BCE = Bernoulli NLL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "\n",
    "- ‚úÖ **Gaussian distribution** ‚Üí MSE loss for regression\n",
    "- ‚úÖ **Bernoulli distribution** ‚Üí Binary Cross-Entropy for binary classification\n",
    "- ‚úÖ **Categorical distribution** ‚Üí Cross-Entropy for multi-class\n",
    "- ‚úÖ **MLE** = finding parameters that maximize probability of data\n",
    "- ‚úÖ **KL Divergence** measures difference between distributions\n",
    "- ‚úÖ **Cross-Entropy = Entropy + KL Divergence**\n",
    "\n",
    "**Key insight:** Loss functions aren't arbitrary - they come from probability theory!\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [Visual Information Theory](https://colah.github.io/posts/2015-09-Visual-Information/) - Excellent visualizations\n",
    "- [Understanding Softmax Cross-Entropy](https://cs231n.github.io/linear-classify/#softmax) - Stanford CS231n\n",
    "- [KL Divergence Explained](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) - Intuitive explanation\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import gc\ngc.collect()\n\nprint(\"‚úÖ Cleanup complete!\")\nprint(\"\\nüéâ Congratulations! You've completed Module: 1.4: Mathematics for Deep Learning!\")\nprint(\"\\n‚û°Ô∏è  Next: Module 1.4 - Neural Network Fundamentals\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}