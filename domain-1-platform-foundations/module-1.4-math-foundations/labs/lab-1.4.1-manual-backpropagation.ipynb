{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 1.4.1: Manual Backpropagation\n\n**Module:** 1.4 - Mathematics for Deep Learning  \n**Time:** 3 hours  \n**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê\n\n---\n\n## üéØ Learning Objectives\n\nBy the end of this notebook, you will:\n- [ ] Understand the forward pass as a series of function compositions\n- [ ] Derive gradients using the chain rule step-by-step\n- [ ] Implement backward pass manually without autograd\n- [ ] Verify your gradients match PyTorch autograd (within 1e-6)\n- [ ] Build intuition for how errors flow backward through networks\n\n---\n\n## üìö Prerequisites\n\n- Completed: Module 1.2 (Python for AI/ML), Module 1.3 (CUDA Python)\n- Knowledge of: Basic calculus (derivatives), matrix multiplication\n\n---\n\n## üåç Real-World Context\n\n**Why does this matter?**\n\nEvery time you call `loss.backward()` in PyTorch, the magic of backpropagation happens automatically. But when:\n- Your model isn't learning (gradients vanishing/exploding?)\n- You need to implement a custom layer\n- You're debugging NaN losses\n- You want to understand why certain architectures work\n\n...you need to understand what's happening under the hood.\n\n**Real example:** When OpenAI trained GPT-3, they discovered gradient issues at scale that required understanding backprop deeply to fix. Engineers who understand backprop can debug what others can't."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What is Backpropagation?\n",
    "\n",
    "> **Imagine you're playing a telephone game with your friends...**\n",
    ">\n",
    "> You whisper \"apple\" to Friend 1, who whispers to Friend 2, who whispers to Friend 3, and at the end, Friend 3 says \"purple\" out loud.\n",
    ">\n",
    "> The teacher (loss function) says: \"That's wrong! The answer should be 'apple', not 'purple'.\"\n",
    ">\n",
    "> Now, how do you fix this?\n",
    ">\n",
    "> **You work BACKWARDS:**\n",
    "> 1. Ask Friend 3: \"What did you hear?\" ‚Üí \"I heard 'turtle'\"\n",
    "> 2. Ask Friend 2: \"What did you say?\" ‚Üí \"I said 'turtle'\" ‚Üí OK, Friend 3 heard correctly\n",
    "> 3. Ask Friend 2: \"What did YOU hear?\" ‚Üí \"I heard 'snapple'\"\n",
    "> 4. Ask Friend 1: \"What did you say?\" ‚Üí \"I said 'snapple'\" ‚Üí Aha! Friend 1 made a mistake!\n",
    ">\n",
    "> **In neural network terms:**\n",
    "> - Each friend = a layer in the network\n",
    "> - The whisper = the activation passed forward\n",
    "> - Working backwards = backpropagation\n",
    "> - Figuring out who messed up = computing gradients\n",
    "> - Telling each friend to speak more clearly = updating weights\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Math Behind Backpropagation\n",
    "\n",
    "### 1.1 The Chain Rule - The Hero of Deep Learning\n",
    "\n",
    "The chain rule is THE fundamental concept behind backpropagation.\n",
    "\n",
    "**Simple version:** If `y = f(g(x))`, then:\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{dy}{dg} \\cdot \\frac{dg}{dx}$$\n",
    "\n",
    "**Think of it like this:** How much does `y` change when `x` changes?\n",
    "- First, how much does `g` change when `x` changes?\n",
    "- Then, how much does `y` change when `g` changes?\n",
    "- Multiply them together!\n",
    "\n",
    "### Visual Example\n",
    "\n",
    "```\n",
    "x = 3\n",
    "   ‚Üì g(x) = x¬≤\n",
    "g = 9\n",
    "   ‚Üì f(g) = 2g + 1\n",
    "y = 19\n",
    "```\n",
    "\n",
    "- dg/dx = 2x = 6 (at x=3)\n",
    "- dy/dg = 2\n",
    "- dy/dx = dy/dg √ó dg/dx = 2 √ó 6 = 12\n",
    "\n",
    "Let's verify this numerically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"üöÄ Mathematics for Deep Learning - Manual Backpropagation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's verify the chain rule numerically\n",
    "\n",
    "def g(x):\n",
    "    return x ** 2\n",
    "\n",
    "def f(g_val):\n",
    "    return 2 * g_val + 1\n",
    "\n",
    "def y(x):\n",
    "    return f(g(x))\n",
    "\n",
    "# Analytical derivatives\n",
    "def dg_dx(x):\n",
    "    return 2 * x\n",
    "\n",
    "def df_dg(g_val):\n",
    "    return 2  # Constant\n",
    "\n",
    "def dy_dx_analytical(x):\n",
    "    \"\"\"Chain rule: dy/dx = df/dg * dg/dx\"\"\"\n",
    "    return df_dg(g(x)) * dg_dx(x)\n",
    "\n",
    "# Numerical derivative (for verification)\n",
    "def numerical_derivative(func, x, eps=1e-7):\n",
    "    \"\"\"Compute derivative numerically using finite differences\"\"\"\n",
    "    return (func(x + eps) - func(x - eps)) / (2 * eps)\n",
    "\n",
    "# Test at x = 3\n",
    "x_test = 3.0\n",
    "\n",
    "analytical = dy_dx_analytical(x_test)\n",
    "numerical = numerical_derivative(y, x_test)\n",
    "\n",
    "print(\"Chain Rule Verification\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"At x = {x_test}:\")\n",
    "print(f\"  g(x) = x¬≤ = {g(x_test)}\")\n",
    "print(f\"  y = f(g(x)) = 2g + 1 = {y(x_test)}\")\n",
    "print()\n",
    "print(f\"Gradients:\")\n",
    "print(f\"  dg/dx = 2x = {dg_dx(x_test)}\")\n",
    "print(f\"  df/dg = 2\")\n",
    "print(f\"  dy/dx = df/dg √ó dg/dx = 2 √ó {dg_dx(x_test)} = {analytical}\")\n",
    "print()\n",
    "print(f\"Verification:\")\n",
    "print(f\"  Analytical dy/dx: {analytical}\")\n",
    "print(f\"  Numerical dy/dx:  {numerical}\")\n",
    "print(f\"  Difference:       {abs(analytical - numerical):.2e}\")\n",
    "print()\n",
    "print(\"‚úÖ Chain rule verified!\" if abs(analytical - numerical) < 1e-5 else \"‚ùå Something's wrong!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "We computed the derivative of a **composite function** two ways:\n",
    "1. **Analytically** using the chain rule (multiply derivatives along the chain)\n",
    "2. **Numerically** using finite differences (tiny changes in input/output)\n",
    "\n",
    "They match! This is exactly what `loss.backward()` does, but for millions of parameters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: A Single Neuron - Forward and Backward\n",
    "\n",
    "Let's start simple: one neuron with one input.\n",
    "\n",
    "### The Forward Pass\n",
    "\n",
    "```\n",
    "Input (x) ‚Üí [√ó weight (w)] ‚Üí [+ bias (b)] ‚Üí [œÉ activation] ‚Üí Output (≈∑)\n",
    "```\n",
    "\n",
    "Mathematically:\n",
    "1. `z = w*x + b` (linear transformation)\n",
    "2. `≈∑ = œÉ(z)` (activation function)\n",
    "3. `L = (≈∑ - y)¬≤` (loss - mean squared error)\n",
    "\n",
    "### The Backward Pass (What we need to find)\n",
    "\n",
    "We need: `‚àÇL/‚àÇw` and `‚àÇL/‚àÇb` (how does the loss change when we change each parameter?)\n",
    "\n",
    "Using chain rule:\n",
    "- `‚àÇL/‚àÇw = ‚àÇL/‚àÇ≈∑ √ó ‚àÇ≈∑/‚àÇz √ó ‚àÇz/‚àÇw`\n",
    "- `‚àÇL/‚àÇb = ‚àÇL/‚àÇ≈∑ √ó ‚àÇ≈∑/‚àÇz √ó ‚àÇz/‚àÇb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Neuron: Manual Forward and Backward Pass\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"Derivative of sigmoid: œÉ'(z) = œÉ(z) √ó (1 - œÉ(z))\"\"\"\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Initialize\n",
    "x = 2.0       # Input\n",
    "y = 1.0       # Target (ground truth)\n",
    "w = 0.5       # Weight\n",
    "b = 0.1       # Bias\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FORWARD PASS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Forward pass (step by step)\n",
    "z = w * x + b\n",
    "print(f\"Step 1: z = w√óx + b = {w}√ó{x} + {b} = {z}\")\n",
    "\n",
    "y_hat = sigmoid(z)\n",
    "print(f\"Step 2: ≈∑ = œÉ(z) = œÉ({z}) = {y_hat:.6f}\")\n",
    "\n",
    "loss = (y_hat - y) ** 2\n",
    "print(f\"Step 3: L = (≈∑ - y)¬≤ = ({y_hat:.6f} - {y})¬≤ = {loss:.6f}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"BACKWARD PASS (Chain Rule)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Backward pass (computing gradients)\n",
    "# ‚àÇL/‚àÇ≈∑ = 2(≈∑ - y)\n",
    "dL_dy_hat = 2 * (y_hat - y)\n",
    "print(f\"‚àÇL/‚àÇ≈∑ = 2(≈∑ - y) = 2({y_hat:.6f} - {y}) = {dL_dy_hat:.6f}\")\n",
    "\n",
    "# ‚àÇ≈∑/‚àÇz = œÉ'(z) = œÉ(z)(1 - œÉ(z))\n",
    "dy_hat_dz = sigmoid_derivative(z)\n",
    "print(f\"‚àÇ≈∑/‚àÇz = œÉ'(z) = œÉ(z)(1-œÉ(z)) = {y_hat:.6f}√ó{1-y_hat:.6f} = {dy_hat_dz:.6f}\")\n",
    "\n",
    "# ‚àÇz/‚àÇw = x\n",
    "dz_dw = x\n",
    "print(f\"‚àÇz/‚àÇw = x = {dz_dw}\")\n",
    "\n",
    "# ‚àÇz/‚àÇb = 1\n",
    "dz_db = 1\n",
    "print(f\"‚àÇz/‚àÇb = 1\")\n",
    "\n",
    "# Chain rule for final gradients\n",
    "print()\n",
    "print(\"Applying Chain Rule:\")\n",
    "dL_dw = dL_dy_hat * dy_hat_dz * dz_dw\n",
    "print(f\"‚àÇL/‚àÇw = ‚àÇL/‚àÇ≈∑ √ó ‚àÇ≈∑/‚àÇz √ó ‚àÇz/‚àÇw = {dL_dy_hat:.6f} √ó {dy_hat_dz:.6f} √ó {dz_dw} = {dL_dw:.6f}\")\n",
    "\n",
    "dL_db = dL_dy_hat * dy_hat_dz * dz_db\n",
    "print(f\"‚àÇL/‚àÇb = ‚àÇL/‚àÇ≈∑ √ó ‚àÇ≈∑/‚àÇz √ó ‚àÇz/‚àÇb = {dL_dy_hat:.6f} √ó {dy_hat_dz:.6f} √ó {dz_db} = {dL_db:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify with PyTorch autograd\n",
    "\n",
    "# Create tensors with gradient tracking\n",
    "x_t = torch.tensor(x, dtype=torch.float32)\n",
    "y_t = torch.tensor(y, dtype=torch.float32)\n",
    "w_t = torch.tensor(w, dtype=torch.float32, requires_grad=True)\n",
    "b_t = torch.tensor(b, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "z_t = w_t * x_t + b_t\n",
    "y_hat_t = torch.sigmoid(z_t)\n",
    "loss_t = (y_hat_t - y_t) ** 2\n",
    "\n",
    "# Backward pass (PyTorch does this automatically!)\n",
    "loss_t.backward()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VERIFICATION WITH PYTORCH AUTOGRAD\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Manual ‚àÇL/‚àÇw:   {dL_dw:.6f}\")\n",
    "print(f\"PyTorch ‚àÇL/‚àÇw:  {w_t.grad.item():.6f}\")\n",
    "print(f\"Difference:     {abs(dL_dw - w_t.grad.item()):.2e}\")\n",
    "print()\n",
    "print(f\"Manual ‚àÇL/‚àÇb:   {dL_db:.6f}\")\n",
    "print(f\"PyTorch ‚àÇL/‚àÇb:  {b_t.grad.item():.6f}\")\n",
    "print(f\"Difference:     {abs(dL_db - b_t.grad.item()):.2e}\")\n",
    "print()\n",
    "\n",
    "if abs(dL_dw - w_t.grad.item()) < 1e-6 and abs(dL_db - b_t.grad.item()) < 1e-6:\n",
    "    print(\"üéâ SUCCESS! Manual gradients match PyTorch autograd!\")\n",
    "else:\n",
    "    print(\"‚ùå Gradients don't match. Check your calculations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã Try It Yourself #1\n",
    "\n",
    "Change the activation function from sigmoid to ReLU and compute the gradients manually.\n",
    "\n",
    "**ReLU Definition:**\n",
    "- `ReLU(z) = max(0, z)`\n",
    "- `ReLU'(z) = 1 if z > 0, else 0`\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "The only thing that changes is `‚àÇ≈∑/‚àÇz`. For ReLU:\n",
    "- If z > 0: derivative is 1\n",
    "- If z ‚â§ 0: derivative is 0\n",
    "\n",
    "Everything else (‚àÇL/‚àÇ≈∑, ‚àÇz/‚àÇw, ‚àÇz/‚àÇb) stays the same!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE: Implement ReLU activation and backward pass\n\ndef relu(z):\n    \"\"\"ReLU activation\"\"\"\n    # TODO: Implement ReLU\n    raise NotImplementedError(\"Implement: return np.maximum(0, z)\")\n\ndef relu_derivative(z):\n    \"\"\"Derivative of ReLU\"\"\"\n    # TODO: Implement ReLU derivative\n    raise NotImplementedError(\"Implement: return (z > 0).astype(float)\")\n\n# Test with the same inputs\n# z = w * x + b  (already computed above)\n# TODO: Compute y_hat_relu, loss_relu, and gradients\n\nprint(\"Your manual ReLU gradients:\")\n# print(f\"‚àÇL/‚àÇw = {your_dL_dw}\")\n# print(f\"‚àÇL/‚àÇb = {your_dL_db}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Multi-Layer Perceptron - The Real Challenge\n",
    "\n",
    "Now let's scale up to a **3-layer network** (input ‚Üí hidden1 ‚Üí hidden2 ‚Üí output).\n",
    "\n",
    "### Network Architecture\n",
    "\n",
    "```\n",
    "Input (2) ‚Üí Hidden1 (3) ‚Üí Hidden2 (2) ‚Üí Output (1)\n",
    "   x          h1            h2           ≈∑\n",
    "```\n",
    "\n",
    "### üßí ELI5: Why Do We Need Multiple Layers?\n",
    "\n",
    "> **Imagine you're teaching a child to recognize a dog...**\n",
    ">\n",
    "> **One friend (single layer):** \"Is it furry? Then it's a dog!\" ‚ùå (Cats are furry too!)\n",
    ">\n",
    "> **Three friends working together (multiple layers):**\n",
    "> - Friend 1: \"Does it have fur? Four legs? A tail?\"\n",
    "> - Friend 2: \"Combining those... it looks like a mammal pet!\"\n",
    "> - Friend 3: \"Does it bark? Chase balls? Then... DOG!\" ‚úÖ\n",
    ">\n",
    "> Each layer extracts more complex features from the previous layer's output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualMLP:\n",
    "    \"\"\"\n",
    "    A 3-layer MLP implemented from scratch.\n",
    "    \n",
    "    Architecture: Input(2) ‚Üí Hidden1(3) ‚Üí Hidden2(2) ‚Üí Output(1)\n",
    "    Activation: ReLU for hidden layers, Sigmoid for output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=2, hidden1_size=3, hidden2_size=2, output_size=1):\n",
    "        # Initialize weights with small random values\n",
    "        # Using Xavier initialization: scale by sqrt(2/fan_in)\n",
    "        self.W1 = np.random.randn(input_size, hidden1_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((1, hidden1_size))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden1_size, hidden2_size) * np.sqrt(2.0 / hidden1_size)\n",
    "        self.b2 = np.zeros((1, hidden2_size))\n",
    "        \n",
    "        self.W3 = np.random.randn(hidden2_size, output_size) * np.sqrt(2.0 / hidden2_size)\n",
    "        self.b3 = np.zeros((1, output_size))\n",
    "        \n",
    "        # Store intermediate values for backprop\n",
    "        self.cache = {}\n",
    "        \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # Clip for numerical stability\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        s = self.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through all layers.\n",
    "        \n",
    "        Args:\n",
    "            X: Input data of shape (batch_size, input_size)\n",
    "            \n",
    "        Returns:\n",
    "            Output predictions of shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        # Layer 1: Input ‚Üí Hidden1\n",
    "        self.cache['X'] = X\n",
    "        self.cache['z1'] = X @ self.W1 + self.b1          # Linear transformation\n",
    "        self.cache['h1'] = self.relu(self.cache['z1'])    # ReLU activation\n",
    "        \n",
    "        # Layer 2: Hidden1 ‚Üí Hidden2\n",
    "        self.cache['z2'] = self.cache['h1'] @ self.W2 + self.b2\n",
    "        self.cache['h2'] = self.relu(self.cache['z2'])\n",
    "        \n",
    "        # Layer 3: Hidden2 ‚Üí Output\n",
    "        self.cache['z3'] = self.cache['h2'] @ self.W3 + self.b3\n",
    "        self.cache['y_hat'] = self.sigmoid(self.cache['z3'])  # Sigmoid for probability output\n",
    "        \n",
    "        return self.cache['y_hat']\n",
    "    \n",
    "    def compute_loss(self, y_hat, y):\n",
    "        \"\"\"Mean Squared Error loss\"\"\"\n",
    "        return np.mean((y_hat - y) ** 2)\n",
    "    \n",
    "    def backward(self, y):\n",
    "        \"\"\"\n",
    "        Backward pass - compute gradients for all parameters.\n",
    "        \n",
    "        This is where the magic happens! We apply the chain rule\n",
    "        layer by layer, from output back to input.\n",
    "        \n",
    "        Args:\n",
    "            y: Ground truth labels of shape (batch_size, output_size)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing all gradients\n",
    "        \"\"\"\n",
    "        batch_size = y.shape[0]\n",
    "        \n",
    "        # ===== OUTPUT LAYER (Layer 3) =====\n",
    "        # Loss gradient: ‚àÇL/‚àÇ≈∑ = 2(≈∑ - y) / batch_size\n",
    "        # (divided by batch_size because we use MEAN squared error)\n",
    "        dL_dy_hat = 2 * (self.cache['y_hat'] - y) / batch_size\n",
    "        \n",
    "        # Through sigmoid: ‚àÇL/‚àÇz3 = ‚àÇL/‚àÇ≈∑ √ó ‚àÇ≈∑/‚àÇz3\n",
    "        dy_hat_dz3 = self.sigmoid_derivative(self.cache['z3'])\n",
    "        dL_dz3 = dL_dy_hat * dy_hat_dz3\n",
    "        \n",
    "        # Parameter gradients for layer 3\n",
    "        # ‚àÇL/‚àÇW3 = h2·µÄ √ó ‚àÇL/‚àÇz3\n",
    "        dL_dW3 = self.cache['h2'].T @ dL_dz3\n",
    "        # ‚àÇL/‚àÇb3 = sum(‚àÇL/‚àÇz3) over batch\n",
    "        dL_db3 = np.sum(dL_dz3, axis=0, keepdims=True)\n",
    "        \n",
    "        # ===== HIDDEN LAYER 2 =====\n",
    "        # Propagate gradient back through W3: ‚àÇL/‚àÇh2 = ‚àÇL/‚àÇz3 √ó W3·µÄ\n",
    "        dL_dh2 = dL_dz3 @ self.W3.T\n",
    "        \n",
    "        # Through ReLU: ‚àÇL/‚àÇz2 = ‚àÇL/‚àÇh2 √ó ‚àÇh2/‚àÇz2\n",
    "        dh2_dz2 = self.relu_derivative(self.cache['z2'])\n",
    "        dL_dz2 = dL_dh2 * dh2_dz2\n",
    "        \n",
    "        # Parameter gradients for layer 2\n",
    "        dL_dW2 = self.cache['h1'].T @ dL_dz2\n",
    "        dL_db2 = np.sum(dL_dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        # ===== HIDDEN LAYER 1 =====\n",
    "        # Propagate gradient back through W2\n",
    "        dL_dh1 = dL_dz2 @ self.W2.T\n",
    "        \n",
    "        # Through ReLU\n",
    "        dh1_dz1 = self.relu_derivative(self.cache['z1'])\n",
    "        dL_dz1 = dL_dh1 * dh1_dz1\n",
    "        \n",
    "        # Parameter gradients for layer 1\n",
    "        dL_dW1 = self.cache['X'].T @ dL_dz1\n",
    "        dL_db1 = np.sum(dL_dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        return {\n",
    "            'dW1': dL_dW1, 'db1': dL_db1,\n",
    "            'dW2': dL_dW2, 'db2': dL_db2,\n",
    "            'dW3': dL_dW3, 'db3': dL_db3\n",
    "        }\n",
    "    \n",
    "    def update_params(self, grads, learning_rate=0.01):\n",
    "        \"\"\"Update parameters using gradient descent\"\"\"\n",
    "        self.W1 -= learning_rate * grads['dW1']\n",
    "        self.b1 -= learning_rate * grads['db1']\n",
    "        self.W2 -= learning_rate * grads['dW2']\n",
    "        self.b2 -= learning_rate * grads['db2']\n",
    "        self.W3 -= learning_rate * grads['dW3']\n",
    "        self.b3 -= learning_rate * grads['db3']\n",
    "\n",
    "print(\"ManualMLP class defined successfully!\")\n",
    "print(\"Architecture: Input(2) ‚Üí Hidden1(3) ‚Üí Hidden2(2) ‚Üí Output(1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the manual implementation\n",
    "\n",
    "# Create sample data (XOR problem - classic neural network test)\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]], dtype=np.float64)\n",
    "\n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]], dtype=np.float64)\n",
    "\n",
    "print(\"XOR Problem:\")\n",
    "print(\"Input (X)     Target (y)\")\n",
    "for i in range(len(X)):\n",
    "    print(f\"  {X[i]}     ‚Üí    {y[i][0]}\")\n",
    "print()\n",
    "\n",
    "# Initialize network\n",
    "np.random.seed(42)  # For reproducibility\n",
    "mlp = ManualMLP()\n",
    "\n",
    "# Forward pass\n",
    "y_hat = mlp.forward(X)\n",
    "loss = mlp.compute_loss(y_hat, y)\n",
    "\n",
    "print(\"Initial Forward Pass:\")\n",
    "print(f\"Predictions: {y_hat.flatten().round(4)}\")\n",
    "print(f\"Targets:     {y.flatten()}\")\n",
    "print(f\"Initial Loss: {loss:.6f}\")\n",
    "print()\n",
    "\n",
    "# Backward pass\n",
    "grads = mlp.backward(y)\n",
    "\n",
    "print(\"Manual Gradients Computed:\")\n",
    "for name, grad in grads.items():\n",
    "    print(f\"  {name}: shape {grad.shape}, mean {grad.mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Verifying Against PyTorch Autograd\n",
    "\n",
    "The moment of truth! Let's create an equivalent PyTorch model and verify our manual gradients match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PyTorchMLP(nn.Module):\n",
    "    \"\"\"Equivalent PyTorch implementation for verification\"\"\"\n",
    "    \n",
    "    def __init__(self, W1, b1, W2, b2, W3, b3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create linear layers with pre-defined weights\n",
    "        self.fc1 = nn.Linear(2, 3)\n",
    "        self.fc2 = nn.Linear(3, 2)\n",
    "        self.fc3 = nn.Linear(2, 1)\n",
    "        \n",
    "        # Copy weights from our manual implementation\n",
    "        self.fc1.weight.data = torch.tensor(W1.T, dtype=torch.float64)\n",
    "        self.fc1.bias.data = torch.tensor(b1.flatten(), dtype=torch.float64)\n",
    "        \n",
    "        self.fc2.weight.data = torch.tensor(W2.T, dtype=torch.float64)\n",
    "        self.fc2.bias.data = torch.tensor(b2.flatten(), dtype=torch.float64)\n",
    "        \n",
    "        self.fc3.weight.data = torch.tensor(W3.T, dtype=torch.float64)\n",
    "        self.fc3.bias.data = torch.tensor(b3.flatten(), dtype=torch.float64)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Create PyTorch model with same weights as our manual MLP\n",
    "torch_mlp = PyTorchMLP(\n",
    "    mlp.W1, mlp.b1,\n",
    "    mlp.W2, mlp.b2,\n",
    "    mlp.W3, mlp.b3\n",
    ").double()  # Use float64 for accurate comparison\n",
    "\n",
    "# Convert data to tensors\n",
    "X_t = torch.tensor(X, dtype=torch.float64)\n",
    "y_t = torch.tensor(y, dtype=torch.float64)\n",
    "\n",
    "# Forward pass\n",
    "y_hat_t = torch_mlp(X_t)\n",
    "loss_t = torch.mean((y_hat_t - y_t) ** 2)\n",
    "\n",
    "# Backward pass\n",
    "loss_t.backward()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GRADIENT VERIFICATION: Manual vs PyTorch Autograd\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Compare each gradient\n",
    "comparisons = [\n",
    "    ('W1', grads['dW1'], torch_mlp.fc1.weight.grad.numpy().T),\n",
    "    ('b1', grads['db1'], torch_mlp.fc1.bias.grad.numpy().reshape(1, -1)),\n",
    "    ('W2', grads['dW2'], torch_mlp.fc2.weight.grad.numpy().T),\n",
    "    ('b2', grads['db2'], torch_mlp.fc2.bias.grad.numpy().reshape(1, -1)),\n",
    "    ('W3', grads['dW3'], torch_mlp.fc3.weight.grad.numpy().T),\n",
    "    ('b3', grads['db3'], torch_mlp.fc3.bias.grad.numpy().reshape(1, -1)),\n",
    "]\n",
    "\n",
    "all_match = True\n",
    "for name, manual, pytorch in comparisons:\n",
    "    max_diff = np.abs(manual - pytorch).max()\n",
    "    match = max_diff < 1e-6\n",
    "    all_match = all_match and match\n",
    "    \n",
    "    status = \"‚úÖ\" if match else \"‚ùå\"\n",
    "    print(f\"{status} {name}:\")\n",
    "    print(f\"   Manual:  {manual.flatten()[:4]}...\" if manual.size > 4 else f\"   Manual:  {manual.flatten()}\")\n",
    "    print(f\"   PyTorch: {pytorch.flatten()[:4]}...\" if pytorch.size > 4 else f\"   PyTorch: {pytorch.flatten()}\")\n",
    "    print(f\"   Max difference: {max_diff:.2e}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "if all_match:\n",
    "    print(\"üéâ ALL GRADIENTS MATCH! Your manual backprop is correct!\")\n",
    "    print(\"   You've just implemented deep learning from scratch!\")\n",
    "else:\n",
    "    print(\"‚ùå Some gradients don't match. Review your chain rule derivations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Training the Network\n",
    "\n",
    "Let's use our manual implementation to actually train on the XOR problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network from scratch\n",
    "\n",
    "# Re-initialize with fresh weights\n",
    "np.random.seed(123)\n",
    "mlp = ManualMLP()\n",
    "\n",
    "# Training hyperparameters\n",
    "learning_rate = 1.0  # XOR needs aggressive learning\n",
    "epochs = 1000\n",
    "\n",
    "# Track losses for plotting\n",
    "losses = []\n",
    "\n",
    "print(\"Training Manual MLP on XOR Problem\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    y_hat = mlp.forward(X)\n",
    "    loss = mlp.compute_loss(y_hat, y)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    grads = mlp.backward(y)\n",
    "    \n",
    "    # Update parameters\n",
    "    mlp.update_params(grads, learning_rate)\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 200 == 0 or epoch == epochs - 1:\n",
    "        predictions = (y_hat > 0.5).astype(int)\n",
    "        accuracy = np.mean(predictions == y) * 100\n",
    "        print(f\"Epoch {epoch:4d}: Loss = {loss:.6f}, Accuracy = {accuracy:.1f}%\")\n",
    "\n",
    "print()\n",
    "print(\"Final Predictions:\")\n",
    "print(\"Input          Predicted    Target\")\n",
    "for i in range(len(X)):\n",
    "    pred = y_hat[i, 0]\n",
    "    pred_class = 1 if pred > 0.5 else 0\n",
    "    correct = \"‚úÖ\" if pred_class == y[i, 0] else \"‚ùå\"\n",
    "    print(f\"{X[i]}  ‚Üí  {pred:.4f} ({pred_class})    {int(y[i, 0])}  {correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(losses, 'b-', linewidth=1)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss (MSE)', fontsize=12)\n",
    "axes[0].set_title('Training Loss Over Time', fontsize=14)\n",
    "axes[0].set_yscale('log')  # Log scale shows learning better\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=0.01, color='r', linestyle='--', label='Target threshold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Decision boundary visualization\n",
    "# Create a grid to visualize what the network learned\n",
    "xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 100),\n",
    "                     np.linspace(-0.5, 1.5, 100))\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = mlp.forward(grid_points).reshape(xx.shape)\n",
    "\n",
    "# Plot decision regions\n",
    "contour = axes[1].contourf(xx, yy, Z, levels=np.linspace(0, 1, 11), \n",
    "                          cmap='RdYlBu_r', alpha=0.8)\n",
    "plt.colorbar(contour, ax=axes[1], label='Prediction')\n",
    "\n",
    "# Plot training points\n",
    "for i, (xi, yi) in enumerate(zip(X, y)):\n",
    "    color = 'red' if yi[0] == 0 else 'blue'\n",
    "    marker = 'o' if yi[0] == 0 else 's'\n",
    "    axes[1].scatter(xi[0], xi[1], c=color, s=200, marker=marker, \n",
    "                   edgecolors='black', linewidth=2,\n",
    "                   label=f'Class {int(yi[0])}' if i < 2 else None)\n",
    "\n",
    "axes[1].set_xlabel('Input 1', fontsize=12)\n",
    "axes[1].set_ylabel('Input 2', fontsize=12)\n",
    "axes[1].set_title('Learned XOR Decision Boundary', fontsize=14)\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].set_xlim(-0.5, 1.5)\n",
    "axes[1].set_ylim(-0.5, 1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "We trained a neural network **entirely from scratch** using our manual backpropagation!\n",
    "\n",
    "- **Left plot:** Loss decreases over training (note the log scale)\n",
    "- **Right plot:** The network learned the XOR function!\n",
    "  - Blue regions: Network predicts class 1\n",
    "  - Red regions: Network predicts class 0\n",
    "  - The boundary is non-linear (impossible with a single layer!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting the Chain Rule Order\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: multiplying in wrong order\n",
    "dL_dW = dz_dW * dL_dz  # This doesn't work for matrices!\n",
    "\n",
    "# ‚úÖ Right: proper matrix multiplication\n",
    "dL_dW = h_prev.T @ dL_dz  # Previous layer output transposed √ó gradient\n",
    "```\n",
    "\n",
    "**Why:** Matrix dimensions must align. `dL_dW` should have same shape as `W`.\n",
    "\n",
    "### Mistake 2: Missing Batch Dimension\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: bias gradient without summing over batch\n",
    "dL_db = dL_dz  # Shape: (batch_size, hidden_size) - wrong!\n",
    "\n",
    "# ‚úÖ Right: sum over batch dimension\n",
    "dL_db = np.sum(dL_dz, axis=0, keepdims=True)  # Shape: (1, hidden_size)\n",
    "```\n",
    "\n",
    "**Why:** Each sample contributes to the gradient; we sum their contributions.\n",
    "\n",
    "### Mistake 3: ReLU Derivative at Zero\n",
    "\n",
    "```python\n",
    "# ‚ùå Technically undefined at z=0\n",
    "def relu_deriv(z):\n",
    "    return 1 if z > 0 else 0  # What about z == 0?\n",
    "\n",
    "# ‚úÖ Standard convention: use 0 at z=0\n",
    "def relu_deriv(z):\n",
    "    return (z > 0).astype(float)  # z=0 ‚Üí derivative=0\n",
    "```\n",
    "\n",
    "**Why:** In practice, hitting exactly z=0 is rare with floating point. The convention is well-tested.\n",
    "\n",
    "### Mistake 4: Not Normalizing Loss by Batch Size\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: gradient grows with batch size\n",
    "loss = np.sum((y_hat - y) ** 2)\n",
    "dL_dy_hat = 2 * (y_hat - y)\n",
    "\n",
    "# ‚úÖ Right: normalize by batch size\n",
    "loss = np.mean((y_hat - y) ** 2)\n",
    "dL_dy_hat = 2 * (y_hat - y) / batch_size\n",
    "```\n",
    "\n",
    "**Why:** Without normalization, larger batches = larger gradients, which breaks learning rate tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself #2: Add a Layer\n",
    "\n",
    "Modify the `ManualMLP` class to add a fourth layer:\n",
    "\n",
    "```\n",
    "Input(2) ‚Üí Hidden1(4) ‚Üí Hidden2(3) ‚Üí Hidden3(2) ‚Üí Output(1)\n",
    "```\n",
    "\n",
    "Steps:\n",
    "1. Add `W4`, `b4` to `__init__`\n",
    "2. Add layer 4 in `forward()`\n",
    "3. Add layer 4 gradients in `backward()`\n",
    "4. Verify against PyTorch\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "The pattern is consistent for each layer:\n",
    "```python\n",
    "# In backward():\n",
    "# 1. Get gradient from next layer: dL_dh_current = dL_dz_next @ W_next.T\n",
    "# 2. Through activation: dL_dz_current = dL_dh_current * activation_derivative(z_current)\n",
    "# 3. Weight gradient: dL_dW_current = h_prev.T @ dL_dz_current\n",
    "# 4. Bias gradient: dL_db_current = sum(dL_dz_current, axis=0)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE: Implement 4-Layer MLP\n\nclass ManualMLP4Layer:\n    \"\"\"4-layer MLP for practice\"\"\"\n    \n    def __init__(self):\n        # TODO: Initialize W1, b1, W2, b2, W3, b3, W4, b4\n        raise NotImplementedError(\"Complete this exercise\")\n    \n    def forward(self, X):\n        # TODO: Implement 4-layer forward pass\n        raise NotImplementedError(\"Complete this exercise\")\n    \n    def backward(self, y):\n        # TODO: Implement 4-layer backward pass\n        raise NotImplementedError(\"Complete this exercise\")\n\n# Test your implementation\n# mlp4 = ManualMLP4Layer()\n# y_hat = mlp4.forward(X)\n# grads = mlp4.backward(y)\n# Verify against PyTorch..."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Challenge: Implement with Different Loss Function\n",
    "\n",
    "Modify the network to use **Binary Cross-Entropy** loss instead of MSE:\n",
    "\n",
    "$$L = -\\frac{1}{n}\\sum_{i}[y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]$$\n",
    "\n",
    "The gradient is:\n",
    "$$\\frac{\\partial L}{\\partial \\hat{y}} = -\\frac{y}{\\hat{y}} + \\frac{1-y}{1-\\hat{y}}$$\n",
    "\n",
    "Can you derive why this simplifies to:\n",
    "$$\\frac{\\partial L}{\\partial z} = \\hat{y} - y$$\n",
    "\n",
    "when combined with sigmoid activation?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Mathematical hint</summary>\n",
    "\n",
    "Use the fact that:\n",
    "- $\\frac{\\partial \\hat{y}}{\\partial z} = \\hat{y}(1-\\hat{y})$ for sigmoid\n",
    "- Multiply: $\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z}$\n",
    "- After algebra: $\\frac{\\partial L}{\\partial z} = \\frac{1}{n}(\\hat{y} - y)$\n",
    "\n",
    "This is beautiful! Cross-entropy + sigmoid gives a simple gradient, which is why it's used together.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "Congratulations! You've learned:\n",
    "\n",
    "- ‚úÖ **Chain rule** is the foundation of backpropagation\n",
    "- ‚úÖ **Forward pass** computes activations layer by layer\n",
    "- ‚úÖ **Backward pass** computes gradients in reverse order\n",
    "- ‚úÖ Each layer's gradients depend on the layer ahead (chain rule!)\n",
    "- ‚úÖ Your manual gradients can match autograd to machine precision\n",
    "\n",
    "**Key insight:** Backpropagation isn't magic‚Äîit's just the chain rule applied systematically!\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [3Blue1Brown: Backpropagation Calculus](https://www.youtube.com/watch?v=tIeHLnjs5U8) - Visual intuition\n",
    "- [CS231n: Backprop Notes](http://cs231n.github.io/optimization-2/) - Stanford's excellent notes\n",
    "- [Andrej Karpathy: micrograd](https://github.com/karpathy/micrograd) - Tiny autograd engine\n",
    "- [The Matrix Calculus You Need for Deep Learning](https://explained.ai/matrix-calculus/) - Deep dive\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì¶ Using Production-Ready Implementations\n",
    "\n",
    "This module includes production-ready implementations in the `scripts/` folder:\n",
    "\n",
    "- **`math_utils.py`**: Activation functions, loss functions, optimizers (SGD, Adam, AdamW)\n",
    "- **`visualization_utils.py`**: Loss landscape plotting, training curves, SVD analysis\n",
    "\n",
    "You can import these for your own projects or to verify your implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example: Using production-ready implementations from scripts/\nimport sys\nsys.path.insert(0, '..')  # Add parent directory to path\n\n# Import from scripts\nfrom scripts.math_utils import (\n    sigmoid, sigmoid_derivative,\n    relu, relu_derivative,\n    softmax,\n    mse_loss, cross_entropy_loss,\n    SGD, SGDMomentum, Adam, AdamW,\n    numerical_gradient, check_gradient\n)\n\n# Test the implementations\ntest_x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\nprint(\"Testing script implementations:\")\nprint(f\"  sigmoid({test_x}) = {sigmoid(test_x).round(4)}\")\nprint(f\"  relu({test_x}) = {relu(test_x)}\")\nprint(f\"  softmax({test_x}) = {softmax(test_x).round(4)}\")\n\n# Test optimizer\nopt = Adam(lr=0.1)\nparams = np.array([5.0, 5.0])\nprint(f\"\\nAdam optimizer test:\")\nprint(f\"  Initial params: {params}\")\nfor i in range(3):\n    grads = 2 * params  # Gradient of x^2 + y^2\n    params = opt.step(params, grads)\nprint(f\"  After 3 steps: {params.round(4)}\")\n\nprint(\"\\n‚úÖ Script implementations work correctly!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory\n",
    "import gc\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")\n",
    "print(\"\\n‚û°Ô∏è  Next: Lab 1.4.2 - Optimizer Implementation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}