{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 1.4.4: SVD for LoRA Intuition\n\n**Module:** 1.4 - Mathematics for Deep Learning  \n**Time:** 2 hours  \n**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê\n\n---\n\n## üéØ Learning Objectives\n\nBy the end of this notebook, you will:\n- [ ] Understand Singular Value Decomposition (SVD) intuitively\n- [ ] Perform SVD on weight matrices and reconstruct with varying ranks\n- [ ] Visualize reconstruction error vs rank trade-off\n- [ ] Connect SVD to LoRA's low-rank adaptation approach\n- [ ] Calculate memory savings from low-rank approximations\n\n---\n\n## üìö Prerequisites\n\n- Completed: Labs 1.4.1-1.4.3\n- Knowledge of: Matrix multiplication, basic linear algebra\n\n---\n\n## üåç Real-World Context\n\n**Why does SVD matter for deep learning?**\n\n- **LoRA fine-tuning:** Fine-tune a 70B model with only ~1% of parameters!\n- **Model compression:** Reduce model size while preserving accuracy\n- **Understanding representations:** SVD reveals what information matrices capture\n\n**Real examples:**\n- LLaMA-70B has ~70 billion parameters, but LoRA can adapt it with ~10 million\n- Stable Diffusion LoRAs are typically 10-100MB vs 4GB for full fine-tuning\n- GPT-4 adapters likely use similar low-rank techniques"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What is SVD?\n",
    "\n",
    "> **Imagine you have a big recipe book with 1000 recipes...**\n",
    ">\n",
    "> Each recipe is a list of 100 ingredients with amounts. That's a lot of information!\n",
    ">\n",
    "> But wait... most recipes are combinations of a few **basic patterns**:\n",
    "> - \"Italian base\" = tomatoes + olive oil + garlic + basil\n",
    "> - \"Asian base\" = soy sauce + ginger + sesame oil\n",
    "> - \"French base\" = butter + wine + herbs\n",
    ">\n",
    "> **SVD finds these patterns!** It discovers:\n",
    "> 1. The fundamental \"flavor patterns\" (singular vectors)\n",
    "> 2. How important each pattern is (singular values)\n",
    "> 3. How to combine patterns to recreate any recipe\n",
    ">\n",
    "> **The magic:** You can approximate the entire cookbook with just 10-20 patterns,\n",
    "> instead of memorizing 1000 recipes √ó 100 ingredients!\n",
    ">\n",
    "> **For neural networks:**\n",
    "> - A weight matrix W (768√ó768) = 590,000 numbers\n",
    "> - SVD: W = U √ó Œ£ √ó V^T\n",
    "> - Keep only top 16 patterns: ~25,000 numbers (96% smaller!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"üöÄ SVD for LoRA Intuition Lab\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 1: Understanding SVD Mathematically\n\n### The SVD Decomposition\n\nAny matrix $W \\in \\mathbb{R}^{m \\times n}$ can be decomposed as:\n\n$$W = U \\Sigma V^T$$\n\nWhere:\n- $U \\in \\mathbb{R}^{m \\times m}$ - Left singular vectors (orthonormal columns)\n- $\\Sigma \\in \\mathbb{R}^{m \\times n}$ - Diagonal matrix of singular values\n- $V^T \\in \\mathbb{R}^{n \\times n}$ - Right singular vectors (orthonormal rows)\n\n### Low-Rank Approximation\n\nKeep only the top $r$ singular values:\n\n$$W \\approx W_r = U_r \\Sigma_r V_r^T$$\n\nThis is the **best** rank-$r$ approximation (by Frobenius norm)!\n\n### NumPy's SVD Function\n\nWe'll use `np.linalg.svd()` to compute SVD:\n\n```python\nimport numpy as np\n\n# Compute SVD of a matrix W\nU, S, Vt = np.linalg.svd(W, full_matrices=False)\n\n# U:  Left singular vectors, shape (m, min(m,n))\n# S:  Singular values (1D array, sorted descending), shape (min(m,n),)\n# Vt: Right singular vectors (transposed!), shape (min(m,n), n)\n\n# Reconstruct: W = U @ np.diag(S) @ Vt\n```\n\nNote: `full_matrices=False` gives the \"economy\" SVD which is more memory efficient."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with a simple visual example\n",
    "\n",
    "# Create a simple matrix (like pixel values of an image)\n",
    "def create_test_matrix(size=64):\n",
    "    \"\"\"Create a matrix with clear structure (easier to compress)\"\"\"\n",
    "    x = np.linspace(-3, 3, size)\n",
    "    y = np.linspace(-3, 3, size)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Combination of smooth patterns (low rank!)\n",
    "    Z = np.sin(X) * np.cos(Y) + 0.5 * np.exp(-(X**2 + Y**2)/5)\n",
    "    return Z\n",
    "\n",
    "# Create matrix\n",
    "W = create_test_matrix(64)\n",
    "print(f\"Original matrix shape: {W.shape}\")\n",
    "print(f\"Total elements: {W.size:,}\")\n",
    "\n",
    "# Perform SVD\n",
    "U, S, Vt = np.linalg.svd(W, full_matrices=False)\n",
    "\n",
    "print(f\"\\nSVD components:\")\n",
    "print(f\"  U shape: {U.shape}\")\n",
    "print(f\"  S shape: {S.shape}\")\n",
    "print(f\"  V^T shape: {Vt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the singular values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot singular values\n",
    "axes[0].bar(range(len(S)), S, color='steelblue', alpha=0.7)\n",
    "axes[0].set_xlabel('Singular Value Index', fontsize=12)\n",
    "axes[0].set_ylabel('Singular Value', fontsize=12)\n",
    "axes[0].set_title('Singular Values (Importance of Each Component)', fontsize=14)\n",
    "axes[0].set_xlim(-1, 30)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot cumulative energy\n",
    "energy = (S ** 2) / (S ** 2).sum() * 100\n",
    "cumulative_energy = np.cumsum(energy)\n",
    "\n",
    "axes[1].plot(cumulative_energy, 'b-', linewidth=2)\n",
    "axes[1].axhline(y=95, color='r', linestyle='--', label='95% energy')\n",
    "axes[1].axhline(y=99, color='g', linestyle='--', label='99% energy')\n",
    "\n",
    "# Find where we hit 95% and 99%\n",
    "rank_95 = np.argmax(cumulative_energy >= 95) + 1\n",
    "rank_99 = np.argmax(cumulative_energy >= 99) + 1\n",
    "\n",
    "axes[1].axvline(x=rank_95-1, color='r', linestyle=':', alpha=0.7)\n",
    "axes[1].axvline(x=rank_99-1, color='g', linestyle=':', alpha=0.7)\n",
    "\n",
    "axes[1].set_xlabel('Number of Components (Rank)', fontsize=12)\n",
    "axes[1].set_ylabel('Cumulative Energy (%)', fontsize=12)\n",
    "axes[1].set_title('Cumulative Information Captured', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim(0, 30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Information concentration:\")\n",
    "print(f\"  - {rank_95} components capture 95% of information\")\n",
    "print(f\"  - {rank_99} components capture 99% of information\")\n",
    "print(f\"  - Full rank: {len(S)} components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "- **Left plot:** Singular values decrease rapidly - most information is in the first few!\n",
    "- **Right plot:** With just a few components, we capture most of the \"energy\" (information)\n",
    "\n",
    "This is the key insight behind low-rank approximations!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Low-Rank Reconstruction\n",
    "\n",
    "Let's see how well we can reconstruct the original matrix with different ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_low_rank(U, S, Vt, rank):\n",
    "    \"\"\"Reconstruct matrix using only top 'rank' singular values\"\"\"\n",
    "    return U[:, :rank] @ np.diag(S[:rank]) @ Vt[:rank, :]\n",
    "\n",
    "def relative_error(original, reconstructed):\n",
    "    \"\"\"Compute relative reconstruction error (Frobenius norm)\"\"\"\n",
    "    return np.linalg.norm(original - reconstructed) / np.linalg.norm(original)\n",
    "\n",
    "# Reconstruct with different ranks\n",
    "ranks_to_try = [1, 2, 4, 8, 16, 32, 64]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Original\n",
    "im = axes[0].imshow(W, cmap='viridis')\n",
    "axes[0].set_title(f'Original\\n({W.size:,} values)', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Reconstructions\n",
    "for i, rank in enumerate(ranks_to_try):\n",
    "    W_approx = reconstruct_low_rank(U, S, Vt, rank)\n",
    "    error = relative_error(W, W_approx) * 100\n",
    "    storage = rank * (W.shape[0] + W.shape[1] + 1)  # U_r + S_r + V_r\n",
    "    \n",
    "    axes[i+1].imshow(W_approx, cmap='viridis')\n",
    "    axes[i+1].set_title(f'Rank {rank}\\nError: {error:.2f}%\\nStorage: {storage:,}', fontsize=11)\n",
    "    axes[i+1].axis('off')\n",
    "\n",
    "plt.suptitle('Low-Rank Approximations of a Matrix', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Storage comparison:\")\n",
    "print(f\"  Original: {W.size:,} values\")\n",
    "for rank in [4, 8, 16]:\n",
    "    storage = rank * (W.shape[0] + W.shape[1] + 1)\n",
    "    savings = (1 - storage/W.size) * 100\n",
    "    error = relative_error(W, reconstruct_low_rank(U, S, Vt, rank)) * 100\n",
    "    print(f\"  Rank {rank:2d}: {storage:,} values ({savings:.1f}% smaller, {error:.2f}% error)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: SVD on Neural Network Weights\n",
    "\n",
    "Now let's apply this to actual neural network weight matrices!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a weight matrix similar to what you'd find in a transformer\n",
    "# (e.g., query/key/value projection in attention)\n",
    "\n",
    "d_model = 768  # Hidden dimension (like BERT-base)\n",
    "\n",
    "# Simulate a trained weight matrix (not purely random - has structure)\n",
    "# Real trained weights have low effective rank!\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a low-rank matrix plus noise (simulates trained weights)\n",
    "true_rank = 64  # The \"true\" information content\n",
    "A = np.random.randn(d_model, true_rank) / np.sqrt(true_rank)\n",
    "B = np.random.randn(true_rank, d_model) / np.sqrt(true_rank)\n",
    "noise = np.random.randn(d_model, d_model) * 0.01  # Small noise\n",
    "\n",
    "W_neural = A @ B + noise\n",
    "\n",
    "print(f\"Simulated weight matrix: {W_neural.shape}\")\n",
    "print(f\"Total parameters: {W_neural.size:,}\")\n",
    "print(f\"Memory (float32): {W_neural.size * 4 / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SVD on the neural network weight\n",
    "U_nn, S_nn, Vt_nn = np.linalg.svd(W_neural, full_matrices=False)\n",
    "\n",
    "# Analyze singular value spectrum\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Singular values (log scale)\n",
    "axes[0].semilogy(S_nn, 'b-', linewidth=2)\n",
    "axes[0].axhline(y=S_nn[true_rank-1], color='r', linestyle='--', \n",
    "               label=f'Rank {true_rank} threshold')\n",
    "axes[0].set_xlabel('Singular Value Index', fontsize=12)\n",
    "axes[0].set_ylabel('Singular Value (log scale)', fontsize=12)\n",
    "axes[0].set_title(f'Singular Values of {d_model}√ó{d_model} Weight Matrix', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xlim(0, 200)\n",
    "\n",
    "# Reconstruction error vs rank\n",
    "ranks = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 768]\n",
    "errors = []\n",
    "for r in ranks:\n",
    "    W_approx = reconstruct_low_rank(U_nn, S_nn, Vt_nn, r)\n",
    "    errors.append(relative_error(W_neural, W_approx) * 100)\n",
    "\n",
    "axes[1].semilogx(ranks, errors, 'bo-', linewidth=2, markersize=8)\n",
    "axes[1].axvline(x=true_rank, color='r', linestyle='--', label=f'True rank ({true_rank})')\n",
    "axes[1].set_xlabel('Rank (log scale)', fontsize=12)\n",
    "axes[1].set_ylabel('Reconstruction Error (%)', fontsize=12)\n",
    "axes[1].set_title('Error vs Rank Trade-off', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Reconstruction quality:\")\n",
    "for r, e in zip(ranks, errors):\n",
    "    if r <= 128:\n",
    "        print(f\"  Rank {r:3d}: {e:6.3f}% error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Connecting to LoRA\n",
    "\n",
    "### The LoRA Insight\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is based on this observation:\n",
    "\n",
    "> The **change** in weights during fine-tuning is often low-rank!\n",
    "\n",
    "Instead of updating the full weight matrix:\n",
    "$$W_{new} = W_{pretrained} + \\Delta W$$\n",
    "\n",
    "LoRA parameterizes $\\Delta W$ as a low-rank product:\n",
    "$$W_{new} = W_{pretrained} + BA$$\n",
    "\n",
    "Where:\n",
    "- $B \\in \\mathbb{R}^{d \\times r}$ (d=768, r=16 typically)\n",
    "- $A \\in \\mathbb{R}^{r \\times d}$\n",
    "\n",
    "### üßí ELI5: LoRA\n",
    "\n",
    "> **Instead of repainting your entire house (fine-tuning all weights)...**\n",
    ">\n",
    "> You just add a thin layer of new wallpaper in the rooms that need updating!\n",
    ">\n",
    "> - Full fine-tuning: 768√ó768 = 590,000 parameters to update\n",
    "> - LoRA (rank 16): 768√ó16 + 16√ó768 = 24,576 parameters (96% less!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer:\n",
    "    \"\"\"\n",
    "    Simplified LoRA implementation for understanding.\n",
    "    \n",
    "    Instead of modifying W directly, we add a low-rank update:\n",
    "    output = x @ W + x @ (B @ A) * scaling\n",
    "    \n",
    "    During training, W is frozen and only B, A are updated.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_in, d_out, rank=16, alpha=16):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_in: Input dimension\n",
    "            d_out: Output dimension\n",
    "            rank: Rank of the low-rank update\n",
    "            alpha: Scaling factor for the update\n",
    "        \"\"\"\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # Pretrained weight (frozen during fine-tuning)\n",
    "        self.W = np.random.randn(d_in, d_out) * 0.02\n",
    "        \n",
    "        # LoRA matrices (trainable)\n",
    "        # A is initialized with small random values\n",
    "        # B is initialized to zero (so initial output is same as pretrained)\n",
    "        self.A = np.random.randn(d_in, rank) * 0.01\n",
    "        self.B = np.zeros((rank, d_out))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass: original + low-rank update\"\"\"\n",
    "        # Original output\n",
    "        out_pretrained = x @ self.W\n",
    "        \n",
    "        # LoRA update\n",
    "        out_lora = (x @ self.A @ self.B) * self.scaling\n",
    "        \n",
    "        return out_pretrained + out_lora\n",
    "    \n",
    "    def get_merged_weight(self):\n",
    "        \"\"\"Merge LoRA into the weight (for inference)\"\"\"\n",
    "        return self.W + (self.A @ self.B) * self.scaling\n",
    "    \n",
    "    def count_trainable_params(self):\n",
    "        \"\"\"Count trainable parameters (A and B only)\"\"\"\n",
    "        return self.A.size + self.B.size\n",
    "    \n",
    "    def count_total_params(self):\n",
    "        \"\"\"Count all parameters\"\"\"\n",
    "        return self.W.size + self.A.size + self.B.size\n",
    "\n",
    "# Example usage\n",
    "d = 768  # BERT-base hidden size\n",
    "rank = 16  # Typical LoRA rank\n",
    "\n",
    "lora = LoRALayer(d, d, rank=rank)\n",
    "\n",
    "print(\"LoRA Layer Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Input/Output dimension: {d}\")\n",
    "print(f\"LoRA rank: {rank}\")\n",
    "print()\n",
    "print(f\"Pretrained W: {d}√ó{d} = {d*d:,} params (frozen)\")\n",
    "print(f\"LoRA A: {d}√ó{rank} = {d*rank:,} params (trainable)\")\n",
    "print(f\"LoRA B: {rank}√ó{d} = {rank*d:,} params (trainable)\")\n",
    "print()\n",
    "print(f\"Total trainable: {lora.count_trainable_params():,} params\")\n",
    "print(f\"Percentage trainable: {lora.count_trainable_params() / lora.count_total_params() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize memory savings at different ranks\n",
    "\n",
    "d_model = 768\n",
    "ranks = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "\n",
    "full_params = d_model * d_model\n",
    "lora_params = [2 * d_model * r for r in ranks]\n",
    "savings = [(1 - lp/full_params) * 100 for lp in lora_params]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Parameters comparison\n",
    "axes[0].bar(['Full'] + [f'r={r}' for r in ranks], \n",
    "           [full_params] + lora_params,\n",
    "           color=['red'] + ['steelblue']*len(ranks),\n",
    "           alpha=0.7)\n",
    "axes[0].set_ylabel('Number of Trainable Parameters', fontsize=12)\n",
    "axes[0].set_title('Full Fine-tuning vs LoRA Parameters', fontsize=14)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Savings percentage\n",
    "axes[1].bar([f'r={r}' for r in ranks], savings, color='green', alpha=0.7)\n",
    "axes[1].axhline(y=95, color='red', linestyle='--', label='95% savings')\n",
    "axes[1].set_ylabel('Memory Savings (%)', fontsize=12)\n",
    "axes[1].set_title('Memory Savings with LoRA', fontsize=14)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Memory savings:\")\n",
    "for r, s in zip(ranks, savings):\n",
    "    print(f\"  Rank {r:3d}: {s:.1f}% savings ({2*d_model*r:,} vs {full_params:,} params)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Real-World Example - Transformer Attention\n",
    "\n",
    "Let's see how this applies to a real transformer layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AttentionWithLoRA(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention with optional LoRA adapters.\n",
    "    \n",
    "    This shows how LoRA is typically applied in practice.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model=768, n_heads=12, lora_rank=0, lora_alpha=16):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        self.lora_rank = lora_rank\n",
    "        \n",
    "        # Standard attention projections (frozen if using LoRA)\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        # LoRA adapters (only created if rank > 0)\n",
    "        if lora_rank > 0:\n",
    "            self.scaling = lora_alpha / lora_rank\n",
    "            \n",
    "            # LoRA for Q and V (common choice)\n",
    "            self.lora_q_A = nn.Linear(d_model, lora_rank, bias=False)\n",
    "            self.lora_q_B = nn.Linear(lora_rank, d_model, bias=False)\n",
    "            \n",
    "            self.lora_v_A = nn.Linear(d_model, lora_rank, bias=False)\n",
    "            self.lora_v_B = nn.Linear(lora_rank, d_model, bias=False)\n",
    "            \n",
    "            # Initialize B to zero\n",
    "            nn.init.zeros_(self.lora_q_B.weight)\n",
    "            nn.init.zeros_(self.lora_v_B.weight)\n",
    "            \n",
    "            # Freeze original weights\n",
    "            for p in [self.W_q, self.W_k, self.W_v, self.W_o]:\n",
    "                p.weight.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass with LoRA if enabled\"\"\"\n",
    "        # Standard projections\n",
    "        q = self.W_q(x)\n",
    "        k = self.W_k(x)\n",
    "        v = self.W_v(x)\n",
    "        \n",
    "        # Add LoRA updates\n",
    "        if self.lora_rank > 0:\n",
    "            q = q + self.lora_q_B(self.lora_q_A(x)) * self.scaling\n",
    "            v = v + self.lora_v_B(self.lora_v_A(x)) * self.scaling\n",
    "        \n",
    "        # Simplified attention (just for illustration)\n",
    "        # In practice, would reshape for multi-head, apply softmax, etc.\n",
    "        return self.W_o(v)  # Simplified output\n",
    "    \n",
    "    def count_params(self):\n",
    "        \"\"\"Count trainable vs total parameters\"\"\"\n",
    "        total = sum(p.numel() for p in self.parameters())\n",
    "        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        return trainable, total\n",
    "\n",
    "# Compare full fine-tuning vs LoRA\n",
    "print(\"Attention Layer Parameter Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Full fine-tuning\n",
    "full_attn = AttentionWithLoRA(lora_rank=0)\n",
    "full_train, full_total = full_attn.count_params()\n",
    "print(f\"\\nFull Fine-tuning:\")\n",
    "print(f\"  Trainable: {full_train:,} ({full_train/full_total*100:.1f}%)\")\n",
    "print(f\"  Total: {full_total:,}\")\n",
    "\n",
    "# LoRA with different ranks\n",
    "for rank in [4, 8, 16, 32]:\n",
    "    lora_attn = AttentionWithLoRA(lora_rank=rank)\n",
    "    lora_train, lora_total = lora_attn.count_params()\n",
    "    savings = (1 - lora_train/full_train) * 100\n",
    "    print(f\"\\nLoRA (rank={rank}):\")\n",
    "    print(f\"  Trainable: {lora_train:,} ({lora_train/lora_total*100:.1f}%)\")\n",
    "    print(f\"  Savings vs full: {savings:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: When Does Low-Rank Work?\n",
    "\n",
    "Not all matrices can be well-approximated with low rank. Let's explore when this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different types of matrices\n",
    "\n",
    "def analyze_matrix(W, name):\n",
    "    \"\"\"Analyze how compressible a matrix is\"\"\"\n",
    "    U, S, Vt = np.linalg.svd(W, full_matrices=False)\n",
    "    \n",
    "    # Compute energy captured at different ranks\n",
    "    total_energy = (S ** 2).sum()\n",
    "    \n",
    "    # Find rank needed for 95% energy\n",
    "    cumsum = np.cumsum(S ** 2) / total_energy\n",
    "    rank_95 = np.argmax(cumsum >= 0.95) + 1\n",
    "    rank_99 = np.argmax(cumsum >= 0.99) + 1\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'shape': W.shape,\n",
    "        'rank_95': rank_95,\n",
    "        'rank_99': rank_99,\n",
    "        'full_rank': len(S),\n",
    "        'singular_values': S\n",
    "    }\n",
    "\n",
    "# Create different matrix types\n",
    "size = 256\n",
    "\n",
    "matrices = {\n",
    "    'Low-rank (r=10)': np.random.randn(size, 10) @ np.random.randn(10, size) / 10,\n",
    "    'Smooth (natural)': create_test_matrix(size),\n",
    "    'Random (full rank)': np.random.randn(size, size) / np.sqrt(size),\n",
    "    'Identity-like': np.eye(size) + np.random.randn(size, size) * 0.1,\n",
    "}\n",
    "\n",
    "# Analyze each\n",
    "results = [analyze_matrix(W, name) for name, W in matrices.items()]\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Singular value decay\n",
    "for res in results:\n",
    "    S_normalized = res['singular_values'] / res['singular_values'][0]\n",
    "    axes[0].semilogy(S_normalized[:100], linewidth=2, label=res['name'])\n",
    "\n",
    "axes[0].set_xlabel('Singular Value Index', fontsize=12)\n",
    "axes[0].set_ylabel('Normalized Singular Value', fontsize=12)\n",
    "axes[0].set_title('Singular Value Decay Rate', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Rank needed for 95% energy\n",
    "names = [r['name'] for r in results]\n",
    "rank_95 = [r['rank_95'] for r in results]\n",
    "rank_99 = [r['rank_99'] for r in results]\n",
    "\n",
    "x = np.arange(len(names))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].bar(x - width/2, rank_95, width, label='95% energy', alpha=0.7)\n",
    "axes[1].bar(x + width/2, rank_99, width, label='99% energy', alpha=0.7)\n",
    "axes[1].axhline(y=size, color='red', linestyle='--', label=f'Full rank ({size})')\n",
    "axes[1].set_xlabel('Matrix Type', fontsize=12)\n",
    "axes[1].set_ylabel('Rank Needed', fontsize=12)\n",
    "axes[1].set_title('Rank Needed to Capture Information', fontsize=14)\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels([n.split()[0] for n in names], rotation=15)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Compressibility Analysis:\")\n",
    "print(f\"{'Matrix Type':<20} {'95% Rank':<12} {'99% Rank':<12} {'Full Rank':<10}\")\n",
    "print(\"-\" * 55)\n",
    "for res in results:\n",
    "    print(f\"{res['name']:<20} {res['rank_95']:<12} {res['rank_99']:<12} {res['full_rank']:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Key Insights\n",
    "\n",
    "1. **Low-rank matrices:** Easily compressed (by design)\n",
    "2. **Smooth/natural patterns:** Also compressible (patterns repeat)\n",
    "3. **Random matrices:** NOT compressible (no structure)\n",
    "4. **Trained neural networks:** Usually closer to smooth/low-rank!\n",
    "\n",
    "**Why LoRA works:** Training encourages weights to become structured!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Rank Too Low\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Rank 1 loses too much information\n",
    "lora = LoRALayer(d_model=768, rank=1)  # Very lossy!\n",
    "\n",
    "# ‚úÖ Right: Use rank 8-64 for most tasks\n",
    "lora = LoRALayer(d_model=768, rank=16)  # Good balance\n",
    "```\n",
    "\n",
    "### Mistake 2: Not Initializing B to Zero\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong: Random initialization changes pretrained behavior immediately\n",
    "self.B = nn.Linear(rank, d_out)\n",
    "\n",
    "# ‚úÖ Right: Initialize B to zero so initial output = pretrained\n",
    "self.B = nn.Linear(rank, d_out)\n",
    "nn.init.zeros_(self.B.weight)\n",
    "```\n",
    "\n",
    "### Mistake 3: Applying LoRA to All Layers\n",
    "\n",
    "```python\n",
    "# ‚ùå Often unnecessary: LoRA on every layer\n",
    "for layer in model.layers:\n",
    "    layer.q_proj = add_lora(layer.q_proj)\n",
    "    layer.k_proj = add_lora(layer.k_proj)\n",
    "    layer.v_proj = add_lora(layer.v_proj)\n",
    "    layer.o_proj = add_lora(layer.o_proj)\n",
    "\n",
    "# ‚úÖ Better: Focus on Q and V (empirically effective)\n",
    "for layer in model.layers:\n",
    "    layer.q_proj = add_lora(layer.q_proj)\n",
    "    layer.v_proj = add_lora(layer.v_proj)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise: Find the Optimal Rank\n",
    "\n",
    "Given a weight matrix, find the minimum rank needed to achieve less than 1% reconstruction error.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "1. Perform SVD on the matrix\n",
    "2. Loop through ranks from 1 to full\n",
    "3. Compute reconstruction error at each rank\n",
    "4. Return first rank where error < 1%\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def find_optimal_rank(W, target_error=0.01):\n    \"\"\"\n    Find minimum rank needed to achieve target reconstruction error.\n    \n    Args:\n        W: Input matrix\n        target_error: Maximum acceptable relative error (default 1%)\n    \n    Returns:\n        Optimal rank (int)\n    \"\"\"\n    # TODO: Implement this\n    # 1. Perform SVD on W\n    # 2. Loop through ranks from 1 to len(S)\n    # 3. For each rank, reconstruct and compute error using relative_error()\n    # 4. Return first rank where error < target_error\n    raise NotImplementedError(\"Implement the find_optimal_rank function\")\n\n# Test on our neural network weight (uncomment after implementing)\n# optimal_r = find_optimal_rank(W_neural, target_error=0.01)\n# print(f\"Optimal rank for <1% error: {optimal_r}\")\n# print(f\"Full rank: {W_neural.shape[0]}\")\n# print(f\"Compression ratio: {W_neural.size / (2 * W_neural.shape[0] * optimal_r):.1f}x\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "\n",
    "- ‚úÖ **SVD** decomposes matrices into singular values and vectors\n",
    "- ‚úÖ **Low-rank approximation** captures most information with fewer parameters\n",
    "- ‚úÖ **LoRA** uses this insight to fine-tune with 96%+ fewer parameters\n",
    "- ‚úÖ **Memory savings** scale with rank (lower rank = smaller adapters)\n",
    "- ‚úÖ Trained weights are usually **more compressible** than random matrices\n",
    "\n",
    "**Key insight:** LoRA works because weight updates during fine-tuning are low-rank!\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [LoRA Paper](https://arxiv.org/abs/2106.09685) - Original LoRA paper\n",
    "- [QLoRA Paper](https://arxiv.org/abs/2305.14314) - Quantized LoRA\n",
    "- [SVD Tutorial](https://www.youtube.com/watch?v=mBcLRGuAFUk) - Visual explanation\n",
    "- [Hugging Face PEFT](https://huggingface.co/docs/peft) - LoRA implementation\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")\n",
    "print(\"\\n‚û°Ô∏è  Next: Lab 1.4.5 - Probability Distributions Lab\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}