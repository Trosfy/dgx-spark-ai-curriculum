{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.4.5 Solutions: Probability Distributions Lab\n",
    "\n",
    "This notebook contains solutions for the exercises in the Probability Distributions lab.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ðŸ“ Note:** This solution notebook is designed to be self-contained and includes all necessary\n",
    "> helper functions. However, if you prefer to use your own implementations from the main notebook,\n",
    "> ensure those cells have been run first.\n",
    ">\n",
    "> You can also import production-ready implementations from the scripts:\n",
    "> ```python\n",
    "> from scripts.math_utils import sigmoid, relu, Adam, SGD\n",
    "> from scripts.visualization_utils import plot_loss_landscape, plot_training_curve\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Probability Distributions Solutions\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions from the Main Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_pmf(k, p):\n",
    "    \"\"\"Bernoulli probability mass function: p(k) = p^k * (1-p)^(1-k)\"\"\"\n",
    "    return (p ** k) * ((1 - p) ** (1 - k))\n",
    "\n",
    "def bernoulli_log_pmf(k, p):\n",
    "    \"\"\"Log of Bernoulli PMF\"\"\"\n",
    "    eps = 1e-10\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    return k * np.log(p) + (1 - k) * np.log(1 - p)\n",
    "\n",
    "def softmax(logits):\n",
    "    \"\"\"Convert logits to probabilities\"\"\"\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / exp_logits.sum()\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise Solution: Implement Binary Cross-Entropy\n",
    "\n",
    "### ðŸ§’ ELI5: What is Binary Cross-Entropy?\n",
    "\n",
    "> **Imagine you're judging a weather forecaster...**\n",
    ">\n",
    "> - Forecaster says: \"70% chance of rain\" (p = 0.7)\n",
    "> - Reality: It rained! (y = 1)\n",
    ">\n",
    "> How do we score them?\n",
    "> - If they said 90% rain and it rained: Small penalty (good prediction!)\n",
    "> - If they said 10% rain and it rained: Large penalty (bad prediction!)\n",
    ">\n",
    "> **BCE formula:** `loss = -[y*log(p) + (1-y)*log(1-p)]`\n",
    ">\n",
    "> This is EXACTLY the same as asking:\n",
    "> \"What's the negative log-probability of the outcome under a Bernoulli distribution?\"\n",
    "\n",
    "### The Math\n",
    "\n",
    "**Bernoulli log-PMF:**\n",
    "$$\\log p(y|\\hat{p}) = y \\log(\\hat{p}) + (1-y) \\log(1-\\hat{p})$$\n",
    "\n",
    "**Binary Cross-Entropy (negative log-PMF):**\n",
    "$$BCE = -[y \\log(\\hat{p}) + (1-y) \\log(1-\\hat{p})]$$\n",
    "\n",
    "They're the same thing, just with opposite signs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Binary Cross-Entropy Loss.\n",
    "    \n",
    "    Formula: BCE = -[y*log(p) + (1-y)*log(1-p)]\n",
    "    \n",
    "    This is the standard loss for binary classification.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Binary labels (0 or 1)\n",
    "        y_pred: Predicted probabilities (between 0 and 1)\n",
    "        \n",
    "    Returns:\n",
    "        Mean BCE loss over all samples\n",
    "    \"\"\"\n",
    "    # Clip predictions to avoid log(0) = -inf\n",
    "    eps = 1e-10\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    \n",
    "    # Compute BCE for each sample\n",
    "    bce = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    # Return mean\n",
    "    return np.mean(bce)\n",
    "\n",
    "print(\"binary_cross_entropy() function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "y_true_binary = np.array([1, 0, 1, 1, 0])\n",
    "y_pred_probs = np.array([0.9, 0.2, 0.8, 0.7, 0.3])\n",
    "\n",
    "print(\"Test Data:\")\n",
    "print(f\"  True labels:  {y_true_binary}\")\n",
    "print(f\"  Predictions:  {y_pred_probs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute BCE using our function\n",
    "bce = binary_cross_entropy(y_true_binary, y_pred_probs)\n",
    "\n",
    "# Compute as Bernoulli NLL (negative log-PMF)\n",
    "bernoulli_nll_values = [-bernoulli_log_pmf(y, p) for y, p in zip(y_true_binary, y_pred_probs)]\n",
    "bernoulli_nll_mean = np.mean(bernoulli_nll_values)\n",
    "\n",
    "print(\"\\nComparison: BCE vs Bernoulli NLL\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Binary Cross-Entropy: {bce:.6f}\")\n",
    "print(f\"Bernoulli NLL (mean): {bernoulli_nll_mean:.6f}\")\n",
    "print(f\"\\nDifference: {abs(bce - bernoulli_nll_mean):.10f}\")\n",
    "print(\"\\nâœ… They're the same! BCE = Bernoulli NLL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show per-sample breakdown\n",
    "print(\"\\nPer-Sample Breakdown:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Sample':<8} {'y':<5} {'pÌ‚':<8} {'BCE':<12} {'Bernoulli NLL':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, (y, p) in enumerate(zip(y_true_binary, y_pred_probs)):\n",
    "    bce_i = -(y * np.log(p + 1e-10) + (1-y) * np.log(1-p + 1e-10))\n",
    "    nll_i = -bernoulli_log_pmf(y, p)\n",
    "    print(f\"{i:<8} {y:<5} {p:<8.2f} {bce_i:<12.6f} {nll_i:<12.6f}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Mean:':<14} {'':<8} {bce:<12.6f} {bernoulli_nll_mean:<12.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Does This Matter?\n",
    "\n",
    "Understanding that **BCE = Bernoulli NLL** tells us:\n",
    "\n",
    "1. **Loss functions aren't arbitrary** - they come from probability theory\n",
    "2. **Minimizing BCE** = Finding parameters that maximize the likelihood of observing our data\n",
    "3. **Model outputs are probabilities** - the sigmoid/softmax isn't just for \"squashing\", it's giving us actual probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Visualizing BCE Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how BCE penalizes predictions\n",
    "\n",
    "p_range = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "# BCE when y = 1 (should predict high)\n",
    "bce_y1 = -np.log(p_range)\n",
    "\n",
    "# BCE when y = 0 (should predict low)\n",
    "bce_y0 = -np.log(1 - p_range)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot for each true label\n",
    "axes[0].plot(p_range, bce_y1, 'b-', linewidth=2, label='True label y=1')\n",
    "axes[0].plot(p_range, bce_y0, 'r-', linewidth=2, label='True label y=0')\n",
    "axes[0].set_xlabel('Predicted probability pÌ‚', fontsize=12)\n",
    "axes[0].set_ylabel('BCE Loss', fontsize=12)\n",
    "axes[0].set_title('BCE Loss vs Predicted Probability', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim(0, 5)\n",
    "\n",
    "# Gradient (derivative)\n",
    "grad_y1 = -1 / p_range  # d/dp [-log(p)] = -1/p\n",
    "grad_y0 = 1 / (1 - p_range)  # d/dp [-log(1-p)] = 1/(1-p)\n",
    "\n",
    "axes[1].plot(p_range, np.abs(grad_y1), 'b-', linewidth=2, label='True label y=1')\n",
    "axes[1].plot(p_range, np.abs(grad_y0), 'r-', linewidth=2, label='True label y=0')\n",
    "axes[1].set_xlabel('Predicted probability pÌ‚', fontsize=12)\n",
    "axes[1].set_ylabel('|Gradient|', fontsize=12)\n",
    "axes[1].set_title('Gradient Magnitude vs Predicted Probability', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(0, 20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Key Observations:\")\n",
    "print(\"  - When y=1: Loss decreases as pÌ‚ â†’ 1 (predicting positive correctly)\")\n",
    "print(\"  - When y=0: Loss decreases as pÌ‚ â†’ 0 (predicting negative correctly)\")\n",
    "print(\"  - Gradients are LARGER for worse predictions (faster learning when very wrong!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus 2: Comparing BCE with MSE for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why don't we use MSE for classification?\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    \"\"\"Mean Squared Error\"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# For y = 1, compare losses as a function of prediction\n",
    "p_range = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "bce_values = -np.log(p_range)  # BCE when y=1\n",
    "mse_values = (1 - p_range) ** 2  # MSE when y=1\n",
    "\n",
    "# Gradients\n",
    "bce_grad = 1 / p_range  # |gradient| of BCE\n",
    "mse_grad = 2 * (1 - p_range)  # |gradient| of MSE\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss comparison\n",
    "axes[0].plot(p_range, bce_values, 'b-', linewidth=2, label='BCE')\n",
    "axes[0].plot(p_range, mse_values, 'r-', linewidth=2, label='MSE')\n",
    "axes[0].set_xlabel('Predicted probability pÌ‚ (true y=1)', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('BCE vs MSE Loss (when y=1)', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim(0, 5)\n",
    "\n",
    "# Gradient comparison\n",
    "axes[1].plot(p_range, bce_grad, 'b-', linewidth=2, label='BCE gradient')\n",
    "axes[1].plot(p_range, mse_grad, 'r-', linewidth=2, label='MSE gradient')\n",
    "axes[1].set_xlabel('Predicted probability pÌ‚ (true y=1)', fontsize=12)\n",
    "axes[1].set_ylabel('|Gradient|', fontsize=12)\n",
    "axes[1].set_title('BCE vs MSE Gradient (when y=1)', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(0, 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Why BCE is better for classification:\")\n",
    "print(\"  - MSE gradient â†’ 0 when pÌ‚ â†’ 0 (model is VERY wrong but learns slowly!)\")\n",
    "print(\"  - BCE gradient â†’ âˆž when pÌ‚ â†’ 0 (model learns FAST when very wrong)\")\n",
    "print(\"  - This is called the 'vanishing gradient problem' with MSE for classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus 3: Implementing Multi-class Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_true_onehot, y_pred_probs):\n",
    "    \"\"\"\n",
    "    Multi-class Cross-Entropy Loss.\n",
    "    \n",
    "    Formula: CE = -sum(y_k * log(p_k))\n",
    "    \n",
    "    This is the standard loss for multi-class classification.\n",
    "    \n",
    "    Args:\n",
    "        y_true_onehot: One-hot encoded true labels (shape: [n_samples, n_classes])\n",
    "        y_pred_probs: Predicted probabilities (shape: [n_samples, n_classes])\n",
    "        \n",
    "    Returns:\n",
    "        Mean CE loss over all samples\n",
    "    \"\"\"\n",
    "    eps = 1e-10\n",
    "    y_pred_probs = np.clip(y_pred_probs, eps, 1 - eps)\n",
    "    \n",
    "    # For one-hot labels, this simplifies to -log(p_true_class)\n",
    "    ce = -np.sum(y_true_onehot * np.log(y_pred_probs), axis=-1)\n",
    "    \n",
    "    return np.mean(ce)\n",
    "\n",
    "# Test data: 3 samples, 5 classes\n",
    "n_samples = 3\n",
    "n_classes = 5\n",
    "\n",
    "# True labels (as indices)\n",
    "true_classes = np.array([0, 2, 4])\n",
    "\n",
    "# Convert to one-hot\n",
    "y_true_onehot = np.eye(n_classes)[true_classes]\n",
    "\n",
    "# Predicted logits (network output)\n",
    "logits = np.array([\n",
    "    [2.0, 0.5, 0.1, -0.5, -1.0],  # Should predict class 0\n",
    "    [0.1, 0.3, 2.5, 0.2, -0.1],   # Should predict class 2\n",
    "    [-0.5, -0.3, 0.1, 0.2, 1.8],  # Should predict class 4\n",
    "])\n",
    "\n",
    "# Convert to probabilities\n",
    "y_pred_probs_multi = np.array([softmax(l) for l in logits])\n",
    "\n",
    "# Compute cross-entropy\n",
    "ce_loss = cross_entropy(y_true_onehot, y_pred_probs_multi)\n",
    "\n",
    "print(\"Multi-class Cross-Entropy Example\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"True classes: {true_classes}\")\n",
    "print(f\"\\nPredicted probabilities:\")\n",
    "for i, probs in enumerate(y_pred_probs_multi):\n",
    "    pred_class = np.argmax(probs)\n",
    "    print(f\"  Sample {i}: {probs.round(3)} â†’ pred: {pred_class}, true: {true_classes[i]}\")\n",
    "print(f\"\\nCross-Entropy Loss: {ce_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: CE should equal -log(p_true_class) averaged over samples\n",
    "\n",
    "manual_ce = []\n",
    "for i, true_class in enumerate(true_classes):\n",
    "    p_true = y_pred_probs_multi[i, true_class]\n",
    "    nll = -np.log(p_true)\n",
    "    manual_ce.append(nll)\n",
    "    print(f\"Sample {i}: p(true class {true_class}) = {p_true:.4f}, NLL = {nll:.4f}\")\n",
    "\n",
    "print(f\"\\nMean NLL: {np.mean(manual_ce):.6f}\")\n",
    "print(f\"CE Loss:  {ce_loss:.6f}\")\n",
    "print(f\"\\nâœ… They match! CE = mean of negative log-likelihood of true classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus 4: KL Divergence Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    \"\"\"Compute entropy of distribution p\"\"\"\n",
    "    eps = 1e-10\n",
    "    p = np.clip(p, eps, 1-eps)\n",
    "    return -np.sum(p * np.log(p))\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    \"\"\"Compute KL divergence D_KL(p || q)\"\"\"\n",
    "    eps = 1e-10\n",
    "    p = np.clip(p, eps, 1-eps)\n",
    "    q = np.clip(q, eps, 1-eps)\n",
    "    return np.sum(p * np.log(p / q))\n",
    "\n",
    "def cross_entropy_probs(p, q):\n",
    "    \"\"\"Compute cross-entropy H(p, q)\"\"\"\n",
    "    eps = 1e-10\n",
    "    q = np.clip(q, eps, 1-eps)\n",
    "    return -np.sum(p * np.log(q))\n",
    "\n",
    "# Demonstration: CE = Entropy + KL Divergence\n",
    "p_true = np.array([0.6, 0.3, 0.1])  # True distribution\n",
    "q_pred = np.array([0.4, 0.4, 0.2])  # Predicted distribution\n",
    "\n",
    "H_p = entropy(p_true)\n",
    "D_kl = kl_divergence(p_true, q_pred)\n",
    "H_pq = cross_entropy_probs(p_true, q_pred)\n",
    "\n",
    "print(\"The Fundamental Relationship\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"True distribution P:      {p_true}\")\n",
    "print(f\"Predicted distribution Q: {q_pred}\")\n",
    "print()\n",
    "print(f\"Entropy H(P):         {H_p:.6f}\")\n",
    "print(f\"KL Divergence D(P||Q): {D_kl:.6f}\")\n",
    "print(f\"Cross-Entropy H(P,Q):  {H_pq:.6f}\")\n",
    "print()\n",
    "print(f\"H(P) + D_KL(P||Q) = {H_p + D_kl:.6f}\")\n",
    "print()\n",
    "print(\"âœ… Cross-Entropy = Entropy + KL Divergence\")\n",
    "print(\"\\nðŸ“Š Since Entropy(P) is constant during training:\")\n",
    "print(\"   Minimizing Cross-Entropy = Minimizing KL Divergence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: The Probability Foundations of Loss Functions\n",
    "\n",
    "| Task | Distribution | Loss Function | Formula |\n",
    "|------|--------------|---------------|---------|\n",
    "| Regression | Gaussian | MSE | $(y - \\hat{y})^2$ |\n",
    "| Binary Classification | Bernoulli | BCE | $-[y\\log(p) + (1-y)\\log(1-p)]$ |\n",
    "| Multi-class Classification | Categorical | CE | $-\\sum_k y_k \\log(p_k)$ |\n",
    "\n",
    "**Key insight:** All these loss functions are just **negative log-likelihoods** of different probability distributions!\n",
    "\n",
    "Training a neural network = Finding parameters that **maximize the likelihood** of observing our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import gc\ngc.collect()\n\nprint(\"\\nâœ… Solution notebook complete!\")\nprint(\"\\nðŸŽ‰ Congratulations! You've completed all Module 1.4 solutions!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}