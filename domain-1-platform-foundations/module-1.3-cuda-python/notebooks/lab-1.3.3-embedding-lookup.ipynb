{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.3.3: Custom Embedding Lookup\n",
    "\n",
    "**Module:** 1.3 - CUDA Python & GPU Programming  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand how embeddings work in neural networks\n",
    "- [ ] Implement a custom CUDA kernel for batched embedding lookup\n",
    "- [ ] Optimize memory access patterns for embedding tables\n",
    "- [ ] Compare with PyTorch's `nn.Embedding` performance\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Labs 1.3.1 and 1.3.2\n",
    "- Knowledge of: How text is tokenized into IDs\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**Embeddings are the foundation of every LLM.**\n",
    "\n",
    "When you type \"Hello, how are you?\" into ChatGPT:\n",
    "1. **Tokenizer** converts text ‚Üí `[15496, 11, 703, 389, 499, 30]` (token IDs)\n",
    "2. **Embedding layer** converts IDs ‚Üí vectors (the first neural network operation)\n",
    "3. The rest of the model processes these vectors\n",
    "\n",
    "**Scale of embeddings in modern LLMs:**\n",
    "\n",
    "| Model | Vocab Size | Embedding Dim | Embedding Table Size |\n",
    "|-------|------------|---------------|---------------------|\n",
    "| GPT-2 | 50,257 | 768 | 147 MB |\n",
    "| Llama-2-7B | 32,000 | 4,096 | 500 MB |\n",
    "| Llama-3-70B | 128,256 | 8,192 | **4 GB** |\n",
    "| GPT-4 (estimated) | ~100K | ~12K | **~5 GB** |\n",
    "\n",
    "With batch sizes of thousands and sequences of thousands of tokens, embedding lookup becomes a significant operation. Understanding its memory access pattern is crucial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What Are Embeddings?\n",
    "\n",
    "> **Imagine a giant library** where every word in every language has its own book. There are 50,000+ books (vocabulary size).\n",
    ">\n",
    "> Each book contains a **secret code** - a list of 768 numbers that describe everything important about that word:\n",
    "> - Its meaning\n",
    "> - What other words it's related to\n",
    "> - Whether it's a noun, verb, emotion, etc.\n",
    ">\n",
    "> When you give the AI a word like \"cat\" (book #1234), it looks up book #1234 and reads the secret code. That's the **embedding**!\n",
    ">\n",
    "> **The magic:** Similar words have similar codes. \"Cat\" and \"kitten\" have codes that are close together. \"Cat\" and \"refrigerator\" have very different codes.\n",
    ">\n",
    "> **In code terms:**\n",
    "> ```python\n",
    "> # embedding_table shape: (50000, 768)\n",
    "> # Like a 2D array: 50000 rows (words), 768 columns (features)\n",
    "> token_id = 1234  # The word \"cat\"\n",
    "> embedding = embedding_table[token_id]  # Get the 768-dim vector for \"cat\"\n",
    "> ```\n",
    "\n",
    "```\n",
    "Token ID: 1234 (\"cat\")\n",
    "    ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ     Embedding Table            ‚îÇ\n",
    "‚îÇ  (50,000 √ó 768)               ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ Row 0:    [0.1, -0.3, ...]    ‚îÇ ‚Üê \"the\"\n",
    "‚îÇ Row 1:    [0.5,  0.2, ...]    ‚îÇ ‚Üê \"a\"\n",
    "‚îÇ ...                           ‚îÇ\n",
    "‚îÇ Row 1234: [0.8, -0.1, ...]    ‚îÇ ‚Üê \"cat\" ‚úì (we want this row!)\n",
    "‚îÇ ...                           ‚îÇ\n",
    "‚îÇ Row 49999: [-0.2, 0.7, ...]   ‚îÇ ‚Üê \"zebra\"\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "    ‚Üì\n",
    "Output: [0.8, -0.1, 0.3, -0.5, ...] (768 numbers)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 0: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from typing import Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from numba import cuda, float32, int32\n",
    "import numba\n",
    "\n",
    "# Check for PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    HAS_TORCH = True\n",
    "    print(f\"‚úÖ PyTorch {torch.__version__} available\")\n",
    "    print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   Device: {torch.cuda.get_device_name()}\")\n",
    "except ImportError:\n",
    "    HAS_TORCH = False\n",
    "    print(\"‚ö†Ô∏è PyTorch not available\")\n",
    "\n",
    "print(f\"\\n‚úÖ Numba {numba.__version__} available\")\n",
    "print(f\"   CUDA available: {cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding PyTorch's nn.Embedding\n",
    "\n",
    "First, let's understand how the standard PyTorch embedding works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example\n",
    "if HAS_TORCH:\n",
    "    # Create a small embedding table: 10 words, 4-dimensional embeddings\n",
    "    vocab_size = 10\n",
    "    embedding_dim = 4\n",
    "    \n",
    "    embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    \n",
    "    print(\"Embedding table (10 words √ó 4 features):\")\n",
    "    print(embedding.weight.data)\n",
    "    \n",
    "    # Look up embeddings for specific tokens\n",
    "    token_ids = torch.tensor([0, 3, 7])  # Look up words 0, 3, and 7\n",
    "    result = embedding(token_ids)\n",
    "    \n",
    "    print(f\"\\nInput token IDs: {token_ids.tolist()}\")\n",
    "    print(f\"Output embeddings (shape {result.shape}):\")\n",
    "    print(result)\n",
    "    \n",
    "    print(\"\\nüí° Notice: Output row 0 matches embedding.weight[0]\")\n",
    "    print(f\"   embedding.weight[0]: {embedding.weight[0].tolist()}\")\n",
    "    print(f\"   result[0]:           {result[0].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batched Embedding Lookup\n",
    "\n",
    "In practice, we process entire sequences in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TORCH:\n",
    "    # Simulate a batch of 3 sequences, each 5 tokens long\n",
    "    batch_size = 3\n",
    "    seq_length = 5\n",
    "    \n",
    "    # Random token IDs (simulating tokenized text)\n",
    "    token_ids = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "    \n",
    "    print(f\"Input shape: {token_ids.shape} (batch √ó sequence length)\")\n",
    "    print(f\"Token IDs:\\n{token_ids}\")\n",
    "    \n",
    "    # Embedding lookup\n",
    "    embeddings = embedding(token_ids)\n",
    "    \n",
    "    print(f\"\\nOutput shape: {embeddings.shape} (batch √ó seq_len √ó embed_dim)\")\n",
    "    print(f\"\\nüí° For each of {batch_size * seq_length} tokens, we get a {embedding_dim}-dim vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: CPU Implementation (NumPy)\n",
    "\n",
    "Before writing CUDA, let's implement embedding lookup in pure NumPy to understand the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_lookup_cpu(embedding_table: np.ndarray, token_ids: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    CPU embedding lookup using NumPy advanced indexing.\n",
    "    \n",
    "    Args:\n",
    "        embedding_table: Shape (vocab_size, embedding_dim)\n",
    "        token_ids: Shape (batch_size, seq_length) or any shape\n",
    "    \n",
    "    Returns:\n",
    "        embeddings: Shape (*token_ids.shape, embedding_dim)\n",
    "    \"\"\"\n",
    "    # NumPy's advanced indexing does all the work!\n",
    "    # embedding_table[token_ids] gathers rows from embedding_table\n",
    "    return embedding_table[token_ids]\n",
    "\n",
    "\n",
    "# Test\n",
    "vocab_size = 10\n",
    "embedding_dim = 4\n",
    "\n",
    "# Create embedding table\n",
    "np.random.seed(42)\n",
    "embed_table = np.random.randn(vocab_size, embedding_dim).astype(np.float32)\n",
    "\n",
    "# Token IDs to look up\n",
    "token_ids_np = np.array([[0, 3, 7], [2, 5, 9]], dtype=np.int32)  # 2 sequences √ó 3 tokens\n",
    "\n",
    "result_cpu = embedding_lookup_cpu(embed_table, token_ids_np)\n",
    "\n",
    "print(f\"Embedding table shape: {embed_table.shape}\")\n",
    "print(f\"Token IDs shape: {token_ids_np.shape}\")\n",
    "print(f\"Output shape: {result_cpu.shape}\")\n",
    "\n",
    "print(f\"\\nToken IDs:\\n{token_ids_np}\")\n",
    "print(f\"\\nFirst sequence embeddings (tokens 0, 3, 7):\")\n",
    "print(result_cpu[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Understanding the Memory Access Pattern\n",
    "\n",
    "```\n",
    "Embedding table in memory (row-major):\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ [Row 0: emb[0,0], emb[0,1], ..., emb[0,767]]    ‚îÇ ‚Üê Contiguous in memory\n",
    "‚îÇ [Row 1: emb[1,0], emb[1,1], ..., emb[1,767]]    ‚îÇ\n",
    "‚îÇ [Row 2: ...]                                    ‚îÇ\n",
    "‚îÇ ...                                             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "Token IDs: [1234, 5678, 9012, ...]\n",
    "\n",
    "Access pattern:\n",
    "1. Jump to row 1234 ‚Üí read 768 floats\n",
    "2. Jump to row 5678 ‚Üí read 768 floats  ‚Üê Random access! Cache miss!\n",
    "3. Jump to row 9012 ‚Üí read 768 floats  ‚Üê Random access! Cache miss!\n",
    "```\n",
    "\n",
    "**Key insight:** Embedding lookup is essentially **random memory access**. Unlike matrix multiplication where we can reuse data, each token accesses a different, unpredictable row. This makes it **memory bandwidth bound**, not compute bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: CUDA Kernel for Embedding Lookup\n",
    "\n",
    "### Design Choices\n",
    "\n",
    "**Option 1:** One thread per token (each thread reads entire embedding)\n",
    "- Simple, but poor parallelism for large embedding_dim\n",
    "\n",
    "**Option 2:** One thread per embedding element (our choice)\n",
    "- Better parallelism\n",
    "- Need to handle the token ID broadcast\n",
    "\n",
    "**Option 3:** One block per token, threads cooperate\n",
    "- Best for very large embedding dimensions\n",
    "- More complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def embedding_lookup_kernel(embedding_table, token_ids, output):\n",
    "    \"\"\"\n",
    "    CUDA kernel for batched embedding lookup.\n",
    "    \n",
    "    Grid layout:\n",
    "    - x dimension: embedding dimension (which feature)\n",
    "    - y dimension: flattened token index (which token)\n",
    "    \n",
    "    Each thread copies one float from embedding_table to output.\n",
    "    \n",
    "    Args:\n",
    "        embedding_table: (vocab_size, embedding_dim)\n",
    "        token_ids: (total_tokens,) - flattened token IDs\n",
    "        output: (total_tokens, embedding_dim) - flattened output\n",
    "    \"\"\"\n",
    "    # Which embedding dimension (feature) this thread handles\n",
    "    embed_idx = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    # Which token this thread handles\n",
    "    token_idx = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
    "    \n",
    "    vocab_size, embedding_dim = embedding_table.shape\n",
    "    total_tokens = token_ids.shape[0]\n",
    "    \n",
    "    # Bounds check\n",
    "    if embed_idx < embedding_dim and token_idx < total_tokens:\n",
    "        # Look up which row of embedding table to use\n",
    "        token_id = token_ids[token_idx]\n",
    "        \n",
    "        # Copy the embedding value\n",
    "        # Note: This is coalesced access across threads in x-dimension!\n",
    "        output[token_idx, embed_idx] = embedding_table[token_id, embed_idx]\n",
    "\n",
    "\n",
    "def embedding_lookup_gpu(embedding_table: np.ndarray, token_ids: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    GPU embedding lookup.\n",
    "    \n",
    "    Args:\n",
    "        embedding_table: (vocab_size, embedding_dim)\n",
    "        token_ids: Any shape, will be flattened\n",
    "    \n",
    "    Returns:\n",
    "        embeddings: (*token_ids.shape, embedding_dim)\n",
    "    \"\"\"\n",
    "    original_shape = token_ids.shape\n",
    "    vocab_size, embedding_dim = embedding_table.shape\n",
    "    \n",
    "    # Flatten token_ids\n",
    "    flat_tokens = token_ids.flatten().astype(np.int32)\n",
    "    total_tokens = flat_tokens.shape[0]\n",
    "    \n",
    "    # Transfer to GPU\n",
    "    d_embed = cuda.to_device(embedding_table)\n",
    "    d_tokens = cuda.to_device(flat_tokens)\n",
    "    d_output = cuda.device_array((total_tokens, embedding_dim), dtype=np.float32)\n",
    "    \n",
    "    # Configure grid\n",
    "    threads_per_block = (32, 8)  # 256 threads per block\n",
    "    blocks_x = (embedding_dim + 31) // 32\n",
    "    blocks_y = (total_tokens + 7) // 8\n",
    "    blocks = (blocks_x, blocks_y)\n",
    "    \n",
    "    # Launch kernel\n",
    "    embedding_lookup_kernel[blocks, threads_per_block](d_embed, d_tokens, d_output)\n",
    "    \n",
    "    # Get result and reshape\n",
    "    result = d_output.copy_to_host()\n",
    "    return result.reshape(*original_shape, embedding_dim)\n",
    "\n",
    "\n",
    "# Test\n",
    "result_gpu = embedding_lookup_gpu(embed_table, token_ids_np)\n",
    "\n",
    "print(f\"GPU result shape: {result_gpu.shape}\")\n",
    "print(f\"Results match CPU: {np.allclose(result_gpu, result_cpu)}\")\n",
    "print(f\"\\nFirst sequence embeddings:\")\n",
    "print(result_gpu[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Why This Kernel Design?\n",
    "\n",
    "```\n",
    "Block (0, 0):\n",
    "  Thread (0,0) ‚Üí output[0, 0]\n",
    "  Thread (1,0) ‚Üí output[0, 1]  ‚Üê Coalesced: adjacent threads access adjacent memory\n",
    "  Thread (2,0) ‚Üí output[0, 2]\n",
    "  ...\n",
    "  Thread (31,0) ‚Üí output[0, 31]\n",
    "  Thread (0,1) ‚Üí output[1, 0]\n",
    "  Thread (1,1) ‚Üí output[1, 1]\n",
    "  ...\n",
    "\n",
    "Memory access pattern (output):\n",
    "  32 adjacent threads write to 32 adjacent memory locations\n",
    "  = Perfect coalescing = Maximum bandwidth utilization!\n",
    "\n",
    "Memory access pattern (embedding_table):\n",
    "  Each row (token_id) may be different\n",
    "  But within a row, access is coalesced\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Optimized Version with Better Coalescing\n",
    "\n",
    "The previous version has good write coalescing, but the read pattern could be better. Let's optimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_BLOCK_SIZE = 256  # Threads per block for embedding dimension\n",
    "\n",
    "@cuda.jit\n",
    "def embedding_lookup_optimized_kernel(embedding_table, token_ids, output):\n",
    "    \"\"\"\n",
    "    Optimized embedding lookup with better thread organization.\n",
    "    \n",
    "    Each block handles one token. Threads within the block cooperatively\n",
    "    copy the entire embedding vector.\n",
    "    \n",
    "    Benefits:\n",
    "    - Threads in same warp read from same row = better cache utilization\n",
    "    - Write coalescing maintained\n",
    "    - Works well for any embedding_dim\n",
    "    \"\"\"\n",
    "    # Block index = which token\n",
    "    token_idx = cuda.blockIdx.x\n",
    "    # Thread index = position in embedding\n",
    "    tx = cuda.threadIdx.x\n",
    "    \n",
    "    vocab_size, embedding_dim = embedding_table.shape\n",
    "    total_tokens = token_ids.shape[0]\n",
    "    \n",
    "    if token_idx >= total_tokens:\n",
    "        return\n",
    "    \n",
    "    # All threads in this block read the same token_id\n",
    "    token_id = token_ids[token_idx]\n",
    "    \n",
    "    # Each thread copies multiple elements if embedding_dim > block_size\n",
    "    for embed_idx in range(tx, embedding_dim, cuda.blockDim.x):\n",
    "        output[token_idx, embed_idx] = embedding_table[token_id, embed_idx]\n",
    "\n",
    "\n",
    "def embedding_lookup_gpu_optimized(embedding_table: np.ndarray, token_ids: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Optimized GPU embedding lookup.\n",
    "    \"\"\"\n",
    "    original_shape = token_ids.shape\n",
    "    vocab_size, embedding_dim = embedding_table.shape\n",
    "    \n",
    "    flat_tokens = token_ids.flatten().astype(np.int32)\n",
    "    total_tokens = flat_tokens.shape[0]\n",
    "    \n",
    "    d_embed = cuda.to_device(embedding_table)\n",
    "    d_tokens = cuda.to_device(flat_tokens)\n",
    "    d_output = cuda.device_array((total_tokens, embedding_dim), dtype=np.float32)\n",
    "    \n",
    "    # One block per token, up to 256 threads per block\n",
    "    threads = min(EMBED_BLOCK_SIZE, embedding_dim)\n",
    "    blocks = total_tokens\n",
    "    \n",
    "    embedding_lookup_optimized_kernel[blocks, threads](d_embed, d_tokens, d_output)\n",
    "    \n",
    "    result = d_output.copy_to_host()\n",
    "    return result.reshape(*original_shape, embedding_dim)\n",
    "\n",
    "\n",
    "# Test\n",
    "result_opt = embedding_lookup_gpu_optimized(embed_table, token_ids_np)\n",
    "print(f\"Optimized GPU result matches: {np.allclose(result_opt, result_cpu)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Benchmarking\n",
    "\n",
    "Let's compare our implementations against PyTorch at realistic LLM scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realistic LLM-scale parameters\n",
    "configs = [\n",
    "    {\"name\": \"GPT-2 Small\", \"vocab_size\": 50257, \"embedding_dim\": 768, \"batch_size\": 32, \"seq_length\": 512},\n",
    "    {\"name\": \"Llama-2-7B\", \"vocab_size\": 32000, \"embedding_dim\": 4096, \"batch_size\": 8, \"seq_length\": 1024},\n",
    "    {\"name\": \"Llama-3-8B\", \"vocab_size\": 128256, \"embedding_dim\": 4096, \"batch_size\": 8, \"seq_length\": 2048},\n",
    "]\n",
    "\n",
    "print(\"üìä Embedding Lookup Benchmark\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for config in configs:\n",
    "    name = config[\"name\"]\n",
    "    vocab_size = config[\"vocab_size\"]\n",
    "    embedding_dim = config[\"embedding_dim\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    seq_length = config[\"seq_length\"]\n",
    "    total_tokens = batch_size * seq_length\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Vocab: {vocab_size:,}, Embed: {embedding_dim}, Batch√óSeq: {batch_size}√ó{seq_length} = {total_tokens:,} tokens\")\n",
    "    \n",
    "    # Create data\n",
    "    embed_table = np.random.randn(vocab_size, embedding_dim).astype(np.float32)\n",
    "    token_ids = np.random.randint(0, vocab_size, (batch_size, seq_length), dtype=np.int32)\n",
    "    \n",
    "    # Embedding table size\n",
    "    table_size_mb = vocab_size * embedding_dim * 4 / 1024 / 1024\n",
    "    output_size_mb = total_tokens * embedding_dim * 4 / 1024 / 1024\n",
    "    print(f\"  Table size: {table_size_mb:.1f} MB, Output size: {output_size_mb:.1f} MB\")\n",
    "    \n",
    "    # Pre-transfer to GPU\n",
    "    d_embed = cuda.to_device(embed_table)\n",
    "    d_tokens = cuda.to_device(token_ids.flatten())\n",
    "    d_output = cuda.device_array((total_tokens, embedding_dim), dtype=np.float32)\n",
    "    \n",
    "    # Benchmark NumPy\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(5):\n",
    "        _ = embed_table[token_ids]\n",
    "    time_numpy = (time.perf_counter() - start) / 5\n",
    "    \n",
    "    # Benchmark basic GPU\n",
    "    threads = (32, 8)\n",
    "    blocks = ((embedding_dim + 31) // 32, (total_tokens + 7) // 8)\n",
    "    \n",
    "    # Warm up\n",
    "    embedding_lookup_kernel[blocks, threads](d_embed, d_tokens, d_output)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(10):\n",
    "        embedding_lookup_kernel[blocks, threads](d_embed, d_tokens, d_output)\n",
    "    cuda.synchronize()\n",
    "    time_gpu_basic = (time.perf_counter() - start) / 10\n",
    "    \n",
    "    # Benchmark optimized GPU\n",
    "    threads_opt = min(EMBED_BLOCK_SIZE, embedding_dim)\n",
    "    blocks_opt = total_tokens\n",
    "    \n",
    "    embedding_lookup_optimized_kernel[blocks_opt, threads_opt](d_embed, d_tokens, d_output)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(10):\n",
    "        embedding_lookup_optimized_kernel[blocks_opt, threads_opt](d_embed, d_tokens, d_output)\n",
    "    cuda.synchronize()\n",
    "    time_gpu_opt = (time.perf_counter() - start) / 10\n",
    "    \n",
    "    # Benchmark PyTorch if available\n",
    "    time_pytorch = None\n",
    "    if HAS_TORCH and torch.cuda.is_available():\n",
    "        torch_embed = nn.Embedding(vocab_size, embedding_dim).cuda()\n",
    "        torch_tokens = torch.from_numpy(token_ids).cuda()\n",
    "        \n",
    "        # Warm up\n",
    "        _ = torch_embed(torch_tokens)\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        for _ in range(10):\n",
    "            _ = torch_embed(torch_tokens)\n",
    "        torch.cuda.synchronize()\n",
    "        time_pytorch = (time.perf_counter() - start) / 10\n",
    "        \n",
    "        del torch_embed, torch_tokens\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Calculate bandwidth\n",
    "    bytes_transferred = total_tokens * embedding_dim * 4  # Output writes\n",
    "    bandwidth_gpu = bytes_transferred / time_gpu_opt / 1e9\n",
    "    \n",
    "    print(f\"\\n  {'Method':<20} {'Time (ms)':<12} {'Speedup':<12}\")\n",
    "    print(f\"  {'-'*44}\")\n",
    "    print(f\"  {'NumPy':<20} {time_numpy*1000:<12.3f} {'1.0x':<12}\")\n",
    "    print(f\"  {'GPU Basic':<20} {time_gpu_basic*1000:<12.3f} {f'{time_numpy/time_gpu_basic:.1f}x':<12}\")\n",
    "    print(f\"  {'GPU Optimized':<20} {time_gpu_opt*1000:<12.3f} {f'{time_numpy/time_gpu_opt:.1f}x':<12}\")\n",
    "    if time_pytorch:\n",
    "        print(f\"  {'PyTorch':<20} {time_pytorch*1000:<12.3f} {f'{time_numpy/time_pytorch:.1f}x':<12}\")\n",
    "        print(f\"\\n  Our optimized vs PyTorch: {time_gpu_opt/time_pytorch:.2f}x\")\n",
    "    print(f\"  Effective bandwidth: {bandwidth_gpu:.1f} GB/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Understanding the Results\n",
    "\n",
    "**Why might our kernel be slower than PyTorch?**\n",
    "\n",
    "PyTorch's `nn.Embedding` uses highly optimized CUDA primitives:\n",
    "1. **Vectorized loads/stores** - Uses `float4` (128-bit) operations\n",
    "2. **Memory prefetching** - Hints to GPU to load data in advance\n",
    "3. **Warp-level optimizations** - Uses cooperative group operations\n",
    "\n",
    "**Why embedding lookup is memory-bound:**\n",
    "\n",
    "```\n",
    "Arithmetic operations: 0 (just copying data)\n",
    "Memory operations: read vocab√óembed, write batch√óseq√óembed\n",
    "\n",
    "For Llama-3:\n",
    "- Read: 128K √ó 4K √ó 4B = 2 GB (worst case, all tokens different)\n",
    "- Write: 16K √ó 4K √ó 4B = 256 MB\n",
    "\n",
    "At 273 GB/s bandwidth: theoretical minimum ~8ms\n",
    "```\n",
    "\n",
    "The operation is fundamentally limited by memory bandwidth, not compute!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Implementing Backward Pass (Gradient)\n",
    "\n",
    "For training, we need the gradient of the embedding lookup. This is a **scatter add** operation: gradients are accumulated back into the embedding table rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def embedding_backward_kernel(grad_output, token_ids, grad_embedding):\n",
    "    \"\"\"\n",
    "    Backward pass for embedding lookup.\n",
    "    \n",
    "    For each token, add its gradient back to the corresponding row\n",
    "    of the embedding table gradient.\n",
    "    \n",
    "    Note: This uses atomic add because multiple tokens might map\n",
    "    to the same embedding row!\n",
    "    \n",
    "    Args:\n",
    "        grad_output: (total_tokens, embedding_dim) - gradient from next layer\n",
    "        token_ids: (total_tokens,) - which rows to update\n",
    "        grad_embedding: (vocab_size, embedding_dim) - gradient to accumulate\n",
    "    \"\"\"\n",
    "    token_idx = cuda.blockIdx.x\n",
    "    embed_idx = cuda.threadIdx.x\n",
    "    \n",
    "    total_tokens, embedding_dim = grad_output.shape\n",
    "    \n",
    "    if token_idx >= total_tokens:\n",
    "        return\n",
    "    \n",
    "    token_id = token_ids[token_idx]\n",
    "    \n",
    "    for idx in range(embed_idx, embedding_dim, cuda.blockDim.x):\n",
    "        # Atomic add because multiple tokens might update same row!\n",
    "        cuda.atomic.add(grad_embedding, (token_id, idx), grad_output[token_idx, idx])\n",
    "\n",
    "\n",
    "def embedding_backward_gpu(grad_output: np.ndarray, token_ids: np.ndarray, \n",
    "                           vocab_size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute gradient of embedding table.\n",
    "    \"\"\"\n",
    "    total_tokens, embedding_dim = grad_output.shape\n",
    "    \n",
    "    d_grad_output = cuda.to_device(grad_output)\n",
    "    d_token_ids = cuda.to_device(token_ids.flatten().astype(np.int32))\n",
    "    d_grad_embedding = cuda.to_device(np.zeros((vocab_size, embedding_dim), dtype=np.float32))\n",
    "    \n",
    "    threads = min(256, embedding_dim)\n",
    "    blocks = total_tokens\n",
    "    \n",
    "    embedding_backward_kernel[blocks, threads](d_grad_output, d_token_ids, d_grad_embedding)\n",
    "    \n",
    "    return d_grad_embedding.copy_to_host()\n",
    "\n",
    "\n",
    "# Test backward pass\n",
    "print(\"Testing Embedding Backward Pass\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "vocab_size = 10\n",
    "embedding_dim = 4\n",
    "batch_size = 2\n",
    "seq_length = 3\n",
    "\n",
    "# Random gradients coming from next layer\n",
    "grad_output = np.random.randn(batch_size * seq_length, embedding_dim).astype(np.float32)\n",
    "# Some tokens repeat to test atomic add\n",
    "token_ids = np.array([0, 1, 0, 2, 1, 0], dtype=np.int32)  # Token 0 appears 3 times!\n",
    "\n",
    "# GPU backward\n",
    "grad_embedding_gpu = embedding_backward_gpu(grad_output, token_ids, vocab_size)\n",
    "\n",
    "# CPU reference\n",
    "grad_embedding_cpu = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "for i, token_id in enumerate(token_ids):\n",
    "    grad_embedding_cpu[token_id] += grad_output[i]\n",
    "\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"Token 0 appears {np.sum(token_ids == 0)} times\")\n",
    "print(f\"\\nGradient for embedding row 0 (should be sum of 3 gradients):\")\n",
    "print(f\"  CPU: {grad_embedding_cpu[0]}\")\n",
    "print(f\"  GPU: {grad_embedding_gpu[0]}\")\n",
    "print(f\"\\nResults match: {np.allclose(grad_embedding_gpu, grad_embedding_cpu)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting Atomic Operations in Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå WRONG: Regular assignment overwrites instead of accumulating\n",
    "# grad_embedding[token_id, idx] = grad_output[token_idx, idx]  # BUG!\n",
    "\n",
    "# ‚úÖ CORRECT: Use atomic add\n",
    "# cuda.atomic.add(grad_embedding, (token_id, idx), grad_output[token_idx, idx])\n",
    "\n",
    "print(\"üí° When multiple tokens map to the same row (very common!),\")\n",
    "print(\"   you MUST use atomic add to accumulate gradients correctly.\")\n",
    "print(\"   Without atomics, gradients get overwritten and training fails!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Int64 Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå WRONG: Using int64 (wasteful, may cause issues)\n",
    "# token_ids = np.array([1, 2, 3], dtype=np.int64)\n",
    "\n",
    "# ‚úÖ CORRECT: Use int32 (sufficient for vocab sizes up to 2 billion)\n",
    "# token_ids = np.array([1, 2, 3], dtype=np.int32)\n",
    "\n",
    "print(\"üí° Token IDs should be int32:\")\n",
    "print(\"   - Vocab sizes are typically < 200K (well under 2B limit)\")\n",
    "print(\"   - int32 uses half the memory of int64\")\n",
    "print(\"   - Better memory coalescing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Not Handling Out-of-Vocabulary Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In production, always validate token IDs!\n",
    "def safe_embedding_lookup(embedding_table, token_ids):\n",
    "    vocab_size = embedding_table.shape[0]\n",
    "    \n",
    "    # Check for invalid token IDs\n",
    "    if np.any(token_ids < 0) or np.any(token_ids >= vocab_size):\n",
    "        invalid_tokens = token_ids[(token_ids < 0) | (token_ids >= vocab_size)]\n",
    "        raise ValueError(f\"Invalid token IDs: {invalid_tokens[:5]}... (vocab_size={vocab_size})\")\n",
    "    \n",
    "    return embedding_lookup_gpu_optimized(embedding_table, token_ids)\n",
    "\n",
    "print(\"üí° Always validate token IDs before embedding lookup!\")\n",
    "print(\"   Out-of-bounds access = undefined behavior or crash.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself: Implement Positional Embeddings\n",
    "\n",
    "**Challenge:** Extend the embedding kernel to also add positional embeddings.\n",
    "\n",
    "In transformers, the input to the model is:\n",
    "```\n",
    "input = token_embedding + position_embedding\n",
    "```\n",
    "\n",
    "Where position_embedding depends on the position (0, 1, 2, ...) in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement combined token + position embedding lookup\n",
    "\n",
    "@cuda.jit\n",
    "def combined_embedding_kernel(token_embed_table, pos_embed_table, \n",
    "                               token_ids, positions, output):\n",
    "    \"\"\"\n",
    "    Combined token and positional embedding lookup.\n",
    "    \n",
    "    Args:\n",
    "        token_embed_table: (vocab_size, embedding_dim)\n",
    "        pos_embed_table: (max_seq_length, embedding_dim)\n",
    "        token_ids: (total_tokens,)\n",
    "        positions: (total_tokens,) - position index for each token\n",
    "        output: (total_tokens, embedding_dim)\n",
    "    \n",
    "    Output should be: token_embedding[token_id] + pos_embedding[position]\n",
    "    \"\"\"\n",
    "    # TODO: Implement this!\n",
    "    # Hint: Similar to embedding_lookup_optimized_kernel, but add two lookups\n",
    "    pass\n",
    "\n",
    "\n",
    "# When implemented, test with:\n",
    "# vocab_size = 100\n",
    "# max_seq_length = 512\n",
    "# embedding_dim = 64\n",
    "# token_embed = np.random.randn(vocab_size, embedding_dim).astype(np.float32)\n",
    "# pos_embed = np.random.randn(max_seq_length, embedding_dim).astype(np.float32)\n",
    "# token_ids = np.array([5, 10, 15, 20], dtype=np.int32)\n",
    "# positions = np.array([0, 1, 2, 3], dtype=np.int32)\n",
    "# \n",
    "# Expected: token_embed[5] + pos_embed[0], token_embed[10] + pos_embed[1], ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "```python\n",
    "@cuda.jit\n",
    "def combined_embedding_kernel(token_embed_table, pos_embed_table, \n",
    "                               token_ids, positions, output):\n",
    "    token_idx = cuda.blockIdx.x\n",
    "    tx = cuda.threadIdx.x\n",
    "    \n",
    "    _, embedding_dim = token_embed_table.shape\n",
    "    total_tokens = token_ids.shape[0]\n",
    "    \n",
    "    if token_idx >= total_tokens:\n",
    "        return\n",
    "    \n",
    "    token_id = token_ids[token_idx]\n",
    "    position = positions[token_idx]\n",
    "    \n",
    "    for embed_idx in range(tx, embedding_dim, cuda.blockDim.x):\n",
    "        # Add both embeddings!\n",
    "        output[token_idx, embed_idx] = (\n",
    "            token_embed_table[token_id, embed_idx] + \n",
    "            pos_embed_table[position, embed_idx]\n",
    "        )\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "Congratulations! You've learned:\n",
    "\n",
    "- ‚úÖ **How embeddings work** - The foundation of all language models\n",
    "- ‚úÖ **Memory access patterns** - Why embedding lookup is memory-bound\n",
    "- ‚úÖ **Custom CUDA kernels** - Forward and backward passes\n",
    "- ‚úÖ **Atomic operations** - Essential for gradient accumulation\n",
    "- ‚úÖ **Performance analysis** - Understanding bandwidth limitations\n",
    "\n",
    "You now understand one of the most fundamental operations in deep learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Advanced Challenge: Implement Sparse Gradient Updates**\n",
    "\n",
    "In training, most embedding rows are never updated in a single batch. Instead of computing gradients for the entire embedding table, you can:\n",
    "\n",
    "1. Find unique token IDs in the batch\n",
    "2. Only compute gradients for those rows\n",
    "3. Use sparse tensor representations\n",
    "\n",
    "This is how efficient embedding implementations work in practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [Word2Vec Original Paper](https://arxiv.org/abs/1301.3781) - The paper that popularized embeddings\n",
    "- [Efficient Estimation of Word Representations](https://arxiv.org/abs/1310.4546) - Negative sampling\n",
    "- [Rotary Position Embedding (RoPE)](https://arxiv.org/abs/2104.09864) - Used in modern LLMs like Llama\n",
    "- [FlashAttention Embedding Techniques](https://github.com/Dao-AILab/flash-attention)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import gc\n\n# Clean up large arrays safely\nif 'embed_table' in dir():\n    del embed_table\nif 'd_embed' in dir():\n    del d_embed\nif 'd_tokens' in dir():\n    del d_tokens\nif 'd_output' in dir():\n    del d_output\n\ngc.collect()\n\nif HAS_TORCH:\n    torch.cuda.empty_cache()\n\ntry:\n    cuda.current_context().reset()\nexcept Exception:\n    pass  # Context might not exist\n\nprint(\"‚úÖ GPU memory cleared!\")\nprint(\"\\n‚û°Ô∏è Ready for Lab 1.3.4: CuPy Integration\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}