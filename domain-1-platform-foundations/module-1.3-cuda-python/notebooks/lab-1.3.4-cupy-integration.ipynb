{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.3.4: CuPy Integration\n",
    "\n",
    "**Module:** 1.3 - CUDA Python & GPU Programming  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê (Beginner-Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Use CuPy as a drop-in replacement for NumPy\n",
    "- [ ] Port a data preprocessing pipeline to GPU\n",
    "- [ ] Write custom CUDA kernels using CuPy's RawKernel\n",
    "- [ ] Transfer data seamlessly between CuPy and PyTorch (zero-copy!)\n",
    "- [ ] Achieve **10x+ speedup** on data preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Labs 1.3.1-1.3.3 (helpful but not required)\n",
    "- Knowledge of: NumPy basics\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**The data preprocessing bottleneck is real.**\n",
    "\n",
    "In many ML pipelines, data preprocessing can take longer than model training:\n",
    "- **Image preprocessing:** Resize, normalize, augment (rotation, flip, crop)\n",
    "- **Text preprocessing:** Tokenization, padding, attention masks\n",
    "- **Tabular data:** Feature engineering, normalization, encoding\n",
    "- **Time series:** Windowing, FFT, filtering\n",
    "\n",
    "**The problem:** NumPy only uses CPU. While your $50,000 GPU sits idle during preprocessing!\n",
    "\n",
    "**The solution:** CuPy - a drop-in NumPy replacement that runs on GPU.\n",
    "\n",
    "```python\n",
    "# NumPy (CPU)\n",
    "import numpy as np\n",
    "data = np.random.randn(10000, 10000)\n",
    "result = np.linalg.svd(data)  # CPU, slow\n",
    "\n",
    "# CuPy (GPU) - just change the import!\n",
    "import cupy as cp\n",
    "data = cp.random.randn(10000, 10000)\n",
    "result = cp.linalg.svd(data)  # GPU, fast!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What is CuPy?\n",
    "\n",
    "> **Imagine you have a recipe book** (your NumPy code) with instructions for one cook (CPU).\n",
    ">\n",
    "> **CuPy** is like a translator that takes the same recipe and adapts it for a massive kitchen with 6,144 cooks (CUDA cores) who can all work simultaneously.\n",
    ">\n",
    "> The best part? You don't need to rewrite the recipe! Just change \"NumPy\" to \"CuPy\" and the translator handles everything.\n",
    ">\n",
    "> **Example:**\n",
    "> - \"Chop 1000 carrots\" ‚Üí CPU: one knife, 1000 cuts\n",
    "> - \"Chop 1000 carrots\" ‚Üí GPU: 1000 knives, all cutting at once!\n",
    "\n",
    "### CuPy vs NumPy: API Comparison\n",
    "\n",
    "| NumPy | CuPy | Notes |\n",
    "|-------|------|-------|\n",
    "| `np.array([1,2,3])` | `cp.array([1,2,3])` | Arrays live on GPU |\n",
    "| `np.zeros((100,100))` | `cp.zeros((100,100))` | Allocated on GPU memory |\n",
    "| `np.dot(a, b)` | `cp.dot(a, b)` | Uses cuBLAS internally |\n",
    "| `np.fft.fft(x)` | `cp.fft.fft(x)` | Uses cuFFT internally |\n",
    "| `np.linalg.svd(A)` | `cp.linalg.svd(A)` | Uses cuSOLVER internally |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 0: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from typing import Tuple, Callable\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import CuPy\n",
    "try:\n",
    "    import cupy as cp\n",
    "    HAS_CUPY = True\n",
    "    print(f\"‚úÖ CuPy {cp.__version__} available\")\n",
    "    print(f\"   CUDA version: {cp.cuda.runtime.runtimeGetVersion()}\")\n",
    "    \n",
    "    # Get GPU info\n",
    "    device = cp.cuda.Device(0)\n",
    "    mem_info = device.mem_info\n",
    "    print(f\"   Device: {cp.cuda.runtime.getDeviceProperties(0)['name'].decode()}\")\n",
    "    print(f\"   Free memory: {mem_info[0] / 1024**3:.1f} GB / {mem_info[1] / 1024**3:.1f} GB\")\n",
    "except ImportError:\n",
    "    HAS_CUPY = False\n",
    "    print(\"‚ùå CuPy not available\")\n",
    "    print(\"   Install with: pip install cupy-cuda12x\")\n",
    "    print(\"   Or in NGC container: already included!\")\n",
    "\n",
    "# PyTorch for interop demo\n",
    "try:\n",
    "    import torch\n",
    "    HAS_TORCH = True\n",
    "    print(f\"\\n‚úÖ PyTorch {torch.__version__} available\")\n",
    "except ImportError:\n",
    "    HAS_TORCH = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: CuPy Basics - Drop-in NumPy Replacement\n",
    "\n",
    "Let's start with simple operations to see how CuPy mirrors NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_CUPY:\n",
    "    print(\"üîß Basic CuPy Operations\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Creating arrays\n",
    "    a_np = np.array([1, 2, 3, 4, 5], dtype=np.float32)\n",
    "    a_cp = cp.array([1, 2, 3, 4, 5], dtype=cp.float32)  # Same syntax!\n",
    "    \n",
    "    print(f\"NumPy array: {a_np}, type: {type(a_np)}\")\n",
    "    print(f\"CuPy array:  {a_cp}, type: {type(a_cp)}\")\n",
    "    \n",
    "    # Array operations\n",
    "    print(f\"\\nüìä Element-wise operations:\")\n",
    "    print(f\"np.sqrt(a): {np.sqrt(a_np)}\")\n",
    "    print(f\"cp.sqrt(a): {cp.sqrt(a_cp)}\")\n",
    "    \n",
    "    # Reductions\n",
    "    print(f\"\\nüìä Reductions:\")\n",
    "    print(f\"np.sum(a): {np.sum(a_np)}\")\n",
    "    print(f\"cp.sum(a): {cp.sum(a_cp)}\")\n",
    "    \n",
    "    # Matrix operations\n",
    "    print(f\"\\nüìä Matrix operations:\")\n",
    "    A_np = np.random.randn(3, 3).astype(np.float32)\n",
    "    A_cp = cp.asarray(A_np)  # Convert NumPy to CuPy\n",
    "    \n",
    "    print(f\"NumPy matrix multiply: {np.dot(A_np, A_np)[0]}\")\n",
    "    print(f\"CuPy matrix multiply:  {cp.dot(A_cp, A_cp)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Between NumPy and CuPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_CUPY:\n",
    "    print(\"üîÑ Data Transfer Between CPU and GPU\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # NumPy ‚Üí CuPy (CPU ‚Üí GPU)\n",
    "    cpu_array = np.random.randn(1000, 1000).astype(np.float32)\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    gpu_array = cp.asarray(cpu_array)  # Copy to GPU\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    time_to_gpu = time.perf_counter() - start\n",
    "    \n",
    "    print(f\"CPU ‚Üí GPU transfer ({cpu_array.nbytes / 1e6:.1f} MB): {time_to_gpu*1000:.2f} ms\")\n",
    "    print(f\"Effective bandwidth: {cpu_array.nbytes / time_to_gpu / 1e9:.1f} GB/s\")\n",
    "    \n",
    "    # CuPy ‚Üí NumPy (GPU ‚Üí CPU)\n",
    "    start = time.perf_counter()\n",
    "    cpu_array_back = cp.asnumpy(gpu_array)  # Copy to CPU\n",
    "    time_to_cpu = time.perf_counter() - start\n",
    "    \n",
    "    print(f\"GPU ‚Üí CPU transfer: {time_to_cpu*1000:.2f} ms\")\n",
    "    \n",
    "    # Alternative: .get() method\n",
    "    cpu_array_back2 = gpu_array.get()  # Same as cp.asnumpy()\n",
    "    \n",
    "    print(f\"\\nüí° On DGX Spark with unified memory, these transfers are faster\")\n",
    "    print(f\"   than discrete GPUs because CPU and GPU share the same memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Benchmarking CuPy vs NumPy\n",
    "\n",
    "Let's see the real speedups on common operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(name: str, numpy_func: Callable, cupy_func: Callable, \n",
    "              np_args: tuple, cp_args: tuple, iterations: int = 10):\n",
    "    \"\"\"Benchmark NumPy vs CuPy function.\"\"\"\n",
    "    # Warm up\n",
    "    _ = numpy_func(*np_args)\n",
    "    _ = cupy_func(*cp_args)\n",
    "    if HAS_CUPY:\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "    \n",
    "    # NumPy timing\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        _ = numpy_func(*np_args)\n",
    "    time_np = (time.perf_counter() - start) / iterations\n",
    "    \n",
    "    # CuPy timing\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        _ = cupy_func(*cp_args)\n",
    "    if HAS_CUPY:\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "    time_cp = (time.perf_counter() - start) / iterations\n",
    "    \n",
    "    speedup = time_np / time_cp\n",
    "    print(f\"{name:<30} NumPy: {time_np*1000:>8.2f}ms  CuPy: {time_cp*1000:>8.2f}ms  Speedup: {speedup:>6.1f}x\")\n",
    "    return speedup\n",
    "\n",
    "\n",
    "if HAS_CUPY:\n",
    "    print(\"üìä CuPy vs NumPy Benchmark\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Test arrays\n",
    "    N = 5000\n",
    "    M = 5000\n",
    "    \n",
    "    A_np = np.random.randn(N, M).astype(np.float32)\n",
    "    B_np = np.random.randn(N, M).astype(np.float32)\n",
    "    x_np = np.random.randn(M).astype(np.float32)\n",
    "    \n",
    "    A_cp = cp.asarray(A_np)\n",
    "    B_cp = cp.asarray(B_np)\n",
    "    x_cp = cp.asarray(x_np)\n",
    "    \n",
    "    print(f\"\\nArray size: {N}√ó{M} ({N*M*4/1e6:.0f} MB per array)\\n\")\n",
    "    \n",
    "    # Element-wise operations\n",
    "    benchmark(\"Element-wise multiply\", \n",
    "              lambda a, b: a * b, lambda a, b: a * b,\n",
    "              (A_np, B_np), (A_cp, B_cp))\n",
    "    \n",
    "    benchmark(\"Element-wise exp\",\n",
    "              lambda a: np.exp(a), lambda a: cp.exp(a),\n",
    "              (A_np,), (A_cp,))\n",
    "    \n",
    "    benchmark(\"Element-wise sin\",\n",
    "              lambda a: np.sin(a), lambda a: cp.sin(a),\n",
    "              (A_np,), (A_cp,))\n",
    "    \n",
    "    # Reductions\n",
    "    benchmark(\"Sum (all elements)\",\n",
    "              lambda a: np.sum(a), lambda a: cp.sum(a),\n",
    "              (A_np,), (A_cp,))\n",
    "    \n",
    "    benchmark(\"Sum (along axis)\",\n",
    "              lambda a: np.sum(a, axis=1), lambda a: cp.sum(a, axis=1),\n",
    "              (A_np,), (A_cp,))\n",
    "    \n",
    "    benchmark(\"Mean\",\n",
    "              lambda a: np.mean(a), lambda a: cp.mean(a),\n",
    "              (A_np,), (A_cp,))\n",
    "    \n",
    "    benchmark(\"Standard deviation\",\n",
    "              lambda a: np.std(a), lambda a: cp.std(a),\n",
    "              (A_np,), (A_cp,))\n",
    "    \n",
    "    # Linear algebra\n",
    "    benchmark(\"Matrix multiply\",\n",
    "              lambda a, b: np.dot(a, b.T), lambda a, b: cp.dot(a, b.T),\n",
    "              (A_np, B_np), (A_cp, B_cp))\n",
    "    \n",
    "    benchmark(\"Matrix-vector multiply\",\n",
    "              lambda a, x: np.dot(a, x), lambda a, x: cp.dot(a, x),\n",
    "              (A_np, x_np), (A_cp, x_cp))\n",
    "    \n",
    "    # Smaller array for expensive operations\n",
    "    A_small_np = A_np[:1000, :1000].copy()\n",
    "    A_small_cp = cp.asarray(A_small_np)\n",
    "    \n",
    "    benchmark(\"SVD (1000√ó1000)\",\n",
    "              lambda a: np.linalg.svd(a), lambda a: cp.linalg.svd(a),\n",
    "              (A_small_np,), (A_small_cp,), iterations=3)\n",
    "    \n",
    "    benchmark(\"QR decomposition (1000√ó1000)\",\n",
    "              lambda a: np.linalg.qr(a), lambda a: cp.linalg.qr(a),\n",
    "              (A_small_np,), (A_small_cp,), iterations=5)\n",
    "    \n",
    "    # FFT\n",
    "    benchmark(\"2D FFT\",\n",
    "              lambda a: np.fft.fft2(a), lambda a: cp.fft.fft2(a),\n",
    "              (A_np,), (A_cp,))\n",
    "    \n",
    "    # Sorting\n",
    "    benchmark(\"Sort (flatten)\",\n",
    "              lambda a: np.sort(a.flatten()), lambda a: cp.sort(a.flatten()),\n",
    "              (A_np,), (A_cp,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Understanding the Results\n",
    "\n",
    "**Large speedups (10-100x):**\n",
    "- Matrix multiplication: Highly parallel, uses cuBLAS\n",
    "- FFT: Uses cuFFT, highly optimized\n",
    "- Element-wise operations: Embarrassingly parallel\n",
    "\n",
    "**Moderate speedups (2-10x):**\n",
    "- Reductions: Need to aggregate across threads\n",
    "- Sorting: Complex algorithms, but GPU helps\n",
    "\n",
    "**Similar or slower:**\n",
    "- Very small arrays: Transfer overhead dominates\n",
    "- Operations that don't parallelize well\n",
    "\n",
    "**Rule of thumb:** If your array has > 10,000 elements and the operation is parallelizable, CuPy will likely be faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Real-World Pipeline - Data Preprocessing\n",
    "\n",
    "Let's port a realistic data preprocessing pipeline to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic tabular dataset (like you might have in ML)\n",
    "np.random.seed(42)\n",
    "\n",
    "N_SAMPLES = 1_000_000  # 1 million rows\n",
    "N_FEATURES = 100       # 100 features\n",
    "\n",
    "print(f\"üìä Creating synthetic dataset: {N_SAMPLES:,} samples √ó {N_FEATURES} features\")\n",
    "print(f\"   Total size: {N_SAMPLES * N_FEATURES * 4 / 1e9:.2f} GB\")\n",
    "\n",
    "# Raw data with some realistic properties\n",
    "data_np = np.random.randn(N_SAMPLES, N_FEATURES).astype(np.float32)\n",
    "\n",
    "# Add some missing values (NaN)\n",
    "mask = np.random.random((N_SAMPLES, N_FEATURES)) < 0.05  # 5% missing\n",
    "data_np[mask] = np.nan\n",
    "\n",
    "# Add some outliers\n",
    "outlier_mask = np.random.random((N_SAMPLES, N_FEATURES)) < 0.01  # 1% outliers\n",
    "data_np[outlier_mask] = data_np[outlier_mask] * 10\n",
    "\n",
    "print(f\"   Missing values: {np.isnan(data_np).sum():,} ({100*np.isnan(data_np).mean():.1f}%)\")\n",
    "print(f\"   Memory: {data_np.nbytes / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_numpy(data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    NumPy preprocessing pipeline.\n",
    "    \n",
    "    Steps:\n",
    "    1. Fill NaN with column median\n",
    "    2. Clip outliers (outside 3 std devs)\n",
    "    3. Standardize (z-score normalization)\n",
    "    4. Apply tanh transformation (bounded output)\n",
    "    \"\"\"\n",
    "    result = data.copy()\n",
    "    \n",
    "    # Step 1: Fill NaN with column median\n",
    "    for col in range(result.shape[1]):\n",
    "        col_data = result[:, col]\n",
    "        median = np.nanmedian(col_data)\n",
    "        col_data[np.isnan(col_data)] = median\n",
    "    \n",
    "    # Step 2: Clip outliers\n",
    "    mean = np.mean(result, axis=0, keepdims=True)\n",
    "    std = np.std(result, axis=0, keepdims=True)\n",
    "    lower = mean - 3 * std\n",
    "    upper = mean + 3 * std\n",
    "    result = np.clip(result, lower, upper)\n",
    "    \n",
    "    # Step 3: Standardize (recompute after clipping)\n",
    "    mean = np.mean(result, axis=0, keepdims=True)\n",
    "    std = np.std(result, axis=0, keepdims=True)\n",
    "    result = (result - mean) / (std + 1e-8)\n",
    "    \n",
    "    # Step 4: Apply tanh\n",
    "    result = np.tanh(result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Time NumPy version\n",
    "print(\"\\n‚è±Ô∏è  NumPy Preprocessing Pipeline\")\n",
    "start = time.perf_counter()\n",
    "result_np = preprocess_numpy(data_np)\n",
    "time_numpy = time.perf_counter() - start\n",
    "print(f\"   Time: {time_numpy:.2f} seconds\")\n",
    "print(f\"   Throughput: {N_SAMPLES * N_FEATURES / time_numpy / 1e6:.1f} million elements/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_CUPY:\n",
    "    def preprocess_cupy(data: cp.ndarray) -> cp.ndarray:\n",
    "        \"\"\"\n",
    "        CuPy preprocessing pipeline - same logic, different library!\n",
    "        \"\"\"\n",
    "        result = data.copy()\n",
    "        \n",
    "        # Step 1: Fill NaN with column median\n",
    "        # CuPy's nanmedian works the same way\n",
    "        for col in range(result.shape[1]):\n",
    "            col_data = result[:, col]\n",
    "            median = cp.nanmedian(col_data)\n",
    "            col_data[cp.isnan(col_data)] = median\n",
    "        \n",
    "        # Step 2: Clip outliers\n",
    "        mean = cp.mean(result, axis=0, keepdims=True)\n",
    "        std = cp.std(result, axis=0, keepdims=True)\n",
    "        lower = mean - 3 * std\n",
    "        upper = mean + 3 * std\n",
    "        result = cp.clip(result, lower, upper)\n",
    "        \n",
    "        # Step 3: Standardize\n",
    "        mean = cp.mean(result, axis=0, keepdims=True)\n",
    "        std = cp.std(result, axis=0, keepdims=True)\n",
    "        result = (result - mean) / (std + 1e-8)\n",
    "        \n",
    "        # Step 4: Apply tanh\n",
    "        result = cp.tanh(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    \n",
    "    # Transfer to GPU and time\n",
    "    print(\"\\n‚è±Ô∏è  CuPy Preprocessing Pipeline\")\n",
    "    \n",
    "    # Transfer\n",
    "    start = time.perf_counter()\n",
    "    data_cp = cp.asarray(data_np)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    time_transfer = time.perf_counter() - start\n",
    "    print(f\"   CPU‚ÜíGPU transfer: {time_transfer:.3f} seconds\")\n",
    "    \n",
    "    # Warm up\n",
    "    _ = preprocess_cupy(data_cp[:1000])\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    \n",
    "    # Time CuPy version\n",
    "    start = time.perf_counter()\n",
    "    result_cp = preprocess_cupy(data_cp)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    time_cupy = time.perf_counter() - start\n",
    "    \n",
    "    print(f\"   Processing time: {time_cupy:.3f} seconds\")\n",
    "    print(f\"   Throughput: {N_SAMPLES * N_FEATURES / time_cupy / 1e6:.1f} million elements/sec\")\n",
    "    \n",
    "    # Transfer back\n",
    "    start = time.perf_counter()\n",
    "    result_cp_np = cp.asnumpy(result_cp)\n",
    "    time_back = time.perf_counter() - start\n",
    "    print(f\"   GPU‚ÜíCPU transfer: {time_back:.3f} seconds\")\n",
    "    \n",
    "    # Verify correctness\n",
    "    print(f\"\\n‚úÖ Results match: {np.allclose(result_np, result_cp_np, rtol=1e-4, equal_nan=True)}\")\n",
    "    \n",
    "    # Summary\n",
    "    total_cupy = time_transfer + time_cupy + time_back\n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"   NumPy:      {time_numpy:.2f} seconds (total)\")\n",
    "    print(f\"   CuPy:       {time_cupy:.3f} seconds (processing only)\")\n",
    "    print(f\"   CuPy total: {total_cupy:.3f} seconds (with transfers)\")\n",
    "    print(f\"\\n   üöÄ Speedup (processing): {time_numpy/time_cupy:.1f}x\")\n",
    "    print(f\"   üöÄ Speedup (total):      {time_numpy/total_cupy:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Key Observations\n",
    "\n",
    "1. **Same code structure** - Just changed `np` to `cp`\n",
    "2. **Transfer overhead matters** - If you're only processing once, transfers add up\n",
    "3. **Keep data on GPU** - In real pipelines, data stays on GPU for multiple operations\n",
    "4. **The loop is slow** - Even with CuPy, Python loops are bottlenecks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Optimizing the Pipeline (Vectorization)\n",
    "\n",
    "The loop in our preprocessing is slow. Let's vectorize it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_CUPY:\n",
    "    def preprocess_cupy_optimized(data: cp.ndarray) -> cp.ndarray:\n",
    "        \"\"\"\n",
    "        Optimized CuPy preprocessing - no Python loops!\n",
    "        \"\"\"\n",
    "        result = data.copy()\n",
    "        \n",
    "        # Step 1: Fill NaN with column median - VECTORIZED\n",
    "        # Compute medians for all columns at once\n",
    "        medians = cp.nanmedian(result, axis=0)  # Shape: (N_FEATURES,)\n",
    "        # Create mask of NaN positions\n",
    "        nan_mask = cp.isnan(result)\n",
    "        # Use broadcasting to fill NaN with corresponding column median\n",
    "        # This is a bit tricky: we need to get the column index for each NaN\n",
    "        nan_rows, nan_cols = cp.where(nan_mask)\n",
    "        result[nan_rows, nan_cols] = medians[nan_cols]\n",
    "        \n",
    "        # Steps 2-4: Already vectorized, same as before\n",
    "        mean = cp.mean(result, axis=0, keepdims=True)\n",
    "        std = cp.std(result, axis=0, keepdims=True)\n",
    "        lower = mean - 3 * std\n",
    "        upper = mean + 3 * std\n",
    "        result = cp.clip(result, lower, upper)\n",
    "        \n",
    "        mean = cp.mean(result, axis=0, keepdims=True)\n",
    "        std = cp.std(result, axis=0, keepdims=True)\n",
    "        result = (result - mean) / (std + 1e-8)\n",
    "        \n",
    "        result = cp.tanh(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    \n",
    "    # Benchmark optimized version\n",
    "    print(\"\\n‚è±Ô∏è  Optimized CuPy Pipeline (no Python loops)\")\n",
    "    \n",
    "    # Warm up\n",
    "    _ = preprocess_cupy_optimized(data_cp[:1000])\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    result_opt = preprocess_cupy_optimized(data_cp)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    time_optimized = time.perf_counter() - start\n",
    "    \n",
    "    print(f\"   Processing time: {time_optimized:.3f} seconds\")\n",
    "    print(f\"   Throughput: {N_SAMPLES * N_FEATURES / time_optimized / 1e6:.1f} million elements/sec\")\n",
    "    \n",
    "    # Verify\n",
    "    result_opt_np = cp.asnumpy(result_opt)\n",
    "    print(f\"\\n‚úÖ Results match original: {np.allclose(result_np, result_opt_np, rtol=1e-4, equal_nan=True)}\")\n",
    "    \n",
    "    print(f\"\\nüìä Comparison:\")\n",
    "    print(f\"   NumPy:            {time_numpy:.2f} seconds\")\n",
    "    print(f\"   CuPy (with loop): {time_cupy:.3f} seconds\")\n",
    "    print(f\"   CuPy (optimized): {time_optimized:.3f} seconds\")\n",
    "    print(f\"\\n   üöÄ Total speedup: {time_numpy/time_optimized:.1f}x faster than NumPy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: CuPy Custom Kernels (RawKernel)\n",
    "\n",
    "Sometimes CuPy's built-in functions aren't enough. You can write raw CUDA C!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_CUPY:\n",
    "    # Custom CUDA kernel using CuPy's RawKernel\n",
    "    fused_normalize_tanh_kernel = cp.RawKernel(r'''\n",
    "    extern \"C\" __global__\n",
    "    void fused_normalize_tanh(\n",
    "        const float* input,\n",
    "        const float* mean,\n",
    "        const float* std,\n",
    "        float* output,\n",
    "        int n_samples,\n",
    "        int n_features\n",
    "    ) {\n",
    "        // Each thread handles one element\n",
    "        int idx = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "        int total = n_samples * n_features;\n",
    "        \n",
    "        if (idx < total) {\n",
    "            int col = idx % n_features;  // Which feature\n",
    "            \n",
    "            // Normalize\n",
    "            float normalized = (input[idx] - mean[col]) / (std[col] + 1e-8f);\n",
    "            \n",
    "            // Tanh\n",
    "            output[idx] = tanhf(normalized);\n",
    "        }\n",
    "    }\n",
    "    ''', 'fused_normalize_tanh')\n",
    "    \n",
    "    \n",
    "    def preprocess_cupy_custom(data: cp.ndarray) -> cp.ndarray:\n",
    "        \"\"\"\n",
    "        Preprocessing with custom fused kernel for normalize+tanh.\n",
    "        \"\"\"\n",
    "        result = data.copy()\n",
    "        n_samples, n_features = result.shape\n",
    "        \n",
    "        # Step 1: Fill NaN (same as before)\n",
    "        medians = cp.nanmedian(result, axis=0)\n",
    "        nan_mask = cp.isnan(result)\n",
    "        nan_rows, nan_cols = cp.where(nan_mask)\n",
    "        result[nan_rows, nan_cols] = medians[nan_cols]\n",
    "        \n",
    "        # Step 2: Clip (same as before)\n",
    "        mean = cp.mean(result, axis=0)\n",
    "        std = cp.std(result, axis=0)\n",
    "        lower = mean - 3 * std\n",
    "        upper = mean + 3 * std\n",
    "        result = cp.clip(result, lower.reshape(1, -1), upper.reshape(1, -1))\n",
    "        \n",
    "        # Steps 3+4: Fused normalize + tanh with custom kernel\n",
    "        mean = cp.mean(result, axis=0).astype(cp.float32)\n",
    "        std = cp.std(result, axis=0).astype(cp.float32)\n",
    "        \n",
    "        # Make result contiguous for kernel\n",
    "        result = cp.ascontiguousarray(result.astype(cp.float32))\n",
    "        output = cp.empty_like(result)\n",
    "        \n",
    "        # Launch kernel\n",
    "        total_elements = n_samples * n_features\n",
    "        threads_per_block = 256\n",
    "        blocks = (total_elements + threads_per_block - 1) // threads_per_block\n",
    "        \n",
    "        fused_normalize_tanh_kernel(\n",
    "            (blocks,), (threads_per_block,),\n",
    "            (result, mean, std, output, n_samples, n_features)\n",
    "        )\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    # Benchmark custom kernel version\n",
    "    print(\"\\n‚è±Ô∏è  CuPy with Custom Fused Kernel\")\n",
    "    \n",
    "    # Warm up\n",
    "    _ = preprocess_cupy_custom(data_cp[:1000])\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    result_custom = preprocess_cupy_custom(data_cp)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    time_custom = time.perf_counter() - start\n",
    "    \n",
    "    print(f\"   Processing time: {time_custom:.3f} seconds\")\n",
    "    \n",
    "    # Verify\n",
    "    result_custom_np = cp.asnumpy(result_custom)\n",
    "    print(f\"\\n‚úÖ Results match: {np.allclose(result_np, result_custom_np, rtol=1e-3)}\")\n",
    "    \n",
    "    print(f\"\\nüìä All versions compared:\")\n",
    "    print(f\"   NumPy:              {time_numpy:.2f}s\")\n",
    "    print(f\"   CuPy (basic):       {time_cupy:.3f}s  ({time_numpy/time_cupy:.0f}x)\")\n",
    "    print(f\"   CuPy (optimized):   {time_optimized:.3f}s  ({time_numpy/time_optimized:.0f}x)\")\n",
    "    print(f\"   CuPy (custom):      {time_custom:.3f}s  ({time_numpy/time_custom:.0f}x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: CuPy ‚Üî PyTorch Interoperability\n",
    "\n",
    "One of CuPy's superpowers: zero-copy sharing with PyTorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_CUPY and HAS_TORCH:\n",
    "    print(\"üîÑ CuPy ‚Üî PyTorch Zero-Copy Interop\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create CuPy array\n",
    "    cp_array = cp.random.randn(1000, 1000).astype(cp.float32)\n",
    "    print(f\"CuPy array: {cp_array.shape}, device: GPU\")\n",
    "    \n",
    "    # Method 1: Using DLPack (recommended, zero-copy)\n",
    "    torch_tensor = torch.from_dlpack(cp_array.toDlpack())\n",
    "    print(f\"PyTorch tensor: {torch_tensor.shape}, device: {torch_tensor.device}\")\n",
    "    \n",
    "    # Verify they share memory (modifying one affects the other)\n",
    "    cp_array[0, 0] = 999.0\n",
    "    print(f\"\\nAfter setting cp_array[0,0] = 999:\")\n",
    "    print(f\"  torch_tensor[0,0] = {torch_tensor[0,0].item()}\")\n",
    "    print(f\"  ‚úÖ Zero-copy: same memory!\")\n",
    "    \n",
    "    # Method 2: PyTorch ‚Üí CuPy\n",
    "    torch_tensor2 = torch.randn(500, 500, device='cuda')\n",
    "    cp_array2 = cp.from_dlpack(torch_tensor2)\n",
    "    print(f\"\\nPyTorch ‚Üí CuPy: {cp_array2.shape}\")\n",
    "    \n",
    "    # Use case: Preprocess with CuPy, train with PyTorch\n",
    "    print(\"\\nüí° Typical workflow:\")\n",
    "    print(\"   1. Load data with CuPy/NumPy\")\n",
    "    print(\"   2. Preprocess with CuPy (fast!)\")\n",
    "    print(\"   3. Zero-copy convert to PyTorch\")\n",
    "    print(\"   4. Train model with PyTorch\")\n",
    "    print(\"   5. No memory copies = maximum performance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_CUPY and HAS_TORCH:\n",
    "    # Practical example: Preprocess ‚Üí Train pipeline\n",
    "    print(\"\\nüìä End-to-End Pipeline Demo\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Simulate raw data arriving\n",
    "    raw_data = np.random.randn(10000, 100).astype(np.float32)\n",
    "    \n",
    "    # Transfer to GPU with CuPy\n",
    "    cp_data = cp.asarray(raw_data)\n",
    "    \n",
    "    # Preprocess with CuPy\n",
    "    mean = cp.mean(cp_data, axis=0, keepdims=True)\n",
    "    std = cp.std(cp_data, axis=0, keepdims=True)\n",
    "    preprocessed = (cp_data - mean) / (std + 1e-8)\n",
    "    \n",
    "    # Zero-copy to PyTorch\n",
    "    torch_data = torch.from_dlpack(preprocessed.toDlpack())\n",
    "    \n",
    "    # Now use in PyTorch model\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(100, 50),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(50, 10)\n",
    "    ).cuda()\n",
    "    \n",
    "    output = model(torch_data)\n",
    "    print(f\"Input shape: {torch_data.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"\\n‚úÖ Seamless CuPy ‚Üí PyTorch pipeline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting to Synchronize for Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_CUPY:\n",
    "    # ‚ùå WRONG: Not synchronizing\n",
    "    # start = time.time()\n",
    "    # result = cp.sum(large_array)  # Async!\n",
    "    # elapsed = time.time() - start  # Measures launch time, not execution!\n",
    "    \n",
    "    # ‚úÖ CORRECT: Synchronize before timing\n",
    "    # start = time.time()\n",
    "    # result = cp.sum(large_array)\n",
    "    # cp.cuda.Stream.null.synchronize()  # Wait for GPU!\n",
    "    # elapsed = time.time() - start\n",
    "    \n",
    "    print(\"üí° CuPy operations are asynchronous!\")\n",
    "    print(\"   Always use cp.cuda.Stream.null.synchronize() before timing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Mixing NumPy and CuPy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_CUPY:\n",
    "    # ‚ùå WRONG: Mixing array types\n",
    "    # a_np = np.array([1, 2, 3])\n",
    "    # b_cp = cp.array([4, 5, 6])\n",
    "    # c = a_np + b_cp  # TypeError or implicit slow transfer!\n",
    "    \n",
    "    # ‚úÖ CORRECT: Ensure same array type\n",
    "    # a_cp = cp.array([1, 2, 3])\n",
    "    # b_cp = cp.array([4, 5, 6])\n",
    "    # c = a_cp + b_cp  # Both on GPU\n",
    "    \n",
    "    print(\"üí° Never mix NumPy and CuPy arrays in operations!\")\n",
    "    print(\"   Convert explicitly: cp.asarray(np_array) or cp.asnumpy(cp_array)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Not Managing Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_CUPY:\n",
    "    print(\"üí° Memory management tips:\")\n",
    "    print()\n",
    "    print(\"   1. Clear unused arrays:\")\n",
    "    print(\"      del large_array\")\n",
    "    print(\"      cp.get_default_memory_pool().free_all_blocks()\")\n",
    "    print()\n",
    "    print(\"   2. Check memory usage:\")\n",
    "    print(\"      mempool = cp.get_default_memory_pool()\")\n",
    "    print(\"      print(f'Used: {mempool.used_bytes() / 1e9:.2f} GB')\")\n",
    "    print()\n",
    "    print(\"   3. Set memory limit:\")\n",
    "    print(\"      cp.cuda.set_allocator(cp.cuda.MemoryPool(cp.cuda.malloc_managed).malloc)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself: Image Preprocessing Pipeline\n",
    "\n",
    "**Challenge:** Port this image preprocessing pipeline to CuPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images_numpy(images: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocess batch of images.\n",
    "    \n",
    "    Input: (batch, height, width, channels) uint8 [0-255]\n",
    "    Output: (batch, channels, height, width) float32 [-1, 1]\n",
    "    \n",
    "    Steps:\n",
    "    1. Convert to float32 and normalize to [0, 1]\n",
    "    2. Apply per-channel normalization (ImageNet mean/std)\n",
    "    3. Transpose to (batch, channels, height, width)\n",
    "    4. Random horizontal flip (50% chance per image)\n",
    "    \"\"\"\n",
    "    # ImageNet normalization constants\n",
    "    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "    std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "    \n",
    "    # Step 1: Normalize to [0, 1]\n",
    "    result = images.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Step 2: Per-channel normalization\n",
    "    result = (result - mean) / std\n",
    "    \n",
    "    # Step 3: Transpose NHWC -> NCHW\n",
    "    result = np.transpose(result, (0, 3, 1, 2))\n",
    "    \n",
    "    # Step 4: Random horizontal flip\n",
    "    flip_mask = np.random.random(result.shape[0]) < 0.5\n",
    "    result[flip_mask] = result[flip_mask, :, :, ::-1]\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# TODO: Implement CuPy version\n",
    "def preprocess_images_cupy(images):\n",
    "    \"\"\"\n",
    "    CuPy version of image preprocessing.\n",
    "    \n",
    "    Hint: The code is almost identical! Just change np ‚Üí cp.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test data\n",
    "batch_size = 64\n",
    "height, width = 224, 224\n",
    "channels = 3\n",
    "\n",
    "images = np.random.randint(0, 256, (batch_size, height, width, channels), dtype=np.uint8)\n",
    "\n",
    "# Test NumPy version\n",
    "np.random.seed(42)\n",
    "result_np = preprocess_images_numpy(images)\n",
    "print(f\"NumPy result shape: {result_np.shape}\")\n",
    "\n",
    "# When implemented:\n",
    "# np.random.seed(42)\n",
    "# result_cp = preprocess_images_cupy(cp.asarray(images))\n",
    "# print(f\"CuPy result shape: {result_cp.shape}\")\n",
    "# print(f\"Results match: {np.allclose(result_np, cp.asnumpy(result_cp))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Solution</summary>\n",
    "\n",
    "```python\n",
    "def preprocess_images_cupy(images):\n",
    "    mean = cp.array([0.485, 0.456, 0.406], dtype=cp.float32)\n",
    "    std = cp.array([0.229, 0.224, 0.225], dtype=cp.float32)\n",
    "    \n",
    "    result = images.astype(cp.float32) / 255.0\n",
    "    result = (result - mean) / std\n",
    "    result = cp.transpose(result, (0, 3, 1, 2))\n",
    "    \n",
    "    flip_mask = cp.random.random(result.shape[0]) < 0.5\n",
    "    result[flip_mask] = result[flip_mask, :, :, ::-1]\n",
    "    \n",
    "    return result\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "Congratulations! You've learned:\n",
    "\n",
    "- ‚úÖ **CuPy basics** - Drop-in NumPy replacement\n",
    "- ‚úÖ **Performance characteristics** - When CuPy shines\n",
    "- ‚úÖ **Vectorization** - Avoiding Python loops\n",
    "- ‚úÖ **Custom kernels** - RawKernel for specialized ops\n",
    "- ‚úÖ **PyTorch interop** - Zero-copy data sharing\n",
    "\n",
    "You achieved **10x+ speedup** on a realistic preprocessing pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [CuPy Documentation](https://docs.cupy.dev/en/stable/)\n",
    "- [CuPy User Guide](https://docs.cupy.dev/en/stable/user_guide/index.html)\n",
    "- [CuPy Kernel Fusion](https://docs.cupy.dev/en/stable/user_guide/kernel.html)\n",
    "- [RAPIDS cuDF](https://github.com/rapidsai/cudf) - GPU-accelerated pandas\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Clean up\n",
    "if HAS_CUPY:\n",
    "    # Clear CuPy memory pool\n",
    "    mempool = cp.get_default_memory_pool()\n",
    "    pinned_mempool = cp.get_default_pinned_memory_pool()\n",
    "    \n",
    "    mempool.free_all_blocks()\n",
    "    pinned_mempool.free_all_blocks()\n",
    "\n",
    "if HAS_TORCH:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ GPU memory cleared!\")\n",
    "print(\"\\n‚û°Ô∏è Ready for Lab 1.3.5: Profiling Workshop\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
