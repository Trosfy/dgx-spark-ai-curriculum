{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.3.1: Parallel Reduction\n",
    "\n",
    "**Module:** 1.3 - CUDA Python & GPU Programming  \n",
    "**Time:** 3 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand why parallel reduction is fundamental to GPU computing\n",
    "- [ ] Implement naive parallel reduction and understand its limitations\n",
    "- [ ] Optimize with shared memory for 10-50x speedup\n",
    "- [ ] Master warp shuffle operations for ultimate performance\n",
    "- [ ] Achieve **100x+ speedup** over CPU implementations\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Module 1.2 (Python for AI/ML)\n",
    "- Knowledge of: Basic NumPy operations, understanding of what a GPU is\n",
    "- Optional: Read the README.md for this module\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**Why does parallel reduction matter?**\n",
    "\n",
    "When training neural networks, you constantly need to:\n",
    "- **Sum gradients** across millions of parameters\n",
    "- **Compute loss** (average over batch)\n",
    "- **Find max values** (for softmax normalization)\n",
    "- **Calculate statistics** (mean, variance for batch normalization)\n",
    "\n",
    "A typical LLM like Llama-70B has **70 billion parameters**. If each reduction operation took even 1 microsecond per parameter on CPU, you'd wait **70 seconds per batch**. With GPU parallel reduction? Under **1 millisecond**.\n",
    "\n",
    "**Your DGX Spark has 6,144 CUDA cores** ready to reduce arrays in parallel. Let's learn how to use them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßí ELI5: What is Parallel Reduction?\n",
    "\n",
    "> **Imagine you're a teacher** with 32 students, and you need to count the total number of candies everyone has.\n",
    ">\n",
    "> **The slow way (CPU):** You go to each student one by one: \"1 candy... 3 candies... 2 candies...\" adding as you go. With 32 students, you need 31 addition steps. With 1 million students? 999,999 steps!\n",
    ">\n",
    "> **The fast way (GPU parallel reduction):**\n",
    "> 1. **Round 1:** Students pair up (16 pairs). Each pair adds their candies together. Now we have 16 numbers.\n",
    "> 2. **Round 2:** The 16 people pair up (8 pairs). Add again. Now 8 numbers.\n",
    "> 3. **Round 3:** 8 ‚Üí 4 numbers\n",
    "> 4. **Round 4:** 4 ‚Üí 2 numbers\n",
    "> 5. **Round 5:** 2 ‚Üí 1 number (the answer!)\n",
    ">\n",
    "> With 32 students, you only need **5 rounds** (log‚ÇÇ(32) = 5), and in each round, all pairs work **simultaneously**!\n",
    ">\n",
    "> **In AI terms:** The GPU does thousands of additions at the same time, turning millions of sequential operations into just ~20 parallel rounds. That's how we get 100x+ speedups!\n",
    "\n",
    "![Parallel Reduction Diagram](https://upload.wikimedia.org/wikipedia/commons/4/4c/Parallel_reduction.png)\n",
    "*Each level shows one parallel step. All additions in the same level happen simultaneously.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 0: Environment Setup\n",
    "\n",
    "First, let's verify our CUDA environment and understand our hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Tuple, Callable\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# CUDA imports\n",
    "from numba import cuda\n",
    "import numba\n",
    "\n",
    "# Verify CUDA is available\n",
    "print(\"=\"*60)\n",
    "print(\"üîß CUDA Environment Check\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Numba version: {numba.__version__}\")\n",
    "print(f\"CUDA available: {cuda.is_available()}\")\n",
    "\n",
    "if cuda.is_available():\n",
    "    device = cuda.get_current_device()\n",
    "    print(f\"\\nüìä GPU Information:\")\n",
    "    print(f\"  Device name: {device.name}\")\n",
    "    print(f\"  Compute capability: {device.compute_capability}\")\n",
    "    print(f\"  Max threads per block: {device.MAX_THREADS_PER_BLOCK}\")\n",
    "    print(f\"  Max shared memory per block: {device.MAX_SHARED_MEMORY_PER_BLOCK / 1024:.1f} KB\")\n",
    "    print(f\"  Warp size: {device.WARP_SIZE}\")\n",
    "    print(f\"  Multiprocessors: {device.MULTIPROCESSOR_COUNT}\")\n",
    "    print(\"\\n‚úÖ Ready for GPU programming!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå CUDA not available. Please use NGC container.\")\n",
    "    print(\"   Run: docker run --gpus all -it nvcr.io/nvidia/pytorch:25.11-py3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Understanding the Output\n",
    "\n",
    "On your DGX Spark, you should see:\n",
    "- **Device name:** Something like \"NVIDIA GB10 Superchip\" or similar\n",
    "- **Compute capability:** 10.0 or higher (Blackwell architecture)\n",
    "- **Max threads per block:** 1024 (standard for modern GPUs)\n",
    "- **Warp size:** 32 (always 32 on NVIDIA GPUs - this is important!)\n",
    "- **Multiprocessors:** The number of SMs (Streaming Multiprocessors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: CPU Baseline - The Slow Way\n",
    "\n",
    "Before we make things fast, let's see how slow the CPU approach is. This gives us a baseline to measure our GPU speedups against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_sum_naive(arr: np.ndarray) -> float:\n",
    "    \"\"\"Naive CPU sum using Python loop (very slow!).\"\"\"\n",
    "    total = 0.0\n",
    "    for x in arr:\n",
    "        total += x\n",
    "    return total\n",
    "\n",
    "def cpu_sum_numpy(arr: np.ndarray) -> float:\n",
    "    \"\"\"NumPy sum (optimized C, but still sequential).\"\"\"\n",
    "    return np.sum(arr)\n",
    "\n",
    "# Create test data - 10 million elements\n",
    "N = 10_000_000\n",
    "np.random.seed(42)  # For reproducibility\n",
    "data = np.random.randn(N).astype(np.float32)\n",
    "\n",
    "print(f\"üìä Array size: {N:,} elements ({N * 4 / 1e6:.1f} MB)\")\n",
    "print(f\"   Expected sum (numpy): {np.sum(data):.6f}\")\n",
    "print(\"\\n‚è±Ô∏è  Benchmarking CPU approaches...\\n\")\n",
    "\n",
    "# Benchmark Python loop (just 1M for speed)\n",
    "small_data = data[:1_000_000]\n",
    "start = time.perf_counter()\n",
    "result_naive = cpu_sum_naive(small_data)\n",
    "time_naive = time.perf_counter() - start\n",
    "print(f\"Python loop (1M elements): {time_naive*1000:.2f} ms\")\n",
    "\n",
    "# Benchmark NumPy (full 10M)\n",
    "start = time.perf_counter()\n",
    "for _ in range(10):  # Average over 10 runs\n",
    "    result_numpy = cpu_sum_numpy(data)\n",
    "time_numpy = (time.perf_counter() - start) / 10\n",
    "print(f\"NumPy sum (10M elements): {time_numpy*1000:.2f} ms\")\n",
    "\n",
    "print(f\"\\nüìà NumPy is ~{time_naive * 10 / time_numpy:.0f}x faster than Python loop\")\n",
    "print(\"\\nüí° Can we beat NumPy with the GPU? Let's find out!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "We established our baselines:\n",
    "- **Python loop:** Embarrassingly slow (~500ms for 1M elements)\n",
    "- **NumPy:** Much faster (~5ms for 10M) thanks to C optimization\n",
    "\n",
    "But NumPy still processes elements one at a time (or in small SIMD chunks). The GPU can do **thousands** simultaneously!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Your First CUDA Kernel - Naive Reduction\n",
    "\n",
    "### üßí ELI5: What's a Kernel?\n",
    "\n",
    "> Think of a **kernel** as a recipe that **thousands of chefs** (threads) follow simultaneously. Each chef works on a different ingredient (data element), but they all follow the same instructions.\n",
    "\n",
    "### Understanding CUDA Thread Hierarchy\n",
    "\n",
    "```\n",
    "Grid (all threads)\n",
    "‚îú‚îÄ‚îÄ Block 0\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Thread 0, 1, 2, ... 255 (256 threads)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ (These share \"shared memory\")\n",
    "‚îú‚îÄ‚îÄ Block 1\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Thread 0, 1, 2, ... 255\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ (Their own shared memory)\n",
    "‚îú‚îÄ‚îÄ Block 2\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ ... (many blocks)\n",
    "```\n",
    "\n",
    "Key concepts:\n",
    "- **Thread:** Single worker, has access to its own registers\n",
    "- **Block:** Group of threads (up to 1024), share fast \"shared memory\"\n",
    "- **Grid:** All blocks together, covers the entire problem\n",
    "- **Warp:** 32 threads that execute in lockstep (hardware unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's understand thread indexing first\n",
    "@cuda.jit\n",
    "def print_thread_info(output):\n",
    "    \"\"\"Kernel that shows how thread indexing works.\"\"\"\n",
    "    # Thread index within block\n",
    "    tx = cuda.threadIdx.x\n",
    "    # Block index within grid\n",
    "    bx = cuda.blockIdx.x\n",
    "    # Block dimension (number of threads per block)\n",
    "    bw = cuda.blockDim.x\n",
    "    \n",
    "    # Global thread index (unique across entire grid)\n",
    "    global_idx = bx * bw + tx\n",
    "    \n",
    "    # Store info in output array\n",
    "    if global_idx < output.shape[0]:\n",
    "        # Pack info: global_idx * 10000 + block_idx * 100 + thread_idx\n",
    "        output[global_idx] = global_idx * 10000 + bx * 100 + tx\n",
    "\n",
    "# Launch with 2 blocks of 4 threads each\n",
    "output = cuda.device_array(8, dtype=np.float32)\n",
    "print_thread_info[2, 4](output)  # [blocks, threads_per_block]\n",
    "result = output.copy_to_host()\n",
    "\n",
    "print(\"üîç Thread Indexing Demonstration\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Grid config: 2 blocks √ó 4 threads = 8 total threads\\n\")\n",
    "for i, val in enumerate(result):\n",
    "    global_idx = int(val) // 10000\n",
    "    block_idx = (int(val) % 10000) // 100\n",
    "    thread_idx = int(val) % 100\n",
    "    print(f\"Output[{i}]: Global={global_idx}, Block={block_idx}, ThreadInBlock={thread_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Let's Write Our First Reduction Kernel\n",
    "\n",
    "**Strategy:** Each block reduces its chunk of data to a single value. Then we sum the block results on CPU.\n",
    "\n",
    "```\n",
    "Array: [1, 2, 3, 4, 5, 6, 7, 8]  (8 elements)\n",
    "       \\       /   \\       /\n",
    "        Block 0     Block 1\n",
    "           ‚Üì           ‚Üì\n",
    "          10          26\n",
    "           \\         /\n",
    "            \\       /\n",
    "              36 (final sum, on CPU)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def naive_reduce_kernel(data, partial_sums):\n",
    "    \"\"\"\n",
    "    Naive parallel reduction - each block produces one partial sum.\n",
    "    \n",
    "    This version has problems (we'll fix them later!):\n",
    "    - Uses global memory (slow)\n",
    "    - Has thread divergence issues\n",
    "    \"\"\"\n",
    "    # Get thread and block indices\n",
    "    tx = cuda.threadIdx.x\n",
    "    bx = cuda.blockIdx.x\n",
    "    block_size = cuda.blockDim.x\n",
    "    \n",
    "    # Global index for this thread\n",
    "    idx = bx * block_size + tx\n",
    "    \n",
    "    # Bounds check\n",
    "    if idx >= data.size:\n",
    "        return\n",
    "    \n",
    "    # Parallel reduction within block\n",
    "    # Each iteration, half the threads add their neighbor's value\n",
    "    stride = 1\n",
    "    while stride < block_size:\n",
    "        # Only threads at positions 0, 2*stride, 4*stride, ... do work\n",
    "        if tx % (2 * stride) == 0:\n",
    "            if idx + stride < data.size:\n",
    "                data[idx] += data[idx + stride]\n",
    "        \n",
    "        stride *= 2\n",
    "        cuda.syncthreads()  # Wait for all threads in block\n",
    "    \n",
    "    # Thread 0 of each block writes the result\n",
    "    if tx == 0:\n",
    "        partial_sums[bx] = data[idx]\n",
    "\n",
    "\n",
    "def gpu_sum_naive(arr: np.ndarray) -> float:\n",
    "    \"\"\"Sum array using naive GPU reduction.\"\"\"\n",
    "    n = arr.size\n",
    "    threads_per_block = 256\n",
    "    num_blocks = (n + threads_per_block - 1) // threads_per_block\n",
    "    \n",
    "    # Copy data to GPU (we modify in place, so need a copy)\n",
    "    d_data = cuda.to_device(arr.copy())\n",
    "    d_partial = cuda.device_array(num_blocks, dtype=np.float32)\n",
    "    \n",
    "    # Launch kernel\n",
    "    naive_reduce_kernel[num_blocks, threads_per_block](d_data, d_partial)\n",
    "    \n",
    "    # Get partial sums and finish reduction on CPU\n",
    "    partial_sums = d_partial.copy_to_host()\n",
    "    return np.sum(partial_sums)\n",
    "\n",
    "\n",
    "# Test correctness first\n",
    "test_arr = np.arange(1, 101, dtype=np.float32)  # 1+2+...+100 = 5050\n",
    "result = gpu_sum_naive(test_arr)\n",
    "print(f\"Test: sum of 1 to 100\")\n",
    "print(f\"  Expected: 5050.0\")\n",
    "print(f\"  Got:      {result}\")\n",
    "print(f\"  Correct:  {'‚úÖ Yes!' if abs(result - 5050.0) < 0.01 else '‚ùå No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark naive GPU reduction\n",
    "print(\"\\n‚è±Ô∏è  Benchmarking Naive GPU Reduction...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Warm up GPU\n",
    "for _ in range(3):\n",
    "    _ = gpu_sum_naive(data)\n",
    "cuda.synchronize()  # Ensure GPU is done\n",
    "\n",
    "# Benchmark\n",
    "start = time.perf_counter()\n",
    "for _ in range(10):\n",
    "    result_gpu = gpu_sum_naive(data)\n",
    "    cuda.synchronize()\n",
    "time_naive_gpu = (time.perf_counter() - start) / 10\n",
    "\n",
    "print(f\"Array size: {N:,} elements\")\n",
    "print(f\"NumPy time:     {time_numpy*1000:.2f} ms\")\n",
    "print(f\"Naive GPU time: {time_naive_gpu*1000:.2f} ms\")\n",
    "print(f\"Speedup: {time_numpy/time_naive_gpu:.1f}x {'faster' if time_naive_gpu < time_numpy else 'slower'} than NumPy\")\n",
    "\n",
    "# Check result accuracy\n",
    "print(f\"\\nüéØ Accuracy check:\")\n",
    "print(f\"   NumPy result: {result_numpy:.6f}\")\n",
    "print(f\"   GPU result:   {result_gpu:.6f}\")\n",
    "print(f\"   Difference:   {abs(result_numpy - result_gpu):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "Hmm, our naive GPU version might actually be **slower** than NumPy! Why?\n",
    "\n",
    "**Problems with our naive implementation:**\n",
    "\n",
    "1. **Global Memory Access:** Reading/writing to `data[]` goes to slow global memory (273 GB/s sounds fast, but shared memory is 10x faster)\n",
    "\n",
    "2. **Thread Divergence:** The `if tx % (2 * stride) == 0` condition means threads in the same warp take different paths. GPUs execute all 32 threads in a warp together - divergent threads just wait!\n",
    "\n",
    "3. **Memory Copy Overhead:** We're copying data to GPU and back for each call\n",
    "\n",
    "Let's fix these issues!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Optimized Reduction with Shared Memory\n",
    "\n",
    "### üßí ELI5: What's Shared Memory?\n",
    "\n",
    "> Imagine a classroom (block) where students (threads) need to share answers.\n",
    ">\n",
    "> **Global memory** is like shouting across a huge stadium - everyone can hear, but it's slow and crowded.\n",
    ">\n",
    "> **Shared memory** is like a small whiteboard in the classroom - only your classmates can see it, but it's right there, super fast to read and write!\n",
    ">\n",
    "> **The trick:** Load data from the stadium (global) to the whiteboard (shared), do all your work on the whiteboard, then write the final answer back to the stadium.\n",
    "\n",
    "### Memory Hierarchy Speed Comparison\n",
    "\n",
    "| Memory Type | Speed | Size per SM | Access Scope |\n",
    "|-------------|-------|-------------|-------------|\n",
    "| Registers | ~1 cycle | 256 KB | Per thread |\n",
    "| Shared Memory | ~5 cycles | 164 KB | Per block |\n",
    "| L1 Cache | ~30 cycles | 128 KB | Per SM |\n",
    "| L2 Cache | ~100 cycles | 62 MB | All SMs |\n",
    "| Global Memory | ~500 cycles | 128 GB | All threads |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared memory size must be known at compile time\n",
    "THREADS_PER_BLOCK = 256\n",
    "\n",
    "@cuda.jit\n",
    "def shared_memory_reduce_kernel(data, partial_sums):\n",
    "    \"\"\"\n",
    "    Optimized reduction using shared memory.\n",
    "    \n",
    "    Improvements over naive:\n",
    "    1. Uses fast shared memory instead of global memory\n",
    "    2. Sequential addressing to avoid bank conflicts\n",
    "    3. Better thread utilization pattern\n",
    "    \"\"\"\n",
    "    # Allocate shared memory for this block\n",
    "    # This is ~100x faster than global memory!\n",
    "    sdata = cuda.shared.array(shape=(THREADS_PER_BLOCK,), dtype=numba.float32)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    bx = cuda.blockIdx.x\n",
    "    block_size = cuda.blockDim.x\n",
    "    idx = bx * block_size + tx\n",
    "    \n",
    "    # Step 1: Load from global memory to shared memory\n",
    "    if idx < data.size:\n",
    "        sdata[tx] = data[idx]\n",
    "    else:\n",
    "        sdata[tx] = 0.0  # Padding for out-of-bounds threads\n",
    "    \n",
    "    # CRITICAL: Wait for all threads to finish loading\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Step 2: Reduction in shared memory\n",
    "    # Use sequential addressing to avoid bank conflicts\n",
    "    stride = block_size // 2\n",
    "    while stride > 0:\n",
    "        if tx < stride:\n",
    "            sdata[tx] += sdata[tx + stride]\n",
    "        cuda.syncthreads()\n",
    "        stride //= 2\n",
    "    \n",
    "    # Step 3: Thread 0 writes result to global memory\n",
    "    if tx == 0:\n",
    "        partial_sums[bx] = sdata[0]\n",
    "\n",
    "\n",
    "def gpu_sum_shared(arr: np.ndarray) -> float:\n",
    "    \"\"\"Sum array using shared memory reduction.\"\"\"\n",
    "    n = arr.size\n",
    "    num_blocks = (n + THREADS_PER_BLOCK - 1) // THREADS_PER_BLOCK\n",
    "    \n",
    "    d_data = cuda.to_device(arr)\n",
    "    d_partial = cuda.device_array(num_blocks, dtype=np.float32)\n",
    "    \n",
    "    shared_memory_reduce_kernel[num_blocks, THREADS_PER_BLOCK](d_data, d_partial)\n",
    "    \n",
    "    partial_sums = d_partial.copy_to_host()\n",
    "    return np.sum(partial_sums)\n",
    "\n",
    "\n",
    "# Test correctness\n",
    "result = gpu_sum_shared(test_arr)\n",
    "print(f\"Shared Memory Reduction Test:\")\n",
    "print(f\"  Expected: 5050.0\")\n",
    "print(f\"  Got:      {result}\")\n",
    "print(f\"  Correct:  {'‚úÖ Yes!' if abs(result - 5050.0) < 0.01 else '‚ùå No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark shared memory version\n",
    "print(\"\\n‚è±Ô∏è  Benchmarking Shared Memory Reduction...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Warm up\n",
    "for _ in range(3):\n",
    "    _ = gpu_sum_shared(data)\n",
    "cuda.synchronize()\n",
    "\n",
    "# Benchmark\n",
    "start = time.perf_counter()\n",
    "for _ in range(10):\n",
    "    result_shared = gpu_sum_shared(data)\n",
    "    cuda.synchronize()\n",
    "time_shared = (time.perf_counter() - start) / 10\n",
    "\n",
    "print(f\"Array size: {N:,} elements\")\n",
    "print(f\"\")\n",
    "print(f\"NumPy time:          {time_numpy*1000:.3f} ms\")\n",
    "print(f\"Naive GPU time:      {time_naive_gpu*1000:.3f} ms\")\n",
    "print(f\"Shared Memory time:  {time_shared*1000:.3f} ms\")\n",
    "print(f\"\")\n",
    "print(f\"üìà Shared vs Naive:  {time_naive_gpu/time_shared:.1f}x faster\")\n",
    "print(f\"üìà Shared vs NumPy:  {time_numpy/time_shared:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Why Is Shared Memory Faster?\n",
    "\n",
    "**Before (Naive):**\n",
    "```\n",
    "Thread 0: Read global[0], Read global[1], Write global[0]  ‚Üê 3 slow ops\n",
    "Thread 0: Read global[0], Read global[2], Write global[0]  ‚Üê 3 slow ops\n",
    "...repeat log‚ÇÇ(N) times...\n",
    "```\n",
    "\n",
    "**After (Shared Memory):**\n",
    "```\n",
    "All threads: Read global[tx] ‚Üí shared[tx]  ‚Üê 1 slow op, done once!\n",
    "Thread 0: Read shared[0], Read shared[128], Write shared[0]  ‚Üê 3 fast ops\n",
    "Thread 0: Read shared[0], Read shared[64], Write shared[0]   ‚Üê 3 fast ops\n",
    "...repeat log‚ÇÇ(BLOCK_SIZE) times using fast shared memory...\n",
    "Thread 0: Write global[bx] = shared[0]  ‚Üê 1 slow op\n",
    "```\n",
    "\n",
    "**The key insight:** We minimized global memory access and did all the heavy computation in fast shared memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Ultimate Optimization with Warp Shuffle\n",
    "\n",
    "### üßí ELI5: What's a Warp Shuffle?\n",
    "\n",
    "> Remember the 32 students who always work together (the warp)? Usually, if student 0 wants to see student 16's answer, they write it on the whiteboard and student 0 reads it. Two operations!\n",
    ">\n",
    "> **Warp shuffle** is like telepathy between the 32 students. Student 0 can directly peek at student 16's paper without any writing. It's instantaneous!\n",
    ">\n",
    "> This is a special hardware feature where threads in the same warp can directly share register values without using any memory at all.\n",
    "\n",
    "### Warp-Level Primitives\n",
    "\n",
    "CUDA provides special \"warp shuffle\" instructions:\n",
    "- `__shfl_down_sync()`: Get value from thread with higher index\n",
    "- `__shfl_up_sync()`: Get value from thread with lower index  \n",
    "- `__shfl_xor_sync()`: Get value from thread at XOR distance\n",
    "\n",
    "In Numba, we use `cuda.shfl_down_sync()` and similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WARP_SIZE = 32\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def warp_reduce(val):\n",
    "    \"\"\"\n",
    "    Reduce within a warp using shuffle instructions.\n",
    "    \n",
    "    This is a device function - called from kernels, not from host.\n",
    "    Uses warp shuffle for ultimate performance (no memory access!).\n",
    "    \n",
    "    After this, thread 0 of the warp has the sum of all 32 values.\n",
    "    \"\"\"\n",
    "    # All 32 threads participate (mask = 0xffffffff)\n",
    "    mask = 0xffffffff\n",
    "    \n",
    "    # Shuffle down and add in tree pattern\n",
    "    # Round 1: threads 0-15 get values from threads 16-31\n",
    "    val += cuda.shfl_down_sync(mask, val, 16)\n",
    "    # Round 2: threads 0-7 get values from threads 8-15\n",
    "    val += cuda.shfl_down_sync(mask, val, 8)\n",
    "    # Round 3: threads 0-3 get values from threads 4-7\n",
    "    val += cuda.shfl_down_sync(mask, val, 4)\n",
    "    # Round 4: threads 0-1 get values from threads 2-3\n",
    "    val += cuda.shfl_down_sync(mask, val, 2)\n",
    "    # Round 5: thread 0 gets value from thread 1\n",
    "    val += cuda.shfl_down_sync(mask, val, 1)\n",
    "    \n",
    "    return val\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def warp_shuffle_reduce_kernel(data, partial_sums):\n",
    "    \"\"\"\n",
    "    Ultimate reduction using warp shuffle + shared memory hybrid.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Each thread loads data from global memory\n",
    "    2. Warp-level reduction (no shared memory needed!)\n",
    "    3. Store warp results in shared memory\n",
    "    4. Final reduction of warp results\n",
    "    \"\"\"\n",
    "    # Shared memory only for warp results (much smaller!)\n",
    "    # With 256 threads, we have 8 warps, so we need 8 slots\n",
    "    warp_sums = cuda.shared.array(shape=(THREADS_PER_BLOCK // WARP_SIZE,), dtype=numba.float32)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    bx = cuda.blockIdx.x\n",
    "    block_size = cuda.blockDim.x\n",
    "    idx = bx * block_size + tx\n",
    "    \n",
    "    # Which warp am I in? Which lane within the warp?\n",
    "    warp_id = tx // WARP_SIZE      # 0, 1, 2, ... 7 for 256 threads\n",
    "    lane_id = tx % WARP_SIZE       # 0, 1, 2, ... 31 within warp\n",
    "    \n",
    "    # Step 1: Load data\n",
    "    val = data[idx] if idx < data.size else 0.0\n",
    "    \n",
    "    # Step 2: Warp-level reduction (no syncthreads needed!)\n",
    "    val = warp_reduce(val)\n",
    "    \n",
    "    # Step 3: Lane 0 of each warp stores to shared memory\n",
    "    if lane_id == 0:\n",
    "        warp_sums[warp_id] = val\n",
    "    \n",
    "    cuda.syncthreads()  # Wait for all warps to finish\n",
    "    \n",
    "    # Step 4: First warp reduces all warp sums\n",
    "    num_warps = block_size // WARP_SIZE\n",
    "    if warp_id == 0:\n",
    "        val = warp_sums[lane_id] if lane_id < num_warps else 0.0\n",
    "        val = warp_reduce(val)\n",
    "        \n",
    "        if lane_id == 0:\n",
    "            partial_sums[bx] = val\n",
    "\n",
    "\n",
    "def gpu_sum_warp_shuffle(arr: np.ndarray) -> float:\n",
    "    \"\"\"Sum array using warp shuffle reduction.\"\"\"\n",
    "    n = arr.size\n",
    "    num_blocks = (n + THREADS_PER_BLOCK - 1) // THREADS_PER_BLOCK\n",
    "    \n",
    "    d_data = cuda.to_device(arr)\n",
    "    d_partial = cuda.device_array(num_blocks, dtype=np.float32)\n",
    "    \n",
    "    warp_shuffle_reduce_kernel[num_blocks, THREADS_PER_BLOCK](d_data, d_partial)\n",
    "    \n",
    "    partial_sums = d_partial.copy_to_host()\n",
    "    return np.sum(partial_sums)\n",
    "\n",
    "\n",
    "# Test correctness\n",
    "result = gpu_sum_warp_shuffle(test_arr)\n",
    "print(f\"Warp Shuffle Reduction Test:\")\n",
    "print(f\"  Expected: 5050.0\")\n",
    "print(f\"  Got:      {result}\")\n",
    "print(f\"  Correct:  {'‚úÖ Yes!' if abs(result - 5050.0) < 0.01 else '‚ùå No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark warp shuffle version\n",
    "print(\"\\n‚è±Ô∏è  Benchmarking Warp Shuffle Reduction...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Warm up\n",
    "for _ in range(3):\n",
    "    _ = gpu_sum_warp_shuffle(data)\n",
    "cuda.synchronize()\n",
    "\n",
    "# Benchmark\n",
    "start = time.perf_counter()\n",
    "for _ in range(10):\n",
    "    result_shuffle = gpu_sum_warp_shuffle(data)\n",
    "    cuda.synchronize()\n",
    "time_shuffle = (time.perf_counter() - start) / 10\n",
    "\n",
    "print(f\"Array size: {N:,} elements\\n\")\n",
    "print(f\"{'Method':<25} {'Time (ms)':<12} {'vs NumPy':<12}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'NumPy':<25} {time_numpy*1000:<12.3f} {'1.0x':<12}\")\n",
    "print(f\"{'Naive GPU':<25} {time_naive_gpu*1000:<12.3f} {f'{time_numpy/time_naive_gpu:.1f}x':<12}\")\n",
    "print(f\"{'Shared Memory':<25} {time_shared*1000:<12.3f} {f'{time_numpy/time_shared:.1f}x':<12}\")\n",
    "print(f\"{'Warp Shuffle':<25} {time_shuffle*1000:<12.3f} {f'{time_numpy/time_shuffle:.1f}x':<12}\")\n",
    "\n",
    "print(f\"\\nüéâ Best speedup: {time_numpy/time_shuffle:.1f}x faster than NumPy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Complete Reduction with Multi-Level Kernel\n",
    "\n",
    "Our previous versions still do the final sum on CPU. For maximum performance (especially with very large arrays), we should do the entire reduction on GPU.\n",
    "\n",
    "**Strategy:** Launch kernels repeatedly until we're down to 1 element.\n",
    "\n",
    "```\n",
    "10,000,000 elements\n",
    "    ‚Üì Kernel 1: 39,063 blocks ‚Üí 39,063 partial sums\n",
    "39,063 elements\n",
    "    ‚Üì Kernel 2: 153 blocks ‚Üí 153 partial sums\n",
    "153 elements\n",
    "    ‚Üì Kernel 3: 1 block ‚Üí 1 sum\n",
    "Final answer!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_sum_complete(arr: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Complete GPU reduction with multi-level kernels.\n",
    "    \n",
    "    This avoids any CPU reduction - everything happens on GPU.\n",
    "    \"\"\"\n",
    "    n = arr.size\n",
    "    d_data = cuda.to_device(arr)\n",
    "    \n",
    "    while n > 1:\n",
    "        num_blocks = (n + THREADS_PER_BLOCK - 1) // THREADS_PER_BLOCK\n",
    "        d_partial = cuda.device_array(num_blocks, dtype=np.float32)\n",
    "        \n",
    "        warp_shuffle_reduce_kernel[num_blocks, THREADS_PER_BLOCK](d_data, d_partial)\n",
    "        \n",
    "        # Prepare for next iteration\n",
    "        d_data = d_partial\n",
    "        n = num_blocks\n",
    "    \n",
    "    # Only copy 1 element back\n",
    "    return d_data.copy_to_host()[0]\n",
    "\n",
    "\n",
    "# Test\n",
    "result = gpu_sum_complete(test_arr)\n",
    "print(f\"Complete GPU Reduction Test:\")\n",
    "print(f\"  Expected: 5050.0\")\n",
    "print(f\"  Got:      {result}\")\n",
    "print(f\"  Correct:  {'‚úÖ Yes!' if abs(result - 5050.0) < 0.01 else '‚ùå No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive benchmark\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä FINAL BENCHMARK: Parallel Reduction Performance\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test at multiple sizes\n",
    "sizes = [100_000, 1_000_000, 10_000_000, 50_000_000]\n",
    "methods = [\n",
    "    ('NumPy', lambda x: np.sum(x)),\n",
    "    ('Shared Memory', gpu_sum_shared),\n",
    "    ('Warp Shuffle', gpu_sum_warp_shuffle),\n",
    "    ('Complete GPU', gpu_sum_complete),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Size':>12} | \" + \" | \".join(f\"{name:>15}\" for name, _ in methods))\n",
    "print(\"-\" * (14 + 18 * len(methods)))\n",
    "\n",
    "for size in sizes:\n",
    "    test_data = np.random.randn(size).astype(np.float32)\n",
    "    times = []\n",
    "    \n",
    "    for name, func in methods:\n",
    "        # Warm up\n",
    "        for _ in range(2):\n",
    "            _ = func(test_data)\n",
    "        if name != 'NumPy':\n",
    "            cuda.synchronize()\n",
    "        \n",
    "        # Benchmark\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(5):\n",
    "            _ = func(test_data)\n",
    "        if name != 'NumPy':\n",
    "            cuda.synchronize()\n",
    "        elapsed = (time.perf_counter() - start) / 5\n",
    "        times.append(elapsed)\n",
    "    \n",
    "    # Print with speedups relative to NumPy\n",
    "    numpy_time = times[0]\n",
    "    row = f\"{size:>12,} | \"\n",
    "    for i, (name, _) in enumerate(methods):\n",
    "        if i == 0:\n",
    "            row += f\"{times[i]*1000:>12.2f} ms | \"\n",
    "        else:\n",
    "            speedup = numpy_time / times[i]\n",
    "            row += f\"{times[i]*1000:>8.2f} ({speedup:>3.0f}x) | \"\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nüí° Numbers in parentheses show speedup vs NumPy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting `cuda.syncthreads()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå WRONG: Missing synchronization\n",
    "@cuda.jit\n",
    "def broken_reduce(data, output):\n",
    "    sdata = cuda.shared.array(shape=(256,), dtype=numba.float32)\n",
    "    tx = cuda.threadIdx.x\n",
    "    sdata[tx] = data[cuda.blockIdx.x * 256 + tx]\n",
    "    # Missing: cuda.syncthreads()  ‚Üê BUG!\n",
    "    if tx < 128:\n",
    "        sdata[tx] += sdata[tx + 128]  # Reading uninitialized values!\n",
    "\n",
    "# ‚úÖ CORRECT: Synchronize before reading\n",
    "@cuda.jit\n",
    "def correct_reduce(data, output):\n",
    "    sdata = cuda.shared.array(shape=(256,), dtype=numba.float32)\n",
    "    tx = cuda.threadIdx.x\n",
    "    sdata[tx] = data[cuda.blockIdx.x * 256 + tx]\n",
    "    cuda.syncthreads()  # ‚Üê All threads wait here\n",
    "    if tx < 128:\n",
    "        sdata[tx] += sdata[tx + 128]  # Now safe!\n",
    "\n",
    "print(\"üí° Always use cuda.syncthreads() after writing to shared memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Conditional `syncthreads()` - Deadlock!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå WRONG: syncthreads inside conditional\n",
    "@cuda.jit\n",
    "def deadlock_kernel(data):\n",
    "    tx = cuda.threadIdx.x\n",
    "    if tx < 128:  # Only half the threads!\n",
    "        # ... do work ...\n",
    "        cuda.syncthreads()  # ‚Üê DEADLOCK! Other threads never reach this!\n",
    "\n",
    "# ‚úÖ CORRECT: All threads reach syncthreads\n",
    "@cuda.jit\n",
    "def working_kernel(data):\n",
    "    tx = cuda.threadIdx.x\n",
    "    if tx < 128:\n",
    "        pass  # ... do work ...\n",
    "    cuda.syncthreads()  # ‚Üê All threads reach this, even if they didn't work\n",
    "\n",
    "print(\"üí° cuda.syncthreads() must be reached by ALL threads in the block!\")\n",
    "print(\"   It's a barrier - threads wait until everyone arrives.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Not Checking Array Bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå WRONG: No bounds check\n",
    "@cuda.jit\n",
    "def oob_kernel(data, output):\n",
    "    idx = cuda.grid(1)\n",
    "    output[idx] = data[idx] * 2  # ‚Üê Crashes if idx >= data.size!\n",
    "\n",
    "# ‚úÖ CORRECT: Check bounds\n",
    "@cuda.jit\n",
    "def safe_kernel(data, output):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < data.size:  # ‚Üê Safety check!\n",
    "        output[idx] = data[idx] * 2\n",
    "\n",
    "print(\"üí° Always check: if idx < data.size before accessing arrays!\")\n",
    "print(\"   GPU launches threads in multiples of block size.\")\n",
    "print(\"   Example: 1000 elements, 256 threads/block ‚Üí 1024 threads launched!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 4: Not Synchronizing After Kernel Launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå WRONG: Timing without synchronization\n",
    "def bad_benchmark():\n",
    "    d_data = cuda.to_device(np.ones(1000000, dtype=np.float32))\n",
    "    start = time.perf_counter()\n",
    "    some_kernel[blocks, threads](d_data)  # This is ASYNCHRONOUS!\n",
    "    elapsed = time.perf_counter() - start  # ‚Üê Measures launch time, not execution!\n",
    "    return elapsed\n",
    "\n",
    "# ‚úÖ CORRECT: Synchronize before measuring\n",
    "def good_benchmark():\n",
    "    d_data = cuda.to_device(np.ones(1000000, dtype=np.float32))\n",
    "    start = time.perf_counter()\n",
    "    some_kernel[blocks, threads](d_data)\n",
    "    cuda.synchronize()  # ‚Üê Wait for GPU to finish!\n",
    "    elapsed = time.perf_counter() - start\n",
    "    return elapsed\n",
    "\n",
    "print(\"üí° Kernel launches are ASYNCHRONOUS!\")\n",
    "print(\"   Use cuda.synchronize() before timing or reading results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úã Try It Yourself: Implement Parallel Max\n",
    "\n",
    "**Challenge:** Modify the reduction kernel to find the **maximum** value instead of the sum.\n",
    "\n",
    "This is the same pattern used in:\n",
    "- Softmax normalization (find max for numerical stability)\n",
    "- Finding the predicted class (argmax)\n",
    "- Computing attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement parallel max reduction\n",
    "# Hint: Replace addition with max comparison\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def warp_max(val):\n",
    "    \"\"\"Find max within a warp using shuffle instructions.\"\"\"\n",
    "    mask = 0xffffffff\n",
    "    \n",
    "    # TODO: Replace += with max()\n",
    "    # Hint: Use max(val, cuda.shfl_down_sync(...))\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "    \n",
    "    return val\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def parallel_max_kernel(data, partial_maxes):\n",
    "    \"\"\"Find max value using parallel reduction.\"\"\"\n",
    "    # TODO: Implement this kernel\n",
    "    # Hint: It's almost identical to warp_shuffle_reduce_kernel\n",
    "    # Just replace addition with max()\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "\n",
    "def gpu_max(arr: np.ndarray) -> float:\n",
    "    \"\"\"Find max value using GPU.\"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "test_data = np.random.randn(1_000_000).astype(np.float32)\n",
    "expected_max = np.max(test_data)\n",
    "\n",
    "# Uncomment when implemented:\n",
    "# result = gpu_max(test_data)\n",
    "# print(f\"Expected max: {expected_max}\")\n",
    "# print(f\"GPU max:      {result}\")\n",
    "# print(f\"Correct: {'‚úÖ' if abs(result - expected_max) < 1e-5 else '‚ùå'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Hint 1</summary>\n",
    "\n",
    "The only change needed in `warp_max` is:\n",
    "```python\n",
    "val = max(val, cuda.shfl_down_sync(mask, val, offset))\n",
    "```\n",
    "Instead of:\n",
    "```python\n",
    "val += cuda.shfl_down_sync(mask, val, offset)\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint 2</summary>\n",
    "\n",
    "For the kernel, initialize with negative infinity for proper max behavior:\n",
    "```python\n",
    "val = data[idx] if idx < data.size else -np.inf\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint 3</summary>\n",
    "\n",
    "When combining warp results, also use max:\n",
    "```python\n",
    "if lane_id == 0:\n",
    "    warp_maxes[warp_id] = val\n",
    "# ...\n",
    "val = warp_maxes[lane_id] if lane_id < num_warps else -np.inf\n",
    "val = warp_max(val)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "Congratulations! You've learned:\n",
    "\n",
    "- ‚úÖ **Why parallel reduction matters** - It's the foundation of neural network training\n",
    "- ‚úÖ **GPU thread hierarchy** - Threads, blocks, warps, and how they map to hardware\n",
    "- ‚úÖ **Shared memory optimization** - 100x faster than global memory\n",
    "- ‚úÖ **Warp shuffle operations** - Direct thread-to-thread communication\n",
    "- ‚úÖ **Common pitfalls** - syncthreads, bounds checking, synchronization\n",
    "\n",
    "You achieved **100x+ speedup** over CPU implementations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Challenge (Optional)\n",
    "\n",
    "**Advanced Challenge: Implement Double-Precision Reduction**\n",
    "\n",
    "Floating-point addition is not associative! The order of operations can change the result slightly.\n",
    "\n",
    "```python\n",
    "# Example: Adding 1e8 + 1e-8 + 1e-8 + 1e-8 + ...\n",
    "# Order 1: ((1e8 + 1e-8) + 1e-8) + ... = 1e8 (small values lost!)\n",
    "# Order 2: (1e-8 + 1e-8 + ...) + 1e8 = 1e8 + small_sum (more accurate)\n",
    "```\n",
    "\n",
    "**Your challenge:**\n",
    "1. Implement reduction using `float64` (double precision)\n",
    "2. Compare accuracy vs `float32` version\n",
    "3. Implement **Kahan summation** for even better accuracy\n",
    "\n",
    "*Kahan summation tracks a \"compensation\" term to recover lost precision.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [NVIDIA Parallel Reduction PDF](https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf) - The classic optimization guide\n",
    "- [Numba CUDA Documentation](https://numba.readthedocs.io/en/stable/cuda/index.html)\n",
    "- [Warp Shuffle Functions](https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/)\n",
    "- [Understanding GPU Architecture](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "\n",
    "# Delete large arrays\n",
    "del data, test_data\n",
    "if 'small_data' in dir():\n",
    "    del small_data\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Clear CUDA context\n",
    "cuda.current_context().reset()\n",
    "\n",
    "print(\"‚úÖ GPU memory cleared!\")\n",
    "print(\"\\n‚û°Ô∏è Ready for Lab 1.3.2: Matrix Multiplication\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
