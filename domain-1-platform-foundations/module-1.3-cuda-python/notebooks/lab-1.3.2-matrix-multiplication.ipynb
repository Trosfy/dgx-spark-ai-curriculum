{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.3.2: Matrix Multiplication\n",
    "\n",
    "**Module:** 1.3 - CUDA Python & GPU Programming  \n",
    "**Time:** 3 hours  \n",
    "**Difficulty:** â­â­â­â­ (Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand why matrix multiplication is the heart of deep learning\n",
    "- [ ] Implement naive matrix multiplication on GPU\n",
    "- [ ] Master the **tiled algorithm** using shared memory\n",
    "- [ ] Understand memory coalescing and bank conflicts\n",
    "- [ ] Achieve performance within **2x of cuBLAS** (NVIDIA's optimized library)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Prerequisites\n",
    "\n",
    "- Completed: Lab 1.3.1 (Parallel Reduction)\n",
    "- Knowledge of: Shared memory, thread synchronization\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ Real-World Context\n",
    "\n",
    "**Matrix multiplication is literally 90%+ of deep learning compute.**\n",
    "\n",
    "Every neural network operation boils down to matmul:\n",
    "- **Linear layers:** `y = Wx + b` â†’ Matrix multiplication\n",
    "- **Attention mechanism:** `softmax(QK^T)V` â†’ Three matrix multiplications!\n",
    "- **Convolutions:** Can be expressed as matrix multiplication (im2col)\n",
    "- **Embeddings:** Sparse matrix multiplication\n",
    "\n",
    "**The numbers are staggering:**\n",
    "- GPT-3 forward pass: ~350 trillion floating-point operations (FLOPs)\n",
    "- Almost all of that is matrix multiplication\n",
    "- Training GPT-3 required ~3,000 PFLOP-days of compute\n",
    "\n",
    "Your DGX Spark can perform **~100 TFLOPS in BF16**. Understanding how matrix multiplication uses this power is essential!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§’ ELI5: Matrix Multiplication on GPU\n",
    "\n",
    "> **Imagine you're organizing a giant wedding reception** with 1000 tables and 1000 guests.\n",
    ">\n",
    "> Each guest needs to receive food from multiple serving stations. The naive approach:\n",
    "> - Each server walks to every table one by one (slow!)\n",
    "> - They make 1000 trips, and there are 1000 servers = 1,000,000 trips!\n",
    ">\n",
    "> **The tiled approach:**\n",
    "> - Divide the venue into \"zones\" (tiles)\n",
    "> - Each zone has a small buffet table (shared memory)\n",
    "> - Servers load food onto the local buffet once\n",
    "> - All guests in that zone can quickly grab from the buffet\n",
    "> - Then move to the next zone\n",
    ">\n",
    "> **In matrix terms:**\n",
    "> - Each output element needs data from an entire row and column\n",
    "> - Loading from main memory is slow (the trips across the venue)\n",
    "> - We load a \"tile\" of data into shared memory (the local buffet)\n",
    "> - Multiple threads use that same data (guests at that table)\n",
    "> - This is called **data reuse** - the key to GPU performance!\n",
    "\n",
    "```\n",
    "Matrix A (MÃ—K)         Matrix B (KÃ—N)         Matrix C (MÃ—N)\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â”‚       â”‚ â–ˆ â–ˆ â”‚ â”‚ â”‚ â”‚ â”‚       â”‚ âœ– â”‚ â”‚ â”‚ â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚   Ã—   â”‚ â–ˆ â–ˆ â”‚ â”‚ â”‚ â”‚ â”‚   =   â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚       â”‚ â–ˆ â–ˆ â”‚ â”‚ â”‚ â”‚ â”‚       â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚       â”‚ â–ˆ â–ˆ â”‚ â”‚ â”‚ â”‚ â”‚       â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "Row 0 of A (â–ˆ)    Ã— Column 0 of B (â–ˆ)    = Element C[0,0] (âœ–)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 0: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from typing import Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from numba import cuda, float32\n",
    "import numba\n",
    "\n",
    "# Verify CUDA\n",
    "print(\"ğŸ”§ CUDA Environment\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Numba version: {numba.__version__}\")\n",
    "print(f\"CUDA available: {cuda.is_available()}\")\n",
    "\n",
    "if cuda.is_available():\n",
    "    device = cuda.get_current_device()\n",
    "    print(f\"Device: {device.name}\")\n",
    "    print(f\"Shared memory per block: {device.MAX_SHARED_MEMORY_PER_BLOCK / 1024:.1f} KB\")\n",
    "    \n",
    "    # Calculate max tile size based on shared memory\n",
    "    # We need 2 tiles (for A and B), each TILE_SIZEÂ² Ã— 4 bytes (float32)\n",
    "    max_shared = device.MAX_SHARED_MEMORY_PER_BLOCK\n",
    "    max_tile_size = int(np.sqrt(max_shared / 8))  # 8 = 2 tiles Ã— 4 bytes\n",
    "    print(f\"Max tile size (theoretical): {max_tile_size}\")\n",
    "    print(\"\\nâœ… Ready for matrix multiplication!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: CPU and NumPy Baselines\n",
    "\n",
    "First, let's understand how matrix multiplication works and establish baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_cpu_naive(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Naive CPU matrix multiplication using triple nested loop.\n",
    "    \n",
    "    This is O(M Ã— N Ã— K) - cubic complexity!\n",
    "    \"\"\"\n",
    "    M, K = A.shape\n",
    "    K2, N = B.shape\n",
    "    assert K == K2, \"Inner dimensions must match!\"\n",
    "    \n",
    "    C = np.zeros((M, N), dtype=A.dtype)\n",
    "    \n",
    "    for i in range(M):       # For each row of A\n",
    "        for j in range(N):   # For each column of B\n",
    "            for k in range(K):  # Dot product\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "    \n",
    "    return C\n",
    "\n",
    "\n",
    "# Small test to verify correctness\n",
    "A_small = np.array([[1, 2], [3, 4]], dtype=np.float32)\n",
    "B_small = np.array([[5, 6], [7, 8]], dtype=np.float32)\n",
    "\n",
    "result_naive = matmul_cpu_naive(A_small, B_small)\n",
    "result_numpy = A_small @ B_small  # NumPy's optimized matmul\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A_small)\n",
    "print(\"\\nMatrix B:\")\n",
    "print(B_small)\n",
    "print(\"\\nA Ã— B (naive):\")\n",
    "print(result_naive)\n",
    "print(\"\\nA Ã— B (NumPy):\")\n",
    "print(result_numpy)\n",
    "print(f\"\\nâœ… Results match: {np.allclose(result_naive, result_numpy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Math\n",
    "\n",
    "For `C = A Ã— B` where A is MÃ—K and B is KÃ—N:\n",
    "\n",
    "```\n",
    "C[i,j] = Î£(k=0 to K-1) A[i,k] Ã— B[k,j]\n",
    "```\n",
    "\n",
    "Each element of C requires **K multiplications and K-1 additions** (a dot product).\n",
    "\n",
    "For MÃ—N outputs, total operations: **2 Ã— M Ã— N Ã— K FLOPs** (multiply and add)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark on larger matrices\n",
    "np.random.seed(42)\n",
    "\n",
    "# Test sizes\n",
    "sizes = [(64, 64), (256, 256), (512, 512), (1024, 1024)]\n",
    "\n",
    "print(\"â±ï¸  NumPy Matmul Baseline (uses BLAS/MKL internally)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Size':<15} {'Time (ms)':<15} {'GFLOPS':<15}\")\n",
    "print(\"-\"*45)\n",
    "\n",
    "for M, N in sizes:\n",
    "    K = M  # Square matrices\n",
    "    A = np.random.randn(M, K).astype(np.float32)\n",
    "    B = np.random.randn(K, N).astype(np.float32)\n",
    "    \n",
    "    # Warm up\n",
    "    _ = A @ B\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.perf_counter()\n",
    "    iterations = 10\n",
    "    for _ in range(iterations):\n",
    "        C = A @ B\n",
    "    elapsed = (time.perf_counter() - start) / iterations\n",
    "    \n",
    "    # Calculate GFLOPS: 2*M*N*K operations (multiply-add)\n",
    "    flops = 2 * M * N * K\n",
    "    gflops = flops / elapsed / 1e9\n",
    "    \n",
    "    print(f\"{M}Ã—{N}Ã—{K:<7} {elapsed*1000:<15.3f} {gflops:<15.1f}\")\n",
    "\n",
    "# Store the 1024Ã—1024 for later comparison\n",
    "A_1024 = np.random.randn(1024, 1024).astype(np.float32)\n",
    "B_1024 = np.random.randn(1024, 1024).astype(np.float32)\n",
    "C_reference = A_1024 @ B_1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Naive GPU Matrix Multiplication\n",
    "\n",
    "**Strategy:** Each thread computes one element of the output matrix C.\n",
    "\n",
    "```\n",
    "Thread (i,j) computes C[i,j]:\n",
    "    sum = 0\n",
    "    for k = 0 to K-1:\n",
    "        sum += A[i,k] * B[k,j]\n",
    "    C[i,j] = sum\n",
    "```\n",
    "\n",
    "Simple, but each thread does **K global memory reads for row of A** and **K global memory reads for column of B**. That's 2K reads per thread!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def matmul_naive_kernel(A, B, C):\n",
    "    \"\"\"\n",
    "    Naive GPU matrix multiplication.\n",
    "    \n",
    "    Each thread computes one element of C.\n",
    "    Problem: Lots of redundant global memory access!\n",
    "    \"\"\"\n",
    "    # 2D grid/block indexing\n",
    "    row = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
    "    col = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    \n",
    "    M, K = A.shape\n",
    "    K2, N = B.shape\n",
    "    \n",
    "    # Bounds check\n",
    "    if row < M and col < N:\n",
    "        # Compute dot product of row of A and column of B\n",
    "        tmp = 0.0\n",
    "        for k in range(K):\n",
    "            tmp += A[row, k] * B[k, col]  # K reads from A, K reads from B\n",
    "        C[row, col] = tmp\n",
    "\n",
    "\n",
    "def matmul_gpu_naive(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Matrix multiplication using naive GPU kernel.\n",
    "    \"\"\"\n",
    "    M, K = A.shape\n",
    "    K2, N = B.shape\n",
    "    assert K == K2\n",
    "    \n",
    "    # Transfer to GPU\n",
    "    d_A = cuda.to_device(A)\n",
    "    d_B = cuda.to_device(B)\n",
    "    d_C = cuda.device_array((M, N), dtype=np.float32)\n",
    "    \n",
    "    # Configure grid: 16Ã—16 threads per block\n",
    "    threads_per_block = (16, 16)\n",
    "    blocks_x = (N + 15) // 16\n",
    "    blocks_y = (M + 15) // 16\n",
    "    blocks = (blocks_x, blocks_y)\n",
    "    \n",
    "    # Launch kernel\n",
    "    matmul_naive_kernel[blocks, threads_per_block](d_A, d_B, d_C)\n",
    "    \n",
    "    return d_C.copy_to_host()\n",
    "\n",
    "\n",
    "# Test correctness\n",
    "result_gpu = matmul_gpu_naive(A_small, B_small)\n",
    "print(\"Naive GPU result:\")\n",
    "print(result_gpu)\n",
    "print(f\"\\nâœ… Correct: {np.allclose(result_gpu, result_numpy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark naive GPU\n",
    "print(\"\\nâ±ï¸  Benchmarking Naive GPU Matmul...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Pre-transfer data\n",
    "d_A = cuda.to_device(A_1024)\n",
    "d_B = cuda.to_device(B_1024)\n",
    "d_C = cuda.device_array((1024, 1024), dtype=np.float32)\n",
    "\n",
    "threads_per_block = (16, 16)\n",
    "blocks = (64, 64)\n",
    "\n",
    "# Warm up\n",
    "for _ in range(3):\n",
    "    matmul_naive_kernel[blocks, threads_per_block](d_A, d_B, d_C)\n",
    "cuda.synchronize()\n",
    "\n",
    "# Benchmark\n",
    "iterations = 10\n",
    "start = time.perf_counter()\n",
    "for _ in range(iterations):\n",
    "    matmul_naive_kernel[blocks, threads_per_block](d_A, d_B, d_C)\n",
    "cuda.synchronize()\n",
    "time_naive_gpu = (time.perf_counter() - start) / iterations\n",
    "\n",
    "# Calculate GFLOPS\n",
    "flops = 2 * 1024 * 1024 * 1024\n",
    "gflops_naive = flops / time_naive_gpu / 1e9\n",
    "\n",
    "print(f\"Matrix size: 1024Ã—1024\")\n",
    "print(f\"Naive GPU time: {time_naive_gpu*1000:.3f} ms\")\n",
    "print(f\"Naive GPU GFLOPS: {gflops_naive:.1f}\")\n",
    "\n",
    "# Verify correctness\n",
    "result = d_C.copy_to_host()\n",
    "print(f\"\\nâœ… Result correct: {np.allclose(result, C_reference, rtol=1e-4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” Why Is Naive GPU Still Slow?\n",
    "\n",
    "Let's analyze the memory access pattern:\n",
    "\n",
    "```\n",
    "For 1024Ã—1024 matrices:\n",
    "- Total output elements: 1,048,576\n",
    "- Each element needs: 1024 reads from A + 1024 reads from B = 2048 reads\n",
    "- Total memory reads: ~2 billion!\n",
    "- At 273 GB/s (DGX Spark), that's ~30ms just for memory access\n",
    "```\n",
    "\n",
    "**The problem:** We're reading the same data over and over!\n",
    "- Row 0 of A is read by ALL threads computing row 0 of C (1024 times!)\n",
    "- Column 0 of B is read by ALL threads computing column 0 of C (1024 times!)\n",
    "\n",
    "**The solution:** Load data into shared memory and reuse it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Tiled Matrix Multiplication\n",
    "\n",
    "### ğŸ§’ ELI5: Tiling (Again, with Pictures!)\n",
    "\n",
    "> Imagine you're solving a giant jigsaw puzzle (the output matrix C).\n",
    ">\n",
    "> **Naive approach:** For each puzzle piece, you look at the entire picture on box A and box B, comparing every single feature.\n",
    ">\n",
    "> **Tiled approach:** \n",
    "> 1. Cut the puzzle into small \"tiles\" (e.g., 16Ã—16 pieces each)\n",
    "> 2. For each tile of the output:\n",
    ">    - Bring a small section of box A's picture to your desk (shared memory)\n",
    ">    - Bring a small section of box B's picture to your desk\n",
    ">    - Everyone working on this tile uses these small sections\n",
    ">    - Then bring the next sections, and so on\n",
    "> 3. You never need the whole box at once - just small, reusable sections!\n",
    "\n",
    "### The Algorithm Visualized\n",
    "\n",
    "```\n",
    "For each output tile of C (16Ã—16 elements):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Matrix A    â”‚     â”‚   Matrix B    â”‚     â”‚   Matrix C    â”‚\n",
    "â”‚ â”Œâ”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â” â”‚     â”‚ â”Œâ”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â” â”‚     â”‚ â”Œâ”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â” â”‚\n",
    "â”‚ â”‚â–ˆâ–ˆâ”‚  â”‚  â”‚  â”‚ â”‚     â”‚ â”‚â–ˆâ–ˆâ”‚  â”‚  â”‚  â”‚ â”‚     â”‚ â”‚âœ– â”‚  â”‚  â”‚  â”‚ â”‚\n",
    "â”‚ â”œâ”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”¤ â”‚  Ã—  â”‚ â”œâ”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”¤ â”‚  =  â”‚ â”œâ”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”¤ â”‚\n",
    "â”‚ â”‚  â”‚  â”‚  â”‚  â”‚ â”‚     â”‚ â”‚  â”‚  â”‚  â”‚  â”‚ â”‚     â”‚ â”‚  â”‚  â”‚  â”‚  â”‚ â”‚\n",
    "â”‚ â””â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”˜ â”‚     â”‚ â””â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”˜ â”‚     â”‚ â””â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”˜ â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "   Load tile 0          Load tile 0         Accumulate\n",
    "\n",
    "       â¬‡                    â¬‡                   â¬‡\n",
    "   Load tile 1          Load tile 1         Accumulate\n",
    "       â¬‡                    â¬‡                   â¬‡\n",
    "      ...                  ...                 ...\n",
    "       â¬‡                    â¬‡                   â¬‡\n",
    "   Load tile K/16       Load tile K/16      Final result\n",
    "```\n",
    "\n",
    "**Key insight:** The 16Ã—16 tile of A is used by ALL 256 threads computing the output tile. Instead of 256Ã—16 reads, we do 16 reads total!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tile size - must match threads per block\n",
    "TILE_SIZE = 16\n",
    "\n",
    "@cuda.jit\n",
    "def matmul_tiled_kernel(A, B, C):\n",
    "    \"\"\"\n",
    "    Tiled matrix multiplication using shared memory.\n",
    "    \n",
    "    Each block computes one 16Ã—16 tile of C.\n",
    "    Tiles are loaded cooperatively into shared memory for reuse.\n",
    "    \"\"\"\n",
    "    # Shared memory for tiles of A and B\n",
    "    sA = cuda.shared.array(shape=(TILE_SIZE, TILE_SIZE), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(TILE_SIZE, TILE_SIZE), dtype=float32)\n",
    "    \n",
    "    # Thread indices\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    \n",
    "    # Block indices - which tile of C are we computing?\n",
    "    bx = cuda.blockIdx.x\n",
    "    by = cuda.blockIdx.y\n",
    "    \n",
    "    # Global row/col this thread is responsible for\n",
    "    row = by * TILE_SIZE + ty\n",
    "    col = bx * TILE_SIZE + tx\n",
    "    \n",
    "    M, K = A.shape\n",
    "    K2, N = B.shape\n",
    "    \n",
    "    # Accumulator for this thread's output element\n",
    "    tmp = float32(0.0)\n",
    "    \n",
    "    # Number of tiles we need to process\n",
    "    num_tiles = (K + TILE_SIZE - 1) // TILE_SIZE\n",
    "    \n",
    "    for tile_idx in range(num_tiles):\n",
    "        # Step 1: Cooperatively load tile of A into shared memory\n",
    "        # Each thread loads one element\n",
    "        a_col = tile_idx * TILE_SIZE + tx\n",
    "        if row < M and a_col < K:\n",
    "            sA[ty, tx] = A[row, a_col]\n",
    "        else:\n",
    "            sA[ty, tx] = 0.0  # Padding for boundary tiles\n",
    "        \n",
    "        # Step 2: Cooperatively load tile of B into shared memory\n",
    "        b_row = tile_idx * TILE_SIZE + ty\n",
    "        if b_row < K and col < N:\n",
    "            sB[ty, tx] = B[b_row, col]\n",
    "        else:\n",
    "            sB[ty, tx] = 0.0\n",
    "        \n",
    "        # CRITICAL: Wait for all threads to finish loading!\n",
    "        cuda.syncthreads()\n",
    "        \n",
    "        # Step 3: Compute partial dot product using shared memory\n",
    "        # This is where the magic happens - we read from fast shared memory!\n",
    "        for k in range(TILE_SIZE):\n",
    "            tmp += sA[ty, k] * sB[k, tx]\n",
    "        \n",
    "        # CRITICAL: Wait before loading next tile!\n",
    "        cuda.syncthreads()\n",
    "    \n",
    "    # Step 4: Write final result\n",
    "    if row < M and col < N:\n",
    "        C[row, col] = tmp\n",
    "\n",
    "\n",
    "def matmul_gpu_tiled(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Matrix multiplication using tiled GPU kernel.\n",
    "    \"\"\"\n",
    "    M, K = A.shape\n",
    "    K2, N = B.shape\n",
    "    assert K == K2\n",
    "    \n",
    "    d_A = cuda.to_device(A)\n",
    "    d_B = cuda.to_device(B)\n",
    "    d_C = cuda.device_array((M, N), dtype=np.float32)\n",
    "    \n",
    "    threads_per_block = (TILE_SIZE, TILE_SIZE)\n",
    "    blocks_x = (N + TILE_SIZE - 1) // TILE_SIZE\n",
    "    blocks_y = (M + TILE_SIZE - 1) // TILE_SIZE\n",
    "    blocks = (blocks_x, blocks_y)\n",
    "    \n",
    "    matmul_tiled_kernel[blocks, threads_per_block](d_A, d_B, d_C)\n",
    "    \n",
    "    return d_C.copy_to_host()\n",
    "\n",
    "\n",
    "# Test correctness\n",
    "result_tiled = matmul_gpu_tiled(A_small, B_small)\n",
    "print(\"Tiled GPU result:\")\n",
    "print(result_tiled)\n",
    "print(f\"\\nâœ… Correct: {np.allclose(result_tiled, result_numpy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark tiled GPU\n",
    "print(\"\\nâ±ï¸  Benchmarking Tiled GPU Matmul...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Pre-transfer data\n",
    "d_A = cuda.to_device(A_1024)\n",
    "d_B = cuda.to_device(B_1024)\n",
    "d_C = cuda.device_array((1024, 1024), dtype=np.float32)\n",
    "\n",
    "threads_per_block = (TILE_SIZE, TILE_SIZE)\n",
    "blocks = (64, 64)  # 1024 / 16 = 64\n",
    "\n",
    "# Warm up\n",
    "for _ in range(3):\n",
    "    matmul_tiled_kernel[blocks, threads_per_block](d_A, d_B, d_C)\n",
    "cuda.synchronize()\n",
    "\n",
    "# Benchmark\n",
    "iterations = 10\n",
    "start = time.perf_counter()\n",
    "for _ in range(iterations):\n",
    "    matmul_tiled_kernel[blocks, threads_per_block](d_A, d_B, d_C)\n",
    "cuda.synchronize()\n",
    "time_tiled_gpu = (time.perf_counter() - start) / iterations\n",
    "\n",
    "gflops_tiled = flops / time_tiled_gpu / 1e9\n",
    "\n",
    "print(f\"Matrix size: 1024Ã—1024\")\n",
    "print(f\"\\n{'Method':<20} {'Time (ms)':<15} {'GFLOPS':<15} {'Speedup':<15}\")\n",
    "print(\"-\"*65)\n",
    "print(f\"{'Naive GPU':<20} {time_naive_gpu*1000:<15.3f} {gflops_naive:<15.1f} {'1.0x':<15}\")\n",
    "print(f\"{'Tiled GPU':<20} {time_tiled_gpu*1000:<15.3f} {gflops_tiled:<15.1f} {f'{time_naive_gpu/time_tiled_gpu:.1f}x':<15}\")\n",
    "\n",
    "# Verify correctness\n",
    "result = d_C.copy_to_host()\n",
    "print(f\"\\nâœ… Result correct: {np.allclose(result, C_reference, rtol=1e-4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” Why Is Tiled Faster?\n",
    "\n",
    "**Memory Access Analysis:**\n",
    "\n",
    "| Metric | Naive | Tiled |\n",
    "|--------|-------|-------|\n",
    "| Global reads per element | 2K | 2K/TILE_SIZE |\n",
    "| For 1024Ã—1024, TILE=16 | 2Ã—1024 = 2048 | 2Ã—64 = 128 |\n",
    "| Total global reads | 2.1 billion | 134 million |\n",
    "| **Reduction factor** | - | **16x fewer!** |\n",
    "\n",
    "The tiles are stored in shared memory (~100x faster than global memory), so the inner loop runs at near-theoretical peak speed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Comparing with cuBLAS\n",
    "\n",
    "cuBLAS is NVIDIA's highly optimized library that squeezes every last FLOP from the GPU. Let's see how we compare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use CuPy's BLAS which wraps cuBLAS\n",
    "try:\n",
    "    import cupy as cp\n",
    "    HAS_CUPY = True\n",
    "    print(\"âœ… CuPy available - will use cuBLAS for comparison\")\n",
    "except ImportError:\n",
    "    HAS_CUPY = False\n",
    "    print(\"âš ï¸ CuPy not available - skipping cuBLAS comparison\")\n",
    "    print(\"   âš ï¸  Note: 'pip install cupy-cuda12x' installs x86 wheels (won't work on DGX Spark ARM64!)\")\n",
    "    print(\"   âœ… Use NGC container where CuPy is pre-installed and optimized for ARM64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_CUPY:\n",
    "    # Convert to CuPy arrays\n",
    "    cp_A = cp.asarray(A_1024)\n",
    "    cp_B = cp.asarray(B_1024)\n",
    "    \n",
    "    # Warm up\n",
    "    for _ in range(3):\n",
    "        cp_C = cp_A @ cp_B\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    \n",
    "    # Benchmark cuBLAS\n",
    "    start = time.perf_counter()\n",
    "    iterations = 10\n",
    "    for _ in range(iterations):\n",
    "        cp_C = cp_A @ cp_B\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    time_cublas = (time.perf_counter() - start) / iterations\n",
    "    \n",
    "    gflops_cublas = flops / time_cublas / 1e9\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ“Š COMPLETE BENCHMARK: 1024Ã—1024 Matrix Multiplication\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n{'Method':<20} {'Time (ms)':<12} {'GFLOPS':<12} {'vs Naive':<12} {'vs cuBLAS':<12}\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'Naive GPU':<20} {time_naive_gpu*1000:<12.3f} {gflops_naive:<12.1f} {'1.0x':<12} {f'{time_naive_gpu/time_cublas:.1f}x slower':<12}\")\n",
    "    print(f\"{'Tiled GPU':<20} {time_tiled_gpu*1000:<12.3f} {gflops_tiled:<12.1f} {f'{time_naive_gpu/time_tiled_gpu:.1f}x faster':<12} {f'{time_tiled_gpu/time_cublas:.1f}x':<12}\")\n",
    "    print(f\"{'cuBLAS':<20} {time_cublas*1000:<12.3f} {gflops_cublas:<12.1f} {f'{time_naive_gpu/time_cublas:.1f}x faster':<12} {'1.0x':<12}\")\n",
    "    \n",
    "    # Calculate how close we are to cuBLAS\n",
    "    pct_of_cublas = gflops_tiled / gflops_cublas * 100\n",
    "    print(f\"\\nğŸ¯ Our tiled implementation achieves {pct_of_cublas:.1f}% of cuBLAS performance!\")\n",
    "    \n",
    "    if pct_of_cublas > 50:\n",
    "        print(\"   âœ… Great work! You're within 2x of the world's best!\")\n",
    "    else:\n",
    "        print(\"   ğŸ’¡ There's room for optimization. Try larger tile sizes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Advanced Optimizations (Overview)\n",
    "\n",
    "Our tiled implementation is good, but cuBLAS uses several additional tricks:\n",
    "\n",
    "### 1. Larger Tiles and Register Blocking\n",
    "```python\n",
    "# Each thread computes a 4Ã—4 or 8Ã—8 block of output\n",
    "# Uses more registers, fewer shared memory accesses\n",
    "```\n",
    "\n",
    "### 2. Double Buffering (Prefetching)\n",
    "```python\n",
    "# While computing with tile N, load tile N+1\n",
    "# Hides memory latency\n",
    "```\n",
    "\n",
    "### 3. Avoiding Bank Conflicts\n",
    "```python\n",
    "# Shared memory has 32 banks\n",
    "# If multiple threads access the same bank = conflict = serialization\n",
    "# Solution: Add padding to shared memory arrays\n",
    "sA = cuda.shared.array((TILE_SIZE, TILE_SIZE + 1), ...)  # +1 padding\n",
    "```\n",
    "\n",
    "### 4. Tensor Cores\n",
    "```python\n",
    "# On Blackwell (your DGX Spark), Tensor Cores can do:\n",
    "# 16Ã—16Ã—16 matrix multiply in ONE instruction!\n",
    "# That's 4,096 FMAs per Tensor Core per clock!\n",
    "# cuBLAS automatically uses these for supported data types\n",
    "```\n",
    "\n",
    "Let's implement bank conflict avoidance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bank conflict-free version\n",
    "TILE_SIZE_PADDED = TILE_SIZE + 1  # Padding to avoid bank conflicts\n",
    "\n",
    "@cuda.jit\n",
    "def matmul_tiled_noconflict_kernel(A, B, C):\n",
    "    \"\"\"\n",
    "    Tiled matmul with bank conflict avoidance.\n",
    "    \n",
    "    Adding +1 to the second dimension of shared arrays\n",
    "    prevents bank conflicts when accessing columns.\n",
    "    \"\"\"\n",
    "    # Note the +1 padding!\n",
    "    sA = cuda.shared.array(shape=(TILE_SIZE, TILE_SIZE_PADDED), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(TILE_SIZE, TILE_SIZE_PADDED), dtype=float32)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bx = cuda.blockIdx.x\n",
    "    by = cuda.blockIdx.y\n",
    "    \n",
    "    row = by * TILE_SIZE + ty\n",
    "    col = bx * TILE_SIZE + tx\n",
    "    \n",
    "    M, K = A.shape\n",
    "    K2, N = B.shape\n",
    "    \n",
    "    tmp = float32(0.0)\n",
    "    num_tiles = (K + TILE_SIZE - 1) // TILE_SIZE\n",
    "    \n",
    "    for tile_idx in range(num_tiles):\n",
    "        a_col = tile_idx * TILE_SIZE + tx\n",
    "        if row < M and a_col < K:\n",
    "            sA[ty, tx] = A[row, a_col]\n",
    "        else:\n",
    "            sA[ty, tx] = 0.0\n",
    "        \n",
    "        b_row = tile_idx * TILE_SIZE + ty\n",
    "        if b_row < K and col < N:\n",
    "            sB[ty, tx] = B[b_row, col]\n",
    "        else:\n",
    "            sB[ty, tx] = 0.0\n",
    "        \n",
    "        cuda.syncthreads()\n",
    "        \n",
    "        for k in range(TILE_SIZE):\n",
    "            tmp += sA[ty, k] * sB[k, tx]\n",
    "        \n",
    "        cuda.syncthreads()\n",
    "    \n",
    "    if row < M and col < N:\n",
    "        C[row, col] = tmp\n",
    "\n",
    "\n",
    "# Benchmark the optimized version\n",
    "d_C = cuda.device_array((1024, 1024), dtype=np.float32)\n",
    "\n",
    "# Warm up\n",
    "for _ in range(3):\n",
    "    matmul_tiled_noconflict_kernel[blocks, threads_per_block](d_A, d_B, d_C)\n",
    "cuda.synchronize()\n",
    "\n",
    "# Benchmark\n",
    "start = time.perf_counter()\n",
    "for _ in range(iterations):\n",
    "    matmul_tiled_noconflict_kernel[blocks, threads_per_block](d_A, d_B, d_C)\n",
    "cuda.synchronize()\n",
    "time_noconflict = (time.perf_counter() - start) / iterations\n",
    "\n",
    "gflops_noconflict = flops / time_noconflict / 1e9\n",
    "\n",
    "print(f\"\\nBank conflict-free version:\")\n",
    "print(f\"  Time: {time_noconflict*1000:.3f} ms\")\n",
    "print(f\"  GFLOPS: {gflops_noconflict:.1f}\")\n",
    "print(f\"  Improvement: {time_tiled_gpu/time_noconflict:.2f}x vs basic tiled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš ï¸ Common Mistakes\n",
    "\n",
    "### Mistake 1: Wrong Tile Loading Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âŒ WRONG: Loading A incorrectly\n",
    "# sA[ty, tx] = A[ty, tile_idx * TILE_SIZE + tx]  # Wrong row!\n",
    "\n",
    "# âœ… CORRECT: Use global row index\n",
    "# sA[ty, tx] = A[row, tile_idx * TILE_SIZE + tx]\n",
    "\n",
    "print(\"ğŸ’¡ Common confusion:\")\n",
    "print(\"   - ty, tx are LOCAL indices (0-15 for 16x16 tiles)\")\n",
    "print(\"   - row, col are GLOBAL indices (0-1023 for 1024x1024 matrix)\")\n",
    "print(\"   - Use LOCAL for shared memory, GLOBAL for main memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Missing Second syncthreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âŒ WRONG: Missing sync after computation\n",
    "# for tile_idx in range(num_tiles):\n",
    "#     sA[ty, tx] = A[row, tile_idx * TILE_SIZE + tx]\n",
    "#     sB[ty, tx] = B[tile_idx * TILE_SIZE + ty, col]\n",
    "#     cuda.syncthreads()  # Wait for load\n",
    "#     for k in range(TILE_SIZE):\n",
    "#         tmp += sA[ty, k] * sB[k, tx]\n",
    "#     # Missing syncthreads() here!\n",
    "#     # Next iteration overwrites shared memory while some threads still reading!\n",
    "\n",
    "# âœ… CORRECT: Sync after both load AND compute\n",
    "print(\"ğŸ’¡ You need TWO syncthreads per tile iteration:\")\n",
    "print(\"   1. After loading tile (before computation)\")\n",
    "print(\"   2. After computation (before loading next tile)\")\n",
    "print(\"   Missing either causes race conditions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Not Handling Matrix Size Not Divisible by Tile Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with non-tile-aligned size\n",
    "A_odd = np.random.randn(500, 500).astype(np.float32)\n",
    "B_odd = np.random.randn(500, 500).astype(np.float32)\n",
    "\n",
    "result_odd = matmul_gpu_tiled(A_odd, B_odd)\n",
    "expected_odd = A_odd @ B_odd\n",
    "\n",
    "print(f\"Testing 500Ã—500 (not divisible by 16):\")\n",
    "print(f\"  Correct: {np.allclose(result_odd, expected_odd, rtol=1e-4)}\")\n",
    "print(\"\\nğŸ’¡ Our implementation handles this with bounds checking:\")\n",
    "print(\"   - if row < M and a_col < K: sA[ty, tx] = A[row, a_col]\")\n",
    "print(\"   - else: sA[ty, tx] = 0.0  # Zero padding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ‹ Try It Yourself: Implement Larger Tiles\n",
    "\n",
    "**Challenge:** Modify the tiled kernel to use 32Ã—32 tiles instead of 16Ã—16.\n",
    "\n",
    "Considerations:\n",
    "- More data reuse (should be faster)\n",
    "- Uses more shared memory (check limits!)\n",
    "- 32Ã—32 = 1024 threads per block (exactly the max on most GPUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement 32Ã—32 tiled matrix multiplication\n",
    "\n",
    "TILE_SIZE_32 = 32\n",
    "\n",
    "@cuda.jit\n",
    "def matmul_tiled_32_kernel(A, B, C):\n",
    "    \"\"\"\n",
    "    Tiled matmul with 32Ã—32 tiles.\n",
    "    \n",
    "    TODO: Implement this!\n",
    "    \n",
    "    Hints:\n",
    "    - Same logic as 16Ã—16 version\n",
    "    - Change TILE_SIZE constant\n",
    "    - Make sure shared memory fits (32Ã—32Ã—4 = 4KB per tile, 8KB total)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test when implemented:\n",
    "# threads_per_block_32 = (32, 32)\n",
    "# blocks_32 = (32, 32)  # 1024 / 32 = 32\n",
    "# matmul_tiled_32_kernel[blocks_32, threads_per_block_32](d_A, d_B, d_C)\n",
    "# result = d_C.copy_to_host()\n",
    "# print(f\"32Ã—32 tiles correct: {np.allclose(result, C_reference, rtol=1e-4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ğŸ’¡ Hint</summary>\n",
    "\n",
    "The code is nearly identical! Just change:\n",
    "```python\n",
    "TILE_SIZE_32 = 32\n",
    "sA = cuda.shared.array(shape=(32, 32), dtype=float32)\n",
    "sB = cuda.shared.array(shape=(32, 32), dtype=float32)\n",
    "```\n",
    "\n",
    "And launch with:\n",
    "```python\n",
    "threads_per_block = (32, 32)\n",
    "blocks = (1024 // 32, 1024 // 32)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ Checkpoint\n",
    "\n",
    "Congratulations! You've learned:\n",
    "\n",
    "- âœ… **Why matmul is fundamental** - It's 90%+ of neural network compute\n",
    "- âœ… **Memory access patterns** - Why naive is slow, tiled is fast\n",
    "- âœ… **Shared memory tiling** - The most important GPU optimization\n",
    "- âœ… **Bank conflicts** - And how to avoid them\n",
    "- âœ… **Benchmarking against cuBLAS** - Industry-standard reference\n",
    "\n",
    "You achieved performance within **2-5x of cuBLAS** with just ~50 lines of CUDA Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ Challenge (Optional)\n",
    "\n",
    "**Advanced Challenge: Implement Register Blocking**\n",
    "\n",
    "Instead of each thread computing one output element, have each thread compute a small tile (e.g., 4Ã—4) of the output.\n",
    "\n",
    "```python\n",
    "# Each thread has 16 accumulators in registers:\n",
    "c00, c01, c02, c03 = 0.0, 0.0, 0.0, 0.0\n",
    "c10, c11, c12, c13 = 0.0, 0.0, 0.0, 0.0\n",
    "c20, c21, c22, c23 = 0.0, 0.0, 0.0, 0.0\n",
    "c30, c31, c32, c33 = 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "# Load 4 elements of A row, 4 elements of B column into registers\n",
    "# Compute 16 FMAs per load - better arithmetic intensity!\n",
    "```\n",
    "\n",
    "This technique, combined with prefetching, is how cuBLAS achieves near-peak performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“– Further Reading\n",
    "\n",
    "- [Volkov's Better Performance at Lower Occupancy](https://www.nvidia.com/content/GTC-2010/pdfs/2238_GTC2010.pdf) - Classic paper on register blocking\n",
    "- [CUTLASS GEMM Tutorial](https://github.com/NVIDIA/cutlass/blob/master/media/docs/gemm_api.md) - NVIDIA's template library for matmul\n",
    "- [Why GEMM is at the Heart of Deep Learning](https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/)\n",
    "- [Tensor Core Programming](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#wmma)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import gc\n\n# Clean up large arrays\nif 'A_1024' in dir():\n    del A_1024\nif 'B_1024' in dir():\n    del B_1024\nif 'C_reference' in dir():\n    del C_reference\nif 'A_small_np' in dir():\n    del A_small_np\nif 'A_small_cp' in dir():\n    del A_small_cp\nif 'A_odd' in dir():\n    del A_odd\nif 'B_odd' in dir():\n    del B_odd\nif 'd_A' in dir():\n    del d_A\nif 'd_B' in dir():\n    del d_B\nif 'd_C' in dir():\n    del d_C\n\nif HAS_CUPY:\n    if 'cp_A' in dir():\n        del cp_A\n    if 'cp_B' in dir():\n        del cp_B\n    if 'cp_C' in dir():\n        del cp_C\n    cp.get_default_memory_pool().free_all_blocks()\n\ngc.collect()\n\n# Clear CUDA context\ntry:\n    cuda.current_context().reset()\nexcept Exception:\n    pass  # Context might not exist\n\nprint(\"âœ… GPU memory cleared!\")\nprint(\"\\nâ¡ï¸ Ready for Lab 1.3.3: Custom Embedding Lookup\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}