{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.3.2: Matrix Multiplication - SOLUTIONS\n",
    "\n",
    "This notebook contains complete solutions to the exercises in Lab 1.3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from numba import cuda, float32\n",
    "\n",
    "print(f\"CUDA available: {cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: 32×32 Tiled Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_SIZE_32 = 32\n",
    "\n",
    "@cuda.jit\n",
    "def matmul_tiled_32_kernel(A, B, C):\n",
    "    \"\"\"\n",
    "    SOLUTION: Tiled matmul with 32×32 tiles.\n",
    "    \n",
    "    Benefits:\n",
    "    - More data reuse (32×32 = 1024 elements per tile vs 256)\n",
    "    - Full utilization of 1024 threads per block\n",
    "    - Better memory bandwidth efficiency\n",
    "    \"\"\"\n",
    "    # Shared memory for larger tiles\n",
    "    sA = cuda.shared.array(shape=(32, 32), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(32, 32), dtype=float32)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bx = cuda.blockIdx.x\n",
    "    by = cuda.blockIdx.y\n",
    "    \n",
    "    row = by * 32 + ty\n",
    "    col = bx * 32 + tx\n",
    "    \n",
    "    M, K = A.shape\n",
    "    K2, N = B.shape\n",
    "    \n",
    "    tmp = float32(0.0)\n",
    "    num_tiles = (K + 31) // 32\n",
    "    \n",
    "    for tile_idx in range(num_tiles):\n",
    "        # Load tile of A\n",
    "        a_col = tile_idx * 32 + tx\n",
    "        if row < M and a_col < K:\n",
    "            sA[ty, tx] = A[row, a_col]\n",
    "        else:\n",
    "            sA[ty, tx] = 0.0\n",
    "        \n",
    "        # Load tile of B\n",
    "        b_row = tile_idx * 32 + ty\n",
    "        if b_row < K and col < N:\n",
    "            sB[ty, tx] = B[b_row, col]\n",
    "        else:\n",
    "            sB[ty, tx] = 0.0\n",
    "        \n",
    "        cuda.syncthreads()\n",
    "        \n",
    "        # Compute partial dot product\n",
    "        for k in range(32):\n",
    "            tmp += sA[ty, k] * sB[k, tx]\n",
    "        \n",
    "        cuda.syncthreads()\n",
    "    \n",
    "    if row < M and col < N:\n",
    "        C[row, col] = tmp\n",
    "\n",
    "\n",
    "def matmul_gpu_32(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Matrix multiplication using 32×32 tiles.\"\"\"\n",
    "    M, K = A.shape\n",
    "    K2, N = B.shape\n",
    "    assert K == K2\n",
    "    \n",
    "    d_A = cuda.to_device(A)\n",
    "    d_B = cuda.to_device(B)\n",
    "    d_C = cuda.device_array((M, N), dtype=np.float32)\n",
    "    \n",
    "    threads_per_block = (32, 32)  # 1024 threads per block (max!)\n",
    "    blocks_x = (N + 31) // 32\n",
    "    blocks_y = (M + 31) // 32\n",
    "    blocks = (blocks_x, blocks_y)\n",
    "    \n",
    "    matmul_tiled_32_kernel[blocks, threads_per_block](d_A, d_B, d_C)\n",
    "    \n",
    "    return d_C.copy_to_host()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the solution\n",
    "np.random.seed(42)\n",
    "A = np.random.randn(1024, 1024).astype(np.float32)\n",
    "B = np.random.randn(1024, 1024).astype(np.float32)\n",
    "\n",
    "C_reference = A @ B\n",
    "C_gpu = matmul_gpu_32(A, B)\n",
    "\n",
    "print(\"32×32 Tiled Matrix Multiplication Test\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Matrix size: 1024×1024\")\n",
    "print(f\"Correct: {np.allclose(C_gpu, C_reference, rtol=1e-4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark comparison: 16×16 vs 32×32\n",
    "TILE_SIZE_16 = 16\n",
    "\n",
    "@cuda.jit\n",
    "def matmul_tiled_16_kernel(A, B, C):\n",
    "    \"\"\"16×16 tiled version for comparison.\"\"\"\n",
    "    sA = cuda.shared.array(shape=(16, 16), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(16, 16), dtype=float32)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bx = cuda.blockIdx.x\n",
    "    by = cuda.blockIdx.y\n",
    "    \n",
    "    row = by * 16 + ty\n",
    "    col = bx * 16 + tx\n",
    "    \n",
    "    M, K = A.shape\n",
    "    K2, N = B.shape\n",
    "    \n",
    "    tmp = float32(0.0)\n",
    "    num_tiles = (K + 15) // 16\n",
    "    \n",
    "    for tile_idx in range(num_tiles):\n",
    "        a_col = tile_idx * 16 + tx\n",
    "        if row < M and a_col < K:\n",
    "            sA[ty, tx] = A[row, a_col]\n",
    "        else:\n",
    "            sA[ty, tx] = 0.0\n",
    "        \n",
    "        b_row = tile_idx * 16 + ty\n",
    "        if b_row < K and col < N:\n",
    "            sB[ty, tx] = B[b_row, col]\n",
    "        else:\n",
    "            sB[ty, tx] = 0.0\n",
    "        \n",
    "        cuda.syncthreads()\n",
    "        \n",
    "        for k in range(16):\n",
    "            tmp += sA[ty, k] * sB[k, tx]\n",
    "        \n",
    "        cuda.syncthreads()\n",
    "    \n",
    "    if row < M and col < N:\n",
    "        C[row, col] = tmp\n",
    "\n",
    "\n",
    "# Benchmark\n",
    "print(\"\\nBenchmark: 16×16 vs 32×32 Tiles\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "d_A = cuda.to_device(A)\n",
    "d_B = cuda.to_device(B)\n",
    "d_C = cuda.device_array((1024, 1024), dtype=np.float32)\n",
    "\n",
    "# 16×16 tiles\n",
    "for _ in range(3):\n",
    "    matmul_tiled_16_kernel[(64, 64), (16, 16)](d_A, d_B, d_C)\n",
    "cuda.synchronize()\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(10):\n",
    "    matmul_tiled_16_kernel[(64, 64), (16, 16)](d_A, d_B, d_C)\n",
    "cuda.synchronize()\n",
    "time_16 = (time.perf_counter() - start) / 10\n",
    "\n",
    "# 32×32 tiles\n",
    "for _ in range(3):\n",
    "    matmul_tiled_32_kernel[(32, 32), (32, 32)](d_A, d_B, d_C)\n",
    "cuda.synchronize()\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(10):\n",
    "    matmul_tiled_32_kernel[(32, 32), (32, 32)](d_A, d_B, d_C)\n",
    "cuda.synchronize()\n",
    "time_32 = (time.perf_counter() - start) / 10\n",
    "\n",
    "flops = 2 * 1024 * 1024 * 1024\n",
    "print(f\"16×16 tiles: {time_16*1000:.3f} ms, {flops/time_16/1e9:.1f} GFLOPS\")\n",
    "print(f\"32×32 tiles: {time_32*1000:.3f} ms, {flops/time_32/1e9:.1f} GFLOPS\")\n",
    "print(f\"\\n32×32 is {time_16/time_32:.2f}x faster than 16×16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Register Blocking Concept\n",
    "\n",
    "Each thread computes a small tile (e.g., 2×2 or 4×4) of output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual code - register blocking\n",
    "print(\"\"\"\n",
    "Register Blocking Concept:\n",
    "\n",
    "Instead of each thread computing 1 output element:\n",
    "  Thread 0: C[0,0]\n",
    "  Thread 1: C[0,1]\n",
    "  ...\n",
    "\n",
    "Each thread computes a 2×2 block:\n",
    "  Thread 0: C[0,0], C[0,1], C[1,0], C[1,1]\n",
    "  Thread 1: C[0,2], C[0,3], C[1,2], C[1,3]\n",
    "  ...\n",
    "\n",
    "Benefits:\n",
    "- 4 outputs per thread = 4× more arithmetic per memory load\n",
    "- Better arithmetic intensity (compute/memory ratio)\n",
    "- Uses registers more efficiently\n",
    "- This is how cuBLAS achieves peak performance!\n",
    "\n",
    "Trade-off:\n",
    "- More registers per thread = lower occupancy\n",
    "- But often worth it for compute-bound kernels\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del A, B, C_reference, C_gpu, d_A, d_B, d_C\n",
    "gc.collect()\n",
    "cuda.current_context().reset()\n",
    "print(\"✅ Cleanup complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
