{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 1.3.1: Parallel Reduction - SOLUTIONS\n\nThis notebook contains complete solutions to the exercises in Lab 1.3.1.\n\n---\n\n## ðŸŽ¯ Learning Objectives Checklist\n\nBy completing this lab, you should now be able to:\n- [x] Understand why parallel reduction is fundamental to GPU computing\n- [x] Implement naive parallel reduction and understand its limitations\n- [x] Optimize with shared memory for 10-50x speedup\n- [x] Master warp shuffle operations for ultimate performance\n- [x] Achieve **100x+ speedup** over CPU implementations\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from numba import cuda\n",
    "import numba\n",
    "\n",
    "# Verify CUDA\n",
    "print(f\"CUDA available: {cuda.is_available()}\")\n",
    "\n",
    "WARP_SIZE = 32\n",
    "THREADS_PER_BLOCK = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Parallel Max Reduction\n",
    "\n",
    "The exercise asked to implement parallel max reduction instead of sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit(device=True)\n",
    "def warp_max(val):\n",
    "    \"\"\"\n",
    "    SOLUTION: Find max within a warp using shuffle instructions.\n",
    "    \n",
    "    Key change: Replace += with max()\n",
    "    \"\"\"\n",
    "    mask = 0xffffffff\n",
    "    \n",
    "    # Shuffle down and take max in tree pattern\n",
    "    val = max(val, cuda.shfl_down_sync(mask, val, 16))\n",
    "    val = max(val, cuda.shfl_down_sync(mask, val, 8))\n",
    "    val = max(val, cuda.shfl_down_sync(mask, val, 4))\n",
    "    val = max(val, cuda.shfl_down_sync(mask, val, 2))\n",
    "    val = max(val, cuda.shfl_down_sync(mask, val, 1))\n",
    "    \n",
    "    return val\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def parallel_max_kernel(data, partial_maxes):\n",
    "    \"\"\"\n",
    "    SOLUTION: Find max value using parallel reduction.\n",
    "    \"\"\"\n",
    "    # Shared memory for warp results\n",
    "    warp_maxes = cuda.shared.array(shape=(THREADS_PER_BLOCK // WARP_SIZE,), dtype=numba.float32)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    bx = cuda.blockIdx.x\n",
    "    block_size = cuda.blockDim.x\n",
    "    idx = bx * block_size + tx\n",
    "    \n",
    "    warp_id = tx // WARP_SIZE\n",
    "    lane_id = tx % WARP_SIZE\n",
    "    \n",
    "    # Load data - use negative infinity for out-of-bounds\n",
    "    val = data[idx] if idx < data.size else numba.float32(-1e38)  # -inf equivalent\n",
    "    \n",
    "    # Warp-level max reduction\n",
    "    val = warp_max(val)\n",
    "    \n",
    "    # Lane 0 of each warp stores to shared memory\n",
    "    if lane_id == 0:\n",
    "        warp_maxes[warp_id] = val\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # First warp reduces all warp maxes\n",
    "    num_warps = block_size // WARP_SIZE\n",
    "    if warp_id == 0:\n",
    "        val = warp_maxes[lane_id] if lane_id < num_warps else numba.float32(-1e38)\n",
    "        val = warp_max(val)\n",
    "        \n",
    "        if lane_id == 0:\n",
    "            partial_maxes[bx] = val\n",
    "\n",
    "\n",
    "def gpu_max(arr: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    SOLUTION: Find max value using GPU.\n",
    "    \"\"\"\n",
    "    n = arr.size\n",
    "    d_data = cuda.to_device(arr)\n",
    "    \n",
    "    while n > 1:\n",
    "        num_blocks = (n + THREADS_PER_BLOCK - 1) // THREADS_PER_BLOCK\n",
    "        d_partial = cuda.device_array(num_blocks, dtype=np.float32)\n",
    "        \n",
    "        parallel_max_kernel[num_blocks, THREADS_PER_BLOCK](d_data, d_partial)\n",
    "        \n",
    "        d_data = d_partial\n",
    "        n = num_blocks\n",
    "    \n",
    "    return d_data.copy_to_host()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the solution\n",
    "test_data = np.random.randn(1_000_000).astype(np.float32)\n",
    "expected_max = np.max(test_data)\n",
    "\n",
    "result = gpu_max(test_data)\n",
    "\n",
    "print(\"Parallel Max Reduction Test\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Array size: {test_data.size:,}\")\n",
    "print(f\"Expected max (NumPy): {expected_max}\")\n",
    "print(f\"GPU max:              {result}\")\n",
    "print(f\"Correct: {'âœ… Yes!' if abs(result - expected_max) < 1e-5 else 'âŒ No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "print(\"\\nBenchmark: GPU Max vs NumPy Max\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Warm up\n",
    "for _ in range(3):\n",
    "    _ = gpu_max(test_data)\n",
    "cuda.synchronize()\n",
    "\n",
    "# NumPy time\n",
    "start = time.perf_counter()\n",
    "for _ in range(10):\n",
    "    _ = np.max(test_data)\n",
    "time_numpy = (time.perf_counter() - start) / 10\n",
    "\n",
    "# GPU time\n",
    "start = time.perf_counter()\n",
    "for _ in range(10):\n",
    "    _ = gpu_max(test_data)\n",
    "    cuda.synchronize()\n",
    "time_gpu = (time.perf_counter() - start) / 10\n",
    "\n",
    "print(f\"NumPy time: {time_numpy*1000:.3f} ms\")\n",
    "print(f\"GPU time:   {time_gpu*1000:.3f} ms\")\n",
    "print(f\"Speedup:    {time_numpy/time_gpu:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Solution: Parallel Min and ArgMax\n",
    "\n",
    "Here are additional reductions you might want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit(device=True)\n",
    "def warp_min(val):\n",
    "    \"\"\"Find min within a warp.\"\"\"\n",
    "    mask = 0xffffffff\n",
    "    val = min(val, cuda.shfl_down_sync(mask, val, 16))\n",
    "    val = min(val, cuda.shfl_down_sync(mask, val, 8))\n",
    "    val = min(val, cuda.shfl_down_sync(mask, val, 4))\n",
    "    val = min(val, cuda.shfl_down_sync(mask, val, 2))\n",
    "    val = min(val, cuda.shfl_down_sync(mask, val, 1))\n",
    "    return val\n",
    "\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def warp_argmax(val, idx):\n",
    "    \"\"\"\n",
    "    Find argmax within a warp.\n",
    "    Returns both the max value and its original index.\n",
    "    \"\"\"\n",
    "    mask = 0xffffffff\n",
    "    \n",
    "    for offset in [16, 8, 4, 2, 1]:\n",
    "        other_val = cuda.shfl_down_sync(mask, val, offset)\n",
    "        other_idx = cuda.shfl_down_sync(mask, idx, offset)\n",
    "        \n",
    "        if other_val > val:\n",
    "            val = other_val\n",
    "            idx = other_idx\n",
    "    \n",
    "    return val, idx\n",
    "\n",
    "\n",
    "print(\"Additional reduction operations implemented:\")\n",
    "print(\"- warp_min(): Parallel minimum\")\n",
    "print(\"- warp_argmax(): Parallel argmax (returns value and index)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Solution: Double-Precision Reduction with Kahan Summation\n",
    "\n",
    "For numerically stable summation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def kahan_sum_kernel(data, partial_sums, partial_comps):\n",
    "    \"\"\"\n",
    "    Kahan summation for improved numerical accuracy.\n",
    "    \n",
    "    Kahan's algorithm tracks a \"compensation\" term that captures\n",
    "    the low-order bits lost to rounding.\n",
    "    \n",
    "    sum = 0\n",
    "    c = 0  # compensation\n",
    "    for x in data:\n",
    "        y = x - c        # compensated value\n",
    "        t = sum + y      # new sum\n",
    "        c = (t - sum) - y  # lost low-order bits\n",
    "        sum = t\n",
    "    \"\"\"\n",
    "    sdata = cuda.shared.array(shape=(THREADS_PER_BLOCK,), dtype=numba.float64)\n",
    "    scomp = cuda.shared.array(shape=(THREADS_PER_BLOCK,), dtype=numba.float64)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    bx = cuda.blockIdx.x\n",
    "    idx = bx * cuda.blockDim.x + tx\n",
    "    \n",
    "    # Initialize\n",
    "    if idx < data.size:\n",
    "        sdata[tx] = data[idx]\n",
    "    else:\n",
    "        sdata[tx] = 0.0\n",
    "    scomp[tx] = 0.0\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Reduction with Kahan compensation\n",
    "    stride = cuda.blockDim.x // 2\n",
    "    while stride > 0:\n",
    "        if tx < stride:\n",
    "            # Kahan sum: sdata[tx] + sdata[tx + stride]\n",
    "            y = sdata[tx + stride] - scomp[tx]\n",
    "            t = sdata[tx] + y\n",
    "            scomp[tx] = (t - sdata[tx]) - y\n",
    "            sdata[tx] = t\n",
    "        cuda.syncthreads()\n",
    "        stride //= 2\n",
    "    \n",
    "    if tx == 0:\n",
    "        partial_sums[bx] = sdata[0]\n",
    "        partial_comps[bx] = scomp[0]\n",
    "\n",
    "\n",
    "print(\"Kahan summation kernel implemented for improved numerical accuracy.\")\n",
    "print(\"\\nWhen to use:\")\n",
    "print(\"- Summing many small numbers with a few large ones\")\n",
    "print(\"- When numerical precision is critical\")\n",
    "print(\"- Scientific computing applications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del test_data\n",
    "gc.collect()\n",
    "cuda.current_context().reset()\n",
    "print(\"âœ… Cleanup complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}