{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 1.3.3: Custom Embedding Lookup - SOLUTIONS\n\nThis notebook contains complete solutions to the exercises in Lab 1.3.3.\n\n---\n\n## ðŸŽ¯ Learning Objectives Checklist\n\nBy completing this lab, you should now be able to:\n- [x] Understand how PyTorch's `nn.Embedding` works internally\n- [x] Write a custom CUDA kernel for batched embedding lookup\n- [x] Optimize memory access patterns for embedding tables\n- [x] Compare custom kernel performance with PyTorch's implementation\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "import numba\n",
    "\n",
    "print(f\"CUDA available: {cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Combined Token + Position Embedding Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def combined_embedding_kernel(token_embed_table, pos_embed_table, \n",
    "                               token_ids, positions, output):\n",
    "    \"\"\"\n",
    "    SOLUTION: Combined token and positional embedding lookup.\n",
    "    \n",
    "    This fuses two lookups into one kernel, saving memory bandwidth.\n",
    "    Output = token_embedding[token_id] + pos_embedding[position]\n",
    "    \"\"\"\n",
    "    token_idx = cuda.blockIdx.x\n",
    "    tx = cuda.threadIdx.x\n",
    "    \n",
    "    _, embedding_dim = token_embed_table.shape\n",
    "    total_tokens = token_ids.shape[0]\n",
    "    \n",
    "    if token_idx >= total_tokens:\n",
    "        return\n",
    "    \n",
    "    # Get the token ID and position for this token\n",
    "    token_id = token_ids[token_idx]\n",
    "    position = positions[token_idx]\n",
    "    \n",
    "    # Each thread handles multiple embedding dimensions\n",
    "    for embed_idx in range(tx, embedding_dim, cuda.blockDim.x):\n",
    "        # Add both embeddings in one fused operation!\n",
    "        output[token_idx, embed_idx] = (\n",
    "            token_embed_table[token_id, embed_idx] + \n",
    "            pos_embed_table[position, embed_idx]\n",
    "        )\n",
    "\n",
    "\n",
    "def combined_embedding_lookup(token_embed: np.ndarray, pos_embed: np.ndarray,\n",
    "                               token_ids: np.ndarray, positions: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    GPU combined embedding lookup.\n",
    "    \n",
    "    Args:\n",
    "        token_embed: (vocab_size, embedding_dim) token embeddings\n",
    "        pos_embed: (max_seq_length, embedding_dim) positional embeddings\n",
    "        token_ids: (batch_size, seq_length) token IDs\n",
    "        positions: (batch_size, seq_length) position indices\n",
    "    \n",
    "    Returns:\n",
    "        (batch_size, seq_length, embedding_dim) combined embeddings\n",
    "    \"\"\"\n",
    "    original_shape = token_ids.shape\n",
    "    _, embedding_dim = token_embed.shape\n",
    "    \n",
    "    # Flatten inputs\n",
    "    flat_tokens = token_ids.flatten().astype(np.int32)\n",
    "    flat_positions = positions.flatten().astype(np.int32)\n",
    "    total_tokens = flat_tokens.shape[0]\n",
    "    \n",
    "    # Transfer to GPU\n",
    "    d_token_embed = cuda.to_device(token_embed.astype(np.float32))\n",
    "    d_pos_embed = cuda.to_device(pos_embed.astype(np.float32))\n",
    "    d_tokens = cuda.to_device(flat_tokens)\n",
    "    d_positions = cuda.to_device(flat_positions)\n",
    "    d_output = cuda.device_array((total_tokens, embedding_dim), dtype=np.float32)\n",
    "    \n",
    "    # Launch kernel\n",
    "    threads = min(256, embedding_dim)\n",
    "    blocks = total_tokens\n",
    "    \n",
    "    combined_embedding_kernel[blocks, threads](\n",
    "        d_token_embed, d_pos_embed, d_tokens, d_positions, d_output\n",
    "    )\n",
    "    \n",
    "    # Get result and reshape\n",
    "    result = d_output.copy_to_host()\n",
    "    return result.reshape(*original_shape, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the solution\n",
    "np.random.seed(42)\n",
    "\n",
    "vocab_size = 100\n",
    "max_seq_length = 512\n",
    "embedding_dim = 64\n",
    "batch_size = 4\n",
    "seq_length = 10\n",
    "\n",
    "# Create embedding tables\n",
    "token_embed = np.random.randn(vocab_size, embedding_dim).astype(np.float32)\n",
    "pos_embed = np.random.randn(max_seq_length, embedding_dim).astype(np.float32)\n",
    "\n",
    "# Create input data\n",
    "token_ids = np.random.randint(0, vocab_size, (batch_size, seq_length), dtype=np.int32)\n",
    "positions = np.tile(np.arange(seq_length), (batch_size, 1)).astype(np.int32)\n",
    "\n",
    "# GPU result\n",
    "result_gpu = combined_embedding_lookup(token_embed, pos_embed, token_ids, positions)\n",
    "\n",
    "# CPU reference\n",
    "result_cpu = token_embed[token_ids] + pos_embed[positions]\n",
    "\n",
    "print(\"Combined Embedding Lookup Test\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Token embedding: {token_embed.shape}\")\n",
    "print(f\"Position embedding: {pos_embed.shape}\")\n",
    "print(f\"Input shape: {token_ids.shape}\")\n",
    "print(f\"Output shape: {result_gpu.shape}\")\n",
    "print(f\"\\nCorrect: {np.allclose(result_gpu, result_cpu)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify specific values\n",
    "print(\"\\nVerification (first token of first sequence):\")\n",
    "print(f\"  Token ID: {token_ids[0, 0]}, Position: {positions[0, 0]}\")\n",
    "print(f\"  Token embedding[:5]: {token_embed[token_ids[0, 0], :5]}\")\n",
    "print(f\"  Position embedding[:5]: {pos_embed[positions[0, 0], :5]}\")\n",
    "print(f\"  Expected sum[:5]: {token_embed[token_ids[0, 0], :5] + pos_embed[positions[0, 0], :5]}\")\n",
    "print(f\"  GPU result[:5]: {result_gpu[0, 0, :5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: RoPE (Rotary Position Embedding) Concept\n",
    "\n",
    "Modern LLMs like Llama use rotary position embeddings instead of additive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "Rotary Position Embedding (RoPE):\n",
    "\n",
    "Instead of: output = token_emb + pos_emb\n",
    "RoPE uses:  output = rotate(token_emb, angle(position))\n",
    "\n",
    "The rotation is applied in 2D subspaces:\n",
    "  [x, y] -> [x*cos(Î¸) - y*sin(Î¸), x*sin(Î¸) + y*cos(Î¸)]\n",
    "\n",
    "Where Î¸ depends on position and dimension:\n",
    "  Î¸_d(pos) = pos * 10000^(-2d/D)\n",
    "\n",
    "Benefits:\n",
    "- Relative position information (token at pos 5 always \"sees\" pos 3 the same way)\n",
    "- Extrapolates to longer sequences\n",
    "- No additional parameters to learn\n",
    "\n",
    "CUDA Implementation Idea:\n",
    "- Precompute sin/cos tables for all positions\n",
    "- Apply rotation element-wise during attention\n",
    "- Can be fused with attention kernel for efficiency\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "cuda.current_context().reset()\n",
    "print(\"âœ… Cleanup complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}