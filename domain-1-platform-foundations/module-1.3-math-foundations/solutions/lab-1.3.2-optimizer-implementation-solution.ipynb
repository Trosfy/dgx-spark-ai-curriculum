{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.3.2: Optimizer Implementation - SOLUTIONS\n",
    "\n",
    "This notebook contains complete solutions to all exercises in the optimizer implementation lab.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ðŸ“ Note:** This solution notebook is designed to be self-contained and includes all necessary\n",
    "> helper functions. However, if you prefer to use your own implementations from the main notebook,\n",
    "> ensure those cells have been run first.\n",
    ">\n",
    "> You can also import production-ready implementations from the scripts:\n",
    "> ```python\n",
    "> from scripts.math_utils import sigmoid, relu, Adam, SGD\n",
    "> from scripts.visualization_utils import plot_loss_landscape, plot_training_curve\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Solutions Notebook - Optimizer Implementation\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions from the main notebook\n",
    "\n",
    "def rosenbrock(x, y, a=1, b=100):\n",
    "    \"\"\"Rosenbrock function\"\"\"\n",
    "    return (a - x)**2 + b * (y - x**2)**2\n",
    "\n",
    "def rosenbrock_gradient(x, y, a=1, b=100):\n",
    "    \"\"\"Gradient of Rosenbrock function\"\"\"\n",
    "    df_dx = -2*(a - x) - 4*b*x*(y - x**2)\n",
    "    df_dy = 2*b*(y - x**2)\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "def optimize(optimizer, start_point, gradient_fn, n_steps=1000):\n",
    "    \"\"\"Run optimization and track history\"\"\"\n",
    "    optimizer.reset()\n",
    "    params = start_point.copy()\n",
    "    history = {'params': [params.copy()], 'loss': [rosenbrock(params[0], params[1])]}\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        grads = gradient_fn(params[0], params[1])\n",
    "        params = optimizer.step(params, grads)\n",
    "        loss = rosenbrock(params[0], params[1])\n",
    "        history['params'].append(params.copy())\n",
    "        history['loss'].append(loss)\n",
    "        if loss < 1e-10:\n",
    "            break\n",
    "    \n",
    "    return history\n",
    "\n",
    "start_point = np.array([-1.0, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution 1: RMSprop Optimizer\n",
    "\n",
    "**Exercise:** Implement RMSprop optimizer.\n",
    "\n",
    "RMSprop maintains a moving average of squared gradients and uses it to normalize the gradient:\n",
    "\n",
    "$$v_t = \\beta \\cdot v_{t-1} + (1-\\beta) \\cdot g_t^2$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{v_t} + \\epsilon} \\cdot g_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: RMSprop optimizer\n",
    "\n",
    "class RMSprop:\n",
    "    \"\"\"\n",
    "    RMSprop optimizer.\n",
    "    \n",
    "    RMSprop adapts the learning rate for each parameter based on the\n",
    "    magnitude of recent gradients. Parameters with large gradients get\n",
    "    smaller updates, and vice versa.\n",
    "    \n",
    "    Update rules:\n",
    "        v = Î² Ã— v + (1-Î²) Ã— gÂ²     (exponential moving average of squared gradients)\n",
    "        Î¸ = Î¸ - lr Ã— g / (âˆšv + Îµ)  (normalized update)\n",
    "    \n",
    "    Args:\n",
    "        lr: Learning rate (default 0.01)\n",
    "        beta: Decay rate for moving average (default 0.9)\n",
    "        epsilon: Small constant for numerical stability (default 1e-8)\n",
    "    \n",
    "    Example:\n",
    "        >>> optimizer = RMSprop(lr=0.01, beta=0.9)\n",
    "        >>> params = optimizer.step(params, grads)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr=0.01, beta=0.9, epsilon=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        self.v = None  # Moving average of squared gradients\n",
    "        self.name = f\"RMSprop (lr={lr}, Î²={beta})\"\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        \"\"\"\n",
    "        Perform one RMSprop optimization step.\n",
    "        \n",
    "        Args:\n",
    "            params: Current parameters (numpy array)\n",
    "            grads: Gradients at current parameters\n",
    "            \n",
    "        Returns:\n",
    "            Updated parameters\n",
    "        \"\"\"\n",
    "        # Initialize v on first call\n",
    "        if self.v is None:\n",
    "            self.v = np.zeros_like(params)\n",
    "        \n",
    "        # Update moving average of squared gradients\n",
    "        # v = Î²*v + (1-Î²)*gÂ²\n",
    "        self.v = self.beta * self.v + (1 - self.beta) * (grads ** 2)\n",
    "        \n",
    "        # Update parameters with normalized gradient\n",
    "        # Î¸ = Î¸ - lr * g / (âˆšv + Îµ)\n",
    "        params = params - self.lr * grads / (np.sqrt(self.v) + self.epsilon)\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset optimizer state\"\"\"\n",
    "        self.v = None\n",
    "\n",
    "# Test RMSprop\n",
    "print(\"Testing RMSprop Optimizer\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "rmsprop = RMSprop(lr=0.01, beta=0.9)\n",
    "history_rmsprop = optimize(rmsprop, start_point, rosenbrock_gradient, n_steps=5000)\n",
    "\n",
    "final_params = history_rmsprop['params'][-1]\n",
    "final_loss = history_rmsprop['loss'][-1]\n",
    "\n",
    "print(f\"Start: ({start_point[0]}, {start_point[1]})\")\n",
    "print(f\"Goal:  (1.0, 1.0)\")\n",
    "print(f\"Final: ({final_params[0]:.6f}, {final_params[1]:.6f})\")\n",
    "print(f\"Final loss: {final_loss:.6e}\")\n",
    "print(f\"Steps: {len(history_rmsprop['loss']) - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RMSprop performance\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(history_rmsprop['loss'], 'b-', linewidth=2)\n",
    "plt.xlabel('Step', fontsize=12)\n",
    "plt.ylabel('Loss (log scale)', fontsize=12)\n",
    "plt.title('RMSprop Convergence on Rosenbrock Function', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=1e-6, color='r', linestyle='--', label='Target: 1e-6')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Differences: RMSprop vs Adam\n",
    "\n",
    "| Feature | RMSprop | Adam |\n",
    "|---------|---------|------|\n",
    "| Momentum | âŒ No | âœ… Yes (first moment) |\n",
    "| Adaptive LR | âœ… Yes (second moment) | âœ… Yes (second moment) |\n",
    "| Bias correction | âŒ No | âœ… Yes |\n",
    "| Typical use | RNNs | General purpose |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2: AdamW Optimizer\n",
    "\n",
    "**Exercise:** Implement AdamW (Adam with decoupled weight decay).\n",
    "\n",
    "The key insight is that L2 regularization and weight decay are NOT the same with adaptive optimizers:\n",
    "- L2 regularization adds `Î»*w` to the gradient\n",
    "- Weight decay directly subtracts `Î»*w` from the weights\n",
    "\n",
    "AdamW decouples weight decay from the gradient-based update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: AdamW optimizer\n",
    "\n",
    "class AdamW:\n",
    "    \"\"\"\n",
    "    AdamW optimizer - Adam with decoupled weight decay.\n",
    "    \n",
    "    In standard Adam with L2 regularization, the regularization term\n",
    "    is scaled by the adaptive learning rate, which can be problematic.\n",
    "    \n",
    "    AdamW fixes this by applying weight decay directly to the weights,\n",
    "    separately from the gradient-based update.\n",
    "    \n",
    "    Update rules:\n",
    "        m = Î²1 Ã— m + (1-Î²1) Ã— g           (momentum)\n",
    "        v = Î²2 Ã— v + (1-Î²2) Ã— gÂ²          (variance)\n",
    "        m_hat = m / (1 - Î²1^t)            (bias correction)\n",
    "        v_hat = v / (1 - Î²2^t)            (bias correction)\n",
    "        Î¸ = Î¸ - lr Ã— m_hat / (âˆšv_hat + Îµ) (Adam update)\n",
    "        Î¸ = Î¸ - lr Ã— Î» Ã— Î¸                (weight decay - DECOUPLED!)\n",
    "    \n",
    "    Args:\n",
    "        lr: Learning rate (default 0.001)\n",
    "        beta1: Momentum coefficient (default 0.9)\n",
    "        beta2: Variance coefficient (default 0.999)\n",
    "        epsilon: Numerical stability (default 1e-8)\n",
    "        weight_decay: Weight decay coefficient (default 0.01)\n",
    "    \n",
    "    Example:\n",
    "        >>> optimizer = AdamW(lr=0.001, weight_decay=0.01)\n",
    "        >>> params = optimizer.step(params, grads)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        self.m = None  # First moment\n",
    "        self.v = None  # Second moment\n",
    "        self.t = 0     # Timestep\n",
    "        \n",
    "        self.name = f\"AdamW (lr={lr}, wd={weight_decay})\"\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        \"\"\"\n",
    "        Perform one AdamW optimization step.\n",
    "        \n",
    "        Args:\n",
    "            params: Current parameters\n",
    "            grads: Gradients at current parameters\n",
    "            \n",
    "        Returns:\n",
    "            Updated parameters\n",
    "        \"\"\"\n",
    "        # Initialize on first call\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(params)\n",
    "            self.v = np.zeros_like(params)\n",
    "        \n",
    "        self.t += 1\n",
    "        \n",
    "        # Update biased first moment (momentum)\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * grads\n",
    "        \n",
    "        # Update biased second moment (variance)\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (grads ** 2)\n",
    "        \n",
    "        # Bias correction\n",
    "        m_hat = self.m / (1 - self.beta1 ** self.t)\n",
    "        v_hat = self.v / (1 - self.beta2 ** self.t)\n",
    "        \n",
    "        # Adam update\n",
    "        params = params - self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "        \n",
    "        # DECOUPLED weight decay (this is the key difference from Adam+L2!)\n",
    "        params = params - self.lr * self.weight_decay * params\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset optimizer state\"\"\"\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "\n",
    "# Test AdamW\n",
    "print(\"Testing AdamW Optimizer\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "adamw = AdamW(lr=0.1, weight_decay=0.01)\n",
    "history_adamw = optimize(adamw, start_point, rosenbrock_gradient, n_steps=5000)\n",
    "\n",
    "final_params = history_adamw['params'][-1]\n",
    "final_loss = history_adamw['loss'][-1]\n",
    "\n",
    "print(f\"Start: ({start_point[0]}, {start_point[1]})\")\n",
    "print(f\"Goal:  (1.0, 1.0)\")\n",
    "print(f\"Final: ({final_params[0]:.6f}, {final_params[1]:.6f})\")\n",
    "print(f\"Final loss: {final_loss:.6e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Adam vs AdamW\n",
    "\n",
    "class Adam:\n",
    "    \"\"\"Standard Adam optimizer\"\"\"\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "        self.name = f\"Adam (lr={lr})\"\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(params)\n",
    "            self.v = np.zeros_like(params)\n",
    "        self.t += 1\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * grads\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (grads ** 2)\n",
    "        m_hat = self.m / (1 - self.beta1 ** self.t)\n",
    "        v_hat = self.v / (1 - self.beta2 ** self.t)\n",
    "        return params - self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "\n",
    "# Run both\n",
    "adam = Adam(lr=0.1)\n",
    "adamw = AdamW(lr=0.1, weight_decay=0.01)\n",
    "\n",
    "history_adam = optimize(adam, start_point, rosenbrock_gradient, 5000)\n",
    "history_adamw = optimize(adamw, start_point, rosenbrock_gradient, 5000)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(history_adam['loss'], 'b-', linewidth=2, label='Adam', alpha=0.8)\n",
    "plt.semilogy(history_adamw['loss'], 'r--', linewidth=2, label='AdamW (wd=0.01)', alpha=0.8)\n",
    "plt.xlabel('Step', fontsize=12)\n",
    "plt.ylabel('Loss (log scale)', fontsize=12)\n",
    "plt.title('Adam vs AdamW Comparison', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAdam final loss:  {history_adam['loss'][-1]:.6e}\")\n",
    "print(f\"AdamW final loss: {history_adamw['loss'][-1]:.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution 3: Nesterov Accelerated Gradient (NAG)\n",
    "\n",
    "**Challenge:** Implement Nesterov momentum.\n",
    "\n",
    "The key insight of Nesterov momentum is to compute the gradient at the \"lookahead\" position:\n",
    "\n",
    "$$v_t = \\beta \\cdot v_{t-1} + \\nabla L(\\theta_t - \\beta \\cdot v_{t-1})$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta \\cdot v_t$$\n",
    "\n",
    "This \"looks ahead\" to where momentum would take us, then computes the gradient there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Nesterov Accelerated Gradient\n",
    "\n",
    "class NesterovMomentum:\n",
    "    \"\"\"\n",
    "    Nesterov Accelerated Gradient (NAG) optimizer.\n",
    "    \n",
    "    Unlike standard momentum which computes the gradient at the current\n",
    "    position, Nesterov computes the gradient at the \"lookahead\" position\n",
    "    (where momentum would take us).\n",
    "    \n",
    "    This often leads to faster convergence because we're evaluating the\n",
    "    gradient at a more informed position.\n",
    "    \n",
    "    Update rules:\n",
    "        lookahead = Î¸ - Î² Ã— v           (where momentum would take us)\n",
    "        g = âˆ‡L(lookahead)               (gradient at lookahead)\n",
    "        v = Î² Ã— v + g                   (update velocity with lookahead gradient)\n",
    "        Î¸ = Î¸ - lr Ã— v                  (update parameters)\n",
    "    \n",
    "    Note: This requires access to the gradient function, not just the gradient.\n",
    "    \n",
    "    Args:\n",
    "        lr: Learning rate\n",
    "        momentum: Momentum coefficient (typically 0.9)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.velocity = None\n",
    "        self.name = f\"Nesterov (lr={lr}, Î²={momentum})\"\n",
    "    \n",
    "    def step(self, params, grads, gradient_fn=None):\n",
    "        \"\"\"\n",
    "        Perform one Nesterov momentum step.\n",
    "        \n",
    "        Args:\n",
    "            params: Current parameters\n",
    "            grads: Gradients at current parameters (used if gradient_fn is None)\n",
    "            gradient_fn: Optional function to compute gradients at any point\n",
    "            \n",
    "        Returns:\n",
    "            Updated parameters\n",
    "        \"\"\"\n",
    "        if self.velocity is None:\n",
    "            self.velocity = np.zeros_like(params)\n",
    "        \n",
    "        if gradient_fn is not None:\n",
    "            # True Nesterov: compute gradient at lookahead position\n",
    "            lookahead = params - self.momentum * self.velocity\n",
    "            grads = gradient_fn(lookahead[0], lookahead[1])\n",
    "        \n",
    "        # Update velocity with (lookahead) gradient\n",
    "        self.velocity = self.momentum * self.velocity + grads\n",
    "        \n",
    "        # Update parameters\n",
    "        return params - self.lr * self.velocity\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset velocity\"\"\"\n",
    "        self.velocity = None\n",
    "\n",
    "\n",
    "def optimize_nesterov(optimizer, start_point, gradient_fn, n_steps=1000):\n",
    "    \"\"\"Special optimization loop for Nesterov (needs gradient_fn)\"\"\"\n",
    "    optimizer.reset()\n",
    "    params = start_point.copy()\n",
    "    history = {'params': [params.copy()], 'loss': [rosenbrock(params[0], params[1])]}\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        # Pass gradient function for true Nesterov\n",
    "        params = optimizer.step(params, None, gradient_fn=gradient_fn)\n",
    "        loss = rosenbrock(params[0], params[1])\n",
    "        history['params'].append(params.copy())\n",
    "        history['loss'].append(loss)\n",
    "        if loss < 1e-10:\n",
    "            break\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Test Nesterov\n",
    "print(\"Testing Nesterov Accelerated Gradient\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "nesterov = NesterovMomentum(lr=0.001, momentum=0.9)\n",
    "history_nesterov = optimize_nesterov(nesterov, start_point, rosenbrock_gradient, 5000)\n",
    "\n",
    "final_params = history_nesterov['params'][-1]\n",
    "final_loss = history_nesterov['loss'][-1]\n",
    "\n",
    "print(f\"Final: ({final_params[0]:.6f}, {final_params[1]:.6f})\")\n",
    "print(f\"Final loss: {final_loss:.6e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare standard momentum vs Nesterov\n",
    "\n",
    "class SGDMomentum:\n",
    "    \"\"\"Standard SGD with momentum\"\"\"\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.velocity = None\n",
    "        self.name = f\"Momentum (lr={lr}, Î²={momentum})\"\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        if self.velocity is None:\n",
    "            self.velocity = np.zeros_like(params)\n",
    "        self.velocity = self.momentum * self.velocity + grads\n",
    "        return params - self.lr * self.velocity\n",
    "    \n",
    "    def reset(self):\n",
    "        self.velocity = None\n",
    "\n",
    "# Run both\n",
    "momentum_opt = SGDMomentum(lr=0.001, momentum=0.9)\n",
    "nesterov_opt = NesterovMomentum(lr=0.001, momentum=0.9)\n",
    "\n",
    "history_momentum = optimize(momentum_opt, start_point, rosenbrock_gradient, 5000)\n",
    "history_nesterov = optimize_nesterov(nesterov_opt, start_point, rosenbrock_gradient, 5000)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].semilogy(history_momentum['loss'], 'b-', linewidth=2, label='Standard Momentum', alpha=0.8)\n",
    "axes[0].semilogy(history_nesterov['loss'], 'r--', linewidth=2, label='Nesterov Momentum', alpha=0.8)\n",
    "axes[0].set_xlabel('Step', fontsize=12)\n",
    "axes[0].set_ylabel('Loss (log scale)', fontsize=12)\n",
    "axes[0].set_title('Momentum vs Nesterov: Convergence', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Trajectories\n",
    "x_range = np.linspace(-1.5, 1.5, 100)\n",
    "y_range = np.linspace(-0.5, 2, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = rosenbrock(X, Y)\n",
    "\n",
    "levels = np.logspace(-1, 3, 30)\n",
    "axes[1].contour(X, Y, Z, levels=levels, cmap='Greys', alpha=0.5)\n",
    "\n",
    "path_m = np.array(history_momentum['params'])\n",
    "path_n = np.array(history_nesterov['params'])\n",
    "\n",
    "step = max(1, len(path_m) // 100)\n",
    "axes[1].plot(path_m[::step, 0], path_m[::step, 1], 'b-', linewidth=1.5, \n",
    "            label='Standard Momentum', alpha=0.7)\n",
    "axes[1].plot(path_n[::step, 0], path_n[::step, 1], 'r--', linewidth=1.5,\n",
    "            label='Nesterov Momentum', alpha=0.7)\n",
    "axes[1].scatter([1], [1], color='gold', s=200, marker='*', zorder=5, label='Optimum')\n",
    "axes[1].set_xlabel('x', fontsize=12)\n",
    "axes[1].set_ylabel('y', fontsize=12)\n",
    "axes[1].set_title('Momentum vs Nesterov: Trajectories', fontsize=14)\n",
    "axes[1].legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nStandard Momentum final loss: {history_momentum['loss'][-1]:.6e}\")\n",
    "print(f\"Nesterov Momentum final loss: {history_nesterov['loss'][-1]:.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Nesterov Works Better\n",
    "\n",
    "**Standard momentum:** \"Where am I now? Let me add momentum and go.\"\n",
    "\n",
    "**Nesterov momentum:** \"If I keep going with my momentum, where will I be? Let me compute the gradient THERE.\"\n",
    "\n",
    "This \"lookahead\" prevents overshooting by giving an early warning when the gradient changes direction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: All Optimizers Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all implemented optimizers\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.name = f\"SGD (lr={lr})\"\n",
    "    def step(self, params, grads):\n",
    "        return params - self.lr * grads\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "# Create all optimizers\n",
    "optimizers = [\n",
    "    SGD(lr=0.001),\n",
    "    SGDMomentum(lr=0.001, momentum=0.9),\n",
    "    RMSprop(lr=0.01, beta=0.9),\n",
    "    Adam(lr=0.1),\n",
    "    AdamW(lr=0.1, weight_decay=0.01),\n",
    "]\n",
    "\n",
    "# Run all\n",
    "histories = {}\n",
    "for opt in optimizers:\n",
    "    histories[opt.name] = optimize(opt, start_point, rosenbrock_gradient, 5000)\n",
    "\n",
    "# Add Nesterov separately (needs special loop)\n",
    "nesterov = NesterovMomentum(lr=0.001, momentum=0.9)\n",
    "histories[nesterov.name] = optimize_nesterov(nesterov, start_point, rosenbrock_gradient, 5000)\n",
    "\n",
    "# Plot all\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(histories)))\n",
    "\n",
    "for (name, hist), color in zip(histories.items(), colors):\n",
    "    plt.semilogy(hist['loss'], linewidth=2, label=name, color=color, alpha=0.8)\n",
    "\n",
    "plt.xlabel('Step', fontsize=12)\n",
    "plt.ylabel('Loss (log scale)', fontsize=12)\n",
    "plt.title('All Optimizers Comparison on Rosenbrock Function', fontsize=14)\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final losses\n",
    "print(\"\\nFinal Losses:\")\n",
    "print(\"-\" * 40)\n",
    "for name, hist in sorted(histories.items(), key=lambda x: x[1]['loss'][-1]):\n",
    "    print(f\"{name:<35} {hist['loss'][-1]:.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **RMSprop:** Adapts learning rate per-parameter using gradient magnitude\n",
    "2. **AdamW:** Decouples weight decay from gradient scaling (better for transformers)\n",
    "3. **Nesterov:** Computes gradient at \"lookahead\" position for faster convergence\n",
    "4. **Adam/AdamW** are usually the best default choices for deep learning\n",
    "5. **RMSprop** can be good for RNNs where Adam sometimes struggles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}