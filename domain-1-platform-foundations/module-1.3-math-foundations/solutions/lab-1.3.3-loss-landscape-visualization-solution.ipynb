{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.3.3 Solutions: Loss Landscape Visualization\n",
    "\n",
    "This notebook contains solutions for the exercises in the Loss Landscape Visualization lab.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **üìù Note:** This solution notebook is designed to be self-contained and includes all necessary\n",
    "> helper functions. However, if you prefer to use your own implementations from the main notebook,\n",
    "> ensure those cells have been run first.\n",
    ">\n",
    "> You can also import production-ready implementations from the scripts:\n",
    "> ```python\n",
    "> from scripts.math_utils import sigmoid, relu, Adam, SGD\n",
    "> from scripts.visualization_utils import plot_loss_landscape, plot_training_curve\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Loss Landscape Solutions\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Recreate the Dataset and Network\n",
    "\n",
    "First, we'll recreate the two moons dataset and the simple network from the main notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two moons dataset\n",
    "def create_moons_dataset(n_samples=200, noise=0.1):\n",
    "    \"\"\"Create a two-moons classification dataset\"\"\"\n",
    "    n_samples_per_class = n_samples // 2\n",
    "    \n",
    "    theta1 = np.linspace(0, np.pi, n_samples_per_class)\n",
    "    X1 = np.column_stack([np.cos(theta1), np.sin(theta1)])\n",
    "    \n",
    "    theta2 = np.linspace(0, np.pi, n_samples_per_class)\n",
    "    X2 = np.column_stack([1 - np.cos(theta2), 1 - np.sin(theta2) - 0.5])\n",
    "    \n",
    "    X = np.vstack([X1, X2]) + np.random.randn(n_samples, 2) * noise\n",
    "    y = np.array([0] * n_samples_per_class + [1] * n_samples_per_class)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(42)\n",
    "X_np, y_np = create_moons_dataset(200, noise=0.15)\n",
    "X_data = torch.FloatTensor(X_np)\n",
    "y_data = torch.FloatTensor(y_np).unsqueeze(1)\n",
    "\n",
    "print(f\"Dataset: {X_np.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network\n",
    "class SimpleNet(nn.Module):\n",
    "    \"\"\"A tiny network for loss landscape visualization\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size=4):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Helper functions\n",
    "def get_params_as_vector(model):\n",
    "    \"\"\"Flatten all model parameters into a single vector\"\"\"\n",
    "    return torch.cat([p.data.view(-1) for p in model.parameters()])\n",
    "\n",
    "def set_params_from_vector(model, params_vector):\n",
    "    \"\"\"Set model parameters from a flattened vector\"\"\"\n",
    "    idx = 0\n",
    "    for p in model.parameters():\n",
    "        numel = p.numel()\n",
    "        p.data = params_vector[idx:idx+numel].view(p.shape)\n",
    "        idx += numel\n",
    "\n",
    "def compute_loss(model, X, y, criterion):\n",
    "    \"\"\"Compute loss without gradient tracking\"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X)\n",
    "        return criterion(outputs, y).item()\n",
    "\n",
    "print(\"Network and helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise Solution: Compare Optimizers on the Same Landscape\n",
    "\n",
    "### üßí ELI5: What We're Doing\n",
    "\n",
    "> **Imagine three hikers trying to reach the bottom of a valley...**\n",
    ">\n",
    "> - **SGD Hiker:** Takes small careful steps, directly downhill\n",
    "> - **Momentum Hiker:** Builds up speed as they go, like a ball rolling downhill\n",
    "> - **Adam Hiker:** Uses a smart compass that adapts based on terrain\n",
    ">\n",
    "> We'll start all three at the SAME spot and watch their paths!\n",
    "\n",
    "### The Task\n",
    "\n",
    "1. Train 3 models with SGD, SGD+Momentum, and Adam\n",
    "2. Record their parameter trajectories\n",
    "3. Project to 2D using PCA\n",
    "4. Visualize on the same loss landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define optimizers to compare\n",
    "optimizers_to_compare = [\n",
    "    ('SGD', torch.optim.SGD, {'lr': 0.5}),\n",
    "    ('SGD + Momentum', torch.optim.SGD, {'lr': 0.5, 'momentum': 0.9}),\n",
    "    ('Adam', torch.optim.Adam, {'lr': 0.05}),\n",
    "]\n",
    "\n",
    "print(\"Optimizers to compare:\")\n",
    "for name, _, kwargs in optimizers_to_compare:\n",
    "    print(f\"  - {name}: {kwargs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Train each optimizer and record trajectory\n",
    "\n",
    "def train_and_record(model, optimizer, X_data, y_data, n_epochs=200):\n",
    "    \"\"\"\n",
    "    Train model and record parameter trajectory.\n",
    "    \n",
    "    Returns:\n",
    "        trajectory: List of parameter vectors at each step\n",
    "        losses: List of loss values\n",
    "    \"\"\"\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    trajectory = [get_params_as_vector(model).clone()]\n",
    "    losses = [compute_loss(model, X_data, y_data, criterion)]\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_data)\n",
    "        loss = criterion(outputs, y_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        trajectory.append(get_params_as_vector(model).clone())\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return trajectory, losses\n",
    "\n",
    "# Train all optimizers from the SAME starting point\n",
    "trajectories = {}\n",
    "all_losses = {}\n",
    "\n",
    "for name, opt_class, opt_kwargs in optimizers_to_compare:\n",
    "    # Use same seed for identical initialization!\n",
    "    torch.manual_seed(42)\n",
    "    model = SimpleNet(hidden_size=8)\n",
    "    optimizer = opt_class(model.parameters(), **opt_kwargs)\n",
    "    \n",
    "    traj, losses = train_and_record(model, optimizer, X_data, y_data, n_epochs=200)\n",
    "    trajectories[name] = torch.stack(traj).numpy()\n",
    "    all_losses[name] = losses\n",
    "    \n",
    "    print(f\"{name}: Loss {losses[0]:.4f} ‚Üí {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Project all trajectories to 2D using PCA\n",
    "\n",
    "# Combine all trajectory points for fitting PCA\n",
    "all_points = np.vstack([traj for traj in trajectories.values()])\n",
    "\n",
    "# Fit PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(all_points)\n",
    "\n",
    "# Transform each trajectory\n",
    "trajectories_2d = {}\n",
    "for name, traj in trajectories.items():\n",
    "    trajectories_2d[name] = pca.transform(traj)\n",
    "\n",
    "print(f\"Variance explained by 2 PCs: {pca.explained_variance_ratio_.sum()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create loss landscape along PCA directions\n",
    "\n",
    "# Use the final point of Adam as reference (usually best)\n",
    "center = trajectories['Adam'][-1]\n",
    "pc1 = pca.components_[0]\n",
    "pc2 = pca.components_[1]\n",
    "\n",
    "# Determine range from all trajectories\n",
    "all_2d = np.vstack(list(trajectories_2d.values()))\n",
    "margin = 0.5\n",
    "x_min, x_max = all_2d[:, 0].min() - margin, all_2d[:, 0].max() + margin\n",
    "y_min, y_max = all_2d[:, 1].min() - margin, all_2d[:, 1].max() + margin\n",
    "\n",
    "# Create grid\n",
    "x_range = np.linspace(x_min, x_max, 50)\n",
    "y_range = np.linspace(y_min, y_max, 50)\n",
    "\n",
    "# Compute loss surface\n",
    "print(\"Computing loss surface...\")\n",
    "model_temp = SimpleNet(hidden_size=8)\n",
    "criterion = nn.BCELoss()\n",
    "loss_surface = np.zeros((len(y_range), len(x_range)))\n",
    "\n",
    "for i, y_val in enumerate(y_range):\n",
    "    for j, x_val in enumerate(x_range):\n",
    "        # Reconstruct parameters in original space\n",
    "        params = center + x_val * pc1 + y_val * pc2\n",
    "        set_params_from_vector(model_temp, torch.FloatTensor(params))\n",
    "        loss_surface[i, j] = compute_loss(model_temp, X_data, y_data, criterion)\n",
    "\n",
    "print(f\"Loss surface computed! Range: [{loss_surface.min():.3f}, {loss_surface.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Visualize all trajectories on the landscape\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Colors for each optimizer\n",
    "colors = {\n",
    "    'SGD': 'red',\n",
    "    'SGD + Momentum': 'blue',\n",
    "    'Adam': 'green'\n",
    "}\n",
    "\n",
    "# Left plot: Trajectories on contour\n",
    "X_grid, Y_grid = np.meshgrid(x_range, y_range)\n",
    "contour = axes[0].contourf(X_grid, Y_grid, loss_surface, levels=50, cmap='viridis')\n",
    "axes[0].contour(X_grid, Y_grid, loss_surface, levels=15, colors='white', alpha=0.3, linewidths=0.5)\n",
    "\n",
    "for name, traj_2d in trajectories_2d.items():\n",
    "    color = colors[name]\n",
    "    \n",
    "    # Plot trajectory line (subsample for clarity)\n",
    "    step = max(1, len(traj_2d) // 50)\n",
    "    axes[0].plot(traj_2d[::step, 0], traj_2d[::step, 1], '-', \n",
    "                color=color, linewidth=2, alpha=0.8, label=name)\n",
    "    \n",
    "    # Start point (circle)\n",
    "    axes[0].scatter(traj_2d[0, 0], traj_2d[0, 1], color=color, s=150, \n",
    "                   marker='o', edgecolors='white', linewidth=2, zorder=5)\n",
    "    \n",
    "    # End point (star)\n",
    "    axes[0].scatter(traj_2d[-1, 0], traj_2d[-1, 1], color=color, s=200, \n",
    "                   marker='*', edgecolors='black', linewidth=1, zorder=5)\n",
    "\n",
    "axes[0].set_xlabel('PC1', fontsize=12)\n",
    "axes[0].set_ylabel('PC2', fontsize=12)\n",
    "axes[0].set_title('Optimization Trajectories on Loss Landscape\\n(‚óã = Start, ‚òÖ = End)', fontsize=14)\n",
    "axes[0].legend(loc='upper right')\n",
    "plt.colorbar(contour, ax=axes[0], label='Loss')\n",
    "\n",
    "# Right plot: Loss curves\n",
    "for name, losses in all_losses.items():\n",
    "    color = colors[name]\n",
    "    axes[1].semilogy(losses, color=color, linewidth=2, label=name, alpha=0.8)\n",
    "\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Loss (log scale)', fontsize=12)\n",
    "axes[1].set_title('Convergence Comparison', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Analysis of the Trajectories\n",
    "\n",
    "Let's analyze what we observe from each optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics for each optimizer\n",
    "print(\"Optimizer Comparison Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Optimizer':<20} {'Initial Loss':<15} {'Final Loss':<15} {'Improvement':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, losses in all_losses.items():\n",
    "    initial = losses[0]\n",
    "    final = losses[-1]\n",
    "    improvement = (initial - final) / initial * 100\n",
    "    print(f\"{name:<20} {initial:<15.4f} {final:<15.4f} {improvement:<14.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nüìä Key Observations:\")\n",
    "print(\"  1. All optimizers start at the SAME point (same seed)\")\n",
    "print(\"  2. SGD: Takes a direct but slow path\")\n",
    "print(\"  3. Momentum: Builds speed, may overshoot but recovers\")\n",
    "print(\"  4. Adam: Adaptive steps, often reaches good minimum fastest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute path lengths (how far each optimizer traveled)\n",
    "print(\"\\nPath Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, traj in trajectories.items():\n",
    "    # Compute total path length in parameter space\n",
    "    diffs = np.diff(traj, axis=0)\n",
    "    path_length = np.sum(np.linalg.norm(diffs, axis=1))\n",
    "    \n",
    "    # Compute direct distance (start to end)\n",
    "    direct_dist = np.linalg.norm(traj[-1] - traj[0])\n",
    "    \n",
    "    # Efficiency (how direct was the path)\n",
    "    efficiency = direct_dist / path_length * 100\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Path length:    {path_length:.2f}\")\n",
    "    print(f\"  Direct distance: {direct_dist:.2f}\")\n",
    "    print(f\"  Efficiency:      {efficiency:.1f}% (100% = perfectly direct)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: 3D Visualization with Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a beautiful 3D visualization\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the loss surface\n",
    "X_grid, Y_grid = np.meshgrid(x_range, y_range)\n",
    "surf = ax.plot_surface(X_grid, Y_grid, loss_surface, cmap='viridis', \n",
    "                       alpha=0.6, linewidth=0)\n",
    "\n",
    "# Plot each trajectory in 3D\n",
    "for name in ['SGD', 'SGD + Momentum', 'Adam']:\n",
    "    traj_2d = trajectories_2d[name]\n",
    "    losses = all_losses[name]\n",
    "    color = colors[name]\n",
    "    \n",
    "    # Subsample for clarity\n",
    "    step = max(1, len(traj_2d) // 50)\n",
    "    \n",
    "    ax.plot(traj_2d[::step, 0], traj_2d[::step, 1], losses[::step], \n",
    "           '-', color=color, linewidth=2.5, label=name, alpha=0.9)\n",
    "    \n",
    "    # Start and end markers\n",
    "    ax.scatter([traj_2d[0, 0]], [traj_2d[0, 1]], [losses[0]], \n",
    "              color=color, s=100, marker='o')\n",
    "    ax.scatter([traj_2d[-1, 0]], [traj_2d[-1, 1]], [losses[-1]], \n",
    "              color=color, s=150, marker='*')\n",
    "\n",
    "ax.set_xlabel('PC1', fontsize=12)\n",
    "ax.set_ylabel('PC2', fontsize=12)\n",
    "ax.set_zlabel('Loss', fontsize=12)\n",
    "ax.set_title('3D Optimization Trajectories', fontsize=14)\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "# Set viewing angle\n",
    "ax.view_init(elev=25, azim=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus 2: Comparing Learning Rate Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different learning rates for SGD\n",
    "learning_rates = [0.1, 0.5, 1.0, 2.0]\n",
    "\n",
    "lr_trajectories = {}\n",
    "lr_losses = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    torch.manual_seed(42)\n",
    "    model = SimpleNet(hidden_size=8)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    traj, losses = train_and_record(model, optimizer, X_data, y_data, n_epochs=100)\n",
    "    lr_trajectories[f'LR={lr}'] = torch.stack(traj).numpy()\n",
    "    lr_losses[f'LR={lr}'] = losses\n",
    "\n",
    "# Project to same PCA space\n",
    "lr_trajectories_2d = {}\n",
    "for name, traj in lr_trajectories.items():\n",
    "    lr_trajectories_2d[name] = pca.transform(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learning rate comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "lr_colors = plt.cm.coolwarm(np.linspace(0.2, 0.8, len(learning_rates)))\n",
    "\n",
    "# Left: Trajectories\n",
    "contour = axes[0].contourf(X_grid, Y_grid, loss_surface, levels=50, cmap='viridis')\n",
    "axes[0].contour(X_grid, Y_grid, loss_surface, levels=15, colors='white', alpha=0.3, linewidths=0.5)\n",
    "\n",
    "for (name, traj_2d), color in zip(lr_trajectories_2d.items(), lr_colors):\n",
    "    step = max(1, len(traj_2d) // 30)\n",
    "    axes[0].plot(traj_2d[::step, 0], traj_2d[::step, 1], 'o-', \n",
    "                color=color, linewidth=2, markersize=3, alpha=0.8, label=name)\n",
    "    axes[0].scatter(traj_2d[0, 0], traj_2d[0, 1], color=color, s=100, marker='o', zorder=5)\n",
    "    axes[0].scatter(traj_2d[-1, 0], traj_2d[-1, 1], color=color, s=150, marker='*', zorder=5)\n",
    "\n",
    "axes[0].set_xlabel('PC1', fontsize=12)\n",
    "axes[0].set_ylabel('PC2', fontsize=12)\n",
    "axes[0].set_title('SGD with Different Learning Rates', fontsize=14)\n",
    "axes[0].legend(loc='upper right')\n",
    "plt.colorbar(contour, ax=axes[0], label='Loss')\n",
    "\n",
    "# Right: Loss curves\n",
    "for (name, losses), color in zip(lr_losses.items(), lr_colors):\n",
    "    axes[1].semilogy(losses, color=color, linewidth=2, label=name, alpha=0.8)\n",
    "\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Loss (log scale)', fontsize=12)\n",
    "axes[1].set_title('Learning Rate Impact on Convergence', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Learning Rate Observations:\")\n",
    "print(\"  - Small LR (0.1): Slow but stable\")\n",
    "print(\"  - Medium LR (0.5): Good balance\")\n",
    "print(\"  - Large LR (1.0): Fast but may oscillate\")\n",
    "print(\"  - Very Large LR (2.0): May diverge or bounce around!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Loss landscapes** give us visual insight into the optimization process\n",
    "\n",
    "2. **Different optimizers** take different paths:\n",
    "   - SGD: Steady, predictable, but can be slow\n",
    "   - Momentum: Faster, but may overshoot\n",
    "   - Adam: Adaptive, often best for deep learning\n",
    "\n",
    "3. **Learning rate matters!**\n",
    "   - Too small: Slow convergence\n",
    "   - Too large: Oscillation or divergence\n",
    "   - Just right: Fast and stable\n",
    "\n",
    "4. **PCA projection** lets us visualize high-dimensional trajectories in 2D\n",
    "\n",
    "5. **Path efficiency** varies by optimizer - shorter path doesn't always mean faster convergence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "print(\"\\n‚úÖ Solution notebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}