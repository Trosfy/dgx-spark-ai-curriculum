{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.1: Manual Backpropagation - SOLUTIONS\n",
    "\n",
    "This notebook contains complete solutions to all exercises in the manual backpropagation lab.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **\ud83d\udcdd Note:** This solution notebook is designed to be self-contained and includes all necessary\n",
    "> helper functions. However, if you prefer to use your own implementations from the main notebook,\n",
    "> ensure those cells have been run first.\n",
    ">\n",
    "> You can also import production-ready implementations from the scripts:\n",
    "> ```python\n",
    "> from scripts.math_utils import sigmoid, relu, Adam, SGD\n",
    "> from scripts.visualization_utils import plot_loss_landscape, plot_training_curve\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Solutions Notebook - Manual Backpropagation\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1: ReLU Backward Pass\n",
    "\n",
    "**Exercise:** Implement backward pass with ReLU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: ReLU activation and backward pass\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"ReLU activation: max(0, z)\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"ReLU derivative: 1 if z > 0, else 0\"\"\"\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "# Single neuron with ReLU\n",
    "x = 2.0\n",
    "y = 1.0\n",
    "w = 0.5\n",
    "b = 0.1\n",
    "\n",
    "# Forward pass\n",
    "z = w * x + b\n",
    "print(f\"z = w*x + b = {w}*{x} + {b} = {z}\")\n",
    "\n",
    "y_hat = relu(z)\n",
    "print(f\"\u0177 = ReLU(z) = ReLU({z}) = {y_hat}\")\n",
    "\n",
    "loss = (y_hat - y) ** 2\n",
    "print(f\"Loss = (\u0177 - y)\u00b2 = ({y_hat} - {y})\u00b2 = {loss}\")\n",
    "\n",
    "# Backward pass\n",
    "print(\"\\nBackward pass:\")\n",
    "\n",
    "# \u2202L/\u2202\u0177 = 2(\u0177 - y)\n",
    "dL_dy_hat = 2 * (y_hat - y)\n",
    "print(f\"\u2202L/\u2202\u0177 = 2(\u0177 - y) = 2({y_hat} - {y}) = {dL_dy_hat}\")\n",
    "\n",
    "# \u2202\u0177/\u2202z = ReLU'(z) = 1 (since z > 0)\n",
    "dy_hat_dz = relu_derivative(z)\n",
    "print(f\"\u2202\u0177/\u2202z = ReLU'(z) = ReLU'({z}) = {dy_hat_dz}\")\n",
    "\n",
    "# \u2202z/\u2202w = x\n",
    "dz_dw = x\n",
    "print(f\"\u2202z/\u2202w = x = {dz_dw}\")\n",
    "\n",
    "# \u2202z/\u2202b = 1\n",
    "dz_db = 1\n",
    "print(f\"\u2202z/\u2202b = 1\")\n",
    "\n",
    "# Chain rule\n",
    "dL_dw = dL_dy_hat * dy_hat_dz * dz_dw\n",
    "dL_db = dL_dy_hat * dy_hat_dz * dz_db\n",
    "\n",
    "print(f\"\\n\u2202L/\u2202w = {dL_dy_hat} \u00d7 {dy_hat_dz} \u00d7 {dz_dw} = {dL_dw}\")\n",
    "print(f\"\u2202L/\u2202b = {dL_dy_hat} \u00d7 {dy_hat_dz} \u00d7 {dz_db} = {dL_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify with PyTorch\n",
    "\n",
    "x_t = torch.tensor(x)\n",
    "y_t = torch.tensor(y)\n",
    "w_t = torch.tensor(w, requires_grad=True)\n",
    "b_t = torch.tensor(b, requires_grad=True)\n",
    "\n",
    "z_t = w_t * x_t + b_t\n",
    "y_hat_t = torch.relu(z_t)\n",
    "loss_t = (y_hat_t - y_t) ** 2\n",
    "loss_t.backward()\n",
    "\n",
    "print(\"Verification with PyTorch:\")\n",
    "print(f\"Manual \u2202L/\u2202w:   {dL_dw}\")\n",
    "print(f\"PyTorch \u2202L/\u2202w:  {w_t.grad.item()}\")\n",
    "print(f\"Manual \u2202L/\u2202b:   {dL_db}\")\n",
    "print(f\"PyTorch \u2202L/\u2202b:  {b_t.grad.item()}\")\n",
    "print(\"\\n\u2705 Gradients match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2: 4-Layer MLP\n",
    "\n",
    "**Exercise:** Extend the MLP to 4 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: 4-layer MLP with manual backprop\n",
    "\n",
    "class ManualMLP4Layer:\n",
    "    \"\"\"\n",
    "    4-layer MLP: Input(2) \u2192 Hidden1(4) \u2192 Hidden2(3) \u2192 Hidden3(2) \u2192 Output(1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Xavier initialization\n",
    "        self.W1 = np.random.randn(2, 4) * np.sqrt(2.0 / 2)\n",
    "        self.b1 = np.zeros((1, 4))\n",
    "        \n",
    "        self.W2 = np.random.randn(4, 3) * np.sqrt(2.0 / 4)\n",
    "        self.b2 = np.zeros((1, 3))\n",
    "        \n",
    "        self.W3 = np.random.randn(3, 2) * np.sqrt(2.0 / 3)\n",
    "        self.b3 = np.zeros((1, 2))\n",
    "        \n",
    "        self.W4 = np.random.randn(2, 1) * np.sqrt(2.0 / 2)\n",
    "        self.b4 = np.zeros((1, 1))\n",
    "        \n",
    "        self.cache = {}\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        s = self.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.cache['X'] = X\n",
    "        \n",
    "        # Layer 1\n",
    "        self.cache['z1'] = X @ self.W1 + self.b1\n",
    "        self.cache['h1'] = self.relu(self.cache['z1'])\n",
    "        \n",
    "        # Layer 2\n",
    "        self.cache['z2'] = self.cache['h1'] @ self.W2 + self.b2\n",
    "        self.cache['h2'] = self.relu(self.cache['z2'])\n",
    "        \n",
    "        # Layer 3\n",
    "        self.cache['z3'] = self.cache['h2'] @ self.W3 + self.b3\n",
    "        self.cache['h3'] = self.relu(self.cache['z3'])\n",
    "        \n",
    "        # Layer 4 (output)\n",
    "        self.cache['z4'] = self.cache['h3'] @ self.W4 + self.b4\n",
    "        self.cache['y_hat'] = self.sigmoid(self.cache['z4'])\n",
    "        \n",
    "        return self.cache['y_hat']\n",
    "    \n",
    "    def compute_loss(self, y_hat, y):\n",
    "        return np.mean((y_hat - y) ** 2)\n",
    "    \n",
    "    def backward(self, y):\n",
    "        batch_size = y.shape[0]\n",
    "        \n",
    "        # Layer 4 (output)\n",
    "        dL_dy_hat = 2 * (self.cache['y_hat'] - y) / batch_size\n",
    "        dy_hat_dz4 = self.sigmoid_derivative(self.cache['z4'])\n",
    "        dL_dz4 = dL_dy_hat * dy_hat_dz4\n",
    "        dL_dW4 = self.cache['h3'].T @ dL_dz4\n",
    "        dL_db4 = np.sum(dL_dz4, axis=0, keepdims=True)\n",
    "        \n",
    "        # Layer 3\n",
    "        dL_dh3 = dL_dz4 @ self.W4.T\n",
    "        dh3_dz3 = self.relu_derivative(self.cache['z3'])\n",
    "        dL_dz3 = dL_dh3 * dh3_dz3\n",
    "        dL_dW3 = self.cache['h2'].T @ dL_dz3\n",
    "        dL_db3 = np.sum(dL_dz3, axis=0, keepdims=True)\n",
    "        \n",
    "        # Layer 2\n",
    "        dL_dh2 = dL_dz3 @ self.W3.T\n",
    "        dh2_dz2 = self.relu_derivative(self.cache['z2'])\n",
    "        dL_dz2 = dL_dh2 * dh2_dz2\n",
    "        dL_dW2 = self.cache['h1'].T @ dL_dz2\n",
    "        dL_db2 = np.sum(dL_dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        # Layer 1\n",
    "        dL_dh1 = dL_dz2 @ self.W2.T\n",
    "        dh1_dz1 = self.relu_derivative(self.cache['z1'])\n",
    "        dL_dz1 = dL_dh1 * dh1_dz1\n",
    "        dL_dW1 = self.cache['X'].T @ dL_dz1\n",
    "        dL_db1 = np.sum(dL_dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        return {\n",
    "            'dW1': dL_dW1, 'db1': dL_db1,\n",
    "            'dW2': dL_dW2, 'db2': dL_db2,\n",
    "            'dW3': dL_dW3, 'db3': dL_db3,\n",
    "            'dW4': dL_dW4, 'db4': dL_db4\n",
    "        }\n",
    "\n",
    "# Test on XOR\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float64)\n",
    "y = np.array([[0], [1], [1], [0]], dtype=np.float64)\n",
    "\n",
    "np.random.seed(42)\n",
    "mlp4 = ManualMLP4Layer()\n",
    "\n",
    "y_hat = mlp4.forward(X)\n",
    "print(f\"Initial predictions: {y_hat.flatten().round(4)}\")\n",
    "print(f\"Initial loss: {mlp4.compute_loss(y_hat, y):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify 4-layer MLP gradients with PyTorch\n",
    "\n",
    "class PyTorchMLP4(nn.Module):\n",
    "    def __init__(self, W1, b1, W2, b2, W3, b3, W4, b4):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 4)\n",
    "        self.fc2 = nn.Linear(4, 3)\n",
    "        self.fc3 = nn.Linear(3, 2)\n",
    "        self.fc4 = nn.Linear(2, 1)\n",
    "        \n",
    "        # Copy weights\n",
    "        self.fc1.weight.data = torch.tensor(W1.T, dtype=torch.float64)\n",
    "        self.fc1.bias.data = torch.tensor(b1.flatten(), dtype=torch.float64)\n",
    "        self.fc2.weight.data = torch.tensor(W2.T, dtype=torch.float64)\n",
    "        self.fc2.bias.data = torch.tensor(b2.flatten(), dtype=torch.float64)\n",
    "        self.fc3.weight.data = torch.tensor(W3.T, dtype=torch.float64)\n",
    "        self.fc3.bias.data = torch.tensor(b3.flatten(), dtype=torch.float64)\n",
    "        self.fc4.weight.data = torch.tensor(W4.T, dtype=torch.float64)\n",
    "        self.fc4.bias.data = torch.tensor(b4.flatten(), dtype=torch.float64)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "# Create PyTorch model\n",
    "torch_mlp4 = PyTorchMLP4(\n",
    "    mlp4.W1, mlp4.b1, mlp4.W2, mlp4.b2,\n",
    "    mlp4.W3, mlp4.b3, mlp4.W4, mlp4.b4\n",
    ").double()\n",
    "\n",
    "X_t = torch.tensor(X, dtype=torch.float64)\n",
    "y_t = torch.tensor(y, dtype=torch.float64)\n",
    "\n",
    "y_hat_t = torch_mlp4(X_t)\n",
    "loss_t = torch.mean((y_hat_t - y_t) ** 2)\n",
    "loss_t.backward()\n",
    "\n",
    "# Get manual gradients\n",
    "grads = mlp4.backward(y)\n",
    "\n",
    "# Compare\n",
    "print(\"Gradient Verification for 4-Layer MLP\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "comparisons = [\n",
    "    ('W1', grads['dW1'], torch_mlp4.fc1.weight.grad.numpy().T),\n",
    "    ('W2', grads['dW2'], torch_mlp4.fc2.weight.grad.numpy().T),\n",
    "    ('W3', grads['dW3'], torch_mlp4.fc3.weight.grad.numpy().T),\n",
    "    ('W4', grads['dW4'], torch_mlp4.fc4.weight.grad.numpy().T),\n",
    "]\n",
    "\n",
    "all_match = True\n",
    "for name, manual, pytorch in comparisons:\n",
    "    max_diff = np.abs(manual - pytorch).max()\n",
    "    match = max_diff < 1e-6\n",
    "    all_match = all_match and match\n",
    "    status = \"\u2705\" if match else \"\u274c\"\n",
    "    print(f\"{status} {name}: max diff = {max_diff:.2e}\")\n",
    "\n",
    "if all_match:\n",
    "    print(\"\\n\ud83c\udf89 All 4-layer gradients match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 3: Binary Cross-Entropy Loss\n",
    "\n",
    "**Challenge:** Implement BCE loss and show it equals Bernoulli NLL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Binary Cross-Entropy implementation\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred, eps=1e-10):\n",
    "    \"\"\"\n",
    "    Binary Cross-Entropy Loss.\n",
    "    \n",
    "    BCE = -[y*log(p) + (1-y)*log(1-p)]\n",
    "    \"\"\"\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def bce_derivative(y_true, y_pred, eps=1e-10):\n",
    "    \"\"\"\n",
    "    Derivative of BCE with respect to y_pred.\n",
    "    \n",
    "    \u2202BCE/\u2202\u0177 = -y/\u0177 + (1-y)/(1-\u0177)\n",
    "    \"\"\"\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    return -y_true / y_pred + (1 - y_true) / (1 - y_pred)\n",
    "\n",
    "# When combined with sigmoid, the gradient simplifies!\n",
    "def bce_sigmoid_gradient(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Combined BCE + sigmoid gradient.\n",
    "    \n",
    "    \u2202BCE/\u2202z = \u0177 - y  (beautiful simplification!)\n",
    "    \"\"\"\n",
    "    return y_pred - y_true\n",
    "\n",
    "# Demonstrate\n",
    "y_true = np.array([1, 0, 1, 1, 0])\n",
    "z = np.array([2.0, -1.5, 1.0, 0.5, -2.0])  # Logits\n",
    "\n",
    "# Forward\n",
    "y_pred = 1 / (1 + np.exp(-z))  # Sigmoid\n",
    "bce_losses = binary_cross_entropy(y_true, y_pred)\n",
    "\n",
    "print(\"Binary Cross-Entropy Example\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"True labels: {y_true}\")\n",
    "print(f\"Logits:      {z}\")\n",
    "print(f\"Predictions: {y_pred.round(4)}\")\n",
    "print(f\"BCE losses:  {bce_losses.round(4)}\")\n",
    "print(f\"Mean BCE:    {bce_losses.mean():.4f}\")\n",
    "\n",
    "# Show gradient simplification\n",
    "print(\"\\nGradient with BCE + Sigmoid:\")\n",
    "grad_full = bce_derivative(y_true, y_pred) * (y_pred * (1 - y_pred))  # Chain rule\n",
    "grad_simple = bce_sigmoid_gradient(y_true, y_pred)\n",
    "print(f\"Full chain rule: {grad_full.round(4)}\")\n",
    "print(f\"Simplified (\u0177-y): {grad_simple.round(4)}\")\n",
    "print(f\"\\nThey match! This is why sigmoid + BCE is elegant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DERIVATION: Why \u2202BCE/\u2202z = \u0177 - y for sigmoid\n",
    "\n",
    "print(\"Mathematical Derivation\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"BCE = -[y\u00b7log(\u0177) + (1-y)\u00b7log(1-\u0177)]\")\n",
    "print()\n",
    "print(\"Where \u0177 = \u03c3(z) = 1/(1+e^(-z))\")\n",
    "print()\n",
    "print(\"Step 1: \u2202BCE/\u2202\u0177 = -y/\u0177 + (1-y)/(1-\u0177)\")\n",
    "print()\n",
    "print(\"Step 2: \u2202\u0177/\u2202z = \u0177(1-\u0177)  (sigmoid derivative)\")\n",
    "print()\n",
    "print(\"Step 3: Chain rule\")\n",
    "print(\"  \u2202BCE/\u2202z = \u2202BCE/\u2202\u0177 \u00d7 \u2202\u0177/\u2202z\")\n",
    "print(\"         = [-y/\u0177 + (1-y)/(1-\u0177)] \u00d7 \u0177(1-\u0177)\")\n",
    "print(\"         = -y(1-\u0177) + (1-y)\u0177\")\n",
    "print(\"         = -y + y\u0177 + \u0177 - y\u0177\")\n",
    "print(\"         = \u0177 - y\")\n",
    "print()\n",
    "print(\"\u2728 The gradient is simply: prediction - target!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **ReLU derivative** is 1 for positive inputs, 0 for negative\n",
    "2. **Adding layers** follows the same pattern: propagate gradient back through each\n",
    "3. **BCE + sigmoid** gives elegant gradient: \u0177 - y\n",
    "4. **Always verify** your gradients with numerical approximation or autograd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}