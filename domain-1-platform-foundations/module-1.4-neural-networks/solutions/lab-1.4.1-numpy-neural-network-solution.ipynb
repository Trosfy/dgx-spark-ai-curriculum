{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.4.1 Solution: NumPy Neural Network from Scratch\n",
    "\n",
    "This notebook contains complete solutions to the exercises from Lab 1.4.1.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Different Architectures\n",
    "\n",
    "Let's compare different network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete MLP implementation\n",
    "class MLP:\n",
    "    def __init__(self, layer_sizes):\n",
    "        self.layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            W = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2.0 / layer_sizes[i])\n",
    "            b = np.zeros(layer_sizes[i + 1])\n",
    "            self.layers.append({'W': W, 'b': b, 'cache': {}, 'dW': None, 'db': None})\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = X\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            layer['cache']['X'] = out\n",
    "            out = out @ layer['W'] + layer['b']\n",
    "            layer['cache']['Z'] = out\n",
    "            out = np.maximum(0, out)\n",
    "        \n",
    "        self.layers[-1]['cache']['X'] = out\n",
    "        out = out @ self.layers[-1]['W'] + self.layers[-1]['b']\n",
    "        out_shifted = out - np.max(out, axis=1, keepdims=True)\n",
    "        exp_out = np.exp(out_shifted)\n",
    "        self.probs = exp_out / np.sum(exp_out, axis=1, keepdims=True)\n",
    "        return self.probs\n",
    "    \n",
    "    def backward(self, targets, lr):\n",
    "        batch_size = len(targets)\n",
    "        grad = self.probs.copy()\n",
    "        grad[np.arange(batch_size), targets] -= 1\n",
    "        \n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            layer = self.layers[i]\n",
    "            X = layer['cache']['X']\n",
    "            layer['dW'] = X.T @ grad / batch_size\n",
    "            layer['db'] = np.mean(grad, axis=0)\n",
    "            grad = grad @ layer['W'].T\n",
    "            if i > 0:\n",
    "                Z = self.layers[i - 1]['cache']['Z']\n",
    "                grad = grad * (Z > 0)\n",
    "            layer['W'] -= lr * layer['dW']\n",
    "            layer['b'] -= lr * layer['db']\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.forward(X), axis=1)\n",
    "\n",
    "# Load MNIST (simplified)\n",
    "import gzip\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "def load_mnist(path='../data'):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    base_url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = ['train-images-idx3-ubyte.gz', 'train-labels-idx1-ubyte.gz',\n",
    "             't10k-images-idx3-ubyte.gz', 't10k-labels-idx1-ubyte.gz']\n",
    "    \n",
    "    for f in files:\n",
    "        fp = os.path.join(path, f)\n",
    "        if not os.path.exists(fp):\n",
    "            urllib.request.urlretrieve(base_url + f, fp)\n",
    "    \n",
    "    def load_img(fp): \n",
    "        with gzip.open(fp) as f: f.read(16); return np.frombuffer(f.read(), np.uint8).reshape(-1,784).astype(np.float32)/255\n",
    "    def load_lbl(fp): \n",
    "        with gzip.open(fp) as f: f.read(8); return np.frombuffer(f.read(), np.uint8)\n",
    "    \n",
    "    return (load_img(os.path.join(path, files[0])), load_lbl(os.path.join(path, files[1])),\n",
    "            load_img(os.path.join(path, files[2])), load_lbl(os.path.join(path, files[3])))\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare architectures\n",
    "architectures = {\n",
    "    'Simple [784, 128, 10]': [784, 128, 10],\n",
    "    'Standard [784, 256, 128, 10]': [784, 256, 128, 10],\n",
    "    'Wide [784, 512, 512, 10]': [784, 512, 512, 10],\n",
    "    'Deep [784, 256, 256, 128, 64, 10]': [784, 256, 256, 128, 64, 10]\n",
    "}\n",
    "\n",
    "def train_and_evaluate(arch, epochs=5, lr=0.1, batch_size=64):\n",
    "    np.random.seed(42)\n",
    "    model = MLP(arch)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        indices = np.random.permutation(len(X_train))\n",
    "        for start in range(0, len(X_train), batch_size):\n",
    "            batch_idx = indices[start:start+batch_size]\n",
    "            model.forward(X_train[batch_idx])\n",
    "            model.backward(y_train[batch_idx], lr)\n",
    "    \n",
    "    return np.mean(model.predict(X_test) == y_test)\n",
    "\n",
    "print(\"Architecture Comparison (5 epochs):\")\n",
    "print(\"=\" * 60)\n",
    "for name, arch in architectures.items():\n",
    "    acc = train_and_evaluate(arch)\n",
    "    params = sum((arch[i] + 1) * arch[i+1] for i in range(len(arch)-1))\n",
    "    print(f\"{name:40s} | Acc: {acc:.2%} | Params: {params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Learning Rate Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare learning rates\n",
    "learning_rates = [0.001, 0.01, 0.1, 1.0]\n",
    "results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    np.random.seed(42)\n",
    "    model = MLP([784, 256, 128, 10])\n",
    "    history = []\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        indices = np.random.permutation(len(X_train))\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for start in range(0, len(X_train), 64):\n",
    "            batch_idx = indices[start:start+64]\n",
    "            probs = model.forward(X_train[batch_idx])\n",
    "            loss = -np.mean(np.log(probs[np.arange(len(batch_idx)), y_train[batch_idx]] + 1e-10))\n",
    "            if not np.isnan(loss):\n",
    "                epoch_loss += loss\n",
    "            model.backward(y_train[batch_idx], lr)\n",
    "        \n",
    "        acc = np.mean(model.predict(X_test[:1000]) == y_test[:1000])\n",
    "        history.append(acc)\n",
    "    \n",
    "    results[lr] = history\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "for lr, history in results.items():\n",
    "    plt.plot(history, label=f'LR={lr}', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Learning Rate Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal Accuracies:\")\n",
    "for lr, history in results.items():\n",
    "    status = \"âœ… Good\" if history[-1] > 0.9 else \"âŒ Too high/low\"\n",
    "    print(f\"  LR={lr}: {history[-1]:.2%} {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Solution: SGD with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPWithMomentum:\n",
    "    \"\"\"MLP with SGD Momentum optimizer.\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes, momentum=0.9):\n",
    "        self.layers = []\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            W = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2.0 / layer_sizes[i])\n",
    "            b = np.zeros(layer_sizes[i + 1])\n",
    "            self.layers.append({\n",
    "                'W': W, 'b': b, 'cache': {},\n",
    "                'vW': np.zeros_like(W),  # Velocity for weights\n",
    "                'vb': np.zeros_like(b)   # Velocity for bias\n",
    "            })\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = X\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            layer['cache']['X'] = out\n",
    "            out = out @ layer['W'] + layer['b']\n",
    "            layer['cache']['Z'] = out\n",
    "            out = np.maximum(0, out)\n",
    "        \n",
    "        self.layers[-1]['cache']['X'] = out\n",
    "        out = out @ self.layers[-1]['W'] + self.layers[-1]['b']\n",
    "        out_shifted = out - np.max(out, axis=1, keepdims=True)\n",
    "        exp_out = np.exp(out_shifted)\n",
    "        self.probs = exp_out / np.sum(exp_out, axis=1, keepdims=True)\n",
    "        return self.probs\n",
    "    \n",
    "    def backward(self, targets, lr):\n",
    "        batch_size = len(targets)\n",
    "        grad = self.probs.copy()\n",
    "        grad[np.arange(batch_size), targets] -= 1\n",
    "        \n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            layer = self.layers[i]\n",
    "            X = layer['cache']['X']\n",
    "            \n",
    "            dW = X.T @ grad / batch_size\n",
    "            db = np.mean(grad, axis=0)\n",
    "            \n",
    "            # Update velocity with momentum\n",
    "            layer['vW'] = self.momentum * layer['vW'] - lr * dW\n",
    "            layer['vb'] = self.momentum * layer['vb'] - lr * db\n",
    "            \n",
    "            # Update weights using velocity\n",
    "            layer['W'] += layer['vW']\n",
    "            layer['b'] += layer['vb']\n",
    "            \n",
    "            grad = grad @ layer['W'].T\n",
    "            if i > 0:\n",
    "                Z = self.layers[i - 1]['cache']['Z']\n",
    "                grad = grad * (Z > 0)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.forward(X), axis=1)\n",
    "\n",
    "# Compare with and without momentum\n",
    "print(\"Comparing SGD vs SGD with Momentum:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Without momentum\n",
    "np.random.seed(42)\n",
    "model_no_mom = MLP([784, 256, 128, 10])\n",
    "for epoch in range(5):\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    for start in range(0, len(X_train), 64):\n",
    "        batch_idx = indices[start:start+64]\n",
    "        model_no_mom.forward(X_train[batch_idx])\n",
    "        model_no_mom.backward(y_train[batch_idx], 0.1)\n",
    "acc_no_mom = np.mean(model_no_mom.predict(X_test) == y_test)\n",
    "\n",
    "# With momentum\n",
    "np.random.seed(42)\n",
    "model_mom = MLPWithMomentum([784, 256, 128, 10], momentum=0.9)\n",
    "for epoch in range(5):\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    for start in range(0, len(X_train), 64):\n",
    "        batch_idx = indices[start:start+64]\n",
    "        model_mom.forward(X_train[batch_idx])\n",
    "        model_mom.backward(y_train[batch_idx], 0.1)\n",
    "acc_mom = np.mean(model_mom.predict(X_test) == y_test)\n",
    "\n",
    "print(f\"SGD (no momentum): {acc_no_mom:.2%}\")\n",
    "print(f\"SGD (momentum=0.9): {acc_mom:.2%}\")\n",
    "print(f\"\\nðŸ’¡ Momentum typically helps with faster convergence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Architecture matters**: Wider/deeper networks have more capacity but need more data\n",
    "2. **Learning rate is critical**: Too high = unstable, too low = slow\n",
    "3. **Momentum accelerates training**: Standard choice is 0.9\n",
    "4. **He initialization** is important for ReLU networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}