{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.4.6 Solution: GPU Acceleration Exercises\n",
    "\n",
    "This notebook contains solutions to the exercises from Lab 1.4.6.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Check PyTorch availability\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    PYTORCH_AVAILABLE = True\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"PyTorch not available\")\n",
    "\n",
    "if PYTORCH_AVAILABLE and torch.cuda.is_available():\n",
    "    print(f\"CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "np.random.seed(42)\n",
    "if PYTORCH_AVAILABLE:\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "import gzip\n",
    "import urllib.request\n",
    "\n",
    "def load_mnist(path='../data'):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    base_url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = {\n",
    "        'train_images': 'train-images-idx3-ubyte.gz',\n",
    "        'train_labels': 'train-labels-idx1-ubyte.gz',\n",
    "        'test_images': 't10k-images-idx3-ubyte.gz',\n",
    "        'test_labels': 't10k-labels-idx1-ubyte.gz'\n",
    "    }\n",
    "    \n",
    "    def download(filename):\n",
    "        filepath = os.path.join(path, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            urllib.request.urlretrieve(base_url + filename, filepath)\n",
    "        return filepath\n",
    "    \n",
    "    def load_images(fp):\n",
    "        with gzip.open(fp, 'rb') as f:\n",
    "            f.read(16)\n",
    "            return np.frombuffer(f.read(), dtype=np.uint8).reshape(-1, 784).astype(np.float32) / 255.0\n",
    "    \n",
    "    def load_labels(fp):\n",
    "        with gzip.open(fp, 'rb') as f:\n",
    "            f.read(8)\n",
    "            return np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    \n",
    "    return (load_images(download(files['train_images'])),\n",
    "            load_labels(download(files['train_labels'])),\n",
    "            load_images(download(files['test_images'])),\n",
    "            load_labels(download(files['test_labels'])))\n",
    "\n",
    "X_train_np, y_train_np, X_test_np, y_test_np = load_mnist()\n",
    "print(f\"Loaded {len(X_train_np)} training samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Train a Larger Model\n",
    "\n",
    "Train a model with architecture `[784, 1024, 512, 256, 128, 10]` and compare CPU vs GPU times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE:\n",
    "    class LargerMLP(nn.Module):\n",
    "        \"\"\"\n",
    "        Larger MLP with architecture: [784, 1024, 512, 256, 128, 10]\n",
    "        \n",
    "        This model has ~1.5M parameters vs ~250K in the smaller version.\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            \n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(784, 1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1024, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 10)\n",
    "            )\n",
    "            \n",
    "            # He initialization\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                    nn.init.zeros_(m.bias)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "    \n",
    "    # Count parameters\n",
    "    model = LargerMLP()\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE:\n",
    "    def train_model(model, train_loader, epochs, lr, device):\n",
    "        \"\"\"Train model and return time and final accuracy.\"\"\"\n",
    "        model = model.to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        \n",
    "        # Warmup for GPU\n",
    "        if device.type == 'cuda':\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                break\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        return elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"Exercise 1: Training Larger Model [784, 1024, 512, 256, 128, 10]\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train_torch = torch.FloatTensor(X_train_np)\n",
    "    y_train_torch = torch.LongTensor(y_train_np)\n",
    "    X_test_torch = torch.FloatTensor(X_test_np)\n",
    "    y_test_torch = torch.LongTensor(y_test_np)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    \n",
    "    EPOCHS = 5\n",
    "    LR = 0.1\n",
    "    \n",
    "    # CPU Training\n",
    "    print(f\"\\nTraining on CPU...\")\n",
    "    torch.manual_seed(42)\n",
    "    model_cpu = LargerMLP()\n",
    "    time_cpu = train_model(model_cpu, train_loader, EPOCHS, LR, torch.device('cpu'))\n",
    "    \n",
    "    model_cpu.eval()\n",
    "    with torch.no_grad():\n",
    "        preds_cpu = model_cpu(X_test_torch).argmax(dim=1)\n",
    "        acc_cpu = (preds_cpu == y_test_torch).float().mean().item()\n",
    "    \n",
    "    print(f\"CPU Time: {time_cpu:.2f}s | Accuracy: {acc_cpu:.2%}\")\n",
    "    \n",
    "    # GPU Training (if available)\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"\\nTraining on GPU...\")\n",
    "        torch.manual_seed(42)\n",
    "        model_gpu = LargerMLP()\n",
    "        time_gpu = train_model(model_gpu, train_loader, EPOCHS, LR, torch.device('cuda'))\n",
    "        \n",
    "        model_gpu.eval()\n",
    "        with torch.no_grad():\n",
    "            preds_gpu = model_gpu(X_test_torch.cuda()).argmax(dim=1).cpu()\n",
    "            acc_gpu = (preds_gpu == y_test_torch).float().mean().item()\n",
    "        \n",
    "        print(f\"GPU Time: {time_gpu:.2f}s | Accuracy: {acc_gpu:.2%}\")\n",
    "        \n",
    "        # Summary\n",
    "        print(f\"\\n\" + \"=\" * 70)\n",
    "        print(f\"SPEEDUP: {time_cpu / time_gpu:.1f}x faster on GPU!\")\n",
    "        print(f\"\\nWith the larger model, GPU advantage is more pronounced\")\n",
    "        print(f\"because there's more computation to parallelize.\")\n",
    "    else:\n",
    "        print(\"\\nGPU not available for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution: Mixed Precision Training\n",
    "\n",
    "Use `torch.cuda.amp` for automatic mixed precision training to see additional speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if PYTORCH_AVAILABLE and torch.cuda.is_available():\n    def train_mixed_precision(model, train_loader, epochs, lr):\n        \"\"\"\n        Train with Automatic Mixed Precision (AMP).\n\n        This implementation automatically detects the best precision:\n        - DGX Spark (Blackwell): Uses bfloat16 (no scaler needed!)\n        - Older GPUs: Uses float16 with GradScaler\n\n        Key insight: bfloat16 has same dynamic range as float32, so it's\n        more stable and doesn't need gradient scaling.\n        \"\"\"\n        device = torch.device('cuda')\n        model = model.to(device)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.SGD(model.parameters(), lr=lr)\n\n        # Detect best precision for this GPU\n        # DGX Spark (Blackwell) supports bfloat16 natively\n        use_bfloat16 = torch.cuda.is_bf16_supported()\n        dtype = torch.bfloat16 if use_bfloat16 else torch.float16\n\n        # GradScaler only needed for float16, NOT for bfloat16\n        scaler = None if use_bfloat16 else torch.amp.GradScaler('cuda')\n\n        print(f\"   Using {'bfloat16 (optimal for DGX Spark)' if use_bfloat16 else 'float16 with GradScaler'}\")\n\n        # Warmup\n        for X_batch, y_batch in train_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            optimizer.zero_grad()\n            with torch.amp.autocast(device_type='cuda', dtype=dtype):\n                outputs = model(X_batch)\n                loss = criterion(outputs, y_batch)\n            if scaler:\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                loss.backward()\n                optimizer.step()\n            break\n        torch.cuda.synchronize()\n\n        start_time = time.time()\n\n        for epoch in range(epochs):\n            for X_batch, y_batch in train_loader:\n                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n\n                optimizer.zero_grad()\n\n                # Forward pass with autocast (uses mixed precision)\n                with torch.amp.autocast(device_type='cuda', dtype=dtype):\n                    outputs = model(X_batch)\n                    loss = criterion(outputs, y_batch)\n\n                # Backward pass (with scaling only for float16)\n                if scaler:\n                    scaler.scale(loss).backward()\n                    scaler.step(optimizer)\n                    scaler.update()\n                else:\n                    loss.backward()\n                    optimizer.step()\n\n        torch.cuda.synchronize()\n        elapsed = time.time() - start_time\n        return elapsed\n\n    print(\"Mixed precision training function defined.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE and torch.cuda.is_available():\n",
    "    print(\"Exercise 2: Mixed Precision Training Comparison\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    EPOCHS = 5\n",
    "    LR = 0.1\n",
    "    \n",
    "    # Standard float32 GPU training\n",
    "    print(\"\\n1. Standard float32 training on GPU...\")\n",
    "    torch.manual_seed(42)\n",
    "    model_fp32 = LargerMLP()\n",
    "    time_fp32 = train_model(model_fp32, train_loader, EPOCHS, LR, torch.device('cuda'))\n",
    "    \n",
    "    model_fp32.eval()\n",
    "    with torch.no_grad():\n",
    "        preds_fp32 = model_fp32(X_test_torch.cuda()).argmax(dim=1).cpu()\n",
    "        acc_fp32 = (preds_fp32 == y_test_torch).float().mean().item()\n",
    "    \n",
    "    print(f\"   Time: {time_fp32:.2f}s | Accuracy: {acc_fp32:.2%}\")\n",
    "    \n",
    "    # Mixed precision (float16) training\n",
    "    print(\"\\n2. Mixed precision (AMP) training on GPU...\")\n",
    "    torch.manual_seed(42)\n",
    "    model_amp = LargerMLP()\n",
    "    time_amp = train_mixed_precision(model_amp, train_loader, EPOCHS, LR)\n",
    "    \n",
    "    model_amp.eval()\n",
    "    with torch.no_grad():\n",
    "        preds_amp = model_amp(X_test_torch.cuda()).argmax(dim=1).cpu()\n",
    "        acc_amp = (preds_amp == y_test_torch).float().mean().item()\n",
    "    \n",
    "    print(f\"   Time: {time_amp:.2f}s | Accuracy: {acc_amp:.2%}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"RESULTS SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Method':<25} {'Time (s)':<12} {'Speedup':<15} {'Accuracy'}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'GPU (float32)':<25} {time_fp32:<12.2f} {'1.0x (baseline)':<15} {acc_fp32:.2%}\")\n",
    "    speedup = time_fp32 / time_amp\n",
    "    print(f\"{'GPU (mixed precision)':<25} {time_amp:<12.2f} {f'{speedup:.2f}x':<15} {acc_amp:.2%}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nMixed precision speedup: {speedup:.2f}x\")\n",
    "    print(\"\\nNote: Speedup varies by GPU architecture:\")\n",
    "    print(\"  - Tensor Cores (Volta+): 2-3x speedup\")\n",
    "    print(\"  - Without Tensor Cores: ~1.2-1.5x speedup\")\n",
    "    print(\"  - DGX Spark (Blackwell): Excellent AMP support with 192 Tensor Cores\")\n",
    "else:\n",
    "    print(\"GPU not available. Mixed precision training requires CUDA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "if PYTORCH_AVAILABLE and torch.cuda.is_available():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    methods = ['CPU', 'GPU (FP32)', 'GPU (AMP)']\n",
    "    times = [time_cpu, time_fp32, time_amp]\n",
    "    accs = [acc_cpu, acc_fp32, acc_amp]\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    \n",
    "    # Time comparison\n",
    "    bars = axes[0].bar(methods, times, color=colors, edgecolor='black', linewidth=1.5)\n",
    "    axes[0].set_ylabel('Time (seconds)', fontsize=12)\n",
    "    axes[0].set_title('Training Time Comparison (5 epochs)', fontsize=14)\n",
    "    for bar, t in zip(bars, times):\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                     f'{t:.2f}s', ha='center', fontsize=11)\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Speedup comparison\n",
    "    speedups = [1.0, time_cpu/time_fp32, time_cpu/time_amp]\n",
    "    bars = axes[1].bar(methods, speedups, color=colors, edgecolor='black', linewidth=1.5)\n",
    "    axes[1].set_ylabel('Speedup vs CPU', fontsize=12)\n",
    "    axes[1].set_title('Speedup Factor', fontsize=14)\n",
    "    axes[1].axhline(y=1, color='red', linestyle='--', alpha=0.5)\n",
    "    for bar, s in zip(bars, speedups):\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3, \n",
    "                     f'{s:.1f}x', ha='center', fontsize=11)\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: BFloat16 Training (DGX Spark Optimal)\n",
    "\n",
    "DGX Spark's Blackwell GPU has native bfloat16 support, which provides:\n",
    "- Same dynamic range as float32 (better stability than float16)\n",
    "- Same speed benefits as float16\n",
    "- No need for gradient scaling in most cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if PYTORCH_AVAILABLE and torch.cuda.is_available():\n    # Check if bfloat16 is supported\n    if torch.cuda.is_bf16_supported():\n        print(\"BFloat16 Training (Optimal for DGX Spark)\")\n        print(\"=\" * 70)\n\n        def train_bfloat16(model, train_loader, epochs, lr):\n            \"\"\"\n            Train with bfloat16 precision using the modern PyTorch API.\n\n            This is the OPTIMAL approach for DGX Spark's Blackwell GPU!\n            \"\"\"\n            device = torch.device('cuda')\n            model = model.to(device)\n            criterion = nn.CrossEntropyLoss()\n            optimizer = optim.SGD(model.parameters(), lr=lr)\n\n            # Warmup\n            for X_batch, y_batch in train_loader:\n                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n                optimizer.zero_grad()\n                # Use modern torch.amp.autocast API with explicit dtype\n                with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n                    outputs = model(X_batch)\n                    loss = criterion(outputs, y_batch)\n                loss.backward()\n                optimizer.step()\n                break\n            torch.cuda.synchronize()\n\n            start_time = time.time()\n\n            for epoch in range(epochs):\n                for X_batch, y_batch in train_loader:\n                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n                    optimizer.zero_grad()\n\n                    # No GradScaler needed for bfloat16!\n                    with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n                        outputs = model(X_batch)\n                        loss = criterion(outputs, y_batch)\n\n                    loss.backward()\n                    optimizer.step()\n\n            torch.cuda.synchronize()\n            return time.time() - start_time\n\n        torch.manual_seed(42)\n        model_bf16 = LargerMLP()\n        time_bf16 = train_bfloat16(model_bf16, train_loader, EPOCHS, LR)\n\n        model_bf16.eval()\n        with torch.no_grad():\n            preds_bf16 = model_bf16(X_test_torch.cuda()).argmax(dim=1).cpu()\n            acc_bf16 = (preds_bf16 == y_test_torch).float().mean().item()\n\n        print(f\"BFloat16 Time: {time_bf16:.2f}s | Accuracy: {acc_bf16:.2%}\")\n        print(f\"\\nBFloat16 advantages:\")\n        print(f\"  - Same dynamic range as float32 (more stable than float16)\")\n        print(f\"  - No gradient scaling needed (simpler code)\")\n        print(f\"  - Native support on Blackwell GPUs (DGX Spark)\")\n        print(f\"  - Uses modern torch.amp.autocast() API\")\n    else:\n        print(\"BFloat16 not supported on this GPU.\")\n        print(\"Note: DGX Spark's Blackwell GPU has native bfloat16 support.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Larger models benefit more from GPU** - More computation = more parallelism\n",
    "2. **Mixed precision (AMP) provides additional speedup** - Often 1.5-3x on top of GPU acceleration\n",
    "3. **BFloat16 is ideal for training** - Better stability than float16, same speed\n",
    "4. **DGX Spark advantages**:\n",
    "   - 128GB unified memory allows huge batch sizes\n",
    "   - 192 Tensor Cores accelerate mixed precision\n",
    "   - Native bfloat16 support on Blackwell\n",
    "\n",
    "### When to Use Each Precision:\n",
    "- **float32**: Default, maximum compatibility\n",
    "- **float16 (AMP)**: General GPU training speedup\n",
    "- **bfloat16**: Best for training (if supported) - DGX Spark optimal\n",
    "- **int8/FP4**: Inference only (quantized models)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Cleanup\nimport gc\n\nif PYTORCH_AVAILABLE and torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\ngc.collect()\n\nprint(\"Cleanup complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}