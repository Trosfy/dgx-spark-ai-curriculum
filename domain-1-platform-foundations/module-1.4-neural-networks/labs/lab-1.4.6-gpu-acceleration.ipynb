{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4.6: GPU Acceleration on DGX Spark\n",
    "\n",
    "**Module:** 4 - Neural Network Fundamentals  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** â­â­â­ (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Port your NumPy MLP to PyTorch\n",
    "- [ ] Measure CPU vs GPU training times\n",
    "- [ ] Understand the speedup from GPU acceleration\n",
    "- [ ] Find optimal batch sizes for DGX Spark's 128GB unified memory\n",
    "- [ ] Appreciate why GPUs revolutionized deep learning\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Prerequisites\n",
    "\n",
    "- Completed: Notebooks 01-05\n",
    "- Environment: DGX Spark with PyTorch NGC container\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ Real-World Context\n",
    "\n",
    "**Why GPUs transformed AI:**\n",
    "\n",
    "Before GPUs, training a neural network on ImageNet took weeks on CPUs. With GPUs, it takes hours. This 100x+ speedup enabled the deep learning revolution!\n",
    "\n",
    "**Your DGX Spark advantage:**\n",
    "- **128GB unified memory**: No CPUâ†”GPU transfer bottleneck\n",
    "- **192 Tensor Cores**: Hardware-accelerated matrix operations\n",
    "- **1 PFLOP FP4**: Native low-precision inference\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§’ ELI5: Why GPUs Are Faster\n",
    "\n",
    "> **Imagine you need to add up 1000 numbers.**\n",
    ">\n",
    "> **CPU approach (like one really smart mathematician):**\n",
    "> - Add numbers one by one: 1+2=3, 3+3=6, 6+4=10...\n",
    "> - Very fast at each addition, but does them sequentially\n",
    "> - 1000 operations total\n",
    ">\n",
    "> **GPU approach (like 500 average calculators working together):**\n",
    "> - Split into pairs: (1+2), (3+4), (5+6)...\n",
    "> - Each pair adds simultaneously\n",
    "> - 500 results â†’ pair those â†’ 250 results â†’ ... â†’ 1 result\n",
    "> - Only ~10 rounds total!\n",
    ">\n",
    "> Neural networks are mostly matrix multiplications, which are **embarrassingly parallel** - perfect for GPUs!\n",
    ">\n",
    "> **DGX Spark's special power:** Unified memory means the CPU and GPU share the same 128GB RAM - no time wasted copying data back and forth!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup\n\n**Important:** This notebook requires PyTorch. On DGX Spark, use the NGC container:\n\n```bash\ndocker run --gpus all -it --rm \\\n    -v $HOME/workspace:/workspace \\\n    -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\n    --ipc=host \\\n    -p 8888:8888 \\\n    nvcr.io/nvidia/pytorch:25.11-py3 \\\n    jupyter lab --ip=0.0.0.0 --allow-root --no-browser\n```\n\n### Understanding the Docker Flags\n\n| Flag | Purpose |\n|------|---------|\n| `--gpus all` | **Required!** Enables GPU access inside the container |\n| `--ipc=host` | **Required for DataLoader workers!** PyTorch DataLoader uses shared memory for inter-process communication. Without this flag, you'll get errors when using `num_workers > 0` |\n| `-p 8888:8888` | **Required for Jupyter!** Maps container port 8888 to host port 8888 so you can access Jupyter Lab in your browser |\n| `-v $HOME/workspace:/workspace` | Mounts your workspace directory |\n| `nvcr.io/nvidia/pytorch:25.11-py3` | NGC container with PyTorch optimized for ARM64 |\n\n**Why `--ipc=host` matters:** PyTorch DataLoader creates worker processes that share data through shared memory (`/dev/shm`). By default, Docker containers have a small shared memory limit (64MB). The `--ipc=host` flag shares the host's IPC namespace, giving you access to the full shared memory space."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Check if PyTorch is available\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    PYTORCH_AVAILABLE = True\n",
    "    print(f\"âœ… PyTorch version: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"âŒ PyTorch not available. Please use NGC container.\")\n",
    "\n",
    "# Check GPU availability\n",
    "if PYTORCH_AVAILABLE:\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"âœ… CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    else:\n",
    "        print(\"âš ï¸ CUDA not available. GPU comparisons will be simulated.\")\n",
    "\n",
    "np.random.seed(42)\n",
    "if PYTORCH_AVAILABLE:\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "import gzip\n",
    "import urllib.request\n",
    "\n",
    "def load_mnist(path='../data'):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    base_url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = {\n",
    "        'train_images': 'train-images-idx3-ubyte.gz',\n",
    "        'train_labels': 'train-labels-idx1-ubyte.gz',\n",
    "        'test_images': 't10k-images-idx3-ubyte.gz',\n",
    "        'test_labels': 't10k-labels-idx1-ubyte.gz'\n",
    "    }\n",
    "    \n",
    "    def download(filename):\n",
    "        filepath = os.path.join(path, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            urllib.request.urlretrieve(base_url + filename, filepath)\n",
    "        return filepath\n",
    "    \n",
    "    def load_images(fp):\n",
    "        with gzip.open(fp, 'rb') as f:\n",
    "            f.read(16)\n",
    "            return np.frombuffer(f.read(), dtype=np.uint8).reshape(-1, 784).astype(np.float32) / 255.0\n",
    "    \n",
    "    def load_labels(fp):\n",
    "        with gzip.open(fp, 'rb') as f:\n",
    "            f.read(8)\n",
    "            return np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    \n",
    "    return (load_images(download(files['train_images'])),\n",
    "            load_labels(download(files['train_labels'])),\n",
    "            load_images(download(files['test_images'])),\n",
    "            load_labels(download(files['test_labels'])))\n",
    "\n",
    "X_train_np, y_train_np, X_test_np, y_test_np = load_mnist()\n",
    "print(f\"Loaded {len(X_train_np)} training samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: NumPy Implementation (CPU Baseline)\n",
    "\n",
    "First, let's establish our CPU baseline using the NumPy implementation from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumPyMLP:\n",
    "    \"\"\"\n",
    "    Our NumPy MLP from Notebook 01 - runs on CPU.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes):\n",
    "        self.layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            W = np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float32) * np.sqrt(2.0 / layer_sizes[i])\n",
    "            b = np.zeros(layer_sizes[i + 1], dtype=np.float32)\n",
    "            self.layers.append({'W': W, 'b': b, 'cache': {}})\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = X\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            layer['cache']['X'] = out\n",
    "            out = out @ layer['W'] + layer['b']\n",
    "            layer['cache']['Z'] = out\n",
    "            out = np.maximum(0, out)  # ReLU\n",
    "        \n",
    "        self.layers[-1]['cache']['X'] = out\n",
    "        out = out @ self.layers[-1]['W'] + self.layers[-1]['b']\n",
    "        \n",
    "        # Softmax\n",
    "        out_shifted = out - np.max(out, axis=1, keepdims=True)\n",
    "        exp_out = np.exp(out_shifted)\n",
    "        self.probs = exp_out / np.sum(exp_out, axis=1, keepdims=True)\n",
    "        return self.probs\n",
    "    \n",
    "    def backward(self, targets, lr):\n",
    "        batch_size = len(targets)\n",
    "        grad = self.probs.copy()\n",
    "        grad[np.arange(batch_size), targets] -= 1\n",
    "        \n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            layer = self.layers[i]\n",
    "            X = layer['cache']['X']\n",
    "            \n",
    "            dW = X.T @ grad / batch_size\n",
    "            db = np.mean(grad, axis=0)\n",
    "            grad = grad @ layer['W'].T\n",
    "            \n",
    "            if i > 0:\n",
    "                Z = self.layers[i - 1]['cache']['Z']\n",
    "                grad = grad * (Z > 0)\n",
    "            \n",
    "            layer['W'] -= lr * dW\n",
    "            layer['b'] -= lr * db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.forward(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_numpy(model, X_train, y_train, epochs, batch_size, lr):\n",
    "    \"\"\"Train NumPy model and return time.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        indices = np.random.permutation(len(X_train))\n",
    "        for start in range(0, len(X_train), batch_size):\n",
    "            batch_idx = indices[start:start + batch_size]\n",
    "            X_batch = X_train[batch_idx]\n",
    "            y_batch = y_train[batch_idx]\n",
    "            \n",
    "            model.forward(X_batch)\n",
    "            model.backward(y_batch, lr)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    return elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: PyTorch Implementation\n",
    "\n",
    "Now let's create the same model in PyTorch, which can run on CPU or GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE:\n",
    "    class PyTorchMLP(nn.Module):\n",
    "        \"\"\"\n",
    "        Same architecture as NumPyMLP, but in PyTorch.\n",
    "        \n",
    "        PyTorch handles:\n",
    "        - Automatic differentiation (no manual backward!)\n",
    "        - GPU acceleration (just move to device)\n",
    "        - Optimized kernels (cuDNN)\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, layer_sizes):\n",
    "            super().__init__()\n",
    "            \n",
    "            layers = []\n",
    "            for i in range(len(layer_sizes) - 1):\n",
    "                layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "                if i < len(layer_sizes) - 2:\n",
    "                    layers.append(nn.ReLU())\n",
    "            \n",
    "            self.model = nn.Sequential(*layers)\n",
    "            \n",
    "            # Initialize weights like NumPy version (He initialization)\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                    nn.init.zeros_(m.bias)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "    \n",
    "    print(\"âœ… PyTorch MLP class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE:\n",
    "    def train_pytorch(model, train_loader, epochs, lr, device):\n",
    "        \"\"\"Train PyTorch model and return time.\"\"\"\n",
    "        model = model.to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        \n",
    "        # Warmup (important for GPU timing!)\n",
    "        if device.type == 'cuda':\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                break\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Ensure GPU operations are complete\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        return elapsed\n",
    "    \n",
    "    print(\"âœ… Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: CPU vs GPU Comparison\n",
    "\n",
    "Let's measure the speedup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.1\n",
    "ARCHITECTURE = [784, 256, 128, 10]\n",
    "\n",
    "print(\"ðŸŽï¸ CPU vs GPU Comparison\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Architecture: {ARCHITECTURE}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Training samples: {len(X_train_np)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. NumPy (CPU)\n",
    "print(\"\\n1ï¸âƒ£ NumPy (CPU)...\")\n",
    "np.random.seed(42)\n",
    "model_numpy = NumPyMLP(ARCHITECTURE)\n",
    "time_numpy = train_numpy(model_numpy, X_train_np, y_train_np, EPOCHS, BATCH_SIZE, LR)\n",
    "acc_numpy = np.mean(model_numpy.predict(X_test_np) == y_test_np)\n",
    "print(f\"   Time: {time_numpy:.2f}s\")\n",
    "print(f\"   Accuracy: {acc_numpy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if PYTORCH_AVAILABLE:\n    # Prepare PyTorch data\n    X_train_torch = torch.FloatTensor(X_train_np)\n    y_train_torch = torch.LongTensor(y_train_np)\n    X_test_torch = torch.FloatTensor(X_test_np)\n    y_test_torch = torch.LongTensor(y_test_np)\n    \n    train_dataset = TensorDataset(X_train_torch, y_train_torch)\n    \n    # DataLoader options explained:\n    # - num_workers: Number of subprocesses for data loading (requires --ipc=host in Docker)\n    #   Use 0 for simple datasets, 2-4 for larger datasets with transforms\n    # - pin_memory: Copies data to CUDA pinned memory for faster CPUâ†’GPU transfer\n    #   Less important on DGX Spark due to unified memory architecture\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=BATCH_SIZE, \n        shuffle=True,\n        # num_workers=2,    # Uncomment for faster loading (requires --ipc=host)\n        # pin_memory=True,  # Uncomment for faster GPU transfer (less important on DGX Spark)\n    )\n    \n    # 2. PyTorch (CPU)\n    print(\"\\n2. PyTorch (CPU)...\")\n    torch.manual_seed(42)\n    model_cpu = PyTorchMLP(ARCHITECTURE)\n    device_cpu = torch.device('cpu')\n    time_pytorch_cpu = train_pytorch(model_cpu, train_loader, EPOCHS, LR, device_cpu)\n    \n    model_cpu.eval()\n    with torch.no_grad():\n        preds_cpu = model_cpu(X_test_torch).argmax(dim=1)\n        acc_pytorch_cpu = (preds_cpu == y_test_torch).float().mean().item()\n    \n    print(f\"   Time: {time_pytorch_cpu:.2f}s\")\n    print(f\"   Accuracy: {acc_pytorch_cpu:.2%}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE and torch.cuda.is_available():\n",
    "    # 3. PyTorch (GPU)\n",
    "    print(\"\\n3ï¸âƒ£ PyTorch (GPU)...\")\n",
    "    torch.manual_seed(42)\n",
    "    model_gpu = PyTorchMLP(ARCHITECTURE)\n",
    "    device_gpu = torch.device('cuda')\n",
    "    time_pytorch_gpu = train_pytorch(model_gpu, train_loader, EPOCHS, LR, device_gpu)\n",
    "    \n",
    "    model_gpu.eval()\n",
    "    with torch.no_grad():\n",
    "        preds_gpu = model_gpu(X_test_torch.to(device_gpu)).argmax(dim=1).cpu()\n",
    "        acc_pytorch_gpu = (preds_gpu == y_test_torch).float().mean().item()\n",
    "    \n",
    "    print(f\"   Time: {time_pytorch_gpu:.2f}s\")\n",
    "    print(f\"   Accuracy: {acc_pytorch_gpu:.2%}\")\n",
    "else:\n",
    "    print(\"\\n3ï¸âƒ£ PyTorch (GPU)... SKIPPED (no GPU available)\")\n",
    "    time_pytorch_gpu = None\n",
    "    acc_pytorch_gpu = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"                         RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n{'Method':<20} {'Time (s)':<12} {'Speedup':<12} {'Accuracy'}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'NumPy (CPU)':<20} {time_numpy:<12.2f} {'1.0x (baseline)':<12} {acc_numpy:.2%}\")\n",
    "\n",
    "if PYTORCH_AVAILABLE:\n",
    "    speedup_cpu = time_numpy / time_pytorch_cpu\n",
    "    print(f\"{'PyTorch (CPU)':<20} {time_pytorch_cpu:<12.2f} {speedup_cpu:.1f}x{'':<8} {acc_pytorch_cpu:.2%}\")\n",
    "    \n",
    "    if time_pytorch_gpu:\n",
    "        speedup_gpu = time_numpy / time_pytorch_gpu\n",
    "        print(f\"{'PyTorch (GPU)':<20} {time_pytorch_gpu:<12.2f} {speedup_gpu:.1f}x{'':<8} {acc_pytorch_gpu:.2%}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "if PYTORCH_AVAILABLE:\n",
    "    methods = ['NumPy (CPU)', 'PyTorch (CPU)']\n",
    "    times = [time_numpy, time_pytorch_cpu]\n",
    "    colors = ['#FF6B6B', '#4ECDC4']\n",
    "    \n",
    "    if time_pytorch_gpu:\n",
    "        methods.append('PyTorch (GPU)')\n",
    "        times.append(time_pytorch_gpu)\n",
    "        colors.append('#45B7D1')\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Time comparison\n",
    "    bars = axes[0].bar(methods, times, color=colors, edgecolor='black', linewidth=1.5)\n",
    "    axes[0].set_ylabel('Time (seconds)', fontsize=12)\n",
    "    axes[0].set_title('Training Time Comparison', fontsize=14)\n",
    "    for bar, t in zip(bars, times):\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                     f'{t:.2f}s', ha='center', fontsize=11)\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Speedup comparison\n",
    "    speedups = [1.0, time_numpy/time_pytorch_cpu]\n",
    "    if time_pytorch_gpu:\n",
    "        speedups.append(time_numpy/time_pytorch_gpu)\n",
    "    \n",
    "    bars = axes[1].bar(methods, speedups, color=colors, edgecolor='black', linewidth=1.5)\n",
    "    axes[1].set_ylabel('Speedup (vs NumPy)', fontsize=12)\n",
    "    axes[1].set_title('Speedup Factor', fontsize=14)\n",
    "    axes[1].axhline(y=1, color='red', linestyle='--', label='Baseline')\n",
    "    for bar, s in zip(bars, speedups):\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                     f'{s:.1f}x', ha='center', fontsize=11)\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Finding Optimal Batch Size for DGX Spark\n",
    "\n",
    "Batch size significantly affects training speed. Let's find the optimal value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE and torch.cuda.is_available():\n",
    "    print(\"ðŸ”¬ Finding Optimal Batch Size for DGX Spark\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    batch_sizes = [32, 64, 128, 256, 512, 1024, 2048]\n",
    "    gpu_times = []\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.manual_seed(42)\n",
    "            \n",
    "            train_loader = DataLoader(\n",
    "                TensorDataset(X_train_torch, y_train_torch), \n",
    "                batch_size=batch_size, \n",
    "                shuffle=True\n",
    "            )\n",
    "            \n",
    "            model = PyTorchMLP(ARCHITECTURE)\n",
    "            t = train_pytorch(model, train_loader, epochs=1, lr=0.1, device=torch.device('cuda'))\n",
    "            gpu_times.append(t)\n",
    "            \n",
    "            print(f\"Batch size {batch_size:4d}: {t:.3f}s\")\n",
    "        except RuntimeError as e:\n",
    "            if 'out of memory' in str(e):\n",
    "                print(f\"Batch size {batch_size:4d}: OUT OF MEMORY\")\n",
    "                gpu_times.append(None)\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    # Find optimal\n",
    "    valid_times = [(bs, t) for bs, t in zip(batch_sizes, gpu_times) if t is not None]\n",
    "    if valid_times:\n",
    "        optimal_bs, optimal_time = min(valid_times, key=lambda x: x[1])\n",
    "        print(f\"\\nâœ… Optimal batch size for speed: {optimal_bs}\")\n",
    "else:\n",
    "    print(\"âš ï¸ GPU not available. Batch size optimization skipped.\")\n",
    "    batch_sizes = []\n",
    "    gpu_times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if batch_sizes and any(t is not None for t in gpu_times):\n",
    "    # Visualize batch size impact\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    valid_bs = [bs for bs, t in zip(batch_sizes, gpu_times) if t is not None]\n",
    "    valid_times = [t for t in gpu_times if t is not None]\n",
    "    \n",
    "    ax.plot(valid_bs, valid_times, 'bo-', linewidth=2, markersize=10)\n",
    "    ax.set_xlabel('Batch Size', fontsize=12)\n",
    "    ax.set_ylabel('Time per Epoch (seconds)', fontsize=12)\n",
    "    ax.set_title('Batch Size vs Training Speed (GPU)', fontsize=14)\n",
    "    ax.set_xscale('log', base=2)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mark optimal\n",
    "    min_idx = valid_times.index(min(valid_times))\n",
    "    ax.scatter([valid_bs[min_idx]], [valid_times[min_idx]], color='red', s=200, zorder=5, label='Optimal')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: DGX Spark's Unified Memory Advantage\n",
    "\n",
    "One of DGX Spark's key features is its **unified memory architecture**. Let's understand why this matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"                    DGX SPARK'S UNIFIED MEMORY ADVANTAGE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "Traditional GPU System:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚    CPU      â”‚  â”€â”€â”€â”€ PCIe â”€â”€â”€â”€>   â”‚    GPU      â”‚\n",
    "â”‚   32GB RAM  â”‚  <â”€â”€ Transfer â”€â”€   â”‚   8GB VRAM  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     (SLOW!)        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Problem: Moving data between CPU and GPU memory is slow!\n",
    "- PCIe bandwidth: ~16 GB/s\n",
    "- For a 70B model: Would need ~140GB, but GPU only has 8GB!\n",
    "- Constant swapping = very slow inference\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "DGX Spark (Unified Memory):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                128GB UNIFIED MEMORY                    â”‚\n",
    "â”‚         (CPU and GPU share the same memory!)          â”‚\n",
    "â”‚                                                        â”‚\n",
    "â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
    "â”‚     â”‚   CPU   â”‚              â”‚   GPU   â”‚              â”‚\n",
    "â”‚     â”‚ (Grace) â”‚<-- instant -->â”‚(Blackwell)           â”‚\n",
    "â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   access!     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Advantage:\n",
    "- No data transfer needed between CPU and GPU\n",
    "- 70B model fits entirely in memory!\n",
    "- Memory bandwidth: ~273 GB/s (LPDDR5X)\n",
    "- Perfect for large language models\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "What this means for you:\n",
    "1. Load models once, no swapping\n",
    "2. Larger batch sizes possible\n",
    "3. Run models that wouldn't fit on traditional GPUs\n",
    "4. Faster iteration during development\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Best Practices Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"                    GPU TRAINING BEST PRACTICES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "1. USE APPROPRIATE BATCH SIZES\n",
    "   - Too small: GPU underutilized, slower training\n",
    "   - Too large: May hurt generalization, memory issues\n",
    "   - Sweet spot: Usually 64-512 for most tasks\n",
    "   - DGX Spark: Can go larger (1024+) thanks to unified memory\n",
    "\n",
    "2. USE PROPER DATA TYPES\n",
    "   - float32: Default, good balance\n",
    "   - float16/bfloat16: 2x memory savings, often same accuracy\n",
    "   - DGX Spark: Native bfloat16 support in Blackwell\n",
    "\n",
    "3. PIN MEMORY FOR DATA LOADING\n",
    "   ```python\n",
    "   DataLoader(..., pin_memory=True)  # Faster CPUâ†’GPU transfer\n",
    "   ```\n",
    "   Note: Less important on DGX Spark due to unified memory!\n",
    "\n",
    "4. USE CUDA STREAMS FOR OVERLAP\n",
    "   - Overlap data loading with computation\n",
    "   - PyTorch does this automatically with DataLoader workers\n",
    "\n",
    "5. PROFILE YOUR CODE\n",
    "   ```python\n",
    "   with torch.profiler.profile() as prof:\n",
    "       model(x)\n",
    "   print(prof.key_averages().table())\n",
    "   ```\n",
    "\n",
    "6. CLEAR CACHE BEFORE LARGE MODELS\n",
    "   ```bash\n",
    "   sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'\n",
    "   ```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ‹ Try It Yourself\n",
    "\n",
    "### Exercise 1: Train a Larger Model\n",
    "\n",
    "Train a model with architecture `[784, 1024, 512, 256, 128, 10]` and compare CPU vs GPU times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Train larger model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Mixed Precision Training\n",
    "\n",
    "Try using `torch.cuda.amp` for automatic mixed precision training. How much faster is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Implement mixed precision training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "\n",
    "- âœ… How to port NumPy code to PyTorch\n",
    "- âœ… The dramatic speedup from GPU acceleration\n",
    "- âœ… How to find optimal batch sizes\n",
    "- âœ… Why DGX Spark's unified memory is special\n",
    "- âœ… Best practices for GPU training\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“– Further Reading\n",
    "\n",
    "- [PyTorch Performance Tuning Guide](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)\n",
    "- [NVIDIA DGX Spark Documentation](https://docs.nvidia.com/dgx/)\n",
    "- [Mixed Precision Training](https://pytorch.org/docs/stable/amp.html)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "if PYTORCH_AVAILABLE and torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… Cleanup complete!\")\n",
    "print(\"\\nðŸŽ‰ Congratulations! You've completed Module 4: Neural Network Fundamentals!\")\n",
    "print(\"\\nðŸŽ¯ Next: Proceed to Module 5: Phase 1 Capstone Project\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}