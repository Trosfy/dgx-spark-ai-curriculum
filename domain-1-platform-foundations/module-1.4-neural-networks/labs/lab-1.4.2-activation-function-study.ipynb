{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4.2: Activation Function Study\n",
    "\n",
    "**Module:** 4 - Neural Network Fundamentals  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Implement 6 common activation functions from scratch\n",
    "- [ ] Understand why each activation exists and when to use it\n",
    "- [ ] Visualize the vanishing gradient problem\n",
    "- [ ] Compare training dynamics with different activations\n",
    "- [ ] Choose the right activation for your task\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Completed: Notebook 01 (NumPy Neural Network)\n",
    "- Knowledge of: Derivatives, chain rule\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Context\n",
    "\n",
    "**Why do activation functions matter?**\n",
    "\n",
    "The choice of activation function has evolved over decades:\n",
    "- **1980s-90s:** Sigmoid and Tanh dominated\n",
    "- **2010s:** ReLU revolutionized deep learning\n",
    "- **2020s:** GELU and SiLU power modern transformers\n",
    "\n",
    "The right activation can mean the difference between a network that trains in minutes vs. one that never converges!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßí ELI5: What Are Activation Functions?\n",
    "\n",
    "> **Imagine you're building a LEGO house with special bricks.**\n",
    ">\n",
    "> Without activation functions, all your bricks would be straight lines - you could only build straight walls. That's boring!\n",
    ">\n",
    "> **Activation functions are like magic bricks that can bend and curve.** They let you build arches, curves, and complex shapes.\n",
    ">\n",
    "> Different activations = different bending abilities:\n",
    "> - **Sigmoid**: Smoothly curves between 0 and 1 (like a gentle slide)\n",
    "> - **ReLU**: Simple bend at zero (like a hockey stick)\n",
    "> - **GELU**: Smooth bend with a probabilistic feel (like a sophisticated curve)\n",
    ">\n",
    "> Without these \"bending\" abilities, neural networks would just be fancy linear regression!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Tuple, Callable, Dict, List\nimport time\nimport sys\nimport os\nfrom pathlib import Path\n\n# Add scripts directory to path (robust approach)\nnotebook_dir = Path().resolve()\nif notebook_dir.name == 'notebooks':\n    scripts_dir = notebook_dir.parent / 'scripts'\nelse:\n    scripts_dir = notebook_dir / 'scripts'\n    if not scripts_dir.exists():\n        scripts_dir = notebook_dir.parent / 'scripts'\n\nif scripts_dir.exists():\n    sys.path.insert(0, str(scripts_dir))\n\nnp.random.seed(42)\nplt.style.use('default')\n%matplotlib inline\n\nprint(\"Setup complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Implementing All 6 Activation Functions\n",
    "\n",
    "Let's implement each activation with its forward pass and gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    Sigmoid activation: œÉ(x) = 1 / (1 + e^(-x))\n",
    "    \n",
    "    Output range: (0, 1)\n",
    "    \n",
    "    ELI5: Sigmoid squashes any number into a probability between 0 and 1.\n",
    "    Very negative numbers become almost 0, very positive become almost 1.\n",
    "    \n",
    "    Use cases:\n",
    "    - Binary classification output (probability of class 1)\n",
    "    - Gates in LSTMs and GRUs\n",
    "    \n",
    "    Problems:\n",
    "    - Vanishing gradient (gradient max is 0.25!)\n",
    "    - Output not zero-centered\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "        self.name = 'Sigmoid'\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        x = np.clip(x, -500, 500)  # Prevent overflow\n",
    "        out = 1.0 / (1.0 + np.exp(-x))\n",
    "        self.cache['out'] = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
    "        out = self.cache['out']\n",
    "        # Derivative: œÉ(x) * (1 - œÉ(x))\n",
    "        grad = out * (1 - out)\n",
    "        return grad_output * grad\n",
    "    \n",
    "    def gradient(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute gradient directly (for visualization).\"\"\"\n",
    "        self.forward(x)\n",
    "        return self.backward(np.ones_like(x))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    \"\"\"\n",
    "    Hyperbolic tangent: tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "    \n",
    "    Output range: (-1, 1)\n",
    "    \n",
    "    ELI5: Like sigmoid, but centered around zero.\n",
    "    Zero-centered outputs help with learning!\n",
    "    \n",
    "    Use cases:\n",
    "    - Hidden layers in older networks\n",
    "    - Output layer when you need values in [-1, 1]\n",
    "    \n",
    "    Problems:\n",
    "    - Still has vanishing gradient problem\n",
    "    - Gradient max is 1.0 at x=0\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "        self.name = 'Tanh'\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        out = np.tanh(x)\n",
    "        self.cache['out'] = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
    "        out = self.cache['out']\n",
    "        # Derivative: 1 - tanh(x)^2\n",
    "        grad = 1 - out ** 2\n",
    "        return grad_output * grad\n",
    "    \n",
    "    def gradient(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.forward(x)\n",
    "        return self.backward(np.ones_like(x))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit: ReLU(x) = max(0, x)\n",
    "    \n",
    "    Output range: [0, ‚àû)\n",
    "    \n",
    "    ELI5: A simple on/off switch. Positive values pass through,\n",
    "    negative values are blocked. Simple but surprisingly effective!\n",
    "    \n",
    "    Use cases:\n",
    "    - Default choice for hidden layers\n",
    "    - CNNs, MLPs, most modern architectures\n",
    "    \n",
    "    Problems:\n",
    "    - \"Dead ReLU\": Neurons that always output 0 can never recover\n",
    "    - Not zero-centered\n",
    "    \n",
    "    Why it works:\n",
    "    - No vanishing gradient for positive inputs\n",
    "    - Computationally very fast\n",
    "    - Sparse activations (many zeros) = efficient\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "        self.name = 'ReLU'\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.cache['x'] = x\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
    "        x = self.cache['x']\n",
    "        # Derivative: 1 if x > 0, else 0\n",
    "        grad = (x > 0).astype(float)\n",
    "        return grad_output * grad\n",
    "    \n",
    "    def gradient(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.forward(x)\n",
    "        return self.backward(np.ones_like(x))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLU:\n",
    "    \"\"\"\n",
    "    Leaky ReLU: f(x) = x if x > 0, else alpha * x\n",
    "    \n",
    "    Output range: (-‚àû, ‚àû)\n",
    "    \n",
    "    ELI5: Like ReLU, but the valve isn't fully closed for negative values.\n",
    "    A tiny trickle can still get through, preventing \"dead neurons\".\n",
    "    \n",
    "    Use cases:\n",
    "    - When you're worried about dead ReLUs\n",
    "    - GANs often use Leaky ReLU\n",
    "    \n",
    "    Typical alpha: 0.01 or 0.1\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: float = 0.01):\n",
    "        self.alpha = alpha\n",
    "        self.cache = {}\n",
    "        self.name = f'LeakyReLU(Œ±={alpha})'\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.cache['x'] = x\n",
    "        return np.where(x > 0, x, self.alpha * x)\n",
    "    \n",
    "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
    "        x = self.cache['x']\n",
    "        grad = np.where(x > 0, 1.0, self.alpha)\n",
    "        return grad_output * grad\n",
    "    \n",
    "    def gradient(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.forward(x)\n",
    "        return self.backward(np.ones_like(x))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU:\n",
    "    \"\"\"\n",
    "    Gaussian Error Linear Unit: GELU(x) = x * Œ¶(x)\n",
    "    \n",
    "    Where Œ¶ is the CDF of the standard normal distribution.\n",
    "    \n",
    "    Approximation: 0.5 * x * (1 + tanh(sqrt(2/œÄ) * (x + 0.044715 * x^3)))\n",
    "    \n",
    "    Output range: (-0.17, ‚àû)\n",
    "    \n",
    "    ELI5: GELU is like a smarter ReLU. Instead of a hard cutoff,\n",
    "    it uses probability to decide how much of each input to keep.\n",
    "    Values near zero might partially get through - it's probabilistic!\n",
    "    \n",
    "    Use cases:\n",
    "    - BERT, GPT, and most modern transformers\n",
    "    - State-of-the-art NLP models\n",
    "    \n",
    "    Why it works:\n",
    "    - Smooth (helps optimization)\n",
    "    - Non-monotonic (the dip allows for interesting dynamics)\n",
    "    - Works great with layer normalization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "        self.name = 'GELU'\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.cache['x'] = x\n",
    "        # Tanh approximation\n",
    "        return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x ** 3)))\n",
    "    \n",
    "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
    "        x = self.cache['x']\n",
    "        # Approximate derivative\n",
    "        cdf = 0.5 * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x ** 3)))\n",
    "        pdf = np.exp(-0.5 * x ** 2) / np.sqrt(2 * np.pi)\n",
    "        grad = cdf + x * pdf\n",
    "        return grad_output * grad\n",
    "    \n",
    "    def gradient(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.forward(x)\n",
    "        return self.backward(np.ones_like(x))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiLU:\n",
    "    \"\"\"\n",
    "    Sigmoid Linear Unit (also known as Swish): SiLU(x) = x * œÉ(x)\n",
    "    \n",
    "    Output range: (-0.28, ‚àû)\n",
    "    \n",
    "    ELI5: SiLU multiplies each value by its own sigmoid.\n",
    "    It's like asking \"how confident are you?\" and scaling the value\n",
    "    by that confidence. Self-gated activation!\n",
    "    \n",
    "    Use cases:\n",
    "    - EfficientNet, MobileNetV3\n",
    "    - Many modern architectures\n",
    "    - LLaMA and other LLMs\n",
    "    \n",
    "    Why it works:\n",
    "    - Smooth and non-monotonic\n",
    "    - Self-gating provides regularization\n",
    "    - Performs well across many tasks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "        self.name = 'SiLU/Swish'\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.cache['x'] = x\n",
    "        sigmoid = 1.0 / (1.0 + np.exp(-np.clip(x, -500, 500)))\n",
    "        self.cache['sigmoid'] = sigmoid\n",
    "        return x * sigmoid\n",
    "    \n",
    "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
    "        x = self.cache['x']\n",
    "        sigmoid = self.cache['sigmoid']\n",
    "        # Derivative: œÉ(x) + x * œÉ(x) * (1 - œÉ(x))\n",
    "        grad = sigmoid + x * sigmoid * (1 - sigmoid)\n",
    "        return grad_output * grad\n",
    "    \n",
    "    def gradient(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.forward(x)\n",
    "        return self.backward(np.ones_like(x))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Visualizing the Activations\n",
    "\n",
    "Let's plot each activation function and its derivative side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all activations\n",
    "activations = [\n",
    "    Sigmoid(),\n",
    "    Tanh(),\n",
    "    ReLU(),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    GELU(),\n",
    "    SiLU()\n",
    "]\n",
    "\n",
    "# Generate x values\n",
    "x = np.linspace(-4, 4, 200)\n",
    "\n",
    "# Create the plot\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "\n",
    "for idx, (ax, activation) in enumerate(zip(axes.flat, activations)):\n",
    "    color = colors[idx]\n",
    "    \n",
    "    # Compute forward and gradient\n",
    "    y = activation(x.copy())\n",
    "    grad = activation.gradient(x.copy())\n",
    "    \n",
    "    # Plot activation\n",
    "    ax.plot(x, y, color=color, linewidth=2.5, label='f(x)')\n",
    "    \n",
    "    # Plot gradient\n",
    "    ax.plot(x, grad, color=color, linewidth=2, linestyle='--', alpha=0.7, label=\"f'(x)\")\n",
    "    \n",
    "    # Reference lines\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "    \n",
    "    # Styling\n",
    "    ax.set_title(activation.name, fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(-4, 4)\n",
    "    ax.set_ylim(-2, 4)\n",
    "\n",
    "plt.suptitle('Activation Functions and Their Derivatives', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What to Notice:\n",
    "\n",
    "1. **Sigmoid/Tanh gradients flatten at extremes** ‚Üí Vanishing gradients!\n",
    "2. **ReLU has constant gradient for positive inputs** ‚Üí No vanishing!\n",
    "3. **GELU and SiLU have smooth transitions** ‚Üí Easier optimization\n",
    "4. **Leaky ReLU never has zero gradient** ‚Üí No dead neurons\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The Vanishing Gradient Problem\n",
    "\n",
    "Let's visualize why sigmoid and tanh cause problems in deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_gradient_flow(activation, num_layers: int = 10) -> List[float]:\n",
    "    \"\"\"\n",
    "    Simulate gradient flowing backward through multiple layers.\n",
    "    \n",
    "    In backprop, gradients get multiplied at each layer.\n",
    "    If local gradient < 1, the total gradient shrinks exponentially!\n",
    "    \"\"\"\n",
    "    # Start with gradient = 1\n",
    "    gradient = 1.0\n",
    "    gradient_history = [gradient]\n",
    "    \n",
    "    # Assume activations are around 0 (common after BatchNorm)\n",
    "    x_typical = np.array([0.0])\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        # Get local gradient\n",
    "        local_grad = activation.gradient(x_typical)[0]\n",
    "        \n",
    "        # Chain rule: multiply gradients\n",
    "        gradient *= local_grad\n",
    "        gradient_history.append(gradient)\n",
    "    \n",
    "    return gradient_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare gradient flow through 20 layers\n",
    "num_layers = 20\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Activations to compare\n",
    "test_activations = [Sigmoid(), Tanh(), ReLU(), GELU()]\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#9467bd']\n",
    "\n",
    "for activation, color in zip(test_activations, colors):\n",
    "    history = simulate_gradient_flow(activation, num_layers)\n",
    "    \n",
    "    # Linear scale\n",
    "    axes[0].plot(history, color=color, linewidth=2, marker='o', \n",
    "                 markersize=4, label=activation.name)\n",
    "    \n",
    "    # Log scale\n",
    "    axes[1].semilogy(history, color=color, linewidth=2, marker='o', \n",
    "                     markersize=4, label=activation.name)\n",
    "\n",
    "axes[0].set_xlabel('Layer (from output to input)', fontsize=12)\n",
    "axes[0].set_ylabel('Gradient Magnitude', fontsize=12)\n",
    "axes[0].set_title('Gradient Flow (Linear Scale)', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Layer (from output to input)', fontsize=12)\n",
    "axes[1].set_ylabel('Gradient Magnitude (log scale)', fontsize=12)\n",
    "axes[1].set_title('Gradient Flow (Log Scale)', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìâ Gradient after 20 layers:\")\n",
    "for activation in test_activations:\n",
    "    history = simulate_gradient_flow(activation, num_layers)\n",
    "    print(f\"   {activation.name:15s}: {history[-1]:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Key Insight: Why Sigmoid/Tanh Vanish\n",
    "\n",
    "- Sigmoid's max gradient = 0.25 (at x=0)\n",
    "- After 20 layers: 0.25^20 ‚âà 10^-12 (essentially zero!)\n",
    "- This is why deep sigmoid networks couldn't be trained in the 1990s\n",
    "\n",
    "**ReLU's brilliance:** Gradient = 1 for positive inputs ‚Üí No vanishing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Training Comparison\n",
    "\n",
    "Let's train the same architecture with different activations and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create a flexible MLP that accepts any activation\n",
    "\n",
    "class FlexibleMLP:\n",
    "    \"\"\"\n",
    "    MLP that can use any activation function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes: List[int], activation_class):\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # Linear layer\n",
    "            W = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2.0 / layer_sizes[i])\n",
    "            b = np.zeros(layer_sizes[i + 1])\n",
    "            self.layers.append({'W': W, 'b': b, 'cache': {}, 'dW': None, 'db': None})\n",
    "            \n",
    "            # Add activation for all but last layer\n",
    "            if i < len(layer_sizes) - 2:\n",
    "                self.activations.append(activation_class())\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        out = X\n",
    "        \n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            layer['cache']['X'] = out\n",
    "            out = out @ layer['W'] + layer['b']\n",
    "            out = self.activations[i](out)\n",
    "        \n",
    "        # Last layer (no activation, handled by softmax)\n",
    "        self.layers[-1]['cache']['X'] = out\n",
    "        out = out @ self.layers[-1]['W'] + self.layers[-1]['b']\n",
    "        \n",
    "        # Softmax\n",
    "        out_shifted = out - np.max(out, axis=1, keepdims=True)\n",
    "        exp_out = np.exp(out_shifted)\n",
    "        self.probs = exp_out / np.sum(exp_out, axis=1, keepdims=True)\n",
    "        \n",
    "        return self.probs\n",
    "    \n",
    "    def backward(self, targets: np.ndarray, learning_rate: float = 0.01):\n",
    "        batch_size = targets.shape[0]\n",
    "        \n",
    "        # Gradient from softmax + cross-entropy\n",
    "        grad = self.probs.copy()\n",
    "        grad[np.arange(batch_size), targets] -= 1\n",
    "        \n",
    "        # Backward through layers\n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            layer = self.layers[i]\n",
    "            X = layer['cache']['X']\n",
    "            \n",
    "            # Compute gradients\n",
    "            layer['dW'] = X.T @ grad / batch_size\n",
    "            layer['db'] = np.mean(grad, axis=0)\n",
    "            \n",
    "            # Gradient for next layer\n",
    "            grad = grad @ layer['W'].T\n",
    "            \n",
    "            # Apply activation gradient (except for first layer in backward)\n",
    "            if i > 0:\n",
    "                grad = self.activations[i - 1].backward(grad)\n",
    "            \n",
    "            # Update weights\n",
    "            layer['W'] -= learning_rate * layer['dW']\n",
    "            layer['b'] -= learning_rate * layer['db']\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "import gzip\n",
    "import urllib.request\n",
    "\n",
    "def load_mnist(path='../data'):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    base_url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = {\n",
    "        'train_images': 'train-images-idx3-ubyte.gz',\n",
    "        'train_labels': 'train-labels-idx1-ubyte.gz',\n",
    "        'test_images': 't10k-images-idx3-ubyte.gz',\n",
    "        'test_labels': 't10k-labels-idx1-ubyte.gz'\n",
    "    }\n",
    "    \n",
    "    def download(filename):\n",
    "        filepath = os.path.join(path, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            urllib.request.urlretrieve(base_url + filename, filepath)\n",
    "        return filepath\n",
    "    \n",
    "    def load_images(fp):\n",
    "        with gzip.open(fp, 'rb') as f:\n",
    "            f.read(16)\n",
    "            return np.frombuffer(f.read(), dtype=np.uint8).reshape(-1, 784).astype(np.float32) / 255.0\n",
    "    \n",
    "    def load_labels(fp):\n",
    "        with gzip.open(fp, 'rb') as f:\n",
    "            f.read(8)\n",
    "            return np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    \n",
    "    X_train = load_images(download(files['train_images']))\n",
    "    y_train = load_labels(download(files['train_labels']))\n",
    "    X_test = load_images(download(files['test_images']))\n",
    "    y_test = load_labels(download(files['test_labels']))\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "print(\"Loading MNIST...\")\n",
    "X_train, y_train, X_test, y_test = load_mnist()\n",
    "\n",
    "# Use subset for faster comparison\n",
    "X_train_subset = X_train[:10000]\n",
    "y_train_subset = y_train[:10000]\n",
    "print(f\"Using {len(X_train_subset)} training samples for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(activation_class, epochs=5, lr=0.1):\n",
    "    \"\"\"\n",
    "    Train a model with given activation and return history.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    model = FlexibleMLP([784, 256, 128, 10], activation_class)\n",
    "    \n",
    "    history = {'loss': [], 'accuracy': []}\n",
    "    batch_size = 64\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle data\n",
    "        indices = np.random.permutation(len(X_train_subset))\n",
    "        epoch_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for start in range(0, len(X_train_subset), batch_size):\n",
    "            batch_idx = indices[start:start + batch_size]\n",
    "            X_batch = X_train_subset[batch_idx]\n",
    "            y_batch = y_train_subset[batch_idx]\n",
    "            \n",
    "            # Forward\n",
    "            probs = model.forward(X_batch)\n",
    "            \n",
    "            # Loss\n",
    "            loss = -np.mean(np.log(probs[np.arange(len(y_batch)), y_batch] + 1e-10))\n",
    "            epoch_loss += loss\n",
    "            n_batches += 1\n",
    "            \n",
    "            # Backward\n",
    "            model.backward(y_batch, lr)\n",
    "        \n",
    "        # Record metrics\n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        preds = model.predict(X_test[:1000])\n",
    "        accuracy = np.mean(preds == y_test[:1000])\n",
    "        \n",
    "        history['loss'].append(avg_loss)\n",
    "        history['accuracy'].append(accuracy)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with each activation\n",
    "print(\"üèãÔ∏è Training with different activations...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "activations_to_test = [\n",
    "    ('Sigmoid', Sigmoid),\n",
    "    ('Tanh', Tanh),\n",
    "    ('ReLU', ReLU),\n",
    "    ('LeakyReLU', lambda: LeakyReLU(0.1)),\n",
    "    ('GELU', GELU),\n",
    "    ('SiLU', SiLU)\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, activation_class in activations_to_test:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create a wrapper if needed\n",
    "    if callable(activation_class) and not isinstance(activation_class, type):\n",
    "        act_class = type(name, (), {\n",
    "            '__init__': lambda self: setattr(self, '_act', activation_class()),\n",
    "            '__call__': lambda self, x: self._act(x),\n",
    "            'backward': lambda self, g: self._act.backward(g),\n",
    "            'gradient': lambda self, x: self._act.gradient(x),\n",
    "            'cache': property(lambda self: self._act.cache)\n",
    "        })\n",
    "    else:\n",
    "        act_class = activation_class\n",
    "    \n",
    "    history = train_and_evaluate(activation_class if isinstance(activation_class, type) else (lambda: activation_class()), epochs=5, lr=0.1)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    results[name] = history\n",
    "    \n",
    "    print(f\"{name:12s} | Final Acc: {history['accuracy'][-1]:.2%} | \"\n",
    "          f\"Final Loss: {history['loss'][-1]:.4f} | Time: {elapsed:.2f}s\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(results)))\n",
    "\n",
    "for (name, history), color in zip(results.items(), colors):\n",
    "    axes[0].plot(history['loss'], color=color, linewidth=2, marker='o', \n",
    "                 markersize=5, label=name)\n",
    "    axes[1].plot(history['accuracy'], color=color, linewidth=2, marker='o', \n",
    "                 markersize=5, label=name)\n",
    "\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training Loss by Activation', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Test Accuracy by Activation', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Recommendations Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"                    ACTIVATION FUNCTION RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "recommendations = [\n",
    "    (\"ReLU\", \"Default choice for most networks\", \"CNNs, MLPs, general hidden layers\"),\n",
    "    (\"LeakyReLU\", \"When worried about dead neurons\", \"GANs, very deep networks\"),\n",
    "    (\"GELU\", \"Modern NLP and transformers\", \"BERT, GPT, ViT\"),\n",
    "    (\"SiLU/Swish\", \"Modern efficient architectures\", \"EfficientNet, MobileNet, LLaMA\"),\n",
    "    (\"Sigmoid\", \"Binary classification output only\", \"Final layer for binary prediction\"),\n",
    "    (\"Tanh\", \"When output needs to be in [-1,1]\", \"RNN hidden states, LSTM gates\")\n",
    "]\n",
    "\n",
    "print(f\"{'Activation':<15} {'When to Use':<35} {'Common Applications'}\")\n",
    "print(\"-\" * 80)\n",
    "for name, when, apps in recommendations:\n",
    "    print(f\"{name:<15} {when:<35} {apps}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"                         KEY TAKEAWAYS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "1. NEVER use Sigmoid/Tanh in hidden layers of deep networks (vanishing gradients)\n",
    "\n",
    "2. Start with ReLU - it's simple, fast, and works well\n",
    "\n",
    "3. For transformers, use GELU (it's what BERT/GPT use)\n",
    "\n",
    "4. For efficient models, try SiLU/Swish\n",
    "\n",
    "5. If you see \"dead neurons\" (many zeros), switch to LeakyReLU\n",
    "\n",
    "6. Match your activation to your framework's default for best performance\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "### Mistake 1: Using Sigmoid in hidden layers\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - vanishing gradients\n",
    "model = Sequential([\n",
    "    Linear(784, 256), Sigmoid(),\n",
    "    Linear(256, 128), Sigmoid(),\n",
    "    Linear(128, 10)\n",
    "])\n",
    "\n",
    "# ‚úÖ Right\n",
    "model = Sequential([\n",
    "    Linear(784, 256), ReLU(),\n",
    "    Linear(256, 128), ReLU(),\n",
    "    Linear(128, 10)\n",
    "])\n",
    "```\n",
    "\n",
    "### Mistake 2: Forgetting activation altogether\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - just linear regression!\n",
    "out = Linear(784, 256)(x)\n",
    "out = Linear(256, 10)(out)\n",
    "\n",
    "# ‚úÖ Right\n",
    "out = Linear(784, 256)(x)\n",
    "out = ReLU()(out)  # Don't forget this!\n",
    "out = Linear(256, 10)(out)\n",
    "```\n",
    "\n",
    "### Mistake 3: Wrong activation for output layer\n",
    "\n",
    "```python\n",
    "# For multi-class classification:\n",
    "# ‚úÖ Use Softmax on final layer (gives probabilities that sum to 1)\n",
    "\n",
    "# For binary classification:\n",
    "# ‚úÖ Use Sigmoid on final layer (gives probability of class 1)\n",
    "\n",
    "# For regression:\n",
    "# ‚úÖ Use no activation (or linear) on final layer\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úã Try It Yourself\n",
    "\n",
    "### Exercise 1: Implement PReLU\n",
    "\n",
    "PReLU (Parametric ReLU) is like LeakyReLU, but the slope for negative values is learned:\n",
    "\n",
    "$$\\text{PReLU}(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases}$$\n",
    "\n",
    "Where $\\alpha$ is a learnable parameter!\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Start with LeakyReLU, but make alpha a trainable parameter.\n",
    "You'll need to compute the gradient with respect to alpha too!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Implement PReLU\n",
    "class PReLU:\n",
    "    def __init__(self, alpha_init=0.01):\n",
    "        self.alpha = alpha_init\n",
    "        # TODO: Implement forward and backward\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Experiment with Deep Sigmoid Network\n",
    "\n",
    "Try training a 10-layer network with Sigmoid activations. What happens?\n",
    "Then try with ReLU. Compare the gradients in early layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Compare deep networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Checkpoint\n",
    "\n",
    "You've learned:\n",
    "\n",
    "- ‚úÖ How to implement 6 activation functions from scratch\n",
    "- ‚úÖ Why the vanishing gradient problem occurs\n",
    "- ‚úÖ How to choose the right activation for your task\n",
    "- ‚úÖ The historical evolution of activation functions\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [Delving Deep into Rectifiers (He et al.)](https://arxiv.org/abs/1502.01852) - PReLU and He initialization\n",
    "- [GELU Paper](https://arxiv.org/abs/1606.08415) - Gaussian Error Linear Units\n",
    "- [Swish/SiLU Paper](https://arxiv.org/abs/1710.05941) - Searching for Activation Functions\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")\n",
    "print(\"\\nüéØ Next: Proceed to notebook 03-regularization-experiments.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}