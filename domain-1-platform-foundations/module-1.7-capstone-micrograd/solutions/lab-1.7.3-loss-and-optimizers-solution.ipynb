{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.7.3 Solutions: Loss Functions and Optimizers\n",
    "\n",
    "This notebook contains solutions to the exercises from Lab 1.7.3.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def _find_module_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / 'micrograd_plus' / '__init__.py').exists():\n",
    "            return str(parent)\n",
    "    return str(Path.cwd().parent)\n",
    "\n",
    "sys.path.insert(0, _find_module_root())\n",
    "\n",
    "from micrograd_plus import Tensor, Linear, ReLU, Sequential\n",
    "from micrograd_plus.losses import Loss\n",
    "from micrograd_plus.optimizers import Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Focal Loss\n",
    "\n",
    "Focal Loss addresses class imbalance by down-weighting easy examples.\n",
    "\n",
    "FL(p_t) = -alpha_t * (1 - p_t)^gamma * log(p_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(Loss):\n",
    "    \"\"\"\n",
    "    Focal Loss for handling class imbalance.\n",
    "    \n",
    "    FL(p_t) = -alpha_t * (1 - p_t)^gamma * log(p_t)\n",
    "    \n",
    "    Where:\n",
    "    - p_t is the probability of the correct class\n",
    "    - alpha is the class weight (optional)\n",
    "    - gamma is the focusing parameter\n",
    "    \n",
    "    When gamma=0, this reduces to standard cross-entropy.\n",
    "    Higher gamma focuses more on hard examples.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):\n",
    "        super().__init__(reduction)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, predictions: Tensor, targets: Tensor) -> Tensor:\n",
    "        # Apply softmax to get probabilities\n",
    "        logits = predictions.data\n",
    "        \n",
    "        # Numerically stable softmax\n",
    "        logits_max = np.max(logits, axis=-1, keepdims=True)\n",
    "        exp_logits = np.exp(logits - logits_max)\n",
    "        probs = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "        \n",
    "        # Get probability of correct class\n",
    "        batch_size = logits.shape[0]\n",
    "        target_indices = targets.data.astype(np.int32)\n",
    "        p_t = probs[np.arange(batch_size), target_indices]\n",
    "        \n",
    "        # Compute focal weight: (1 - p_t)^gamma\n",
    "        focal_weight = (1 - p_t) ** self.gamma\n",
    "        \n",
    "        # Compute focal loss\n",
    "        ce_loss = -np.log(p_t + 1e-8)\n",
    "        focal_loss = self.alpha * focal_weight * ce_loss\n",
    "        \n",
    "        # Reduction\n",
    "        if self.reduction == 'mean':\n",
    "            loss_value = np.mean(focal_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            loss_value = np.sum(focal_loss)\n",
    "        else:\n",
    "            loss_value = focal_loss\n",
    "        \n",
    "        out = Tensor(loss_value, requires_grad=predictions.requires_grad)\n",
    "        \n",
    "        if predictions.requires_grad:\n",
    "            out._prev = {predictions}\n",
    "            out._op = 'focal_loss'\n",
    "            \n",
    "            def _backward():\n",
    "                # Gradient of focal loss\n",
    "                grad = probs.copy()\n",
    "                grad[np.arange(batch_size), target_indices] -= 1\n",
    "                \n",
    "                # Apply focal weight to gradient\n",
    "                focal_grad = self.alpha * (\n",
    "                    self.gamma * (1 - p_t) ** (self.gamma - 1) * p_t * np.log(p_t + 1e-8)\n",
    "                    + (1 - p_t) ** self.gamma\n",
    "                )\n",
    "                grad *= focal_grad[:, np.newaxis]\n",
    "                \n",
    "                if self.reduction == 'mean':\n",
    "                    grad /= batch_size\n",
    "                \n",
    "                predictions.grad = predictions.grad + out.grad * grad if predictions.grad is not None else out.grad * grad\n",
    "            \n",
    "            out._backward = _backward\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Focal Loss\n",
    "print(\"Testing Focal Loss\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create imbalanced scenario: easy and hard examples\n",
    "logits_easy = Tensor([[5.0, 0.0, 0.0]])  # Very confident about class 0\n",
    "logits_hard = Tensor([[0.5, 0.3, 0.2]])  # Not confident\n",
    "\n",
    "targets = Tensor([0])  # True class is 0\n",
    "\n",
    "focal = FocalLoss(gamma=2.0)\n",
    "ce = FocalLoss(gamma=0.0)  # Standard cross-entropy\n",
    "\n",
    "print(\"Easy example (high confidence):\")\n",
    "print(f\"  Cross-Entropy: {ce(logits_easy, targets).data:.4f}\")\n",
    "print(f\"  Focal Loss:    {focal(logits_easy, targets).data:.4f}\")\n",
    "\n",
    "print(\"\\nHard example (low confidence):\")\n",
    "print(f\"  Cross-Entropy: {ce(logits_hard, targets).data:.4f}\")\n",
    "print(f\"  Focal Loss:    {focal(logits_hard, targets).data:.4f}\")\n",
    "\n",
    "print(\"\\nFocal loss focuses more on hard examples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2 Solution: Smooth L1 Loss (Huber Loss)\n",
    "\n",
    "Combines L1 and L2 loss for robustness to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothL1Loss(Loss):\n",
    "    \"\"\"\n",
    "    Smooth L1 Loss (Huber Loss).\n",
    "    \n",
    "    L(x) = 0.5 * x^2           if |x| < beta\n",
    "    L(x) = |x| - 0.5 * beta    otherwise\n",
    "    \n",
    "    Benefits:\n",
    "    - Less sensitive to outliers than L2\n",
    "    - Smoother than L1 near zero\n",
    "    - Used in object detection (Faster R-CNN)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, beta=1.0, reduction='mean'):\n",
    "        super().__init__(reduction)\n",
    "        self.beta = beta\n",
    "    \n",
    "    def forward(self, predictions: Tensor, targets: Tensor) -> Tensor:\n",
    "        diff = predictions.data - targets.data\n",
    "        abs_diff = np.abs(diff)\n",
    "        \n",
    "        # Compute smooth L1\n",
    "        small_mask = abs_diff < self.beta\n",
    "        loss = np.where(\n",
    "            small_mask,\n",
    "            0.5 * diff ** 2 / self.beta,\n",
    "            abs_diff - 0.5 * self.beta\n",
    "        )\n",
    "        \n",
    "        # Reduction\n",
    "        if self.reduction == 'mean':\n",
    "            loss_value = np.mean(loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            loss_value = np.sum(loss)\n",
    "        else:\n",
    "            loss_value = loss\n",
    "        \n",
    "        out = Tensor(loss_value, requires_grad=predictions.requires_grad)\n",
    "        \n",
    "        if predictions.requires_grad:\n",
    "            out._prev = {predictions}\n",
    "            out._op = 'smooth_l1'\n",
    "            \n",
    "            def _backward():\n",
    "                # Gradient: x/beta if |x| < beta, else sign(x)\n",
    "                grad = np.where(\n",
    "                    small_mask,\n",
    "                    diff / self.beta,\n",
    "                    np.sign(diff)\n",
    "                )\n",
    "                \n",
    "                if self.reduction == 'mean':\n",
    "                    grad /= predictions.data.size\n",
    "                \n",
    "                predictions.grad = predictions.grad + out.grad * grad if predictions.grad is not None else out.grad * grad\n",
    "            \n",
    "            out._backward = _backward\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Smooth L1 Loss\n",
    "print(\"Testing Smooth L1 Loss\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from micrograd_plus import MSELoss\n",
    "\n",
    "pred = Tensor([0.0, 1.0, 5.0, 10.0], requires_grad=True)  # Include outlier (10.0)\n",
    "target = Tensor([0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "smooth_l1 = SmoothL1Loss(beta=1.0)\n",
    "mse = MSELoss()\n",
    "\n",
    "loss_smooth = smooth_l1(pred, target)\n",
    "loss_mse = mse(pred, target)\n",
    "\n",
    "print(f\"Predictions: {pred.data}\")\n",
    "print(f\"Targets:     {target.data}\")\n",
    "print(f\"\\nSmooth L1 Loss: {loss_smooth.data:.4f}\")\n",
    "print(f\"MSE Loss:       {loss_mse.data:.4f}\")\n",
    "print(\"\\nSmooth L1 is less affected by the outlier (10.0)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3 Solution: Contrastive Loss\n",
    "\n",
    "Used for learning embeddings (Siamese networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(Loss):\n",
    "    \"\"\"\n",
    "    Contrastive Loss for learning embeddings.\n",
    "    \n",
    "    L = (1-Y) * 0.5 * D^2 + Y * 0.5 * max(0, margin - D)^2\n",
    "    \n",
    "    Where:\n",
    "    - D is the distance between embeddings\n",
    "    - Y is 1 for dissimilar pairs, 0 for similar pairs\n",
    "    - margin is the minimum distance for dissimilar pairs\n",
    "    \n",
    "    Used in:\n",
    "    - Face verification (Siamese networks)\n",
    "    - Learning similarity metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, margin=1.0, reduction='mean'):\n",
    "        super().__init__(reduction)\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, embedding1: Tensor, embedding2: Tensor, labels: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding1: First set of embeddings (batch, dim)\n",
    "            embedding2: Second set of embeddings (batch, dim)\n",
    "            labels: 0 for similar, 1 for dissimilar pairs\n",
    "        \"\"\"\n",
    "        # Compute Euclidean distance\n",
    "        diff = embedding1.data - embedding2.data\n",
    "        distance_sq = np.sum(diff ** 2, axis=-1)\n",
    "        distance = np.sqrt(distance_sq + 1e-8)\n",
    "        \n",
    "        # Contrastive loss\n",
    "        y = labels.data\n",
    "        margin_distance = np.maximum(0, self.margin - distance)\n",
    "        \n",
    "        loss = (1 - y) * 0.5 * distance_sq + y * 0.5 * margin_distance ** 2\n",
    "        \n",
    "        # Reduction\n",
    "        if self.reduction == 'mean':\n",
    "            loss_value = np.mean(loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            loss_value = np.sum(loss)\n",
    "        else:\n",
    "            loss_value = loss\n",
    "        \n",
    "        out = Tensor(loss_value, requires_grad=embedding1.requires_grad)\n",
    "        \n",
    "        if embedding1.requires_grad:\n",
    "            out._prev = {embedding1, embedding2}\n",
    "            out._op = 'contrastive'\n",
    "            \n",
    "            def _backward():\n",
    "                # Gradient w.r.t. embeddings\n",
    "                grad1 = (1 - y)[:, np.newaxis] * diff\n",
    "                \n",
    "                dissimilar_grad = np.where(\n",
    "                    distance[:, np.newaxis] < self.margin,\n",
    "                    -margin_distance[:, np.newaxis] * diff / (distance[:, np.newaxis] + 1e-8),\n",
    "                    0\n",
    "                )\n",
    "                grad1 += y[:, np.newaxis] * dissimilar_grad\n",
    "                \n",
    "                if self.reduction == 'mean':\n",
    "                    grad1 /= labels.data.size\n",
    "                \n",
    "                embedding1.grad = embedding1.grad + out.grad * grad1 if embedding1.grad is not None else out.grad * grad1\n",
    "                embedding2.grad = embedding2.grad - out.grad * grad1 if embedding2.grad is not None else -out.grad * grad1\n",
    "            \n",
    "            out._backward = _backward\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Contrastive Loss\n",
    "print(\"Testing Contrastive Loss\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create embedding pairs\n",
    "e1 = Tensor(np.random.randn(4, 8).astype(np.float32), requires_grad=True)\n",
    "e2_similar = Tensor(e1.data + 0.1 * np.random.randn(4, 8).astype(np.float32))  # Close to e1\n",
    "e2_dissimilar = Tensor(np.random.randn(4, 8).astype(np.float32))  # Far from e1\n",
    "\n",
    "labels_similar = Tensor(np.zeros(4))  # 0 = similar\n",
    "labels_dissimilar = Tensor(np.ones(4))  # 1 = dissimilar\n",
    "\n",
    "contrastive = ContrastiveLoss(margin=1.0)\n",
    "\n",
    "loss_similar = contrastive(e1, e2_similar, labels_similar)\n",
    "loss_dissimilar = contrastive(e1, e2_dissimilar, labels_dissimilar)\n",
    "\n",
    "print(f\"Similar pair loss:    {loss_similar.data:.4f} (should be small)\")\n",
    "print(f\"Dissimilar pair loss: {loss_dissimilar.data:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4 Solution: Implement AdaGrad Optimizer\n",
    "\n",
    "AdaGrad adapts learning rates based on historical gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad(Optimizer):\n",
    "    \"\"\"\n",
    "    Adagrad optimizer.\n",
    "    \n",
    "    Adapts learning rate for each parameter based on historical gradients.\n",
    "    \n",
    "    Update rule:\n",
    "    G_t = G_{t-1} + grad^2\n",
    "    param = param - lr * grad / sqrt(G_t + eps)\n",
    "    \n",
    "    Pros:\n",
    "    - Adapts learning rate per parameter\n",
    "    - Good for sparse data\n",
    "    \n",
    "    Cons:\n",
    "    - Learning rate decays too aggressively over time\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params, lr=0.01, eps=1e-8, weight_decay=0.0):\n",
    "        super().__init__(params, lr)\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # Initialize sum of squared gradients\n",
    "        self.sum_sq = [np.zeros_like(p.data) for p in self.params]\n",
    "    \n",
    "    def step(self):\n",
    "        for i, param in enumerate(self.params):\n",
    "            if param.grad is None:\n",
    "                continue\n",
    "            \n",
    "            grad = param.grad\n",
    "            \n",
    "            # Apply weight decay\n",
    "            if self.weight_decay != 0:\n",
    "                grad = grad + self.weight_decay * param.data\n",
    "            \n",
    "            # Accumulate squared gradients\n",
    "            self.sum_sq[i] += grad ** 2\n",
    "            \n",
    "            # Update parameter\n",
    "            param.data -= self.lr * grad / (np.sqrt(self.sum_sq[i]) + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Adagrad\n",
    "print(\"Testing Adagrad Optimizer\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simple optimization problem: minimize (x - 3)^2 + (y - 4)^2\n",
    "x = Tensor([0.0], requires_grad=True)\n",
    "y = Tensor([0.0], requires_grad=True)\n",
    "\n",
    "optimizer = Adagrad([x, y], lr=1.0)\n",
    "\n",
    "print(\"Optimizing f(x,y) = (x-3)^2 + (y-4)^2\")\n",
    "print(f\"Initial: x={x.item():.2f}, y={y.item():.2f}\")\n",
    "\n",
    "for i in range(100):\n",
    "    loss = (x - 3) ** 2 + (y - 4) ** 2\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"Final:   x={x.item():.2f}, y={y.item():.2f}\")\n",
    "print(f\"Expected: x=3.00, y=4.00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Solution: Learning Rate Warmup + Decay\n",
    "\n",
    "Common in training transformers: linear warmup followed by cosine decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupCosineScheduler:\n",
    "    \"\"\"\n",
    "    Learning rate scheduler with linear warmup and cosine decay.\n",
    "    \n",
    "    Used in training transformers (BERT, GPT, etc.).\n",
    "    \n",
    "    Schedule:\n",
    "    1. Linear warmup: 0 -> max_lr over warmup_steps\n",
    "    2. Cosine decay: max_lr -> min_lr over remaining steps\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, warmup_steps, total_steps, min_lr=0.0):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = optimizer.lr\n",
    "        self.current_step = 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.current_step += 1\n",
    "        \n",
    "        if self.current_step <= self.warmup_steps:\n",
    "            # Linear warmup\n",
    "            lr = self.max_lr * (self.current_step / self.warmup_steps)\n",
    "        else:\n",
    "            # Cosine decay\n",
    "            progress = (self.current_step - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n",
    "            lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(np.pi * progress))\n",
    "        \n",
    "        self.optimizer.lr = lr\n",
    "        return lr\n",
    "    \n",
    "    def get_lr(self):\n",
    "        return self.optimizer.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize warmup + cosine schedule\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = Tensor([0.0], requires_grad=True)\n",
    "opt = Adagrad([x], lr=0.001)\n",
    "\n",
    "scheduler = WarmupCosineScheduler(\n",
    "    optimizer=opt,\n",
    "    warmup_steps=100,\n",
    "    total_steps=1000,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "lrs = []\n",
    "for step in range(1000):\n",
    "    lr = scheduler.step()\n",
    "    lrs.append(lr)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(lrs)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Warmup + Cosine Decay Schedule')\n",
    "plt.axvline(x=100, color='r', linestyle='--', label='End of warmup')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Focal Loss**: Down-weights easy examples, focuses on hard ones. Great for class imbalance.\n",
    "\n",
    "2. **Smooth L1 (Huber)**: Combines L1/L2, robust to outliers.\n",
    "\n",
    "3. **Contrastive Loss**: Learns embeddings where similar items are close, dissimilar are far.\n",
    "\n",
    "4. **Adagrad**: Adapts learning rate per parameter based on gradient history.\n",
    "\n",
    "5. **Learning Rate Schedules**: Warmup prevents early training instability, decay improves convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
