{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.7.2 Solutions: Layer Implementation\n",
    "\n",
    "This notebook contains solutions to the exercises from Lab 1.7.2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution: Custom Activation Function (LeakyReLU)\n",
    "\n",
    "Implement LeakyReLU: f(x) = x if x > 0, else alpha * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def _find_module_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / 'micrograd_plus' / '__init__.py').exists():\n",
    "            return str(parent)\n",
    "    return str(Path.cwd().parent)\n",
    "\n",
    "sys.path.insert(0, _find_module_root())\n",
    "\n",
    "from micrograd_plus import Tensor\n",
    "from micrograd_plus.layers import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLU(Module):\n",
    "    \"\"\"\n",
    "    LeakyReLU activation function.\n",
    "    \n",
    "    f(x) = x if x > 0\n",
    "    f(x) = alpha * x if x <= 0\n",
    "    \n",
    "    This prevents \"dying ReLU\" by allowing small negative gradients.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, negative_slope=0.01):\n",
    "        super().__init__()\n",
    "        self.negative_slope = negative_slope\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # For x > 0: output = x\n",
    "        # For x <= 0: output = alpha * x\n",
    "        \n",
    "        positive_mask = (x.data > 0).astype(np.float32)\n",
    "        negative_mask = (x.data <= 0).astype(np.float32)\n",
    "        \n",
    "        # Create output\n",
    "        out_data = x.data * positive_mask + self.negative_slope * x.data * negative_mask\n",
    "        out = Tensor(out_data, requires_grad=x.requires_grad)\n",
    "        \n",
    "        if x.requires_grad:\n",
    "            out._prev = {x}\n",
    "            out._op = 'leaky_relu'\n",
    "            \n",
    "            def _backward():\n",
    "                # Gradient is 1 for positive, alpha for negative\n",
    "                grad_mask = positive_mask + self.negative_slope * negative_mask\n",
    "                x.grad = x.grad + out.grad * grad_mask if x.grad is not None else out.grad * grad_mask\n",
    "            \n",
    "            out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"LeakyReLU(negative_slope={self.negative_slope})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LeakyReLU\n",
    "print(\"Testing LeakyReLU\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "leaky_relu = LeakyReLU(negative_slope=0.1)\n",
    "\n",
    "# Test forward pass\n",
    "x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)\n",
    "y = leaky_relu(x)\n",
    "\n",
    "print(f\"Input:  {x.data}\")\n",
    "print(f\"Output: {y.data}\")\n",
    "print(f\"Expected: [-0.2, -0.1, 0.0, 1.0, 2.0]\")\n",
    "\n",
    "# Test backward pass\n",
    "y.sum().backward()\n",
    "print(f\"\\nGradient: {x.grad}\")\n",
    "print(f\"Expected: [0.1, 0.1, 0.1, 1.0, 1.0]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2 Solution: Implement ELU Activation\n",
    "\n",
    "ELU: f(x) = x if x > 0, else alpha * (exp(x) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELU(Module):\n",
    "    \"\"\"\n",
    "    Exponential Linear Unit.\n",
    "    \n",
    "    f(x) = x if x > 0\n",
    "    f(x) = alpha * (exp(x) - 1) if x <= 0\n",
    "    \n",
    "    Properties:\n",
    "    - Smooth everywhere (unlike ReLU)\n",
    "    - Outputs can be negative (unlike ReLU)\n",
    "    - Helps push mean activations toward zero\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        positive_mask = (x.data > 0).astype(np.float32)\n",
    "        negative_mask = (x.data <= 0).astype(np.float32)\n",
    "        \n",
    "        # Compute ELU\n",
    "        positive_part = x.data * positive_mask\n",
    "        negative_part = self.alpha * (np.exp(x.data) - 1) * negative_mask\n",
    "        out_data = positive_part + negative_part\n",
    "        \n",
    "        out = Tensor(out_data, requires_grad=x.requires_grad)\n",
    "        \n",
    "        if x.requires_grad:\n",
    "            out._prev = {x}\n",
    "            out._op = 'elu'\n",
    "            \n",
    "            def _backward():\n",
    "                # Gradient: 1 for x > 0, alpha * exp(x) for x <= 0\n",
    "                grad = positive_mask + self.alpha * np.exp(x.data) * negative_mask\n",
    "                x.grad = x.grad + out.grad * grad if x.grad is not None else out.grad * grad\n",
    "            \n",
    "            out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ELU(alpha={self.alpha})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ELU\n",
    "print(\"Testing ELU\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "elu = ELU(alpha=1.0)\n",
    "\n",
    "x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)\n",
    "y = elu(x)\n",
    "\n",
    "print(f\"Input:  {x.data}\")\n",
    "print(f\"Output: {y.data}\")\n",
    "\n",
    "# Expected: [alpha*(exp(-2)-1), alpha*(exp(-1)-1), 0, 1, 2]\n",
    "expected = [1.0*(np.exp(-2)-1), 1.0*(np.exp(-1)-1), 0.0, 1.0, 2.0]\n",
    "print(f\"Expected: {expected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3 Solution: GELU Activation\n",
    "\n",
    "GELU (Gaussian Error Linear Unit) is used in transformers like BERT and GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(Module):\n",
    "    \"\"\"\n",
    "    Gaussian Error Linear Unit.\n",
    "    \n",
    "    GELU(x) = x * Phi(x) where Phi is the CDF of standard normal distribution.\n",
    "    \n",
    "    Approximation: 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))\n",
    "    \n",
    "    Used in BERT, GPT-2, and other transformers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, approximate=True):\n",
    "        super().__init__()\n",
    "        self.approximate = approximate\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.approximate:\n",
    "            # Approximate GELU using tanh\n",
    "            sqrt_2_pi = np.sqrt(2.0 / np.pi)\n",
    "            cdf_approx = 0.5 * (1.0 + np.tanh(sqrt_2_pi * (x.data + 0.044715 * x.data ** 3)))\n",
    "            out_data = x.data * cdf_approx\n",
    "        else:\n",
    "            # Exact GELU using error function\n",
    "            from scipy.special import erf\n",
    "            cdf = 0.5 * (1.0 + erf(x.data / np.sqrt(2.0)))\n",
    "            out_data = x.data * cdf\n",
    "        \n",
    "        out = Tensor(out_data, requires_grad=x.requires_grad)\n",
    "        \n",
    "        if x.requires_grad:\n",
    "            out._prev = {x}\n",
    "            out._op = 'gelu'\n",
    "            \n",
    "            def _backward():\n",
    "                if self.approximate:\n",
    "                    # Derivative of approximate GELU\n",
    "                    sqrt_2_pi = np.sqrt(2.0 / np.pi)\n",
    "                    inner = sqrt_2_pi * (x.data + 0.044715 * x.data ** 3)\n",
    "                    tanh_inner = np.tanh(inner)\n",
    "                    sech2 = 1.0 - tanh_inner ** 2\n",
    "                    inner_deriv = sqrt_2_pi * (1 + 3 * 0.044715 * x.data ** 2)\n",
    "                    \n",
    "                    grad = 0.5 * (1 + tanh_inner) + 0.5 * x.data * sech2 * inner_deriv\n",
    "                else:\n",
    "                    from scipy.special import erf\n",
    "                    cdf = 0.5 * (1.0 + erf(x.data / np.sqrt(2.0)))\n",
    "                    pdf = np.exp(-0.5 * x.data ** 2) / np.sqrt(2 * np.pi)\n",
    "                    grad = cdf + x.data * pdf\n",
    "                \n",
    "                x.grad = x.grad + out.grad * grad if x.grad is not None else out.grad * grad\n",
    "            \n",
    "            out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"GELU(approximate={self.approximate})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GELU\n",
    "print(\"Testing GELU\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "gelu = GELU(approximate=True)\n",
    "\n",
    "x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)\n",
    "y = gelu(x)\n",
    "\n",
    "print(f\"Input:  {x.data}\")\n",
    "print(f\"Output: {y.data}\")\n",
    "\n",
    "# Compare with expected values\n",
    "print(f\"\\nGELU(0) = {y.data[2]:.4f} (expected ~0)\")\n",
    "print(f\"GELU(1) = {y.data[3]:.4f} (expected ~0.841)\")\n",
    "print(f\"GELU(-1) = {y.data[1]:.4f} (expected ~-0.159)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4 Solution: Implement a Custom Layer (Residual Block)\n",
    "\n",
    "Implement a residual connection: output = F(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd_plus import Linear, ReLU, Sequential\n",
    "\n",
    "class ResidualBlock(Module):\n",
    "    \"\"\"\n",
    "    Residual Block with skip connection.\n",
    "    \n",
    "    output = F(x) + x\n",
    "    \n",
    "    Where F is typically two linear layers with activation.\n",
    "    Skip connections help with:\n",
    "    - Gradient flow in deep networks\n",
    "    - Training deeper models\n",
    "    - Learning identity mappings when needed\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, features, hidden_factor=4):\n",
    "        super().__init__()\n",
    "        hidden = features * hidden_factor\n",
    "        \n",
    "        self.block = Sequential(\n",
    "            Linear(features, hidden),\n",
    "            ReLU(),\n",
    "            Linear(hidden, features)\n",
    "        )\n",
    "        self.relu = ReLU()\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # F(x) + x\n",
    "        residual = x\n",
    "        out = self.block(x)\n",
    "        out = out + residual  # Skip connection\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.block.parameters()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ResidualBlock({self.block})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Residual Block\n",
    "print(\"Testing ResidualBlock\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "np.random.seed(42)\n",
    "block = ResidualBlock(features=8)\n",
    "\n",
    "x = Tensor(np.random.randn(4, 8).astype(np.float32), requires_grad=True)\n",
    "y = block(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Parameters: {sum(p.data.size for p in block.parameters())}\")\n",
    "\n",
    "# Test gradient flow\n",
    "y.sum().backward()\n",
    "print(f\"Input gradient exists: {x.grad is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Solution: Multi-Head Attention Layer\n",
    "\n",
    "Implement scaled dot-product attention used in transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention mechanism from \"Attention Is All You Need\".\n",
    "    \n",
    "    Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n",
    "    \n",
    "    Multi-head allows the model to jointly attend to information\n",
    "    from different representation subspaces.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.q_proj = Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, query: Tensor, key: Tensor = None, value: Tensor = None) -> Tensor:\n",
    "        \"\"\"Self-attention when key/value not provided.\"\"\"\n",
    "        if key is None:\n",
    "            key = query\n",
    "        if value is None:\n",
    "            value = query\n",
    "        \n",
    "        batch_size, seq_len, _ = query.shape\n",
    "        \n",
    "        # Project Q, K, V\n",
    "        Q = self.q_proj(query)  # (batch, seq, embed)\n",
    "        K = self.k_proj(key)\n",
    "        V = self.v_proj(value)\n",
    "        \n",
    "        # Reshape for multi-head: (batch, seq, heads, head_dim)\n",
    "        Q = Q.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Transpose to (batch, heads, seq, head_dim)\n",
    "        Q = Q.transpose(0, 2, 1, 3)\n",
    "        K = K.transpose(0, 2, 1, 3)\n",
    "        V = V.transpose(0, 2, 1, 3)\n",
    "        \n",
    "        # Compute attention scores: (batch, heads, seq, seq)\n",
    "        scores = (Q @ K.transpose(0, 1, 3, 2)) * self.scale\n",
    "        \n",
    "        # Apply softmax\n",
    "        attn_weights = scores.softmax(axis=-1)\n",
    "        \n",
    "        # Apply attention to values: (batch, heads, seq, head_dim)\n",
    "        attn_output = attn_weights @ V\n",
    "        \n",
    "        # Reshape back: (batch, seq, embed)\n",
    "        attn_output = attn_output.transpose(0, 2, 1, 3)\n",
    "        attn_output = attn_output.reshape(batch_size, seq_len, self.embed_dim)\n",
    "        \n",
    "        # Final projection\n",
    "        output = self.out_proj(attn_output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for proj in [self.q_proj, self.k_proj, self.v_proj, self.out_proj]:\n",
    "            params.extend(proj.parameters())\n",
    "        return params\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"MultiHeadAttention(embed_dim={self.embed_dim}, num_heads={self.num_heads})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Multi-Head Attention\n",
    "print(\"Testing MultiHeadAttention\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "np.random.seed(42)\n",
    "mha = MultiHeadAttention(embed_dim=64, num_heads=8)\n",
    "\n",
    "# Input: (batch, sequence_length, embed_dim)\n",
    "x = Tensor(np.random.randn(2, 10, 64).astype(np.float32), requires_grad=True)\n",
    "y = mha(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Parameters: {sum(p.data.size for p in mha.parameters())}\")\n",
    "\n",
    "# Test gradient flow\n",
    "y.sum().backward()\n",
    "print(f\"Gradient computed: {x.grad is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Custom Activations**: Can be implemented by defining forward/backward with proper gradient computation\n",
    "\n",
    "2. **LeakyReLU**: Allows small negative gradients (prevents dying ReLU)\n",
    "\n",
    "3. **ELU**: Smooth activation with negative values to center activations\n",
    "\n",
    "4. **GELU**: Gaussian-gated activation used in modern transformers\n",
    "\n",
    "5. **Residual Blocks**: Skip connections enable training much deeper networks\n",
    "\n",
    "6. **Multi-Head Attention**: Core building block of transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
