{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.7.6 Solutions: Documentation and Benchmarks\n",
    "\n",
    "This notebook contains notes and additional examples for the documentation and benchmarking concepts from Lab 1.7.6.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on This Notebook\n",
    "\n",
    "Lab 1.7.6 (Documentation and Benchmarks) is primarily an educational walkthrough that:\n",
    "\n",
    "1. Reviews the API documentation of MicroGrad+\n",
    "2. Compares MicroGrad+ performance against PyTorch\n",
    "3. Explains why PyTorch is faster\n",
    "4. Summarizes what was accomplished in the capstone\n",
    "\n",
    "Unlike the previous notebooks, Lab 1.7.6 does not have specific exercises to solve. Instead, this solution notebook provides additional context and examples for students who want to explore further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def _find_module_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / 'micrograd_plus' / '__init__.py').exists():\n",
    "            return str(parent)\n",
    "    return str(Path.cwd().parent)\n",
    "\n",
    "sys.path.insert(0, _find_module_root())\n",
    "\n",
    "from micrograd_plus import (\n",
    "    Tensor, Linear, ReLU, Dropout, Sequential,\n",
    "    CrossEntropyLoss, MSELoss, Adam, SGD\n",
    ")\n",
    "from micrograd_plus.utils import set_seed\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Benchmarking Examples\n",
    "\n",
    "Here are some additional benchmarking patterns you might find useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_operation(name, operation, n_runs=100, warmup=5):\n",
    "    \"\"\"\n",
    "    Benchmark a single operation with proper warmup and timing.\n",
    "    \n",
    "    Args:\n",
    "        name: Name of the operation for display\n",
    "        operation: Callable that performs the operation\n",
    "        n_runs: Number of timed runs\n",
    "        warmup: Number of warmup runs (not timed)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with timing statistics\n",
    "    \"\"\"\n",
    "    # Warmup runs\n",
    "    for _ in range(warmup):\n",
    "        operation()\n",
    "    \n",
    "    # Timed runs\n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.perf_counter()\n",
    "        operation()\n",
    "        elapsed = time.perf_counter() - start\n",
    "        times.append(elapsed)\n",
    "    \n",
    "    times = np.array(times)\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'mean_ms': times.mean() * 1000,\n",
    "        'std_ms': times.std() * 1000,\n",
    "        'min_ms': times.min() * 1000,\n",
    "        'max_ms': times.max() * 1000\n",
    "    }\n",
    "\n",
    "# Example: Benchmark matrix multiplication\n",
    "A = Tensor(np.random.randn(256, 512).astype(np.float32))\n",
    "B = Tensor(np.random.randn(512, 256).astype(np.float32))\n",
    "\n",
    "result = benchmark_operation(\n",
    "    'MatMul (256x512 @ 512x256)',\n",
    "    lambda: A @ B,\n",
    "    n_runs=100\n",
    ")\n",
    "\n",
    "print(f\"Operation: {result['name']}\")\n",
    "print(f\"  Mean: {result['mean_ms']:.2f} ms\")\n",
    "print(f\"  Std:  {result['std_ms']:.2f} ms\")\n",
    "print(f\"  Min:  {result['min_ms']:.2f} ms\")\n",
    "print(f\"  Max:  {result['max_ms']:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Writing Good Documentation\n",
    "\n",
    "Here's an example of comprehensive documentation following Google-style docstrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_documented_function(input_tensor, learning_rate=0.01, momentum=0.9,\n",
    "                                 epsilon=1e-8, verbose=False):\n",
    "    \"\"\"\n",
    "    Perform an example computation with comprehensive documentation.\n",
    "    \n",
    "    This function demonstrates how to write good documentation following\n",
    "    Google-style docstring conventions. The docstring should include:\n",
    "    \n",
    "    1. A brief one-line summary\n",
    "    2. A longer description if needed\n",
    "    3. Arguments with types and descriptions\n",
    "    4. Return value with type and description\n",
    "    5. Exceptions that might be raised\n",
    "    6. Example usage\n",
    "    \n",
    "    Args:\n",
    "        input_tensor: Input tensor of shape (batch_size, features).\n",
    "            Must be a Tensor object with requires_grad=True for\n",
    "            gradient computation.\n",
    "        learning_rate: Step size for the computation. Defaults to 0.01.\n",
    "            Typical values are in the range [1e-4, 1e-1].\n",
    "        momentum: Momentum factor for smoothing. Defaults to 0.9.\n",
    "            Set to 0 to disable momentum.\n",
    "        epsilon: Small constant for numerical stability. Defaults to 1e-8.\n",
    "        verbose: Whether to print debug information. Defaults to False.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor: The computed output tensor of the same shape as input.\n",
    "            The output maintains gradient tracking if input has gradients.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If input_tensor has fewer than 2 dimensions.\n",
    "        TypeError: If input_tensor is not a Tensor object.\n",
    "    \n",
    "    Example:\n",
    "        >>> x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "        >>> output = example_documented_function(x, learning_rate=0.01)\n",
    "        >>> print(output.shape)  # (2, 2)\n",
    "    \n",
    "    Note:\n",
    "        This is a demonstration function and doesn't perform any\n",
    "        meaningful computation. See the MicroGrad+ source code for\n",
    "        real implementations.\n",
    "    \"\"\"\n",
    "    if not isinstance(input_tensor, Tensor):\n",
    "        raise TypeError(f\"Expected Tensor, got {type(input_tensor)}\")\n",
    "    \n",
    "    if len(input_tensor.shape) < 2:\n",
    "        raise ValueError(f\"Input must have at least 2 dimensions, got {len(input_tensor.shape)}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Input shape: {input_tensor.shape}\")\n",
    "        print(f\"Learning rate: {learning_rate}\")\n",
    "    \n",
    "    # Example computation (just returns scaled input)\n",
    "    return input_tensor * learning_rate\n",
    "\n",
    "# Test the documentation\n",
    "help(example_documented_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Key documentation best practices:\n",
    "\n",
    "1. **Module docstring**: Explain what the module does and provide a quick example\n",
    "2. **Class docstring**: Describe purpose, args, attributes, and usage\n",
    "3. **Method docstring**: Document args, returns, raises, and provide examples\n",
    "4. **Type hints**: Use them for better IDE support and clarity\n",
    "\n",
    "Key benchmarking best practices:\n",
    "\n",
    "1. **Warmup runs**: Always run a few iterations before timing\n",
    "2. **Multiple runs**: Take average over many runs for reliable results\n",
    "3. **Report statistics**: Include mean, std, min, max\n",
    "4. **Fair comparison**: Compare at the same batch sizes and configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed all the notebooks in the Domain 1 Capstone: MicroGrad+ project!\n",
    "\n",
    "**What you've accomplished:**\n",
    "\n",
    "- Built a complete autograd engine from scratch\n",
    "- Implemented neural network layers (Linear, ReLU, Sigmoid, Softmax, Dropout)\n",
    "- Created loss functions (MSE, CrossEntropy)\n",
    "- Built optimizers (SGD with momentum, Adam)\n",
    "- Trained a real model on MNIST\n",
    "- Learned to test and benchmark your implementations\n",
    "\n",
    "**Next steps:**\n",
    "\n",
    "In Domain 2, you'll use PyTorch to:\n",
    "- Work with real GPU hardware on DGX Spark\n",
    "- Build advanced architectures (CNNs, RNNs, Transformers)\n",
    "- Handle real-world datasets\n",
    "- Scale to larger models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
