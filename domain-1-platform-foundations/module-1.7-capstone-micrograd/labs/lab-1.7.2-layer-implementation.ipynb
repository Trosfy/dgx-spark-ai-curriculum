{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab 1.7.2: Layer Implementation\n",
    "\n",
    "**Module:** 1.7 - Domain 1 Capstone: MicroGrad+  \n",
    "**Time:** 2 hours  \n",
    "**Difficulty:** â­â­â­â­\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand how neural network layers are structured\n",
    "- [ ] Implement Linear, ReLU, Sigmoid, Softmax layers\n",
    "- [ ] Build a Dropout layer that behaves differently in train/eval mode\n",
    "- [ ] Combine layers into a Sequential container\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Prerequisites\n",
    "\n",
    "- Completed: Lab 1.7.1 (Core Tensor Implementation)\n",
    "- Knowledge of: Neural network basics, activation functions\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ Real-World Context\n",
    "\n",
    "Layers are the building blocks of neural networks. Understanding how they work helps you:\n",
    "- Design custom architectures for your specific problems\n",
    "- Debug training issues (\"Why are my gradients zero?\")\n",
    "- Implement novel layers from research papers\n",
    "- Optimize inference by understanding what each layer does"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ§’ ELI5: Neural Network Layers\n",
    "\n",
    "> **Imagine a factory assembly line** where each station does one specific job.\n",
    ">\n",
    "> - **Linear Layer**: A station that weighs and combines ingredients. Each output is a weighted sum of inputs, like mixing paint colors.\n",
    ">\n",
    "> - **ReLU Activation**: A quality control station that only lets positive values through. Negative? Becomes zero. Positive? Passes unchanged.\n",
    ">\n",
    "> - **Softmax**: A station that converts scores into percentages. \"You scored 80, 90, 70\" becomes \"You have 24%, 46%, 30% chance of winning.\"\n",
    ">\n",
    "> - **Dropout**: A random vacation scheduler! During training, some workers randomly take the day off, forcing everyone else to learn multiple jobs.\n",
    ">\n",
    "> **In AI terms:** Each layer transforms the data in a specific way. Linear layers learn patterns, activations add non-linearity (necessary for complex functions), and regularization layers like Dropout prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "> **Learning Note:**\n",
    ">\n",
    "> In this notebook, we implement neural network layers from scratch for educational purposes.\n",
    "> The complete, tested implementations are already available in the `micrograd_plus` package.\n",
    "> After understanding how each layer works, you can import them directly:\n",
    ">\n",
    "> ```python\n",
    "> from micrograd_plus import Linear, ReLU, Sigmoid, Dropout, Sequential\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Robust path resolution - works regardless of working directory\n",
    "def _find_module_root():\n",
    "    \"\"\"Find the module root directory containing micrograd_plus.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / 'micrograd_plus' / '__init__.py').exists():\n",
    "            return str(parent)\n",
    "    return str(Path.cwd().parent)\n",
    "\n",
    "sys.path.insert(0, _find_module_root())\n",
    "\n",
    "from micrograd_plus import Tensor\n",
    "from micrograd_plus.utils import set_seed, gradient_check\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Module Base Class\n",
    "\n",
    "All layers inherit from a common `Module` class that provides:\n",
    "- Training/evaluation mode switching\n",
    "- Parameter collection\n",
    "- Gradient zeroing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class Module:\n",
    "    \"\"\"\n",
    "    Base class for all neural network layers.\n",
    "    \n",
    "    Key methods:\n",
    "    - forward(x): Compute output (must be implemented by subclasses)\n",
    "    - parameters(): Return list of trainable tensors\n",
    "    - train()/eval(): Switch between training and evaluation modes\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._training = True\n",
    "    \n",
    "    @property\n",
    "    def training(self) -> bool:\n",
    "        return self._training\n",
    "    \n",
    "    def train(self, mode: bool = True) -> 'Module':\n",
    "        \"\"\"Set training mode (affects Dropout, BatchNorm, etc.)\"\"\"\n",
    "        self._training = mode\n",
    "        return self\n",
    "    \n",
    "    def eval(self) -> 'Module':\n",
    "        \"\"\"Set evaluation mode\"\"\"\n",
    "        return self.train(False)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Compute forward pass - must be implemented by subclasses\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Make the layer callable: layer(x) calls layer.forward(x)\"\"\"\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def parameters(self) -> List[Tensor]:\n",
    "        \"\"\"Return list of trainable parameters\"\"\"\n",
    "        return []\n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        \"\"\"Reset all parameter gradients to zero\"\"\"\n",
    "        for p in self.parameters():\n",
    "            p.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Linear Layer\n",
    "\n",
    "The **Linear layer** (also called Dense or Fully Connected) computes:\n",
    "\n",
    "$$y = xW + b$$\n",
    "\n",
    "Where:\n",
    "- $x$: Input of shape `(batch_size, in_features)`\n",
    "- $W$: Weight matrix of shape `(in_features, out_features)`\n",
    "- $b$: Bias vector of shape `(out_features,)`\n",
    "- $y$: Output of shape `(batch_size, out_features)`\n",
    "\n",
    "### Weight Initialization\n",
    "\n",
    "Good initialization is crucial! We use **Xavier/Glorot initialization**:\n",
    "$$W \\sim \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{in} + n_{out}}}\\right)$$\n",
    "\n",
    "This keeps the variance of activations roughly constant across layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    Fully connected (dense) layer.\n",
    "    \n",
    "    Computes: y = x @ W + b\n",
    "    \n",
    "    Args:\n",
    "        in_features: Number of input features\n",
    "        out_features: Number of output features\n",
    "        bias: Whether to include bias term (default: True)\n",
    "    \n",
    "    Example:\n",
    "        >>> layer = Linear(784, 256)\n",
    "        >>> x = Tensor(np.random.randn(32, 784))  # Batch of 32\n",
    "        >>> y = layer(x)  # Shape: (32, 256)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Xavier/Glorot initialization\n",
    "        scale = np.sqrt(2.0 / (in_features + out_features))\n",
    "        self.weight = Tensor(\n",
    "            np.random.randn(in_features, out_features).astype(np.float32) * scale,\n",
    "            requires_grad=True\n",
    "        )\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = Tensor(\n",
    "                np.zeros(out_features, dtype=np.float32),\n",
    "                requires_grad=True\n",
    "            )\n",
    "        else:\n",
    "            self.bias = None\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Forward pass: y = x @ W + b\"\"\"\n",
    "        out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "        return out\n",
    "    \n",
    "    def parameters(self) -> List[Tensor]:\n",
    "        \"\"\"Return weight and bias\"\"\"\n",
    "        if self.bias is not None:\n",
    "            return [self.weight, self.bias]\n",
    "        return [self.weight]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Linear({self.in_features}, {self.out_features}, bias={self.bias is not None})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Linear layer\n",
    "set_seed(42)\n",
    "\n",
    "# Create layer\n",
    "linear = Linear(4, 3)\n",
    "print(f\"Layer: {linear}\")\n",
    "print(f\"Weight shape: {linear.weight.shape}\")\n",
    "print(f\"Bias shape: {linear.bias.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "x = Tensor(np.random.randn(2, 4).astype(np.float32), requires_grad=True)\n",
    "y = linear(x)\n",
    "\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Output:\\n{y.data}\")\n",
    "\n",
    "# Backward pass\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nWeight gradient shape: {linear.weight.grad.shape}\")\n",
    "print(f\"Bias gradient: {linear.bias.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### ðŸ” What Just Happened?\n",
    "\n",
    "1. **Forward**: We computed `y = x @ W + b` where:\n",
    "   - `x` is (2, 4) - 2 samples, 4 features each\n",
    "   - `W` is (4, 3) - transforms 4 features to 3\n",
    "   - `b` is (3,) - one bias per output feature\n",
    "   - `y` is (2, 3) - 2 samples, 3 outputs each\n",
    "\n",
    "2. **Backward**: Since `loss = sum(y)`, each element of `y` contributes equally:\n",
    "   - `dL/dW = x.T @ ones` - each weight gets gradient proportional to input\n",
    "   - `dL/db = sum(ones)` - bias gradient is just the number of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Activation Functions\n",
    "\n",
    "Activation functions introduce **non-linearity**. Without them, stacking linear layers would just be one big linear transformation!\n",
    "\n",
    "### Why Non-linearity Matters\n",
    "\n",
    "Linear functions can only model straight lines/planes. Real-world patterns are curved:\n",
    "- The relationship between \"hours studied\" and \"exam score\" isn't linear (diminishing returns)\n",
    "- Image features like \"edges\" require detecting patterns, not just weighted sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit: ReLU(x) = max(0, x)\n",
    "    \n",
    "    The most popular activation function because:\n",
    "    - Simple and fast\n",
    "    - Gradient is 0 or 1 (no vanishing gradient for positive inputs)\n",
    "    - Creates sparse representations\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x.relu()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ReLU()\"\n",
    "\n",
    "\n",
    "class Sigmoid(Module):\n",
    "    \"\"\"\n",
    "    Sigmoid: sigma(x) = 1 / (1 + e^(-x))\n",
    "    \n",
    "    Squashes input to range (0, 1). Good for:\n",
    "    - Binary classification output\n",
    "    - Gate mechanisms (like in LSTMs)\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x.sigmoid()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Sigmoid()\"\n",
    "\n",
    "\n",
    "class Tanh(Module):\n",
    "    \"\"\"\n",
    "    Hyperbolic tangent: tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "    \n",
    "    Squashes input to range (-1, 1). Zero-centered, unlike Sigmoid.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x.tanh()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Tanh()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activation functions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_vals = np.linspace(-3, 3, 100)\n",
    "\n",
    "# Compute activations\n",
    "relu_vals = np.maximum(0, x_vals)\n",
    "sigmoid_vals = 1 / (1 + np.exp(-x_vals))\n",
    "tanh_vals = np.tanh(x_vals)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "\n",
    "axes[0].plot(x_vals, relu_vals, 'b-', linewidth=2)\n",
    "axes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0].set_title('ReLU: max(0, x)')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(x_vals, sigmoid_vals, 'r-', linewidth=2)\n",
    "axes[1].axhline(y=0.5, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1].set_title('Sigmoid: 1/(1+e^(-x))')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylim(-0.1, 1.1)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(x_vals, tanh_vals, 'g-', linewidth=2)\n",
    "axes[2].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[2].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[2].set_title('Tanh')\n",
    "axes[2].set_xlabel('x')\n",
    "axes[2].set_ylim(-1.1, 1.1)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test activation gradients\n",
    "relu = ReLU()\n",
    "sigmoid = Sigmoid()\n",
    "\n",
    "# ReLU gradient\n",
    "x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)\n",
    "y = relu(x)\n",
    "print(f\"ReLU input:  {x.data}\")\n",
    "print(f\"ReLU output: {y.data}\")\n",
    "\n",
    "y.sum().backward()\n",
    "print(f\"ReLU gradient: {x.grad}\")\n",
    "print(\"  (Gradient is 1 where input > 0, else 0)\")\n",
    "\n",
    "# Sigmoid gradient\n",
    "x = Tensor([-2.0, 0.0, 2.0], requires_grad=True)\n",
    "y = sigmoid(x)\n",
    "print(f\"\\nSigmoid input:  {x.data}\")\n",
    "print(f\"Sigmoid output: {y.data}\")\n",
    "\n",
    "y.sum().backward()\n",
    "print(f\"Sigmoid gradient: {x.grad}\")\n",
    "print(f\"  (Gradient = sigmoid(x) * (1 - sigmoid(x)))\")\n",
    "print(f\"  Expected: {y.data * (1 - y.data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Softmax Layer\n",
    "\n",
    "**Softmax** converts raw scores (logits) into a probability distribution:\n",
    "\n",
    "$$\\text{softmax}(x)_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "\n",
    "Properties:\n",
    "- All outputs are positive\n",
    "- All outputs sum to 1\n",
    "- Higher input values get higher probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Module):\n",
    "    \"\"\"\n",
    "    Softmax activation: converts logits to probabilities.\n",
    "    \n",
    "    softmax(x)_i = exp(x_i) / sum(exp(x_j))\n",
    "    \n",
    "    Args:\n",
    "        axis: Dimension along which to compute softmax (default: -1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, axis: int = -1):\n",
    "        super().__init__()\n",
    "        self.axis = axis\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x.softmax(axis=self.axis)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Softmax(axis={self.axis})\"\n",
    "\n",
    "\n",
    "# Test Softmax\n",
    "softmax = Softmax()\n",
    "\n",
    "# Example: Classification scores for 3 classes\n",
    "logits = Tensor([[2.0, 1.0, 0.1]], requires_grad=True)\n",
    "probs = softmax(logits)\n",
    "\n",
    "print(f\"Logits (raw scores): {logits.data}\")\n",
    "print(f\"Probabilities: {probs.data}\")\n",
    "print(f\"Sum of probabilities: {probs.data.sum():.4f} (should be 1.0)\")\n",
    "\n",
    "# Batch example\n",
    "batch_logits = Tensor([\n",
    "    [2.0, 1.0, 0.1],  # Sample 1: clearly class 0\n",
    "    [0.1, 0.2, 3.0],  # Sample 2: clearly class 2\n",
    "    [1.0, 1.0, 1.0]   # Sample 3: uncertain (equal)\n",
    "])\n",
    "batch_probs = softmax(batch_logits)\n",
    "\n",
    "print(f\"\\nBatch probabilities:\")\n",
    "for i, (logit, prob) in enumerate(zip(batch_logits.data, batch_probs.data)):\n",
    "    pred = np.argmax(prob)\n",
    "    print(f\"  Sample {i}: logits={logit} -> probs={prob.round(3)} -> class {pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Dropout Layer\n",
    "\n",
    "**Dropout** is a regularization technique that randomly \"drops\" (sets to zero) a fraction of neurons during training.\n",
    "\n",
    "### Why Dropout Works\n",
    "\n",
    "1. **Prevents co-adaptation**: Neurons can't rely on specific other neurons\n",
    "2. **Ensemble effect**: Each training step uses a different \"sub-network\"\n",
    "3. **Implicit regularization**: Forces the network to be more robust\n",
    "\n",
    "### Important: Inverted Dropout\n",
    "\n",
    "During training, we scale up remaining activations by `1/(1-p)` so that expected values match. This way, we don't need to scale during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Module):\n",
    "    \"\"\"\n",
    "    Dropout regularization layer.\n",
    "    \n",
    "    During training: randomly sets elements to 0 with probability p,\n",
    "    and scales remaining elements by 1/(1-p).\n",
    "    \n",
    "    During evaluation: does nothing (identity function).\n",
    "    \n",
    "    Args:\n",
    "        p: Probability of dropping each element (default: 0.5)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, p: float = 0.5):\n",
    "        super().__init__()\n",
    "        if not 0 <= p < 1:\n",
    "            raise ValueError(f\"p must be in [0, 1), got {p}\")\n",
    "        self.p = p\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # During eval, return unchanged\n",
    "        if not self.training or self.p == 0:\n",
    "            return x\n",
    "        \n",
    "        # Create random mask\n",
    "        mask = (np.random.rand(*x.shape) > self.p).astype(np.float32)\n",
    "        scale = 1.0 / (1.0 - self.p)\n",
    "        \n",
    "        # Apply mask with scaling\n",
    "        out = Tensor(\n",
    "            x.data * mask * scale,\n",
    "            requires_grad=x.requires_grad,\n",
    "            _children=(x,),\n",
    "            _op='dropout'\n",
    "        )\n",
    "        \n",
    "        def _backward():\n",
    "            if x.requires_grad:\n",
    "                # Gradient only flows through non-dropped elements\n",
    "                x.grad += mask * scale * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Dropout(p={self.p})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dropout\n",
    "set_seed(42)\n",
    "\n",
    "dropout = Dropout(p=0.5)\n",
    "\n",
    "x = Tensor(np.ones((2, 5)), requires_grad=True)\n",
    "print(f\"Input (all ones):\\n{x.data}\")\n",
    "\n",
    "# Training mode\n",
    "dropout.train()\n",
    "y_train = dropout(x)\n",
    "print(f\"\\nTraining mode output (50% dropped, rest scaled by 2):\")\n",
    "print(f\"{y_train.data}\")\n",
    "print(f\"Mean (should be ~1.0): {y_train.data.mean():.2f}\")\n",
    "\n",
    "# Evaluation mode\n",
    "dropout.eval()\n",
    "y_eval = dropout(x)\n",
    "print(f\"\\nEval mode output (unchanged):\")\n",
    "print(f\"{y_eval.data}\")\n",
    "\n",
    "# Show that training outputs vary\n",
    "dropout.train()\n",
    "print(f\"\\nMultiple training forward passes:\")\n",
    "for i in range(3):\n",
    "    y = dropout(x)\n",
    "    print(f\"  Pass {i+1}: mean={y.data.mean():.2f}, zeros={np.sum(y.data == 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Sequential Container\n",
    "\n",
    "The **Sequential** container lets us stack layers and treat them as a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \"\"\"\n",
    "    Container that runs layers in sequence.\n",
    "    \n",
    "    Example:\n",
    "        model = Sequential(\n",
    "            Linear(784, 256),\n",
    "            ReLU(),\n",
    "            Dropout(0.2),\n",
    "            Linear(256, 10)\n",
    "        )\n",
    "        output = model(input)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *layers):\n",
    "        super().__init__()\n",
    "        self.layers = list(layers)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Pass input through all layers sequentially\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self) -> List[Tensor]:\n",
    "        \"\"\"Collect parameters from all layers\"\"\"\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params.extend(layer.parameters())\n",
    "        return params\n",
    "    \n",
    "    def train(self, mode: bool = True):\n",
    "        \"\"\"Set training mode for all layers\"\"\"\n",
    "        self._training = mode\n",
    "        for layer in self.layers:\n",
    "            layer.train(mode)\n",
    "        return self\n",
    "    \n",
    "    def __repr__(self):\n",
    "        lines = [\"Sequential(\"]\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            lines.append(f\"  ({i}): {layer}\")\n",
    "        lines.append(\")\")\n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple MLP (Multi-Layer Perceptron)\n",
    "set_seed(42)\n",
    "\n",
    "model = Sequential(\n",
    "    Linear(4, 8),     # Input layer: 4 features -> 8 hidden\n",
    "    ReLU(),\n",
    "    Dropout(0.2),\n",
    "    Linear(8, 8),     # Hidden layer\n",
    "    ReLU(),\n",
    "    Linear(8, 3)      # Output layer: 3 classes\n",
    ")\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "params = model.parameters()\n",
    "total_params = sum(p.size for p in params)\n",
    "print(f\"\\nTotal parameters: {total_params}\")\n",
    "for i, p in enumerate(params):\n",
    "    print(f\"  Param {i}: shape={p.shape}, size={p.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass through the model\n",
    "x = Tensor(np.random.randn(2, 4).astype(np.float32), requires_grad=True)\n",
    "\n",
    "# Training mode\n",
    "model.train()\n",
    "y_train = model(x)\n",
    "print(f\"Training mode output:\")\n",
    "print(f\"  Input shape: {x.shape}\")\n",
    "print(f\"  Output shape: {y_train.shape}\")\n",
    "print(f\"  Output:\\n{y_train.data}\")\n",
    "\n",
    "# Backward pass\n",
    "loss = y_train.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nGradients computed:\")\n",
    "for i, p in enumerate(model.parameters()):\n",
    "    grad_norm = np.sqrt(np.sum(p.grad ** 2))\n",
    "    print(f\"  Param {i}: grad_norm={grad_norm:.4f}\")\n",
    "\n",
    "# Eval mode\n",
    "model.eval()\n",
    "y_eval = model(x)\n",
    "print(f\"\\nEval mode output (deterministic):\")\n",
    "print(f\"{y_eval.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Exercise\n",
    "\n",
    "Implement a **LeakyReLU** activation function:\n",
    "\n",
    "$$\\text{LeakyReLU}(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases}$$\n",
    "\n",
    "Where $\\alpha$ is a small positive number (typically 0.01).\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "The gradient is:\n",
    "- 1 where input > 0\n",
    "- alpha where input <= 0\n",
    "\n",
    "You can implement this using `np.where` or by combining masks.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class LeakyReLU(Module):\n",
    "    \"\"\"\n",
    "    Leaky ReLU: allows small gradient for negative inputs.\n",
    "    \n",
    "    Args:\n",
    "        alpha: Slope for negative inputs (default: 0.01)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: float = 0.01):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # TODO: Implement LeakyReLU\n",
    "        # Hint: Use np.where or combine x.relu() with x * alpha\n",
    "        pass\n",
    "\n",
    "# Test your implementation\n",
    "# leaky_relu = LeakyReLU(alpha=0.1)\n",
    "# x = Tensor([-2, -1, 0, 1, 2], requires_grad=True)\n",
    "# y = leaky_relu(x)\n",
    "# print(f\"Input: {x.data}\")\n",
    "# print(f\"Output: {y.data}\")  # Should be [-0.2, -0.1, 0, 1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "### Mistake 1: Forgetting to Switch Modes\n",
    "\n",
    "```python\n",
    "# Wrong: Using training mode for inference\n",
    "model = Sequential(Linear(10, 5), Dropout(0.5), Linear(5, 2))\n",
    "# Dropout will randomly drop neurons!\n",
    "prediction = model(test_input)\n",
    "\n",
    "# Right: Switch to eval mode\n",
    "model.eval()\n",
    "prediction = model(test_input)\n",
    "```\n",
    "\n",
    "### Mistake 2: Wrong Weight Initialization\n",
    "\n",
    "```python\n",
    "# Wrong: Too large initial weights\n",
    "weight = Tensor(np.random.randn(100, 50) * 10, requires_grad=True)  # Too big!\n",
    "\n",
    "# Wrong: Too small initial weights\n",
    "weight = Tensor(np.random.randn(100, 50) * 0.0001, requires_grad=True)  # Too small!\n",
    "\n",
    "# Right: Xavier/Glorot initialization\n",
    "scale = np.sqrt(2.0 / (100 + 50))\n",
    "weight = Tensor(np.random.randn(100, 50) * scale, requires_grad=True)\n",
    "```\n",
    "\n",
    "### Mistake 3: Applying Softmax Twice\n",
    "\n",
    "```python\n",
    "# Wrong: CrossEntropyLoss already includes softmax!\n",
    "logits = model(x)\n",
    "probs = softmax(logits)  # Don't do this!\n",
    "loss = cross_entropy(probs, targets)  # Double softmax = wrong gradients\n",
    "\n",
    "# Right: Pass raw logits to CrossEntropyLoss\n",
    "logits = model(x)\n",
    "loss = cross_entropy(logits, targets)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've learned:\n",
    "- How to structure neural network layers with the Module base class\n",
    "- How Linear layers transform features with learnable weights\n",
    "- Why activation functions (ReLU, Sigmoid, Softmax) are essential\n",
    "- How Dropout regularization works and why it helps\n",
    "- How to combine layers into a Sequential model\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge (Optional)\n",
    "\n",
    "Implement **Batch Normalization**:\n",
    "\n",
    "$$\\hat{x} = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$$\n",
    "$$y = \\gamma \\hat{x} + \\beta$$\n",
    "\n",
    "Where:\n",
    "- $\\mu_B, \\sigma_B^2$ are batch mean and variance\n",
    "- $\\gamma, \\beta$ are learnable scale and shift parameters\n",
    "- During eval, use running averages instead of batch statistics\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Understanding Xavier Initialization](https://cs231n.github.io/neural-networks-2/#init)\n",
    "- [Dropout Paper (Srivastava et al., 2014)](https://jmlr.org/papers/v15/srivastava14a.html)\n",
    "- [Batch Normalization Paper (Ioffe & Szegedy, 2015)](https://arxiv.org/abs/1502.03167)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup - release memory\n",
    "from micrograd_plus.utils import cleanup_notebook\n",
    "cleanup_notebook(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you understand neural network layers, we'll build on this in:\n",
    "- **Lab 1.7.3**: Loss functions and optimizers\n",
    "- **Lab 1.7.4**: Testing suite for verifying your implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
