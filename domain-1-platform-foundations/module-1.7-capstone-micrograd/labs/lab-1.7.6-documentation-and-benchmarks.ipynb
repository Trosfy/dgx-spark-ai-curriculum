{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.7.6: Documentation and Benchmarks\n",
    "\n",
    "**Module:** 1.7 - Domain 1 Capstone: MicroGrad+  \n",
    "**Time:** 1 hour  \n",
    "**Difficulty:** ⭐⭐\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Understand how to write good API documentation\n",
    "- [ ] Compare MicroGrad+ performance against PyTorch\n",
    "- [ ] Learn why PyTorch is faster and what optimizations it uses\n",
    "- [ ] Complete the Domain 1 Capstone!\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed: Labs 1.7.1-1.7.5\n",
    "- Knowledge of: Python documentation practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def _find_module_root():\n",
    "    \"\"\"Find the module root directory containing micrograd_plus.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / 'micrograd_plus' / '__init__.py').exists():\n",
    "            return str(parent)\n",
    "    # Fallback to parent directory\n",
    "    return str(Path.cwd().parent)\n",
    "\n",
    "sys.path.insert(0, _find_module_root())\n",
    "\n",
    "from micrograd_plus import (\n",
    "    Tensor, Linear, ReLU, Dropout,\n",
    "    CrossEntropyLoss, MSELoss, Adam, SGD, Sequential\n",
    ")\n",
    "from micrograd_plus.utils import set_seed\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: API Documentation\n",
    "\n",
    "Good documentation makes your code usable by others (and your future self!). Let's review the key components of our MicroGrad+ API.\n",
    "\n",
    "### ELI5: Why Documentation Matters\n",
    "\n",
    "Think of documentation like the instruction manual for a LEGO set:\n",
    "- Without it, you can eventually figure things out, but it takes much longer\n",
    "- With good instructions, anyone can build what you built\n",
    "- Bad instructions are sometimes worse than no instructions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the module documentation\n",
    "import micrograd_plus\n",
    "\n",
    "print(\"MicroGrad+ Module Documentation\")\n",
    "print(\"=\" * 60)\n",
    "print(micrograd_plus.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check individual class documentation\n",
    "print(\"Tensor Class Documentation\")\n",
    "print(\"=\" * 60)\n",
    "print(Tensor.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of good docstring format\n",
    "print(\"Linear Layer Documentation\")\n",
    "print(\"=\" * 60)\n",
    "print(Linear.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation Best Practices\n",
    "\n",
    "1. **Module docstring**: Explain what the module does and provide a quick example\n",
    "2. **Class docstring**: Describe purpose, args, attributes, and usage\n",
    "3. **Method docstring**: Document args, returns, raises, and provide examples\n",
    "4. **Type hints**: Use them for better IDE support and clarity\n",
    "\n",
    "### The Anatomy of a Good Docstring\n",
    "\n",
    "```python\n",
    "def my_function(param1: int, param2: str = \"default\") -> bool:\n",
    "    \"\"\"Short one-line description of what the function does.\n",
    "    \n",
    "    Longer description if needed, explaining the behavior in more\n",
    "    detail, edge cases, and any important notes.\n",
    "    \n",
    "    Args:\n",
    "        param1: Description of first parameter.\n",
    "        param2: Description of second parameter with default.\n",
    "    \n",
    "    Returns:\n",
    "        Description of what is returned.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: When param1 is negative.\n",
    "    \n",
    "    Example:\n",
    "        >>> my_function(5, \"hello\")\n",
    "        True\n",
    "    \"\"\"\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore what methods are available on our Tensor class\n",
    "print(\"Available Tensor Methods and Attributes:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Filter out private methods (those starting with _)\n",
    "public_attrs = [attr for attr in dir(Tensor) if not attr.startswith('_')]\n",
    "for attr in sorted(public_attrs):\n",
    "    obj = getattr(Tensor, attr)\n",
    "    if callable(obj):\n",
    "        print(f\"  {attr}()\")\n",
    "    else:\n",
    "        print(f\"  {attr} (property)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Benchmarking Against PyTorch\n",
    "\n",
    "Let's compare MicroGrad+ performance against PyTorch. Spoiler: PyTorch will be much faster - and that's okay! The goal is to understand **why**.\n",
    "\n",
    "### ELI5: Why Compare Ourselves to PyTorch?\n",
    "\n",
    "Imagine you built a bicycle from scratch in your garage. It works! But you wouldn't use it to race against a professional racing bike. However, by understanding *why* the racing bike is faster (lighter materials, precision engineering, aerodynamics), you learn valuable engineering principles.\n",
    "\n",
    "Similarly, comparing MicroGrad+ to PyTorch teaches us about optimization, not to make us feel bad about our code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if PyTorch is available\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    PYTORCH_AVAILABLE = True\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"PyTorch not available. Skipping PyTorch benchmarks.\")\n",
    "    print(\"Install with: pip install torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_micrograd(batch_size, in_features, hidden, out_features, num_iterations):\n",
    "    \"\"\"Benchmark MicroGrad+ forward and backward pass.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Number of samples per batch\n",
    "        in_features: Input dimension\n",
    "        hidden: Hidden layer dimension\n",
    "        out_features: Output dimension (number of classes)\n",
    "        num_iterations: Number of iterations to benchmark\n",
    "    \n",
    "    Returns:\n",
    "        Average time per iteration in seconds\n",
    "    \"\"\"\n",
    "    set_seed(42)\n",
    "    \n",
    "    # Create model\n",
    "    model = Sequential(\n",
    "        Linear(in_features, hidden),\n",
    "        ReLU(),\n",
    "        Linear(hidden, hidden),\n",
    "        ReLU(),\n",
    "        Linear(hidden, out_features)\n",
    "    )\n",
    "    \n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Create data\n",
    "    X = np.random.randn(batch_size, in_features).astype(np.float32)\n",
    "    y = np.random.randint(0, out_features, batch_size)\n",
    "    \n",
    "    # Warmup (important for fair benchmarking!)\n",
    "    for _ in range(5):\n",
    "        x_tensor = Tensor(X, requires_grad=True)\n",
    "        logits = model(x_tensor)\n",
    "        loss = loss_fn(logits, Tensor(y))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.time()\n",
    "    for _ in range(num_iterations):\n",
    "        x_tensor = Tensor(X, requires_grad=True)\n",
    "        logits = model(x_tensor)\n",
    "        loss = loss_fn(logits, Tensor(y))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    return elapsed / num_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE:\n",
    "    def benchmark_pytorch(batch_size, in_features, hidden, out_features, num_iterations, device='cpu'):\n",
    "        \"\"\"Benchmark PyTorch forward and backward pass.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Number of samples per batch\n",
    "            in_features: Input dimension\n",
    "            hidden: Hidden layer dimension\n",
    "            out_features: Output dimension (number of classes)\n",
    "            num_iterations: Number of iterations to benchmark\n",
    "            device: 'cpu' or 'cuda'\n",
    "        \n",
    "        Returns:\n",
    "            Average time per iteration in seconds\n",
    "        \"\"\"\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        # Create model\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, out_features)\n",
    "        ).to(device)\n",
    "        \n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Create data\n",
    "        X = torch.randn(batch_size, in_features, device=device)\n",
    "        y = torch.randint(0, out_features, (batch_size,), device=device)\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(5):\n",
    "            logits = model(X)\n",
    "            loss = loss_fn(logits, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if device == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        # Benchmark\n",
    "        start = time.time()\n",
    "        for _ in range(num_iterations):\n",
    "            logits = model(X)\n",
    "            loss = loss_fn(logits, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if device == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        return elapsed / num_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmarks\n",
    "print(\"Benchmark: Forward + Backward + Optimizer Step\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Different configurations to test\n",
    "configs = [\n",
    "    (32, 784, 256, 10, 100),   # Small: MNIST-like\n",
    "    (64, 784, 512, 10, 50),    # Medium\n",
    "    (128, 1024, 512, 100, 20), # Large\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for batch_size, in_features, hidden, out_features, iters in configs:\n",
    "    print(f\"\\nConfig: batch={batch_size}, input={in_features}, hidden={hidden}, output={out_features}\")\n",
    "    \n",
    "    # MicroGrad+\n",
    "    mg_time = benchmark_micrograd(batch_size, in_features, hidden, out_features, iters)\n",
    "    print(f\"  MicroGrad+: {mg_time*1000:.2f} ms/iteration\")\n",
    "    \n",
    "    if PYTORCH_AVAILABLE:\n",
    "        # PyTorch CPU\n",
    "        pt_cpu_time = benchmark_pytorch(batch_size, in_features, hidden, out_features, iters, 'cpu')\n",
    "        print(f\"  PyTorch CPU: {pt_cpu_time*1000:.2f} ms/iteration\")\n",
    "        \n",
    "        ratio_cpu = mg_time / pt_cpu_time\n",
    "        print(f\"  Ratio (MicroGrad+ / PyTorch CPU): {ratio_cpu:.1f}x slower\")\n",
    "        \n",
    "        # PyTorch GPU (if available)\n",
    "        if torch.cuda.is_available():\n",
    "            pt_gpu_time = benchmark_pytorch(batch_size, in_features, hidden, out_features, iters, 'cuda')\n",
    "            print(f\"  PyTorch GPU: {pt_gpu_time*1000:.2f} ms/iteration\")\n",
    "            ratio_gpu = mg_time / pt_gpu_time\n",
    "            print(f\"  Ratio (MicroGrad+ / PyTorch GPU): {ratio_gpu:.1f}x slower\")\n",
    "        \n",
    "        results.append({\n",
    "            'config': f'{batch_size}x{in_features}',\n",
    "            'micrograd': mg_time * 1000,\n",
    "            'pytorch_cpu': pt_cpu_time * 1000,\n",
    "            'ratio': ratio_cpu\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize benchmark results\n",
    "if PYTORCH_AVAILABLE and results:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    configs = [r['config'] for r in results]\n",
    "    mg_times = [r['micrograd'] for r in results]\n",
    "    pt_times = [r['pytorch_cpu'] for r in results]\n",
    "    ratios = [r['ratio'] for r in results]\n",
    "    \n",
    "    # Time comparison\n",
    "    x = np.arange(len(configs))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0].bar(x - width/2, mg_times, width, label='MicroGrad+', color='#3498db')\n",
    "    axes[0].bar(x + width/2, pt_times, width, label='PyTorch CPU', color='#e74c3c')\n",
    "    axes[0].set_ylabel('Time (ms)')\n",
    "    axes[0].set_title('Execution Time Comparison')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(configs)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ratio\n",
    "    colors = ['#2ecc71' if r < 20 else '#f39c12' if r < 50 else '#e74c3c' for r in ratios]\n",
    "    axes[1].bar(configs, ratios, color=colors)\n",
    "    axes[1].set_ylabel('Slowdown Factor (x times slower)')\n",
    "    axes[1].set_title('MicroGrad+ vs PyTorch CPU')\n",
    "    axes[1].axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping visualization (PyTorch not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Why is PyTorch Faster?\n",
    "\n",
    "PyTorch is typically 10-100x faster than our implementation. Here's why:\n",
    "\n",
    "### 1. Compiled Operations (C++/CUDA)\n",
    "\n",
    "| MicroGrad+ | PyTorch |\n",
    "|------------|----------|\n",
    "| Pure Python + NumPy | C++ kernels, CUDA for GPU |\n",
    "| Interpreted at runtime | Pre-compiled, optimized |\n",
    "\n",
    "**ELI5**: MicroGrad+ is like doing math by hand - correct but slow. PyTorch is like using a calculator - does the same math but much faster because it's purpose-built.\n",
    "\n",
    "### 2. Optimized Memory Layout\n",
    "\n",
    "| MicroGrad+ | PyTorch |\n",
    "|------------|----------|\n",
    "| NumPy arrays (general purpose) | Custom tensor layout optimized for ML |\n",
    "| Standard memory allocation | Memory pools, caching |\n",
    "\n",
    "**ELI5**: NumPy is like a general-purpose toolbox. PyTorch tensors are like a specialized toolkit designed specifically for neural networks.\n",
    "\n",
    "### 3. Operator Fusion\n",
    "\n",
    "| MicroGrad+ | PyTorch |\n",
    "|------------|----------|\n",
    "| Each operation creates new array | Fuses operations to reduce memory traffic |\n",
    "| `a = x + y; b = a * z` | `b = fused_add_mul(x, y, z)` |\n",
    "\n",
    "**ELI5**: MicroGrad+ writes down each step on a new piece of paper. PyTorch combines steps when possible, using less paper and time.\n",
    "\n",
    "### 4. Parallelization\n",
    "\n",
    "| MicroGrad+ | PyTorch |\n",
    "|------------|----------|\n",
    "| Single-threaded (mostly) | Multi-threaded CPU |\n",
    "| NumPy's limited parallelism | Massively parallel GPU |\n",
    "\n",
    "**ELI5**: MicroGrad+ is one person doing all the work. PyTorch has thousands of helpers (GPU cores) working simultaneously.\n",
    "\n",
    "### 5. Autograd Optimization\n",
    "\n",
    "| MicroGrad+ | PyTorch |\n",
    "|------------|----------|\n",
    "| Python closures for backward functions | Compiled autograd engine |\n",
    "| Full computation graph in memory | Checkpointing and memory optimization |\n",
    "\n",
    "**ELI5**: Our backward functions are like detailed step-by-step recipes. PyTorch's are like a chef who's memorized everything and doesn't need to read instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate the power of GPU parallelism\nprint(\"Why GPUs are so fast for neural networks:\")\nprint(\"=\" * 60)\nprint(\"\"\"\nMatrix multiplication example: (1000 x 1000) @ (1000 x 1000)\n\nOperations needed: 1000 * 1000 * 1000 = 1 billion multiply-adds\n\nCPU (8 cores, ~100 GFLOPS):\n  - Can do ~100 billion ops/sec\n  - Time: ~10 ms\n\nGPU (DGX Spark with NVIDIA Blackwell GB10 Superchip):\n  - FP8: ~209 TFLOPS\n  - NVFP4: 1 PFLOP\n  - Time: ~5 microseconds!\n\nThe GPU wins because:\n1. 6,144 CUDA cores + 192 Tensor cores (5th generation)\n2. 273 GB/s memory bandwidth with 128GB unified memory\n3. Operations optimized for data parallelism\n4. Native support for FP8/NVFP4 quantization\n\"\"\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate actual GPU speedup if available\n",
    "if PYTORCH_AVAILABLE and torch.cuda.is_available():\n",
    "    print(\"Real GPU Benchmark: Large Matrix Multiplication\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    sizes = [256, 512, 1024, 2048, 4096]\n",
    "    \n",
    "    for size in sizes:\n",
    "        # CPU\n",
    "        a_cpu = torch.randn(size, size)\n",
    "        b_cpu = torch.randn(size, size)\n",
    "        \n",
    "        start = time.time()\n",
    "        for _ in range(10):\n",
    "            c_cpu = torch.mm(a_cpu, b_cpu)\n",
    "        cpu_time = (time.time() - start) / 10 * 1000\n",
    "        \n",
    "        # GPU\n",
    "        a_gpu = a_cpu.cuda()\n",
    "        b_gpu = b_cpu.cuda()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        start = time.time()\n",
    "        for _ in range(10):\n",
    "            c_gpu = torch.mm(a_gpu, b_gpu)\n",
    "        torch.cuda.synchronize()\n",
    "        gpu_time = (time.time() - start) / 10 * 1000\n",
    "        \n",
    "        speedup = cpu_time / gpu_time\n",
    "        print(f\"  {size}x{size}: CPU={cpu_time:.2f}ms, GPU={gpu_time:.3f}ms, Speedup={speedup:.1f}x\")\n",
    "else:\n",
    "    print(\"GPU not available - skipping GPU benchmark demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: What You've Accomplished\n",
    "\n",
    "Let's take a moment to appreciate what you've built in this capstone project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count lines of code in MicroGrad+\n",
    "import os\n",
    "\n",
    "def count_lines(filepath):\n",
    "    \"\"\"Count non-empty, non-comment lines of code.\"\"\"\n",
    "    count = 0\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f:\n",
    "                stripped = line.strip()\n",
    "                if stripped and not stripped.startswith('#'):\n",
    "                    count += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read {filepath}: {e}\")\n",
    "    return count\n",
    "\n",
    "# Find package directory\n",
    "module_root = Path(_find_module_root())\n",
    "package_dir = module_root / 'micrograd_plus'\n",
    "\n",
    "total_lines = 0\n",
    "\n",
    "print(\"MicroGrad+ Package Statistics\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if package_dir.exists():\n",
    "    for filepath in sorted(package_dir.glob('*.py')):\n",
    "        lines = count_lines(filepath)\n",
    "        total_lines += lines\n",
    "        print(f\"  {filepath.name:25s}: {lines:4d} lines\")\n",
    "    \n",
    "    print(f\"  {'-'*30}\")\n",
    "    print(f\"  {'TOTAL':25s}: {total_lines:4d} lines\")\n",
    "else:\n",
    "    print(f\"Package directory not found at {package_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of what was built\n",
    "print(\"\"\"\n",
    "CONGRATULATIONS! You've completed the Domain 1 Capstone!\n",
    "\n",
    "You built MicroGrad+, featuring:\n",
    "\n",
    "Tensor Class with Autograd\n",
    "   - Automatic differentiation (reverse-mode)\n",
    "   - Broadcasting support\n",
    "   - 15+ differentiable operations\n",
    "   - Gradient accumulation and zeroing\n",
    "\n",
    "Neural Network Layers\n",
    "   - Linear (fully connected)\n",
    "   - ReLU, Sigmoid, Tanh, Softmax activations\n",
    "   - Dropout for regularization\n",
    "   - BatchNorm and LayerNorm for normalization\n",
    "   - Embedding for discrete inputs\n",
    "   - Flatten for reshaping\n",
    "\n",
    "Loss Functions\n",
    "   - MSE Loss for regression\n",
    "   - Cross-Entropy Loss for classification\n",
    "   - BCE Loss for binary classification\n",
    "   - L1 and Huber Loss for robust regression\n",
    "   - NLL Loss for log-probability inputs\n",
    "\n",
    "Optimizers\n",
    "   - SGD with momentum and weight decay\n",
    "   - Adam (adaptive learning rates)\n",
    "   - AdamW (decoupled weight decay)\n",
    "   - RMSprop\n",
    "   - Learning rate schedulers\n",
    "\n",
    "Training Utilities\n",
    "   - Sequential container\n",
    "   - DataLoader for batching\n",
    "   - Training and evaluation loops\n",
    "   - Model save/load\n",
    "\n",
    "Testing Suite\n",
    "   - Unit tests for all components\n",
    "   - Gradient checking against numerical gradients\n",
    "   - Edge case validation\n",
    "\n",
    "Real Application\n",
    "   - Trained on MNIST with >95% accuracy!\n",
    "   - Visualizations of training progress\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 5: What's Next?\n\nIn **Domain 2: Deep Learning Frameworks**, you'll use PyTorch to:\n\n### 1. Work with Real Hardware\n- Use the DGX Spark with NVIDIA Blackwell GB10 Superchip\n- Leverage 128GB unified memory with 273 GB/s bandwidth\n- Experience 10-100x+ speedups with native FP8/NVFP4 support!\n\n### 2. Build Advanced Architectures\n- Convolutional Neural Networks (CNNs) for images\n- Recurrent Neural Networks (RNNs) for sequences\n- Transformers and Attention mechanisms\n\n### 3. Handle Real-World Data\n- Image classification (CIFAR-10, ImageNet)\n- Natural Language Processing\n- Transfer learning from pre-trained models\n\n### 4. Scale to Larger Models\n- Fine-tuning pre-trained models (up to 100-120B with QLoRA)\n- Efficient training techniques (mixed precision, gradient checkpointing)\n- NVFP4 inference for models up to ~200B parameters\n- Model optimization and deployment\n\n### Why Your MicroGrad+ Experience Matters\n\nEverything you learned building MicroGrad+ directly applies:\n\n| MicroGrad+ Concept | PyTorch Equivalent |\n|-------------------|--------------------|\n| `Tensor` | `torch.Tensor` |\n| `requires_grad=True` | `requires_grad=True` |\n| `loss.backward()` | `loss.backward()` |\n| `optimizer.step()` | `optimizer.step()` |\n| `Sequential` | `nn.Sequential` |\n| `Linear`, `ReLU` | `nn.Linear`, `nn.ReLU` |\n\nYou understand the **internals** now. PyTorch will feel familiar but much faster!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Final Reflection\n",
    "\n",
    "Before we wrap up, take a moment to reflect on your learning journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: What was the most challenging concept?\n",
    "# Fill in your reflection here:\n",
    "\n",
    "most_challenging = \"\"\"\n",
    "# Your answer here\n",
    "# Example: \"Understanding how backward() propagates gradients through the graph\"\n",
    "\"\"\"\n",
    "\n",
    "most_interesting = \"\"\"\n",
    "# Your answer here  \n",
    "# Example: \"Seeing how simple operations combine to train a neural network\"\n",
    "\"\"\"\n",
    "\n",
    "next_steps = \"\"\"\n",
    "# Your answer here\n",
    "# Example: \"I want to try implementing a CNN from scratch\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"Your Domain 1 Capstone Reflection\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nMost challenging: {most_challenging.strip()}\")\n",
    "print(f\"\\nMost interesting: {most_interesting.strip()}\")\n",
    "print(f\"\\nNext steps: {next_steps.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Domain 1 Complete!\n",
    "\n",
    "You now have:\n",
    "- **Deep understanding** of how neural networks work internally\n",
    "- **Your own working autograd engine** that you built from scratch\n",
    "- **Solid foundation** in Python, NumPy, and software engineering\n",
    "- **Practical experience** training real models on real data\n",
    "\n",
    "These fundamentals will serve you well as you move to more advanced topics in Domain 2!\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Autograd is just careful bookkeeping** - track operations, apply chain rule\n",
    "2. **Neural networks are function compositions** - layers are just functions with learnable parameters\n",
    "3. **Training is optimization** - minimize loss by following gradients\n",
    "4. **Understanding beats memorization** - knowing *why* something works lets you debug and innovate\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "print(\"Domain 1 Capstone Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}