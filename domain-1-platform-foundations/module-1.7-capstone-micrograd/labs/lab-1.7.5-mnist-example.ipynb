{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab 1.7.5: MNIST Example\n",
    "\n",
    "**Module:** 1.7 - Domain 1 Capstone: MicroGrad+  \n",
    "**Time:** 1.5 hours  \n",
    "**Difficulty:** â­â­â­â­\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- [ ] Train a neural network on real handwritten digit images\n",
    "- [ ] Achieve >95% accuracy using your MicroGrad+ library\n",
    "- [ ] Visualize the learning process and predictions\n",
    "- [ ] Understand how your library performs on a real problem\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Prerequisites\n",
    "\n",
    "- Completed: Labs 1.7.1-1.7.4\n",
    "- Knowledge of: Image classification basics\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ Real-World Context\n",
    "\n",
    "MNIST (Modified National Institute of Standards and Technology) is the \"Hello World\" of machine learning. It contains 70,000 grayscale images of handwritten digits (0-9), each 28x28 pixels.\n",
    "\n",
    "Despite being simple, MNIST helped establish key ML practices:\n",
    "- Benchmarking model performance\n",
    "- Developing training techniques\n",
    "- Testing new architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ§’ ELI5: Image Classification\n",
    "\n",
    "> **Imagine teaching a child to recognize numbers.**\n",
    ">\n",
    "> You show them many examples: \"This is a 3. This is also a 3. This different-looking one is also a 3.\"\n",
    ">\n",
    "> Over time, they learn patterns:\n",
    "> - \"3's have two bumps on the right\"\n",
    "> - \"8's look like two circles stacked\"\n",
    "> - \"1's are just a vertical line\"\n",
    ">\n",
    "> **In AI terms:**\n",
    "> - Each image is 784 numbers (28x28 pixels, 0-255 grayscale)\n",
    "> - The neural network learns which pixel patterns indicate which digit\n",
    "> - After training, it can recognize digits it's never seen before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "import gzip\n",
    "import os\n",
    "import socket\n",
    "from urllib import request\n",
    "from pathlib import Path\n",
    "\n",
    "# Robust path resolution - works regardless of working directory\n",
    "def _find_module_root():\n",
    "    \"\"\"Find the module root directory containing micrograd_plus.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / 'micrograd_plus' / '__init__.py').exists():\n",
    "            return str(parent)\n",
    "    return str(Path.cwd().parent)\n",
    "\n",
    "sys.path.insert(0, _find_module_root())\n",
    "\n",
    "from micrograd_plus import (\n",
    "    Tensor, Linear, ReLU, Sigmoid, Dropout,\n",
    "    CrossEntropyLoss, Adam, SGD, Sequential\n",
    ")\n",
    "from micrograd_plus.utils import set_seed, DataLoader\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Loading MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_mnist(path=None, timeout=30):\n",
    "    \"\"\"\n",
    "    Download MNIST dataset with fallback URLs and timeout.\n",
    "    \n",
    "    Uses multiple mirror URLs to ensure reliability:\n",
    "    1. PyTorch/OSSCI S3 mirror (most reliable)\n",
    "    2. Original yann.lecun.com (backup)\n",
    "    \n",
    "    Args:\n",
    "        path: Directory to save files.\n",
    "        timeout: Download timeout in seconds.\n",
    "    \"\"\"\n",
    "    # Resolve path relative to module root if not specified\n",
    "    if path is None:\n",
    "        path = os.path.join(_find_module_root(), 'data')\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    # Primary and fallback URLs\n",
    "    urls = [\n",
    "        'https://ossci-datasets.s3.amazonaws.com/mnist/',  # PyTorch mirror (reliable)\n",
    "        'http://yann.lecun.com/exdb/mnist/',  # Original (sometimes unreliable)\n",
    "    ]\n",
    "    \n",
    "    files = [\n",
    "        'train-images-idx3-ubyte.gz',\n",
    "        'train-labels-idx1-ubyte.gz',\n",
    "        't10k-images-idx3-ubyte.gz',\n",
    "        't10k-labels-idx1-ubyte.gz'\n",
    "    ]\n",
    "    \n",
    "    for f in files:\n",
    "        filepath = os.path.join(path, f)\n",
    "        if not os.path.exists(filepath):\n",
    "            downloaded = False\n",
    "            for base_url in urls:\n",
    "                try:\n",
    "                    print(f\"Downloading {f} from {base_url}...\")\n",
    "                    # Set timeout using socket\n",
    "                    old_timeout = socket.getdefaulttimeout()\n",
    "                    socket.setdefaulttimeout(timeout)\n",
    "                    try:\n",
    "                        request.urlretrieve(base_url + f, filepath)\n",
    "                        downloaded = True\n",
    "                        print(f\"  Success!\")\n",
    "                        break\n",
    "                    finally:\n",
    "                        socket.setdefaulttimeout(old_timeout)\n",
    "                except Exception as e:\n",
    "                    print(f\"  Failed: {e}\")\n",
    "                    # Remove partial download if exists\n",
    "                    if os.path.exists(filepath):\n",
    "                        os.remove(filepath)\n",
    "                    continue\n",
    "            \n",
    "            if not downloaded:\n",
    "                raise RuntimeError(\n",
    "                    f\"Could not download {f} from any source.\\n\"\n",
    "                    f\"Please download manually from:\\n\"\n",
    "                    f\"  https://ossci-datasets.s3.amazonaws.com/mnist/{f}\\n\"\n",
    "                    f\"and place in {os.path.abspath(path)}/\"\n",
    "                )\n",
    "    \n",
    "    print(\"MNIST data ready!\")\n",
    "\n",
    "def load_mnist(path=None):\n",
    "    \"\"\"\n",
    "    Load MNIST data from gzipped files.\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test: Training and test data/labels.\n",
    "        (Follows sklearn train_test_split convention)\n",
    "    \"\"\"\n",
    "    # Resolve path relative to module root if not specified\n",
    "    if path is None:\n",
    "        path = os.path.join(_find_module_root(), 'data')\n",
    "    \n",
    "    def load_images(filename):\n",
    "        with gzip.open(os.path.join(path, filename), 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        return data.reshape(-1, 784).astype(np.float32) / 255.0\n",
    "    \n",
    "    def load_labels(filename):\n",
    "        with gzip.open(os.path.join(path, filename), 'rb') as f:\n",
    "            return np.frombuffer(f.read(), np.uint8, offset=8).astype(np.int32)\n",
    "    \n",
    "    X_train = load_images('train-images-idx3-ubyte.gz')\n",
    "    y_train = load_labels('train-labels-idx1-ubyte.gz')\n",
    "    X_test = load_images('t10k-images-idx3-ubyte.gz')\n",
    "    y_test = load_labels('t10k-labels-idx1-ubyte.gz')\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Download and load\n",
    "download_mnist()\n",
    "X_train, X_test, y_train, y_test = load_mnist()\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} images\")\n",
    "print(f\"Test set: {X_test.shape[0]} images\")\n",
    "print(f\"Image size: {X_train.shape[1]} pixels (28x28 flattened)\")\n",
    "print(f\"Pixel value range: [{X_train.min():.1f}, {X_train.max():.1f}]\")\n",
    "print(f\"Label distribution: {np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some samples\n",
    "fig, axes = plt.subplots(2, 10, figsize=(15, 3))\n",
    "\n",
    "for i in range(20):\n",
    "    ax = axes[i // 10, i % 10]\n",
    "    ax.imshow(X_train[i].reshape(28, 28), cmap='gray')\n",
    "    ax.set_title(f'{y_train[i]}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Sample MNIST Images', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Building the Model\n",
    "\n",
    "We'll build a simple Multi-Layer Perceptron (MLP):\n",
    "- Input: 784 features (28x28 pixels)\n",
    "- Hidden Layer 1: 256 neurons + ReLU\n",
    "- Hidden Layer 2: 128 neurons + ReLU\n",
    "- Output: 10 classes (digits 0-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "# Build model\n",
    "model = Sequential(\n",
    "    Linear(784, 256),\n",
    "    ReLU(),\n",
    "    Dropout(0.2),\n",
    "    Linear(256, 128),\n",
    "    ReLU(),\n",
    "    Dropout(0.2),\n",
    "    Linear(128, 10)\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.size for p in model.parameters())\n",
    "print(f\"Model Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "# Setup training\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X, y, batch_size=256):\n",
    "    \"\"\"Evaluate model accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i in range(0, len(X), batch_size):\n",
    "        X_batch = Tensor(X[i:i+batch_size])\n",
    "        y_batch = y[i:i+batch_size]\n",
    "        \n",
    "        logits = model(X_batch)\n",
    "        predictions = np.argmax(logits.data, axis=1)\n",
    "        \n",
    "        correct += np.sum(predictions == y_batch)\n",
    "        total += len(y_batch)\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "def train_epoch(model, X, y, loss_fn, optimizer, batch_size=64):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    indices = np.random.permutation(len(X))\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for i in range(0, len(X), batch_size):\n",
    "        batch_idx = indices[i:i+batch_size]\n",
    "        X_batch = Tensor(X[batch_idx], requires_grad=True)\n",
    "        y_batch = Tensor(y[batch_idx])\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(X_batch)\n",
    "        loss = loss_fn(logits, y_batch)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    return total_loss / n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Use smaller subset for faster training (optional)\n",
    "# Comment these lines to use full dataset\n",
    "TRAIN_SIZE = 10000\n",
    "X_train_subset = X_train[:TRAIN_SIZE]\n",
    "y_train_subset = y_train[:TRAIN_SIZE]\n",
    "\n",
    "# History tracking\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_acc': []\n",
    "}\n",
    "\n",
    "print(f\"Training on {len(X_train_subset)} samples for {NUM_EPOCHS} epochs...\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, X_train_subset, y_train_subset, \n",
    "                             loss_fn, optimizer, BATCH_SIZE)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_acc = evaluate(model, X_train_subset, y_train_subset)\n",
    "    test_acc = evaluate(model, X_test, y_test)\n",
    "    \n",
    "    # Record history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}/{NUM_EPOCHS} | \"\n",
    "          f\"Loss: {train_loss:.4f} | \"\n",
    "          f\"Train Acc: {train_acc:.2%} | \"\n",
    "          f\"Test Acc: {test_acc:.2%} | \"\n",
    "          f\"Time: {epoch_time:.1f}s\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(\"-\" * 60)\n",
    "print(f\"Training complete! Total time: {total_time:.1f}s\")\n",
    "print(f\"Final test accuracy: {history['test_acc'][-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Visualizing Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(history['train_loss'], 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[1].plot(history['train_acc'], 'b-', linewidth=2, label='Train')\n",
    "axes[1].plot(history['test_acc'], 'r-', linewidth=2, label='Test')\n",
    "axes[1].axhline(y=0.95, color='g', linestyle='--', alpha=0.5, label='95% target')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if history['test_acc'][-1] >= 0.95:\n",
    "    print(\"Congratulations! You achieved >95% test accuracy!\")\n",
    "else:\n",
    "    print(f\"Current accuracy: {history['test_acc'][-1]:.2%}. \"\n",
    "          f\"Try training for more epochs or adjusting hyperparameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Visualizing Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some predictions\n",
    "model.eval()\n",
    "\n",
    "# Get predictions on test set\n",
    "test_logits = model(Tensor(X_test[:100]))\n",
    "test_probs = test_logits.softmax().data\n",
    "test_preds = np.argmax(test_probs, axis=1)\n",
    "\n",
    "# Find correct and incorrect predictions\n",
    "correct_mask = test_preds == y_test[:100]\n",
    "incorrect_indices = np.where(~correct_mask)[0]\n",
    "\n",
    "# Show some predictions\n",
    "fig, axes = plt.subplots(3, 10, figsize=(15, 5))\n",
    "\n",
    "for i in range(30):\n",
    "    ax = axes[i // 10, i % 10]\n",
    "    ax.imshow(X_test[i].reshape(28, 28), cmap='gray')\n",
    "    \n",
    "    pred = test_preds[i]\n",
    "    true = y_test[i]\n",
    "    conf = test_probs[i, pred] * 100\n",
    "    \n",
    "    color = 'green' if pred == true else 'red'\n",
    "    ax.set_title(f'{pred} ({conf:.0f}%)', color=color, fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Predictions (green=correct, red=wrong)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some misclassified examples\n",
    "if len(incorrect_indices) > 0:\n",
    "    print(\"Misclassified examples:\")\n",
    "    \n",
    "    n_show = min(10, len(incorrect_indices))\n",
    "    fig, axes = plt.subplots(2, n_show, figsize=(n_show * 1.5, 4))\n",
    "    \n",
    "    for i, idx in enumerate(incorrect_indices[:n_show]):\n",
    "        # Show image\n",
    "        axes[0, i].imshow(X_test[idx].reshape(28, 28), cmap='gray')\n",
    "        axes[0, i].set_title(f'True: {y_test[idx]}\\nPred: {test_preds[idx]}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Show probability distribution\n",
    "        axes[1, i].bar(range(10), test_probs[idx])\n",
    "        axes[1, i].set_ylim(0, 1)\n",
    "        axes[1, i].set_xticks(range(10))\n",
    "        axes[1, i].set_xlabel('Class')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No misclassified examples in the first 100 test samples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "for i in range(0, len(X_test), 256):\n",
    "    logits = model(Tensor(X_test[i:i+256]))\n",
    "    preds = np.argmax(logits.data, axis=1)\n",
    "    all_preds.extend(preds)\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "\n",
    "# Build confusion matrix\n",
    "confusion = np.zeros((10, 10), dtype=np.int32)\n",
    "for pred, true in zip(all_preds, y_test):\n",
    "    confusion[true, pred] += 1\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(confusion, cmap='Blues')\n",
    "plt.colorbar(label='Count')\n",
    "\n",
    "# Add numbers\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        color = 'white' if confusion[i, j] > confusion.max() / 2 else 'black'\n",
    "        plt.text(j, i, str(confusion[i, j]), ha='center', va='center', color=color)\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(range(10))\n",
    "plt.yticks(range(10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"Per-class accuracy:\")\n",
    "for i in range(10):\n",
    "    class_acc = confusion[i, i] / confusion[i].sum()\n",
    "    print(f\"  Digit {i}: {class_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: What the Model Learned\n",
    "\n",
    "Let's visualize the first layer weights to see what patterns the model detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first layer weights\n",
    "first_layer_weights = model[0].weight.data.T  # (256, 784)\n",
    "\n",
    "# Show first 64 filters\n",
    "fig, axes = plt.subplots(8, 8, figsize=(10, 10))\n",
    "\n",
    "for i in range(64):\n",
    "    ax = axes[i // 8, i % 8]\n",
    "    w = first_layer_weights[i].reshape(28, 28)\n",
    "    ax.imshow(w, cmap='RdBu', vmin=-np.abs(w).max(), vmax=np.abs(w).max())\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('First Layer Filters (Red=positive, Blue=negative)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each filter looks for different patterns in the image.\")\n",
    "print(\"Red areas activate for bright pixels, blue areas activate for dark pixels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try It Yourself: Improve the Model\n",
    "\n",
    "Try these modifications to improve accuracy:\n",
    "\n",
    "1. **More training**: Increase `NUM_EPOCHS` to 20 or more\n",
    "2. **Larger model**: Add more neurons or layers\n",
    "3. **Different optimizer**: Try SGD with momentum\n",
    "4. **Learning rate tuning**: Try 0.0001 or 0.01\n",
    "5. **Full dataset**: Use all 60,000 training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Experiment with different configurations\n",
    "\n",
    "# Example: Larger model\n",
    "# model_large = Sequential(\n",
    "#     Linear(784, 512),\n",
    "#     ReLU(),\n",
    "#     Dropout(0.3),\n",
    "#     Linear(512, 256),\n",
    "#     ReLU(),\n",
    "#     Dropout(0.3),\n",
    "#     Linear(256, 10)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've accomplished:\n",
    "- Loaded and preprocessed the MNIST dataset\n",
    "- Built an MLP classifier with your MicroGrad+ library\n",
    "- Trained to >95% accuracy on handwritten digits\n",
    "- Visualized training progress and model predictions\n",
    "- Analyzed model behavior with confusion matrix and filter visualization\n",
    "\n",
    "### Performance Note\n",
    "\n",
    "MicroGrad+ is **10-100x slower than PyTorch** by design - it's built for learning, not speed.\n",
    "In Domain 2, you'll use PyTorch with GPU acceleration for production-level performance.\n",
    "\n",
    "### Memory Management\n",
    "\n",
    "When training on the full 60,000 samples or experimenting with larger models,\n",
    "you may want to clear memory between runs:\n",
    "\n",
    "```python\n",
    "import gc\n",
    "gc.collect()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup - release memory\n",
    "from micrograd_plus.utils import cleanup_notebook\n",
    "cleanup_notebook(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to the final lab:\n",
    "- **Lab 1.7.6**: Documentation and Benchmarks - documenting your library and comparing with PyTorch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
