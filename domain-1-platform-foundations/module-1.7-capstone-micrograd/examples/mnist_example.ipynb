{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MicroGrad+ MNIST Example\n",
    "\n",
    "This notebook demonstrates how to train a neural network on MNIST using MicroGrad+.\n",
    "\n",
    "**Goal**: Train a multi-layer perceptron to achieve >95% accuracy on MNIST.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport sys\nimport os\nimport gzip\nimport socket\nfrom urllib import request\nfrom pathlib import Path\n\n# Robust path resolution - works regardless of working directory\ndef _find_module_root():\n    \"\"\"Find the module root directory containing micrograd_plus.\"\"\"\n    current = Path.cwd()\n    for parent in [current] + list(current.parents):\n        if (parent / 'micrograd_plus' / '__init__.py').exists():\n            return str(parent)\n    return str(Path.cwd().parent)\n\nsys.path.insert(0, _find_module_root())\n\nfrom micrograd_plus import (\n    Tensor, Linear, ReLU, Dropout, Sequential,\n    CrossEntropyLoss, Adam\n)\nfrom micrograd_plus.utils import set_seed, DataLoader\n\n# Set seed for reproducibility\nset_seed(42)\n\nprint(\"MicroGrad+ MNIST Example\")\nprint(\"=\" * 50)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load MNIST Dataset\n",
    "\n",
    "We'll download MNIST from sklearn or keras and preprocess it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def download_mnist(path=None, timeout=30):\n    \"\"\"\n    Download MNIST dataset with fallback URLs and timeout.\n    \n    Uses multiple mirror URLs to ensure reliability:\n    1. PyTorch/OSSCI S3 mirror (most reliable)\n    2. Original yann.lecun.com (backup)\n    \"\"\"\n    # Resolve path relative to module root if not specified\n    if path is None:\n        path = os.path.join(_find_module_root(), 'data')\n    os.makedirs(path, exist_ok=True)\n    \n    urls = [\n        'https://ossci-datasets.s3.amazonaws.com/mnist/',  # PyTorch mirror (reliable)\n        'http://yann.lecun.com/exdb/mnist/',  # Original (sometimes unreliable)\n    ]\n    \n    files = [\n        'train-images-idx3-ubyte.gz',\n        'train-labels-idx1-ubyte.gz',\n        't10k-images-idx3-ubyte.gz',\n        't10k-labels-idx1-ubyte.gz'\n    ]\n    \n    for f in files:\n        filepath = os.path.join(path, f)\n        if not os.path.exists(filepath):\n            downloaded = False\n            for base_url in urls:\n                try:\n                    print(f\"Downloading {f} from {base_url}...\")\n                    old_timeout = socket.getdefaulttimeout()\n                    socket.setdefaulttimeout(timeout)\n                    try:\n                        request.urlretrieve(base_url + f, filepath)\n                        downloaded = True\n                        print(f\"  Success!\")\n                        break\n                    finally:\n                        socket.setdefaulttimeout(old_timeout)\n                except Exception as e:\n                    print(f\"  Failed: {e}\")\n                    if os.path.exists(filepath):\n                        os.remove(filepath)\n                    continue\n            \n            if not downloaded:\n                raise RuntimeError(\n                    f\"Could not download {f}.\\n\"\n                    f\"Please download manually from:\\n\"\n                    f\"  https://ossci-datasets.s3.amazonaws.com/mnist/{f}\"\n                )\n    \n    print(\"MNIST data ready!\")\n\n\ndef load_mnist_local(path=None):\n    if path is None:\n        path = os.path.join(_find_module_root(), 'data')\n    \"\"\"Load MNIST from local gzipped files.\"\"\"\n    def load_images(filename):\n        with gzip.open(os.path.join(path, filename), 'rb') as f:\n            data = np.frombuffer(f.read(), np.uint8, offset=16)\n        return data.reshape(-1, 784).astype(np.float32) / 255.0\n    \n    def load_labels(filename):\n        with gzip.open(os.path.join(path, filename), 'rb') as f:\n            return np.frombuffer(f.read(), np.uint8, offset=8).astype(np.int32)\n    \n    X_train = load_images('train-images-idx3-ubyte.gz')\n    y_train = load_labels('train-labels-idx1-ubyte.gz')\n    X_test = load_images('t10k-images-idx3-ubyte.gz')\n    y_test = load_labels('t10k-labels-idx1-ubyte.gz')\n    \n    return X_train, X_test, y_train, y_test\n\n\ndef load_mnist():\n    \"\"\"\n    Load MNIST dataset with multiple fallback methods.\n    \n    Priority:\n    1. Local files (if already downloaded)\n    2. Download from reliable mirrors\n    3. sklearn's fetch_openml (slower but reliable)\n    4. tensorflow/keras (if available)\n    \n    Returns:\n        X_train, X_test: (N, 784) normalized images\n        y_train, y_test: (N,) integer labels 0-9\n    \"\"\"\n    data_path = os.path.join(_find_module_root(), 'data')\n    local_files = ['train-images-idx3-ubyte.gz', 'train-labels-idx1-ubyte.gz',\n                   't10k-images-idx3-ubyte.gz', 't10k-labels-idx1-ubyte.gz']\n    \n    # Method 1: Check for local files\n    if all(os.path.exists(os.path.join(data_path, f)) for f in local_files):\n        print(\"Loading MNIST from local files...\")\n        return load_mnist_local(data_path)\n    \n    # Method 2: Download from mirrors\n    try:\n        print(\"Downloading MNIST from mirrors...\")\n        download_mnist(data_path)\n        return load_mnist_local(data_path)\n    except Exception as e:\n        print(f\"Mirror download failed: {e}\")\n    \n    # Method 3: Try sklearn\n    try:\n        from sklearn.datasets import fetch_openml\n        print(\"Loading MNIST from sklearn (may be slow on first run)...\")\n        mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\n        X, y = mnist.data.astype(np.float32), mnist.target.astype(np.int32)\n        X = X / 255.0\n        return X[:60000], X[60000:], y[:60000], y[60000:]\n    except Exception as e:\n        print(f\"sklearn failed: {e}\")\n    \n    # Method 4: Try keras/tensorflow\n    try:\n        from tensorflow.keras.datasets import mnist\n        print(\"Loading MNIST from keras...\")\n        (X_train, y_train), (X_test, y_test) = mnist.load_data()\n        X_train = X_train.reshape(-1, 784).astype(np.float32) / 255.0\n        X_test = X_test.reshape(-1, 784).astype(np.float32) / 255.0\n        return X_train, X_test, y_train.astype(np.int32), y_test.astype(np.int32)\n    except Exception as e:\n        print(f\"keras failed: {e}\")\n    \n    raise RuntimeError(\n        \"Could not load MNIST from any source.\\n\"\n        \"Please download manually from:\\n\"\n        \"  https://ossci-datasets.s3.amazonaws.com/mnist/\\n\"\n        \"and place files in the data/ directory.\"\n    )\n\nX_train, X_test, y_train, y_test = load_mnist()\n\nprint(f\"\\nDataset loaded:\")\nprint(f\"  Training: {X_train.shape[0]} samples\")\nprint(f\"  Test:     {X_test.shape[0]} samples\")\nprint(f\"  Image shape: 28x28 = 784 pixels\")\nprint(f\"  Classes: 0-9 digits\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Some Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some training examples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_train[i].reshape(28, 28), cmap='gray')\n",
    "    ax.set_title(f'Label: {y_train[i]}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Sample MNIST Images', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a subset for faster training (or full dataset for best accuracy)\n",
    "TRAIN_SIZE = 10000  # Use 10000 for quick demo, 60000 for full training\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(X_train[:TRAIN_SIZE], y_train[:TRAIN_SIZE], \n",
    "                          batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(X_test, y_test, \n",
    "                         batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define the Model\n",
    "\n",
    "We'll create a multi-layer perceptron (MLP) with:\n",
    "- Input: 784 features (28x28 pixels)\n",
    "- Hidden layers: 256 -> 128 neurons with ReLU\n",
    "- Output: 10 classes (digits 0-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential(\n",
    "    Linear(784, 256),\n",
    "    ReLU(),\n",
    "    Dropout(0.2),\n",
    "    Linear(256, 128),\n",
    "    ReLU(),\n",
    "    Dropout(0.2),\n",
    "    Linear(128, 10)\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.data.size for p in model.parameters())\n",
    "print(f\"Model Architecture:\")\n",
    "print(f\"  Input:  784 (28x28 pixels)\")\n",
    "print(f\"  Hidden: 256 -> ReLU -> Dropout(0.2)\")\n",
    "print(f\"  Hidden: 128 -> ReLU -> Dropout(0.2)\")\n",
    "print(f\"  Output: 10 (digits 0-9)\")\n",
    "print(f\"\\nTotal Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training settings\n",
    "EPOCHS = 10\n",
    "\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: 0.001\")\n",
    "print(f\"  Optimizer: Adam\")\n",
    "print(f\"  Loss: CrossEntropyLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, loss_fn, optimizer):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X = Tensor(X_batch, requires_grad=True)\n",
    "        y = Tensor(y_batch)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(X)\n",
    "        loss = loss_fn(logits, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        total_loss += loss.item() * len(y_batch)\n",
    "        predictions = np.argmax(logits.data, axis=1)\n",
    "        correct += np.sum(predictions == y_batch)\n",
    "        total += len(y_batch)\n",
    "    \n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, loss_fn):\n",
    "    \"\"\"Evaluate on test set.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X = Tensor(X_batch)\n",
    "        y = Tensor(y_batch)\n",
    "        \n",
    "        logits = model(X)\n",
    "        loss = loss_fn(logits, y)\n",
    "        \n",
    "        total_loss += loss.item() * len(y_batch)\n",
    "        predictions = np.argmax(logits.data, axis=1)\n",
    "        correct += np.sum(predictions == y_batch)\n",
    "        total += len(y_batch)\n",
    "    \n",
    "    return total_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training!\n",
    "print(\"\\nStarting Training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, loss_fn, optimizer)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = evaluate(model, test_loader, loss_fn)\n",
    "    \n",
    "    # Record history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}% | \"\n",
    "          f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc*100:.2f}%\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Final Test Accuracy: {history['test_acc'][-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "ax1.plot(history['test_loss'], 'r-', label='Test Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Test Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot([a*100 for a in history['train_acc']], 'b-', label='Train Accuracy', linewidth=2)\n",
    "ax2.plot([a*100 for a in history['test_acc']], 'r-', label='Test Accuracy', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Test Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "model.eval()\n",
    "\n",
    "# Get predictions for first 20 test images\n",
    "X_sample = Tensor(X_test[:20])\n",
    "logits = model(X_sample)\n",
    "predictions = np.argmax(logits.data, axis=1)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 10, figsize=(15, 4))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_test[i].reshape(28, 28), cmap='gray')\n",
    "    \n",
    "    color = 'green' if predictions[i] == y_test[i] else 'red'\n",
    "    ax.set_title(f'Pred: {predictions[i]}\\nTrue: {y_test[i]}', \n",
    "                 color=color, fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Model Predictions (Green=Correct, Red=Wrong)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "for X_batch, y_batch in test_loader:\n",
    "    X = Tensor(X_batch)\n",
    "    logits = model(X)\n",
    "    predictions = np.argmax(logits.data, axis=1)\n",
    "    all_predictions.extend(predictions)\n",
    "    all_labels.extend(y_batch)\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Build confusion matrix\n",
    "confusion = np.zeros((10, 10), dtype=np.int32)\n",
    "for true, pred in zip(all_labels, all_predictions):\n",
    "    confusion[true, pred] += 1\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(confusion, cmap='Blues')\n",
    "plt.colorbar(label='Count')\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        color = 'white' if confusion[i, j] > confusion.max() / 2 else 'black'\n",
    "        plt.text(j, i, str(confusion[i, j]), ha='center', va='center', color=color)\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(range(10))\n",
    "plt.yticks(range(10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nPer-class Accuracy:\")\n",
    "for i in range(10):\n",
    "    class_acc = confusion[i, i] / confusion[i].sum() * 100\n",
    "    print(f\"  Digit {i}: {class_acc:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 11. Summary\n\nIn this example, we:\n\n1. **Loaded MNIST** - 60,000 training + 10,000 test images\n2. **Built an MLP** - 784 -> 256 -> 128 -> 10 with dropout\n3. **Trained with Adam** - Learning rate 0.001\n4. **Achieved >95% accuracy** on the test set\n\n### Performance Note\n\nMicroGrad+ is an educational implementation built in pure Python/NumPy. It is intentionally\n**10-100x slower than PyTorch**, which uses optimized C++/CUDA kernels. This is expected!\n\nThe goal is understanding, not speed. Once you understand how autograd works internally,\nyou can use PyTorch (Phase 2) for production workloads with full GPU acceleration.\n\n### Tips for Higher Accuracy:\n- Train on full 60,000 samples\n- Add more epochs (20-30)\n- Try learning rate scheduling\n- Add data augmentation (shifts, rotations)\n- Use deeper/wider networks\n\n### Memory Management\n\nWhen training larger models or using the full dataset, run garbage collection:\n```python\nimport gc\ngc.collect()\n```"
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Cleanup",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cleanup - release memory\nfrom micrograd_plus.utils import cleanup_notebook\ncleanup_notebook(globals())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}